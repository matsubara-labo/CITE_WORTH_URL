sentence
Experimental studies of interactive language use have shed light on the cognitive and interpersonal processes that shape conversation; corpora are the emergent products of these processes.
"I will survey studies that focus on under-modelled aspects of interactive language use, including the processing of spontaneous speech and disfluencies; metalinguistic displays such as hedges; interactive processes that affect choices of referring expressions; and how communication media shape conversations."
The findings suggest some agendas for computational linguistics.
"Language is shaped not only by grammar, but also by the cognitive processing of speakers and addressees, and by the medium in which it is used."
"These forces have, until recently, received little attention, having been originally consigned to &quot;performance&quot; by Chomsky, and considered to be of secondary importance by many others."
"But as anyone who has listened to a tape of herself lecturing surely knows, spoken language is formally quite different from written language."
"And as those who have transcribed conversation are excruciatingly aware, interactive, spontaneous speech is especially messy and disfluent."
"This fact is rarely acknowledged by psychological theories of comprehension and production (although see Brennan &amp; Schober, in press;[REF_CITE]1997;[REF_CITE])."
"In fact, experimental psycholinguists still make up most of their materials, so that much of what we know about sentence processing is based on a sanitized, ideal form of language that no one actually speaks."
But the field of computational linguistics has taken an interesting turn:
Linguists and computational linguists who formerly used made-up sentences are now using naturally- and experimentally-generated corpora on which to base and test their theories.
One of the most exciting developments since the early 1990s has been the focus on corpus data.
"Organized efforts such as LDC and ELRA have assembled large and varied corpora of speech and text, making them widely available to researchers and creators of natural language and speech recognition systems."
"Finally, Internet usage has generated huge corpora of interactive spontaneous text or &quot;visible conversations&quot; that little resemble edited texts."
"Of course, ethnographers and sociolinguists who practice conversation analysis (e.g., Sacks, Schegloff, &amp;[REF_CITE]) have known for a long time that spontaneous interaction is interesting in its own right, and that although conversation seems messy at first glance, it is actually orderly."
"Conversation analysts have demonstrated that speakers coordinate with each other such feats as achieving a joint focus of attention, producing closely timed turn exchanges, and finishing each another’s utterances."
"These demonstrations have been compelling enough to inspire researchers from psychology, linguistics, computer science, and human-computer interaction to turn their attention to naturalistic language data."
"But it is important to keep in mind that a corpus is, after all, only an artifact—a product that emerges from the processes that occur between and within speakers and addressees."
"Researchers who analyze the textual records of conversation are only overhearers, and there is ample evidence that overhearers experience a conversation quite differently from addressees and from side participants (Schober &amp;[REF_CITE]; Wilkes-Gibbs &amp;[REF_CITE])."
"With a corpus alone, there is no independent evidence of what people actually intend or understand at different points in a conversation, or why they make the choices they do."
"Conversation experiments that provide partners with a task to do have much to offer, such as independent measures of communicative success as well as evidence of precisely when one partner is confused or has reached a hypothesis about the other’s beliefs or intentions."
Task-oriented corpora in combination with information about how they were generated are important for discourse studies.
We still don&apos;t know nearly enough about the cognitive and interpersonal processes that underlie spontaneous language use—how speaking and listening are coordinated between individuals as well as within the mind of someone who is switching speaking and listening roles in rapid succession.
"Hence, determining what information needs to be represented moment by moment in a dialog model, as well as how and when it should be updated and used, is still an open frontier."
In this paper I start with an example and identify some distinctive features of spoken language interchanges.
Then describeI several experiments aimed at understanding the processes that generate them.
I conclude by proposing some desiderata for a dialog model.
"To begin, consider the following conversational interchange from a laboratory experiment on referential communication."
"A director and a matcher who could not see each another were trying to get identical sets of picture cards lined up in the same order. (1) D:ah boy this one ah boy all right it looks kinda like-on the right top there’s a square that looks diagonal M: uh huh D: and you have sort of another like rectangle shape, the-like a triangle, angled, and on the bottom it’s uh I don’t know what that is, glass shaped (Stellmann &amp;[REF_CITE])"
Several things are apparent from this exchange.
"First, it contains several disfluencies or interruptions in fluent speech."
The director restarts her first turn twice and her second turn once.
"She delivers a description in a series of installments, with backchannels from the matcher to confirm them."
"She seasons her speech with fillers like uh, pauses occasionally, and displays her commitment (or lack thereof) to what she is saying with displays like ah boy this one ah boy and I don’t know what that is."
"Even though she is the one who knows what the target picture is, it is the matcher who ends up proposing the description that they both end up ratifying: like a monk praying or something."
"Once the director has ratified this proposal, they have succeeded in establishing a conceptual pact (see Brennan &amp;[REF_CITE])."
"En route, both partners hedged their descriptions liberally, marking them as provisional, pending evidence of acceptance from the other."
"This example is typical; in fact, 24 pairs of partners who discussed this object ended up synthesizing nearly 24 different but mutually agreed-upon perspectives."
"Finally, the disfluencies, hedges, and turns would have been distributed quite differently if this conversation had been conducted over a different medium—through instant messaging, or if the partners had had visual contact."
Next I will consider the proceses that underlie these aspects of interactive spoken communication.
"The implicit assumptions of psychological and computational theories that ignore disfluencies must be either that people aren&apos;t disfluent, or that disfluencies make processing more difficult, and so theories of fluent speech processing should be developed before the research agenda turns to disfluent speech processing."
"The first assumption is clearly false; disfluency rates in spontaneous speech are estimated[REF_CITE]and by Bortfeld, Leon, Bloom, Schober, and[REF_CITE]to be about 6 disfluencies per 100 words, not including silent pauses."
"The rate is lower for speech to machines[REF_CITE], due in part to utterance length; that is, disfluency rates are higher in longer utterances, where planning is more difficult, and utterances addressed to machines tend to be shorter than those addressed to people, often because dialogue interfaces are designed to take on more initiative."
"The average speaker may believe, quite rightly, that machines are imperfect speech processors, and plan their utterances to machines more carefully."
The good news is that speakers can adapt to machines; the bad news is that they do so by recruiting limited cognitive resources that could otherwise be focused on the task itself.
"As for the second assumption, if the goal is to eventually process unrestricted, natural human speech, then committing to an early and exclusive focus on processing fluent utterances is risky."
"In humans, speech production and speech processing are done incrementally, using contextual information from the earliest moments of processing (see, e.g.,[REF_CITE])."
This sort of processing requires quite a different architecture and different mechanisms for ambiguity resolution than one that begins processing only at the end of a complete and well-formed utterance.
"Few approaches to parsing have tried to handle disfluent utterances (notable exceptions are Core &amp;[REF_CITE]; Nakatani &amp;[REF_CITE]; Shriberg, Bear, &amp;[REF_CITE])."
The few psycholinguistic experiments that have examined human processing of disfluent speech also throw into question the assumption that disfluent speech is harder to process than fluent speech.
"What information exists in disfluencies, and how might speakers use it?"
"Speech production processes can be broken into three phases: a message or semantic process, a formulation process in which a syntactic frame is chosen and words are filled in, and an articulation process ([REF_CITE]; Bock &amp;[REF_CITE])."
"Speakers monitor their speech both internally and externally; that is, they can make covert repairs at the point when an internal monitoring loop checks the output of the formulation phase before articulation begins, or overt repairs when a problem is discovered after the articulation phase via the speaker&apos;s external monitor—the point at which listeners also have access to the signal[REF_CITE]."
"According to Nooteboom&apos;s (1980) Main Interruption Rule, speakers tend to halt speaking as soon as they detect a problem."
Production data from Levelt&apos;s (1983) corpus supported this rule; speakers interrupted themselves within or right after a problem word 69% of the time.
How are regularities in disfluencies exploited by listeners?
"We have looked at the comprehension of simple fluent and disfluent instructions in a constrained situation where the listener had the opportunity to develop expectations about what the speaker would say (Brennan &amp; Schober, in press)."
"We tested two hypotheses drawn from some suggestions of Levelt&apos;s (1989): that &quot;by interrupting a word, a speaker signals to the addressee that the word is an error,&quot; and that an editing expression like er or uh may &quot;warn the addressee that the current message is to be replaced,&quot; as with Move to the ye— uh, orange square."
"We collected naturally fluent and disfluent utterances by having a speaker watch a display of objects; when one was highlighted he issued a command about it, like &quot;move to the yellow square.&quot; Sometimes the highlight changed suddenly; this sometimes caused the speaker to produce disfluencies."
"We recorded enough tokens of simple disfluencies to compare the impact of three ways in which speakers interrupt themselves: immediately after a problem word, within a problem word, or within a problem word and with the filler uh."
"We reasoned that if a disfluency indeed bears useful information, then we should be able to find a situation where a target word is faster to comprehend in a disfluent utterance than in a fluent one."
Imagine a situation in which a listener expects a speaker to refer to one of two objects.
"If the speaker begins to name one and then stops and names the other, the way in which she interrupts the utterance might be an early clue as to her intentions."
So the listener may be faster to recognize her intentions relative to a target word in a disfluent utterance than in an utterance in which disfluencies are absent.
"We compared the following types of utterances: a. Move to the orange square (naturally fluent) b. Move to the |orange square (disfluency excised) c. Move to the yellow- orange square d. Move to the ye- orange square e. Move to the ye- uh, orange square f. Move to the orange square g. Move to the ye- orange square h. Move to the uh, orange square Utterances c, d, and e were spontaneous disfluencies, and f, g, and h were edited versions that replaced the removed material with pauses of equal length to control for timing."
"In utterances c—h, the reparandum began after the word the and continued until the interruption site (after the unintended color word, color word fragment, or location where this information had been edited out)."
"The edit interval in c—h began with the interruption site, included silence or a filler, and ended with the onset of the repair color word."
"Response times were calculated relative to the onset of the repair, orange."
"The results were that listeners made fewer errors, the less incorrect information they heard in the reparandum (that is, the shorter the reparandum), and they were faster to respond to the target word when the edit interval before the repair was longer."
"They comprehended target words after mid-word interruptions with fillers faster than they did after mid-word interruptions without fillers (since a filler makes the edit interval longer), and faster than they did when the disfluency was replaced by a pause of equal length."
"This filler advantage did not occur at the expense of accuracy—unlike with disfluent utterances without fillers, listeners made no more errors on disfluent utterances with fillers than they did on fluent utterances."
These findings highlight the importance of timing in speech recognition and utterance interpretation.
The form and length of the reparandum and edit interval bear consequences for how quickly a disfluent utterance is processed as well as for whether the listener makes a commitment to an interpretation the speaker does not intend.
"Listeners respond to pauses and fillers on other levels as well, such as to make inferences about speakers’ alignment to their utterances."
"People coordinate both the content and the process of conversation; fillers, pauses, and self-speech can serve as displays by speakers that provide an account to listeners for difficulties or delays in speaking ([REF_CITE]; Clark &amp;[REF_CITE])."
Speakers signal their Feeling-of-Knowing (FOK) when answering a question by the displays they put on right before the answer (or right before they respond with I don’t know) (Brennan &amp;[REF_CITE]; Smith &amp;[REF_CITE]).
"In these experiments, longer latencies, especially ones that contained fillers, were associated with answers produced with a lower FOK and that turned out to be incorrect."
"Thus in the following example, A1 displayed a lower FOK than A2:"
Q: Who founded the American Red Cross?
A1: .....um......... Florence Nightingale?
"Likewise, non-answers (e.g., I don’t know) after a filler or a long latency were produced by speakers who were more likely to recognize the correct answers later on a multiple choice test; those who produced a non-answer immediately did not know the answers."
"Not only do speakers display their difficulties and metalinguistic knowledge using such devices, but listeners can process this information to produce an accurate Feeling-of-Another&apos;s-Knowing, or estimate of the speaker’s likelihood of knowing the correct answer (Brennan &amp;[REF_CITE])."
These programs of experiments hold implications for both the generation and interpretation of spoken utterances.
"A system could indicate its confidence in its message with silent pauses, fillers, and intonation, and users should be able to interpret this information accurately."
"If machine speech recognition were conducted in a fashion more like human speech recognition, timing would be a critical cue and incremental parses would be continually made and unmade."
"Although this approach would be computationally expensive, it might produce better results with spontaneous speech."
Consider again the exchange in Example (1).
"After some work, the director and matcher eventually settled on a mutual perspective."
"When they finished matching the set of 12 picture cards, the cards were shuffled and the task was repeated several more times."
"In the very next round, the conversation went like this: (2) B: nine is that monk praying A: yup"
"Later on, referring was even more efficient: (3) A: three is the monk B: ok"
"A and B, who switched roles on each round, marked the fact that they had achieved a mutual perspective by reusing the same term, monk, in repeated references to the same object."
These references tend to shorten over time.
"This process of conceptual entrainment appears to be partner-specific—upon repeated referring to the same object but with a new partner, speakers were more likely to revert to the basic level term, due in part to the feedback they received from their partners (Brennan &amp;[REF_CITE])."
These examples depict the interpersonal processes that lead to conceptual entrainment.
"The director and matcher used many hedges in their initial proposals and counter-proposals (e.g., it’s almost like a person kind of in a weird way, and yeah like like a monk praying or something)."
Hedges dropped out upon repeated referring.
We have proposed (Brennan &amp;[REF_CITE]) that hedges are devices for signaling a speaker&apos;s commitment to the perspective she is proposing.
"Hedges serve social needs as well, by inviting counter-proposals from the addressee without risking loss of face due to overt disagreements (Brennan &amp;[REF_CITE])."
"It is worth noting that people&apos;s referring expressions converge not only with those of their human partners, but also with those of computer partners[REF_CITE]."
"In our text and spoken dialogue Wizard-of-Oz studies, when simulated computer partners used deliberately different terms than the ones people first presented to them, people tended to adopt the computers&apos; terms, even though the computers had apparently &quot;understood&quot; the terms people had first produced[REF_CITE]."
"The impetus toward conceptual entrainment marked by repeated referring expressions appears to be so compelling that native speakers of English will even produce non-idiomatic referring expressions (e.g., the chair in which I shake my body, referring to a rocking chair) in order to ratify a mutually-achieved perspective with non-native speakers (Bortfeld &amp;[REF_CITE])."
Such findings hold many implications for utterance generation and the design of dialogue models.
"Spoken and text dialogue interfaces of the future should include resources for collaboration, including those for negotiating meanings, modeling context, recognizing which referring expressions are likely to index a particular conceptualization, keeping track of the referring expressions used by a partner so far, and reusing those expressions."
"This would help solve the “vocabulary problem” in human-computer interaction (Brennan, to appear)."
"Grounding is the process by which people coordinate their conversational activities, establishing, for instance, that they understand one another well enough for current purposes."
"There are many activities to coordinate in conversation, each with its own cost, including: • getting an addressee’s attention in order to begin the conversation • planning utterances the addressee is likely to understand • producing utterances • recognizing when the addressee does not understand • initiating and managing repairs • determining what inferences to make when there is a delay • receiving utterances • recognizing the intention behind an utterance • displaying or acknowledging this understanding • keeping track of what has been discussed so far (common ground due to linguistic co-presence) • determining when to take a turn • monitoring and furthering the main purposes or tasks at hand • serving other important social needs, such as face-management (adapted from Clark &amp;[REF_CITE])"
Most of these activities are relatively easy to do when interaction is face-to-face.
"However, the affordances of different media affect the costs of coordinating these activities."
The actual forms of speech and text corpora are shaped by how people balance and trade off these costs in the context of communication.
"In a referential communication study, I compared task-oriented conversations in which one person either had or didn’t have visual evidence about the other’s progress[REF_CITE]."
Pairs of people discussed many different locations on identical maps displayed on networked computer screens in adjoining cubicles.
The task was for the matcher to get his car icon parked in the same spot as the car displayed on only the director’s screen.
"In one condition, Visual Evidence, the director could see the matcher’s car icon and its movements."
"In the other, Verbal-Only Evidence, she could not."
"In both conditions, they could talk freely."
Language-action transcripts were produced for a randomly chosen 10% of 480 transcribed interchanges.
"During each trial, the x and y coordinates of the matcher&apos;s icon were recorded and time-stamped, as a moment-by-moment estimate of where the matcher thought the target location was."
"For the sample of 48 trials, I plotted the distance between the matchers&apos; icon and the target (the director&apos;s icon) over time, to provide a visible display of how their beliefs about the target location converged."
Sample time-distance plots are shown in Figures 1 and 2.
"Matchers&apos; icons got closer to the target over time, but not at a steady rate."
"Typically, distance diminished relatively steeply early in the trial, while the matcher interpreted the director&apos;s initial description and rapidly moved his icon toward the target location."
"Many of the plots then showed a distinct elbow followed by a nearly horizontal region, meaning that the matcher then paused or moved away only slightly before returning to park his car icon."
"This suggests that it wasn’t sufficient for the matcher to develop a reasonable hypothesis about what the director meant by the description she presented, but that they also had to ground their understanding, or exchange sufficient evidence in order to establish mutual belief."
The region after the elbow appears to correspond to the acceptance phase proposed by Clark &amp;[REF_CITE]; the figures show that it was much shorter when directors had visual evidence than when they did not.
"The accompanying speech transcripts, when synchronized with the time-distance plots, showed that matchers gave verbal acknowledgements when directors did not have visual evidence and withheld them when directors did have visual evidence."
"Matchers made this adjustment to directors even though the information on the matchers’ own screen was the same for both conditions, which alternated after every 10 locations for a total of 80 locations discussed by each pair."
These results document the grounding process and the time course of how directors’ and matchers’hypotheses converge.
"The process is a flexible one; partners shift the responsibility to whomever can pay a particular cost most easily, expending the least collaborative effort (Clark &amp;[REF_CITE])."
In another study of how media affect conversation (Brennan &amp;[REF_CITE]) we looked at how grounding shapes conversation held face-to-face vs. via chat windows in which people sent text messages that appeared immediately on their partners’ screens.
Three-person groups had to reach a consensus account of a complex movie clip they had viewed together.
We examined the costs of serving face-management needs (politeness) and looked at devices that serve these needs by giving a partner options or seeking their input.
The devices counted were hedges and questions.
"Although both kinds of groups recalled the events equally well, they produced only half as many words typing as speaking."
"There were much lower rates of hedging (per 100 words) in the text conversations than face-to-face, but the same rates of questions."
"We explained these findings by appealing to the costs of grounding over different media: Hedging requires using additional words, and therefore is more costly in typed than spoken utterances."
"Questions, on the other hand, require only different intonation or punctuation, and so are equally easy, regardless of medium."
"The fact that people used just as many questions in both kinds of conversations suggests that people in electronic or remote groups don’t cease to care about face-management needs, as some have suggested; it’s just harder to meet these needs when the medium makes the primary task more difficult."
Findings such as these hold a number of implications for both computational linguistics and human-computer interaction.
First is a methodological point: corpus data and dialogue feature coding are particularly useful when they include systematic information about the tasks conversants were engaged in.
"Second, there is a large body of evidence that people accomplish utterance production and interpretation incrementally, using information from all available sources in parallel."
"If computational language systems are ever to approach the power, error recovery ability, and flexibility of human language processing, then more research needs to be done using architectures that can support incremental processing."
"Architectures should not be based on assumptions that utterances are complete and well-formed, and that processing is modular."
A related issue is that timing is critically important in interactive systems.
Many models of language processing focus on the propositional content of speech with little attention to “performance” or “surface” features such as timing. (Other non-propositional aspects such as intonation are important as well.)
Computational dialogue systems (both text and spoken) should include resources for collaboration.
"When a new referring expression is introduced, it could be marked as provisional."
"Fillers can be used to display trouble, and hedges, to invite input."
"Dialogue models should track the forms of referring expressions used in a discourse so far, enabling agents to use the same terms consistently to refer to the same things."
"Because communication media shape conversations and their emergent corpora, minor differences in features of a dialogue interface can have major impact on the form of the language that is generated, as well as on coordination costs that language users pay."
"Finally, dialogue models should keep a structured record of jointly achieved contributions that is updated and revised incrementally."
No agent is omniscient; a dialogue model represents only one agent&apos;s estimate of the common ground so far (see Cahn &amp;[REF_CITE]).
"There are many open and interesting questions about how to best structure the contributions from interacting partners into a dialogue model, as well as how such a model can be used to support incremental processes of generation, interpretation, and repair."
"We have witnessed signi cant progress in NLP applications such as information ex-traction (IE), summarization, machine trans-lation, cross-lingual information retrieval (CLIR), etc."
"The progress will be accelerated by advances in speech technology, which not only enables us to interact with systems via speech but also to store and retrieve texts in-putTheviaprogressspeech. of NLP applications in this decade has been mainly accomplished by the rapid development of corpus-based and sta-tistical techniques, while rather simple tech-niques have been used as far as the structural aspects of language are concerned. canIncombinethis papermore, sophisticatedwe will discuss, linguisticallyhow we elaborate techniques with the current statis-tical techniques and what kinds of improve-ment we can expect from such an integration of di erent knowledge types and methods."
"Eurotra, a European MT project, had attracted a large number of theoretical lin-guists into MT and the linguists developed clean and linguistically elaborate frameworks such as CTA-2, Simple Transfer, Eurotra-6, etcATR. , a Japanese research institute for telephone dialogue translation supported by a consortium of private companies and the Ministry of Post and Communication, also adopted a linguistics-based framework, al-though they changed their direction in the later stage of the project."
"They also adopted sophisticated plan-based dialogue models as well at the initial stage of the project. ticallyHoweverin ,thetheearlytrend90schangedand mostratherresearchdras-groups with practical applications in mind gave up such strategies and switched to more corpus-oriented and statistical methods."
"In-stead of sentential parsing based on linguis-tically well founded grammar, for example, they started to use simpler but more ro-bust techniques based on nite-state models."
"Neither did knowledge-based techniques like plan-recognition, etc. survive, which presume explicit representation of domain knowledge. of Onetheseoftechniquesthe majoris reasonsthat, whilefor thesethe failuretech-niques alone cannot solve the whole range of problems that NLP application encounters, both linguists and AI researchers made strong claims that their techniques would be able to solve most, if not all, of the problems."
"Al-though formalisms based on linguistic theo-ries can certainly contribute to the develop-ment of clean and modular frameworks for NLP, it is rather obvious that linguistics the-ories alone cannot solve most of NLP&apos;s prob-lems."
"Most of MT&apos;s problems, for example, are related with semantics or interpretation of language which linguistic theories of syntax can hardly o er solutions[REF_CITE]. frameworksHowever, basedthis doeson linguisticnot implytheories, either,arethatof no use for MT or NLP application in general."
This only implies that we need techniques complementary to those based on linguistic theories and that frameworks based on lin-guistic theories should be augmented or com-bined with other techniques.
"Since techniques from complementary elds such as statistical or corpus-based ones have made signi cant progresses, it is our contention in this paper that we should start to think seriously about combining the fruits of the research results of the 80s with those of the 90s. andTheknowledge-basedother claims againsttechniqueslinguistics-basedwhich have often been made by practical-minded people are (1) : : The techniques such as sen-tential parsing and knowledge-based in-ference, etc. are slow and require a large amount of memory (2) Ambiguity parsing tends of to Parsing generate : Sententialthousands of parse results from which systems cannot choose the correct one. (3) IncompletenessRobustness: In of practice Knowledge one cannot and provide systems with complete knowl-edge."
"Defects in knowledge often cause failures in processing, which result in the fragile behavior of systems."
"Insteadhave, largelythe disadvantagesremoved theseof cur-dif-nologies, etc.rent technologies havebased increasinglyon nite statebecometech-clearer; the disadvantages such ad-hocness and opaqueness of systems which prevent them from being transferred from an appli-cation in one domain to another domain."
"In a ve-year project funded by JSPS (Japan Society of Promotion of Science) which started[REF_CITE]we have focussed our research on generic techniques that will be used for di erent kinds of NLP application and domains. groupsThe fromprojectthe comprisesUniversity threeof Tokyouniversity, Tokyo"
"Institute of Technology (Prof. Tokunaga) and Kyoto University (Dr. Kurohashi), and coordinated by myself (at the University of Tokyo)."
"The University of Tokyo has been engaged in development of software infras-tructure for  NLP, parsing technology and ontology building from texts, while the groups of Tokyo Institute of Technology and Kyoto University have been responsible for NLP application to IR and Knowledge-based NLP techniques, respectively. researchSince weonhavegenericdeliveredNLP methodspromising, weresultsare nowin engaged in developing several application sys-tems that integrate various research results to show their feasibility in actual application en-vironments."
"One such application is a system that helps biochemists working in the eld of genome research. sultsTheofsystemour projectintegratessuchvariousas newresearchtechniquesre-for query expansion and intelligent indexing in IR, etc."
"The two results to be integrated into the system that we focus on in this paper are IE using a full-parser (sentential parser based on grammar) and ontology building from texts. searchIE is, sincevery muchquite ainlargedemandportionin genomeof researchre-is now being targeted to construct systems that model complete sequences of interac-tion of various materials in biological organ-isms."
These systems require extraction of rel-evant information from texts and its integra-tion in xed formats.
"This entails that the researchers there should have a model of in-teraction among materials, into which actual piecestted. ofSuchinformationa modelextractedshould havefrom textsa setareof classes of interaction (event classes) and a set of classes of entities that participate in events."
"That is, the ontology of the domain should ex-ist."
"However, since the building of an ultimate ontology is, in a sense, the goal of science, the explicit ontology exists only in a very re-stricted and partial form."
"In other words, IE and Ontology building are inevitably inter-twined here."
"In short, we found that IE and Ontology building from texts in genome research pro-vide an ideal test bed for our generic NLP techniques, namely software infrastructure for  NLP, parsing technology, and ontol-ogy building from texts with initial partial knowledge of the domain."
"While tree structures are a versatile scheme for linguistic representation, invention of fea-ture structures that allow complex features and reentrancy (structure sharing) makes linguistic representation concise and allows declarative speci cations of mutual relation-ships among representation of di erent lin-guistic levels (e.g.: morphology, syntax, se-mantics, discourse, etc.)."
"More importantly, using bundles of features instead of simple non-terminal symbols to characterize linguis-tic objects allow us to use much richer statis-tical means such as ME (maximum entropy model), etc. instead of simple probabilistic pursuedCFG."
"Howeveryet mostly, theduepotentialto thehashardly beenand fragility of parsing based on feature-based for-malisms. weInhaveorderintotheremoverst twothe yearsdevotedobstacleour-, selves to the development of : (A) cessingSoftwareofinfrastructurefeature-based thatformalismsmakes pro-cient enough both for practical applica-tion and for combining it with statistical means. (B) wideGrammarcoverage(Japanesefor processingand Englishreal)worldwith texts (not examples in textbooks of lin-guistics)."
"At the same time, processing techniques that make a system robust enough for application. (C) linguistics-based parsingframeworksalgorithm, in particu-for lar HPSG."
We describe the current states of these three in the following. (A) Software Infrastructure[REF_CITE]:
"We designed and develop a programming sys-tem, LiLFeS, which is an extension of Pro-log for expressing typed feature structures in-stead of rst order terms."
The system&apos;s core engine is an abstract machine that can pro-cess features and execute de nite clause pro-gram.
"While similar attempts treat feature structure processing separately from that of de nite clause programs, the LiLFeS abstract machine increases processing speed by seam-lessly processing feature structures and de -nite clause programs. andDiverseJapanesesystemsgrammar, such,asalargestatisticalscale Englishdisam-biguation module for the Japanese parser, a robust parser for English, etc., have already been developed in the LiLFeS system. temWewithcomparedother systemsthe performance, in particularof thewithsys- LKB developed by CSLI, Stanford Univer-sity, by using the same grammar (LinGo also provided by Stanford University)."
"A parsing system in the LiLFeS system, which adopts a naive CKY algorithm without any sophis-tication, shows similar performance as that of LKB which uses a more re ned algorithm to lter out unnecessary uni cation."
"The de-tailed examination reveals that feature uni-timescationfasterof thethanLiLFeSLKB. system is about four built-inFurthermorefunctions, sincethatLiLFeSfacilitatehas quitefast asub-few sumption checking,  memory manage-ment, etc., the performance comparison re-veals that more advanced parsing algorithms like the one we developed in (C) can bene t from the LiLFeS system."
"We have almost n-ished the second version of the LiLFeS system that uses a more ne-grained instruction set, directly translatable to naive machine code of a Pentium CPU."
"The new version shows more than twice improvement in execution speed, which means the naive CKY algorithm with-out any sophistication in the LiLFeS system will outperform LKB."
"While LinGo that we used for comparison is an interesting grammar from the view point of linguistics, the coverage of the grammar is rather restricted."
"We have cooperated with thegrammarUniversitywith ofwidePennsylvaniacoverage. toIndevelopthis co-a operation, we translated an existing wide-coverage grammar of XTAG to the framework of HPSG, since our parsing algorithms in (C) all assume that the grammar are HPSG."
"As we discuss in the following section, we will use this translated grammar as the core gram-mar for information extraction from texts in genome science. marAs, weforhavewide-coveragedeveloped ourJapaneseown grammarGram- (SLUNG) ."
"SLUNG exploits the property of HPSG that allows under-speci ed con-straintscoverage. fromThattheis,veryin orderbeginningto obtainof grammarwide-development, we only give loose constraints to individual words that may over-generate wrong interpretations but nonetheless guar-antee correct ones to be always generated. straintsInstead, weofpreparerather76rigidtemplatesand strictfor lexicalcon-entries that specify behaviors of words be-longing to these 76 classes."
The approach is against the spirit of HPSG or lexicalized grammar that emphasizes constraints speci c to individual lexical items.
"However, our goal is rst to develop wide-coverage gram-mar that can be improved by adding lexical-item speci c constraints in the later stage of grammar development."
The strategy has proved to be e ective and the current gram-mar can produce successful parse results for 98.3 % of sentences in the EDR corpus with high  (0.38 sec per sentence for the EDR corpus).
"Since the grammar overgen-erates, we have to choose single parse results among a combinatorially large number of pos-sible parses."
"However, an experiment shows that a statistic method using ME (we use the program for ME developed by NYU) can se-lect around 88.6 % of correct analysis in terms of dependency relationships among ! ! bun- setsu&apos;s - the phrases in Japanese). (C)  parsing algorithm[REF_CITE]:"
"While feature structure representation pro-vides an e ective means of representing lin-guistic objects and constraints on them, checking satis ability of constraints by lin-guistic objects, i.e. uni cation, is computa-tionally expensive in terms of time and space."
"One way of improving the  is to avoid uni cation operations as much as possible, while the other way is to provide  soft-ware infrastructure such as in (A)."
"Once we choose a speci c task like parsing, genera-tion, etc., we can devise  algorithms for avoiding uni cation. spectingLKB accomplishesdependenciessuchamongreductionfeatures,bywhilein-the algorithm we chose is to reduce necessary uni cation by compiling given HPSG gram-mar into CFG."
"The CFG skeleton of given HPSG, which is semi-automatically extracted from the original HPSG, is applied to pro-ducerst phasepossible."
Thecandidatesskeletal parsingof parsebasedtrees onin theex-tracted CFG lters out the local constituent structures which do not contribute to any parse covering the whole sentence.
"Since a large proportion of local constituent struc-tures do not actually contribute to the whole parse, this rst CFG phase helps the second phase to avoid most of the globally mean-ingless uni cation."
The  gain by this compilation technique depends on the na-ture of the original grammar to be compiled.
"While the  gain for SLUNG is just two times, the gain for XHPSG (HPSG gram-mar obtained by translating the XTAG gram-mar into HPSG) is around 47 times for the ATIS corpus[REF_CITE]."
"The basic arguments against use of sentential parsing in practical application such as IE are the  in terms of time and space, the fragility of systems based on linguistically rigid frameworks and highly ambiguous parse results that we often have as results of pars-ing. forOnsententialthe otherparsinghand, ortherethe aredeepargumentsanalysis proach basedOneon linguisticallyargument is soundthat anframe-ap-approach. worksre-use.makesThe systemsother istransparentthe limit onandtheeasyqual-to ity that is achievable by the pattern match-ing approach."
"While a higher recall rate of IE requires a large amount of patterns to cover diverse surface realization of the same information, we have to widen linguistic con-texts to improve the precision by preventing extraction of false information."
"A pattern-based system may end up with a set of pat-terns whose complex mutual nullify the initial appeal of simplicity of the pattern-based ap-proach. ciencyAs weproblemsee in thebecomespreviouslesssectionproblematic, the by utilizing the current parsing technology."
"It is still a problem when we apply the deep analysis to texts in the eld of genome sci-encetences, whichthan intendthe toATIShavecorpusmuch."
"Howeverlonger sen-, as in the pattern-based approach, we can reduce the complexity of problems by combining dif-ferent techniques. shallowIn a preliminaryparser (ENGCGexperiment) to reduce, we rstpart-of-use a speech ambiguities before sentential parsing."
"Unlike statistic POS taggers, the constraint grammar adopted by ENGCG preserves all possible POS interpretations just by dropping interpretations that are impossible in given lo-caldoescontextsnot a .ectThereforethe soundness, the useandofcomplete-ENGCG ness of the whole system, while it reduces sig-ni cantly the local ambiguities that do not contribute to the whole parse. vents[REF_CITE]xperiment% of edgesshowsproducedthat ENGCGby a parserpre- Based on naive CKY algorithm, when it is ap-plied to 180 sentences randomly chosen from MEDLINE abstracts[REF_CITE]."
"As a result, the parsing by XHPSG becomes four times faster from 20.0 seconds to 4.8 second per sentence, which is further improved by us- ing chunking based on the output of a Named Entity recognition tool to 2.57 second per sen-tence."
"Since the experiment was conducted with a naive parser based on CYK and the old version of LiLFeS, the performance can be improved further. stillTheremainproblems."
XHPSGof fragilityfails to produceand ambiguityparses for about half of the sentences that cover the whole.
"However, in application such as IE, a system needs not have parses covering the whole sentence."
"If the part in which the relevant pieces of information appear can be parsed, the system can extract them."
This is one of the major reasons why pattern-based systems can work in a robust manner.
The same idea can be used in IE based on sen-tential parser.
"That is, techniques that can extract information from partial parse results will make the system robust. a similarThe problemmannerof. ambiguityIn a pattern-basedcan be treatedsystemin, the system extracts information when parts of the text match with a pattern, independently of whether other interpretations that compete with the interpretation intended by the pat-tern exist or not."
"In this way, a pattern-based system treats ambiguity implicitly."
"In case of the approach based on sentential parsing, we treat the ambiguity problem by preference."
"That is, an interpretation that indicates rel-evant pieces of information exist is preferred to other interpretations. aboveAlthoughmake theIE methodsbased onillustratedsententialinpars-the ing similar to the pattern-based approach, the approach retains the advantages over the pattern-based one."
"For example, it can pre-vent false extraction if the pattern that dic-tates extraction contradicts with wider lin-guistic structures or with the more preferred interpretations."
It keeps separate the general linguistic knowledge embodied in the form of XHPSG grammar that can be used in any do-main.
The mapping between syntactic struc-tures to predicate structures can also be sys-tematic.
"The named entity tool mentioned above, called NEHMM[REF_CITE], has been de-veloped as a generalizable supervised learning method for identifying and classifying terms given a training corpus of SGML marked-up texts."
HMMs themselves belong to a class of learning algorithms that can be considered to be stochastic nite state machines.
They have enjoyed success in a wide number of elds in-cluding speech recognition and part of speech tagging.
"We therefore consider their exten-sion to the named entity task, which is es-sentially a kind of semantic tagging of words based on their class, to be quite natural. eralizableNEHMMtoitselftermsstrivesin di erentto bedomainshighly gen-and the initial version uses bigrams based on lex-per name class.ical and character Data-sparsenessfeatures with oneis over-state come using the character features and linear-interpolation. theNobataparticularet al. ([REF_CITE]dentifying) commentandon classifying terms in the biochemistry domain including an open vocabulary and irregular naming conventions as well as extensive cross-over in vocabulary between classes."
"The irreg-ular naming arises in part because of the num-areber workingof researcherson thefromsamediknowledgeerent eldsdiscov-who ery area as well as the large number of pro-teins, DNA etc. that need to be named."
"De-spite the best e orts of major journals to stan-dardize the terminology, there is also a sig-ni cant problem with synonymy so that of-ten an entity has more than one name that is widely used such as the protein names AKT and PKB."
"Class cross-over of terms is another problem that arises because many DNA and RNA are named after the protein with which they transcribe. knowledgeDespite intheNEHMMapparent, thesimplicitymodel has provenof the to be quite powerful in application."
In the genome domain with only 80 training MED-
LINE abstracts it could achieve over 74% F-score (a common metric for evaluation used in IE that combines recall and precision).
"Simi-lar performance has been found when training using the dry-run and test set for MUC-6 (60 articles) in the news domain. modelThe nextis to stagetrain inusingthe largerdevelopmenttest setsof andour to incorporate wider contextual knowledge, perhaps by marking-up for dependencies of named-entities in the training corpus."
This extra level of structural knowledge should help to constrain class assignment and also to aid in higher levels of IE such as event ex-traction.
Annotated corpora constitute not only an in-tegral part of a linguistic investigation but also an essential part of the design methodol-ogy for an NLP systems.
"In particular, the de-sign of IE systems requires clear understand-ing of information formats of the domain, i.e. what kinds of entities and events are consid-ered as essential ingredients of information."
"However, such information formats are often implicit in the minds of domain specialists and the process of annotating texts helps to reveal them. tweenIt isinformationalso the caseformatsthat theandmappingsurface lin-be-guistic realization is not trivial and that cap-turing the mapping requires empirical exam-ination of actual corpora."
"While generic pro-grams with learning ability may learn such a mapping, learning algorithms need training data, i.e. annotated corpora. gramIn ,orderfor exampleto design, weahaveNEtorecognitionhave a reason-pro-able amount of annotated texts which show in what linguistic contexts named entities ap-pear and what internal structures typical lin-guisticeld haveexpressions."
"Such humanof namedinspectionentities ofofaanno-given tated texts suggests feasible tools for NE (e.g. HMM, ME, decision trees, dictionary look-up, etc.) and a set of feasible features, if one uses programs with learning ability."
"Human in- spection of annotated corpora is still an in-evitableuses programsstep ofwithfeaturelearningselectionability, even. if one namedMoreentitiesimportantlyand ,eventsto determinewhich shouldclassesre-of empiricalect the viewsinvestigationof domain, sincespecialiststhese oftenrequiresexist implicitly only in the mind of specialists."
"This is particularly the case in the eld of med-ical and biological sciences, since they have a much larger collection of terms (i.e. class names) than, for example, mathematical sci-ence, physics, etc. andIn order to seeinvolvedthe magnitude, we choseof thea workwell-circumscribed eld and collected texts (MED-LINE abstracts) in the eld to be annotated."
The eld is the reaction of transcription fac-tors in human blood cells.
The kinds of infor-mation that we try to extract are the infor-mation on protein-protein interactions. groupThe ofeldNationalwas chosenHealthbecauseResearcha
Instituteresearch of the Ministry of Health in Japan is building a database called CSNDB (Cell Signal Net-workmationDB.
"They), whichreadgatherspapersthiseverytypeweekof toinfor-ex-tract relevant information and store them in the database."
"IE of this eld can reduce the work that is done manually at present. theWekeyselectedwords ofabstracts&quot;human&quot;from, &quot;transcriptionMEDLINEfac-by tors&quot; and &quot;blood cells&quot;, which yield 3300 ab-stracts."
The abstracts are from 100 to 200 words in length. 500 abstracts were chosen randomly and annotated.
"Currently, seman-tic annotation of 300 abstracts has been n-ished and we expect 500 abstracts to be done by April[REF_CITE]. identifyingThe taskandof annotationclassifyingcanthe betermsregardedthat ap-as pear in texts according to a pre-de ned clas-si cation scheme."
"The classi cation scheme, in turn, re ects the view of the elds that bio-chemists have."
"That is, semantic tags we use are the class names in an ontology of the eld. beenOntologiescreated ofinbiologicalprojects terminologysuch as thehaveEU funded GALEN project to provide a model of biological concepts that can be used to integrate heterogeneous information sources while some ontologies such as MeSH are built for the purpose of information retrieval Ac-cording to their purposes, ontologies di er from ne-grained to coarse ones and from as-sociative to logical ones."
"Since there is no appropriate ontology that covers the domain that we are interested in, we decided to build one for this speci c domain. inThewhichdesignwe distinguishof our ontologyclassiiscationin progressbased, on roles that proteins play in events from that based on internal structures of proteins."
The former classi cation is closely linked with classi cation of events.
"Since classi cation is based on feature lattices, we plan to use the LiLFeS system to de ne these classi cation schemes and their relationships among them."
"While the researches of the 80s and 90s in NLP focussed on di erent aspects of lan-guage, they have been so far considered sepa-rate development and no serious attempt has been made to integrate them. essaryIn thebackgroundJSPS projectfor,suchwe haveintegrationprepared."
"Tech-nec-nological background such as  parsing, a programming system based on types, etc. will contribute to resolving  prob-lems."
"The techniques such as NE recogni-tion, staged architecture in conventional IE, etc. will give hints on how to incorporate sev-eral di erent techniques in the whole system."
"A reasonable size of semantically annotated textsbeen,preparedtogether. with relevant ontology, have componentsWe are engagedin the wholenow insystemintegrating, in ordertheseto show how theoretical work, together with col-lection of empirical data, can facilitate sys-tematic development of NLP application sys-tems."
"|® ¯3°²±´³3µ1³  »l¼E±±½® ¯u¶J¼E±]¶ ¿h¾ ·ÁÄ,µ»¯E°Å­u¶¹®]$· µ­E± Æ²µ1® ° ¾ ­=Ç¾¿ »lµ± »lµºu¶ZºÁÀ3­3°Â®]¶± ® µ1®]¶Ã®]·µ­E±ÈÉ­ºE¼3»U¶l·±} Ë°Å±pµ1³E³3ÆÅ°Â¶Zº®¶l· ¾¿ ±Ä,µÆ²ÆÍÌ¾ »ºu¶Zº3°²»lµ1®¾ ­dÏ¶l· ]¶ZºÎ®]¶Z­d®]]·µ­E±¶Z­E»U¶t³ºE¼3»U¶l·±*µ°Â·± ¿µÆÅ°ÂÒl¶ZºÓ®· ¾ ÄÐµ ]Ë·µ­3±°ÅÆ²°²­uÑª¼EµÆÆ²µ1® ° ¾ ­Ô³3µ1®» ¾ · ]]® ¾ Ñ¶Z­E¶l·]]¶Ê ³ µ1®]®]¶l·­E±lÌ,® ¾ Ñ¶l® ¯E¶l·C¸×°²® ¯Ø® ¯u¶´®]*  ¶® ¯E¶Z­)¼E± ¶Zº)µ± µ¯E°Â¶l·µ1·»$¯E°ÙÊ »lµÆ&lt;]® ·µ­E± ÆÅµ1® ° ¾  ¾ · Ú ¿h¾ · ¿ ¼EÆ²ÆÂÚtµ¼`Ê ® ¾ Ä:µ1® ]·µ­E± ÆÅµ1® ° ¾  ¼EÆ²® ± ¾ ­® ¯u¶ Ü ¶l·Ä,µ­}ÝdÞ5­uÑªÆ²°²±  &lt;ä *â åæç» ¾ ]· Ê ³  ¶zÑª°ÂÏ¶Z­=Ç* è é 7.HêH«@ }ªëì9. & lt;7 í ¾ °· ¾³*­C»¼E± ¾Ë µ±]¶Zºµ1³E³E·#}¾¼EÄµ»¯u¶Z±Ë ¶l· ®¾¿¾ µ¼E®ºE°"
"Ùï ¾¶l·Ä,¶Z­d®zð3µ©Ïµ1® °²»?]® ¾ ·±lÇ"
" ,#/§ ¨,©«ª¬bU­W&apos;®[¤&apos;¯°0ª²±^ ª°¤³±b´±µ/&apos;©[E¶¨t©[ª¬b5&apos;®[¤0¯°0ª²± ,,¶¤¤º[®0¨g0ª°¨ &apos;©®[; ¤0®¶&lt;[0ª¬¶¼¥»¤t¼[¤&apos;ª¬[¨g&apos;¼b0´&apos;®[¾¼«¤µm½[¯À¸«Á¡ t¯²¯²[[[¾&apos;©Ä±µ¸«t¾&apos;©Eª¬¤ÂX:Å±&apos;¥&lt;[;[[±½0¦¥r¼¤;¤0¦¥7±gÃ¼«¸Ä±5¤[¨g[#¸«®®[¥7&apos;©b«¨gg¡ ±¼¸rª²0ª²¦¤[/ª¬,½&apos;®[¤Â[&apos;±®¿«¯²¶È±µ[¤0¯°0ª²±±®¤:À¶+±b¥[[[¨g0¦¤0¦¥0±¼b¾ ©[ª°¨t© ¤,©b±0[¤;&apos;©b#¯²b¾&apos;©Ë±µ¸«t[¤± b¾&apos;©½® [[ ª²0ª²¦¤5ª¬»&apos;®«¤0¯°0ª²±[¤0¯°0ª²±Ï¸«Ì¿ .º ,Ð¤¦¯²¦¨g0ª²±[¼ ¤0ª¬ ¯²Í¦¯Î&apos;®,®[¤0¯°0ª²±¸r,[*[0±®¸  ®¶¸«¯²¼«¤0ª¬b¾7&lt;&lt;¶0¨t©[ª¬b¾Î®+&apos;©[[¥³¤¶®®0ª°¨[/¨g± ±¤0&apos;ª¬«¥;®Î¿ºÈ&apos; ©b¦¤&apos;®¼b¯²Í¦¯Æ*&apos;©b¶+±®¤Ò,¼[¤Â[&gt;&apos;¼b&apos;¯ÑÔ&apos;©bÕ¤0¦¨t¡&apos;®«¤0¯°N¡ 0ª²±¤0¦¯²¦¨g¦¥Ë®¶±¯°0ª²±,ÕµÖ±×&apos;©bb¾Õ&apos;©[Î¤¦¯²¦¨g¦¥Ë&apos;®/Í&apos;¿½¸r ©b&apos;¤/«¤Á¡ª°¤ ¤000ª°¤0ª°¨´ª¬»¸«bµÖ±,,S¨¾±&apos;ª²¦¤Õ¿¶0ª²±¹±µ&lt;&apos;©[½0&apos;¾ºS¼«¤0ª¬b¾ ¯°®[¾¼[¾Â?&quot;×¨g±¼[¯°¥Y¤&apos;©[¤0ª¬[±0[&apos;©b¯²ª°¤;[¸ 0±®¸ [±®¤¦¥¾&apos;© ±µX¸«¿rª²¾¼[ª²0ª²¦¤ª¬,Ø,®[¤0¯°0ª²± Â  Ù Ú H  V9Q:"
"G:  A£V*H ¶ &apos;©[­W&apos;®[¤0µm¡£¢X¤¦¥b&apos;¯°¯²ºØ ©[¤Xµm±¼/§ [¨,©[ª¬[ *­ &apos;®«¤0¯°0ª²±Þ¦ßßNàbá0â Û + ¤º+[®0¨g0ª°¨/®[¯²ºb¤0ª°¤±µÀ¤±¼b&apos;¨g7¯°®[¾¼[,¼[¨t¡ ¶ ±,¸r©b±®¯²±¾®ª°¨¯;®[[¾¼[¾Æ &apos;¼b0Ë&apos;®[b¾¼[¾Æ#®[¥"
"&quot;v5wyx{{~| }{ v_@ ^s | ^u l#xGx{|\|]{x aw/^sJ__| a}ysp| @}y@|O} Ma}ys|   {x z@|  aw  zyya} } }yG@@]| s_a@ @|xx%@{|~5Q} @@x{_| ~|z} Q]J_@@x{|5^x/ (#0 |~(}/ @¡¢~v 5Q  x{|¤£a5zy| ~ xGwSªa@}  {a}y]{~|)}{x{^ }ys}y§]|&amp;z¦¨)¬ @w©@5}   {x &amp; |5­WG@|~}y®s5} ,@}y§]x zyy} @|  aw     x{¯| } {}ª^@_5z% vWx^O}°@£OxG@x{| y§} ]s]s§k}±§5@ @ xG}"
E v5wy@_x{ &quot;} @_| z z y§5a} }¢y§} ]xGCawyx ]@ }¯@xGwyZwy@_5zyG«¢uê7} }y§]^@ wyv__^s|~}y@5z¯| %{|O| {5| ^x±vW@w¯  &quot; §5S§J^s|_yzs} 5| /@_ Æzy5sx|} ã &quot; @^ s~| }y@|Jy§} ]x } à @w @ vW@w# y§} ]{x | e ¡  ¡
Þ ì /s¼¢fá{¾ e ¡Mz&amp;}  · GxGwy I êë Û êë ] ¡ {x ?ê 9|
"Þë }y3 ì Û }y§]xG@?¡S£ ãèh  &quot;0,  é |O­W5| }°@]£ xG@{x (| }y§]s]s§\  e {OGG]|5}Û @zÛ {c Û s|5^%| }y§])x }@M5| ]| ^@ wyv_5z{«¢¬ /wyx{ x  J}y§5MzG£@u ^ s vW]}  s@|&quot;y§Àwyx} {zyv,{x ^ } }%]| x{aw¯^s% _M5a| }ys|@#uÈ¯\Ôª@|  zy c@}y§5|]º   {x  -î =£ @}y§]@ s @ 0 ¼¢fç ¾ ¿?¡ 2 »Ô X3ï0 ,¡  ¢fç î ¾ è Ðy¿Q¡S«#u| }y§]x ^x  {x |~yz} &amp;@wy}  , x{q/£su&amp;5zyx  /5_| ¨@wS î £5&quot;y} § ­ ÀÊ 0  ¸ }  _~ 5@ ys}}y§5x ªf|  @w @5z±G| @|â Õ@Õc/« ^ s v_]}  M| ð v_awS@x{, |]| _@ @xGw&quot;}y§]} @5| |^@ v _ &quot;z 5zyM]| y§} ]@@@y§5} |­Ws]wyxªÔa«&quot;¬&quot;]§ xJM|]x_ @x  &quot;}y§Ø@Á| @}yaÇ} @{x z®|~} @G^s_9| } y§} ]xÎM5| ^wyx{@ ;| ¼¢fá{¾Q¡ e  5x\} {5| @@ I {M|] }y§]x  } _5}ys|Æ¼¢fç ¾Q¡ e #af}&quot;} y} | á  Ú¾ ñ Q¡0 e }  =ûQüSÿÚõ?þêý=öúû{ÿÚþlýÚþ÷ ÿÚöúûQù lõ?üTö þ"
"Ô ý a ,ü , $ @þÿÚõ?þ0öúû j ÿÚþê÷?êýÚý ö@@ü þ ?þêýÚþ ö ±ö yý7ÿÚü#ÿÚõQþ , yÿÚþ # $ ÿ¤ýÚöúùSý , 7õQö °õ  ,ü , %$ + ^,þ ÿÚõ?þ i ±þêÿÚõQü )$ ? $ þ  $ þ÷ lýÚö @ $ þ  $ - þ - qýÚþ {öúü , Q÷7÷¤þ lÿÚöøüS - û Zlõ÷ö/ 3ù , ý û { + öúûQù    e ¡ ! ãè #&quot; qãè Ë]¾ ä ®¾ ì s]wyx»ÔaÉ E @@ y§} _ f@(s@M|5zG«"
Q¡ s@{x z&quot;}y§]/x ~| _  @y} x{z  COGG])¿s«| f
"I GxGwy]£ ¨@/w {x @@w  á  Ê¹ 7 |p}y§]x @ cGaW5awy@« ¬   ]  v3}y§]@@@ wS}@ x  y§5z} }|5k¨@}y§]%á  =zy5S§}y§5a} ñ Ú¾ a¡ e HG â ] ñ Ôa« ¬ §5Mz=G@{zy§5x} #s@_| =z f@wv_@   e á ¡ ñ =}y§5a}) ¼¢fázy_| ^ xÀ}y§5xsâ @5z| %@ @ awyx®? |;@ |~ ñ ¾a e ¡ JI ]ñ } %}?@ xGwyx{yªa} }  £5_]} G@@£c}y§]x)@|5Çc| @y§} ] } a@5a_@&amp;z 5|5MÇ@{x }  G@{^}  « K[REF_CITE]~· RQ Ø2 ´ s&gt; &lt; @|3x^ {x ~| }yzÆsÒ| } y§5x ú ¯@|5  @|   ; @|5zyaw  ^ @ wyv__zG£&amp;&quot;}|]sMzy§æ@z%}^x@|]s5a@x  |  ì5wyx{|@z y§5x} &quot;}yawy@xG} @]| s5a@@x « E ¨}{O| }@{|5^xº@s| {|9}]| k}y§]xZ xGy§5}  x{zy^,x  | ¸   xG}@¤«£¢Ô{@Õ"
"Õ@y§} ]xJ^@_@_} |~}  |~} x{|~yzk} @]q&quot;|ÁM|Ñya} _xÎÔa« ¼¢¬ æxGa@5a} @£,&quot;§5xGwyx7¼@@z5| ^y§x}@]  x{ {]| &quot;^xGaO@}°,É S ¾ ä ¯¡ å  T à S v5a}  £/@| Û é Û  &quot; å Ð ä ¡Mzy§} ]xÒ} {}^@wyv_5z{«  G _x^] }#@c  |  Ga} @@@@5| ^x&quot;f@w y§]xÒ¬ @|5zy¬#cv,xpav5v_MGays} |  {x zy^,x  Ø| }y§]x} xGa|~}@5a ys5} ^ys} |Ä|@Æf5M@|   _}#§5@@{Z| _  M| }y§]x º x  @x  ¸ ( ¬ } { E  |5@ n { I @|pxG/} @¤«£0Ô{@Õ Õ@ x{5zy| ] wyx º @ wÆ^s% _Ga_5Mawy@« v | @&amp;   x{zÆ_  } y§]@ x } @xGC} @a ìs](# ( @@|5^@&amp;| @7fx{a}y]wyx{z¯f@{ay} ]wyx^°zyx{x{^ ys} | xG}y§O   zG«   ¬zG£Zup­ ^s_v_\}@(]| @Ç# x ( Ä¨x@ ZÖ{a}y]^{x{^}ys%|  xGy§O} ? |é_M|]s5@ @w  _v @^~OGG]wywS]| Ñ&quot;}y§5M| @Ms|]x  {O| }{5| ^(x _v @ M|»y§} ]x®}@|_|]3^@wyv_5zª5zy]| py§} ]x (uª@|  @5zº| xGy§} ]  {G@y§} ]x 0 s@5z±| xGy§} ]  @ @ wyxÆx^Ov,{x ]|  zy@x@£a} @  }&quot;@]| |]@wy} }@5av] {]}{ºÔ  añO£úñ"
"H @ñ@ñCvW@  @x  ~ %   xGwys5| y§]xZ}@vØÔGñ@ñO£úñ@ñ@ñ3G@|    a} { {x @ @±y§} ]x}  y§} ]xGw }y§]  zG« E zzy§]| |/ya} _x)ðc£@}y§]}y§]wyxGx@  xG@xG{x }y§5| @ z ss|]Ò@x&quot;}y§zy]]x}}@y@9| yM} @@|]@Ç x  ¥vW,xGwyx@{~| ;}&amp;ì]@@]| wZxÇO{|]@ sz{£ c]xGx{}y§5|5^ @£Æuº=zy5G}@^M{x |]x @x{ªMawy0@xGw# fx{ay} ]x{zs| yz±} ^s|O^  y} |]º@y§} ]x } @v]@]| @Ç x  # @w  _v @@/w y§5a} } ]xG}y§5}  }yM« )¬ x&quot;§]^xwyxs|{zy} _@}yz|9}yzawyxkzy§5q{£"
"W  V @@ñ&quot;ñQ0ÎM| @|»­|  _ÖañO£úñs]wyxkðc«@@ñ ñQ  fx{ay} ]   x{#} @|5x  s|@w }y§]x uÈ¯\Ô fx{a}y]wyx;}yzG£J&quot;§5S§ s5},@  }y§]@}y§]xGw   X &quot;!&apos;#$ ,  ! þ yÿ ? ý ýÚþ ´lõ÷ö û · ZY ,,÷a ±ü  qþ $ ) 0OþûGÿÚ ¬ ÿ  ^ + öþ  ÷ è/# ? ýÚü q &quot; ö ìs] ¢xGwyf@@_| ^ xæ@ }y§]xæM]| x{awk   x{"
"G @_z)~| _@_@}  xGy§} ]  @|   ¸  M|5{x ^x±y§} ]@&quot;x |c5J,xGw0@@,MxGw0fx{a}yy§5} ]wyx@{À| #z }y§|]y§} ]| (# ( #@  _@}|(}y§]x%f5M=uÈ¯\@wyx ^s _v  v]| Æy§5x} &quot;( #(Á@|  |]{x aw   {x Mz&amp;u ¯@~| }  } ]wyxªy§5a} C} @|~3v,@@|5^x  {5| ^@z |]@}  ]xÆ} ?@_} }y|]Zy§} ]x/} @M|5|]% ^ @   v_5z{«¢¬ Cx{|5a} )y§} _z&amp;}l@£@u¢@v_} "
Gx I  }y§]xª|/@ _@}/cp} @|5|5 5x| {aw    {x z&amp;&quot; }y§%_z zy 
"Gx I {&amp;z @ } @_| zyays} | v_@}}yz&quot;@ 5}y@M]| x  fwys y§} ]xCuÈ¯\ÔJs@| @]| ÇO]| ]« E ]?&quot;|k­| _s]wyxCÖc£W}y§]@|] x aw&quot;   {x z  % ^x ]§5_}%@ xGwyºzys§9}@xGwy} @O| {|]\ ^x ,¥ x{^ }&quot;}y§Äy§} ]x@v5} @ xG}} zy  Gxawys5"
I |  ÔG\^s vWawyx  } Ö ? a _ @ ^x v   {x  @w  _v @ wSz v5v9 ` ù= Ö OÔa«úñ a a H
Ö   £øÕ ? @ H Õc£øÖ@ÖOÔ
"Ö ]Ôa£úñ@ñ@ñO£úñ@@ñ ñ ù=è2 9Öc« ñ   ø H ù=è2 0Ö 9Öc«øð ñO«øÕc«ií   ø ÖañO£úñÔa£úñ@@@ññ@ññ ø 5Ôa«Ô  b 0 ð Öc«øð@ ø©H c«øÕ b ¬¢a_xCÖcÉ % ¯s ú v_Z| @&amp;    x{@@5| ^{x z{« )¬ @§5wx d  ce _v @%{x{^}  ~ \ }y§]xZuÈ¯\Ôºs@M\| wS@|]Ç9 ?½  ý ½ ih ^ s5ª|\s@x{z }y§]x~| _%@ |5J_x^xGOy§} ]}l@£¢ @£O|y§} ] x j y§}  ]x l Æ© ` ^^s_Zs| |3s@{x z¯@x}{{±}}y§]^xZv@ wyv,_xGwy5z v_x^Ol}  wy@væ@zC®v,xGwS^x{9| }ya@xZ@y§5x} _@{|5x@« ù= Ö ¹z)y§5x} /}@@    x{=@|  æô25æ  x{|5@}{|O {x |9} ,sMays} | }|ºy§} ]xÆ     x{¤« x{¬¢a@|_x±Ö 0×&quot;v5wyx{{|~yz}  ­W |5{x @zG«~wyx{¬)zy5§5x/}yz (f@# ( _z  Mx{O| Mz s@}|5æ @x{ {x ~| ºq} @xGwy§} ]x3|5x{aw    @x{z{/£ |&quot; y§} }})ÔGñ@y§5} @ñ ñQÚfx@®| {a}y§]y} ]/x wyxÎ0{)} |5x{  x{x{   x{zyv_} ^s9| y} @|_|] ÔGñ@ñ@ñÑyM} x{@w   v_@w v_@ xG} @|  }y§])x {#} (#(   {x  Ox{  M]| ^O}°®wyx  5^ys} |p@¯ @wyx }y§5@| ø©  q@y§} ]xÆ_@{M]| xÆ|]x{aw)   x{¤« m n ·°at&lt; ] ¶#&lt;Qa·°4&lt; ´ )§5x @|¹wyx{zy5}3@®y§5z3vWav} ,y§} _a\} }y§]x¬ @;^x ,¥ x{^ @y} @x@wyÇ }^{sCW}]| xÎ|]f@}ys| fwys  ¥,{9| } ^{x zªy§} _@Ä| 5x| {9| },sMQ ys7£Oa| }±x{@ zy}@w¯y§} ]@_x{}y  x  §5xGwyx@&amp;« ul}} z  @ {@}|9y5} }y@{x &quot;~§ / }y§5z¢zy§]s5   &amp;^yy§}} ]@x xæzyG@^@@É¢wyx{(# ( ; ]| {x; fwys{|9}yM@ ¥,{9|yv} }kzys_x{]zWv5wyx^{x^z y§} ]xGw¯}y§5@|À{@ xGwSas|]% }y§]{x « )¬ §_z s@x{z¯5¨| @ }ys|]wS^{x z&quot;&quot;§_(@zyzyskx| {}y§]@xGwy®§5s§ @wC@^@wyx{\ @ wyxªM| º"
"W]x{|5^x ? @xGw y§] | {aWx0xGO  x{|5^@x £9y§5z¢} &quot;cx{  }} } } ]5| @¢wyx{G} è « Ñ§ D ]{x |3zy_^@ wyx{zCawyx%_@    |5xx{#zG« }]]| ±wyx{7} @5}y§5x{#^x c {x{^ }ysx{~| |Ã} xG@}y§zº] y§5a}  wyxº} {zy5y§} ]}  | @} }5zÀfx| {a }y ]x{wyxz } @|Äy§} ](x 0 s@|_zª xGy§} ]  £  x{} }y§]xy§5  @^} }y§_a} } } } _@ zyx  spºJ| _ @wyx  {^} {x @ ] @=x{@S§¨x{a}y5wy ? x  æ @wyy§º} &quot;y§5M} | y§]xÒ0    {x ¤« E v,_x(^x cvW@5a| }ys|} f@#w y§5Mz0z} #y§5a} } }y§]&quot;x s@|Zq@xGw±}{5| ^} @@ Mz |]@ }@~     ^}@w @ }y§]xs@;| | y§]x® v_wyx{{|5^x@@|~»@}y§]{a}y]wyx{}y§5z z} @|])x s50c} }y§]&quot;x  @^#} y§} _aG£} ~f@#w @@]fx{Q }y]}|ª}y§]x)@w  xGw0@ÔGñ@@w  z @|  x{zyzS¡S£ y§} ]y§} ] %    s]} @ y§} ]  xG}y§]  « E |]@}y§]xGwæx^cv_M@|5ays} |×z3M5| @GG] Gx{|º}y§]/x s@|kav5v5wyqOªa}ys5z| )^s vW]}  c G wS9| } ?"
I æ W xGy§} ]  £5&quot;~| @s@x{&quot;z ª@|9®c5|  G@v_awS@ xG} }y§5aÆwyx} {y5} _| |]]«Cì55wyy§} ]xGw |~@x{}ysays} | z wyx{  9| } }@|  @}y§]xGw {_|5c]x{@|  |]@  # @ w  vW@5| ^x} @ xG}y§]  zZ}{}  cx{  x  5­WG| @|~ªc5} @O| }yyx} {zJ@&quot;|]@s|  ÖañO£úñ@@ñ ñpv_@ {{G@ }y§]^x @cGaW5awy^s9| y} @|_z/]} è añO£úñ  @ñ@ñ @w  } @ccs_@/| 5]| wyx{@ }yG@&quot;@ @}@| ys} |5zG«|
"E y§} ]s]s§Ù}y§]@| @w»y§} ]xÑ   {x Zu §5q@x  {x zy^  | }y§5zv_éav5v| _GQ ys} |5zÄÇ@x ¬ @|5zy¬#~ &quot; ]| xGx  } @x _  _v wyx  ^ys} _| &quot;z @ ]^]| Àyawy} @xG}/} ^O}G£ } z~| }{yM} |]; } {G5a} ]}]xG}y§]xGwk 0    x{]f@%¾ è0Ðy¿?¡0^s5  @ /)^  5]f@w ¸ ¬Æ« c  ú _awyx  }}y§]}y@|  aw  |] _@ 5| |]x{@&amp;y§5zJ§5} @y§} ]xZ  a@~| }ya@x@ } yM} ]| 3J5S§Îx{zyzª^s _v x^{^x^  ]{zGî&quot;@C@Mq&quot;M|]æ@|~Î|]f@}ysÄ| &quot;§5S§Äz  {^}yp@_ _xÀÒ| y§} ]xZ} @|5M|]k^@ wyv__z/} @xGwy x{@ zyÁ|5^@ ,@wSa}  |~}}y§]x\   {x  {@ | ¨x{a}y]{zGîÀ@|  @ ª@| {x } ys} | v5wyO^x  ]  &quot;§](} @_| zya}ys|Ø   {x _@ x^ @| )@ v5} "
Gx I  f@_)&quot; }y§ª@|^]}y]| } @]| s_a@xª   x{¤ p « o  a@|~ya} @x{zJ_| G  %x y§} ]x qTr û  ? ýÚöúû ö # øþ  @õ  ù  SùSþ &amp;
"Sû  $ ý SûQ÷ yÿÚöøüSû 5 ü }y§5a5§ s§À&quot;} ^¼¢fçszy%}¾ è0Ðy¿@ ?} ¡CzCzys@5M| ] | xG&quot;§5a}Cx{ zyz/ @ x{x{zG£c]| xGwSy§}@] y§5} @@^|} ¼¢ ¿c¾½Q¡C¨@%w _5M  |]{@ }yª} @|5zyMa}ys»|    {z{@|  }y§5xºM@@ x{_@|5zy {x c5a@x{|~}x  x{|_} %Å _@x{@z¯@ ~|}}y§_¦¨(# @( wCMÃ5| ^@   @{x z/ y} zyxGxZ]| ]ì sz  } }{£,ðañ@ñ@ñ9¡±f@&quot;w   À| @}y§5z¯v_wy@_ x{Z¡S« s t 4 ´ t M °¶0&lt;a·°4 ´ ¬)@_ x{ @#{]| Àf@} {} yawy} @xG} } ^O}#|ya} } yG} @_ } @|5zyMa}ys| _MGa}ys|5z&amp;G@| @{a}y\zyM _v ­Wx  ±}y§]xÀf5|  @  x{|~y} @   }_]ys} C¼¢f½c¾| ?¿ ^cv_@|  x  wyx{^ yª} |À} @ }y§]x  zy} _]  ys} | ¾è Ðy¿Q¡S£ }y§5xGw/}y§5@| 5Ò| }y§]y} @|  aw  | ~@5| |5x{)av5v5wys@  s vWawyx  ªzy _v x/]| x{aw&quot;  {x 7¨@w0¼¢fç%¾è0Ðy¿?¡^  @zyx  s|\uÈ¯ Û úzª æ   x{&quot;}y§\@|»x{c5a@x{|~}_ 0    x{¤£&quot;@ |  fs5|  y§5a} }º}y§]x3(# (  v x  {x Æ§5@z®q@xGw 7ø©  ?# xGw®}{zyk} ^@wyv_5z®v,xGwy _ ^O}l@£  x{_} #5zyM]| &quot;}l#@w  xGwSz @ |5}y  x fxG #xG}y§5xGwpv_zC¨awS@@w%xG}{x{^ }yM|]k0u®@ ^ _@awyxw  Úv _@wC¨x@{Q@ }y]{zG£]@ |  ¨s5|  y§} _a¯} %zyM v_ xG}y§]   @|5Çcz \x{WÔv_@@G^@ w~ }M]|  }} }}y§]x{@@|@|5&quot;^x0}y§@_| | zys@ |5­WG@@x{~|| } @/ 0^s _fxv {5aya} y} ]ys} wyx^°zyx5| @{=x{^^ szyys} })|Áy§5} @@k|@a }y§5  5x)} 5  G ~| } @ I «&amp;ì¢M5| @@£su ]@ @{x } y§5a} }} ,}} ^Ov_@wyxÆy§5xC}  x{@&amp;5zy5| %  0¹   {x  ¨@#w ¼¢fç ¾è Ðy¿?@@|@}|5ay} @x }y§5xC|]~_@ |5]| x{@} ¸ ( /¬ «} 3t  ´  4 7"
"M[REF_CITE]Ø2 ´ s&gt; &lt;n ¬)§_@wyÇ×¯@  s]»} @z»v_awy} @®}y§]x ¬ @|5zy¬#cv,(x {^ a} } xw E ï=_|  x  cÎ}y§]x C y} ]@ ¸ Gx{|_^{x (z @ |  0|5s|]|5  ){x { ú _|@ ) ¯ ú @|5  &quot;Mzy§}}y§_@]| Ç ­  = ]®ï=Q _@M xk@|  E |  wyx{@zª#{x®f@wº^ {x |~}yzªs|v }y§]&quot;x v_{£s@|  # G §5v_v,=@ |]sM@z¨@#w | _] |   MzyG5zyzys5zG«| y 2 256s2 ´ tO2_&lt;"
§©¨8ªz«2¬®­B¯++°¶­z±`º8¯+ )ÁFµ¶²Â«U°J¨c¸ /­z¬»ªÆ¬®µ¶¨+^É1Ê# ´¾±Æªz«2µº¿¬®­jµ¶²Æ¬®±l¨}ªÆ±`º¿ÁËµ¶²? ¯  Èv¹2­z±Æª5µ¶Áeªl°J²Æ¼/±lª+ ­·}¨gª^ ^/±lª /+ ªzÈc²Æ±^#^±`Åz«a°¶­BÎ#: ^ ´XÈ2Ã»ª^°J¨2±`µ¶È2­zÃ»·/É¾Ê#+°J²z¸   ª^ ªz«2± ^ª /}±`ºUÉ
"7cao),,A F/ =?C21FI ) 1 k C`,) . [ 8àX ,j 36ypCEB1_ =qA+6. [ /`)GCh_0&gt; -BaE1ba~ChH1YCE_ /F.= G8H) ) t8H+ k [.h/ @ &lt;=?C2,) arC2) k .æ  ? ê ]Î.[ m1/ &lt;d61&lt;/hC`,) A1eGCo1) &lt;8 k [.- 8 C21&lt;`abj/= B1_ ~&gt;J1&lt;\&gt; [.Em1/ &lt;8 k [ I01baChH1bah1arCF_ ?= C2,) arC2) k ap)G8DCE= 2/ ), 1 IB=?C&lt;=9arCh/E- k EC -021/ .*5 = k _ @ ?= Ch_¿]f/2. [ &gt;H1S_ /`[ .[ C*[.]0Ch_H1 Ch`/ )1\C2[= k &lt;1 /ECF=.)G88H [ k [./h/@.[ 8HI0aJC2[s:= C2),,A .1 y [./2)G8H+DChH1pC`_ )A,19arCF=?C`,) ) k aG8JCh) H1_ k . [ /h/ @ [.8HI0t8H) + 8B[ I01.\j _H1m=?/ k A°=?1bA,ai=.A,. [ 8B+\[REF_CITE]@z?= CE_?= /21Ch_H1 vxÁDZBy)&lt;=?8 k 1ba4@ ?= 8BIwA¡=?z1bA,1bI0/F= kEÝ &lt;1 C`a[.] Ch_H1 k . [ /E21bar/ @.[ 8HI )G8HD+ @ ?= ChC`1&lt;/p,)"
"A1Fax[.] k ?= 8BI0,) IH=?C21 G8BarCF?= 8 k 1ba k =?81421/ &lt;CE2/ ),1&lt;d61bIf] /`[.&gt; B14_ J1&gt; &lt;\&gt; .[ E/) `/ t8H) ~+ @ ?= 2ah/ )G8H+9 YÎ] [6A,A,[ )G8H~+ Ch_H1 k . [ /h/ @ .[ 8HI0t8H) + z?= CE_iG8YCh) H1Ch_ `/ ),1.jÏ_H1CE2/ ,) 1S,) a k /=?C21FIiI - `/ )G8B+~Ch_H1@ Ch&lt;/ .= )"
G8B)G8H+s0@ _=. k . [ h-/ k C2G8H) +wChH1aE1_ &lt;&lt;8 k 1 +./F=?0@ _i] . [ /Ï1l= k Jah1_ &lt;8 C21&lt;8 k 1c=?8HI1+ &lt;8B1&lt;/&lt;=?C2G8H) +D.= AA
"Ch_H1 C2,) ,"
"A 1bap].[ /= k e_ @z=/h8) &lt;=?8 k 1.j *ì,í.î ï`ðYñ ?ö4÷Bø,ðYôEð&lt;&lt;ùBûbðí.ïFõ?÷0üoý9ìGúhü:ûbþ.&lt; zþ6øôHúhüHðúFõ?ï`í6ð&lt;úp÷zõ?úEú2ð&lt;ïEùHôcõ?ï2ð &quot;õ?ù  &quot;!$&amp ;# % (*),+ -üHðY÷õ?ïhú`ì¡õ.øz÷õ?ï2ôEìGùBíoõ.ø,í6þ.ï`ìtúEü0ö ï`ðbûbðbì/6ðbôiõ. ?ùì[REF_CITE]÷0î úôhð&lt;ù ú2ð&lt;ùHûbð&lt;÷0ï2ðbôEð&lt; &amp;õ¿ôhð876îHð&lt;ùHûbð ? õ ù mø°õðbø,ôxþ;: úFõ?ï`í6ð&lt;úx÷õ?úEú2ð&lt;ïEùBô &lt;&gt;= @? .ù10 &lt;ú`ôwõÄôhð&lt;ù ú2ð&lt;ùHûbðí.ïFõ?÷0ü³ûbþ.ùHôhì,ôrú2ìGùHí þ"
"Jþ: .ùHøA @4B6ð80í6ðbôoõ?ù úhüHð&lt;ù÷ð&lt;ïC:Îþ.ïEö\ô4úhüHðD:þ6ø,ø, úÀýþ\ôrú2ð&lt;÷Hôbò üHð- w:Îþ6ø,,ø þlý9ìGùBíYôrîBôEðbû&lt;ú2ì,þ.tù 0ðbôEû&lt;ï2ì/ðbôúEüHð9ûbþ.ï2ð ÷õ?ïhú9þ *: úEüBðDõ.,ø í6þ.ï2ìGúEü ö Hôhûbþ.ï`ìtùHímõiûlõ?ù 0ìRHõ?ú2ð &lt; | ! ~q%*),} I )"
I + FXH +}  ì/6ð. &lt;ùõiü.úhüHðbôEìRbð8?ù
"Rì Hõ?ú2ð :þ.ïõ4ûbð&lt;ïhú&lt;õ.ìGù ÷õ?úhú`ð&lt;_?ù0ùBìGùHíð2úÀýxðbð&lt;ùà÷þ6ôhìGú`ìþ.ùHô ?õ ù ^úhüHðoõ.,ø í6þ.ï2ìGúhü0ö ?ï`ûhüHðbôlÎþ: .\ï õ.ø,øÏú2ì,,ø ðbôiþ : ?ù 0ìRBõ?ú`ðìGù&amp;úhüHðöJð&lt;\ö þ.ïC úEï2ì,ð &lt;-üHì,ôì,ô þ ùHð úEïFõ 6ð. &lt;ï2ôEìGùBíúEüBðeôhð&lt;ù ú2ð&lt;ùHûbðí.ïFõ?÷0ü:fï`þ.ö. .ûEü÷zþ6ôhôEìHø,?ïEú2ìGùHím÷þ6ôEìGú2ì,.þ ù:Îþ.~ï ?ù 0ìA0 Bõ?ú`ðú2ì,ø,ðbô6îBôEìGùBíiõ ëk620Àø,ì/ ?ï2ûEüP&lt;L{6ð. &lt;ï ?õ ùið80í6ðxì,ôúEïFõ . ð2ï2ôhð \ìGùYúEüBðôEð&lt; í.ïFõ?÷0ü?úhüHð ûbþ. 0ìGùBíð í6ð/cõ: . õ.,ì ø°õHø,ð ,ôDúhï&lt;õ8.6ð&lt;ï2ôEð8 .ø,ôhþìGùúhüHðJúhï`ì,ðmú2þ:ð&lt;ú2ûEüúhüHðoôrúFõ?ú2ì,ôrú2ì,ûbô4þ :9úhüHðõ ûbþ. 0ìGùBí4ú`ì,øð;&lt; -üBðpõ.,ø í6þ.ï2ìGúEü .ö4÷ î0ú`ðbô^úhüHðk:þ6ø,ø,.ï2ð fî0ùBû2ú2ì,þ."
"Dù Îþ: .ï~.ûhüeú2ì,ø,3ð :Îþ.î0ù ìGùoúEüBðúEï2ì,ð4ò: ÷þ6ô ûbþ.;o( z o* ÷þ6ô ûbþ.*,wùHðbí ûbþ.;o(  -pì,øðFô9ô2õ?ú`ìô: g o*&gt; k ýpüBð2ï2ðw  ,ì ôxõ~÷0ï2ð{0Àôr÷ðbûbì/?Hð8Júhü0ï2ðbôrüHþ6ø ?ï`ð9ûbþ.10ù ôhìR0ð&lt;ï2ð8õ.~ô  ÷zþ.ïhú2ìGùHímð{.R0ð&lt;ùHûbð :þ.?ù 0ìA0 Bõ?ú`ðDõ?ù eûbþ./0î ú`ðDú2þ4ìGú2ô9ôEûbþ.ï2ð &lt; 4cùHûbðYõ.ø, ÷zþ.ïhú`ìtùHí4ú2ì,,ø ðbôcõ?ï`ð3:Îþ.î0ù 0úhüHðYõ./ø 0 í6þ.ï2ìGúEü öúhï`ì,þ: .ïSûbþz6ð. &lt;ï2ìGùHíiúEüBð ð&lt;0ù ú2ìGï2ð ? ù Rì Hõ?ú2ð &lt;-pìø,ð~ìGù:þ.ïhömõ?ú2ì,þ.ùmì,.ï2ð mìtùõ \{GgbTU õ.ô*ìGù4úhüHð@nzõ?ú&quot;. &lt;ð ï`ôhì,.þ ù, ¸õ: .ûFìø,ìGú&lt;?õ ú2ð ð \¢ .ï2ìGùHí1&lt;póSU9&apos;\{£:Îþ.ïõûlõ?ù 0ìRHõ?ú2ðYì,ôõ{ ôhð&lt;ú~þ ú`ì: ,,ø ?úlò ¤ .ûhüwýxþ.ïUþ : ?ù 0ìRHõ?ú2ðJì,ôDûbþ.&lt;õ.ìtùHð8 ìtù:õ?ú9ø,..ùHðú2ì,ø,ð ¤ ùBþú2ì,ø,ðDûbþ.&lt;.õ ìGùBô~õ?ùBþ.úEüBð2ï9ú2ì,,ø ðDð&lt;/J&lt; 4 fú2ð: &lt;ù,ð&lt;ïSþ;: ûbþz. ? ù?ú`ð8 fï`þ: .ö õôhð&lt;úJþ :cú`ì,ø,ðbô.ûEüö¤õ ÄûFþ..ìGù&quot;þz6ð. &lt;ï2ø°õ?÷0÷BìGùHí ú2ì,,ø ðbôþ.ïSú`ì,øðFôýpüBì,ûEüûbþz. ð2ïw /ì ¥ð&lt;ï`ð&lt;?ïhú`ôþ :úhüHð ûbþ.&lt;g&lt; -üBðàôhûbþ.ï`ìGùBífî0ùBû: &lt;ú`ì,.þ ù îHôEðbô&amp;úhüHð¦Îþ6ø: ,,ø þlý9ìtùHí 6îõ?,ðbôwÎþ: .ïcõí6ì/.6ð&lt;ùûlõ?ù 0ìRHõ?ú2ð.ò7 ¤ üBð- ïFõ?ú2ì, þ þ : àí.ïFõ?ù d0¸ú`þ.úFõ.ø$÷þ6ôEìtú`ì/.  .í ïFõ?ù d0¸ú`þ.úFõ.øp÷zþ6ôhìGú2ì/6ð. g~?ú2ì/.6ðûbþ. : ú2ì,ø,ðbô9ìGùõ.ø,øBúhüHðDûbþg6ð. &lt;ï`ô ; I  I  o*§zp{ ¨r©@{{/ºX«{µU«{±2¬¦»8®z¼«{µU½8¾ ¿XÀ^ÁÂ*ÃgÄb¾ ¨r©@ °R«{   »8®z¼J«{µg¾ Á¸È(,¿ É;ÈMÊJËhÂ*ÃzÄ{¾ ¨r©@/{{Í2¬ ¹/{µcÂ*°R«{/"
"ÁÓÒdÔ,;(ÃgÄ{ ¨r©@ ¯9Íd¹A³B²³Ø®g¼J«{µ@{@"
"N &amp;³ ¹/{  ¬_ {¬L¬b® 9WYX ämñ &gt;[Z : £¶* /° ¬U½ {, GN à ³Ï¹//³  { ± /±Ì¬Cª «Gûh¯9¬&gt;¼J«{@{ /{{{/{¯9±  ÅJ¹/&amp;{ Ú ¯Ñ1ª {« Ú ± &amp;«{{{ ³Ï¹//³  {Í2¬gàL©@ ¯9Íy¹/@°R«{ +%\ ª {µ /{^¹/³cÑµU®g¼J«{³&amp;{« ±2¬ Û£ª  T] ² ½¬b¹//&amp;«{  &amp;# íÑ1µU«8½Æ«{*/{ ò ?  = / ·ý¶( ¹/ = Ûw¹/ 9 Là ^  H-IKJML äØñ1¾Pò 9 &gt; ¬ÆµU¹A´1²1¬b«¬U®¹/±1¶*{« µb«{ 8\ c {µC¶*  ·¹/ºX«bµb«{/;³øò ? a/¹ { û ¹R»{¬lµb«8½C®J°/@{ºX«8»{&amp;¬  ? ½ô¹R½ó»8°R«g¯9µU°/Ð ¼y¹R½Æ¹/{± H I3JML _ ä k¹R½^@/± Å 87# ;&lt; /³cÑµU®g¼J«{³&amp;{«  g ; ñ &lt; ý¹/  ± à ^  ? "
"Â* b J/± { /± /± ¦«{Í2ÑX«b± ¹ ,  à {Í2ÑX«bµb¹/³Ï«{±¬b«8·Ûw¹/¬CªN½C«{ÑX«{µÆÚ ¯9¬b¹/ ? ;½Æ«{Ú_ ? ò ½ ç à cN Ù½,  @ .[h2xoYp.h*[i.[.i qAi...s8nmjup[.*b{ | }%~{o8xojh2 *nfjp[*[ s-o.*@ ¡¢&gt;z£Y¤  oo8§F§&gt; &gt; ¼ [® ½ &gt;¦©ozY8s8º% ?¶¿Á º sOÂ©Y8§F§V¥&gt;&gt;® ;Æ ¾$®_ÇROÈª¨ @¥&gt;¦§FO°sO&apos;;*s2ÌO©osÌ&gt; Í ±B³K´Mµ?. s¬«?­ÏÐ ® ÇROS.*O£»bb2B.&gt;(.zF£85 ¥  ÏÏsF8OÕ©Y.! .s&gt;¨&gt; O&gt;®"
"ÇM. á «?;ã ä ¶ê¸&gt;å[Û%%® âb£»b¢½YëÁY¸)!-o¡Êo;$ÑB82o¨ «?­ R»ìS¥&gt;à3O8ÌOà3O¦¥Aªz£8¡í©¦£8¡C-£ szV£8O¦® ÇROob.*%!.CsOC©¦£8¡ÔÑO¦¥ÈÒV­.sî8§!.s &gt;¦2o; 2A«9­MºT£8Ñ&gt;8O ¦¥. O¥°«?­ï¦3Ì§zA£&gt;®_Çð2£8CèS¡ñé&gt;.  Í  Ý Ý8Î ;ÃÂoBY.¦¥É©Y8s©Y8¥&gt;O&gt;ºc2¡$F§î.S£A£8Ì&gt;S*ÌO§Ñ »bsCso-oo.. 2R«?­ T.O¥ÔO£ sO£;*.&gt;ìz¦&gt;*§F§àO.Í  Ý Ú8ÎìëÂ8. ¡Êà8¥O.%!ºO? .Ñ-SoÖ¥(£C§FoÖ!@F ©Y8§¥O.C.*2sÌO©osÌ&gt;o8§ ô &gt;ùD÷8÷  ¯&quot;!·&gt;¦2o;¦¥ÞÊ¡Êo¡Ê£8sÃ;.2OÄO $.¥ÉoÖo2O£@;O§F£ ..2$&gt;(&quot;.©Y.).¥&gt;§F ¡ *;.&gt;Ì 2ÄOC¡ÔÑ-oÞ£8ªÀ*Ã@.z .2o2O*RÌOso¨&quot;F  ).szà ©oÌO§î.Yº&gt;@)¥ ¥&gt;@£ ¦RO£8bzo¨* ÿ ÌOAªuÌ§F§Ã;.2¦¥°2o;©¦¦)8Cs8OO&gt;ºìÌ&gt;&gt;¨ §FÄ ÇRÀ28OO5¡À.. î8§.zsO¢¡Êo2£%;o8 £8O§Ã ! ê£8ªCsO&quot;o. .szosO ¦ºÔ¡C§Ã@O ¡ÔÌO©s s¡C§Fo¹so8Oí¡À.î8§&quot; sO B.zsOGo8* àÞ§F¡ÊÄz¦¥B® ¡$o2O£@&gt;¥ Ô.8ºM8 ©¦©¦£8¥&gt;O §Ã º . ÇROFÅ2so8©o¤.O¥Ê&amp;2Ä¡ÊF§î©Y82¦. .zs ¦¥Ô©¦£82&gt; 8F§î. so8O&gt;º;.zs?FO£8DO¦©¦¦2.sÃ .¥&gt;§FO¢2OÍ ?&gt; £8ÑO§Fo¡)® B.@ézFs£8Â£8ªMsO .Ý Ý8Î  &gt;bé£¤@üÈ¡@F¥&gt;$V9sO£8z£8Ì8Ê©¦£8¡C¨o2O£@¥)»bs   ­ìòSé ¥.oý£8§Ã _ ©£8¡¢-£ 2¨ % ¥Â¦2o¡ÔÑO§àSsO SòV­ÏÁÔ¡$£%¥&gt;¦§Ä £8¥&gt;.&quot;oÂ£Ù©¦£8O3ssÌO©o²o¤²ÌOs¦É*Ì&gt;Ñ&gt;¨3©¦£8O3sÌOo@F¥&gt;;Þ&gt;.F£8ýÄ¦§  2sÌO©osÌ&gt;.z8ºcO£»ìo¤ oºì2£8¡$À¥&gt; -os¨* oO©¦¦ á &gt;Ì&gt;   O®8þf º .O¥ ;Ñ-R§FY¤! @£8TF  Oº@.;2Ñ&gt; £  ¢¤%îÔ2B.&gt;o9«?­M®%þf ! bé%üº@2¦2 8 »bà§F§B.. &quot; CF§F¦b8  Ä # .? F¦º $ $»ì£8ÌO§F¥ÂÑ-S©¦£82F¥@¨ o¦¥A8 &amp;( O® ª %&amp; Ê8T»R¦§F§-8 * £So¤ ) ¤ oÔ@F¥&gt;*Ì&gt;&gt;-£ sO -  + SOº.;O2¥ ,  . 9¥ ! &gt;bé%;2Ñ?£8&gt;ª /   »bà§F§ÑBÔ©Y8§F©oÌ§î.  (¦;Y¥&gt;¦¥²8¬ o2¡. ¥Cs©¦£8zb£8$?ÇRB $ ª  .·FMO£8T.;oÀ;&gt;; Y® þf .ÑOF§FKÃ&quot; 0"
ÂO£@&gt;¥  »ì£8ÌO§F¥ÊÑ-*zÄ2Ì;®?©Y8§F©oÌO§î.¥Àªz£8¡ .ì£8ªR©¦£8&gt;¨ ÇRRs©£8OV&gt;z£@©¦¦2M£8ª 1 $  o.F*zF©Y8§O¡$£%&gt;¥ ; &amp; bé%üTº@* ÑB82¦¥&quot;.¥²ªz£8¡ sO
F+N 68Q2\Z Q$&amp;@?S Q@?@BfK7BjSW^RSW^ ^WIJFHQ~]NO:0[REF_CITE]QRQ_ _ $/W47G_$WSUBRG_ ?@B9SW]^ ^0?@B9N&quot;: KLBjNOM c d6RB8^ =$U68_ =@&amp;Nk^0G$476 K7N :&quot;Q1KLG$DCMON G$b D@SU5SUG? $D@SUG]WNOSW_ fI^ $FHQ]NORQ_ &amp;B9SWV=$NB9SxQ$=$^; NO_ @G `u47B8:&quot; $G DCMOSxKLG]D0·+ y I7`u47B r-r KL6R6fK7RQC_ : [|v^x p K7SWMOSU:KLG$^\SU6.K7MYXOI1VVV dQ^ $4zF%_ G@`u47B8&quot;:[REF_CITE]GN :&lt;?@Bj4zTSW^ r KLG$D ¹ BfKfFADV=$r N _8Q$=CG@{ G$!b ?
"CB9SW:0SUK7^.^ FgSWMOMYX:047B8cVX$G , $G _WS47_=@:0[REF_CITE]Q^xF$+NOMOMwB9SU=$^8Sf^; `u47B-#?Kv`¥=C6R@N=CB9SHFG$\$G 47B8^d6UKLG{ ]_\S+?$[REF_CITE]4x&gt;½V»7¿&gt;=$ÀjÁjI5KLG8SH6RQ^ $$+SD0D^@Nk9KL^^e6jSU: : ; 3][REF_CITE]Q= $SU:Â=$RNG^ $bhKLG~[REF_CITE]BjN 6RQC:Ã:047BjS&amp;^e=]N 6R; KL3]MOSx`Y47BHQ/KLG$@D MONG$_ GC`Y47B8:&quot;[REF_CITE]GJX[|+;[REF_CITE]G/K7M]?54V^8^RN ]3 NOMON 6[REF_CITE]^ $= ^8SxF\;?5SWfI^ /e^ =$_8Q K7K^ #^d5SW?"
RN^ ; 6j[REF_CITE]G$^[MON VS47`zVFHQ{ ]NO_RQ!KL6R6fK7RQ_ ]Sf^[&quot;: K7N ]G M c06j4sG]47=@$G ^ &gt;  { .$G SKLG8N^
"PNOMtKLBTVK7N: $D vBjNOdQ^ @G694&quot;: ,KLGIlVVVjX{ @= 6sKLG$D \^,dVV7 6jSUGKLG]$D@^.\KLG$= &quot;8Q_ @$Q e6jN^ G$\^dQ1K7MOMk4zFH+;68Q?/$[REF_CITE]N^ 0[REF_CITE]Q: $G $:0[REF_CITE]Q$; 6j4*/Q KLG$@D MOS*f47_ : ? 54V^RN d6RB8^ =$U68_ @= BjSf^ &quot;@^!K ^8NOb7G$N ] _ &lt;: ?:\6RQ]S\Ä1KL6:0[REF_CITE]Q$ SW^d5Sf?"
WNtK7MkM_ c`u47[REF_CITE]G]b2KLG$DP47B9S: _W47&lt;: $?
"MOSU¡^e68BR]= _U; 68@= BjSWfX^ | ^,_ /?"
"SWU6jSWD_ /[REF_CITE]Q$S0?5SUBR`u47BR:KLG$_WS $?/KLBR6jNtK7M7:0[REF_CITE]Q$,Nkl^ ^ /[REF_CITE]` `a=$1KLBj? ^RSUBj^ $NO8Q_ &quot;?$MO4VN $DB9SWV=$: $= 8Q_"
BjNORQ_ ]SjBsN G@`u47B8:&quot;[REF_CITE]GJX[\Z $Q S%BjSW^e=$M69^ $Nk.^ MkN G$ BjSW^_RQ2SUG@[REF_CITE]Q$^e/?
N G]b0KL?@? _8Q$SW^fI/
K7N 0N: ]G b&lt;=$[REF_CITE]Q_ $%S ?&amp;/3 SU; eQ^ /K7MOMO4zFÅKLG$D`a$= MkM]?/8N^ $G @b X
Æ ¹&amp;SWSUG$^&quot;ÌaÍÎnÏ@$KL3]N G$S \ ]=
RQ_ Ó@Q]XhÔ4ÖMk¨7I&lt;[REF_CITE]Q1KLGKLG$@D { ^ÔxQ1K7MkNOM  :&quot; KLG0`u47B6RQ]47[REF_CITE]$= b7Q#KLG$&lt;D $Q SWM C`¥? ]=
"M1D@NO^8U_ $= ^R^8[REF_CITE]G$WI^ K7^ \SWMOM+^8_RQK7^ `Y47BHQ K7M]6jSUBSWM ?@`ap$=[REF_CITE]::KLG: $0KLG^ WX^ ]D|+G6fK7M\TVKLG $ÚdÙ@$Ü[Ý@Ù$Þ +öåjû7îþýdÿWìdí9+à ÷\à[ú  %   [/á .à ÷\àgü]å8äVäLæLú  .$ / #  ( * 9ú 6  5  í 8: !&lt;; $&quot; :&amp;&amp;# V =%&apos; à ?)&gt; (+ òLùwåRìUú @*,*# A ÿfìdû7ìdåjð&gt;ñýUà ë 4 xÿWä B $úUôRà A ëWïëfä$ D û C và \ &gt; ìEæ &lt; s B ÿ ÿ9ù @ [é 4 Öà =  E â x B å ë B&amp; xÿWìEæ @I@  F &lt;å J ë &amp;  5 5&lt;  5 &quot;ýdÿ G ä K@  ##POQSR&quot; ë &amp;@H$@ ÿ9ù  VXWZY\[2] 6T&quot; /ú +  5 í _^`: &lt;; &amp;: Ö a ú &lt; PÿfäýEìdå9 b ë &amp;  @ ë 4 xÿWä B $úUôRà A ëWïëfä$û D v C à \ &gt; ìEæ &lt; s B ÿ ÿ9ù @ [ â E x B å ë &amp; xÿWìEæ B L@ @  F &lt;å G ë &amp;  5 &lt;5  5 &quot;à J ýd  ÿ G&amp;# @(  &amp;LdO8e# $ ë &amp;@H@ ÿ9ù  % ),* &amp;&amp; #* &amp; ]&apos;V ú1çzç 4ka4^ é M; z a 4 é Vàg l ÷ mb á $F nporq é 4=^4l"
 ? ¢Z?£¤¡U¥ ? d?C??  ? £«C¬Z£­?§+¤?.° ± =²;³2´³ ? ¤¬µ¯C¤ ¢Z¼»½¹JÅ²U£¤ÆÅJÇÈ`É¼HÇÉMÈC¾v¸??v &quot;¿ ¹ÁÀÂ¹nÃl¿­¹JÃÄ lv¦§Jz©t©???$n°£ ± $? ¦lUC¤Ïz£ÐC¹J¸Ô·U£¤&quot;?v d©$?£­ ? £®Ö+CC`¯§J?$£­?Yª  Ù Ú @    9 Û @ Þ U¤Zª?;JÂ?§nU?ßv&quot;U°á®Â° ? £h?d?£n·
We present an LFG-DOP parser which uses fragments from LFG-annotated sentences to parse new sentences.
Experiments with the Verbmobil and Homecentre corpora show that (1) Viterbi n best search performs about 100 times faster than Monte Carlo search while both achieve the same accuracy; (2) the DOP hypothesis which states that parse accuracy increases with increasing frag-ment size is confirmed for LFG-DOP; (3) LFG-DOP&apos;s relative frequency estimator performs worse than a discounted frequency estimator; and (4) LFG-DOP significantly outperforms Tree-DOP if evaluated on tree structures only.
Data-Oriented Parsing (DOP) models learn how to provide linguistic representations for an unlimited set of utterances by generalizing from a given corpus of properly annotated exemplars.
They operate by decomposing the given representations into (arbitrarily large) fragments and recomposing those pieces to analyze new utterances.
A probability model is used to choose from the collection of different fragments of different sizes those that make up the most appropriate analysis of an utterance.
DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the Wall Street Journal corpus (see[REF_CITE]).
"The original DOP model[REF_CITE]was based on utterance analyses represented as surface trees, and is equivalent to a Stochastic Tree-Substitution Grammar."
"But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar[REF_CITE], Tree-Adjoining Grammar[REF_CITE], Lexical-Functional Grammar (Bod &amp;[REF_CITE]), Head-driven Phrase Structure Grammar (Neumann &amp;[REF_CITE]), and Montague Grammar[REF_CITE]."
"Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although[REF_CITE]trains fragment probabilities by a maximum likelihood reestimation procedure belonging to the class of expectation-maximization algorithms."
The DOP model has also been tested as a model for human sentence processing[REF_CITE].
"This paper presents ongoing work on DOP models for Lexical-Functional Grammar representations, known as LFG-DOP (Bod &amp;[REF_CITE])."
"We develop a parser which uses fragments from LFG-annotated sentences to parse new sentences, and we derive some experimental properties of LFG-DOP on two LFG-annotated corpora: the Verbmobil and Homecentre corpus."
"The experiments show that the DOP hypothesis, which states that there is an increase in parse accuracy if larger fragments are taken into account[REF_CITE], is confirmed for LFG-DOP."
We report on an improved search technique for estimating the most probable analysis.
"While a Monte Carlo search converges provably to the most probable parse, a Viterbi n best search performs as well as Monte Carlo while its processing time is two orders of magnitude faster."
We also show that LFG-DOP outperforms Tree-DOP if evaluated on tree structures only.
"In accordance[REF_CITE], a particular DOP model is described by • a definition of a well-formed representation for utterance analyses, • a set of decomposition operations that divide a given utterance analysis into a set of fragments, • a set of composition operations by which such fragments may be recombined to derive an analysis of a new utterance, and • a definition of a probability model that indicates how the probability of a new utterance analysis is computed."
"In defining a DOP model for LFG representations, Bod &amp;[REF_CITE]give the following settings for DOP&apos;s four parameters."
"The representations used by LFG-DOP are directly taken from LFG: they consist of a c-structure, an f-structure and a mapping φ between them."
The following figure shows an example representation for Kim eats. (We leave out some features to keep the example simple.)
Bod &amp; Kaplan also introduce the notion of accessibility which they later use for defining the decomposition operations of LFG-DOP:
"An f-structure unit f is φ-accessible from a node n iff either n is φ-linked to f (that is, f = φ(n) ) or f is contained within φ(n) (that is, there is a chain of attributes that leads from φ(n) to f)."
"According to the LFG representation theory, c-structures and f-structures must satisfy certain formal well-formedness conditions."
"A c-structure/f-structure pair is a valid LFG represent-ation only if it satisfies the Nonbranching Dominance, Uniqueness, Coherence and Com-pleteness conditions (Kaplan &amp;[REF_CITE])."
The fragments for LFG-DOP consist of connected subtrees whose nodes are in φ-correspondence with the correponding sub-units of f-structures.
To give a precise definition of
"LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the &quot;Tree-DOP&quot; model[REF_CITE]: (1) Root: the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates. (2) Frontier: the Frontier operation then chooses a set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes."
"Bod &amp; Kaplan extend Tree-DOP&apos;s Root and Frontier operations so that they also apply to the nodes of the c-structure in LFG, while respecting the principles of c/f-structure correspondence."
"When a node is selected by the Root operation, all nodes outside of that node&apos;s subtree are erased, just as in Tree-DOP."
"Further, for LFG-DOP, all φ links leaving the erased nodes are removed and all f-structure units that are not φ-accessible from the remaining nodes are erased."
"For example, if Root selects the NP in figure 1, then the f-structure corresponding to the S node is erased, giving figure 2 as a possible fragment:"
"In addition the Root operation deletes from the remaining f-structure all semantic forms that are local to f-structures that correspond to erased c-structure nodes, and it thereby also maintains the fundamental two-way connection between words and meanings."
"Thus, if Root selects the VP node so that the NP is erased, the subject semantic form &quot;Kim&quot; is also deleted:"
"As with Tree-DOP, the Frontier operation then selects a set of frontier nodes and deletes all subtrees they dominate."
"Like Root, it also removes the φ links of the deleted nodes and erases any semantic form that corresponds to any of those nodes."
"For instance, if the NP in figure 1 is selected as a frontier node, Frontier erases the predicate &quot;Kim&quot; from the fragment:"
"Finally, Bod &amp; Kaplan present a third decomposition operation, Discard, defined to construct generalizations of the fragments supplied by Root and Frontier."
Discard acts to delete combinations of attribute-value pairs subject to the following condition: Discard does not delete pairs whose values φ-correspond to remaining c-structure nodes.
"According to Bod &amp;[REF_CITE], Discard-generated fragments are needed to parse sentences that are &quot;ungrammatical with respect to the corpus&quot;, thus increasing the robustness of the model."
In LFG-DOP the operation for combining fragments is carried out in two steps.
"First the c-structures are combined by leftmost substitution subject to the category-matching condition, as in Tree-DOP."
This is followed by the recursive unification of the f-structures corresponding to the matching nodes.
"A derivation for an LFG-DOP representation R is a sequence of fragments the first of which is labeled with S and for which the iterative application of the composition operation produces R. For an illustration of the composition operation, see Bod &amp;[REF_CITE]."
"As in Tree-DOP, an LFG-DOP representation R can typically be derived in many different ways."
"If each derivation D has a probability P(D), then the probability of deriving R is the sum of the individual derivation probabilities: (1) P(R) ="
Σ D derives R P(D)
An LFG-DOP derivation is produced by a stochastic process which starts by randomly choosing a fragment whose c-structure is labeled with the initial category.
"At each subsequent step, a next fragment is chosen at random from among the fragments that can be composed with the current subanalysis."
The chosen fragment is composed with the current subanalysis to produce a new one; the process stops when an analysis results with no non-terminal leaves.
We will call the set of composable fragments at a certain step in the stochastic process the competition set at that step.
"Let CP(f | CS) denote the probability of choosing a fragment f from a competition set CS containing f, then the probability of a derivation D = &lt;f 1 , f 2 ... f k &gt; is (2) P(&lt;f 1 , f 2 ... f k &gt;) ="
Π i CP(f i | CS i ) where the competition probability CP(f | CS) is expressed in terms of fragment probabilities P(f):
P(f) (3) CP(f | CS) =
Σ f&apos;∈CS P(f&apos;)
"Bod &amp; Kaplan give three definitions of increasing complexity for the competition set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both Category-matching and Uniqueness; and the third definition groups all fragments which satisfy Category-matching, Uniqueness and Coherence."
"Bod &amp; Kaplan point out that the Completeness condition cannot be enforced at each step of the stochastic derivation process, and is a property of the final representation which can only be enforced by sampling valid representations from the output of the stochastic process."
"In this paper, we will only deal with the third definition of competition set, as it selects only those fragments at each derivation step that may finally result into a valid LFG representation, thus reducing the off-line validity checking to the Completeness condition."
Note that the computation of the competition probability in the above formulas still requires a definition for the fragment probability P(f).
"Bod and Kaplan define the probability of a fragment simply as its relative frequency in the bag of all fragments generated from the corpus, just as in most Tree-DOP models."
We will refer to this fragment estimator as &quot;simple relative frequency&quot; or &quot;simple RF&quot;.
We will also use an alternative definition of fragment probability which is a refinement of simple RF.
This alternative fragment probability definition distinguishes between fragments supplied by Root/Frontier and fragments supplied by Discard.
"We will treat the first type of fragments as seen events, and the second type of fragments as previously unseen events."
"We thus create two separate bags corresponding to two separate distributions: a bag with fragments generated by Root and Frontier, and a bag with fragments generated by Discard."
We assign probability mass to the fragments of each bag by means of discounting: the relative frequencies of seen events are discounted and the gained probability mass is reserved for the bag of unseen events (cf.[REF_CITE]).
We accomplish this by a very simple estimator: the Turing-Good estimator[REF_CITE]which computes the probability mass of unseen events as n 1 /N where n 1 is the number of singleton events and N is the total number of seen events.
This probability mass is assigned to the bag of Discard-generated fragments.
The remaining mass (1 − n 1 /N) is assigned to the bag of Root/Frontier-generated fragments.
The probability of each fragment is then computed as its relative frequency in its bag multiplied by the probability mass assigned to this bag.
"Let | f | denote the frequency of a fragment f, then its probability is given by: (4) P(f | f is generated byRoot/Frontier) = | f | (1 − n 1 /N)"
Σ f&apos;: f&apos;is generated by Root/Frontier | f&apos;| (5) P(f | f is generated by Discard) = | f | (n 1 /N)
Σ f&apos;: f&apos; is generated by Discard | f&apos;|
We will refer to this fragment probability estimator as &quot;discounted relative frequency&quot; or &quot;discounted RF&quot;.
"In his PhD-thesis,[REF_CITE]presents a parsing algorithm for LFG-DOP which is based on the Tree-DOP parsing technique described[REF_CITE]."
Cormons first converts LFG-representations into more compact indexed trees: each node in the c-structure is assigned an index which refers to the φ-corresponding f-structure unit.
"For example, the representation in figure 1 is indexed as (S.1 (NP.2 Kim.2) (VP.1 eats.1)) where 1 --&gt; [ (SUBJ = 2) (TENSE = PRES) (PRED = eat(SUBJ)) ] 2 --&gt; [ (PRED = Kim) (NUM = SG) ]"
The indexed trees are then fragmented by applying the Tree-DOP decomposition operations described in section 2.
"Next, the LFG-DOP decomposition operations Root, Frontier and Discard are applied to the f-structure units that correspond to the indices in the c-structure subtrees."
"Having obtained the set of LFG-DOP fragments in this way, each test sentence is parsed by a bottom-up chart parser using initially the indexed subtrees only."
Thus only the Category-matching condition is enforced during the chart-parsing process.
The Uniqueness and Coherence conditions of the corresponding f-structure units are enforced during the disambiguation or chart -decoding process.
Disambiguation is accomplished by computing a large number of random derivations from the chart and by selecting the analysis which results most often from these derivations.
This technique is known as &quot;Monte Carlo disambiguation&quot; and has been extensively described in the literature (e.g.[REF_CITE]; Chappelier &amp;[REF_CITE]).
Sampling a random derivation from the chart consists of choosing at random one of the fragments from the set of composable fragments at every labeled chart-entry (where the random choices at each chart-entry are based on the probabilities of the fragments).
"The derivations are sampled in a top-down, leftmost order so as to maintain the LFG-DOP derivation order."
Thus the competition sets of composable fragments are computed on the fly during the Monte Carlo sampling process by grouping the f-structure units that unify and that are coherent with the subderivation built so far.
"As mentioned in section 3, the Completeness condition can only be checked after the derivation process."
Incomplete derivations are simply removed from the sampling distribution.
"After sampling a sufficiently large number of random derivations that satisfy the LFG validity requirements, the most probable analysis is estimated by the analysis which results most often from the sampled derivations."
"As a stop condition on the number of sampled derivations, we compute the probability of error, which is the probability that the analysis that is most frequently generated by the sampled derivations is not equal to the most probable analysis, and which is set to 0.05 (see[REF_CITE])."
"In order to rule out the possibility that the sampling process never stops, we use a maximum sample size of 10,000 derivations."
"While the Monte Carlo disambiguation technique converges provably to the most probable analysis, it is quite inefficient."
"It is possible to use an alternative, heuristic search based on Viterbi n best (we will not go into the PCFG-reduction technique presented[REF_CITE]since that heuristic only works for Tree-DOP and is beneficial only if all subtrees are taken into account and if the so-called &quot;labeled recall parse&quot; is computed)."
"A Viterbi n best search for LFG-DOP estimates the most probable analysis by computing n most probable derivations, and by then summing up the probabilities of the valid derivations that produce the same analysis."
The algorithm for computing n most probable derivations follows straight-forwardly from the algorithm which computes the most probable derivation by means of Viterbi optimization (see e.g. Sima&apos;an 1999).
We derived some experimental properties of LFG-DOP by studying its behavior on the two LFG-annotated corpora that are currently available: the Verbmobil corpus and the Homecentre corpus.
Both corpora were annotated at Xerox PARC.
They contain packed LFG-representations (Maxwell &amp;[REF_CITE]) of the grammatical parses of each sentence together with an indication which of these parses is the correct one.
For our experiments we only used the correct parses of each sentence resulting in 540 Verbmobil parses and 980 Homecentre parses.
Each corpus was divided into a 90% training set and a 10% test set.
This division was random except for one constraint: that all the words in the test set actually occurred in the training set.
The sentences from the test set were parsed and disambiguated by means of the fragments from the training set.
"Due to memory limitations, we restricted the maximum depth of the indexed subtrees to 4."
Because of the small size of the corpora we averaged our results on 10 different training/test set splits.
"Besides an exact match accuracy metric, we also used a more fine-grained score based on the well-known PARSEVAL metrics that evaluate phrase-structure trees[REF_CITE]."
The PARSEVAL metrics compare a proposed parse P with the corresponding correct treebank parse T as follows: # correct constituents in P Precision = # constituents in P # correct constituents in P Recall = # constituents in T
A constituent in P is correct if there exists a constituent in T of the same label that spans the same words and that φ-corresponds to the same f-structure unit (see[REF_CITE]for some illustrations of these metrics for LFG-DOP).
We were first interested in comparing the performance of the simple RF estimator against the discounted RF estimator.
"Furthermore, we want to study the contribution of generalized fragments to the parse accuracy."
We therefore created for each training set two sets of fragments: one which contains all fragments (up to depth 4) and one which excludes the generalized fragments as generated by Discard.
The exclusion of these Discard-generated fragments means that all probability mass goes to the fragments generated by Root and Frontier in which case the two estimators are equivalent.
The following two tables present the results of our experiments where +Discard refers to the full set of fragments and −Discard refers to the fragment set without Discard-generated fragments.
"The tables show that the simple RF estimator scores extremely bad if all fragments are used: the exact match is only 1.1% on the Verbmobil corpus and 2.7% on the Homecentre corpus, whereas the discounted RF estimator scores respectively 35.9% and 38.4% on these corpora."
"Also the more fine-grained precision and recall scores obtained with the simple RF estimator are quite low: e.g. 13.8% and 11.5% on the Verbmobil corpus, where the discounted RF estimator obtains 77.5% and 76.4%."
"Interestingly, the accuracy of the simple RF estimator is much higher if Discard-generated fragments are excluded."
This suggests that treating generalized fragments probabilistically in the same way as ungeneralized fragments is harmful.
The tables also show that the inclusion of Discard-generated fragments leads only to a slight accuracy increase under the discounted RF estimator.
"Unfortunately, according to paired t-testing only the differences for the precision scores on the Homecentre corpus were statistically significant."
We were also interested in the impact of fragment size on the parse accuracy.
We therefore performed a series of experiments where the fragment set is restricted to fragments of a certain maximum depth (where the depth of a fragment is defined as the longest path from root to leaf of its c-structure unit).
"We used the same training/test set splits as in the previous experiments and used both ungeneralized and generalized fragments together with the discounted RF estimator. fragments are included, but that the increase itself decreases."
"This phenomenon is also known as the DOP hypothesis[REF_CITE], and has been confirmed for Tree-DOP on the ATIS, OVIS and Wall Street Journal treebanks (see[REF_CITE]1999, 2000a; Sima&apos;an 1999;[REF_CITE])."
The current result thus extends the validity of the DOP hypothesis to LFG annotations.
"We do not yet know whether the accuracy continues to increase if even larger fragments are included (for Tree-DOP it has been shown that the accuracy decreases after a certain depth, probably due to overfitting -- cf."
"In the following experiment, we are interested in the impact of functional structures on predicting the correct tree structures."
"We therefore removed all f-structure units from the fragments, thus yielding a Tree-DOP model, and compared the results against the full LFG-DOP model (using the discounted RF estimator and all fragments up to depth 4)."
"We evaluated the parse accuracy on the tree structures only, using exact match together with the standard PARSEVAL measures."
We used the same training/test set splits as in the previous experiments.
The results indicate that LFG-DOP&apos;s functional structures help to improve the parse accuracy of tree structures.
"In other words, LFG-DOP outperforms Tree-DOP if evaluated on tree structures only."
According to paired t-tests all differences in accuracy were statistically significant.
This result is promising since Tree-DOP has been shown to obtain state-of-the-art performance on the Wall Street Journal corpus (see[REF_CITE]).
"Finally, we were interested in comparing an alternative, more efficient search method for estimating the most probable analysis."
"In the following set of experiments we use a Viterbi n best search heuristic (as explained in section 4), and let n range from 1 to 10,000 derivations."
We also compute the results obtained by Monte Carlo for the same number of derivations.
We used the same training/test set splits as in the previous experiments and used both ungeneralized and generalized fragments up to depth 4 together with the discounted RF estimator.
The tables show that Viterbi n best already achieves a maximum accuracy at 100 derivations (at least on the Verbmobil corpus) while Monte Carlo needs a much larger number of derivations to obtain these results.
"On the Homecentre corpus, Monte Carlo slightly outperforms Viterbi n best at 10,000 derivations, but these differences are not statistically significant."
Also remarkable are the relatively high results obtained with Viterbi n best if only one derivation is used.
This score corresponds to the analysis generated by the most probable (valid) derivation.
Thus Viterbi n best is a promising alternative to Monte Carlo resulting in a speed up of about two orders of magnitude.
We presented a parser which analyzes new input by probabilistically combining fragments from LFG-annotated corpora into new analyses.
"We have seen that the parse accuracy increased with increasing fragment size, and that LFG&apos;s functional structures contribute to significantly higher parse accuracy on tree structures."
"We tested two search techniques for the most probable analysis, Viterbi n best and Monte Carlo."
"While these two techniques achieved about the same accuracy, Viterbi n best was about 100 times faster than Monte Carlo."
"We introduce an annotation scheme for temporal expressions, and describe a method for resolving temporal expressions in print and broadcast news."
"The system, which is based on both hand-crafted and machine-learnt rules, achieves an 83.2% accuracy (F-measure) against hand-annotated data."
Some initial steps towards tagging event chronologies are also described.
The extraction of temporal information from news offers many interesting linguistic challenges in the coverage and representation of temporal expressions.
It is also of considerable practical importance in a variety of current applications.
"For example, in question-answering, it is useful to be able to resolve the underlined reference in “the next year, he won the Open” in response to a question like “When did X win the U.S. Open?”."
"In multi-document summarization, providing fine-grained chronologies of events over time (e.g., for a biography of a person, or a history of a crisis) can be very useful."
"In information retrieval, being able to index broadcast news stories by event times allows for powerful multimedia browsing capabilities."
"Our focus here, in contrast to previous work such[REF_CITE], is on resolving time expressions, especially indexical expressions like “now”, “today”, “tomorrow”, “next Tuesday”, “two weeks ago”, “20 mins after the next hour”, etc., which designate times that are dependent on the speaker and some “reference” time [Footnote_1] ."
1 Some of these indexicals have been called “relative times” in the[REF_CITE]temporal tagging task.
"In this paper, we discuss a temporal annotation scheme for representing dates and times in temporal expressions."
This is followed by details and performance measures for a tagger to extract this information from news sources.
"The tagger uses a variety of hand-crafted and machine-discovered rules, all of which rely on lexical features that are easily recognized."
We also report on a preliminary effort towards constructing event chronologies from this data.
"Any annotation scheme should aim to be simple enough to be executed by humans, and yet precise enough for use in various natural language processing tasks."
Our approach[REF_CITE]has been to annotate those things that a human could be expected to tag.
"Our representation of times uses the ISO standard CC:YY:MM:DD:HH:XX:SS, with an optional time zone[REF_CITE]."
"In other words, time points are represented in terms of a calendric coordinate system, rather than a real number line."
The standard also supports the representation of weeks and days of the week in the format CC:YY:Wwwd where ww specifies which week within the year ([Footnote_1]-53) and d specifies the day of the week (1-7).
1 Some of these indexicals have been called “relative times” in the[REF_CITE]temporal tagging task.
"For example, “last week” might receive the[REF_CITE]:00:W16."
"A time (TIMEX) expression (of type TIME or DATE) representing a particular point on the ISO line, e.g., “Tuesday,[REF_CITE]” (or “next Tuesday”) is represented with the ISO time Value (VAL), 20:00:11:02."
Interval expressions like “From
"In addition to the values provided by the ISO standard, we have added several extensions, including a list of additional tokens to represent some commonly occurring temporal units; for example, “summer of ‘69” could be represented as 19:69:SU."
The intention here is to capture the information in the text while leaving further interpretation of the Values to applications using the markup.
"It is worth noting that there are several kinds of temporal expressions that are not to be tagged, and that other expressions tagged as a time expression are not assigned a value, because doing so would violate the simplicity and preciseness requirements."
"We do not tag unanchored intervals, such as “half an hour (long)” or “(for) one month”."
"Non-specific time expressions like generics, e.g., “April” in “April is usually wet”, or “today” in “today’s youth”, and indefinites, e.g., “a Tuesday”, are tagged without a value."
"Finally, expressions which are ambiguous without a strongly preferred reading are left without a value."
"This representation treats points as primitive (as do[REF_CITE],[REF_CITE], among others); other representations treat intervals as primitive, e.g.,[REF_CITE]."
"Arguments can be made for either position, as long as both intervals and points are accommodated."
"The annotation scheme does not force committing to end-points of intervals, and is compatible with current temporal ontologies such[REF_CITE]; this may help eventually support advanced inferential capabilities based on temporal information extraction."
The system architecture of the temporal tagger is shown in Figure 1.
The tagging program takes in a document which has been tokenized into words and sentences and tagged for part-of-speech.
"The program passes each sentence first to a module that identifies time expressions, and then to another module (SC) that resolves self-contained time expressions."
The program then takes the entire document and passes it to a discourse processing module (DP) which resolves context-dependent time expressions (indexicals as well as other expressions).
"The DP module tracks transitions in temporal focus, uses syntactic clues, and various other knowledge sources."
The module uses a notion of Reference Time to help resolve context-dependent expressions.
"Here, the Reference Time is the time a context-dependent expression is relative to."
"In our work, the reference time is assigned the value of either the Temporal Focus or the document (creation) date."
The Temporal Focus is the time currently being talked about in the narrative.
The initial reference time is the document date.
We now discuss the modules that assign values to identified time expressions.
"Times which are fully specified are tagged with their value, e.g, “[REF_CITE]” as 19:99:06 by the SC module."
The DP module uses an ordered sequence of rules to handle the context-dependent expressions.
These cover the following cases:
"Explicit offsets from reference time: indexicals like “yesterday”, “today”, “tomorrow”, “this afternoon”, etc., are ambiguous between a specific and a non-specific reading."
"The specific use (distinguished from the generic one by machine learned rules discussed below) gets assigned a value based on an offset from the reference time, but the generic use does not."
Positional offsets from reference time:
"Expressions like “next month”, “last year” and “this coming Thursday” use lexical markers (underlined) to describe the direction and magnitude of the offset from the reference time."
Implicit offsets based on verb tense:
"Expressions like “Thursday” in “the action taken Thursday”, or bare month names like “February” are passed to rules that try to determine the direction of the offset from the reference time."
"Once the direction is determined, the magnitude of the offset can be computed."
The tense of a neighboring verb is used to decide what direction to look to resolve the expression.
"Such a verb is found by first searching backward to the last TIMEX, if any, in the sentence, then forward to the end of the sentence and finally backwards to the beginning of the sentence."
"If the tense is past, then the direction is backwards from the reference time."
"If the tense is future, the direction is forward."
"If the verb is present tense, the expression is passed on to subsequent rules for resolution."
"For example, in the following passage, “Thursday” is resolved to the Thursday prior to the reference date because “was”, which has a past tense tag, is found earlier in the sentence:"
"The Iraqi news agency said the first shipment of 600,000 barrels was loaded Thursday by the oil tanker Edinburgh."
"Further use of lexical markers: Other expressions lacking a value are examined for the nearby presence of a few additional markers, such as “since” and “until”, that suggest the direction of the offset."
"Nearby Dates: If a direction from the reference time has not been determined, some dates, like “Feb. 14”, and other expressions that indicate a particular date, like “Valentine’s Day”, may still be untagged because the year has not been determined."
"If the year can be chosen in a way that makes the date in question less than a month from the reference date, that year is chosen."
"For example, if the reference date is Feb. 20, 2000 and the expression “Feb. 14” has not been assigned a value, this rule would assign it the value Feb. 14, 2000."
Dates more than a month away are not assigned values by this rule.
There were two different genres used in the testing: print news and broadcast news transcripts.
The print news consisted of 22 New York Times (NYT) articles[REF_CITE].
The broadcast news data consisted of 199 transcripts of Voice of America (VOA) broadcasts[REF_CITE]taken from the TDT2 collecti[REF_CITE].
"The print data was much cleaner than the transcribed broadcast data in the sense that there were very few typographical errors, spelling and grammar were good."
"On the other hand, the print data also had longer, more complex sentences with somewhat greater variety in the words used to represent dates."
"The broadcast collection had a greater proportion of expressions referring to time of day, primarily due to repeated announcements of the current time and the time of upcoming shows."
The test data was marked by hand tagging the time expressions and assigning value to them where appropriate.
"This hand-marked data was used to evaluate the performance of a frozen version of the machine tagger, which was trained and engineered on a separate body of NYT, ABC News, and CNN data."
Only the body of the text was included in the tagging and evaluation.
The system performance is shown in Table 1 [Footnote_2] .
2 The evaluated version of the system does not adjust the Reference Time for subsequent sentences.
"Note that if the human said the TIMEX had no value, and the system decided it had a value, this is treated as an error."
"A baseline of just tagging values of absolute, fully specified TIMEXs (e.g., “[REF_CITE]st , 1999”) is shown for comparison in parentheses."
"Obviously, we would prefer a larger data sample; we are currently engaged in an effort within the information extraction community to annotate a large sample of the TDT2 collection and to conduct an inter-annotator reliability study."
"Our approach has been to develop initial rules by hand, conduct an initial evaluation on an unseen test set, determine major errors, and then handling those errors by augmenting the rule set with additional rules discovered by machine learning."
"As noted earlier, distinguishing between specific use of a time expression and a generic use (e.g., “today”, “now”, etc.) was and is a significant source of error."
"Some of the other problems that these methods could be applied to distinguishing a calendar year reference from a fiscal year one (as in “this year”), and distinguishing seasonal from specific day references."
"For example, “Christmas” has a seasonal use (e.g., “I spent Christmas visiting European capitals”) distinct from its reference to a specific day use as “[REF_CITE]th ” (e.g., “We went to a great party on Christmas”)."
Here we discuss machine learning results in distinguishing specific use of “today” (meaning the day of the utterance) from its generic use meaning “nowadays”.
"In addition to features based on words co-occurring with “today” (Said, Will, Even, Most, and Some features below), some other features (DOW and CCYY) were added based on a granularity hypothesis."
"Specifically, it seems possible that “today” meaning the day of the utterance sets a scale of events at a day or a small number of days."
"The generic use, “nowadays”, seems to have a broader scale."
"Therefore, terms that might point to one of these scales such as the names of days of the week, the word “year” and four digit years were also included in the training features."
"To summarize, the features we used for the “today” problem are as follows (features are boolean except for string-valued POS1 and POS2):"
Poss: whether “today” has a possessive inflection
Qcontext: whether “today” is inside a
Table 3 shows the performance of different classifiers in classifying occurrences of “today” as generic versus specific.
"The results are for 377 training vectors and 191 test vectors, measured in terms of Predictive Accuracy (percentage test vectors correctly classified)."
We incorporated some of the rules learnt by C4.5 Rules (the only classifier which directly output rules) into the current version of the program.
"These rules included classifying “today” as generic based on (1) feature Most being true (74.1% accuracy) or (2) based on feature FW being true and Poss, Some and Most being false (67.4% accuracy)."
"The granularity hypothesis was partly borne out in that C4.5 rules also discovered that the mention of a day of a week (e.g. “Monday”), anywhere in the sentence predicted specific use (73.3% accuracy)."
Event Ordering Our work in this area is highly preliminary.
"To extract temporal relations between events, we have developed an event-ordering component, following[REF_CITE]."
"We encode the tense associated with each verb using their modified Reichenbachian[REF_CITE]representation based on the tuple &lt;s i , lge, r i , lge, e i &gt;."
"Here s i is an index for the speech time, r i for the reference time, and e i for the event time, with lge being the temporal relations precedes, follows, or coincides."
"With each successive event, the temporal focus is either maintained or shifted, and a temporal ordering relation between the event and the focus is asserted, using heuristics defining coherent tense sequences; see[REF_CITE]for more details."
"Note that the tagged TIME expressions aren&apos;t used in determining these inter-event temporal relations, so this event-ordering component could be used to order events which don&apos;t have time VALs."
"The most relevant prior work is[REF_CITE], who dealt with meeting scheduling dialogs (see also[REF_CITE],[REF_CITE]), where the goal is to schedule a time for the meeting."
"The temporal references in meeting scheduling are somewhat more constrained than in news, where (e.g., in a historical news piece on toxic dumping) dates and times may be relatively unconstrained."
"In addition, their model requires the maintenance of a focus stack."
"They obtained roughly .91 Precision and .80 Recall on one test set, and .87 Precision and .68 Recall on another."
"However, they adjust the reference time during processing, which is something that we have not yet addressed."
"More recently,[REF_CITE]have independently developed an annotation scheme which represents both time values and more fine-grained inter-event and event-time temporal relations."
"Although our work is much more limited in scope, and doesn&apos;t exploit the internal structure of events, their annotation scheme may be leveraged in evaluating aspects of our work."
"The MUC-7 task[REF_CITE]did not require VALs, but did test TIMEX recognition accuracy."
"However, at least 30% of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices. )"
"Finally, there is a large body of work, e.g.,[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE], that has focused on a computational analysis of tense and aspect."
"While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work."
¥¶·R¹º«¯»7¤5¼­½»¼©ª²X»7¼º©¾»7¥§®£m¤R¥§¦m¨5©ª¨&lt;.©¦7²R¨Q«©ª´±´¶¿µ¦7«¬³O¥§¦7«g­µ´¶«g©ª¬7·R¸´¶«g®^.» À¬¼ªº &quot;© ¦7«¯»¼ÁÀW·R¼²R·¨R¤R¬ &quot;¨5¬7¼¨&lt;«g¬ &gt;­R¥ÄÃp« ¬« ·O»¦7« º©Á·O»7¥§®.®¯´§©ª¦¦7«g¦gÂ¾»7¤R«g¥±¬ »¤R«Æ´¶«g©ª¬7·R¥¶·R¹Ç¨5¬7¼X®¯«h¦7¦&lt;^® »7¥¶³ª«µÀ«g©¾»²R¬«g¦gÅ½£m¤R«¬«g¦7²R´±»[&quot;¼ÁÀ É ¤R¥§® &quot;¤ ®¯´§©ª¦¦7¥ÄÊ5«h¦W©Á²5·RË·R¼ É ¨R¬¼ª¨Q« ¬W·5©ªº« ¼·Ì»¤R«Í°5©ª¦7¼ª¥±»¦G® ¼ª·O»7« Î»¢¼ªÀÏ¼X® ® ²R¬7¸ ¬« ·5®  º ©¾»«Ô»7¤R«¨R¬¼ª°&lt;©Á°R¥¶´±¥±»u¿Æ­X¥§¦H»7¬¥±°R²R»7¥¶¼ª·Õ¼ÁÀ}©ª· »©Á¹¦7«¯»hÅG£m¤R¥¶¦¨5¬7¼°5©Á°R¥¶´¶¥Ä»u¿Æ­R¥¶¦H»7¬¥±°5²X»7¥¶¼ª·Õ¥¶¦©Á°5²R´¶©ª¬7¿¢¨R¬¼ª¨Q« ¬ ·5©ªº«Ï¼¾³« ¬© ¥±»¦7«©¦H»7¼X®¤5©ª¦H»7¥§®}¨5©Á¬7». «h®   ©Á¹¹ª«©Áº«¬hÅ»7« ¬ ¦¼ÁÀ Ö × S = -RØ 2TÙ 6 = 4HØS
The order of prenominal adjectival modifiers in English is governed by complex and difficult to describe con-straints which straddle the boundary between competence and performance.
This paper describes and compares a number of statistical and machine learning techniques for ordering se-quences of adjectives in the context of a natural language generation system.
The question of robustness is a perennial prob-lem for parsing systems.
"In order to be useful, a parser must be able to accept a wide range of input types, and must be able to gracefully deal with dysfluencies, false starts, and other ungram-matical input."
"In natural language generation, on the other hand, robustness is not an issue in the same way."
"While a tactical generator must be able to deal with a wide range of semantic inputs, it only needs to produce grammatical strings, and the grammar writer can select in advance which construction types will be considered grammati-cal."
"However, it is important that a generator not produce strings which are strictly speaking gram-matical but for some reason unusual."
This is a particular problem for dialog systems which use the same grammar for both parsing and genera-tion.
The looseness required for robust parsing is in direct opposition to the tightness needed for high quality generation.
One area where this tension shows itself clearly is in the order of prenominal modifiers in English.
"In principle, prenominal adjectives can, depend-ing on context, occur in almost any order: the large red American car ?? the American red large car *car American red the large"
"Some orders are more marked than others, but none are strictly speaking ungrammatical."
"So, the grammar should not put any strong constraints on adjective order."
"For a generation system, how-ever, it is important that sequences of adjectives be produced in the ‘correct’ order."
Any other or-der will at best sound odd and at worst convey an unintended meaning.
"Unfortunately, while there are rules of thumb for ordering adjectives, none lend themselves to a computational implementation."
"For example, ad-jectives denoting size do tend to precede adjec-tives denoting color."
"However, these rules under-specify the relative order for many pairs of adjec-tives and are often difficult to apply in practice."
"In this paper, we will discuss a number of statisti-cal and machine learning approaches to automati-cally extracting from large corpora the constraints on the order of prenominal adjectives in English."
The problem of generating ordered sequences of adjectives is an instance of the more general prob-lem of selecting among a number of possible outputs from a natural language generation sys-tem.
"One approach to this more general problem, taken by the ‘Nitrogen’ generator[REF_CITE], takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model."
Langkilde and Knight report that this strategy yields good results for problems like generating verb/object collocations and for selecting the cor-rect morphological form of a word.
It also should be straightforwardly applicable to the more spe-cific problem we are addressing here.
"To deter-mine the correct order for a sequence of prenom-inal adjectives, we can simply generate all possi-ble orderings and choose the one with the high-est probability."
"This has the advantage of reduc-ing the problem of adjective ordering to the prob-lem of estimating n-gram probabilities, some-thing which is relatively well understood."
"To test the effectiveness of this strategy, we took as a dataset the first one million sentences of the written portion of the British National Cor-pus[REF_CITE]. [Footnote_1]"
1 The relevant files were identified by the absence of the &lt;settDesc&gt; (spoken text “setting description”) SGML tag in the file header. Thanks to John Carroll for help in prepar-ing the corpus.
We held out a randomly se-lected 10% of this dataset and constructed a back-off bigram model from the remaining 90% using the CMU-Cambridge statistical language model-ing toolkit[REF_CITE].
We then evaluated the model by extracting all se-quences of two or more adjectives followed by a noun from the held-out test data and counted the number of such sequences for which the most likely order was the actually observed order.
"Note that while the model was constructed using the entire training set, it was evaluated based on only sequences of adjectives."
The results of this experiment were some-what disappointing.
The apparent reason that this method performs as poorly as it does for this par-ticular problem is that sequences of adjectives are relatively rare in written English.
This is evi-denced by the fact that in the test data only one se-quence of adjectives was found for every twenty sentences.
"With adjective sequences so rare, the chances of finding information about any particu-lar sequence of adjectives is extremely small."
The data is simply too sparse for this to be a reliable method.
"Since Langkilde and Knight’s general approach does not seem to be very effective in this particu-lar case, we instead chose to pursue more focused solutions to the problem of generating correctly ordered sequences of prenominal adjectives."
"In addition, at least one generation algorithm[REF_CITE]inserts adjectival modifiers in a post-processing step."
This makes it easy to in-tegrate a distinct adjective-ordering module with the rest of the generation system.
"To evaluate various methods for ordering prenominal adjectives, we first constructed a dataset by taking all sequences of two or more adjectives followed by a common noun in the 100 million tokens of written English in the British National Corpus."
"Among these pairs, there were 127,016 different pair types, and 23,941 different adjective types."
"For test purposes, we then randomly held out 10% of the pairs, and used the remaining 90% as the training sample."
"Before we look at the different methods for predicting the order of adjective pairs, there are two properties of this dataset which bear noting."
"First, it is quite sparse."
"Second, we get no useful information about the syntag-matic context in which a pair appears."
"The left-hand context is almost always a determiner, and including information about the modified head noun would only make the data even sparser."
"This lack of context makes this problem different from other problems, such as part-of-speech tagging and grapheme-to-phoneme conversion, for which statistical and machine learning solutions have been proposed."
The simplest strategy for ordering adjectives is wh[REF_CITE]call the direct evidence method.
"To order the pair {a,b}, count how many times the ordered sequences ha,bi and hb,ai appear in the training data and output the pair in the order which occurred more often."
"This method has the advantage of being con-ceptually very simple, easy to implement, and highly accurate for pairs of adjectives which ac-tually appear in the training data."
Applying this method to the adjectives sequences taken from the BNC yields better than 98% accuracy for pairs that occurred in the training data.
"However, since as we have seen, the majority of pairs occur only once, the overall accuracy of this method is 59.72%, only slightly better than random guess-ing."
"Fortunately, another strength of this method is that it is easy to identify those pairs for which it is likely to give the right result."
This means that one can fall back on another less accurate but more general method for pairs which did not oc-cur in the training data.
"In particular, if we ran-domly assign an order to unseen pairs, we can cut the error rate in half and raise the overall accuracy to 78.28%."
"It should be noted that the direct evidence method as employed here is slightly different from Shaw and Hatzivassiloglou’s: we simply compare raw token counts and take the larger value, while they applied a significance test to es-timate the probability that a difference between counts arose strictly by chance."
"Like one finds in a trade-off between precision and recall, the use of a significance test slightly improved the accu-racy of the method for those pairs which it had an opinion about, but also increased the number of pairs which had to be randomly assigned an order."
"As a result, the net impact of using a sig-nificance test for the BNC data was a very slight decrease in the overall prediction accuracy."
The direct evidence method is straightforward to implement and gives impressive results for ap-plications that involve a small number of frequent adjectives which occur in all relevant combina-tions in the training data.
"However, as a general approach to ordering adjectives, it leaves quite a bit to be desired."
"In order to overcome the sparseness inherent to this kind of data, we need a method which can generalize from the pairs which occur in the training data to unseen pairs."
One way to think of the direct evidence method is to see that it defines a relation ≺ on the set of En-glish adjectives.
"Given two adjectives, if the or-dered pair ha,bi appears in the training data more often then the pair hb,ai, then a ≺ b."
"If the re- verse is true, and hb,ai is found more often than ha,bi, then b ≺ a."
"If neither order appears in the training data, then neither a ≺ b nor b ≺ a and an order must be randomly assigned."
"However, the pairs hlarge,newi and hnew,greeni occur fairly frequently."
"There-fore, in the face of this evidence we can assign this pair the order hlarge,greeni, which not coin-cidently is the correct English word order."
The difficulty with applying the transitive clo-sure method to any large dataset is that there of-ten will be evidence for both orders of any given pair.
"For instance, alongside the evidence sup-porting the order hlarge,greeni, we also find the pairs hgreen,byzantinei, hbyzantine,decorativei, and hdecorative,newi, which suggest the order hgreen,largei."
"Intuitively, the evidence for the first order is quite a bit stronger than the evidence for the sec-ond."
"The first ordered pairs are more frequent, as are the individual adjectives involved."
"To quan-tify the relative strengths of these transitive in-ferences,[REF_CITE]pro-pose to assign a weight to each link."
"Say the order ha,bi occurs m times and the pair {a,b} occurs n times in total."
Then the weight of the pair a → b is: n ! n −log 1 − ∑ n · 21 k=m k
This weight decreases as the probability that the observed order did not occur strictly by chance increases.
"This way, the problem of finding the order best supported by the evidence can be stated as a general shortest path problem: to find the pre-ferred order for {a,b}, find the sum of the weights of the pairs in the lowest-weighted path from a to b and from b to a and choose whichever is lower."
"Using this method, Shaw and Hatzivassiloglou report predictions ranging from 81% to 95% ac-curacy on small, domain specific samples."
"How-ever, they note that the results are very domain- specific."
"Applying a graph trained on one domain to a text from another another generally gives very poor results, ranging from 54% to 58% accu-racy."
"Applying this method to the BNC data gives 83.91% accuracy, in line with Shaw and Hatzivas-siloglou’s results and considerably better than the direct evidence method."
"However, applying the method is computationally a bit expensive."
"Like the direct evidence method, it requires storing ev-ery pair of adjectives found in the training data along with its frequency."
"In addition, it also re-quires solving the all-pairs shortest path problem, for which common algorithms run in O(n 3 ) time."
Another way to look at the direct evidence method is as a comparison between two proba-bilities.
"Given an adjective pair {a,b}, we com-pare the number of times we observed the order ha,bi to the number of times we observed the or-der hb,ai."
"Dividing each of these counts by the total number of times {a,b} occurred gives us the maximum likelihood estimate of the probabilities P(ha,bi|{a,b}) and P(hb,ai|{a,b})."
"Looking at it this way, it should be clear why the direct evidence method does not work well, as maximum likelihood estimation of bigram proba-bilities is well known to fail in the face of sparse data."
It should also be clear how we might im-prove the direct evidence method.
"Using the same strategy as described in section 2, we constructed a back-off bigram model of adjective pairs, again using the CMU-Cambridge toolkit."
"Since this model was constructed using only data specifi-cally about adjective sequences, the relative in-frequency of such sequences does not degrade its performance."
"Therefore, while the word bigram model gave an accuracy of only 75.57%, the ad-jective bigram model yields an overall prediction accuracy of 88.02% for the BNC data."
An important property of the direct evidence method for ordering adjectives is that it requires storing all of the adjective pairs observed in the training data.
"In this respect, the direct evidence method can be thought of as a kind of memory-based learning."
"Memory-based (also known as lazy, near-est neighbor, instance-based, or case-based) ap-proaches to classification work by storing all of the instances in the training data, along with their classes."
"To classify a new instance, the store of previously seen instances is searched to find those instances which most resemble the new instance with respect to some similarity metric."
The new instance is then assigned a class based on the ma-jority class of its nearest neighbors in the space of previously seen instances.
"To make the comparison between the direct evidence method and memory-based learning clearer, we can frame the problem of adjective or-dering as a classification problem."
"Given an un-ordered pair {a,b}, we can assign it some canon-ical order to get an instance ab."
"Then, if a pre-cedes b more often than b precedes a in the train-ing data, we assign the instance ab to the class a ≺ b."
"Otherwise, we assign it to the class b ≺ a."
"Seen as a solution to a classification problem, the direct evidence method then is an application of memory-based learning where the chosen sim-ilarity metric is strict identity."
"As with the inter-pretation of the direct evidence method explored in the previous section, this view both reveals a reason why the method is not very effective and also indicates a direction which can be taken to improve it."
"By requiring the new instance to be identical to a previously seen instance in order to classify it, the direct evidence method is unable to generalize from seen pairs to unseen pairs."
"There-fore, to improve the method, we need a more ap-propriate similarity metric that allows the classi-fier to get information from previously seen pairs which are relevant to but not identical to new un-seen pairs."
"Following the conventional linguistic wisdom ([REF_CITE]e.g.), this similarity metric should pick out adjectives which belong to the same semantic class."
"Unfortunately, for many adjectives this information is difficult or impos-sible to come by."
Machine readable dictionar-ies and lexical databases such as WordNet[REF_CITE]do provide some information about semantic classes.
"However, the semantic classifi-cation in a lexical database may not make exactly the distinctions required for predicting adjective order."
"More seriously, available lexical databases are by necessity limited to a relatively small num-ber of words, of which a relatively small fraction are adjectives."
"In practice, the available sources of semantic information only provide semantic classifications for fairly common adjectives, and these are precisely the adjectives which are found frequently in the training data and so for which semantic information is least necessary."
"While we do not reliably have access to the meaning of an adjective, we do always have ac-cess to its form."
"And, fortunately, for many of the cases in which the direct evidence method fails, finding a previously seen pair of adjec-tives with a similar form has the effect of find-ing a pair with a similar meaning."
"For ex-ample, suppose we want to order the adjective pair {21-year-old,Armenian}."
"If this pair ap-pears in the training data, then the previous oc-currences of this pair will be used to predict the order and the method reduces to direct ev-idence."
"If, on the other hand, that particu-lar pair did not appear in the training data, we can base the classification on previously seen pairs with a similar form."
"In this way, we may find pairs like {73-year-old,Colombian} and {44-year-old,Norwegian}, which have more or less the same distribution as the target pair."
"To test the effectiveness of a form-based sim-ilarity metric, we encoded each adjective pair ab as a vector of 16 features (the last 8 characters of a and the last 8 characters of b) and a class a ≺ b or b ≺ a. Constructing the instance base and testing the classification was performed using the TiMBL 3.0[REF_CITE]memory-based learning system."
Instances to be classified were compared to previously seen instances by counting the number of feature values that the two instances had in common.
"In computing the similarity score, features were weighted by their information gain, an in-formation theoretic measure of the relevance of a feature for determining the correct classification ([REF_CITE]; Daelemans and van den[REF_CITE])."
This weighting reduces the sensitivity of memory based learning to the presence of irrele-vant features.
"Given the probability p i of finding each class i in the instance base D, we can compute the en-tropy H(D), a measure of the amount of uncer-tainty in D:"
H(D) = − ∑ p i log 2 p i p i
"In the case of the adjective ordering data, there are two classes a ≺ b and b ≺ a, each of which occurs with a probability of roughly 0.5, so the entropy of the instance base is close to 1 bit."
We can also compute the entropy of a feature f which takes values V as the weighted sum of the entropy of each of the values V:
H(D f ) = ∑ H(D f=v )|D|D| f=v i | i v i ∈V
Here H(D f=v i ) is the entropy of subset of the in-stance base which has value v i for feature f .
The information gain of a feature then is simply the difference between the total entropy of the in-stance base and the entropy of a single feature:
"G(D, f ) = H(D) − H(D f )"
"The information gain G(D, f ) is the reduction in uncertainty in D we expect to achieve by learning the value of the feature f ."
"In other words, know-ing the value of a feature with a higher G gets us closer on average to knowing the class of an in-stance than knowing the value of a feature with a lower G does."
"The similarity ∆ between two instances then is the number of feature values they have in com-mon, weighted by the information gain: n ∆(X,Y) = ∑ G(D,i)δ(x i ,y i ) i=1 where: 1 if x i = y i δ(x i ,y i ) = 0 otherwise"
"Classification was based on the five training in-stances most similar to the instance to be classi-fied, and produced an overall prediction accuracy of 89.34% for the BNC data."
One difficulty faced by each of the methods de-scribed so far is that they all to one degree or an-other depend on finding particular pairs of adjec-tives.
"For example, in order for the direct evi-dence method to assign an order to a pair of ad-jectives like {blue, large}, this specific pair must have appeared in the training data."
"If not, an or-der will have to be assigned randomly, even if the individual adjectives blue and large appear quite frequently in combination with a wide vari-ety of other adjectives."
"Both the adjective bigram method and the memory-based learning method reduce this dependency on pairs to a certain ex-tent, but these methods still suffer from the fact that even for common adjectives one is much less likely to find a specific pair in the training data than to find some pair of which a specific adjec-tive is a member."
"Recall that the adjective bigram method depended on estimating the probabilities P(ha,bi|{a,b}) and P(hb,ai|{a,b})."
"Suppose we now assume that the probability of a particular adjective appearing first in a sequence depends only on that adjective, and not the the other ad-jectives in the sequence."
"We can easily estimate the probability that if an adjective pair includes some given adjective a, then that adjective occurs first (let us call that P(ha,xi|{a,x})) by looking at each pair in the training data that includes that adjective a. Then, given the assumption of independence, the probability P(ha,bi|{a,b}) is simply the product of P(ha,xi|{a,x}) and P(hx,bi|{b,x})."
"Taking the most likely order for a pair of adjectives using this alternative method for estimating P(ha,bi|{a,b}) and P(ha,bi|{a,b}) gives quite good results: a prediction accuracy of 89.73% for the BNC data."
"At first glance, the effectiveness of this method may be surprising since it is based on an indepen-dence assumption which common sense indicates must not be true."
"However, to order a pair of ad-jectives, this method brings to bear information from all the previously seen pairs which include either of adjectives in the pair in question."
"Since it makes much more effective use of the train-ing data, it can nevertheless achieve high accu-racy."
This method also has the advantage of be-ing computationally quite simple.
Applying this method requires only one easy-to-calculate value be stored for each possible adjective.
"Compared to the other methods, which require at a mini-mum that all of the training data be available dur-ing classification, this represents a considerable resource savings."
"The two highest scoring methods, using memory-based learning and positional probability, perform similarly, and from the point of view of accuracy there is little to recommend one method over the other."
"However, it is interesting to note that the er-rors made by the two methods do not completely overlap: while either of the methods gives the right answer for about 89% of the test data, one of the two is right 95.00% of the time."
This in-dicates that a method which combined the infor-mation used by the memory-based learning and positional probability methods ought to be able to perform better than either one individually.
"To test this possibility, we added two new fea-tures to the representation described in section 3.5."
"Besides information about the morphological form of the adjectives in the pair, we also included the positional probabilities P(ha,xi|{a,x}) and P(hb,xi|{b,x}) as real-valued features."
"For nu-meric features, the similarity metric ∆ is com-puted using the scaled difference between the val-ues: x i − y i δ(x i ,y i ) = max i − min i"
"Repeating the MBL experiment with these two additional features yields 91.85% accuracy for the BNC data, a 24% reduction in error rate over purely morphological MBL with only a modest increase in resource requirements."
"To get an idea of what the upper bound on ac-curacy is for this task, we tried applying the di-rect evidence method trained on both the train-ing data and the held-out test data."
"This gave an accuracy of approximately 99%, which means that 1% of the pairs in the corpus are in the ‘wrong’ order."
"For an even larger percentage of pairs either order is acceptable, so an evaluation procedure which assumes that the observed or-der is the only correct order will underestimate the classification accuracy."
"Native speaker intu-itions about infrequently-occurring adjectives are not very strong, so it is difficult to estimate what fraction of adjective pairs in the corpus are ac-tually unordered."
"However, it should be clear that even a perfect method for ordering adjectives would score well below 100% given the experi-mental set-up described here."
"While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement."
Future work will pur-sue at least two directions for improving the re-sults.
"First, while semantic information is not available for all adjectives, it is clearly available for some."
"Furthermore, any realistic dialog sys-tem would make use of some limited vocabulary for which semantic information would be avail-able."
"More generally, distributional clustering techniques[REF_CITE]could be applied to extract semantic classes from the corpus itself."
"Since the constraints on adjec-tive ordering in English depend largely on seman-tic classes, the addition of semantic information to the model ought to improve the results."
The second area where the methods described here could be improved is in the way that multi-ple information sources are integrated.
The tech-nique method described in section 3.7 is a fairly crude method for combining frequency informa-tion with symbolic data.
It would be worthwhile to investigate applying some of the more sophis-ticated ensemble learning techniques which have been proposed in the literature[REF_CITE].
"In particular, boosting[REF_CITE]offers the possibility of achieving high accuracy from a collection of classifiers which in-dividually perform quite poorly."
"In this paper, we have presented the results of ap-plying a number of statistical and machine learn-ing techniques to the problem of predicting the order of prenominal adjectives in English."
The scores for each of the methods are summarized in table 1.
"The best methods yield around 90% ac-curacy, better than the best previously published methods when applied to the broad domain data of the British National Corpus."
Note that Mc-Nemar’s test[REF_CITE]confirms the sig-nificance of all of the differences reflected here (with p &lt; 0.005) with the exception of the differ-ence between purely morphological MBL and the method based on positional probabilities.
"From this investigation, we can draw some ad-ditional conclusions."
"First, a solution specific to adjective ordering works better than a gen- eral probabilistic filter."
"Second, machine learn-ing techniques can be applied to a different kind of linguistic problem with some success, even in the absence of syntagmatic context, and can be used to augment a hand-built competence gram-mar."
"Third, in some cases statistical and memory based learning techniques can be combined in a way that performs better than either individually."
"I am indebted to Carol Bleyle, John Carroll, Ann Copestake, Guido Minnen, Miles Osborne, au-diences at the University of Groningen and the University of Sussex, and three anonymous re-viewers for their comments and suggestions."
The work described here was supported by the School of Behavioral and Cognitive Neurosciences at the University of Groningen.
Spoken dialogue managers have benefited from using stochastic planners such as Markov Decision Processes (MDPs).
"How-ever, so far, MDPs do not handle well noisy and ambiguous speech utterances."
"We use a Partially Observable Markov Decision Pro-cess (POMDP)-style approach to generate dialogue strategies by inverting the notion of dialogue state; the state represents the user’s intentions, rather than the system state."
"We demonstrate that under the same noisy con-ditions, a POMDP dialogue manager makes fewer mistakes than an MDP dialogue man-ager."
"Furthermore, as the quality of speech recognition degrades, the POMDP dialogue manager automatically adjusts the policy."
The development of automatic speech recognition has made possible more natural human-computer interaction.
"Speech recognition and speech un-derstanding, however, are not yet at the point where a computer can reliably extract the in-tended meaning from every human utterance."
"Human speech can be both noisy and ambigu-ous, and many real-world systems must also be speaker-independent."
"Regardless of these diffi-culties, any system that manages human-machine dialogues must be able to perform reliably even with noisy and stochastic speech input."
"Recent research in dialogue management has shown that Markov Decision Processes (MDPs) can be useful for generating effective dialogue strategies[REF_CITE]; the system is modelled as a set of states that represent the dialogue as a whole, and a set of actions corre-sponding to speech productions from the system."
The goal is to maximise the reward obtained for fulfilling a user’s request.
"However, the correct way to represent the state of the dialogue is still an open problem[REF_CITE]."
A common solution is to restrict the system to a single goal.
"For example, in booking a flight in an automated travel agent system, the system state is described in terms of how close the agent is to being able to book the flight."
Such systems suffer from a principal prob-lem.
"A conventional MDP-based dialogue man-ager must know the current state of the system at all times, and therefore the state has to be wholly contained in the system representation."
"These systems perform well under certain conditions, but not all."
"For example, MDPs have been used successfully for such tasks as retrieving e-mail or making travel arrangements[REF_CITE]over the phone, task domains that are generally low in both noise and ambigu-ity."
"However, the issue of reliability in the face of noise is a major concern for our application."
"Our dialogue manager was developed for a mobile robot application that has knowledge from sev-eral domains, and must interact with many peo-ple over time."
"For speaker-independent systems and systems that must act in a noisy environment, the user’s action and intentions cannot always be used to infer the dialogue state; it may be not be possible to reliably and completely determine the state of the dialogue following each utterance."
"The poor reliability of the audio signal on a mo-bile robot, coupled with the expectations of nat-ural interaction that people have with more an-thropomorphic interfaces, increases the demands placed on the dialogue manager."
"Most existing dialogue systems do not model confidences on recognition accuracy of the hu-man utterances, and therefore do not account for the reliability of speech recognition when apply-ing a dialogue strategy."
"Some systems do use the log-likelihood values for speech utterances, how-ever these values are only thresholded to indicate whether the utterance needs to be confirmed[REF_CITE]."
"An important concept lying at the heart of this issue is that of observability – the ultimate goal of a dialogue system is to satisfy a user request; how-ever, what the user really wants is at best partially observable."
We handle the problem of partial observabil-ity by inverting the conventional notion of state in a dialogue.
The world is viewed as partially unobservable – the underlying state is the inten-tion of the user with respect to the dialogue task.
"The only observations about the user’s state are the speech utterances given by the speech recog-nition system, from which some knowledge about the current state can be inferred."
"By accepting the partial observability of the world, the dia-logue problem becomes one that is addressed by Partially Observable Markov Decision Processes (POMDPs)[REF_CITE]."
Finding an optimal policy for a given POMDP model corresponds to defining an optimal dialogue strategy.
Optimality is attained within the context of a set of rewards that define the relative value of taking various ac-tions.
"We will show that conventional MDP solutions are insufficient, and that a more robust method-ology is required."
"Note that in the limit of per-fect sensing, the POMDP policy will be equiva-lent to an MDP policy."
What the POMDP policy offers is an ability to compensate appropriately for better or worse sensing.
"As the speech recog-nition degrades, the POMDP policy acquires re-ward more slowly, but makes fewer mistakes and blind guesses compared to a conventional MDP policy."
There are several POMDP algorithms that may be the natural choice for policy genera-ti[REF_CITE].
"However, solving real world dialogue scenarios is computationally in- tractable for full-blown POMDP solvers, as the complexity is doubly exponential in the number of states."
We therefore will use an algorithm for finding approximate solutions to POMDP-style problems and apply it to dialogue management.
"This algorithm, the Augmented MDP, was devel-oped for mobile robot navigati[REF_CITE], and operates by augmenting the state de-scription with a compression of the current belief state."
"By representing the belief state succinctly with its entropy, belief-space planning can be ap-proximated without the expected complexity."
"In the first section of this paper, we develop the model of dialogue interaction."
"This model allows for a more natural description of dialogue prob-lems, and in particular allows for intuitive han-dling of noisy and ambiguous dialogues."
"Few existing dialogues can handle ambiguous input, typically relying on natural language processing to resolve semantic ambiguities[REF_CITE]."
"Secondly, we present a description of an example problem domain, and finally we present experimental results comparing the performance of the POMDP (approximated by the Augmented MDP) to conventional MDP dialogue strategies."
"A Partially Observable Markov Decision Process (POMDP) is a natural way of modelling dialogue processes, especially when the state of the sys-tem is viewed as the state of the user."
The par-tial observability capabilities of a POMDP pol-icy allows the dialogue planner to recover from noisy or ambiguous utterances in a natural and autonomous way.
"At no time does the machine interpreter have any direct knowledge of the state of the user, i.e, what the user wants."
The machine interpreter can only infer this state from the user’s noisy input.
The POMDP framework provides a principled mechanism for modelling uncertainty about what the user is trying to accomplish.
"The POMDP consists of an underlying, unob-servable Markov Decision Process."
The MDP is specified by: 8 a set of states ;9 :=&lt; @&gt; ?.&gt;(( 8 a set of actions F2:=*&lt; ?=((. 8 a R KL set &gt;  &gt; of . transition probabilities J2KL&gt;&apos;MNA..(&gt; OQP 8 a set of rewards TVU*9XWYF[\Z^] 8 an initial state &gt; _
The actions represent the set of responses that the system can carry out.
"The transition prob-abilities form a structure over the set of states, connecting the states in a directed graph with arcs between states with non-zero transition prob-abilities."
The rewards define the relative value of accomplishing certain actions when in certain states.
The POMDP adds: 8 a set of observations `a:=*&lt; b ?=b*(.b*cdE 8 a set of observation probabilities .&gt ; =A
"R KLb S &gt; A= and replaces 8 the initial state (&gt; _ with an initial belief, R Kh&gt; _ &gt; _ij9: ;O( 8 the set of rewards with rewards conditioned on observations as well: TVU*9XWYFkWl`VZ\^]"
The observations consist of a set of keywords which are extracted from the speech utterances.
"The POMDP plans in belief space; each belief consists of a probability distribution over the set of states, representing the respective probability that the user is in each of these states."
The ini-tial belief specified in the model is updated every time the system receives a new observation from the user.
"The POMDP model, as defined above, first goes through a planning phase, during which it finds an optimal strategy, or policy, which de-scribes R an optimal mapping of action G to be-lief KL&gt;5Um&gt; _ : ;O , for all possible beliefs."
The dialogue manager uses this policy to direct its behaviour during conversations with users.
The optimal strategy for a POMDP is one that pre-scribes action selection that maximises the ex-pected reward.
"Unfortunately, finding an opti-mal policy exactly for all but the most trivial POMDP problems is computationally intractable."
"A near-optimal policy can be computed signifi-cantly faster than an exact one, at the expense of a slight reduction in performance."
"This is often done by imposing restrictions on the policies that can be selected, or by simplifying the belief state and solving for a simplified uncertainty represen-tation."
"In the Augmented MDP approach, the POMDP problem is simplified by noticing that the belief state of the system tends to have a certain struc-ture."
The uncertainty that the system has is usu-ally domain-specific and localised.
"For example, it may be likely that a household robot system can confuse TV channels (‘ABC’ for ‘NBC’), but it is unlikely that the system will confuse a TV chan-nel request for a request to get coffee."
"By making the localised assumption about the uncertainty, it becomes possible to summarise any given belief vector by a pair consisting of the most likely state, and the entropy of the belief state. n  rts u y s@x n .|{ K n KLo(} (1)   n |K n KLo(+O O~P   ?"
KLo( v B n KLo(O { (2)
The entropy of the belief state approximates a suf-ficient statistic for the entire belief state [Footnote_1] .
"1 Although sufficient statistics are usually moments of continuous distributions, our experience has shown that the entropy serves equally well."
"Given this assumption, we can plan a policy for every possible such &lt; state, entropy E pair, that approx-imates the POMDP policy for the corresponding belief n KLo(O ."
"The system that was used throughout these ex-periments is based on a mobile robot, Florence"
"Nightingale (Flo), developed as a prototype nurs-ing home assistant."
"Flo uses the Sphinx II speech recognition system[REF_CITE], and the Festival speech synthesis system[REF_CITE]."
Figure 1 shows a picture of the robot.
"Since the robot is a nursing home assistant, we use task domains that are relevant to assisted liv-ing in a home environment."
"Table 1 shows a list of the task domains the user can inquire about (the time, the patient’s medication schedule, what is on different TV stations), in addition to a list of robot motion commands."
These abilities have all been implemented on Flo.
"The medication sched-ule is pre-programmed, the information about the TV schedules is downloaded on request from the web, and the motion commands correspond to pre-selected robot navigation sequences."
"If we translate these tasks into the framework that we have described, the decision problem has 13 states, and the state transition graph is given in Figure 2."
"The different tasks have varying lev-els of complexity, from simply saying the time, to going through a list of medications."
"For simplic-ity, only the maximum-likelihood transitions are shown in Figure 2."
Note that this model is hand-crafted.
There is ongoing research into learning policies automatically using reinforcement learn-ing[REF_CITE]; dialogue models could be learned in a similar manner.
This example model is simply to illustrate the utility of the POMDP approach.
"The remain-ing 10 actions are clarification or confirmation ac-tions, such as re-confirming the desired TV chan-nel."
The reward structure gives the most reward for choosing actions that satisfy the user request.
These actions then lead back to the beginning state.
Most other actions are penalised with an equivalent negative amount.
"However, the confir- mation/clarification actions are penalised lightly (values close to 0), and the motion commands are penalised heavily if taken from the wrong state, to illustrate the difference between an undesirable action that is merely irritating (i.e., giving an in-appropriate response) and an action that can be much more costly (e.g., having the robot leave the room at the wrong time, or travel to the wrong destination)."
Table 2 shows an example dialogue obtained by having an actual user interact with the system on the robot.
The left-most column is the emitted observation from the speech recognition system.
"The operating conditions of the system are fairly poor, since the microphone is on-board the robot and subject to background noise as well as being located some distance from the user."
"In the fi-nal two lines of the script, the robot chooses the correct action after some confirmation questions, despite the fact that the signal from the speech recogniser is both very noisy and also ambiguous, containing cues both for the “say hello” response and for robot motion to the kitchen."
"We compared the performance of the three al-gorithms (conventional MDP, POMDP approx-imated by the Augmented MDP, and exact POMDP) over the example domain."
The met-ric used was to look at the total reward accumu-lated over the course of an extended test.
"In or-der to perform this full test, the observations and states from the underlying MDP were generated stochastically from the model and then given to the policy."
"The action taken by the policy was re-turned to the model, and the policy was rewarded based on the state-action-observation triplet."
"The experiments were run for a total of 100 dialogues, where each dialogue is considered to be a cycle of observation-action utterances from the start state request_begun through a sequence of states and back to the start state."
The time was nor-malised by the length of each dialogue cycle.
"The exact POMDP policy was generated using the Incremental Improvement algorithm (Cassan- dra et al., 1997)."
"The solver was unable to com-plete a solution for the full state space, so we cre-ated a much smaller dialogue model, with only 7 states and 2 task domains: time and weather in-formation."
"Figure 3 shows the performance of the three algorithms, over the course of 100 dialogues."
"Notice that the exact POMDP strategy outper-formed both the conventional MDP and approx-imate POMDP; it accumulated the most reward, and did so with the fastest rate of accumulation."
"The good performance of the exact POMDP is not surprising because it is an optimal solution for this problem, but time to compute this strategy is high: 729 secs, compared with 1.6 msec for the[REF_CITE]msec for the Augmented MDP."
Figure 4 demonstrates the algorithms on the full dialogue model as given in Figure 2.
"Because of the number of states, no exact POMDP solution could be computed for this problem; the POMDP policy is restricted to the approximate solution."
"The POMDP solution clearly outperforms the conventional MDP strategy, as it more than triples the total accumulated reward over the lifetime of the strategies, although at the cost of taking longer to reach the goal state in each dialogue."
Table 3 breaks down the numbers in more de-tail.
"The average reward for the[REF_CITE].6 per action, which is the maximum reward for most actions, suggesting that the POMDP is tak-ing the right action about 95% of the time."
"Fur-thermore, the average reward per dialogue for the[REF_CITE]compared to 49.7 for the conven-tional MDP, which suggests that the conventional MDP is making a large number of mistakes in each dialogue."
"Finally, the standard deviation for the POMDP is much narrower, suggesting that this algorithm is getting its rewards much more consistently than the conventional MDP."
We verified the utility of the POMDP approach by testing the approximating model on human users.
"The user testing of the robot is still pre-liminary, and therefore the experiment presented here cannot be considered a rigorous demonstra-tion."
"However, Table 4 shows some promising results."
"Again, the POMDP policy is the one pro-vided by the approximating Augmented MDP."
The experiment consisted of having users inter-act with the mobile robot under a variety of con-ditions.
The users tested both the POMDP and an implementation of a conventional MDP dialogue manager.
Both planners used exactly the same model.
"The users were presented first with one manager, and then the other, although they were not told which manager was first and the order varied from user to user randomly."
"The user la-belled each action from the system as “Correct” (+100 reward), “OK” (-1 reward) or “Wrong” (- 100 reward)."
"The “OK” label was used for re-sponses by the robot that were questions (i.e., did not satisfy the user request) but were relevant to the request, e.g., a confirmation of TV channel when a TV channel was requested."
"The system performed differently for the three test subjects, compensating for the speech recog-nition accuracy which varied significantly be-tween them."
"In user #2’s case, the POMDP man-ager took longer to satisfy the requests, but in general gained more reward per action."
"This is because the speech recognition system generally had lower word-accuracy for this user, either be-cause the user had unusual speech patterns, or be-cause the acoustic signal was corrupted by back-ground noise."
"By comparison, user #3’s results show that in the limit of good sensing, the POMDP policy ap-proaches the MDP policy."
"This user had a much higher recognition rate from the speech recog-niser, and consequently both the POMDP and conventional MDP acquire rewards at equivalent rates, and satisfied requests at similar rates."
This paper discusses a novel way to view the dialogue management problem.
"The domain is represented as the partially observable state of the user, where the observations are speech ut-terances from the user."
"The POMDP represen-tation inverts the traditional notion of state in dia-logue management, treating the state as unknown, but inferrable from the sequences of observations from the user."
"Our approach allows us to model observations from the user probabilistically, and in particular we can compensate appropriately for more or less reliable observations from the speech recognition system."
"In the limit of perfect recog-nition, we achieve the same performance as a conventional MDP dialogue policy."
"However, as recognition degrades, we can model the effects of actively gathering information from the user to offset the loss of information in the utterance stream."
"In the past, POMDPs have not been used for di-alogue management because of the computational complexity involved in solving anything but triv-ial problems."
"We avoid this problem by using an augmented MDP state representation for approxi-mating the optimal policy, which allows us to find a solution that quantitatively outperforms the con-ventional MDP, while dramatically reducing the time to solution compared to an exact POMDP algorithm (linear vs. exponential in the number of states)."
"We have shown experimentally both in sim-ulation and in preliminary user testing that the POMDP solution consistently outperforms the conventional MDP dialogue manager, as a func-tion of erroneous actions during the dialogue."
"We are able to show with actual users that as the speech recognition performance varies, the dia-logue manager is able to compensate appropri-ately."
"While the results of the POMDP approach to the dialogue system are promising, a number of improvements are needed."
"The POMDP is overly cautious, refusing to commit to a particular course of action until it is completely certain that it is ap-propriate."
This is reflected in its liberal use of ver-ification questions.
"This could be avoided by hav-ing some non-static reward structure, where infor-mation gathering becomes increasingly costly as it progresses."
"The policy is extremely sensitive to the param-eters of the model, which are currently set by hand."
"While learning the parameters from scratch for a full POMDP is probably unnecessary, auto-matic tuning of the model parameters would def-initely add to the utility of the model."
"For exam-ple, the optimality of a policy is strongly depen-dent on the design of the reward structure."
It fol-lows that incorporating a learning component that adapts the reward structure to reflect actual user satisfaction would likely improve performance.
The authors would like to thank Tom Mitchell for his advice and support of this research.
"Kevin Lenzo and Mathur Ravishankar made our use of Sphinx possible, answered requests for information and made bug fixes willingly."
"Tony Cassandra was extremely helpful in distributing his POMDP code to us, and answering promptly any questions we had."
"The assistance of the Nursebot team is also gratefully acknowledged, including the members from the School of Nurs-ing and the Department of Computer Science In-telligent Systems at the University of Pittsburgh."
This research was supported in part by Le Fonds pour la Formation de Chercheurs et l’Aide à la Recherche (Fonds FCAR).
Prepositional phrase attachment is a common source of ambiguity in natural language processing.
We present an unsupervised corpus-based approach to prepositional phrase attachment that achieves similar performance to supervised methods.
"Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus."
Attachment decisions are made using a linear combination of features and low frequency events are approximated using contextually similar words.
Prepositional phrase attachment is a common source of ambiguity in natural language processing.
The goal is to determine the attachment site of a prepositional phrase in a sentence.
Consider the following examples: [Footnote_1].
1 This research was conducted at the University of Manitoba.
Mary ate the salad with a fork. 2.
Mary ate the salad with croutons.
"In both cases, the task is to decide whether the prepositional phrase headed by the preposition with attaches to the noun phrase (NP) headed by salad or the verb phrase (VP) headed by ate."
"In the first sentence, with attaches to the VP since Mary is using a fork to eat her salad."
"In sentence 2, with attaches to the NP since it is the salad that contains croutons."
"Formally, prepositional phrase attachment is simplified to the following classification task."
"Given a 4-tuple of the form (V, N 1 , P, N 2 ), where V is the head verb, N 1 is the head noun of the object of V, P is a preposition, and N 2 is the head noun of the prepositional complement, the goal is to classify as either adverbial attachment (attaching to V) or adjectival attachment (attaching to N 1 )."
"For example, the 4-tuple (eat, salad, with, fork) has target classification V."
"In this paper, we present an unsupervised corpus-based approach to prepositional phrase attachment that outperforms previous unsupervised techniques and approaches the performance of supervised methods."
"Unlike previous unsupervised approaches in which training data is obtained by heuristic extraction of unambiguous examples from a corpus, we use an iterative process to extract training data from an automatically parsed corpus."
"The attachment decision for a 4-tuple (V, N 1 , P, N 2 ) is made as follows."
"First, we replace V and N 2 by their contextually similar words and compute the average adverbial attachment score."
"Similarly, the average adjectival attachment score is computed by replacing N 1 and N 2 by their contextually similar words."
Attachment scores are obtained using a linear combination of features of the 4-tuple.
"Finally, we combine the average attachment scores with the attachment score of N 2 attaching to the original V and the attachment score of N 2 attaching to the original N 1 ."
The proposed classification represents the attachment site that scored highest.
Recent work shows that it is generally sufficient to utilize lexical informati[REF_CITE].
One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions[REF_CITE].
"Training data was obtained by extracting all phrases of the form (V, N 1 , P, N 2 ) from a large parsed corpus."
Supervised methods later improved attachment accuracy.
"They experimented with both word features and word class features, their combination yielding 81.6% attachment accuracy."
"Later,[REF_CITE]achieved 84.5% accuracy by employing a backed-off model to smooth for unseen events."
They discovered that P is the most informative lexical item for attachment disambiguation and keeping low frequency events increases performance.
A non-statistical supervised approach[REF_CITE]yielded 81.8% accuracy using a transformation-based approach[REF_CITE]and incorporating word-class information.
They report that the top 20 transformations learned involved specific prepositions supporting Collins and Brooks’ claim that the preposition is the most important lexical item for resolving the attachment ambiguity.
The state of the art is a supervised algorithm that employs a semantically tagged corpus[REF_CITE].
Each word in a labelled corpus is sense-tagged using an unsupervised word-sense disambiguation algorithm with WordNet[REF_CITE].
Testing examples are classified using a decision tree induced from the training examples.
The current unsupervised state of the art achieves 81.9% attachment accuracy[REF_CITE].
"Using an extraction heuristic, unambiguous prepositional phrase attachments of the form (V, P, N 2 ) and (N 1 , P, N 2 ) are extracted from a large corpus."
Co- occurrence frequencies are then used to disambiguate examples with ambiguous attachments.
"The input to our algorithm includes a collocation database and a corpus-based thesaurus, both available on the Internet 2 ."
"Below, we briefly describe these resources."
"Given a word w in a dependency relationship (such as subject or object), the collocation database is used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies[REF_CITE]."
Figure 1 shows excerpts of the entries in the collocation database for the words eat and salad.
The database contains a total of 11 million unique dependency relationships.
"Using the collocation database,[REF_CITE]used an unsupervised method to construct a corpus-based thesaurus consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs."
"Given a word w, the thesaurus returns a set of similar words of w along with their similarity to w. For example, the 20 most similar words of eat and salad are shown in Table 1."
"We parsed a 125-million word newspaper corpus with Minipar 3 , a descendent of Principar[REF_CITE]."
Minipar outputs dependency trees[REF_CITE]from the input sentences.
"For example, the following sentence is decomposed into a dependency tree: subj pcomp obj pcomp det mod det det mod det"
A man in the park saw a dog with a telescope.
"Occasionally, the parser generates incorrect dependency trees."
"For example, in the above sentence, the prepositional phrase headed by with should attach to saw (as opposed to dog)."
Two separate sets of training data were then extracted from this corpus.
"Below, we briefly describe how we obtained these data sets."
"For each input sentence, Minipar outputs a single dependency tree."
"For a sentence containing one or more prepositions, we use a program to detect any alternative prepositional attachment sites."
"For example, in the above sentence, the program would detect that with could attach to saw."
"Using an iterative algorithm, we initially create a table of co-occurrence frequencies for 3-tuples of the form (V, P, N 2 ) and (N 1 , P, N 2 )."
"For each k possible attachment site of a preposition P, we increment the frequency of the corresponding [Footnote_3]-tuple by 1/k."
"For example, Table 2 shows the initial co-occurrence frequency table for the corresponding 3-tuples of the above sentence."
"In the following iterations of the algorithm, we update the frequency table as follows."
"For each k possible attachment site of a preposition P, we refine its attachment score using the formulas described in Section 4: VScore(V k , P k , N 2k ) and"
"NScore(N 1k , P k , N 2k )."
"For any tuple (W k , P k , N 2k ), where W k is either V k or N 2k , we update its frequency as:"
"Score ( W k ,P k ,N 2 ) fr ( Wk ,Pk ,N2k ) = ∑ Score ( W k i ,P i ,N 2i ) k i=1 where Score(W k , P k , N 2k ) ="
"VScore(W k , P k , N 2k ) if W k = V k ; otherwise Score(W k , P k , N 2k ) ="
"NScore(W k , P k , N 2k )."
"Suppose that after the initial frequency table is set NScore(man, in, park) = 1.23, VScore(saw, with, telescope) = 3.65, and NScore(dog, with, telescope) = 0.35."
"Then, the updated co-occurrence frequencies for (man, in, park) and (saw, with, telescope) are:"
Table 3 shows the updated frequency table after the first iteration of the algorithm.
"The resulting database contained 8,900,000 triples."
"As[REF_CITE], we constructed a training data set consisting of only unambiguous attachments of the form (V, P, N 2 ) and (N 1 , P, N 2 )."
We only extract a 3-tuple from a sentence when our program finds no alternative attachment site for its preposition.
Each extracted 3-tuple is assigned a frequency count of 1.
"For example, in the previous sentence, (man, in, park) is extracted since it contains only one attachment site; (dog, with, telescope) is not extracted since with has an alternative attachment site."
"The resulting database contained 4,400,000 triples."
"Essentially, several language learning algorithms (e.g. naïve Bayes estimation, back-off estimation, transformation-based learning) were successfully cast as learning linear separators in their feature space."
Roth modelled prepositional phrase attachment as linear combinations of features.
"The features consisted of all 15 possible sub-sequences of the 4-tuple (V, N 1 , P, N 2 ) shown in Table 4."
The asterix (*) in features represent wildcards.
Roth used supervised learning to adjust the weights of the features.
"In our experiments, we only considered features that contained P since the preposition is the most important lexical item[REF_CITE]."
"Furthermore, we omitted features that included both V and N 1 since their co-occurrence is independent of the attachment decision."
The resulting subset of features considered in our system is shown in bold in Table 4 (equivalent to assigning a weight of 0 or 1 to each feature).
"Let |head, rel, mod| represent the frequency, obtained from the training data, of the head occurring in the given relationship rel with the modifier."
"We then assign a score to each feature as follows: 1. (*, *, P, *) = log(|*, P, *| / |*, *, *|) 2. (V, *, P, N 2 ) = log(|V, P, N 2 | / |*, *, *|) 3. (*, N 1 , P, N 2 ) = log(|N 1 , P, N 2 | / |*, *, *|) 4. (V, *, P, *) = log(|V, P, *| / |V, *, *|) 5. (*, N 1 , P, *) = log(|N 1 , P, *| / |N 1 , *, *|) 6. (*, *, P, N 2 ) = log(|*, P, N 2 | / |*, *, N 2 |) 1, 2, and 3 are the prior probabilities of P, V P N 2 , and N 1 P N 2 respectively. 4, 5, and 6 represent conditional probabilities P(V, P | V), P(N 1 , P | N 1 ), and P(P N 2 | N 2 ) respectively."
"We estimate the adverbial and adjectival attachment scores, VScore(V, P, N 2 ) and NScore(N 1 , P, N 2 ), as a linear combination of these features:"
"VScore(V, P, N 2 ) = (*, *, P, *) + (V, *, P, N 2 ) + (V, *, P, *) + (*, *, P, N 2 )"
"NScore(N 1 , P, N 2 ) = (*, *, P, *) + (*, N 1 , P, N 2 ) + (*, N 1 , P, *) + (*, *, P, N 2 )"
"For example, the attachment scores for (eat, salad, with, fork) are VScore(eat, with, fork) = -3.47 and NScore(salad, with, fork) = -4.77."
The model correctly assigns a higher score to the adverbial attachment.
The contextually similar words of a word w are words similar to the intended meaning of w in its context.
"Below, we describe an algorithm for constructing contextually similar words and we present a method for approximating the attachment scores using these words."
"For our purposes, a context of w is simply a dependency relationship involving w."
"For example, a dependency relationship for saw in the example sentence of Section 3 is saw:obj:dog."
Figure 2 gives the data flow diagram for our algorithm for constructing the contextually similar words of w. We retrieve from the collocation database the words that occurred in the same dependency relationship as w. We refer to this set of words as the cohort of w for the dependency relationship.
Consider the words eat and salad in the context eat salad.
"The cohort of eat consists of verbs that appeared with object salad in Figure 1 (e.g. add, consume, cover, …) and the cohort of salad consists of nouns that appeared as object of eat in Figure 1 (e.g. almond, apple, bean, …)."
"Intersecting the set of similar words and the cohort then forms the set of contextually similar words of w. For example, Table 5 shows the contextually similar words of eat and salad in the context eat salad and the contextually similar words of fork in the contexts eat with fork and salad with fork."
The words in the first row are retrieved by intersecting the similar words of eat in Table 1 with the cohort of eat while the second row represents the intersection of the similar words of salad in Table 1 and the cohort of salad.
The third and fourth rows are determined in a similar manner.
"In the nonsensical context salad with fork (in row 4), no contextually similar words are found."
"While previous word sense disambiguation algorithms rely on a lexicon to provide sense inventories of words, the contextually similar words provide a way of distinguishing between different senses of words without committing to any particular sense inventory."
"Often, sparse data reduces our confidence in the attachment scores of Section 4."
"Using contextually similar words, we can approximate these scores."
"Given the tuple (V, N 1 , P, N 2 ), adverbial attachments are approximated as follows."
We first construct a list CS V containing the contextually similar words of V in context V:obj:N 1 and a list CS N2V containing the contextually similar words of N 2 in context V:P:N 2 (i.e. assuming adverbial attachment).
"For each verb v in CS V , we compute VScore(v, P, N 2 ) and set S V as the average of the largest k of these scores."
"Similarly, for each noun n in CS N2V , we compute VScore(V, P, n) and set S N2V as the average of the largest k of these scores."
"Then, the approximated adverbial attachment score, Vscore&apos;, is:"
"VScore&apos;(V, P, N 2 ) = max(S V , S N2V )"
We approximate the adjectival attachment score in a similar way.
"First, we construct a list CS N1 containing the contextually similar words of N 1 in context V:obj:N 1 and a list CS N2N1 containing the contextually similar words of N 2 in context N 1 :P:N 2 (i.e. assuming adjectival attachment)."
"Now, we compute S N1 as the average of the largest k of NScore(n, P, N 2 ) for each noun n in CS N1 and S N2N1 as the average of the largest k of NScore(N 1 , P, n) for each noun n in CS N2N1 ."
"Then, the approximated adjectival attachment score, NScore&apos;, is:"
"NScore&apos;(N 1 , P, N 2 ) = max(S N1 , S N2N1 )"
"For example, suppose we wish to approximate the attachment score for the 4-tuple (eat, salad, with, fork)."
"First, we retrieve the contextually similar words of eat and salad in context eat salad, and the contextually similar words of fork in contexts eat with fork and salad with fork as shown in Table 5."
Let k = 2.
Table 6 shows the calculation of S V and S N2V while the calculation of S N1 and S N2N1 is shown in Table 7.
Only the top k = 2 scores are shown in these tables.
"VScore&apos; (eat, with, fork) = max(S V , S N 2V ) = -2.92"
"NScore&apos; (salad, with, fork) = max(S N 1 , S N 2 N 1 ) = -4.87"
"Hence, the approximation correctly prefers the adverbial attachment to the adjectival attachment."
Figure 3 describes the prepositional phrase attachment algorithm.
"As in previous approaches, examples with P = of are always classified as adjectival attachments."
"Suppose we wish to approximate the attachment score for the 4-tuple (eat, salad, with, fork)."
"From the previous section, Step 1 returns average V = -2.92 and average N 1 = -[Footnote_4].87."
"From Section 4, Step 2 gives a V = -3.47 and a N1 = -[Footnote_4].77."
"In our training data, f V = 2.97 and f N1 = 0, thus Step 3 gives f = 0.914."
"In Step 4, we compute:"
S(V) = -3.42 and S(N 1 ) = -4.78
"Since S(V) &gt; S(N 1 ), the algorithm correctly classifies this example as an adverbial attachment."
"Given the 4-tuple (eat, salad, with, croutons), the algorithm returns S(V) = -4.31 and S(N 1 ) = -3.88."
"Hence, the algorithm correctly attaches the prepositional phrase to the noun salad."
"In this section, we describe our test data and the baseline for our experiments."
"Finally, we present our results."
The test data consists of 3097 examples derived from the manually annotated attachments in the Penn Treebank Wall Street Journal data[REF_CITE]4 .
"Each line in the test data consists of a 4-tuple and a target classification: V N 1 P N 2 target. 4- TUPLE VS CORE (mix, salad, with, fork) -2.60 (sprinkle, salad, with, fork) -3.24 S V -2.92 (eat, salad, with, spoon) -3.06 (eat, salad, with, finger) -3.50"
The data set contains several erroneous tuples and attachments.
"There are also improbable attachments such as (sing, birthday, to, you) with the target attachment birthday."
"Choosing the most common attachment site, N 1 , yields an accuracy of 58.96%."
"However, we achieve 70.39% accuracy by classifying each occurrence of P = of as N 1 , and V otherwise."
"Human accuracy, given the full context of a sentence, is 93.2% and drops to 88.2% when given only tuples of the form (V, N 1 , P, N 2 )[REF_CITE]."
"Assuming that human accuracy is the upper bound for automatic methods, we expect our accuracy to be bounded above by 88.2% and below by 70.39%."
We used the 3097-example testing corpus described in Section 7.1.
Table 8 presents the precision and recall of our algorithm and Table 9 presents a performance comparison between our system and previous supervised and unsupervised approaches using the same test data.
We describe the different classifiers below: cl base : the baseline described in Section 7.2 cl R1 : uses a maximum entropy model
Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm.
We reconstructed the two earlier unsupervised classifiers cl HR and cl R2 .
The originally reported accuracy for cl R2 is within the 95% confidence interval of our reconstruction.
Our reconstruction of cl HR achieved slightly higher accuracy than the original report.
Our classifier used a mixture of the two training data sets described in Section 3.
"However, since the confidence intervals overlap, we cannot claim with certainty that the contextually similar words improve performance."
"In Section 7.1, we mentioned some testing examples contained N 1 = the or N 2 = the."
"For supervised algorithms, the is represented in the training set as any other noun."
"Consequently, these algorithms collect training data for the and performance is not affected."
"However, unsupervised methods break down on such examples."
The algorithms presented in this paper advance the state of the art for unsupervised approaches to prepositional phrase attachment and draws near the performance of supervised methods.
"Currently, we are exploring different functions for combining contextually similar word approximations with the attachment scores."
A promising approach considers the mutual information between the prepositional relationship of candidate attachments and N 2 .
"As the mutual information decreases, our confidence in the attachment score decreases and the contextually similar word approximation is weighted higher."
"Also, improving the construction algorithm for contextually similar words would possibly improve the accuracy of the system."
One approach first clusters the similar words.
"Then, dependency relationships are used to select the most representative clusters as the contextually similar words."
The assumption is that more representative similar words produce better approximations.
This paper presents a novel statistical model for automatic identification of English baseNP.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
"Unlike the other approaches where the two steps are separated, we integrate them into a unified statistical framework."
Our model also integrates lexical information.
"Finally, Viterbi algorithm is applied to make global search in the entire sentence, allowing us to obtain linear complexity for the entire process."
"Compared with other methods using the same testing set, our approach achieves 92.3% in precision and 93.2% in recall."
The result is comparable with or better than the previously reported results.
"Finding simple and non-recursive base Noun Phrase (baseNP) is an important subtask for many natural language processing applications, such as partial parsing, information retrieval and machine translation."
"A baseNP is a simple noun phrase that does not contain other noun phrase recursively, for example, the elements within [...] in the following example are baseNPs, where NNS, IN VBG etc are part-of-speech tags [as defined in M.[REF_CITE]]. [Measures/NNS] of/IN [manufacturing/VBG activity/NN] fell/VBD more/RBR than/IN [the/DT overall/JJ measures/NNS] ./."
Figure 1: An example sentence with baseNP brackets
A number of researchers have dealt with the problem of baseNP identification ([REF_CITE]; Justeson &amp;[REF_CITE]).
Recently some researchers have made experiments with the same test corpus extracted from the 20 th section of the Penn Treebank Wall Street Journal (Penn Treebank).
"Ramshaw &amp;[REF_CITE]applied transform-based error-driven algorithm[REF_CITE]to learn a set of transformation rules, and using those rules to locally updates the bracket positions."
"Argamon, Dagan &amp;[REF_CITE]introduced a memory-based sequences learning method, the training examples are stored and generalization is performed at application time by comparing subsequence of the new text to positive and negative evidence."
Cardie &amp; Pierce (1998 1999) devised error driven pruning approach trained on Penn Treebank.
"It extracts baseNP rules from the training corpus and prune some bad baseNP by incremental training, and then apply the pruned rules to identify baseNP through maximum length matching (or dynamic program algorithm)."
Most of the prior work treats POS tagging and baseNP identification as two separate procedures.
"However, uncertainty is involved in both steps."
Using the result of the first step as if they are certain will lead to more errors in the second step.
A better approach is to consider the two steps together such that the final output takes the uncertainty in both steps together.
"The approaches proposed by Ramshaw &amp; Markus and Cardie&amp;Pierce are deterministic and local, while Argamon, Dagan &amp; Krymolowski consider the problem globally and assigned a score to each possible baseNP structures."
"However, they did not consider any lexical information."
"This paper presents a novel statistical approach to baseNP identification, which considers both steps together within a unified statistical framework."
It also takes lexical information into account.
"In addition, in order to make the best choice for the entire sentence, Viterbi algorithm is applied."
Our tests with the Penn Treebank showed that our integrated approach achieves 92.3% in precision and 93.2% in recall.
The result is comparable or better that the current state of the art.
"In the following sections, we will describe the detail for the algorithm, parameter estimation and search algorithms in section 2."
The experiment results are given in section 3.
In section 4 we make further analysis and comparison.
In the final section we give some conclusions.
"In this section, we will describe the two-pass statistical model, parameters training and Viterbi algorithm for the search of the best sequences of POS tagging and baseNP identification."
"Before describing our algorithm, we introduce some notations we will use"
Let us express an input sentence E as a word sequence and a sequence of POS respectively as follows:
E = w 1 w 2 ... w n−1 w n T = t 1 t 2 ... t n−1 t n
"Where n is the number of words in the sentence, t i is the POS tag of the word w i ."
"Given E, the result of the baseNP identification is assumed to be a sequence, in which some words are grouped into baseNP as follows ... w i−1 [w i w i+1 ...w j ] w j+1 ..."
"The corresponding tag sequence is as follows: (a) B=.. t. i−1 [t i t i+1 ...t j ] t j+1 ... =..t. i−1 b i,j t j+1 ...=n 1 n 2 ...n m In which b i,j corresponds to the tag sequence of a baseNP: [t i t i+1 ... t j ] . b i,j may also be thought of as a baseNP rule."
Therefore B is a sequence of both POS tags and baseNP rules.
"Thus 1≤ m ≤ n, n i ∈ (POS tag set ∪ baseNP rules set), This is the first expression of a sentence with baseNP annotated."
"Sometime, we also use the following equivalent form: (b)"
"Q=...t( i−1 ,bm i−1 ) (t i ,bm i ) (t i+1 ,bm i+1 )...(t j ,bm j ) (t j+1 ,bm j+1 )...= q 1 q 2 ... q n"
Where each POS tag t i is associated with its positional information bm i with respect to baseNPs.
"The positional information is one of {F, I, E,O, S} ."
"F, E and I mean respectively that the word is the left boundary, right boundary of a baseNP, or at another position inside a baseNP."
O means that the word is outside a baseNP.
S marks a single word baseNP.
This second expression is similar to that used in [[REF_CITE]].
"For example, the two expressions of the example given in Figure 1 are as follows: (a) B= [NNS] IN [VBG NN] VBD RBR"
IN [DT JJ NNS] (b) Q=(NNS S) (IN O) (VBG F) (NN E) (VBD O) (RBR O) (IN O) (DT F) (JJ I) (NNS E) (.
O) 2.2 An ‘integrated’ two-pass procedure
The principle of our approach is as follows.
The most probable baseNP sequence B * may be expressed generally as follows:
B * = argmax(p(B| E)) B
"We separate the whole procedure into two passes, i.e.:"
"B * ≈ argmax(P(T | E)×P(B | T,E)) (1) B"
"In order to reduce the search space and computational complexity, we only consider the N best POS tagging of E, i.e."
"T(N −best) = argmax(P(T | E)) (2) T=T 1 ,...,T N"
"Therefore, we have:"
"B * ≈ argmax(P(T | E)×P(B|T,E)) (3) B,T=T 1 ,...,T N"
"Correspondingly, the algorithm is composed of two steps: determining the N-best POS tagging using Equation (2)."
And then determining the best baseNP sequence from those POS sequences using Equation (3).
"One can see that the two steps are integrated together, rather that separated as in the other approaches."
Let us now examine the two steps more closely.
The goal of the algorithm in the 1 st pass is to search for the N-best POS-sequences within the n P(E | T) ≈ ∏
P(w | t ) i i (5) i=1
"We then use a trigram model as an approximation of P(T ) , i.e.: n P(T) ≈ ∏"
"P(t | t ,t ) i i−2 i−1 (6) i=1"
Finally we have
T(N − best) = argmax(P(T | E))
"T=T 1 ,...,T N n = argmax( ∏ P(w |t )×"
"P(t |t ,t )) i i−2 i−1 (7) i i T=T 1 ,...,T N i=1"
"In Viterbi algorithm of N best search, P(w i | t i ) is called lexical generation (or output) probability, and P(t i | t i−2 ,t i−1 ) is called transition probability in Hidden Markov Model."
"As mentioned before, the goal of the 2 nd pass is to search the best baseNP-sequence given the N-best POS-sequences."
"Considering E , T and B as random variables, according to Bayes’ Rule, we have"
"P(B | T, E) ="
P(B | T ) ×
"P(E | B,T )"
P(E | T )
"Since P(B | T) = P(T | B) × P(B) we have, P(T )"
"P(B | T, E) ="
"P(E | B,T)× P(T | B)× P(B) (8) P(E | T)× P(T)"
"Because we search for the best baseNP sequence for each possible POS-sequence of the given sentence E, so P(E | T ) × P(T ) ="
"P(E ∩ T ) = const , Furthermore from the definition of B, during search space (POS lattice)."
"According to Bayes’ Rule, we have"
P(T | E) =
P(E | T)× P(T) P(E)
"Since P(E) does not affect the maximizing procedure of P(T | E) , equation (2) becomes"
"T(N−best)=argmax(P(T| E))=argmax(P(E|T)×P(T)) (4) T=T 1 ,...,T N T=T 1 ,...T, N"
We now assume that the words in E are independent.
"Thus each search procedure, we have n P(T|B)= ∏"
"P(t ,...t, |b )=1 ."
"Therefore, equation i j i,j i=1 (3) becomes B * = argmax(P(T | E)× P(B|T, E)) B,T=T 1 ,...,T N = arg max(P(T | E) × P(E | B,T) × P(B)) (9) B,T=T 1 ,...,T N using the independence assumption, we have n P(E | B,T) ≈ ∏"
"P(w | t ,bm ) i i (10) i i=1"
"With trigram approximation of P(B) , we have: m P(B) ≈ ∏"
"P(n | n ,n ) i i−2 i−1 (11) i=1"
"Finally, we obtain n B * =argmax(P(T|E)× ∏ P(w |bm,t )× ∏"
"P(n |n ,n )) i i−2 i−1i i i B,T=T 1 ,..T N i=1 i=1,m 12"
"To summarize, In the first step, Viterbi N-best searching algorithm is applied in the POS tagging procedure, It determines a path probability f t for each POS sequence calculated as follows: f t = ∏ p(w | t )× p(t | t ,t ) . i i−2 i−1 i i i=1,n"
"In the second step, for each possible POS tagging result, Viterbi algorithm is applied again to search for the best baseNP sequence."
"Every baseNP sequence found in this pass is also asssociated with a path probability n f b = ∏ p(w | t ,bm )× ∏ p(n | n ,n ) . i−2 i−1 i i i i i=1 i=1,m"
"The integrated probability of a baseNP sequence is determined by f t α × f b , where α is a normalization coefficient ( α = 2.4 in our experiments)."
"When we determine the best baseNP sequence for the given sentence E , we also determine the best POS sequence of E , which corresponds to the best baseNP of E ."
Now let us illustrate the whole process through an example: “stock was down 9.1 points yesterday morning.”.
"In the first pass, one of the"
N-best POS tagging result of the sentence is: T = NN VBD RB CD NNS NN NN.
"For this POS sequence, the 2 nd pass will try to determine the baseNPs as shown in Figure 2."
"The details of the path in the dash line are given in Figure 3, Its probability calculated in the second pass is as follows ( Φ is pseudo variable):"
"In this work, the training and testing data were derived from the 25 sections of Penn Treebank."
"We divided the whole Penn Treebank data into two sections, one for training and the other for testing."
"As required in our statistical model, we have to calculate the following four probabilities: (1) P(t i | t i−2 ,t i−1 ) , (2)"
"P(w i | t i ) , (3) P(n i | n i−2 n i−1 ) and (4)"
"P(w i | t i ,bm i ) ."
The first and the third parameters are trigrams of T and B respectively.
The second and the fourth are lexical generation probabilities.
"Probabilities (1) and (2) can be calculated from POS tagged data with following formulae: count(t i−2 t i−1 t i ) (13) p(t i | t i−2 ,t i−1 ) = ∑ count(t i−2 t i−1 t j ) j p(w i | t i ) = count(w i with tag t i ) (14) count(t i )"
"As each sentence in the training set has both POS tags and baseNP boundary tags, it can be converted to the two sequences as B (a) and Q (b) described in the last section."
"Using these sequences, parameters (3) and (4) can be calculated, The calculation formulas are similar with equations (13) and (14) respectively."
"Before training trigram model (3), all possible baseNP rules should be extracted from the training corpus."
"For instance, the following three sequences are among the baseNP rules extracted."
"There are more than 6,000 baseNP rules in the Penn Treebank."
"When training trigram model (3), we treat those baseNP rules in two ways. (1) Each baseNP rule is assigned a unique identifier (UID)."
This means that the algorithm considers the corresponding structure of each baseNP rule. (2) All of those rules are assigned to the same identifier (SID).
"In this case, those rules are grouped into the same class."
"Nevertheless, the identifiers of baseNP rules are still different from the identifiers assigned to POS tags."
"We used the approach of Katz (Katz.1987) for parameter smoothing, and build a trigram model to predict the probabilities of parameter (1) and (3)."
"In the case that unknown words are encountered during baseNP identification, we calculate parameter (2) and (4) in the following way:"
"In the experiments, the training and testing sets are derived from the 25 sections of Wall Street Journal distributed with the Penn Treebank II, and the definition of baseNP is the same as Ramshaw’s, Table 1 summarizes the average performance on both baseNP tagging and POS tagging, each section of the whole Penn Treebank was used as the testing data and the other 24 sections as the training data, in this way count(bm i ,t i ) p(w i | bm i ,t i ) = (15) max(count(bm j ,t i )) 2 j count(t i ) p(w i | t i ) = (16) max(count(t j )) 2 j"
"Here, bm j indicates all possible baseNP labels attached to t i , and t j is a POS tag guessed for the unknown word w i ."
We designed five experiments as shown in Table 1. “UID” and “SID” mean respectively that an identifier is assigned to each baseNP rule or the same identifier is assigned to all the baseNP rules. “+1” and “+4” denote the number of beat POS sequences retained in the first step.
And “UID+R” means the POS tagging result of the given sentence is totally correct for the 2nd step.
This provides an ideal upper bound for the system.
"The reason why we choose N=4 for the N-best POS tagging can be explained in Figure 4, which shows how the precision of POS tagging changes with the number N. we have done the cross validation experiments 25 times."
"Figure 5 -7 summarize the outcomes of our statistical model on various size of the training data, x-coordinate denotes the size of the training set, where &quot;1&quot; indicates that the training set is from section 0-8 th of Penn Treebank, &quot;2&quot; corresponds to the corpus that add additional three sections 9-11 th into &quot;1&quot; and so on."
In this way the size of the training data becomes larger and larger.
In those cases the testing data is always section 20 (which is excluded from the training data).
"From Figure 7, we learned that the POS tagging and baseNP identification are influenced each other."
We conducted two experiments to study whether the POS tagging process can make use of baseNP information.
"One is UID+4, in which the precision of POS tagging dropped slightly with respect to the standard POS tagging with Trigram Viterbi search."
"In the second experiment SID+4, the precision of POS tagging has increase slightly."
This result shows that POS tagging can benefit from baseNP information.
"Whether or not the baseNP information can improve the precision of POS tagging in our approach is determined by the identifier assignment of the baseNP rules when training trigram model of P(n i | n i−2 , n i−1 ) ."
"In the future, we will further study optimal baseNP rules clustering to further improve the performances of both baseNP identification and POS tagging."
"To our knowledge, three other approaches to baseNP identification have been evaluated using Penn Treebank-Ramshaw &amp; Marcus’s transformation-based chunker, Argamon et al.’s MBSL, and Cardie’s Treebank_lex in Table 2, we give a comparison of our method with other these three."
"In this experiment, we use the testing data prepared by Ramshaw (available[URL_CITE]the training data is selected from the 24 sections of Penn Treebank (excluding the section 20)."
We can see that our method achieves better result than the others
Table 3 summarizes some interesting aspects of our approach and the three other methods.
"Our statistical model unifies baseNP identification and POS tagging through tracing N-best sequences of POS tagging in the pass of baseNP recognition, while other methods use POS tagging as a pre-processing procedure."
"From Table 1, if we reviewed 4 best output of POS tagging, rather that only one, the F-measure of baseNP identification is improved from 93.02 % to 93.07%."
"After considering baseNP information, the error ratio of POS tagging is reduced by 2.4% (comparing SID+4 with SID+1)."
The transformation-based method (R&amp;M 95) identifies baseNP within a local windows of sentence by matching transformation rules.
"Similarly to MBSL, the 2 nd pass of our algorithm traces all possible baseNP brackets, and makes global decision through Viterbi searching."
"On the other hand, unlike MSBL we take lexical information into account."
The experiments show that lexical information is very helpful to improve both precision and recall of baseNP recognition.
If we neglect the probability of n ∏
"P(w | t ,bm ) in the 2 pass of our model, nd i i i i=1 the precision/recall ratios are reduced to 90.0/92.4% from 92.3/93.2%."
"Cardie’s approach to Treebank rule pruning may be regarded as the special case of our statistical model, since the maximum-matching algorithm of baseNP rules is only a simplified processing version of our statistical model."
"Compared with this rule pruning method, all baseNP rules are kept in our model."
"Therefore in principle we have less likelihood of failing to recognize baseNP types As to the complexity of algorithm, our approach is determined by the Viterbi algorithm approach, or O(n) , linear with the length."
This paper presented a unified statistical model to identify baseNP in English text.
"Compared with other methods, our approach has following characteristics: (1) baseNP identification is implemented in two related stages: N-best POS taggings are first determined, then baseNPs are identified given the N best POS-sequences."
"Unlike other approaches that use POS tagging as pre-processing, our approach is not dependant on perfect POS-tagging, Moreover, we can apply baseNP information to further increase the precision of POS tagging can be improved."
These experiments triggered an interesting future research challenge: how to cluster certain baseNP rules into certain identifiers so as to improve the precision of both baseNP and POS tagging.
This is one of our further research topics. (2) Our statistical model makes use of more lexical information than other approaches.
Every word in the sentence is taken into account during baseNP identification. (3) Viterbi algorithm is applied to make global search at the sentence level.
Experiment with the same testing data used by the other methods showed that the precision is 92.3% and the recall is 93.2%.
"To our knowledge, these results are comparable with or better than all previously reported results."
"´² ³Fµ·¶K¸ ¹±&amp;¸ »¼¸F»ºn¶[½d¾¿¶¼¹ÁÀ+ÂjÃ-¸ ³F½ ¶µÄ±º Ã-¸ µÅ»¿µ¹ÇÆ-+À Â±Ã«¸ ¹±»µ·¶Â±½KÈ&amp;É¾ÊÉfÂM¹Ç¸FË ¸F»¿Âj¹jÀ ³F¶MÌÍÂ±»ÏÎFÄ±Âj¸Fµ½FÐÑ¹ÑÈ ¹j¶[º ½FÂjÒF½ ¸F³ » ¿¹j¶[À ³dÒ ½FÓ±³ÕÒFÃÖ¹Ç½A»¿ÒFÆº,É1»¿µÅ¾¹±½ Î j¹ À¾µÄ±º×ÆÅºn¹Ç»¿½Fµ½FÐØÒ ¶µ½FÐ]µ½d»¿¹jÀ¾µÄ±ºÙ»ºn¹ÇÆÅË ¾¿ÄÞµÅ¹ÇÃ«»¿µ·¹Þº^¾³ÕÒFµÂ±½&amp;ÃÖ¶Ö¹±½ÚÂ±½M¹Ç½F½F¹jÀ¾¿Â±µÅÄj¾¿¹Ç¾µÂ±½ÆÅºn¹Ç»¿%ÛÏ½FµÅ½ÜÕÐßÄ±¹Ç»¹ÇÆÝ½FµÅ½ÕÂÞÄjÄ±[¶"
ËÆ ¾¿µÅÐd¹Þ¾ºnÎY¹Ç½ ÎÒF½
"ÎF»¿ÆÅáÕµ½FÐâÀ+Âd¶Ê¾4Ã«ÂÆ·¶4ÌÍÂj» »Âd¶¶[ËÃ«ÂÎF¹±ÆÝÃÖ¹±À ³Fµ½FºÆÅºn¹Ç»¿½Fµ½FÐãÃ«¸ ¹±»µÅË ¶Â±¹Ç»¿º4¸F»¿½d¾ºn¹Ç½ Îäº+å¸FÆÂ±Î%Ûæ1ºn¶[ÒFÆÅ¾¿¶ ¶³FÂÞÉ ¾³ ¹ÇµÅ&quot;¶ Ã«Âj»º??ç +À µ½d¾-¹±½ Ã«Â±»¿º ¶Ò ¶ÊÌÍÒ ÆSÈÕá?¶ ¹ÇÆ Ã«ºn¹±¶ÒF»ºn¶ ¾¿Â&quot;¾¿»¿¹±µÅ½ä¹ á¶Ê.Ã Ò ¶µÅ½ ¹jÀ¾¿µÅÄjÆ¹±»½Fµ½FÐä¹Ç½F½FÂ±¾¿¹Ç¾µÂ±½¶ » ¹Þ¾¿³F»´¾¿³ ¹Ç³ ¹±½ ÎÕËè+À » ¹ÞÌé¾ºnÎä»ÒFÆº4É1»¿µ¾¿µÅ½FÐÖ¹Ç¾ ¹bÀ+Â±Ã«¸ ¹±»¿, Ã«¹±½MÆ·¹ÇÈSÂ±»zµÅ½Ë Äj[¾Ã«½dÛ ê ëjì °CF_1ZÝíÝGS°FO[_ ì î ½F¹Çº]» À ³FÂÇÌÚ»¿¶1¾¿É1³Fº³]Âz¸ Éf»µÂ±ÃÖ»¿Óz¹Ç»¿µáï^½ ½ ¸F»¿ÉÏÂ±È Æ·ÆÅ¹Ç½FÃ«Ðj¶ÙÒ ¹Ç¾Ðj&amp;³ ¹Þ¶1Âj»,ðÝ½ ñ#òóÉÙÎ»Â±º ËË"
"Grammatical relationships (GRs) form an important level of natu-ral language processing, but ent sets of GRs are useful for ent purposes."
"Therefore, one may of-ten only have time to obtain a small training corpus with the desired GR annotations."
"To boost the perfor-mance from using such a small train-ing corpus on a transformation rule learner, we use existing systems that  related types of annotations."
"Grammatical relationships (GRs), which in-clude arguments (e.g., subject and object) and , form an important level of natural language processing."
Examples of GRs in the sentence
"Today, my dog pushed the ball on the . are pushed having the subject my dog , the object the ball and the time  To-day , and the ball having the location  on (the ) ."
The resulting annotation is my dog − subj → pushed on − mod-loc → the ball ∗ This paper reports on work performed at the etc. GRs are the objects of study in rela-tional grammar[REF_CITE].
"In the SPARKLE project[REF_CITE], GRs form the top layer of a three layer syntax scheme."
"Many systems (e.g., the KERNEL system[REF_CITE]) use GRs as an intermediate form when determining the se-mantics of syntactically parsed text."
GRs are often stored in structures similar to the F-structures of lexical-functional grammar[REF_CITE].
A complication is that  sets of GRs are useful for  purposes.
"For exam-ple,[REF_CITE]is interested in seman-tic interpretation, and needs to  between time, location and other ."
"The SPARKLE project[REF_CITE], on the other hand, does not  be-tween these types of ."
"As has been mentioned by John Carroll (personal commu-nication), combining  types together is  for information retrieval."
"Also, having less  of the  can make it easier to  them[REF_CITE]."
"Furthermore, unless the desired set of GRs matches the set already annotated in some large training corpus, [Footnote_1] one will have to either manually write rules to  the GRs, as done[REF_CITE], or anno-tate a new training corpus for the desired set."
1 One example is a memory-based GR[REF_CITE]that uses the GRs annotated in the Penn Treebank[REF_CITE].
"Manually writing rules is expensive, as is an-notating a large corpus."
"Often, one may only have the resources to produce a small annotated training set, and many of the less common features of the set&apos;s domain may not appear at all in that set."
In contrast are existing systems that perform well (probably due to a large annotated train-ing set or a set of carefully hand-crafted rules) on related (but ) annotation stan-dards.
"Such systems will cover many more domain features, but because the annotation standards are slightly , some of those features will be annotated in a  way than in the small training and test set."
A way to try to combine the  advan-tages of these small training data sets and ex-isting systems which produce related annota-tions is to use a sequence of two systems.
"We  use an existing annotation system which can handle many of the less common features, i.e., those which do not appear in the small training set."
We then train a second system with that same small training set to take the output of the  system and correct for the  in annotations.
This approach was used[REF_CITE]for word segmentation.
Both these works deal with just one (word boundary) or two (start and end parse bracket) annotation label types and the same label types are used in both the existing annotation system/training set and the  (small) training set.
"In compari-son, our work handles many annotation la-bel types, and the translation from the types used in the existing annotation system to the types in the small training set tends to be both more complicated and most easily determined by empirical means."
"Also, the type of baseline score being improved upon is ."
"Our work adds an existing system to improve the rules learned, while[REF_CITE]adds rules to improve an existing system&apos;s performance."
We use this related system/small training set combination to improve the performance of the transformation-based error-driven learner described[REF_CITE].
"So far, this learner has started with a blank initial labeling of the GRs."
This paper describes experiments where we replace this blank initial labeling with the output from an existing GR  that is good at a somewhat  set of GR annotations.
"With each of the two existing GR  that we use, we obtained improved results, with the improvement being more noticeable when the training set is smaller."
We also  that the existing GR  are quite uneven on how they improve the re-sults.
"They each tend to concentrate on im-proving the recovery of a few kinds of rela-tions, leaving most of the other kinds alone."
We use this tendency to further boost the learner&apos;s performance by using a merger of these existing GR  output as the initial labeling.
We now improve the performance of the[REF_CITE]transformation rule learner on a small annotated training set by using an existing system to provide initial GR annotations.
"This experiment is repeated on two  existing systems, which are reported[REF_CITE]and[REF_CITE], respectively."
Both of these systems  a somewhat  set of GR annotations than the one learned by the[REF_CITE]sys-tem.
"For example, the[REF_CITE]system ignores verb complements of verbs and is designed to look for relationships to verbs and not GRs that exist between nouns, etc."
This system also handles relative clauses .
"For example, in , who organized ... , this system is trained to indicate that  is the subject of  , while the[REF_CITE]system is trained to indicate that  is the subject of  ."
"As for the[REF_CITE]system, among other things, it does not distinguish between sub-types of  such as time, location and possessive."
"Also, both systems handle copu-las (usually using the verb  )  than[REF_CITE]."
"As described[REF_CITE], the trans-formation rule learner starts with a p-o-s tagged corpus that has been  into noun chunks, etc."
The starting state also in-cludes imperfect estimates of pp-attachments and a blank set of initial GR annotations.
"In these experiments, this blank initial set is changed to be a translated version of the annotations produced by an existing system."
This is how the existing system transmits what it found to the rule learner.
The set-up for this experiment is shown in  1.
The four components with + signs are taken out when one wants the transformation rule learner to start with a blank set of initial GR annotations.
The two arcs in that  with a * indicate where the translations occur.
These transla-tions of the annotations produced by the ex-isting system are basically just an attempt to map each type of annotation that it produces to the most likely type of corresponding an-notation used in the[REF_CITE]sys-tem.
"For example, in our experiments, the[REF_CITE]system uses the anno-tation np-sbj to indicate a subject, while the[REF_CITE]system uses the annota-tion subj ."
We create the mapping by ex-amining the training set to be given to the[REF_CITE]system.
"For each type of relation e i output by the existing system when given the training set text, we look at what relation types (which t k &apos;s) co-occur with e i in the training set."
We look at the t k &apos;s with the highest number of co-occurrences with that e i .
"If that t k is unique (no ties for the highest number of co-occurrences) and translating e i to that t k generates at least as many correct annotations in the training set as false alarms, then make that translation."
"Otherwise, trans-late e i to no relation."
This latter translation is not uncommon.
"For example, in one run of our experiments, 9% of the relation instances in the training set were so translated, in an-other run, 46% of the instances were so trans-lated."
Some relations in the[REF_CITE]system are between three or four elements.
These relations are each  translated into a set of two element sub-relations before the examination process above is performed.
"Even before applying the rules, the trans-lations  many of the desired annotations."
"However, the rules can considerably improve what is found."
"For example, in two of our early experiments, the translations by them-selves produced F-scores (explained below) of about 40% to 50%."
"After the learned rules were applied, those F-scores increased to about 70%."
An alternative to performing translations is to use the un translated initial annotations as an additional type of input to the rule sys-tem.
"This alternative, which we have yet to try, has the advantage of  into the transformation-based error-driven paradigm[REF_CITE]more cleanly than having a translation stage."
"However, this ad-ditional type of input will also further slow-down an already slow rule-learning module."
"For our experiment, we use the same 1151 word (748 GR) test set used[REF_CITE], but for a training set, we use only a subset of the 3299 word training set used[REF_CITE]."
This subset con-tains 1391 (71%) of the 1963 GR instances in the original training set.
The overall results for the test set are
"Smaller Training Set, Overall Results R P F[REF_CITE](63.9%) 77.2% 69.9% 7.7%[REF_CITE](62.3%) 78.1% 69.3% 5.8%[REF_CITE](59.9%) 77.1% 67.4% where row IaB is the result of using the rules learned when the[REF_CITE]sys-tem&apos;s translated GR annotations are used as the Initial Annotations, row IaC is the similar result with the[REF_CITE]system, and row NI is the result of using the rules learned when No Initial GR an-notations are used (the rule learner as run[REF_CITE])."
R(ecall) is the num-ber (and percentage) of the keys that are recalled.
P(recision) is the number of cor- rectly recalled keys divided by the num-ber of GRs the system claims to exist.
F(-score) is the harmonic mean of recall ( r ) and precision ( p ) percentages.
It equals 2pr/(p + r) .
ER stands for Error Reduc-tion.
"It indicates how much adding the ini-tial annotations reduced the missing F-score, where the missing F-score is 100% − F. ER= 100% × (F IA − F NI )/(100 % − F NI ), where F NI is the F-score for the NI row, and F IA is the F-score for using the Initial Annotations of interest."
"Here, the  in recall and F-score between NI and either IaB or IaC (but not between IaB and IaC) are statistically sig-."
The  in precision is not. [Footnote_2]
"2 When comparing in this paper, the statistical of the higher score being bet-ter than the lower score is tested with a one-sided test. deemed statistically are at the 5% level. deemed non-statistically are not at the 10% level. For recall, we use a sign test for matched-pairs ([REF_CITE]Sec. 15.5). For precision and F-score, a randomization test ([REF_CITE]Sec. 5.3) is used."
"In these results, most of the modest F-score gain came from increasing recall."
One may note that the error reductions here are smaller than[REF_CITE]&apos;s error reduc-tions.
"Besides being for  tasks (word segmentation versus GRs), the reductions are also computed using a  type of base-line."
"In this paper, the baseline is the performance of the rules learned without  using an existing system."
"If we were to use the same baseline[REF_CITE], our baseline would be an F of 37.5%[REF_CITE].6% for IaC."
"This would result in a much higher[REF_CITE]% and 36%, respectively."
We now repeat our experiment with the full 1963 GR instance training set.
"These re-sults indicate that as a small training set gets larger, the overall results get better and the initial annotations help less in improving the overall results."
So the initial annotations are more helpful with smaller training sets.
The overall results on the test set are
"Full Training Set, Overall Results R P F[REF_CITE](65.1%) 79.7% 71.7% 6.3%[REF_CITE](65.0%) 76.5% 70.3% 1.7%[REF_CITE](63.6%) 77.3% 69.8%"
"The  in recall, etc. between IaB and NI are now small enough to be not statisti-cally ."
"The  between IaC and NI are statistically , [Footnote_3] but the  in both the absolute F-score (1.9% versus 2.5% with the smaller training set) and ER (6.3% versus 7.7%) has decreased."
"3 The recall is , being sig- at the 10% level."
The overall result of using an existing system is a modest increase in F-score.
"However, this increase is quite unevenly distributed, with a few relation(s) having a large increase, and most relations not having much of a change.   existing systems seem to have ent relations where most of the increase oc-curs."
"As an example, take the results of using the[REF_CITE]system on the 1391 GR instance training set."
"Many GRs, like pos-sessive  , are not  by the added initial annotations."
"Some GRs, like location  , do slightly better (as measured by the F-score) with the added initial annota-tions, but some, like subject , do better with-out."
"With GRs like subject , some  between the initial and desired annotations may be too subtle for the[REF_CITE]system to adjust for."
"Or those  may be just due to chance, as the result  in those GRs are not statistically ."
"The GRs with statistically  result  are the time and  [Footnote_4]  s, where adding the initial annotations helps."
"4 that do not fall into any of the subtypes used, such as time, location, possessive, etc. Examples of unused subtypes are purpose and modality."
The time  [Footnote_5] results are quite :
5[REF_CITE]instances in the test set key.
"Smaller Training Set, Time  s R P F[REF_CITE](64.4%) 80.6% 71.6% 53%[REF_CITE](31.1%) 56.0% 40.0%"
The  in the number recalled (15) for this GR accounts for nearly the entire ence in the overall recall results (18).
"The re-call, precision and F-score  are all statistically ."
"Similarly, when using the[REF_CITE]system on this training set, most GRs are not , while others do slightly better."
"The only GR with a sta-tistically  result  is object , where again adding the initial annotations helps:"
"Smaller Training Set, Object Relations R P F[REF_CITE](79.5%) 79.5% 79.5% 17%[REF_CITE](71.9%) 78.9% 75.2%"
The  in the number recalled (19) for this GR again accounts for most of the dif- ference in the overall recall results (30).
The recall and F-score  are statistically .
The precision  is not.
"As one changes from the smaller 1391 GR instance training set to the larger 1963 GR instance training set, these F-score improve-ments become smaller."
"When using the[REF_CITE]system, the improve-ment in the   is now no longer statistically ."
"However, the time  F-score improvement stays statisti-cally :"
"Full Training Set, Time  s R P F[REF_CITE](64.4%) 74.4% 69.0% 46%[REF_CITE](33.3%) 57.7% 42.3%"
"When using the[REF_CITE]system, the object F-score improvement stays statisti-cally :"
"Full Training Set, Object Relations R P F[REF_CITE](77.9%) 85.1% 81.3% 16%[REF_CITE](75.5%) 80.3% 77.8%"
So the initial annotations from  ex-isting systems tend to each concentrate on improving the performance of  GR types.
"From this observation, one may wonder about combining the annotations from these  systems in order to increase the per-formance on all the GR types  by those  existing systems."
Various works (van[REF_CITE]) on combining  sys-tems exist.
These works use one or both of two types of schemes.
One is to have the  systems simply vote.
"However, this does not really make use of the fact that dif-ferent systems are better at handling ent GR types."
The other approach uses a combiner that takes the systems&apos; output as input and may perform such actions as de-termining which system to use under which circumstance.
"Unfortunately, this approach needs extra training data to train such a com-biner."
"Such data may be more useful when used instead as additional training data for the individual methods that one is consider-ing to combine, especially when the systems being combined were originally given a small amount of training data."
"To avoid the disadvantages of these existing schemes, we came up with a third method."
We combine the existing related systems by taking a union of their translated annota-tions as the new initial GR annotation for our system.
We rerun rule learning on the smaller (1391 GR instance) training set with a Union of the[REF_CITE]and[REF_CITE]systems&apos; translated GR annotations.
The overall results for the test set are (shown in row IaU) where the other rows are as shown in Sec-tion 2.2.
"Compared to the F-score with using[REF_CITE](IaC) , the IaU F-score is  statistically cantly better (11%  level)."
The IaU F-score is statistically  bet-ter than the F-scores with either using[REF_CITE](IaB) or not using any initial annotations (NI).
"As expected, most (42 of 48) of the overall increase in recall going from NI to IaU comes from increasing the recall of the object , time  and other  relations, the re-lations that IaC and IaB concentrate on."
The ER for object is 11% and for time  is 56%.
"When this combining approach is repeated the full 1963 GR instance training set, the overall results for the test set are"
"Full Training Set, Overall Results R P F ER"
"Compared to the smaller training set results, the  between IaU and IaC here is smaller for both the absolute F-score (0.3% versus 1.1%) and ER (1.0% versus 3.3%)."
"In fact, the F-score  is small enough to not be statistically ."
"Given the pre-vious results for IaC and IaB as a small train-ing set gets larger, this is not surprising."
"GRs are important, but  sets of GRs are useful for  purposes and  systems are better at  certain types of GRs."
"Here, we have been looking at ways of improving automatic GR  when one has only a small amount of data with the desired GR annotations."
"In this paper, we improve the performance of the[REF_CITE]"
GR transformation rule learner by using existing systems to  related sets of GRs.
The out-put of these systems is used to supply ini-tial sets of annotations for the rule learner.
We achieve modest gains with the existing systems tried.
"When one examines the re-sults, one notices that the gains tend to be uneven, with a few GR types having large gains, and the rest not being  much."
The  systems concentrate on improv-ing  GR types.
We leverage this ten-dency to make a further modest improvement in the overall results by providing the rule learner with the merged output of these ex-isting systems.
We have yet to try other ways of combining the output of existing systems that do not require extra training data.
"One possibility is the example-based combiner in Brill and Wu (1998, Sec. 3.2). [Footnote_6]"
"6 Based on the paper, we were unsure if extra train-ing data is needed for this combiner. One of the au-thors, Wu, has told us that extra data is not needed."
"Furthermore,  additional existing systems to add to the combination may further improve the re-sults."
± ?¥K¡ «£¢h¥¤y¦¤y¯k?²£¯³$n¯µµQ´|¯ ­O³¤ ± ± ³¯# S·h¡ ¥¸r´?³Q«½¨Q$.¯°­¾«¤y¯k§³Q¤¹§O¶Z§*º k³¿¯µµ?|­¯³?|? ± ¥«y|­ ?¦ ±Å4Æ± ³¯.S·h¡?\¨ÇsÁ¡ £¢|º   ± ´r£¡É¤Ê²S¤ ±É± ¦³?²£¯Uº ³¤ ± ?@¤¡e¥¬³J­O¬µ|¶ ? ¯¥¤«y¦Q¬§ Å  µÐÒÑ Î Ð  Ñ  ÑSÚ ± ¤ZµQ¬uÊV¤¡¾³¤ ± º ­Ok³¯S·h± ?± ? ¯°­*º µ?$ÍÞÝ Ñ Î#ÐÍßÇÙ Õ Ð ÍÔÓ°Î©«¤y¯.#\³Q¤ ± ³¯ §± ¤hµQ¬?$ÍVÎÐÒÑ ±Î Ð³¯S·ZÑ§ ÎrÙ¥?¤y³ÑgÚ§ ?|?¯°­ÉÊX¤h²£¦§©§O¤y¥¬¥¶k³©¾á à á àâ ã ä ²S¤¦§n¯§n¨E¬¯§  Ñ Î Ð Í ßÇÙeÕ
Ð ÍÞÓÎ©«¤y¯ Å å æ L  HyF5IKçK yè #F L  £¢h¥¤y¦?§\ ±    ±  ? §Ob¤ÊE³Q¤ ± º   ­O¤ÂÍÔÝ Ñ Î Ð Í §Oð²S¤y³Zº
"We have developed a system that generates evaluative arguments that are tailored to the user, properly arranged and concise."
We have also developed an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users.
This paper presents the results of a formal experiment we have performed in our framework to verify the influence of argument conciseness on argument effectiveness
"Empirical methods are critical to gauge the scalability and robustness of proposed approaches, to assess progress and to stimulate new research questions."
"In the field of natural language generation, empirical evaluation has only recently become a top research priority (Dale,[REF_CITE])."
"Some empirical work has been done to evaluate models for generating descriptions of objects and processes from a knowledge base[REF_CITE], text summaries of quantitative data[REF_CITE], descriptions of plans (Young to appear) and concise causal arguments (McConachy,[REF_CITE])."
"However, little attention has been paid to the evaluation of systems generating evaluative arguments, communicative acts that attempt to affect the addressee’s attitudes (i.e. evaluative tendencies typically phrased in terms of like and dislike or favor and disfavor)."
"The ability to generate evaluative arguments is critical in an increasing number of online systems that serve as personal assistants, advisors, or shopping assistants 1 ."
"For instance, a shopping assistant may need to compare two similar products and argue why its current user should like one more than the other."
"In the remainder of the paper, we first describe a computational framework for generating evaluative arguments at different levels of conciseness."
"Then, we present an evaluation framework in which the effectiveness of evaluative arguments can be measured with real users."
"Next, we describe the design of an experiment we ran within the framework to verify the influence of argument conciseness on argument effectiveness."
We conclude with a discussion of the experiment’s results.
"Often an argument cannot mention all the available evidence, usually for the sake of brevity."
"According to argumentation theory, the selection of what evidence to mention in an argument should be based on a measure of the evidence strength of support (or opposition) to the main claim of the argument[REF_CITE]."
"Furthermore, argumentation theory suggests that for evaluative arguments the measure of evidence strength should be based on a model of the intended reader’s values and preferences."
"Following argumentation theory, we have designed an argumentative strategy for generating evaluative arguments that are properly arranged and concise[REF_CITE]."
"In our strategy, we assume that the reader’s values and preferences are represented as an additive multiattribute value function (AMVF), a conceptualization based on multiattribute utility theory (MAUT)[REF_CITE]."
This allows us to adopt and extend a measure of evidence strength proposed in previous work on explaining decision theoretic advice based on an AMVF[REF_CITE].
"The argumentation strategy has Formally, an AMVF predicts the value v(e) ofbeen implemented as part of a complete argument generator."
"Other modules of the generator include a microplanner, which performs aggregation, pronominalization and makes decisions about cue phrases and scalar adjectives, along with a sentence realizer, which extends previous work on realizing evaluative statements[REF_CITE]."
An AMVF is a model of a person’s values and preferences with respect to entities in a certain class.
"It comprises a value tree and a set of component value functions, one for each primitive attribute of the entity."
"A value tree is a decomposition of the value of an entity into a hierarchy of aspects of the entity 2 , in which the leaves correspond to the entity primitive attributes (see Figure 1 for a simple value tree in the real estate domain)."
"The arcs of the tree are weighted to represent the importance of the value of an objective in contributing to the value of its parent in the tree (e.g., in Figure 1 location is more than twice as important as size in determining the value of a house)."
Note that the sum of the weights at each level is equal to 1.
"A component value function for an attribute expresses the preferability of each attribute value as a number in the [0,1] interval."
"For instance, in Figure 1 neighborhood n[Footnote_2] has preferability 0.3, and a distance-from-park of 1 mile has preferability (1 - (1/5 * 1))=0.8). an entity e as follows: v(e) = v(x 1 ,…,x n ) ="
"2 In decision theory these aspects are called objectives. For consistency with previous work, we will follow this terminology in the remainder of the paper."
"Σ w i v i (x i ), where - (x 1 ,…,x n ) is the vector of attribute values for an entity e - ∀attribute i, v i is the component value function, which maps the least preferable x i to 0, the most preferable to 1, and the other x i to values in [0,1] - w i is the weight for attribute i, with 0≤ w i ≤1 and Σ w i =1 - w i is equal to the product of all the weights from the root of the value tree to the attribute i"
A function v o (e) can also be defined for each objective.
"When applied to an entity, this function returns the value of the entity with respect to that objective."
"For instance, assuming the value tree shown in Figure 1, we have: v Location (e) = = (0.4∗v Neighborhood (e)) + (0.6∗v Dist−from−park (e))"
"Thus, given someone’s AMVF, it is possible to compute how valuable an entity is to that individual."
"Furthermore, it is possible to compute how valuable any objective (i.e., any aspect of that entity) is for that person."
"All of these values are expressed as a number in the interval [0,1]."
"Given an AMVF for a user applied to an entity (e.g., a house), it is possible to define a precise measure of an objective strength in determining the evaluation of its parent objective for that entity."
"This measure is proportional to two factors: (A) the weight of the objective (which is by itself a measure of importance), (B) a factor that increases equally for high and low values of the objective, because an objective can be important either because it is liked a lot or because it is disliked a lot."
"We call this measure s-compellingness and provide the following definition: s-compellingness(o, e, refo) = (A) ∗ (B) = = w(o,refo) ∗ max[[v o (e)]; [1 – v o (e)]], where − o is an objective, e is an entity, refo is an ancestor of o in the value tree − w(o,refo) is the product of the weights of all the links from o to refo − v o is the component value function for leaf objectives (i.e., attributes), and it is the recursive evaluation over children(o) for nonleaf objectives"
"Given a measure of an objective&apos;s strength, a predicate indicating whether an objective should be included in an argument (i.e., worth mentioning) can be defined as follows: s-notably-compelling?(o,opop,e, refo) ≡ s-compellingness(o, e, refo)&gt;µ x +kσ x , where − o, e, and refo are defined as in the previous Def; opop is an objective population (e.g., siblings(o)), and opop&gt;2 − p ∈ opop; x ∈ X = s-compellingness(p, e, refo) − µ x is the mean of X, σ x is the standard deviation and k is a user-defined constant"
Similar measures for the comparison of two entities are defined and extensively discussed[REF_CITE].
"In the definition of s-notably-compelling?, the constant k determines the lower bound of s-compellingness for an objective to be included in an argument."
"As shown in Figure 2, for k=0 only objectives with s-compellingness greater than the average s-compellingness in a population are included in the argument (4 in the sample population)."
"For higher positive values of k less objectives are included (only 2, when k=1), and the opposite happens for negative values (8 objectives are included, when k=-1)."
"Therefore, by setting the constant k to different values, it is possible to control in a principled way how many objectives (i.e., pieces of evidence) are included in an argument, thus controlling the degree of conciseness of the generated arguments."
Figure 3 clearly illustrates this point by showing seven arguments generated by our argument generator in the real-estate domain.
"These arguments are about the same house, tailored to the same subject, for k ranging from 1 to –1."
"In order to evaluate different aspects of the argument generator, we have developed an evaluation framework based on the task efficacy evaluation method."
"This method allows the experimenter to evaluate a generation model by measuring the effects of its output on user’s behaviors, beliefs and attitudes in the context of a task."
"Aiming at general results, we chose a rather basic and frequent task that has been extensively studied in decision analysis: the selection of a subset of preferred objects (e.g., houses) out of a set of possible alternatives."
"In the evaluation framework that we have developed, the user performs this task by using a computer environment (shown in Figure 5) that supports interactive data exploration and analysis (IDEA) (Roth,[REF_CITE])."
The IDEA environment provides the user with a set of powerful visualization and direct manipulation techniques that facilitate the user’s autonomous exploration of the set of alternatives and the selection of the preferred alternatives.
"Let’s examine now how an argument generator can be evaluated in the context of the selection task, by going through the architecture of the evaluation framework."
Figure 4 shows the architecture of the evaluation framework.
"The framework consists of three main sub-systems: the IDEA system, a User Model Refiner and the Argument Generator."
"The framework assumes that a model of the user’s preferences (an AMVF) has been previously acquired from the user, to assure a reliable initial model."
"At the onset, the user is assigned the task to select from the dataset the four most preferred alternatives and to place them in a Hot List (see Figure 5, upper right corner) ordered by preference."
The IDEA system supports the user in this task (Figure 4 (1)).
"As the interaction unfolds, all user actions are monitored and collected in the User’s Action History (Figure 4 (2a))."
"Whenever the user feels that the task is accomplished, the ordered list of preferred alternatives is saved as her Preliminary Decision (Figure 4 (2b))."
"After that, this list, the User’s Action History and the initial Model of User’s Preferences are analysed by the User Model Refiner (Figure 4 (3)) to produce a Refined Model of the User’s Preferences (Figure 4 (4))."
"At this point, the stage is set for argument generation."
"Given the Refined Model of the User’s Preferences, the Argument Generator produces an evaluative argument tailored to the model (Figure 4 (5-6)), which is presented to the user by the IDEA system (Figure 4 (7)).The argument goal is to introduce a new alternative (not included in the dataset initially presented to the user) and to persuade the user that the alternative is worth being considered."
The new alternative is designed on the fly to be preferable for the user given her preference model.
All the information about the new alternative is also presented graphically.
"Once the argument is presented, the user may (a) decide immediately to introduce the new alternative in her Hot List, or (b) decide to further explore the dataset, possibly making changes to the Hot List adding the new instance to the Hot List, or (c) do nothing."
"Figure 5 shows the display at the end of the interaction, when the user, after reading the argument, has decided to introduce the new alternative in the Hot List first position (Figure 5, top right)."
"Whenever the user decides to stop exploring and is satisfied with her final selections, measures related to argument’s effectiveness can be assessed (Figure 4 (8))."
"These measures are obtained either from the record of the user interaction with the system or from user self-reports in a final questionnaire (see Figure 6 for an example of self-report) and include: - Measures of behavioral intentions and attitude change: (a) whether or not the user adopts the new proposed alternative, (b) in which position in the Hot List she places it and (c) how much she likes the new alternative and the other objects in the Hot List. - A measure of the user’s confidence that she has selected the best for her in the set of alternatives. - A measure of argument effectiveness derived by explicitly questioning the user at the end of the interaction about the rationale for her decisi[REF_CITE]."
"This can provide valuable information on what aspects of the argument were more influential (i.e., better understood and accepted by the user). - An additional measure of argument effectiveness is to explicitly ask the user at the end of the interaction to judge the argument with respect to several dimensions of quality, such as content, organization, writing style and convincigness."
"However, evaluations based on judgements along these dimensions are clearly weaker than evaluations measuring actual behavioural and attitudinal changes[REF_CITE]."
"To summarize, the evaluation framework just described supports users in performing a realistic task at their own pace by interacting with an IDEA system."
"In the context of this task, an evaluative argument is generated and measurements related to its effectiveness can be performed."
We now discuss an experiment that we have performed within the evaluation framework
The argument generator has been designed to facilitate testing the effectiveness of different aspects of the generation process.
"The experimenter can easily control whether the generator tailors the argument to the current user, the degree of conciseness of the argument (by varying k as explained in Section 2.3), and what microplanning tasks the generator performs."
"In the experiment described here, we focused on studying the influence of argument conciseness on argument effectiveness."
A parallel experiment about the influence of tailoring is described elsewhere.
We followed a between-subjects design with three experimental conditions: No-Argument - subjects are simply informed that a new house came on the market.
Tailored-Concise - subjects are presented with an evaluation of the new house tailored to their preferences and at a level of conciseness that we hypothesize to be optimal.
"To start our investigation, we assume that an effective argument (in our domain) should contain slightly more than half of the available evidence."
"By running the generator with different values for k on the user models of the pilot subjects, we found that this corresponds to k=-0.3."
"In fact, with k=-0.3 the arguments contained on average 10 pieces of evidence out of the 19 available."
"Tailored-Verbose - subjects are presented with an evaluation of the new house tailored to their preferences, but at a level of conciseness that we hypothesize to be too low (k=-1, which corresponds on average, in our analysis of the pilot subjects, to 16 pieces of evidence out of the possible 19)."
"In the three conditions, all the information about the new house is also presented graphically, so that no information is hidden from the subject."
Our hypotheses on the outcomes of the experiment are summarized in Figure 7.
We expect arguments generated for the Tailored-Concise condition to be more effective than arguments generated for the Tailored-Verbose condition.
"We also expect the Tailored-Concise condition to be somewhat better than the No-Argument condition, but to a lesser extent, because subjects, in the absence of any argument, may spend more time further exploring the dataset, thus reaching a more informed and balanced decision."
"Finally, we do not have strong hypotheses on comparisons of argument effectiveness between the No-Argument and Tailored-Verbose conditions."
The experiment is organized in two phases.
"In the first phase, the subject fills out a questionnaire on the Web."
The questionnaire implements a method form decision theory to acquire an AMVF model of the subject’s preferences[REF_CITE].
"In the second phase of the experiment, to control for possible confounding variables (including subject’s argumentativeness[REF_CITE], need for cognition (Cacioppo,[REF_CITE]), intelligence and self-esteem), the subject is randomly assigned to one of the three conditions."
"Then, the subject interacts with the evaluation framework and at the end of the interaction measures of the argument effectiveness are collected, as described in Section [Footnote_3].1."
"3 If the subject does not adopt the new house, she is asked to express her satisfaction with the new house in an additional self-report."
"After running the experiment with 8 pilot subjects to refine and improve the experimental procedure, we ran a formal experiment involving 30 subjects, 10 in each experimental condition."
"According to literature on persuasion, the most important measures of arguments effectiveness are the ones of behavioral intentions and attitude change."
"As explained in Section 3.1, in our framework such measures include (a) whether or not the user adopts the new proposed alternative, (b) in which position in the Hot List she places it, (c) how much she likes the proposed new alternative and the other objects in the Hot List."
"Measures (a) and (b) are obtained from the record of the user interaction with the system, whereas measures in (c) are obtained from user self-reports."
A closer analysis of the above measures indicates that the measures in (c) are simply a more precise version of measures (a) and (b).
"In fact, not only they assess the same information as measures (a) and (b), namely a preference ranking among the new alternative and the objects in the Hot List, but they also offer two additional critical advantages: (i) Self-reports allow a subject to express differences in satisfaction more precisely than by ranking."
"For instance, in the self-report shown in Figure 8, the subject was able to specify that the first house in the Hot List was only one space (unit of satisfaction) better then the house preceding it in the ranking, while the third house was two spaces better than the house preceding it. (ii) Self-reports do not force subjects to express a total order between the houses."
"For instance, in Figure 8 the subject was allowed to express that the second and the third house in the Hot List were equally good for her."
"Furthermore, measures of satisfaction obtained through self-reports can be combined in a single, statistically sound measure that concisely express how much the subject liked the new house with respect to the other houses in the Hot List."
"This measure is the z-score of the subject’s self-reported satisfaction with the new house, with respect to the self-reported satisfaction with the houses in the Hot List."
"A z-score is a normalized distance in standard deviation units of a measure x i from the mean of a population X. Formally: x i ∈ X; z-score( x i ,X) = [x i - µ (X)] / σ (X) For instance, the satisfaction z-score for the new instance, given the sample self-reports shown in Figure 8, would be: [7 - µ ({8,7,7,5})] / σ ({8,7,7,5}) = 0.2 The satisfaction z-score precisely and concisely integrates all the measures of behavioral intentions and attitude change."
We have used satisfaction z-scores as our primary measure of argument effectiveness.
"As shown in Figure 9, the satisfaction z-scores obtained in the experiment confirmed our hypotheses."
Arguments generated for the Tailored-Concise condition were significantly more effective than arguments generated for Tailored-Verbose condition.
"The Tailored-Concise condition was also significantly better than the No-Argument condition, but to a lesser extent."
Logs of the interactions suggest that this happened because subjects in the No-Argument condition spent significantly more time further exploring the dataset.
"Finally, there was no significant difference in argument effectiveness between the No-Argument and Tailored-Verbose conditions."
"With respect to the other measures of argument effectiveness mentioned in Section 3.1, we have not found any significant differences among the experimental conditions."
"Argumentation theory indicates that effective arguments should be concise, presenting only pertinent and cogent information."
"However, argumentation theory does not tell us what is the most effective degree of conciseness."
"As a preliminary attempt to answer this question for evaluative arguments, we have compared in a formal experiment the effectiveness of arguments generated by our argument generator at two different levels of conciseness."
The experiment results show that arguments generated at the more concise level are significantly better than arguments generated at the more verbose level.
"However, further experiments are needed to determine what is the optimal level of conciseness."
We examine the bene ts of using multi-ple agents to produce explanations.
"In particular, we identify the ability to con-struct prior plans as a key issue con-straining the e ectiveness of a single-agent approach."
We describe an imple-mented system that uses multiple agents to tackle a problem for which prior plan-ning is particularly impractical: real-time soccer commentary.
Our commen-tary system demonstrates a number of the advantages of decomposing an expla-nation task among several agents.
"Most notably, it shows how individual agents can bene t from following di erent dis-course strategies."
"Further, it illustrates that discourse issues such as controlling interruption, abbreviation, and main-taining consistency can also be decom-posed: rather than considering them at the single level of one linear explana-tion they can also be tackled separately within each individual agent."
"We evalu-ate our system&apos;s output, and show that it closely compares to the speaking pat-terns of a human commentary team."
This paper deals with the issue of high-level vs low-level explanation strategies.
"How should an explanation nd a balance between describing the overall, high-level properties of the discourse sub-ject, and the low-level, procedural details?"
"In par-ticular, we look at the diÆculties presented by do-mains that change in real-time."
"For such domains, the balance between reacting to the domain events as they occur and maintaining the overall, high-level consistency is critical."
We argue that it is bene cial to decompose the overall explanation task so that it is carried out by more than one agent.
"This allows a single agent to deal with the tracking of the low-level develop-ments in the domain, leaving the others to con-centrate on the high-level picture."
"The task of each individual agent is simpli ed, since they only have to maintain consistency for a single discourse strategy."
"Further, discourse issues such as control-ling interruption, abbreviation, and maintaining consistency can also be decomposed: rather than considering them at the single level of one linear explanation they can be tackled separately within each individual agent and then also at the level of inter-agent cooperation."
"We look at real-world examples of explanation tasks that are carried out by multiple agents, and also give a more detailed protocol analysis of one of these examples: World Cup soccer commen-tary by TV announcers."
We then describe an actual implementation of an explanation system that produces multi-agent commentary in real-time for a game of simulated soccer.
"In this sys-tem, each of the agents selects their discourse con-tent on the basis of importance scores attached to events in the domain."
The interaction between the agents is controlled to maximise the impor-tance score of the uttered comments.
"Although our work focuses on real-time do-mains such as soccer, our discussion in x 2 puts our contribution in a wider context and iden-ti es a number of the general bene ts of using multiple agents for explanation tasks."
We chose the game of soccer for our research primarily be-cause it is a multi-agent game in which various events happen simultaneously on the eld.
"Thus, it is an excellent domain to study real-time con-tent selection among many heterogeneous facts."
"A second reason for choosing soccer is that de-tailed, high-quality logs of simulated soccer games are available on a real-time basis from Soccer Server, the oÆcial soccer simulation system for the `RoboCup&apos; Robotic Soccer World Cup initia-tive[REF_CITE]."
"In this paper, we use the term explanation in its broadest possible sense, covering the entire spec-trum from planned lectures to commentating on sports events."
"Any such explanation task is af-fected by many considerations, including the level of knowledge assumed of the listeners and the available explanation time."
"However, the issue we mainly concentrate on here has not previously re-ceived signi cant attention: the bene ts of split-ting an explanation task between multiple agents."
The general task of producing explanations with multiple agents has not been studied in depth in the literature.
"Even for the `naturally&apos; multi-agent task of soccer commentary, the systems de-scribed in the recent AI Magazine special issue on RoboCup ( et al., 2000) are all single-agent."
"However, one general issue that has been studied at the level of single agents is the trade-o be-tween low-level and high-level explanations."
"For example, in tutoring systems[REF_CITE]has described a system that handles real-time interac-tions with a user by separating the control of the content planning and dialogue planning."
We believe that the key issue constraining the use of high-level and low-level explanations in a discourse is the ability to construct prior plans .
"For example, researchers in the eld of discourse analysis, ( e.g. ,[REF_CITE]) have found that relatively formal types of di-alogues follow a regular hierarchical structure."
"When it is possible to nd these kinds of a priori plans for a discourse to follow, approaches such as those cited above for tutoring are very e ective."
"However, if prior plans are hard to specify, a sin-gle agent may simply nd it becomes overloaded."
"Typically there will be two con icting goals: deal with and explain each individual (unplanned) do-main event as it occurs, or build up and explain a more abstract picture that conveys the overall nature of the explanation topic."
"Thus, for any changing domain in which it is hard to plan the overall discourse, it can be bene-cial to divide the explanation task between mul- tiple agents."
"Especially for real-time domains, the primary bene t of decomposing the explanation task in this way is that it allows each agent to use a di erent discourse strategy to explain dif-ferent aspects of the domain (typically, high-level or low-level)."
"However, we can see from Figure 1 that even some activities that are highly planned are sometimes carried out by multiple agents."
"For example, business presentations are often carried out by a team of people, each of which is an expert in some particular area."
"Clearly, there are other bene ts that come from decomposing the explana-tion task between more than one agent."
We can give a partial list of these here:
Agents may start with di erent abilities.
"For example, in a panel session, one panellist may be an expert on Etruscan vases, while another may be an expert on Byzantian art."
"It can take time to observe high-level pat-terns in a domain, and to explain them co-herently."
Having a dedicated agent for com-menting on the low-level changes increases the chance that higher-level agents have a chance to carry out analysis.
A team of agents can converse together.
"In particular, they can make explanations to each other instead of directly explaining things to the listeners."
This can be a more comfortable psychological position for the lis-tener to accept new information.
"The simple label of \expert&quot; adds weight to the words of a speaker, as shown convincingly by the research[REF_CITE]."
The use of multiple agents actually gives a chance to describe individual agents as \ex-perts&quot; on speci c topics.
Even a single agent speaking in isolation could describe itself as an expert on various topics.
"However,[REF_CITE]also show that self-praise has far less impact than the same praise from another source."
"Rather than describing themselves as experts, a multi-agent framework allows agents to de-scribe the other agents as experts."
"To illustrate the di erent roles that can be taken by multiple agents in an explanation task, we carried out a simple protocol analysis of an ex-ample from the far left of our scale of Figure 1: soccer commentary."
We analysed the video of the NHK coverage of the rst half 1998 World Cup nal.
This commentary was carried out by a team of two people who we call the `announcer&apos; and the `expert&apos;.
The gures in Table 1 demonstrate that there are clear di er-ences between the roles assumed by this commen-tary team.
"Although both use some background knowledge to ll out their portions of the commen-tary, the announcer mostly comments on low-level events, whilst the expert mostly gives higher-level, state-based information."
"Further, we can see that the announcer asked questions of the expert with a high frequency."
"Overall, there is a clear indication that one agent follows the low-level events and that the other follows the high-level nature of the game."
"Accordingly, their discourse strategies are also dif-ferent: the announcer tends to speak in shorter phrases, whereas the expert produces longer anal-yses of any given subject."
"The commentary team collaborates so that the consistency between high-level, low-level, and background comments is bal-anced within the content spoken by each individ-ual, and also within the overall commentary."
"As a rst step towards a multi-agent explanation system based on the above observations, the fol-lowing sections describe how we implemented a commentary system for a game of simulated soc-cer."
Our experience with this system re ected the discussion above in that we found it was very diÆcult to consistently manage all the possible discourse topics within a single-agent framework.
"When changing to a multi-agent system, however, we found that a small number of simple rules for inter-agent interaction produced a far more man-ageable system."
We also found that the system was behaviourally very similar to the protocol of Table 1.
Figure 2 shows the basic architecture of our soc-cer commentator system.
"As we mentioned in the Introduction, this system is designed to produce live commentary for games played on RoboCup&apos;s Soccer Server."
"Since the Soccer Server was orig- inally designed as a testbed for multi-agent sys-tems[REF_CITE], we call our commenta-tor Mike (\Multi-agent Interactions Knowledge-ably Explained&quot;)."
"Typically, Mike is used to add atmosphere to games played in the RoboCup tour-naments, so we assume that the people listening to Mike can also see the game being described."
"The Soccer Server provides a real-time game log of a very high quality, sending information on the positions of the players and the ball to a moni-toring program every 100msec."
"Speci cally, this information consists of 1) player location and ori-entation, 2) ball location, and 3) game score and play modes (throw ins, goal kicks, etc )."
"This information is placed in Mike &apos;s shared memory, where it is processed by a number of `Soccer Analyser&apos; modules that analyse higher-level features of a game."
"These features include statistics on player positions, and also `bigrams&apos; of ball play chains represented as rst order Markov chains."
The Voronoi analyser uses Voronoi dia-grams to assess game features such as defensive ar-eas.
Note that we do not consider the Soccer Anal-ysers to be `agents&apos;; they are simply processes that manipulate the information in the shared mem-ory.
"The only true `agents&apos; in the system are the Announcer and the Analyser, which communicate both with each other and with the audience."
All information in Mike &apos;s shared memory is represented in the form of commentary fragments that we call propositions .
Each proposition con-sists of a tag and some attributes.
"For example, a pass from player No.5 to No.11 is represented[REF_CITE], where Pass is the tag, and the numbers 5 and 11 are the attributes."
Mike uses around 80 di erent tags categorised in two ways: as being local or global and as being state-based or event-based.
Table 2 shows some examples of categorised proposition tags.
The operation of the Announcer and the Anal-yser agents is described in detail in the following section.
"Basically, they select propositions from the shared memory (based on their `importance scores&apos;) and process them with inference rules to produce higher-level chains of explanations."
"The discourse control techniques of interruption, rep-etition, abbreviation, and silence are used to con-trol both the dialogue strategies of each individual agent and also the interaction between them."
"To produce variety in the commentary, each possible proposition is associated with several pos-sible commentary templates (output can be in En-glish or Japanese)."
Figure 3 shows the overall repertoire of Mike &apos;s comments.
"The actual spo-ken commentary is realised with o -the-shelf text-to-speech software (Fujitsu&apos;s Japanese Synthesiser for Japanese, and DecTalk for English)."
"In this section, we describe how Mike uses impor-tance scores, real-time inferencing, and discourse control strategies to implement | and control the interaction between | agents with di ering expla- nation strategies."
To form a single coherent commentary with multiple agents we extended the single-agent framework[REF_CITE].
"The ba-sic principle of this framework is that given a set of scores that capture the information transmit-ted by making any utterance, the most e ective dialogue is the one that maximises the total score of all the propositions that are verbalised."
We therefore created two agents with di erent strate-gies for content scheduling.
"One agent acts as an announcer, following the low-level events on the eld."
"This agent&apos;s strategy is biased to allow fre-quent topic change and although it uses inference rules to look for connections between propositions in the shared memory, it only uses short chains of inference."
"On the other hand, the second agent acts as an `expert analyst&apos;, and is predominantly state based."
"The expert&apos;s strategy is biased to have more consistency, and to apply longer chains of inference rules than the announcer."
"In Mike , importance scores are designed to cap-ture the amount of information that any given proposition will transmit to an audience."
"They are not xed values, but are computed from scratch at every game step (100msec)."
"The importance score of each proposition depends on three factors: 1) the elapsed time since the proposition was gener-ated, 2) for event-based propositions, a compar-ison of the place associated with the proposition and the current location of the ball, and 3) the frequency that the proposition has already been stated."
"To keep the number of comments in the shared memory to a manageable number they are simply limited in number, with the oldest entries being removed as new propositions are added."
"Mike &apos;s commentary propositions are the results of large amounts of real-time data processing, but are typically low-level."
A commentary based solely on these propositions would be rather de-tailed and disconnected.
"Thus, to analyse the play more deeply, Mike gives the commentary agents access to a set of forward-chaining rules that describe the possible relationships between the propositions."
"In total, there are 145 of these rules, divided into the two classes of logical con-sequences and second order relations."
We give a representative example from each class here:
Logical consequence: (PassSuccessRate player percentage) (PassPattern player Goal) ! (active player)
Second order relation: (PassSuccessRate player percentage) (PlayerOnVoronoiLine player) ! (Reason @1 @2)
The basic premise of the announcer&apos;s dialogue strategy is to follow the play by repeatedly choos-ing the proposition with the highest importance score.
"Before stating this proposition, however, the announcer checks any applicable inference rules in a top down manner, in an attempt to produce higher-level commentary fragments and background related information."
"In contrast to this, the expert agent has a library of themes ( e.g. , pass statistics, formation, stamina) between which it chooses based on the propositions selected by the announcer so far."
It then uses inference rules to try to construct a series of high-level inferences related to the theme.
The expert applies rules until it succeeds in constructing a single coherent piece of structured commentary.
When it is the agent&apos;s turn to speak it can then send this com-mentary to the TTS software.
Consider a passage of commentary where the an-nouncer is speaking and a proposition with a much larger importance score than the one be-ing uttered appears in the shared memory.
"If this occurs, the total importance score may become larger if the announcer immediately interrupts the current utterance and switches to the new one."
"As an example, the left of Figure 4 shows (solid line) the change of the importance score with time when an interruption takes place (the dotted line represents the importance score without interrup-tion)."
"The left part of the solid line is lower than the dotted, because we assume that the rst utter-ance conveys less of its importance score when it is not completely uttered."
"However, the right part of the solid line is higher than the dotted line, be-cause the importance of the second utterance will be lower by the time it is uttered without inter-rupting the commentary."
"Note that after selecting a proposition to be uttered, its importance score is assumed to decrease with time (as indicated in the gure, the decrease is computed dynamically and will be di erent for each proposition, and of-ten not even linear)."
The decision of whether or not to interrupt is based on a comparison of the area between the solid or dotted lines and the hor-izontal axis.
"Similarly, it may happen that when the two most important propositions in shared memory are of similar importance, the amount of com-municated information can best be maximised by quickly uttering the most important proposition and then moving on to the second before it loses importance due to some development of the game situation."
This is illustrated in the second graph of Figure 4.
"Here, the left hand side of the solid line is lower than that of the dotted because an abbreviated utterance (which might not be gram-matically correct, or whose context might not be fully given) transmits less information than a more complete utterance."
"But since the second propo-sition can be uttered before losing its importance score, the right hand part of the solid line is higher than that of the dotted."
"As before, the bene ts or otherwise of this modi cation should be decided by comparing the two areas made by the solid and the dotted line with the horizontal axis."
We originally designed these techniques just to improve the ability of the announcer agent to follow the play.
"With two commentary agents, however, both interruption and abbreviation can be adapted to control inter-agent switching."
"In Mike , the default operation is for the announcer to keep talking while the ball is in the nal third of the eld, or while there are important proposi-tions to utter."
"When the announcer has nothing to say, the expert agent can speak or both agents can remain silent."
"If the expert agent chooses to speak, it may happen that an important event on the eld makes the announcer wants to speak again."
We model both interruption and abbre-viation as multi-agent versions of the graphs of Figure 4: the agent speaking the rst utterance is the expert and the agent speaking the second is the announcer.
We use two further discourse control techniques in Mike : repetition and silence.
Repetition is depicted in Figure 5.
Sometimes it can happen that the remaining un-uttered propositions in the shared memory have much smaller scores than any of those that have already been selected.
"In this case, we allow individual agents to repeat things that they have previously said."
"Also, we allow them to repeat things that the other agent has said, to increase the e ectiveness of the dia-logue between them."
"Finally, we also model si-lence by adding bonuses to the importance scores of the propositions uttered by the commentators."
"Speci cally, we add a bonus to the scores of propo-sitions uttered directly before a period where both commentators are silent (the longer that a com-mentary continues uninterrupted, the higher the silence bonus)."
This models the bene t of giving listeners time to actually digest the commentary.
"Also, a period of silence contributes a bonus to the importance scores of the immediately follow-ing propositions."
This models the increased em-phasis of pausing before an important statement.
To improve the smoothness of the transfer of the commentary between the two agents we devised a small number of simple communication templates .
The phrases contained in these templates are spo-ken by the agents whenever Mike realises that the commentary is switching between them.
"For the purposes of keeping the two agents distinct, the expert agent is referred to by the announcer as E-Mike (\Expert Mike&quot;)."
"To pass the commentary to the Expert, the Announcer can use a number of phrases such as \ E-Mike , over to you&quot;, \Any impressions, E-Mike ?&quot;, or just \ E-Mike ?&quot;."
The announcer can also pass over control by simply stopping speaking.
"If the commentary switches from Announcer to Expert with a question, the Expert will start with \Yes...&quot; or \Well...&quot;."
The communication templates for passing the commentary in the other direction (Expert to An-nouncer) are shown in Table 3.
"To help listeners distinguish the dialogue between the Announcer and Expert better, we also use a female voice for one agent and a male voice for the other."
"Mike is robust enough for us to have used it to produce live commentary at RoboCup events, and to be distributed on the Internet (it has been downloaded by groups in Australia and Hungary and used for public demonstrations)."
A short ex-ample of Mike &apos;s output is shown in Figure 6.
"To evaluate Mike more rigorously we carried out two questionnaire-based evaluations, and also a log comparison with the data produced from the real-world soccer commentary in x 2."
"For the rst of the questionnaire evaluations, we used as sub- jects twenty of the attendees of a recent RoboCup Spring camp."
All these subjects were familiar with the RoboCup domain and the Soccer Server en-vironment.
We showed them an entire half of a RoboCup game commentated by Mike and col-lated their responses to the questions shown in Table 4.
These results largely show that the lis-teners found the commentary to be useful and to contain enough information to maintain their at-tention.
We also included some open-ended ques-tions on the questionnaire to elicit suggestions for features that should be strengthened or incor-porated in future versions of Mike .
"The most frequent responses here were requests for more background information on previous games played by the teams (possible in RoboCup, but to date we have only done this thoroughly for individ- ual games), more conversation between the agents (we plan to improve this with more communica-tion templates), and more emotion in the voices of the commentators (we have not yet tackled such surface-level NLG issues)."
"We also asked what the ideal number of commentators for a game would be; almost all subjects replied 2, with just two replying 3 and one replying 1."
"The above results are encouraging for Mike , but to show that the use of multiple agents was actually one of the reasons for the favourable audi-ence impression, we carried out a further test."
We created a single-agent version of Mike by switch-ing o the male/female voices in the TTS soft-ware and disabling the communication templates.
"This single-agent commentator comments on al-most exactly the same game content as the multi-agent version, but with a single voice."
We re-cruited ten volunteers with no prior knowledge of RoboCup and showed them both the single-agent and multi-agent versions of Mike commentating the same game as used in the previous experiment.
"We split the subjects into two groups so that one group watched the multi-agent version rst, and the other watched the single-agent version rst."
"Table 5 shows that the average questionnaire re-sponses over the two groups were lower than with the subjects who were familiar with RoboCup, but that the multi-agent version was more highly eval-uated than the single-agent version."
"Thus, even the super cially small modi cation of removing the agent dialogue has a measurable e ect on the commentary."
"Finally, we analysed Mike &apos;s commentary us-ing the same criteria as our protocol analysis of human soccer commentary in x 2.2."
We selected ten half-games at random from the 1998 RoboCup and compiled statistics on Mike &apos;s output with an automatic script.
The results of this analysis (Ta-ble 6) show a marked similarity to those of the human commentators.
This initial result is a very encouraging sign for further work in this area.
"We have argued for superiority of producing explanations with multiple, rather than single, agents."
"In particular, we identi ed the diÆculty of producing prior plans as the key issue constrain-ing the ability of a single agent to switch between high-level and low-level discourse strategies."
"As a rst step towards a multi-agent explana-tion system with solid theoretical underpinnings, we described the explanation strategies used by our live soccer commentary system, Mike ."
"We showed how a set of importance scores and infer- ence rules can be used as the basis for agents with di erent discourse strategies, and how the dis-course control techniques of interruption, abbrevi-ation, repetition and silence can be used not just to moderate the discourse of an individual agent, but also the interaction between agents."
"We eval-uated Mike &apos;s output through listener surveys, showing that it represents an advance over exist-ing commentary programs, which are all single-agent."
We also found that the discourse strategies of Mike &apos;s agents closely resembled those revealed by the protocol analysis of a team of real-life soc-cer commentators.
"In this paper, a computational approach for resolving zero-pronouns in Spanish texts is proposed."
Our approach has been evaluated with partial parsing of the text and the results obtained show that these pronouns can be resolved using similar techniques that those used for pronominal anaphora.
"Compared to other well-known baselines on pronominal anaphora resolution, the results obtained with our approach have been consistently better than the rest."
"In this paper, we focus specifically on the resolution of a linguistic problem for Spanish texts, from the computational point of view: zero-pronouns in the “subject” grammatical position."
"Therefore, the aim of this paper is not to present a new theory regarding zero-pronouns, but to show that other algorithms, which have been previously applied to the computational resolution of other kinds of pronoun, can also be applied to resolve zero-pronouns."
The resolution of these pronouns is implemented in the computational system called Slot Unification Parser for Anaphora resolution (SUPAR).
"This system, which was presented[REF_CITE], resolves anaphora in both English and Spanish texts."
"It is a modular system and currently it is being used for Machine Translation and Question Answering, in which this kind of pronoun is very important to solve due to its high frequency in Spanish texts as this paper will show."
"We are focussing on zero-pronouns in Spanish texts, although they also appear in other languages, such as Japanese, Italian and Chinese."
"In English texts, this sort of pronoun occurs far less frequently, as the use of subject pronouns is generally compulsory in the language."
"While in other languages, zero-pronouns may appear in either the subject´s or the object´s grammatical position, (e.g. Japanese), in Spanish texts, zero-pronouns only appear in the position of the subject."
"In the following section, we present a summary of the present state-of-the-art for zero-pronouns resolution."
This is followed by a description of the process for the detection and resolution of zero-pronouns.
"Finally, we present the results we have obtained with our approach."
"Zero-pronouns have already been studied in other languages, such as Japanese, (e.g.[REF_CITE])."
"They have not yet been studied in Spanish texts, however."
"Among the work done for their resolution in different languages, nevertheless, there are several points that are common for Spanish."
"The first point is that they must first be located in the text, and then resolved."
"Another common point among, they all employ different kinds of knowledge (e.g. morphologic or syntactic) for their resolution."
Some of these works are based on the Centering Theory (e.g.[REF_CITE]).
"Other works, however, distinguish between restrictions and preferences (e.g.[REF_CITE])."
"Restrictions tend to be absolute and, therefore, discard any possible antecedents, whereas preferences tend to be relative and require the use of additional criteria, i.e. heuristics that are not always satisfied by all anaphors."
Our anaphora resolution approach belongs to the second group.
"In computational processing, semantic and domain information is computationally inefficient when compared to other kinds of knowledge."
"Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see[REF_CITE]for example)."
"Such approaches, nevertheless, perform notably well."
"Their approach, however, operates almost exclusively on syntactic information."
"More recently,[REF_CITE]propose an algorithm for anaphora resolution that is actually a modified and extended version of the one developed[REF_CITE]."
It works from a POS tagger output and achieves an accuracy rate of 75%.
"In order to detect zero-pronouns, the sentences should be divided into clauses since the subject could only appear between the clause constituents."
"After that, a noun-phrase (NP) or a pronoun that agrees in person and number with the clause verb is sought, unless the verb is imperative or impersonal."
"As we are also working on unrestricted texts to which partial parsing is applied, zero-pronouns must also be detected when we do not dispose of full syntactic information."
"That study shows that only the following constituents were necessary for anaphora resolution: co-ordinated prepositional and noun phrases, pronouns, conjunctions and verbs, regardless of the order in which they appear in the text."
H1 Let us assume that the beginning of a new clause has been found when a verb is parsed and a free conjunction is subsequently parsed.
"When partial parsing is carried out, one problem that arises is to detect the different clauses of a sentence."
"Another problem is how to detect the zero-pronoun, i.e. the omission of the subject from each clause."
"With regard to the first problem, the heuristic H1 is applied to identify a new clause. (1)John y Jane llegaron tarde al trabajo porque ∅ [Footnote_1] se durmieron (John and Jane were late for work because [they] ∅ over-slept)"
1 The symbol ∅ will always show the position of the
"In this particular case, a free conjunction does not imply conjunctions [Footnote_2] that join co-ordinated noun and prepositional phrases."
"2 For example, it would include punctuation marks such as a semicolon."
"It refers, here, to conjunctions that are parsed in our partial parsing scheme."
"For instance, in sentence (1), the following sequence of constituents is parsed: np(John and Jane), verb(were), freeWord [Footnote_3] (late), pp(for work), conj(because), pron(they), verb(over-slept )"
3 The free words consist of constituents that are not covered by this partial parsing (e.g. adverbs).
"Since the free conjunction porque (because) has been parsed after the verb llegaron (were), the new clause with a new verb durmieron (over-slept) can be detected."
"With reference to the problem about detecting the omission of the subject from each clause with partial parsing, it is solved by searching through the clause constituents that appear before the verb."
"In sentence (1), we can verify that the first verb, llegaron (were), does not have its subject omitted since there appears a np(John and Jane)."
"However, there is a zero-pronoun, (they) ∅ , for the second verb durmieron (over-slept). (2) Pedro j vio a Ana k en el parque. ∅ k Estaba muy guapa (Peter j saw Ann k in the park. [She] ∅ k was very beautiful)"
"When the zero-pronoun is detected, our computational system inserts the pronoun in the position in which it has been omitted."
This pronoun will be resolved in the following module of anaphora resolution.
Person and number information is obtained from the clause verb.
"Sometimes in Spanish, gender information of the pronoun can be obtained when the verb is copulative."
"For example, in sentence (2), the verb estaba (was) is copulative, so that its subject must agree in gender and number with its object whenever the object can have either a masculine or a feminine linguistic form (guapo: masc, guapa: fem)."
"We can therefore obtain information about its gender from the object, guapa (beautiful in its feminine form) which automatically assigns it to the feminine gender so the omitted pronoun would have to be she rather than he."
Gender information can be obtained from the object of the verb with partial omitted pronoun. parsing as we simply have to search for a NP on the right of the verb.
"In this module, anaphors (i.e. anaphoric expressions such as pronominal references or zero-pronouns) are treated from left to right as they appear in the sentence, since, at the detection of any kind of anaphor, the appropriate set of restrictions and preferences begins to run."
The number of previous sentences considered in the resolution of an anaphora is determined by the kind of anaphora itself.
This feature was arrived at following an in depth study of Spanish texts.
"For pronouns and zero-pronouns, the antecedents in the four previous sentences, are considered."
"The following restrictions are first applied to the list of candidates: person and number agreement, c-command [Footnote_4] constraints and semantic consistency [Footnote_5] ."
4 The usage of c-command restrictions on partial parsing is presented in Ferrández et. al. (1998).
5 Semantic knowledge is only used when working on restricted texts.
This list is sorted by proximity to the anaphor.
"Next, if after applying the restrictions there is still more than one candidate, the preferences are then applied, with the degree of importance shown in Figure 1."
This sequence of preferences (from 1 to 10) stops whenever only one candidate remains after having applied a given preference.
"If after all the preferences have been applied there is still more than one candidate left, the most repeated candidates [Footnote_6] in the text are then extracted from the list, and if there is still more than one candidate, then the candidates that have appeared most frequently with the verb of the anaphor are extracted from the previous list."
"6 Here, we mean that we first obtain the maximum number of repetitions for an antecedent in the remaining list. After that, we extract the antecedents that have this value of repetition from the list."
"Finally, if after having applied all the previous preferences, there is still more than one candidate left, the first candidate of the resulting list (the closest to the anaphor) is selected."
"The set of constraints and preferences required for Spanish pronominal anaphora presents two basic differences: a) zero-pronoun resolution has the restriction of agreement only in person and number, (whereas pronominal anaphora resolution requires gender agreement as well), and b) a different set of preferences."
The main difference between the two sets of preferences is the use of two new preferences in our algorithm: Nos. 3 and 10.
"7 For example in: Peter es un genio (Peter is a genius), the tagger does not indicate that the object does not have both masculine and feminine linguistic forms. Therefore, a feminine subject would use the same form: Jane es un genio (Jane is a genius). Consequently, although the tagger says that the verb, es (is), is copulative, and the object, un genio (a genius) is masculine, this gender could not be used as a restriction for the zero-pronoun in the following sentence: ∅ Es un genio."
Gender information must therefore be considered a preference rather than a restriction.
"Another interesting fact is that syntactic parallelism (Preference No. 9) continues to be one of the last preferences, which emphasizes the unique problem that arises in Spanish texts, in which syntactic structure is quite flexible (unlike English)."
Our computational system (SUPAR) has been trained with a handmade corpus [Footnote_8] with 106 zero- pronouns.
8 This corpus has been provided by our colleagues in
"This training has mainly supposed the improvement of the set of preferences, i.e. the optimum order of preferences in order to obtain the best results."
"After that, we have carried out a blind evaluation on unrestricted texts."
"Specifically, SUPAR has been run on two different Spanish corpora: a) a part of the Spanish version of The Blue Book corpus, which contains the handbook of the International Telecommunications Union CCITT, published in English, French and Spanish, and automatically tagged by the Xerox tagger, and b) a part of the Lexesp corpus, which contains Spanish texts from different genres and authors."
"These texts are taken mainly from newspapers, and are automatically tagged by a different tagger than that of The Blue Book."
"The part of the Lexesp corpus that we processed contains ten different stories related by a sole narrator, although they were written by different authors."
"Having worked with different genres and disparate authors, we feel that the applicability of our proposal to other sorts of texts is assured."
"In Figure 2, a brief description of these corpora is given."
"In these corpora, partial parsing of the text with no semantic information has been used."
"To achieve this sort of evaluation, several different tasks may be considered."
Each verb must first be detected.
This task is easily accomplished since both corpora have been previously tagged and manually reviewed.
No errors are therefore expected on verb detection.
"Therefore, a recall [Footnote_9] rate of 100% is accomplished."
"9 By “recall rate”, we mean the number of verbs classified, divided by the total number of verbs in the text."
"The second task is to classify the verbs into two categories: a) verbs whose subjects have been omitted, and b) verbs whose subjects have not."
"The overall results on this sort of detection are presented in Figure 3 (success [Footnote_10] rate of 88% on 1,599 classified verbs, with no significant differences seen between the corpora)."
"10 By “success rate”, we mean the number of verbs successfully classified, divided by the total number of verbs in the text."
"We should also remark that a success rate of 98% has been obtained in the detection of verbs whose subjects were omitted, whereas only 80% was achieved for verbs whose subjects were not."
"This lower success rate is justified, however, for several reasons."
One important reason is the non-detection of impersonal verbs by the POS tagger.
"This problem has been partly resolved by heuristics such as a set of impersonal verbs (e.g. llover (to rain)), but it has failed in some impersonal uses of some verbs."
"For example, in sentence (3), the verb es (to be) is not usually impersonal, but it is in the following sentence, in which SUPAR would fail: (3) ∅"
Es hora de desayunar ([It] ∅ is time to have breakfast)
Two other reasons for the low success rate achieved with verbs whose subjects were not omitted are the lack of semantic information and the inaccuracy of the grammar used.
"The second reason is the ambiguity and the unavoidable incompleteness of the grammars, which also affects the process of clause splitting."
"In Figure 3, an interesting fact can be observed: 46% of the verbs in these corpora have their subjects omitted."
It shows quite clearly the importance of this phenomenon in Spanish.
"Furthermore, it is even more important in narrative texts, as this figure shows: 61% with the Lexesp corpus, compared to 26% with the technical manual."
We should also observe that The Blue Book has no verbs in either the first or the second person.
"This may be explained by the style of the technical manual, which usually consists of a series of isolated definitions, (i.e. many paragraphs that are not related to one another)."
"This explanation is confirmed by the relatively small number of anaphors that are found in that corpus, as compared to the Lexesp corpus."
"We have not considered comparing our results with those of other published works, since, (as we have already explained in the Background section), ours is the first study that has been done specifically for Spanish texts, and the designing of the detection stage depends mainly on the structure of the language in question."
"Any comparisons that might be made concerning other languages, therefore, would prove to be rather insignificant."
"As we have already shown in the previous section, (Figure 3), of the 1,599 verbs classified in these two corpora, 734 of them have zero-pronouns."
"In Figure 4, we present a classification of these third person zero-pronouns, which have been conveniently divided into three categories: cataphoric, exophoric and anaphoric."
"The first category is comprised of those whose antecedent, i.e. the clause subject, comes after the verb."
"For example, in sentence (4) the subject, a boy, appears after the verb compró (bought). (4) ∅ k Compró un niño k en el supermercado (A boy k bought in the supermarket)"
"This kind of verb is quite common in Spanish, as can be seen in this figure (49%)."
This fact represents one of the main difficulties found in resolving anaphora in Spanish: the structure of a sentence is more flexible than in English.
"These represent intonationally marked sentences, where the subject does not occupy its usual position in the sentence, i.e. before the verb."
"Cataphoric zero-pronouns will not be resolved in this paper, since semantic information is needed to be able to discard all of their antecedents and to prefer those that appear within the same sentence and clause after the verb."
"For example, sentence (5) has the same syntactic structure than sentence (4), i.e. verb, np, pp, where the subject function of the np can only be distinguished from the object by means of semantic knowledge. (5) ∅ Compró un regalo en el supermercado ([He] ∅ bought a present in the supermarket)"
"The second category consists of those zero-pronouns whose antecedents do not appear, linguistically, in the text (they refer to items in the external world rather than things referred to in the text)."
"Finally, the third category is that of pronouns that will be resolved by our computational system, i.e., those whose antecedents come before the verb: 228 zero-pronouns."
"These pronouns would be equivalent to the full pronoun he, she, it or they."
The different accuracy results are also shown in Figure 4: A success rate of 75% was attained for the 228 zero-pronouns.
By “successful resolutions” we mean that the solutions offered by our system agree with the solutions offered by two human experts.
"For each zero-pronoun there is, on average, 355 candidates before the restrictions are applied, and 11 candidates after restrictions."
"Furthermore, we repeated the experiment without applying restrictions and the success rate was significantly reduced."
"Since the results provided by other works have been obtained on different languages, texts and sorts of knowledge (e.g. Hobbs and Lappin full parse the text), direct comparisons are not possible."
"Therefore, in order to accomplish this comparison, we have implemented some of these approaches in SUPAR."
"Although some of these approaches were not proposed for zero-pronouns, we have implemented them since as our approach they could also be applied to solve this kind of pronoun."
"For example, with the baseline presented[REF_CITE]an accuracy of 49.1% was obtained, whereas, with our system, we achieved 75% accuracy."
"These results highlight the improvement accomplished with our approach, since Hobbs´ baseline is frequently used to compare most of the work done on anaphora resolution [Footnote_11] ."
"11[REF_CITE], for example, it is compared with an adaptation of the Centering Theory[REF_CITE], and Hobbs´ baseline out-performs it."
The reason why Hobbs´ algorithm works worse than ours is due to the fact that it carries out a full parsing of the text.
"Furthermore, the way to explore the syntactic tree with Hobbs’ algorithm is not the best one for the Spanish language since it is nearly a free-word-order language."
"Our proposal has also been compared with the typical baseline of morphological agreement and proximity preference, (i.e., the antecedent that appears closest to the anaphor is chosen from among those that satisfy the restrictions)."
The result is a 48.6% accuracy rate.
"Our system, therefore, improves on this baseline as well."
"Moreover, in order to compare our proposal with Centering approach, Functional Centering[REF_CITE]has also been implemented, and an accuracy of 60% was attained."
"One of the improvements afforded by our proposal is that statistical information from the text is included with the rest of information (syntactic, morphologic, etc.)."
"To be able to compare our approach to that of Dagan and Itai, and to be able to evaluate the importance of this kind of information, our method was applied with statistical information [Footnote_12] only."
12 This statistical information consists of the number of times that a word appears in the text and the number of times that it appears with a verb.
"If there is more than one candidate after applying statistical information, preference, and then proximity preference are applied."
The results obtained were lower than when all the preferences are applied jointly: 50.8%.
These low results are due to the fact that statistical information has been obtained from the beginning of the text to the pronoun.
A previous training with other texts would be necessary to obtain better results.
"Regarding the success rates reported[REF_CITE]for pronominal references (82.2%[REF_CITE]% for Spanish version of The Blue Book, and 87.3% for the English version), are higher than our 75% success rate for zero-pronouns."
This reduction (from 84% to 75%) is due mainly to the lack of gender information in zero-pronouns.
"It should be pointed out, however, that he used some knowledge that was very close to the genre [Footnote_13] of the text."
"13 For example, the antecedent indicator section heading preference, in which if a NP occurs in the heading of the section, part of which is the current"
"In our study, such information was not used, so we consider our approach to be more easily adaptable to different kinds of texts."
"Moreover, Mitkov worked exclusively with technical manuals whereas we have worked with narrative texts as well."
The difference observed is due mainly to the greater difficulty found in narrative texts than in technical manuals which are generally better written.
"In any case, the applicability of our proposal to different genres of texts seems to have been well proven."
"Anyway, if the order of application of the preferences [Footnote_14] is varied to each different text, an 80% overall accuracy rate is attained."
14 The difference between the individual sets of preferences is the degree of importance of the preferences for proper nouns and syntactic parallelism.
"This fact implies that there is another kind of knowledge, close to the genre and author of the text that should be used for anaphora resolution."
"In this paper, we have proposed the first algorithm for the resolution of zero-pronouns in Spanish texts."
It has been incorporated into a computational system (SUPAR).
"In the evaluation, several baselines on pronominal anaphora resolution have been implemented, and it has achieved better results than either of them have."
"As a future project, the authors shall attempt to evaluate the importance of semantic information for zero-pronoun resolutions in unrestricted texts."
"Such information will be obtained from a lexical tool, (e.g. EuroWordNet), which could be consulted automatically."
"We shall also evaluate our proposal in a Machine Translation application, where we shall test its success rate by its generation of the zero-pronoun in the target language, using the algorithm described[REF_CITE]."
! R =  &quot; C e=z kV5!&quot;_C_  5&quot; e@!@ ¦5z@    ¨§© ¤z &quot;!ªrC Y£  5£S°±5&quot;_  5   &quot;C  ¶ °#z&quot;  &quot;ze=z § ¸ TCER«JRR@&quot; z@¨]@  &quot;   @ ¦5z&quot; z@@.5§  5 º! » ¼ 6©452v( *( 6 ÀC¹Á5@ be @  @ÂJ¦R!  ¢_ 5&quot;¤5¬­! £C &quot;£}&quot;5C5Cvzr@   År5R !­ J¤C«Æ5!_£R&quot;
¦=]ª ´U§G¬Uª-¶  ´U» ³¼®iªE¯¹®i«½«P³¾¬U°¼´U¿]¨®~) ³µ¯J¶¥¯¹´Ã¿U®iª®»«P´]¬r¯¹®cª´]­\]ªP· ³¾ª¿Ò¯¹´Ó¯¹»1¬U³µªN¬UªP«É¯¹®i·¹¯È¬Ôª´]­ (­a¬U± ÀP³µª®M°µ®i¬r»1ªº]­\³µªP¬U°Âº ªP´]­a³µªP¬U°~]ª½¿U®iª®»1¬r¯¹´U» ]!]´ ] ·1³¼¿]ªM«P³¾¬U°¼´U¿]?´U»1³µ®iªE¯¹®i«Þ«®²º% Ïx® ±¬Uªâ¬U±1ÀP³µ®ãU®a¬½äråEæN­\ ­ ] \çiè ]| ­a´]·¹¯[]§ê»1®ië-¨®iª-¯&amp;[¯J¶ÅG®â´U§^Û^ ª´]­a³µªP¬U°!®²Äº ]´ ] ­ ¬UªE¶~\ ]´ ]]ªÃ­ ´«®i°µ·&amp;«P³µ« ªP´U¯!%  ¯¹´U»°µ®i¬r»1ª®i«MªP´%Ë ]­a³µªP¬U°Âº?]ªÈ¿U®iª®» ¬vº
:  ?)«¬­O¢? ?¡ ?? ¬£¨º¬­()?¤ ¡¼¬?®¨X¬/¥§ª»µ ½ ´¾ ½ )¡1¤­O¨º¿4?)?¡ ¡­¥¶´ª¿4¥¶¾¶)¬­´Ä? ÃÇ))ªI¬­Os¥¶ª ¨Xª(¨º¾¶·´¤?¥¶¬­®4ÃÈ¢)¨X¾¶¾¶O?´ :½  )¤?( I)ÃÇ´ª4¡)?¨º¬­)ªb¨X¬­¥:¥¶¬L´ªÍÌ½ ¿Q®4&amp;) ²p±Q¾§¾Îµt¡­¬¥¶ªI¬­)?)Ã6¤?OI±4¿Q¾§¥°¢/Å )?)Ã
Ð Ñ .?Òc. ? ±4¤£¨º¾c¾°¨Xª4·±¨X·)? (: )¢ ¨º¬­)?¿4®4)ÃÇ))¬­®Q)?)?¤ ¡OÅ ¡­¥Ô ±¢½ ? )®_¾¶À})¨X¬?) ª¨º¬­¥?¥ ½ ½ _¥¶ªÕ¨º··¾§±Q¬­¥¶ª¨X¬)?¥ ½ 0¾°¨ºªIµ)¡Áµ }¨º¤£¨&amp;´¤Ç× ±Q¤­ÖI¥¶¡­®; ª¨º¬­¥ ½ OØº¿´¾¶À&gt;?¡ ÀIª&gt;?¬ ®4)¬?¥Ù¢¾Ù¨ºª4·±b¨X·U¡0¾¶¥¶Ö&apos;ÆtªI±4ÖI¬­¥¶¬? ¡­±b¢­®E¾°¨ºª4·±¨º·)¡©¨Ú¡­¥¶ª4·¾¶©«¼´¤£ÛÃ}&gt; Ã})_¡ ¨X¡:¨XªÍ¨ ½ ))ª4·¬­®ÝÜ$ª4·¾¶¥¶¡­® ¡­Uª&gt;?¬ ) à/¥§ªQ¥§¬(? ½ ) ¾Çã¹ä1´¡?Ö)ªQª4¥¶)_¨ºª[ê1)
We present a clustering algorithm for Arabic words sharing the same root.
Root based clusters can substitute dictionaries in indexing for IR.
"Tests show a successful treatment of infixes and accurate clustering to up to 94.06% for unedited Arabic text samples, without the use of dictionaries."
Canonisation of words for indexing is an important and difficult problem for Arabic IR.
Arabic is a highly inflectional language with 85% of words derived from tri-lateral roots[REF_CITE].
Stems are derived from roots through the application of a set of fixed patterns.
Addition of affixes to stems yields words.
Words sharing a root are semantically related and root indexing is reported to outperform stem and word indexing on both recall and precisi[REF_CITE].
"However, Arabic morphology is excruciatingly complex (the Appendix attempts a brief introduction), and root identification on a scale useful for IR remains problematic."
Research on Arabic IR tends to treat automatic indexing and stemming separately.
"In[REF_CITE], stemming is done manually and the IR index is built by manual insertion of roots, stems and words."
"Typically, Arabic stemming algorithms operate by “trial and error”."
"Affixes are stripped away, and stems “undone”, according to patterns and rules, and with reference to dictionaries."
Root candidates are checked against a root lexicon.
"If no match is found, affixes and patterns are re-adjusted and the new candidate is checked."
The process is repeated until a root is found.
Morpho-syntactic parsers offer a possible alternative to stemming algorithms.
"Some work builds on established formalisms such a DATR[REF_CITE], or KIMMO."
This latter strand produced extensive deep analyses.
This was refined[REF_CITE]to the current on-line system capable of analysing over 70 million words.
"So far, these approaches have limited scope for deployment in IR."
"Even if substantial, their morpho-syntactic coverage remains limited and processing efficiency implications are often unclear."
"In addition, modern written Arabic presents a unique range of orthographic problems."
Short vowels are not normally written (but may be).
Different regional spelling conventions may appear together in a single text and show interference with spelling errors.
"These systems, however, assume text to be in perfect (some even vowelised) form, forcing the need for editing prior to processing."
"Finally, the success of these algorithms depends critically on root, stem, pattern or affix dictionary quality, and no sizeable and reliable electronic dictionaries exist."
"1[REF_CITE]estimate there are around 10,000 independent roots."
"Absence of large and reliable electronic lexical resources means dictionaries would have to be updated as new words appear in the text, creating a maintenance overhead."
"Overall, it remains uncertain whether these approaches can be deployed and scaled up cost-effectively to provide the coverage required for full scale IR on unsanitised text."
"Our objective is to circumvent morpho-syntactic analysis of Arabic words, by using clustering as a technique for grouping words sharing a root."
"In practise, since Arabic words derived from the same root are semantically related, root based clusters can substitute root dictionaries for indexing in IR and furnish alternative search terms."
"Clustering works without dictionaries, and the approach removes dictionary overheads completely."
"Clusters can be implemented as a dimension of the index, growing dynamically with text, and without specific maintenance."
They will accommodate effortlessly a mixture of regional spelling conventions and even some spelling errors.
"To our knowledge, there is no application of automatic root-based clustering to Arabic, using morphological similarity without dictionary."
"Clustering and stemming algorithms have mainly been developed for Western European languages, and typically rely on simple heuristic rules to strip affixes and conflate strings."
"For instance,[REF_CITE]and[REF_CITE]confine stemming to suffix removal, yet yield acceptable results for English, where roots are relatively inert."
"Such approaches exploit the morphological frugality of some languages, but do not transfer to heavily inflected languages such as Arabic."
"In contrast,[REF_CITE]developed a technique to calculate a similarity co-efficient between words as a factor of the number of shared sub-strings."
The approach (which we will call Adamson’s algorithm for short) is a promising starting point for Arabic clustering because affix removal is not critical to gauging morphological relatedness.
"In this paper, we explain the algorithm, apply it to raw modern Arabic text and evaluate the result."
"We explain our Two-stage algorithm, which extends the technique by (a) light stemming and (b) refinements sensitive to Arabic morphology."
We show how the adaptation increased successful clustering of both the original and new evaluation data.
"We focus on IR, so experiments use modern, unedited Arabic text, with unmarked short vowels[REF_CITE]."
In all we constructed five data sets.
"The first set is controlled, and was designed for testing on a broad spectrum of morphological variation."
"It contains selected roots with derived words chosen for their problematic structure, featuring infixes, root consonant changes and weak letters."
"It also includes superficially similar words belonging to different roots, and examples of hamza as a root consonant, an affix and a silent sign."
Table 1 gives details.
"Data sets two to four contain articles extracted[REF_CITE], and the fifth[REF_CITE], both newspapers from Qatar."
"Following Adamson, function words have been removed."
The sets have domain bias with the second (575 words) and the fourth (232 words) drawn randomly from the economics and the third (750 words) from the sports section.
The fifth (314 words) is a commentary on political history.
Sets one to three were used to varying extents in refining our Two-stage algorithm.
Sets four and five were used for evaluation only.
"Electronically readable Arabic text has only recently become available on a useful scale, hence our experiments were run on short texts."
"On the other hand, the coverage of the data sets allows us to verify our experiments on demanding samples, and their size lets us verify correct clustering manually."
"The algorithm drags an n-sized window across two strings, with a 1 character overlap, and removes duplicates."
The strings&apos; similarity co-efficient (SC) is calculated by Dice’s equation: SC (Dice) = 2*(number of shared unique n-grams)/(sum of unique n-grams in each string)
"After the SC for all word pairs is known, the single link clustering algorithm is applied."
A similarity (or dissimilarity) threshold is set.
The SC of pairs is collected in a matrix.
The threshold is applied to each pair’s SC to yield clusters.
A cluster absorbs a word as long as its SC to another cluster item exceeds the threshold (van[REF_CITE]).
Similarity to a single item is sufficient.
Cluster size is not pre-set.
This experiment tests Adamson&apos;s algorithm on Arabic data to assess its ability to cluster words sharing a root.
Each of the data sets was
Data set Set 1 Set 2 Benchmark: Total Manual Clusters(A) 9 267 Multi-word (B) 9 130 Single word (C) 0 137 SC cut-off [Footnote_2] 0.50 0.54 Test:(% of Benchmark)
2 Ranges rather than specific values are given where cut-offs between the lower and higher value do not alter cluster distribution.
Correct Clusters (% of A) 11.11% 56.55% Multi-word (% of B) 11.11% 38.46% Single word (% of C) 0.0% 73.72% clustered manually to provide an ideal benchmark.
This task was executed by a native Arabic speaker with reference to dictionaries.
"Since we are working with very small texts, we sought to remove the effects of sampling in the tests."
"To assess Adamson’s algorithm’s potential for clustering Arabic words, we preferred to compare instances of optimal performance."
"We varied the SC to yield, for each data set, the highest number of correct multi-word clusters."
"Note that the higher the SC cut-off, the less likely that words will cluster together, and the more single word clusters will appear."
This has the effect of growing the number of correct clusters because the proportion of correct single word clusters will increase.
"As a consequence, for our purposes, the number of correct multi-word clusters (and not just correct clusters) are an important indicator of success."
A correct multi-word cluster covers at least two words and is found in the manual benchmark.
It contains all and only those words in the data set which share a root.
Comparison with a manual benchmark inevitably introduces a subjective element.
"Also, our evaluation measure is the percentage of correct benchmark clusters retrieved."
This is a “recall” type indicator.
"Together with the strict definition of correct cluster, it cannot measure cluster quality."
Finer grained evaluation of cluster quality would be needed in an IR context.
"However, our main concern is comparing algorithms."
The current metrics aim for a conservative gauge of how Adamson’s algorithm can yield more exact clusters from a full range of problematic data.
Set 3 Set 4[REF_CITE]151 190 164 50 63 173 101 127 0.75 0.58-0.60 0.61-0.66 60.83% 70.86% 74.21% 21.95% 40% 34.92% 97.69% 86.14% 93.70%
"Our interpretation of correct clustering is stringent and therefore conservative, adding to the significance of our results."
Cluster quality will be reviewed informally.
Table 3 shows results for Adamson’s algorithm.
The figures for the first data set have to be suitably interpreted.
The set deliberately did not include single word clusters.
The results suggest that the algorithm is very successful at identifying single word clusters but performs poorly on multi-word clusters.
"The high success rate for single word clusters is partly due to the high SC cut-off, set to yield as many correct multi-word clusters as possible."
"In terms of quality, however, only a small proportion of multi-word clusters were found to contain infix derivations (11.11%, 4.76%, 0.0% 4.35% and 9.09% for each data set respectively), as opposed to other variations."
"In other words, strings sharing character sequences in middle position cluster together more successfully."
Infix recognition is a weak point in this approach.
"Whereas the algorithm is successful for English, it is no surprise that it should not perform equally well on Arabic."
Arabic words tend to be short and the chance of words derived from different roots sharing a significant proportion of characters is high (eg K h br (news) vs
K h bz (bread)).
Dice’s equation assumes the ability to identify an uninterrupted sequence of root consonants.
The heavy use of infixes runs against this.
"Similarly, affixes cause interference (see 4.1.1)."
The challenge of root based clustering for Arabic lies in designing an algorithm which will give relevance to root consonants only.
"Using Adamson’s algorithm as a starting point, we devised a solution by introducing and testing a number of successive refinements based on the morphological knowledge and the first three data sets."
The rationale motivating these refinements is given below.
"The high incidence of affixes keeps accurate cluster formation low, because it increases the SC among words derived from different roots, and lowers the SC between derivations of the same root using different affixes, as illustrated in tables 4 and 5."
"We found empirically that light stemming, removing a small number of obvious affixes, gave better results than heavy stemming aimed at full affix stripping."
"Heavy stemming brought the risk of root consonant loss (eg t’amyn (insurance) from root amn (sheltered): heavy stemming: t’am, light stemming: t’amn)."
"Light stemming, on the other hand, does little more than reducing word size to 3 or 4 characters."
"Weak letters (alif, waw, ya) occur freely as root consonants as well as affixes."
"Under derivation, their form and location may change, or they may disappear."
"As infixes, they interfere with SC, causing failure to cluster (table 6)."
Their effects were reduced by a method we refer to as “cross”.
It adds a bi-gram combining the letters occurring before and after the weak letter. 4.1.3 Suspected affixes and differential weighting:
Our objective is to define an algorithm which gives suitable precedence to root consonants.
"Light stemming, however does not remove all affixes."
"Whereas fool proof affix detection is problematic due to the overlap between affix and root consonants, affixes belong to a closed class and it is possible to identify “suspect” letters which might be part of an affix."
"Giving equal weight of 1 to all substrings equates the evidence contributed by all letters, whether they are root consonants or not."
"Suspected affixes, however, should not be allowed to affect the SC between words on a par with characters contributing stronger evidence."
"We conducted a series of experiments with differential weightings, and determined empirically that 0.25 weight for strings containing weak letters, and 0.50 for strings containing suspected non-weak letter affixes gave the best SC for the first three data sets."
N-gram size can curtail the significance of word boundary letters[REF_CITE].
"To give them opportunity to contribute fully to the SC, we introduced word boundary blanks[REF_CITE]."
"Also, the larger the n-gram, the greater its capacity to mask the shorter substring which can contain important evidence of similarity between word pairs[REF_CITE]."
Of equal importance is the size of the sliding overlap between successive n-grams[REF_CITE].
The problem is to find the best setting for n-gram and overlap size to suit the language.
We sought to determine settings experimentally.
"Bi-grams with single character overlap and blank insertion (* in the examples) at word boundaries raised the SC for words sharing a root in our three data sets, and lowered the SC for words belonging to different roots."
"Dice’s equation boosts the importance of unique shared substrings between word pairs, by doubling their evidence."
"As we argued earlier, since Arabic words tend to be short, the relative impact of shared substrings will already be dramatic."
We replaced the Dice metric with the Jaccard formula below to reduce this effect (see van[REF_CITE]).
SC (Jac) = shared unique n-grams/(sum of unique n-grams in each string -shared unique n-grams)
The Two-stage algorithm is fully implemented.
Words are first submitted to light stemming to remove obvious affixes.
"The second stage is based on Adamson’s algorithm, modified as described above."
"From the original, we retained bi-grams with a one character overlap, but inserted word boundary blanks."
Unique bi-grams are isolated and cross is implemented.
Each bi-gram is assigned a weight (0.25 for bi-grams containing weak letters; 0.5 for bi-grams containing potential non-weak letter affixes; 1 for all other bi-grams).
Jaccard’s equation computes a SC for each pair of words.
We retained the single-link clustering algorithm to ensure comparability.
Table 8 shows the results of the Two-stage algorithm for our data sets.
The maximally effective cut of point for all sets lies closer.
Figures for the first set have to be treated with caution.
The perfect clustering is explained by the text’s perfect spelling and by the sample containing exactly those problematic phenomena on which we wanted to concentrate.
"The algorithm deals with weak letter mutation, and infix appearance and disappearance in words sharing a root (eg the root qwm and its derived words, especially the role of Hamza as an infix in one of its variations)."
"Even though the second and third data sets informed the modifications to a limited extent, their results show that the improvements stood up to free text."
"For the second data set, the Two-stage algorithm showed 31.5% improvement over Adamson’s algorithm."
"Importantly, it discovered 84.13% of the multi-word clusters containing words with infixes, an improvement of 79.37%."
"The values for single word clustering are close and the modifications preserved the strength of Adamson’s algorithm in keeping single word clusters from mixing, because we were able to maintain a high SC threshold."
"On the third data set, the Two-stage algorithm showed an 26.11% overall improvement, with 84% successful multi-word clustering of words with infixes (compare 0% for Adamson)."
The largest cluster contained 14 words. 10 clusters counted as unsuccessful because they contained one superficially similar variation belonging to a different root (eg TwL (lengthened) and bTL (to be abolished)).
"If we allow this error margin, the success rate of multi-word clustering rises to 90%."
"Since our SC cut-off was significantly lower than in Adamson’s base line experiment, we obtained weaker results for single word clustering."
The fourth and fifth data sets played no role in the development of our algorithm and were used for evaluation purposes only.
The Two-stage algorithm showed an 23.18% overall improvement in set four.
"It successfully built all clusters containing words with infixes (100% -compare with 4.35% for Adamson’s algorithm), an improvement of 95.65%."
"The two-stage algorithm again preserved the strength of Adamson at distinguishing single word clusters, in spite of a lower SC cut-off."
The results for the fifth data set are particularly important because the text was drawn from a different source and domain.
"Again, significant improvements in multi and single word clustering are visible, with a slightly higher SC cut-off."
The algorithm performed markedly better at identifying multi-word clusters with infixes (72.72% - compare with 9.09% for Adamson).
"The results suggest that the Two-stage algorithm preserves the strengths[REF_CITE], whilst adding a marked advantage in recognising infixes."
"The outcome of the evaluation on fourth and fifth data sets are very encouraging and though the samples are small, they give a strong indication that this kind of approach may transfer well to text from different domains on a larger scale."
"Weak letters can be root consonants, but our differential weighting technique prevents them from contributing strong evidence, whereas non-weak letters featuring in affixes, are allowed to contribute full weight."
Modifying this arrangement would interfere with successful clustering (eg after light stemming: t is a root consonant in ntj (produced) and an infix in Ltqy (from root Lqy - encountered).
These limitations are a result of light stemming.
"Although the current results are promising, evaluation was hampered by the lack of a sizeable data set to verify whether our solution would scale up."
"We have developed, successfully, an automatic classification algorithm for Arabic words which share the same root, based only on their morphological similarities."
Our approach works on unsanitised text.
"Our experiments show that algorithms designed for relatively uninflected languages can be adapted for highly inflected languages, by using morphological knowledge."
We found that the Two-stage algorithm gave a significant improvement over Adamson’s algorithm for our data sets.
"It dealt successfully with infixes in multi-word clustering, an area where Adamson’s algorithm failed."
"It matched the strength of Adamson in identifying single word clusters, and sometimes did better."
Weak letters and the overlap between root and affix consonants continue to cause interference.
"Nonetheless, the results are promising and suggest that the approach may scale up"
Future work will concentrate on two issues.
The light stemming algorithm and the differential weighting may be modified to improve the identification of affixes.
The extent to which the algorithm can be scaled up must be tested on a large corpus.
"Our thanks go to the Kuwait State&apos;s Public Authority for Applied Education and Training, for the supporting research studentship, and to two anonymous referees for detailed, interesting and constructive comments."
The vast majority of Arabic words are derived from 3 (and a few 4) letter roots via a complex morphology.
Roots give rise to stems by the application of a set of fixed patterns.
Addition of affixes to stems yields words. from 3-letter roots.
"Stem patterns are formulated as variations on the characters f?L (pronounced as f&apos;l - ? is the symbol for ayn, a strong glottal stop), where each of the successive consonants matches a character in the bare root (for ktb, k matches f, t matches ? and b matches L)."
Stems follow the pattern as directed.
"As the examples show, each pattern has a specific effect on meaning."
"Several hundred patterns exist, but on average only about 18 are applicable to each root[REF_CITE]."
The language distinguishes between long and short vowels.
"Short vowels affect meaning, but are not normally written."
"However, patterns may involve short vowels, and the effects of some patterns are indistinguishable in written text."
Readers must infer the intended meaning.
"Affixes may be added to the word, either under derivation, or to mark grammatical function."
"For instance, walktab breaks down as w (and) + al (the) + ktab (writers, or book, depending on the voweling)."
"Other affixes function as person, number, gender and tense markers, subject and direct object pronouns, articles, conjunctions and prepositions, though some of these may also occur as separate words (eg wal (and the))."
Arabic morphology presents some tricky NLP problems.
"Stem patterns “interdigitate” with root consonants, which is difficult to parse."
"Also, the long vowels a (alif), w (waw) and y (ya) can occur as root consonants, in which case they are considered to be weak letters, and the root a weak root."
"Under certain circumstances, weak letters may change shape (eg waw into ya) or disappear during derivation."
"Long vowels also occur as affixes, so identifying them as affix or root consonant is often problematic."
"The language makes heavy use of infixes as well as prefixes and suffixes, all of which may be consonants or long vowels."
"Apart from breaking up root letter sequences (which tend to be short), infixes are easily confused with root consonants, whether weak or not."
The problem for affix detection can be stated as follows: weak root consonants are easily confused with long vowel affixes; consonant affixes are easily confused with non-weak letter root consonants.
Erroneus stripping of affixes will yield the wrong root.
Arabic plurals are difficult.
"The dual and some plurals are formed by suffixes, in which case they are called external plurals."
"The broken, or internal plural, however, changes the internal structure of the word according to a set of patterns."
"To illustrate the complexity, masculine plurals take a -wn or -yn suffix, as in mhnds (engineer), mhndswn."
"Female plurals add the -at suffix, or change word final -h to -at, as in mdrsh (teacher), mdrsat."
"Broken plurals affect root characters, as in mal (fund from root mwl), amwal, or wSL (link from root wSL), ‘aySaL. The examples are rife with long vowels (weak letters?)."
They illustrate the degree of interference between broken plural patterns and other ways of segmenting words.
Regional spelling conventions are common: eg. three versions of word initial alif occur.
"The most prominent orthographic problem is the behaviour of hamza, (’), a sign written over a carrier letter and sounding a lenis glottal stop (not to be confused with ayn)."
Hamza is not always pronounced.
"Like any other consonant, it can take a vowel, long or short."
"In word initial position it is always carried by alif, but may be written above or below, or omitted."
"Mid-word it is often carried by one of the long vowels, depending on rules whose complexity often gives rise to spelling errors."
"At the end of words, it may be carried or written independently."
"Hamza is used both as a root consonant and an affix, and is subject to the same problems as non-weak letter consonants, compounded by unpredictable orthography: identical words may have differently positioned hamzas and would be considered as different strings."
^¥A¦¨§/*©S­;.§*¤ ¹ [ ¹ ¦¸®[ ¯¦¸²G®Sª«· |²£­©A¥A²G·¸² ¹ [§Q¬f§Â²«¾¢µ²£¯;¥Ã­¬ ¹ ³A´» ·¨ª«­/ª£®S¶E¥A¦ ¹ ¥A·½ÁÄ¦¸­­¬ ¹ ;.¥6ªG§ µA­²£³ ¹ ¥]¯ ÇÄµA­¦¸® ¹ÉÈ ¾­²£»Ê¶[¦¨§Q¯;­¦¸µA³A¯;¦½²£®Sª£·[©SªË¯;´  ¹ ¬Ì»|²G®A²£·½¦¸® ¹ &amp;¦º¯¥ ®
A²Ð¶[ ¯k§;;¦¸²G®8ÒÓ¤^¥A¬6ª«· ¹ ²G­;¦¸¯;¥A» $µS¦¸®A¬f§ ¾²G³A­I²£­¦ ¹ ¦¸®
Sª£·Sª«·½¦ ¹ ®A»|]¯I»3²[[¶  µSªG§Q¬f¶&quot; &amp;¾­; ;³Sª£·I§; ¹ ¥];­¦¸® ¹ §Q¦½»|¦º´ ·¨ª«­¦º¯ÖÁ3ª«®S¶|;] [ ´ ;.­ ª«®§;¶[³° ¯;¦½²£®Í©A­²£µSª£µA¦¸·½¦¸¯;$×]¯ª£­Q¯;´ ¦½® ¹ &amp;Ï ¦º¯¥*&amp;¦½® [¿S¬f° ¯;¦½²£®CÕ ª£»3©S·¸¬f§I¾²£­I¯;.­ ª«¦½®A¦½® ¹ |©A­¦¸²G­^;§ [¦½® ¹ ²£¾$·¸¬ ¹ ª£·»|²G­;©A¥S²£·½² ¹ ;»Åª«¯; $²£¾W¯;/ª«®Sª£·¸Á[§; ©SªG§Ö¯;;¬|;¯ ¬f§Ö¯| ¹  àGà ;¥A¬Ä§;&amp;]¯; ÝGæ]&quot;²£®
Í¯;¥A¬Å»|²]§Ö¯¥A ¦ ¹ ¥A·½Áè¦½­;­¬ ¹ ´ ³A·¨ª«­I¾²G­;»Å§^ª«®S¶ à£à |²£®*¾²£­»Å§ ;¦½® ¹ ;;[³ |ë ÎAªË¯;¦½²£®CÒ ì í 5bD]F=4åVgîXÚ9ïYA9ÖÛX@ ¤ ¥A¦¨§&amp;&amp;©A­;]¯§&amp; ¹ ;^ ¹ ²G­;¦¸¯;¥S»=¾²G­¯;;·½Á&quot;³A®S§;]¦¨§;[³° ;¯ ¦½²£®Í²«¾ ¦½®[;¦½²£®Sª£·8»|²£­©A¥A²G·¸² ¹ .]Ï&amp;¦¸¯;[ ²£®|¥A¦ ¹ ;¬ ¹ ;[·¸¬f¶|µÉÁ |²£­©A¥A²G·¸² ¹ Á3¦¸®S¶A³S°  ¹ ²G­;¦¸¯;¥S»| ¾³A·g¯²/[^¯ªG§Qò*ªG§I¯; ;§ .ªË¯;¬Þ§Q¯;
"The paper develops a constraint-based the-ory of prosodic phrasing and prominence, based on an HPSG framework, with an implementation in ALE."
Prominence and juncture are represented by n-ary branching metrical trees.
"The general aim is to define prosodic structures recursively, in parallel with the definition of syntactic structures."
We address a number of prima facie problems arising from the discrepancy between syntactic and prosodic structure
"This paper develops a declarative treatment of pros-odic constituents within the framework of constraint-based phonology, as developed for example[REF_CITE]."
"On such an approach, phonological representations are encoded with typed feature terms."
"In addition to the representational power of complex feature values, the inheritance hierarchy of types provides a flexible mechanism for classifying linguistic structures, and for expressing generalizations by means of type inference."
"To date, little work within constraint-based phono-logy has addressed prosodic structure above the level of the foot."
"In my treatment, I will adopt the following assumptions: 1."
"Phonology is induced in parallel with syntactic structure, rather than being mapped from pre-built parse trees. 2."
Individual lexical items do not impose constraints on their neighbour’s phonology.
"The first of these assumptions ensures that phonology is compositional, in the sense that the only information available when assembling the phonology of a com-plex constituent is the phonology of that constituents daughters."
"The second assumption is one that is standardly adopted in HPSG[REF_CITE], in the sense that heads can be subcategorized with respect to the syntactic and semantic properties of their arguments (i.e., their arguments’ synsem values), but not with respect to their arguments’ phonological properties."
"Although I am not convinced that this restriction is correct, it is worthwhile to explore what kinds of phonological analyses are compatible with it."
Most of the data used in this paper was drawn from the SOLE spoken corpus[REF_CITE]. [Footnote_1]
1 The task of recovering relevant examples from the SOLE corpus was considerably aided by the Gsearch corpus query system[REF_CITE].
The corpus was based on recordings of one speaker reading approximately 40 short descriptive texts concerning jewelry.
Metrical trees were introduced[REF_CITE]as a basis for formulating stress-assignment rules in both words and phrases.
Syntactic constituents are assigned relative prosodic weight according to the following rule: (1) NSR:
"In a configuration [ C A B], if C is a phrasal category, B is strong."
Prominence is taken to be a relational notion: a constituent labelled ‘s’ is stronger than its sister.
"Consequently, if B in (1) is strong, then A must be weak."
"In the case of a tree like (2), Liberman and Prince’s (1) yields a binary-branching structure of the kind illustrated in (3) (where the root of the tree is unlabeled): (2) VP"
V Det N fasten a cloak (3) s w w s fasten a cloak
"For any given constituent analysed by a metrical tree t, the location of its main stress can be found by tracing a path from the root of t to a terminal element α such that all nodes on that path are labelled ‘s’."
Thus the main stress in (3) is located on the element cloak.
"In general, the most prominent element, defined in this way, is called the Designated Terminal Element (DTE)[REF_CITE]."
"Note that (1) is the metrical version of Chomsky and Halle’s (1968) Nuclear Stress Rule (NSR), and encodes the same claim, namely that in the default case, main stress falls on the last constituent in a given phrase."
"Of course, it has often been argued that the notion of ‘default prominence’ is flawed, since it supposes that the acceptability of utterances can be judged in a null context."
"Nevertheless, there is an alternative conception: the predictions of the NSR correctly describe the prominence patterns when the whole proposition expressed by the clause in question receives broad focus[REF_CITE]."
This is the view that I will adopt.
"Although I will concentrate in the rest of the paper on the broad focus pattern of intonation, the approach I develop is intended to link up eventually with pragmatic information about the location of narrow focus."
"In the formulation above, (1) only applies to binary-branching constituents, and the question arises how non-binary branching constituent structures (e.g., for VPs headed by ditranstive verbs) should be treated."
"One opti[REF_CITE]would be to drop the restriction that metrical trees are binary, allowing structures such as Fig 1."
"Since the nested structure which results from binary branching appears to be irrelevant to phonetic interpretation, I will use n-ary metrical trees in the following analysis."
"In this paper, I will not make use of the Pros-odic Hierarchy[REF_CITE]."
"Most of the phenomena that I wish to deal with lie in the blurry regi[REF_CITE]between the Phonological Word and the Intonational Phrase (IP), and I will just refer to ‘prosodic constituents’ without committing myself to a specific set of labels."
I will also not adopt the Strict Layer Hypothesis[REF_CITE]which holds that elements of a given prosodic category (such as Intonational Phrase) must be exhaustively analysed into a sequence of elements of the next lower category (such as Phonological Phrase).
"However, it is important to note that every IP will be a prosodic constituent, in my sense."
"Moreover, my lower-level prosodic constituents could be identified with the ϕ-phrases[REF_CITE], which are grouped together to make IPs."
I shall follow standard assumptions in HPSG by separating the phonology attribute out from syntax-semantics ( SYNSEM ): &quot; # (4) feat-struc !
PHON pros SYNSEM synsem
"The type of value of PHON is pros (i.e., prosody)."
"In this paper, I am going to take word forms as phonologically simple."
This means that the prosodic type of word forms will be maximal in the hierarchy.
The only complex prosodic objects will be metrical trees.
"The minimum requirements for these are that we have, first, a way of representing nested prosodic domains, and second, a way of marking the strong element (Designated Terminal Element; DTE) in a given domain."
"Before elaborating the prosodic signature further, I need to briefly address the prosodic status of monosyllabic function words in English."
"Although these are sometimes classified as clitics,[REF_CITE]proposes the term Leaners."
"These “form a rhythmic unit with the neighbouring material, are normally unstressed with respect to this material, and do not bear the intonational peak of the unit."
"English articles, coordinating conjunctions, complementizers, relative markers, and subject and object pronouns are all leaners in this sense” ([REF_CITE]p5)."
"Zwicky takes pains to differentiate between Leaners and clitics; the former combine with neighbours to form Phonological Phrases (with juncture characterized by external sandhi), whereas clitics combine with their hosts to form Phonological Words (where juncture is characterized by internal sandhi)."
"Since Leaners cannot bear intonational peaks, they cannot act as the DTE of a metrical tree."
"Consequently, the value of the attribute DTE in a metrical tree must be the type of all prosodic objects which are not Leaners."
"I call this type full, and it subsumes both Prosodic Words (of type p-wrd) and metrical trees (of type mtr)."
"Moreover, since Leaners form a closer juncture with their neighbours than Prosodic Words do, we distinguish two kinds of metrical tree."
"In a tree of type full-mtr, all the daughters are of type full, whereas in a tree of type lnr-mtr, only the DTE is of type full."
"In terms of the attribute-value logic, we therefore postulate a type mtr of metrical tree which introduces the feature DOM (prosodic domain) whose value is a list of prosodic elements, and a feature DTE whose value is a full prosodic object: 



"
Fig 2 displays the prosodic signature for the grammar.
"The types lnr-mtr and full-mtr specialise the appropriateness conditions on mtr, as discussed above."
"Notice that in the constraint for objects of type lnr-mtr, is the operation of appending two lists."
"Since elements of type pros can be word-forms or metrical trees, the DOM value in a mtr can, in principle, be a list whose elements range from simple word-forms to lists of any level of embedding."
One way of interpreting this is to say that DOM values need not obey the Strict Layer Hypothesis (briefly mentioned in Section 2.1 above).
"To illustrate, a sign whose phonology value corresponded to the metrical tree (6) (where the word this receives narrow focus) would receive the representation in Fig 3."
"In this section, I will address the way in which prosodic constituents can be constructed in parallel with syntactic ones."
"There are two, orthogonal, dimensions to the discussion."
The first is whether the syntactic construction in question is head-initial or head-final.
The second is whether any of the constituents involved in the construction is a Leaner or not.
"I will take the first dimension as primary, and introduce issues about Leaners as appropriate."
"The approach which I will present has been implemented in ALE[REF_CITE], and although I will largely avoid presenting the rules in ALE notation, I have expressed the operations for building prosodic structures so as to closely reflect the relational constraints encoded in the ALE grammar."
"As far as head-initial constructions are concerned, I will confine my attention to syntactic constituents which are assembled by means of HPSG’s Head-"
"Complement Rule[REF_CITE], illustrated in Fig 4."
"The ALE rendering of the rule is given in (7). (7) head_complement rule (phrase, phon:MoPhon, synsem:(comps:[], spr:S, head:Head)) ===&gt; cat&gt; (word, phon:HdPhon, synsem:(comps:Comps, spr:S, head:Head)), cats&gt; Comps, goal&gt; (getPhon(Comps, PhonList), mkMtr([HdPhon|PhonList], MoPhon))."
"The function mkMtr (make metrical tree) (encoded as a relational constraint in (7)) takes a list consisting of all the daughters’ phonologies and builds an appropriate prosodic object ϕ. As the name of the function suggests, this prosodic object is, in the general case, a metrical tree."
"However, since metrical trees are relational (i.e., one node is stronger than the others), it makes no sense to construct a metrical tree if there is only a single daughter."
"In other words, if the head’s COMPS list is empty, then the argument mkMtr is a singleton list containing only the head’s PHON value, and this is returned unaltered as the function value."
"The general case requires at least the first two elements on the list of prosodies to be of type full, and builds a tree of type full mtr."
"Note that the domain of the output tree is the input list, and the DTE is just the right-hand element of the domain. (10) shows the constraint in ALE notation; the relation rhd DTE/2 simply picks out the last element of the list L. (10) mkMtr(([full, full|_], L), (full_mtr, dom:L, dte:X)) if rhd_DTE(L, X)."
Examples of the prosody constructed for an N-bar and a VP are illustrated in (11)–(12).
"For convenience, I use [of the samurai] to abbreviate the AVM representation of the metrical tree for of the samurai, and similarly for [a cloak] and [at the collar]."
Let’s now briefly consider the case of a weak pronominal NP occurring within a VP.
Pronominal NPs can only form prosodic phrases in their own right if they bear accent; unaccented pro-nominals must combine with a host to be admissible.
Zwicky’s constraints on when this combination can occur are as follows: (15) A personal pronoun NP can form a prosodic phrase with a preceding prosodic host only if the following conditions are satisfied: a. the prosodic host and the pronominal NP are sisters; b. the prosodic host is a lexical category; c. the prosodic host is a category that governs case marking.
"These considerations motivate a third clause to the definition of mkMtr : (16) mkMtr ( h 1 [p-wrd], 2 [lnr]  3 3 ) = mkMtr ( h 264 lnr-mtr 3 i ) h 1 , 2 i 75 DOM DTE 1"
"That is, if the first two elements of the list are a Prosodic Word and a Leaner, then the two of them combine to form a lnr-mtr, followed by any other material on the input list."
"Because of the way in which this prosodic constraint is associated with the Head-Complement Rule, the prosodic host in (16), namely the p-wrd tagged 1 , is automatically the syntactic head of the construction."
"As a result, Zwicky’s conditions in (15) fall out directly. (17)–(18) illustrate the effects of the new clause."
"In the first case, the lnr-mtr consisting of told and it is the only item on the list in the recursive call to mk[REF_CITE], and hence the base clause (8) in the definition of mkMtr applies."
"In the second case, there is more than one item on the list, and the lnr-mtr becomes a subtree in a larger metrical domain. (17) 2 mkMtr ([told, it]) = 3 lnr-mtr D E7"
"By contrast, examples of the form told Noel ǐt fail to parse, since (16) only licenses a head-initial lnr-mtr when the Leaner immediately follows the head."
"We could however admit told Noel ít, if the lexicon contained a suitable entry for accent-bearing ít with prosody of type p wrd, since this would satisfy the requirement that only prosodies of type full can be the value of a metrical tree’s DTE."
"To illustrate head-final constructions, I will focus on NP structures, considering the combination of determiners and prenominal adjectives with N-bar phrases."
I take the general case to be illustrated by combining a determiner like this with a phrase like treasured possession to form one metrical tree.
"Since treasured possession will itself be a metrical tree, I introduce a new, binary, function for this purpose, namely extMtr (extend metrical tree) which adds a new prosodic element to the left boundary of an existing tree."
"For convenience, I will call the leftmost argument of extMtr the extender."
"Fig 5 illustrates the way in which extMtr is used to build the prosody of a specifier-head construction, while (19) provides the definition of extMtr ."
An example of the output is illustrated in (20).
"However, there are now a number of special cases to be considered."
"First, we have to allow that the head phrase is a single Prosodic Word such as possession, rather than a metrical tree."
"Second, the prosodic structure to be built will be more complex if the head phrase itself contains a post-head complement, as in treasured possession of the samurai."
"Crosscutting this dimension is the question of whether the extender is a Leaner, in which case it will form a lnr-mtr with the immediately following element."
We will look at these cases in turn. (i)
"The head is a single Prosodic Word When the second prosodic argument of extMtr is not in fact a metrical tree, it calls mkMtr to build a new metrical tree."
The head contains post-head material Perhaps the most awkward kind of mismatch between syntactic and prosodic structure arises when when the comple-ment or postmodifier of a syntactic head is ‘promoted’ to the level of sister of the constituent in which the head occurs; this creates a disjuncture between the lexical head and whatever follows.
"Fig 6 gives a typical example of this phenomenon, where the noun possession is followed by a prepositional complement, while Fig 7 represents the prosodic constituency."
Let’s consider how treasured should combine with possession of the samurai.
The Head-Complement Rule will have built a prosodic structure of the form [possession [of the samurai]] for the latter phrase.
"To obtain the correct results, we need to be able to detect that this is a metrical tree M whose leftmost element is a lexical head (by contrast, for example, with the structure [treasured possession])."
"In just this case, the extender can not only extend M but also create a new subtree by left-associating with the lexical head. [Footnote_2]"
"2 The special prosodic status of lexical heads is incorpor-ated in Selkirk’s (1981) notion of ϕ-phrase, and subsequent developments thereof, such[REF_CITE]."
"The required definition is shown in (23) and illustrated in example (24). &quot; # DOM [Footnote_2] p-wrd 3 (23) extMtr ( 1 [full], ) = DTE 4"
"2 The special prosodic status of lexical heads is incorpor-ated in Selkirk’s (1981) notion of ϕ-phrase, and subsequent developments thereof, such[REF_CITE]."
"Turning back briefly to the Head-Specifier Rule shown in Fig 5, we can now see that if ϕ 0 is a metrical tree M, then the value of extMtr (ϕ 1 ; ϕ 0 ) depends on the syntactic information associated with the leftmost element P of that tree."
"That is, if P is the phonology of the lexical head of the phrase, then it can be prosodically disjoined from the following material, otherwise the metrical tree M is extended in the standard way."
There are various ways that this sensitivity to syntactic role might be accommodated.
One option would to inspect the DTRS (daughters) attribute of a sign.
"However, I will briefly sketch the treatment implemented in the ALE grammar, which does not build a representation of daughters."
"Instead, I have introduced an attribute LEX inside the value of HEAD which is constrained in the case of lexical items to be token-identical to the PHON value."
"For example, the type for possession is approximately as follows: 2 3"
"Since LEX is a head feature, it percolates up to any phrase projected from that head, and allows the PHON value of the lexical head to be accessed at that projection; i.e., headed phrases will also bear a specification [ LEX phon], which can be interpreted as saying “my lexical head’s phonology value is phon”."
"In addition, we let the function extMtr in Fig 5 take as an extra argument the HEAD value of the mother, and then test whether the leftmost Prosodic Word in the metrical tree being extended is the same as the LEX value of the mother’s HEAD value. (iii) Extending the head with a Leaner Finally, there is an additional clause to accommodate the case where the extending element is a Leaner."
"This triggers a kind of left association, in that the result of combining a with [treasured possession] is a structure of the form [[a treasured] possession]. (26) extMtr ( 1 [lnr], DOM h 2  3 # ) = &quot;"
"This will also allow an unaccented subject pronoun to left-associate with the lexical head of a VP, as in [[he provoked] [the objections of everyone]][REF_CITE]."
"I believe that the preceding analysis demonstrates that despite the well-known mismatches between syntactic and prosodic structure, it is possible to induce the required prosodic structures in tandem with syntax."
"Moreover, the analysis retains rather conventional notions of syntactic constituency, eschewing the non-standard syntactic constituents advocated[REF_CITE], Steedman (1990; 1991)."
"Although I have only mentioned two syntactic rules in HPSG, the radically underspecified nature of these rules, coupled with rich lexical entries, means that the approach I have sketched has more generality than might appear at first."
"With the addition of a rule for prenominal adjectives, prosodically interpreted like the Head-Specifier Rule, we can derive a range of analyses as summarised in (27)."
"Here, I use square brackets to demarcate trees of type full-mtr and parentheses for trees of type lnr-mtr. (27) a. [this possession](of the samurai) b. [this treasured possession](of the samurai) c. (a treasured) possession d. (a treasured) possession [(of these) people] e. Kim gave (the book) (to the boy) f. Kim (gave it) (to the boy) g. Kim is happy [about Lee] h. Kim is happy [(that Lee) is fond (of the bird)] i. Kim wanted (to rely) (on the report) [(that Lee) is fond (of the bird)]"
"It would be straightforward to augment the grammar to accommodate post-modifiers of various kinds, which would behave prosodically like post-head complements."
"By contrast, auxiliaries do not conform to the association between headed structures and prosodic structures that we have seen so far."
"That is, if auxiliaries are a subtype of complement-taking verbs, as assumed within HPSG, then they depart from the usual pattern in behaving prosodically like specifiers rather than heads."
There are numerous directions in which the current work can be extended.
"In terms of empirical coverage, a more detailed account of weak function words seems highly desirable."
"The approach can also be tested within the context of speech synthesis, and preliminary work is underway on extending the Festival system[REF_CITE]to accept input text marked up with metrical trees of the kind presented here."
"In the longer term, the intention is to integrate prosodic realisation within the framework of an HPSG-based concept-to-speech system."
An approach to automatic detection of syllable structure is presented.
"We demonstrate a novel application of EM-based clustering to multivariate data,  by the induction of 3- and 5-dimensional probabilis-tic syllable classes."
The qualitative evaluation shows that the method yields phonologically meaningful syl-lable classes.
We then propose a novel approach to grapheme-to-pho-neme conversion and show that syl-lable structure represents valuable information for pronunciation sys-tems.
In this paper we present an approach to un-supervised learning and automatic detection of syllable structure.
The primary goal of the paper is to demonstrate the application of EM-based clustering to multivariate data.
The suitability of this approach is  by the induction of 3- and 5-dimensional prob-abilistic syllable classes.
A secondary goal is to outline a novel approach to the conversion of graphemes to phonemes (g2p) which uses a context-free grammar (cfg) to generate all se-quences of phonemes corresponding to a given orthographic input word and then ranks the hypotheses according to the probabilistic in-formationOur approachcoded buildsin the onsyllabletwo resourcesclasses. .
The  resource is a cfg for g2p conversion that was constructed manually by a linguistic ex-pert[REF_CITE].
"The grammar describes how words are composed of syllables and how syllables consist of parts that are convention-ally called onset, nucleus and coda, which in turn are composed of phonemes, and corre-sponding graphemes."
The second resource consists of a multivariate clustering algorithm that is used to reveal syllable structure hid-den in unannotated training data.
"In a  step, we collect syllables by going through a large text corpus, looking up the words and their  in a pronunciation dictio-nary and counting the occurrence frequencies of the syllable types."
Probabilistic syllable classes are then computed by applying max-imum likelihood estimation from incomplete data via the EM algorithm.
"Two-dimensional EM-based clustering has been applied to tasks in syntax[REF_CITE], but so far this approach has not been used to derive models of higher dimensionality and, to the best of our knowledge, this is the  time that it is being applied to speech."
"Accordingly, we have trained 3- and 5-dimensional models for English and German syllable structure. tureThewereobtainedevaluatedmodelsin threeof syllableways."
"Firstlystruc-, the 3-dimensional models were subjected to a pseudo-disambiguation task, the result of which shows that the onset is the most vari-able part of the syllable."
"Secondly, the re-sulting syllable classes were qualitatively eval-uated from a phonological and phonotactic point of view."
"Thirdly, a 5-dimensional syl-lable model for German was tested in a g2p conversion task."
"The results compare well with the best currently available data-driven approaches to g2p conversion (e.g.,[REF_CITE]) and suggest that syllable struc- ture represents valuable information for pro-nunciation systems."
"Such systems are critical components in text-to-speech (TTS) conver-sion systems, and they are also increasingly used to generate pronunciation variants in au-tomatic speech recognition. lowsThe."
InrestSectionof the 2paperwe introduceis organizedthe asmulti-fol-variate clustering algorithm.
In Section 3 we present four experiments based on 3- and 5-dimensional data for German and English.
Section 4 is dedicated to evaluation and in Section 5 we discuss our results.
EM-based clustering has been derived and ap-plied to syntax[REF_CITE].
"Unfor-tunately, this approach is not applicable to multivariate data with more than two dimen-sions."
"However, we consider syllables to con-sist of at least three dimensions correspond-ing to parts of the internal syllable structure: onset, nucleus and coda."
We have also experi-mented with 5-dimensional models by adding two more dimensions: position of the sylla-ble in the word and stress status.
"In our multivariate clustering approach, classes cor-responding to syllables are viewed as hidden data in the context of maximum likelihood es-timation from incomplete data via the EM al-gorithm."
"The two main tasks of EM-based clustering are (i) the induction of a smooth probability model on the data, and (ii) the automatic discovery of class structure in the data."
Both aspects are considered in our ap-plication.
We aim to derive a probability distribution p ( y ) on syllables y from a large sample.
"The key idea is to view y as condi-tioned on an unobserved class c 2 C , where the classes are given no prior interpretation."
The probability of a syllable y = ( y 1 ; ::; y d ) 2 Y 1 :: Y d ; d X 3 ; is  X as: p ( y ) = p ( c; y ) = p ( c ) p ( yjc ) c 2 C c 2 C X Y d = p ( c ) p ( y i jc ) c 2 C i =1
Note that conditioning of y i on each other is solely made through the classes c Q via the in-dependence assumption p ( yjc ) = di =1 p ( y i jc ) .
This assumption makes clustering feasible in the  place; later on (in Section 4.1) we will experimentally determine the number jCj of classes such that the assumption is opti-mally met.
The EM algorithm[REF_CITE]is directed at maximizing P the incom-plete data log-likelihood L = y p ~( y ) ln p ( y ) as a function of the probability distribution p for a given empirical probability distribu-tion p ~ .
"Our application is an instance of the EM-algorithm for context-free models[REF_CITE], from which simple re-estimation formulae can be derived."
"Let f ( P y ) the fre-quency of syllable y , and jfj = y 2Y f ( y ) the total frequency of the sample (i.e. ~ p ( y ) = f ( y ) ), and f c ( y ) = f ( y ) p ( cjy ) the estimated frequency j of f j y annotated with c ."
Parameter updates p ^( c ) ; p ^( y i jc ) can thus be computed by ( c 2 C; y i 2 Y i ; i = P 1 ; ::; d ): y 2 Y f c ( y ) p ^( c ) = ; and jfj
P y 2Y 1 ::  i P ; 1  i y  i +1 ::  d f c ( y ) p ^ ( y i jc ) = y 2Y f c ( y )
"As shown[REF_CITE], every such maximization step increases the log-likelihood function L , and a sequence of re-estimates eventually converges to a (local) maximum."
A sample of syllables serves as input to the multivariate clustering algorithm.
"The Ger-man data were extracted from the Stuttgarter Zeitung (STZ), a newspaper corpus of about 31 million words."
"The English data came from the British National Corpus (BNC), a col-lection of written and spoken language con-taining about 100 million words."
"For both languages, syllables were collected by going through the corpus, looking up the words and their  in a pronunciation dictio-nary[REF_CITE]1 and counting the occurrence frequencies of the syllable types 2 ."
"In two experiments, we induced 3-dimensional models based on syllable onset, nucleus, and coda."
The number of syllable classes was system-atically varied in iterated training runs and ranged from 1 to 200. #0Figurefrom a[Footnote_1] 3-dimensionalshows a selected[REF_CITE]classes.
"1 We slightly the English pronunciation lexicon to obtain non-empty nuclei, e.g. /ideal-ism/ [ aI ][ dI@ ][ lIzm, ] was to [ aI ][ dI@ ][ lI ][ z@m ] (SAMPA 2 Subsequenttranscriptionexperiments). on syllable types[REF_CITE]have shown that frequency counts repre-sent valuable information for our clustering task."
The  column displays the class index 0 and the class probability p (0) .
"The most probable onsets and their probabilities are listed in descending order in the second column, as are nucleus and coda in the third and fourth columns, respectively."
Empty on-sets and codas were labeled  [ nucleus ] .
"Class #0 contains the highly frequent func-tion words in, is, it, its as well as the  -ing ,-ting, -ling ."
"Notice that these function words and  appear to be separated in the 5-dimensional model (classes #[Footnote_1] and #3 in Figure 3). dimensionalIn two furthermodelsexperiments, augmented, webyinducedthe addi-5-tional parameters of position of the syllable in the word and stress status."
"1 We slightly the English pronunciation lexicon to obtain non-empty nuclei, e.g. /ideal-ism/ [ aI ][ dI@ ][ lIzm, ] was to [ aI ][ dI@ ][ lI ][ z@m ] (SAMPA 2 Subsequenttranscriptionexperiments). on syllable types[REF_CITE]have shown that frequency counts repre-sent valuable information for our clustering task."
"Syllable position has four values: monosyllabic (ONE), initial (INI), medial (MED), and  (FIN)."
Stress has two values: stressed (STR) and unstressed (USTR).
The number of syllable classes ranged from 1 to 200.
Figure 2 illustrates (part of) class #46 from a 5-dimensional German model with 50 classes.
Syllable position and stress are dis-played in the last two columns.
"In the following sections, (i) the 3-dimen-sional models are subjected to a pseudo-disambiguation task (4.1); (ii) the syllable classes are qualitatively evaluated (4.2); and (iii) the 5-dimensional syllable model for Ger-man is tested in a g2p task (4.3)."
"We evaluated our 3-dimensional cluster-ing models on a pseudo-disambiguation task similar to the one described[REF_CITE], but  to onset, nucleus, and coda ambiguity."
The  task is to judge which of two onsets on and on 0 is more likely to appear in the context of a given nucleus n and a given coda cod .
"For this purpose, we constructed an evaluation cor-pus of 3000 syllables ( on; n; cod ) selected from the original data."
"Then, randomly chosen on-sets on 0 were attached to all syllables in the evaluation corpus, with the resulting syllables ( on 0 ; n; cod ) appearing neither in the training nor in the evaluation corpus."
"Furthermore, the elements on; n; cod , and on 0 were required toClusteringbe part of themodelstrainingwerecorpusparameterized. in (up to 10) starting values of EM-training, in the number of classes of the model (up to 200), resulting in a sequence of 10 20 mod-els."
Accuracy was calculated as the number of times the model decided p ( on;n;cod ) p ( on 0 ;n;cod ) for all choices made.
Two simi-lar tasks were designed for nucleus and coda. shownResultsin forFigurethe4.best[REF_CITE]classesare show the highest accuracy rates.
For German we reached accuracy rates of 88-90% (nucleus and coda) and 77% (onset).
"For English we achieved accuracy rates of 92% (coda), 84% (nucleus), and 76% (onset)."
"The results of the pseudo-disambiguation agree with intu-ition: in both languages (i) the onset is the most variable part of the syllable, as it is easy to  minimal pairs that vary in the onset, (ii) it is easier to predict the coda and nucleus, as their choice is more restricted."
"The following discussion is restricted to the 5-dimensional syllable models, as the quality of the output increased when more dimensions were added."
We can look at the results from  angles.
"For instance, we can verify if any of the classes are mainly representa-tives of a syllable class pertinent to a par-ticular nucleus (as it is the case with the 3-dimensional models)."
"Another interesting as-pect is whether there are syllable classes that represent parts of lexical content words, as op-posed to high-frequency function words."
"Fi-nally, some syllable classes may correspond to productive ."
The majority of syllable classes obtained for German is dominated by one par-ticular nucleus per syllable class.
"The only syllable nuclei that do not dominate any class are the front rounded vowels /y:, Y, 2:, 9/, the front vowel /E:/ and the diphthong /OY/, all of which are among the least frequently occurring nuclei in the lex-icon of German."
Figure 5 depicts the classes that will be discussed now. areAlmostrepresentativesone thirdof(high-frequency28%) of the 50functionclasses words.
"For example, class #7 is dominated by the function words in, ich, ist, im, sind, sich , all of which contain the short vowel /I/. syllables[REF_CITE]%areofmostthe likely50 classesto occurrepresentsin ini-tial, medial and  positions in the open word classes of the lexicon, i.e. nouns, ad-jectives, and verbs."
Class #4 covers several lexical entries involving the diphthong /aI/ mostly in stressed word-initial syllables.
"Class #40 provides complimentary information, as it also includes syllables containing /aI/, but here mostly in word-medial position. sentWeproductivealso observesyllable(e.classesg., ver- that , er-, repre- zer-, vor-, her- in class #26) and  (e.g., -lich, -ig in class #34)."
"Finally, there are two syllable classes (not displayed) that cover the most common   involv-ing the vowel /@/ (schwa). theClassclassesnumbersare rankedare informativeby decreasinginsofarproba-as bility."
Lower-ranked classes tend (i) not to be dominated by one nucleus; (ii) to contain vowels with relatively low frequency of occur-rence; and (iii) to yield less clear patterns in terms of word class or stress or position.
"For illustration, class #46 (Figure 2) represents the syllable ent [ Ent ] , both as a  (INI) and as a  (FIN), the former being un-stressed (as in Entwurf ) and the lat-ter English stressed . ([REF_CITE]nout Dirigent of the 50 syllable classes). obtained for English one dominant nucleus per syllable class is observed."
In all of these cases the probability of the nucleus is larger than 99% and in 7 classes the nucleus probability is 100%.
"Besides several diphthongs only the rel-atively infrequent vowels /V/, /A:/ and /3:/ do not dominate any class."
Figure 3 shows the classes that are described as follows. sented[REF_CITE]syllablefunctionclasseswords.
"Forareexamplerepre-, class #0 and #17 and are dominated by the determiners the and a , respectively, and class #1 contains function words that involve the short vowel /I/, such as in, is, it, his, if, its . inProductiveclass #3 ( -ing word-forming), and commonare found  in class #4 ( -er, -es, -ed )."
"Class #10 is particularly interesting in that it represents a comparably large number of common suf- , such as -tion, -ment, -al, -ant, -ent, -ence and others.[REF_CITE]containsmajoritysyllablesof syllablethatclassesare,likelyviz. 31tooutbe found in initial, medial and  positions in the open word classes of the lexicon."
"For ex-ample, class #14 represents mostly stressed syllables involving the vowels /eI, A:, e:, O:/ and others, in a variety of syllable positions in nouns, adjectives or verbs."
"In this section, we present a novel method of g2p conversion (i) using a cfg to produce all possible phonemic correspondences of a given grapheme string, (ii) applying a prob-abilistic syllable model to rank the pronunci-ation hypotheses, and (iii) predicting pronun-ciation by choosing the most probable anal-tionsysis. ,Webecauseused agrammarscfg for generatingare expressivetranscrip-and writingOur grammargrammar-rulesdescribesishoweasywordsand intuitiveare com-. posed of syllables and syllables branch into onset, nucleus and coda."
"These syllable parts are re-written by the grammar as sequences of natural phone classes, e.g. stops, frica-tives, nasals, liquids, as well as long and short vowels, and diphthongs."
The phone classes are then re-interpreted as the individ-ual phonemes that they are made up of.
"Fi-nally, for each phoneme all possible graphemic correspondences are listed. 100Figure) of the6 illustratesGerman wordtwo Lötzinn analyses ( ( tin out sol- of der) ."
The phoneme strings (represented by non-terminals named =... ) and the syllable boundaries (represented by the non-terminal ) can be extracted from these analyses.
Figure 6 depicts both an incor-rect analysis [l2:ts][i:n] and its correct coun-terpart [l2:t][tsIn].
The next step is to rank these transcriptions by assigning probabilities to them.
The key idea is to take the prod-uct of the syllable probabilities.
Using the 5-dimensional 3 German syllable model yields a probability of 7 : 5 10 ; 7 3 : 1 10 ; 7 = 2 : 3 10 ; 13 for the incorrect analysis and a probability of 1 : 5 10 ; 7 6 : 5 10 ; 6 = 9 : 8 10 ; 13 for the correct one.
Thus we achieve the desired result of as-signing the higher probability to the correct transcription.[REF_CITE]valuatedunseen ourwordsg2p. systemThe ambiguityon a test ex-set pressed as the average number of analyses per word was 289.
"The test set was constructed by collecting 295,102 words from the German Celex dictionary[REF_CITE]that were not seen in the STZ corpus."
"From this set we manually eliminated (i) foreign words, (ii) acronyms, (iii) proper names, (iv) verbs, and (v) words with more than three syllables."
The resulting test set is available on the World Wide Web 4 . systemsFigure.
"The7 showssecondtheandperformancefourth columnsof fourshowg2p the accuracy of two baseline systems: g2p con-version using the 3- and 5-dimensional em-pirical distributions (Section 2), respectively."
"The third and  columns show the word accuracy of two g2p systems using 3- and 5-dimensional syllable models, respectively. bleThemodelsg2p systemachievedusingthe highest5-dimensionalperformancesylla- (75.3%), which is a gain of 3% over the per-formance of the 5-dimensional baseline system and a gain of 8% over the performance of the [Footnote_3]-dimensional models 5 ."
"3 Position can be derived from the cfg analyses, stress placement is controlled by the most likely dis-tribution 4[URL_CITE]"
"We have presented an approach to unsuper-vised learning and automatic detection of syl-lable structure, using EM-based multivariate clustering."
The method yields phonologically meaningful syllable classes.
"These classes are shown to represent valuable input information in a g2p conversion task. dimensionalIn contrastEM-basedto the clusteringapplicationtoofsyntaxtwo-[REF_CITE], where semantic rela-tions were revealed between verbs and objects, the syllable models cannot a priori be ex-pected to yield similarly meaningful proper-ties."
"This is because the syllable constituents (or phones) represent an inventory with a small number of units which can be combined to form meaningful larger units, viz. mor-phemes and words, but which do not them-selves carry meaning."
"Thus, there is no reason why certain syllable types should occur signif-icantly more often than others, except for the fact that certain morphemes and words have a higher frequency count than others in a given text corpus."
"As discussed in Section 4.2, how-ever, we do  some interesting properties of syllable classes, some of which apparently represent high-frequency function words and productive , while others are typically found in lexical content words."
"Subjected to a pseudo-disambiguation task (Section 4.1), the 3-dimensional models  the intu-ition that the onset is the most variable part of the syllable. dimensionalIn a feasibilitysyllablestudymodelweobtainedappliedfortheGer-5-man to a g2p conversion task."
"Automatic conversion of a string of characters, i.e. a word, into a string of phonemes, i.e. its pro-nunciation, is essential for applications such as speech synthesis from unrestricted text in-put, which can be expected to contain words that are not in the system&apos;s pronunciation dictionary or otherwise unknown to the sys-tem."
The main purpose of the feasibility study was to demonstrate the relevance of the phonological information on syllable structure for g2p conversion.
"Therefore, information and probabilities derived from an alignment of grapheme and phoneme strings, i.e. the lowest two levels in the trees displayed in Fig-ure 6, was deliberately ignored."
Data-driven pronunciation systems usually rely on training data that include an alignment of graphemes and phonemes.
"In our experiment, with training on unannotated text corpora and without an alignment of graphemes and phonemes, we ob-tained a word accuracy rate of 75.3% for the [Footnote_5]-dimensional German syllable model. systemsComparisonis of this: (performancei) hardly anywithquantita-other tive g2p performance data are available for German; (ii) comparisons across languages are hard to interpret; (iii) comparisons across dif-ferent approaches require cautious interpreta-tions."
5 45 resp. 95 words could not be disambiguated byThethereported3- resp.relatively5-dimensionalsmall empiricalgains candistributionsbe explained. by the fact that our syllable models were applied only to this small number of ambiguous words.
The most direct point of comparison is the method presented[REF_CITE].
"In one of her experiments, the standard prob-ability model was applied to the hand-crafted cfg presented in this paper, yielding 42% word accuracy as evaluated on our test set."
Run-ning the test set through the pronunciation rule system of the IMS German Festival TTS system[REF_CITE]resulted in 55% word accuracy.
The Bell Labs German TTS sys-tem[REF_CITE]performed at better than 94% word accuracy on our test set.
This TTS system relies on an annotation of morpho-logical structure for the words in its lexicon and it performs a morphological analysis of unknown words[REF_CITE]; the pronun-ciation rules draw on this structural infor-mation.
"These comparative results emphasize themationvalueonofsyllablephonotacticstructureknowledgeand morphologi-and infor-cal structure for g2p conversion. accuracyIn a comparisonrate of 75.3across% forlanguagesour 5-dimensional, a word German syllable model is slightly higher than the best data-driven method[REF_CITE]%[REF_CITE]."
"Recently,[REF_CITE]has reported a word accuracy of 92.6% for Dutch, using a `lazy&apos; training strategy on data aligned with the correct phoneme string, and a hand-crafted system that relied on a large set of rule templates and a many-to-one mapping of characters to graphemes preceding the actual g2p conversion. tionWeofarephonological thatinformationa judiciousof combina-the type employed in our feasibility study with stan-dard techniques such as g2p alignment of training data will produce a pronunciation system with a word accuracy that matches the one reported[REF_CITE]."
"We be-lieve, however, that for an optimally perform-ing system as is desired for TTS, an even more complex design will have to be adopted."
"In many languages, including English, Ger-man and Dutch, access to morphological and phonological information is required to reli-ably predict the pronunciation of words; this view is further evidenced by the performance of the Bell Labs system, which relies on pre-cisely this type of information."
"We agree with Sproat (1998, p. 77) that it is unrealistic to ex-pect optimal results from a system that has no access to this type of information or is trained on data that are  for the task."
Pitch accent placement is a major topic in intonational phonology re-search and its application to speech synthesis.
What factors in uence whether or not a word is made intonationally prominent or not is an open question.
"In this paper, we investigate how one aspect of a word&apos;s local context | its colloca-tion with neighboring words | in u-ences whether it is accented or not."
Results of experiments on two tran-scribed speech corpora in a medical domain show that such collocation information is a useful predictor of pitch accent placement.
"In English, speakers make some words more intonationally prominent than others."
These words are said to be accented or to bear pitch accents .
"Accented words are typically louder and longer than their unaccented coun-terparts, and their stressable syllable is usu-ally aligned with an excursion in the funda-mental frequency ."
This excursion will di er in shape according to the type of pitch ac-cent.
"Pitch accent type, in turn, in uences listeners&apos; interpretation of the accented word or its larger syntactic constituent."
"Previous research has associated pitch accent with vari-ation in various types of information status, including the given/new distinction, focus , and contrastiveness , inter alia."
"Assigning pitch ac-cent in speech generation systems which em-ploy speech synthesizers for output is thus crit-ical to system performance: not only must one convey meaning naturally, as humans would, but one must avoid conveying mis -information which reliance on the synthesizers&apos; defaults may result in."
The speech generation work discussed here is part of a larger e ort in developing an intel-ligent multimedia presentation generation sys-tem called MAGIC (Medical Abstract Gen-eration for Intensive Care)[REF_CITE].
"In MAGIC, given a patient&apos;s medical record stored at Columbia Presbyterian Medi-cal Center (CPMC)&apos;s on-line database system, the system automatically generates a post-operative status report for a patient who has just undergone bypass surgery."
"There are two media-speci c generators in MAGIC: a graph-ics generator which automatically produces graphical presentations from database entities, and a spoken language generator which auto-matically produces coherent spoken language presentations from these entities."
The graph-ical and the speech generators communicate with each other on the y to ensure that the nal multimedia output is synchronized.
"In order to produce natural and coherent speech output, MAGIC&apos;s spoken language gen-erator models a collection of speech features, such as accenting and intonational phrasing, which are critical to the naturalness and intel-ligibility of output speech."
"In order to assign these features accurately, the system needs to identify useful correlates of accent and phrase boundary location to use as predictors."
This work represents part of our e orts in identi-fying useful predictors for pitch accent place-ment.
"Pitch accent placement has long been a re-search focus for scientists working on phonol-ogy, speech analysis and synthesis ([REF_CITE];"
"In general, syntactic fea-tures are the most widely used features in pitch accent predication."
"For example, part-of-speech is traditionally the most useful sin-gle pitch accent predictor[REF_CITE]."
"Function words, such as prepositions and ar-ticles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented."
"Other lin-guistic features, such as inferred given/new status[REF_CITE], con-trastiveness[REF_CITE], and discourse structure[REF_CITE], have also been ex-amined to explain accent assignment in large speech corpora."
"In a previous study[REF_CITE], we investigated how features such as deep syntac-tic/semantic structure and word informative-ness correlate with accent placement."
"In this paper, we focus on how local context in uences accent patterns."
"More speci cally, we investi-gate how word collocation in uences whether nouns are accented or not."
"Determining which nouns are accented and which are not is challenging, since part-of-speech information cannot help here."
"So, other accent predictors must be found."
There are some advantages in looking only at one word class.
"We eliminate the interaction between part-of-speech and collocation, so that the in-uence of collocation is easier to identify."
"It also seems likely that collocation may have a greater impact on content words, like nouns, than on function words, like prepositions."
Previous researchers have speculated that word collocation a ects stress assignment of noun phrases in English.
"For example,[REF_CITE]notes how familiar colloca-tions change their stress, witness the American pronunciation of `Little House&apos; [in the televi-sion series Little House on the Prairie], where stress used to be on HOUSE, but now, since the series is so familiar, is placed on the LITTLE."
"That is, for collocated words, stress shifts to the left element of the compound."
"However, there are numerous counter-examples: con-sider apple PIE , which retains a right stress pattern, despite the collocation."
"So, the ex-tent to which collocational status a ects ac-cent patterns is still unclear."
"Despite some preliminary investigati[REF_CITE], word colloca-tion information has not, to our knowledge, been successfully used to model pitch accent assignment; nor has it been incorporated into any existing speech synthesis systems."
"In this paper, we empirically verify the usefulness of word collocation for accent prediction."
"In Sec-tion 2, we describe our annotated speech cor-pora."
"In Section 3, we present a description of the collocation measures we investigated."
Sec-tion 4 to 7 describe our analyses and machine learning experiments in which we attempt to predict accent location.
In Section 8 we sum up our results and discuss plans for further re-search.
"From the medical domain described in Section 1, we collected two speech corpora and one text corpus for pitch accent modeling."
"The speech corpora consist of one multi-speaker sponta-neous corpus, containing twenty segments and totaling fty minutes, and one read corpus of ve segments, read by a single speaker and to-taling eleven minutes of speech."
The text cor-pus consists of 3.5 million words from 7375 dis-charge summaries of patients who had under-gone surgery.
"The speech corpora only cover cardiac patients, while the text corpus covers a larger group of patients and the majority of them have also undergone cardiac surgery."
"The speech corpora were rst transcribed or-thographically and then intonationally, using the ToBI convention for prosodic labeling of standard American English[REF_CITE]."
"For this study, we used only binary ac-cented/deaccented decisions derived from the ToBI tonal tier, in which location and type of pitch accent is marked."
"After ToBI label-ing, each word in the corpora was tagged with part-of-speech, from a nine-element set: noun, verb, adjective, adverb, article, conjunction, pronoun, cardinal, and preposition."
The spon-taneous corpus was tagged by hand and the read tagged automatically.
"As noted above, we focus here on predicting whether nouns are accented or not."
"We used three measures of word collocation to examine the relationship between collocation and accent placement: word bigram pre-dictability , mutual information , and the Dice coefficient ."
"While word predictabil-ity is not typically used to measure collocation, there is some correlation between word collo-cation and predictability."
"For example, if two words are collocated, then it will be easy to predict the second word from the rst."
"Sim-ilarly, if one word is highly predictable given another word, then there is a higher possibility that these two words are collocated."
Mutual informati[REF_CITE]and the Dice coeÆ-cient[REF_CITE]are two standard measures of collocation.
"In general, mutual information measures uncertainty reduction or departure from independence."
The Dice coeÆcient is a collocation measure widely used in information retrieval.
"In the following, we will give a more detailed de nitions of each."
"Statistically, bigram word predictability is de ned as the log conditional probability of word w i , given the previous word w i 1:"
Pred ( w i ) = log ( Prob ( w i j w i 1))
"Bigram predictability directly measures the likelihood of seeing one word, given the occurrence of the previous word."
Bi-gram predictability has two forms: abso-lute and relative.
Absolute predictability is the value directly computed from the for-mula.
"For example, given four adjacent words w i 1 ;w i ;w i +1 and w i +2, if we assume Prob ( w i j w i 1) = 0 : 0001, Prob ( w i +1 j w i ) = 0 : 001, and Prob ( w i +2 j w i +1) = 0 : 01, the abso-lute bigram predictability will be -4, -3 and -2 for w i ; w i +1 and w i +2."
The relative pre-dictability is de ned as the rank of absolute predictability among words in a constituent.
"In the same example, the relative predictabil-ity will be 1, 2 and 3 for w i ;w i +1 and w i +2, where 1 is associated with the word with the lowest absolute predictability."
"In general, the higher the rank, the higher the absolute pre-dictability."
"Except in Section 7, all the pre-dictability measures mentioned in this paper use the absolute form."
We used our text corpus to compute bigram word predictability for our domain.
"When cal-culating the word bigram predictability, we rst ltered uncommon words (words occur-ring 5 times or fewer in the corpus) then used the Good-Turing discount strategy to smooth the bigram."
Finally we calculated the log con-ditional probability of each word as the mea-sure of its bigram predictability.
"Two measures of mutual information were used for word collocation: pointwise mu-tual information , which is de ned as :"
I 1( w i 1; w i ) = log P r ( w i 1 ;w i )
"P r ( w i 1) P r ( w i ) and average mutual information , which is de ned as:"
I 2( w i 1; w i ) =
P r ( w i 1 ; w i ) log P r ( w i 1 ;w i )
P r ( w i 1) P r ( w i ) +
P r ( w i 1 ; w i ) log P r ( w i 1 ;w i )
P r ( w i 1) P r ( w i ) +
P r ( w i 1 ; w i ) log P r ( w i 1 ;w i )
P r ( w i 1) P r ( w i ) +
P r ( w i 1 ; w i ) log P r ( w i 1 ;w i )
P r ( w i 1) P r ( w i )
The same text corpus was used to compute both mutual information measures.
Only word pairs with bigram frequency greater than ve were retained.
The Dice coeÆcient is de ned as:
Dice ( w i 1 ;w i ) = 2 P r ( w i 1 ;w i )
P r ( w i 1) +
P r ( w i )
"Here, we also use a cut o threshold of ve to lter uncommon bigrams."
"Although all these measures are correlated, one measure can score word pairs quite di er-ently from another."
Table 1 shows the top ten collocations for each metric.
"In the predictability top ten list, we have pairs like scarlet fever where fever is very pre-dictable from scarlet (in our corpus, scarlet is always followed by fever ), thus, it ranks high-est in the predictability list."
"Since scarlet can be diÆcult to predict from fever , these types of pairs will not receive a very high score us-ing mutual information (in the top 5% in I 1 sorted list and in the top 20% in I 2 list) and Dice coeÆcient (top 22%)."
"From this table, it is also quite clear that I 1 tends to rank un-common words high."
All the words in the top ten I 1 list have a frequency less than or equal to seven (we lter all the pairs occurring fewer than six times).
"Of the di erent metrics, only bigram pre-dictability is a unidirectional measure."
It cap-tures how the appearance of one word a ects the appearance of the following word.
"In con-trast, the other measures are all bidirectional measures, making no distinction between the relative position of elements of a pair of col-located items."
"Among the bidirectional mea-sures, point-wise mutual information is sensi-tive to marginal probabilities P r ( word i 1) and P r ( word i )."
"It tends to give higher values as these probabilities decrease, independently of the distribution of their co-occurrence."
"The Dice coeÆcient, however, is not sensitive to marginal probability."
It computes conditional probabilities which are equally weighted in both directions.
"Average mutual information measures the reduction in the uncertainty, of one word, given another, and is totally symmetric."
Since I 2( word i 1; word i )=
"I 2( word i ; word i 1), the uncertainty reduction of the rst word, given the second word, is equal to the uncer-tainty reduction of the second word, given the rst word."
"Further more, because I 2( word i ; word i 1) ="
"I 2( word i ; word i 1), the uncer-tainty reduction of one word, given another, is also equal to the uncertainty reduction of failing to see one word, having failed to see the other."
"Since there is considerable evidence that prior discourse context, such as previous men-tion of a word, a ects pitch accent decisions, it is possible that symmetric measures, such as mutual information and the Dice coeÆ-cient, may not model accent placement as well as asymmetric measures, such as bigram predictability."
"Also, the bias of point-wise mutual information toward uncommon words can a ect its ability to model accent assign-ment, since, in general, uncommon words are more likely to be accented[REF_CITE]."
"Since this metric disproportion-ately raises the mutual information for un-common words, making them more predictable than their appearance in the corpus warrants, it may predict that uncommon words are more likely to be deaccented than they really are."
"In order to determine whether word collo-cation is useful for pitch accent prediction, we rst employed Spearman&apos;s rank correlation test[REF_CITE]."
"In this experiment, we employed a unigram predictability-based baseline model."
The un-igram predictability of a word is de ned as the log probability of a word in the text cor-pus.
The maximum likelihood estimation of this measure is: log P F req ( w i ) i F req ( w i )
"The reason for choosing this as the baseline model is not only because it is context inde-pendent, but also because it is e ective."
"In a previous study[REF_CITE], we showed that when this feature is used, it is as powerful a predictor as part-of-speech."
"When jointly used with part-of-speech infor-mation, the combined model can perform sig-ni cantly better than each individual model."
"When tested on a similar medical corpus, this combined model also outperforms a compre-hensive pitch accent model employed by the"
"Bell Labs&apos; TTS system[REF_CITE], where dis-course information, such as given/new, syntac-tic information, such as POS, and surface in-formation, such as word distance, are incorpo-rated."
Since unigram predictability is context independent.
"By comparing other predictors to this baseline model, we can demonstrate the impact of context, measured by word colloca-tion, on pitch accent assignment."
"Table 2 shows that for our read speech corpus, unigram predictability, bigram pre-dictability and mutual information are all sig-ni cantly correlated ( p &lt; 0 : 001) with pitch ac-cent decision.1 However, the Dice coeÆcient shows only a trend toward correlation ( p &lt; 0 : 07)."
"In addition, both bigram predictabil-ity and (pointwise) mutual information show a slightly stronger correlation with pitch accent than the baseline."
"When we conducted a sim-ilar test on the spontaneous corpus, we found that all but the baseline model are signi cantly correlated with pitch accent placement."
"Since all three models incorporate a context word while the baseline model does not, these re-sults suggest the usefulness of context in ac-cent prediction."
"Overall, for all the di erent measures of collocation, bigram predictability explains the largest amount of variation in ac-cent status for both corpora."
"We conducted a similar test using trigram predictability, where two context words, instead of one, were used to predict the current word."
"The results are slightly worse than bigram predictability (for the read corpus r = 0 : 167, p &lt; 0 : [Footnote_1]; for the spontaneous r = 0 : 355, p &lt; 0 : 0001)."
"1 Since pointwise mutual information performed con-sistently better than average mutual information in our experiment, we present results only for the former."
The failure of the trigram model to improve over the bigram model may be due to sparse data.
"Thus, in the following analysis, we focus on bigram predictability."
"In order to further verify the e ectiveness of word predictability in accent prediction, we will show some exam-ples in our speech corpora rst."
Then we will describe how machine learning helps to derive pitch accent prediction models using this fea-ture.
"Finally, we show that both absolute pre-dictability and relative predictability are use-ful for pitch accent prediction."
"In general, nouns, especially head nouns, are very likely to be accented."
"However, cer-tain nouns consistently do not get accented."
"For example, Table 3 shows some collocations containing the word cell in our speech cor-pus."
"For each context, we list the collocated pair, its most frequent accent pattern in our corpus (upper case indicates that the word was accented and lower case indicates that it was deaccented), its bigram predictability (the larger the number is, the more predictable the word is), and the frequency of this ac-cent pattern, as well as the total occurrence of the bigram in the corpus."
"In the rst ex- ample, cell in [of] CELL is very unpredictable from the occurrence of of and always receives a pitch accent."
"In [RED] CELL , [PACKED] cell , and [BLOOD] cell , cell has the same semantic meaning, but di erent accent patterns: cell in [PACKED] cell and [BLOOD] cell is more pre-dictable and deaccented, while in [RED] CELL it is less predictable and is accented."
These examples show the in uence of context and its usefulness for bigram predictability.
"Other predictable nouns, such as saver in CELL saver usually are not accented even when they function as head nouns."
Saver is deaccented in ten of the eleven instances in our speech cor-pus.
"Its bigram score is -1.5517, which is much higher than that of CELL (-4.6394{3.1083 de-pending upon context)."
"Without collocation information, a typical accent prediction sys-tem is likely to accent saver , which would be inappropriate in this domain."
Both the correlation test results and direct ob-servations provide some evidence on the useful-ness of word predictability.
But we still need to demonstrate that we can successfully use this feature in automatic accent prediction.
"In or-der to achieve this, we used machine learning techniques to automatically build accent pre-diction models using bigram word predictabil-ity scores."
We used RIPPER[REF_CITE]to ex-plore the relations between predictability and accent placement.
RIPPER is a classi cation-based rule induction system.
"From annotated examples, it derives a set of ordered if-then rules, describing how input features can be used to predict an output feature."
"In order to avoid over tting, we use 5-fold cross valida-tion."
The training data include all the nouns in the speech corpora.
"The independent variables used to predict accent status are the unigram and bigram predictability measures, and the dependent variable is pitch accent status."
We used a majority-based predictability model as our baseline (i.e. predict accented ).
"In the combined model, both unigram and bigram predictability are used together for ac-cent prediction."
"From the results in Table 4, we see that the bigram model consistently out-performs the unigram model, and the com-bined model achieves the best performance."
"To evaluate the signi cance of the improve-ments achieved by incorporating a context word, we use the standard error produced by RIPPER."
Two results are statistically signif-icant when the results plus or minus twice the standard error do not overlap[REF_CITE].
"As shown in Table 4, for the read corpus, except for the unigram model, all the models with bigram predictability performed signi cantly better than the baseline model."
"However, the bigram model and the combined model failed to improve signi cantly over the unigram model."
This may result from too small a corpus.
"For the spontaneous corpus, the unigram, bigram and the combined model all achieved signi cant improvement over the baseline."
The bigram also performed signi -cantly better than the unigram model.
The combined model had the best performance.
It also achieved signi cant improvement over the unigram model.
The improvement of the combined model over both unigram and bigram models may be due to the fact that some accent patterns that are not captured by one are indeed cap-tured by the other.
"For example, accent pat-terns for street names have been extensively discussed in the literature[REF_CITE]."
"For example, street in phrases like (e.g. FIFTH street ) is typically deaccented while avenue (e.g. Fifth AVENUE ) is accented."
"While it seems likely that the conditional probability of P r ( Street j Fifth ) is no higher than that of P r ( Avenue j F ifth ), the unigram probability of P r ( street ) is probably higher than that of av-enue P r ( avenue ).2."
"So, incorporating both predictability measures may tease apart these and similar cases."
"In the our previous analysis, we showed the ef-fectiveness of absolute word predictability."
We now consider whether relative predictability is correlated with a larger constituent&apos;s accent pattern.
The following analysis focuses on ac-cent patterns of non-trivial base NPs.[Footnote_3]
3 Non-recursive noun phrases containing at least two elements.
For this study we labeled base NPs by hand for the corpora described in Section 2.
"For each base NP, we calculate which word is the most predictable and which is the least."
"We want to see, when comparing with its neighboring 2 For example, in a 7.5M word general news corpus (from CNN and Reuters), street occurs 2115 times and avenue just 194."
"Therefore, the unigram predictabil-ity of street is higher than that of avenue ."
The most common bigram with street is Wall Street which occurs 116 times and the most common bigram with avenue is Pennsylvania Avenue which occurs 97.
"In this domain, the bigram predictability for street in Fifth Street is ex-tremely low because this combination never occurred, while that for avenue in Fifth Avenue is -3.0995 which is the third most predictable bigrams with avenue as the second word. words, whether the most predictable word is more likely to be deaccented."
"As shown in Ta-ble 5, the \total&quot; column represents the total number of most (or least) predictable words in all baseNPs[Footnote_4]."
4 The total number of most predictable words is not equal to that of least predictable words due to ties.
The next two columns indi-cate how many of them are accented and deac-cented.
The last column is the percentage of words that are accented.
Table 5 shows that the probability of accenting a most predictable word is between 40 : 48% and 45 : 96% and that of a least predictable word is between 72 : 72% and 80 : 08%.
This result indicates that rela-tive predictability is also a useful predictor for a word&apos;s accentability.
"It is diÆcult to directly compare our results with previous accent prediction studies, to determine the general utility of bigram pre-dictability in accent assignment, due to dif-ferences in domain and the scope of our task."
"For example,[REF_CITE]built a com-prehensive accent prediction model using ma-chine learning techniques for predicting ac-cent status for all word classes for a text-to-speech system, employing part-of-speech, var-ious types of information status inferred from the text, and a number of distance metrics, as well as a complex nominal predictor devel-oped[REF_CITE]."
"An algorithm making use of these features achieved 76.5%-80% ac-cent prediction accuracy for a broadcast news corpus, 85% for sentences from the ATIS cor-pus of spontaneous elicited speech, and 98.3% success on a corpus of laboratory read sen-tences."
"Liberman and Sproat&apos;s (1992) success in predicting accent patterns for complex nom-inals alone, using rules combining a number of features, achieved considerably higher suc-cess rates (91% correct, 5.4% acceptable, 3.6% unacceptable when rated by human subjects) for 500 complex nominals of 2 or more ele-ments chosen from the AP Newswire."
"Our re-sults, using bigram predictability alone, 77% for the spontaneous corpus and 85% for the read corpus, and using a di erent success es-timate, while not as impressive[REF_CITE]&apos;s, nonetheless demonstrate the utility of a relatively untested feature for this task."
"In this paper, we have investigated several collocation-based measures for pitch accent prediction."
"Our initial hypothesis was that word collocation a ects pitch accent place-ment, and that the more predictable a word is in terms of its local lexical context, the more likely it is to be deaccented."
"In order to verify this claim, we estimated three col-location measures: word predictability, mu-tual information and the Dice coeÆcient."
We then used statistical techniques to analyze the correlation between our di erent word collo-cation metrics and pitch accent assignment for nouns.
"Our results show that, of all the collocation measures we investigated, bigram word predictability has the strongest correla- tion with pitch accent assignment."
"Based on this nding, we built several pitch accent mod-els, assessing the usefulness of unigram and bigram word predictability {as well as a com-bined model{ in accent predication."
"Our re-sults show that the bigram model performs consistently better than the unigram model, which does not incorporate local context in-formation."
"However, our combined model per-forms best of all, suggesting that both con-textual and non-contextual features of a word are important in determining whether or not it should be accented."
These results are particularly important for the development of future accent assignment algorithms for text-to-speech.
"For our contin-uing research, we will focus on two directions."
The rst is to combine our word predictability feature with other pitch accent predictors that have been previously used for automatic accent prediction.
"Features such as information sta-tus, grammatical function, and part-of-speech, have also been shown to be important deter-minants of accent assignment."
"So, our nal pitch accent model should include many other features."
"Second, we hope to test whether the utility of bigram predictability can be gener-alized across di erent domains."
"For this pur-pose, we have collected an annotated AP news speech corpus and an AP news text corpus, and we will carry out a similar experiment in this domain."
"Thanks for C. Jin, K. McKeown, R. Barzi-lay, J. Shaw, N. Elhadad, M. Kan, D. Jor-dan, and anonymous reviewers for the help on data preparation and useful comments."
"This research is supported in part by the NSF[REF_CITE]998, the NLM[REF_CITE]-01 and the Columbia University Center for Ad-vanced Technology in High Performance Com-puting and Communications in Healthcare."
Chinese input is one of the key challenges for Chinese PC users.
This paper proposes a statistical approach to Pinyin-based Chinese input.
This approach uses a trigram-based language model and a statistically based segmentation.
"Also, to deal with real input, it also includes a typing model which enables spelling correction in sentence-based Pinyin input, and a spelling model for English which enables modeless Pinyin input."
Chinese input method is one of the most difficult problems for Chinese PC users.
There are two main categories of Chinese input method.
"One is shape-based input method, such as &quot;wu bi zi xing&quot;, the other is Pinyin, or pronunciation-based input method, such as &quot;Chinese CStar&quot;, &quot;MSPY&quot;, etc."
"Because of its facility to learn and to use, Pinyin is the most popular Chinese input method."
"Although Pinyin input method has so many advantages, it also suffers from several problems, including Pinyin-to-characters conversion errors, user typing errors, and UI problem such as the need of two separate mode while typing Chinese and English, etc."
Pinyin-based method automatically converts Pinyin to Chinese characters.
"But, there are only about 406 syllables; they correspond to over 6000 common Chinese characters."
So it is very difficult for system to select the correct corresponding Chinese characters automatically.
A higher accuracy may be achieved using a sentence-based input.
Sentence-based input method chooses character by using a language model base on context.
So its accuracy is higher than word-based input method.
"In this paper, all the technology is based on sentence-based input method, but it can easily adapted to word-input method."
In our approach we use statistical language model to achieve very high accuracy.
We design a unified approach to Chinese statistical language modelling.
"This unified approach enhances trigram-based statistical language modelling with automatic, maximum-likelihood-based methods to segment words, select the lexicon, and filter the training data."
"Compared to the commercial product, our system is up to 50% lower in error rate at the same memory size, and about 76% better without memory limits at all ( Jianfeng etc. 2000)."
"However, sentence-based input methods also have their own problems."
One is that the system assumes that users’ input is perfect.
In reality there are many typing errors in users’ input.
Typing errors will cause many system errors.
"Another problem is that in order to type both English and Chinese, the user has to switch between two modes."
This is cumbersome for the user.
"In this paper, a new typing model is proposed to solve these problems."
"The system will accept correct typing, but also tolerate common typing errors."
"Furthermore, the typing model is also combined with a probabilistic spelling model for English, which measures how likely the input sequence is an English word."
"Both models can run in parallel, guided by a Chinese language model to output the most likely sequence of Chinese and/or English characters."
The organization of this paper is as follows.
"In the second section, we briefly discuss the Chinese language model which is used by sentence-based input method."
"In the third section, we introduce a typing model to deal with typing errors made by the user."
"In the fourth section, we propose a spelling model for English, which discriminates between Pinyin and English."
"Finally, we give some conclusions."
Pinyin input is the most popular form of text input in Chinese.
"Basically, the user types a phonetic spelling with optional spaces, like: woshiyigezhongguoren"
"And the system converts this string into a string of Chinese characters, like: ( I am a Chinese )"
A sentence-based input method chooses the probable Chinese word according to the context.
"In our system, statistical language model is used to provide adequate information to predict the probabilities of hypothesized Chinese word sequences."
"In the conversion of Pinyin to Chinese character, for the given Pinyin P , the goal is to find the most probable Chinese character H , so as to maximize Pr(H | P) ."
"Using Bayes law, we have:"
H = arg max Pr(H | P) = arg max Pr(P | H )
Pr(H ) ^
"The problem is divided into two parts, typing model Pr(P | H ) and language model Pr(H ) ."
"Conceptually, all H ’s are enumerated, and the one that gives the largest Pr(H,P) is selected as the best Chinese character sequence."
"In practice, some efficient methods, such as Viterbi Beam Search (Kai-[REF_CITE]; Chin-hui[REF_CITE]), will be used."
"The Chinese language model in equation 2.1, Pr(H ) measures the a priori probability of a Chinese word sequence."
"Usually, it is determined by a statistical language model (SLM), such as Trigram LM."
"Pr(P | H) , called typing model, measures the probability that a Chinese word H is typed as Pinyin P ."
"Usually, H is the combination of Chinese words, it can decomposed into w 1 ,w 2 ,Λ ,w n , where w i can be Chinese word or Chinese character."
"So typing model can be rewritten as equation 2.2. n Pr(P | H ) ≈ ∏ Pr(P f (i) | w i ) , (2.2) i=1 where, P f (i) is the Pinyin of w i ."
The most widely used statistical language model is the so-called n-gram Markov models[REF_CITE].
Sometimes bigram or trigram is used as SLM.
"For English, trigram is widely used."
With a large training corpus trigram also works well for Chinese.
Many articles from newspapers and web are collected for training.
And some new filtering methods are used to select balanced corpus to build the trigram model.
"Finally, a powerful language model is obtained."
"In practice, perplexity (Kai-[REF_CITE]) is used to evaluate the SLM, as equation 2.3."
N − 1 ∑ logP(w |w ) i i−1
PP = 2 N i=1 (2.3) where N is the length of the testing data.
The perplexity can be roughly interpreted as the geometric mean of the branching factor of the document when presented to the language model.
"Clearly, lower perplexities are better."
We build a system for cross-domain general trigram word SLM for Chinese.
We trained the system from 1.6 billion characters of training data.
"We evaluated the perplexity of this system, and found that across seven different domains, the average per-character perplexity was 34.4."
We also evaluated the system for Pinyin-to-character conversion.
"Compared to the commercial product, our system is up to 50% lower in error rate at the same memory size, and about 76% better without memory limits at all. (JianFeng etc. 3."
The sentence-based approach converts Pinyin into Chinese words.
But this approach assumes correct Pinyin input.
Erroneous input will cause errors to propagate in the conversion.
This problem is serious for Chinese users because: 1.
Chinese users do not type Pinyin as frequently as American users type English. 2.
There are many dialects in China.
"Many people do not speak the standard Mandarin Chinese dialect, which is the origin of Pinyin."
"For example people in the southern area of China do not distinguish ‘zh’-‘z’, ‘sh’-‘s’, ‘ch’-‘c’, ‘ng’-‘n’, etc. 3."
"It is more difficult to check for errors while typing Pinyin for Chinese, because Pinyin typing is not WYSIWYG."
"Preview experiments showed that people usually do not check Pinyin for errors, but wait until the Chinese characters start to show up."
"In traditional statistical Pinyin-to-characters conversion systems, Pr(P f (i) | w i ) , as mentioned in equation 2.2, is usually set to 1 if P f(i) is an acceptable spelling of word w i , and 0 if it is not."
"Thus, these systems rely exclusively on the language model to carry out the conversion, and have no tolerance for any variability in Pinyin input."
Some systems have the “southern confused pronunciation” feature to deal with this problem.
But this can only address a small fraction of typing errors because it is not data-driven (learned from real typing errors).
Our solution trains the probability of Pr(P f (i) | w i ) from a real corpus.
There are many ways to build typing models.
"In theory, we can train all possible Pr(P f (i) | w i ) , but there are too many parameters to train."
"In order to reduce the number of parameters that we need to train, we consider only single-character words and map 2000) all characters with equivalent pronunciation into a single syllable."
"There are about 406 syllables in Chinese, so this is essentially training: Pr(Pinyin String | Syllable) , and then mapping each character to its corresponding syllable."
"According to the statistical data from psychology[REF_CITE], most frequently errors made by users can be classified into the following types: 1."
Substitution error: The user types one key instead of another key.
This error is mainly caused by layout of the keyboard.
The correct character was replaced by a character immediately adjacent and in the same row. 43% of the typing errors are of this type.
Substitutions of a neighbouring letter from the same column (column errors) accounted for 15%.
"And the substitution of the homologous (mirror-image) letter typed by the same finger in the same position but the wrong hand, accounted for 10% of the errors overall[REF_CITE]. 2."
Insertion errors: The typist inserts some keys into the typing letter sequence.
One reason of this error is the layout of the keyboard.
Different dialects also can result in insertion errors. 3.
"Deletion errors: some keys are omitted while typing. 4. Other typing errors, all errors except the errors mentioned before."
"For example, transposition errors which means the reversal of two adjacent letters."
"We use models learned from psychology, but train the model parameters from real data, similar to training acoustic model for speech recognition (Kai-[REF_CITE])."
"In speech recognition, each syllable can be represented as a hidden Markov model (HMM)."
The pronunciation sample of each syllable is mapped to a sequence of states in HMM.
Then the transition probability between states can be trained from the real training data.
"Similarly, in Pinyin input each input key can be seen as a state, then we can align the correct input and actual input to find out the transition probability of each state."
"Finally, different HMMs can be used to model typists with different skill levels."
"In order to train all 406 syllables in Chinese, a lot of data are needed."
We reduce this data requirement by tying the same letter in different syllable or same syllable as one state.
"Then the number of states can be reduced to 27 (26 different letters from ‘a’ to ‘z’, plus one to represent the unknown letter which appears in the typing letters)."
This model could be integrated into a Viterbi beam search that utilizes a trigram language model.
Typing model is trained from the real user input.
"We collected actual typing data from 100 users, with about 8 hours of typing data from each user. 90% of this data are used for training and remaining 10% data are used for testing."
"The character perplexity for testing corpus is 66.69, and the word perplexity is 653.71."
"We first, tested the baseline system without spelling correction."
"There are two groups of input: one with perfect input (which means instead of using user input); the other is actual input, which contains real typing errors."
The error rate of Pinyin to Hanzi conversion is shown as table 3.1.
Error Rate Perfect Input 6.82%[REF_CITE].84%
"In the actual input data, approximately 4.6% Chinese characters are typed incorrectly."
This 4.6% error will cause more errors through propagation.
"In the whole system, we found that it results in tripling increase of the error rate from table 3.1."
It shows that error tolerance is very important for typist while using sentence-based input method.
"For example, user types the Pinyin like: wisiyigezhonguoren ( ), system without error tolerance will convert it into Chinese character like: wi u ."
Another experiment is carried out to validate the concept of adaptive spelling correction.
The motivation of adaptive spelling correction is that we want to apply more correction to less skilled typists.
This level of correction can be controlled by the “language model weight”(LM weight) ([REF_CITE]; Bahl etc. 1980; X. Huang etc. 1993).
"The LM weight is applied as in equation 3.1. ^ H = arg max Pr(H | P) = arg max Pr(P | H) Pr(H) α , H H where α is the LM weight. (3.1) Using the same data as last experiment, but applying the typing model and varying the LM weight, results are shown as Figure 3.1."
"As can be seen from Figure 3.1, different LM weight will affect the system performance."
"For a fixed LM weight of 0.5, the error rate of conversion is reduced by approximately 30%."
"For example, the conversion of “wisiyigezhonguoren” is now correct."
"If we apply adaptive LM weight depending on the typing skill of the user, we can obtain further error reduction."
"To verify this, we select 3 users from the testing data, adding one ideal user (suppose input including no errors), we test the error rate of system with different LM weight, and result is as table 3.2."
"As can be seen from table 3.2, the best weight for each user is different."
"In a real system, skilled typist could be assigned lower LM weight, and the skill of typist can be determined by: 1. the number of modification during typing. 2. the difficulty of the text typed distribution of typing time can also be estimated."
It can be applied to judge the skill of the typist. 4.
Another annoying UI problem of Pinyin input is the language mode switch.
The mode switch is needed while typing English words in a Chinese document.
It is easy for users to forget to do this switch.
"In our work, a new spelling model is proposed to let system automatically detect which word is Chinese, and which word is English."
We call it modeless Pinyin input method.
"This is not as easy as it may seem to be, because many legal English words are also legal Pinyin strings."
"And because no spaces are typed between Chinese characters, and between Chinese and English words, we obtain even more ambiguities in the input."
The way to solve this problem is analogous to speech recognition.
"Bayes rule is used to divided the objective function (as equation 4.1) into two parts, one is the spelling model for English, the other is the Chinese language model, as shown in equation 4.2."
"One of the common methods is to consider the English word as one single category, called &lt;English&gt;."
We then train into our Chinese language model (Trigram) by treating &lt;English&gt; like a single Chinese word.
We also train an English spelling model which could be a combination of: 1.
A unigram language model trained on real English inserted in Chinese language texts.
It can deal with many frequently used
"English words, but it cannot predict the unseen English words. 2."
"An “English spelling model” of tri-syllable probabilities – this model should have non-zero probabilities for every 3-syllable sequence, but also should emit a higher probability for words that are likely to be English-like."
"This can be trained from real English words also, and can deal with unseen English words."
"This English spelling models should, in general, return very high probabilities for real English word string, high probabilities for letter strings that look like English words, and low probabilities for non-English words."
"In the actual recognition, this English model will run in parallel to (and thus compete with) the Chinese spelling model."
We will have the following situations: 1.
"If a sequence is clearly Pinyin, Pinyin models will have much higher score. 2."
"If a sequence is clearly English, English models will have much higher score. 3."
"If a sequence is ambiguous, the two models will both survive in the search until further context disambiguates. 4."
"If a sequence does not look like Pinyin, nor an English word, then Pinyin model should be less tolerant than the English tri-syllable model, and the string is likely to remain as English, as it may be a proper name or an acronym (such as “IEEE”)."
"During training, we choose some frequently used English syllables, including 26 upper-case, 26 lower-case letters, English word begin, word end and unknown into the English syllable list."
Then the English words or Pinyin in the training corpus are segmented by these syllables.
We trained the probability for every three syllable.
Thus the syllable model can be applied to search to measure how likely the input sequence is an English word or a Chinese word.
The probability can be combined with Chinese language model to find the most probable Chinese and/or English words.
Some experiments are conducted to test the modeless Pinyin input methods.
"First, we tell the system the boundary between English word and Chinese word, then test the error of system; Second, we let system automatically judge the boundary of English and Chinese word, then test the error rate again."
The result is as table 4.1.
"In our modeless approach, only 52 English letters are added into English syllable list, and a tri-letter spelling model is trained based on corpus."
"If we let system automatically judge the boundary of English word and Chinese word, we found the error rate is approximate 3.6% (which means system make some mistake in judging the boundary)."
"And we found that spelling model for English can be run with spelling correction, with only a small error increase."
Another experiment is done with an increased English syllable list. 1000 frequently used English syllables are selected into English syllable list.
Then we train a tri-syllable model base on corpus.
The result is shown in table 4.2.
"As can be seen from table 4.2, increasing the complexity of spelling model adequately will help system a little."
This paper proposed a statistical approach to Pinyin input using a Chinese SLM.
"We obtained conversion accuracy of 95%, which is 50% better than commercial systems."
"Furthermore, to make the system usable in the real world, we proposed the spelling model, which allows the user to enter Chinese and English without language mode switch, and the typing model, which makes the system resident to typing errors."
"Compared to the baseline of system, our system gets approximate 30% error reduction."
¤9§©¸¸^¡-¢-£z¤¦¥9§©¨«ª¥¦¬^­¨®¢-¯ ^°®£±ªI§¥T¤¦²{^¿I§©¸^ ³¼¬^¥\&quot; ^=¤Ä¥¦°Á² Ê§£¢-½ÐªI§¥Ä§^&quot;°®¨®§¥¦°Á¤=$º ¯&quot;^µT¿¥Ä¢ ^¤9µË§Ò¯^&quot;^^+¢ µT¢-² &quot;^ ªI§¥Ä§^&quot;°®¨´§©¥Ä°¤=º ¯§ªª¥Ä¬^^·¨®ºD¨´§£¸¿I§¸¢^+ &quot;¢§µ¦¿¥¦¢µ2ªd¬ª¿I¨®§©¤¦¢ÂÅ³¼¥Ä¬¯¨´§©¥Ä¸¢W×z¿I§£z¤¦°Á² ¸ ¬¥Ä°¤Ä·¯Û¬¥Ü°®¯&quot;ª¨®¢-¯&quot;$^&quot;$¤Ä·¢Ú§©¨Á² =¸ ¤ #¨´§©£¸^¿I§©¸^+£² ^¨Á°´µ¦·GÌ ß à @áCd=; YâK ã §©¥¦¤T²¶¬©³´²=$Þ+^
"´ ¨ª%$©«©­Ã.£ ë  %® ¨·· ¤¦¸\·×¤¦¥í®%\£ ©Y®%¨¥tîï»½ ¨¥%¥%¤¦· \·×®t»ï©­¨Ã/£ÓÊN»ª. %¤¦¸\·í¤¦®%%¨ÃX®%%ãÀ ¥%°hÃ.£ ¨¥¸\» ,»ªÍ©«ª%³nÇ ´Nµ¯¶ %® ¨·· ¤¦¸\·ò»½Ì®t©XÎ&lt;®×¤¦¥ ª%© Â %©­¬ó½»ªÉ¥% Â °\%»bÃX©­¥%¥t©­¥×¤¦¸ô¹¨¸lã ¥tãb¥t®t©­¹¥«À*©Çõ·\Çö¥%ã&lt;¸l®%¨ÃX®%¤¦Ãc§h¨ª.¥%\Ç&apos;È÷¸&lt;$©«ª .%&lt;%£\%¨·² · ¤¦¸\(Í©³$©«©­¸&apos;©XÎ&lt;§v±Å»ª% ¥t®§h¨ª%%® ¤¦Ã«°h±¦¨ª.±Åã Ê ¤Å®%%(» Íb¤¦¸\·Ð®%¨·· ¤p¸\.¨ÃXãÀ* \\· \©­¸¼ûc¨ª.î»(\©­±¦¥7ëOü=£l°\ª/.Ã  ý­þÿÿ #/ ¨±ÁÇÅÀ^ý­þþ  #=ü °\®t®%¤¦¸\·7©«®Ì¨±ÁÇÅÀ ý­þþ  ì Àbª.%\£  ¾Nª.  ì Àb¹í¨[Îl²  ©­¸l®tª%»§&lt;%\£ #ë  %%:ý­þþ  ì À .ãH²O³h¨¥%%\£   ý­þþ  ì $À  \¸ · %.¥«Ç $\%ª ©­¥.¥t©­¥í®%.°h±Å©X²O³h¨¥t©­¬ ´Nµ^¶ ® ¨·· ¤¦¸\%» ¨Ã.£Ì»½:¾Nª.¤¦±¦±:  ý­þþ  ì *£h¤¦Ã.£%   &quot;!#$ % (&amp; )&apos; *&amp;#+,*.- &amp;(0/ %1(&amp; &amp;(&amp; 243 3/ 53D71&amp; 3   **;: 3 .&lt;&gt;= )?@+ 3  &amp; ±Å©­¨ª/\· \©­±¦¥f®% ."
Â \° .%½»ª.¹í¨® %¤Å» ¸Éª.  ¢ *¥ ì .£ % %© ÃX» ¸l®t©XÎb® %.%%¤¦¸\·YÃX»ªt² ª ©­ÃX® ¨¥%¥%¤Å· %\%» ¨Ã.£º¨±¦±Å»SÊ*¥íÃX©«ªt²% %¨¤¦ ¸Ñ¤p¸H®t©«ª.¨ÃX®%¤¦» $©«®ËÊN©«©­¸Ñª.%©­¥«À\*Ê £\©«ª%©«³&lt;ã® ¨ÑÃ/\·©© &gt;G $©­ÃX®t©­¬7³&lt;ãc»  1G $©­ÃX®^Ê*\£ ©«®%£\©«ª »ª¸\\»®%%°\³h¥t© Â \° ©­¸l®%±Åã IH hª.©Ç ¶ .£ ¤¦¸l®t©«ª.¨ÃX®%¤Å» ¸h¥ $%¥.$»ª%%® ¨¸l®:½»ª ® \£ © %» ¨Ã/£×¤¦¸7ª%©«· ¨ª/¬4®t»Ï¤Å®%¥#¥.%¥#» ¸4®%£\©% ´aµ^¶ %® ¨·· ¤¦¸\·í® %¨¥tîvÇ
J ¸ö®%$&lt;§v±Å»ª%©×®%\£ ©É§$» ¥%¥%¤Å³v¤¦±¦¤Å®ã ® %/£í¤ ¦¸l®t©«ª.¨ÃX®%¤Å» ¸h¥¿¨ª%#© ÞvÝâ ©­¹§h¤Åª.¤¦Ã«¨±p±Åã#% $»ª%®%¨¸l®½j»ªÆ¾Nª.%» ¨Ã/=¨®Æ±Å©­¨¥t® ¨¥í¨§\²§ §h±p¤Å©­¬ï®t» ´aµ^¶ %® ¨·· ¤¦¸\·\Ç ¢ »×®% \² ®tª% ¨¥%%¥ %¤Å»  \
K ÙXÞvØ  Ù«ÞvÚ/.  
"ÀøÊ*.£ô¥t©«ª%,©XÎl² Ã«±¦°h¬\©Òª..¨ÃX®%¤Å» \©«Í©­±Å»§ø®Ê¿»íÍ[¨ª.¤Å² ¨¸l®%.¤¦±¦±ÁÔÕ¥*%» ¨Ã/£Y®%%©­¨±¦¤p¥t©¯®%£\©­¥% % %¤Å» \%&lt;%\° ª.¸¼» \° ®Ñ®t»º³$©¥ ¤¦¸h¥t®%^»½^ØHÙ.Ú«ÛÜ/&lt; ¨Y¥t®%.¬ %» ¨Ã.£öÊ*¤¦®%%£\©×¹¨Ã/\©×±Å©­¨ª.\· OH h©­±p¬  ë  ý­þÿ QP ì \ ©­¥%\©­±¦¥=¨±¦±Å»SÊA½j»ª*®tª.¨¤¦¸\² &lt;©­Ã«°\®%¤¦» ¸c¨±Å·»ª/¤Å®%*%®   ¤¦Í©#.£ %(» Í©­¬f§$©«ª%½j»ª.#®t©«ª.¹¥:»½v¥t§$*¤¦®%£  %¨·· ¤¦¸\.%.¨¸\·©­¥ ½jª%» ¹ %±¦¤Å· £l®º¬\©«·ª/%¤Å» ¸ ®t»ô¥%%(» Í©X²¥   ¢*£\©­¥t©Ïª%©­¥%%¥Ì¥t©«ª.Í©Ð®t»×¥.£\ £l®Æ» ¸ %£\©Ä¤¦¹§!»ª.®%./¨ÃX®%¤Å» %\£ ©Ò§!©«ª%²® ½j»ª.:¾Nª.¤¦±p±ÁÔÕ¥a»ª.¤Å· ¤¦¸h¨±!¹»&lt;\¬ ©­±OÇ R S F ê MM UT  WV D YX4X ê æ X    [Z ¾Nª.¤¦±p±ÁÔÕ¥ .¥ °\§$ ©«ª%Í&lt;¤p¥t©­¬ ±Å©­¨ª.¸h¤¦¸\· ¹»&lt;¬\©­± \ ®tª.¨¸h¥t½»ª/¹¨®%¤Å» ¸b²O³h¨¥%©­¬©«ª%ª%»ªt²Ë¬\/ª ¤ÅÍ©­¸±Å©­¨ª. Q] ÊN»ª%îb¥ ¨¥ó½» ± ¦±¦»(*Ê ¥«Ç \©à®tª.%"
ÃX»ª%ª.©­ÃX®%%¨®t©­¬Ð®t©XÎ&lt;®«Ç#\%ª%©­¥t§$» \· .¨(Êª ®t©XÎ&lt; ½j©­¬ ®%\£ ª%»  £ ¨¸ %¤¦¨± ²Ë¥%%® ¨®t© %¨®t»ª­À *Ê /£ ¹¨î ©­¥ ¨ ¹»ª% ±Å©­¥%¥ ÊN©­±¦± .¹©­¬&apos;¤¦¸h¤Å®%¤¦¨±#· \° ©­¥%¥Ð»½Ì£\(» Ê®%&lt;® %¥ £\» °h±¦ ¬ø³$%**%¤¦¨±n¨¸h¸\»®%¨®%¤Å» ¸ ¤¦¥ ÃX» %%\£ =© ®tª.¨¤¦¸h¤¦¸\%¨Ä¨¥5¨Ò³h¨¥%¤p¥ ½j»ª ±Å©­¨ª.¸h¤¦¸\%© Â °\ ©­¸hÃX©=»½v¢  */%=© ÃX» ¸H®t©XÎb® \©«§$\©­¸l® ÃX»ª%%ª ©­ÃX®%¤¦» #/ª %¬ ¥t© Â °\%%¤¦¨± \»®%¨®%¤Å» ¸K®t» $³ %»(Îb¤p¹¨®t©®%\£ ©Ð®tª.\%¨bÇ \© ®tª. ÃX» %¤p¥t®%¥É»½®%\£ ©Ó¤¦¸h¤Å®%¤¦¨± ²Ë¥t®%¨®t© \»®%¨®t»ª¿®t»·©«®%\£ ©«ªNÊ*¤Å®%%£\©=»ª.%©­¬¥t© Â °\ »½ ¢  *¥­À\*Ê /£ÑÃ«¨¸Ñ³$ ©^  »ª ´Nµ^¶ %® ¨·· \ÀÄ®%£\%¤p¨± ²Ë¥t® %\ ¨®t»ªE¨¥%¥%¤¦· .£6îb¸\»(Ê*¸
"ÊN»ª.¬6¤Å®%  %»³v¨³h±Å©É®%¨·Ó½jª%» ¹  \¸ · ¥t®K®%£\» ¥t©º±p¤¦¥t®t©­¬,¤¦¸ ¨Ó±¦©XÎb¤¦ÃX» .¹¤ ¦¸\©­¬ ½ª%» ¹ ¥t» .¨¤ ¦¸b² ¤¦¸\·7ÃX»ª.%¤¦¨±N®%¨·· \îb¸\ (» *Ê ¸"
"ÊN»ª.$© &lt;$©«ªf»½*Êa¨­ãb¥«À %¤p¸\·Ã«±¦°\..¨ Îb©­¥*¨¸h¬YÃ«¨§h¤Å®%¨±p¤¦¥%¨® %¤Å»  »(%£h¨±¦±h¸\»®a¬\ ¸ ®%£\%©­¨®% \îb¸\»SÊ*¸¼ÊN»ª.%$©«ª­Çù¢*£\ %® ¤¦¨±!\»®%¨® %¤Å» ¸Ñ¤p¥¿ÃX» %©­¬ Ê*¤Å®%£Æ®%\£ ©Ò®tª. %%¤Å½jã4¢  %%»(Í©Ñ¨Ã«Ã«°\.ª ¨ÃXã &lt;ãÆ¹¨îb¤¦¸\$©­Ã«¤ ,  H $» %.ª ©­ÃX®%¤Å» ³ ¥t®t©«§h¥­Àn©Çõ·\Çª%©«§h±p¨Ã«¤¦¸\· %® ¨· *¤Å®%£ %(» Íb¤¦¬\©­¬Ê ® %¨· $©­¨ª.¥  \©­¨ª%³&lt;ãf§$» %¥ ¤¦® %¤Å» ¸ÑëjÊ*\£ ©«ª%© ,$®%¨·í»½:®%£\^© ÊN»ª.¬ ì Ç ^»½ $§ » ¥%.¥ ¤Å³h±Å©Ò¢  * H &lt;ãÑ¥t®% ¤¦¸\. ®t©­¹§h±¦¨®t©­¥«ÔÅÀ^.£¼¨ª. ¥t©­¸l®%¤¦ ¨±¦±Åã \©«ª/¥t§$©­Ã«¤ ,H h©­¬ ¢  *¥«Ç »ªY©XÎ\ » ¸\%»SÍ&lt;¤p¬\©­¥®%/¸ /Ã \· ©a®%¨· È ®t»Ð¾ Ê*\£ ©«ª%© ®%\£ © %©«Íb¤Å» #Ê¿»ª.% ëjÊ*£\©«ª. ü ¨ª%$ ©­Ã«¤ , h©­¬ H ì %£\ ©«ªí¨± ² ±Å»SÊ*¥n®%£\©¿Ã.\·©N¤Å½H®%¨·ÄüÐ¤¦¥¨¥%¥%¤Å· \¸ ©­¬^®t»*©­¤Å®%\£ ©«ª»½ ®%\£ %\.[©« ®%ÃÇ J #¸ %® £\=©  ¢ *¥ %®  ¨ª%©f±Å©­¨ª.\¸ %\£ $©­Ã«¤ , h©­¬ÌÍ¨±¦° H \©­¥#ë #È  ü ì ¨ª%%¨¸l®%¤¦¨®t©­¬¼®t»Ö¥t§$©­Ã«¤ ,H vÃÏ§h¨ª.%® $©«©­Ã. *°h±Å©­¥¯Ã«¨¸¨±¦¥t»cª%© Â °h¤Åª%©í¥t§$©­Ã«¤ ,H !/:.ª ¨ ®%£\©«ª ®  %=®%¨· $ ¸H®t©XÎb®«Ç% ¢ %©4±¦©­¨ª.¸\©­¬A¨¥c¨¸A»ª.\¬ ©«ª.©­¬A¥t© Â \°   Èa®í©­¨Ã/£º¥%®%%\£ ©Y¸\©XÎb®ª.°h±Å©ø¨¬\%\£ © » \¸ ©ø®%£h¨®· %\£ ©Y³!©­¥%®ÑÞ!%»( ®%¨·· ¤¦¸\.¨ÃXãÖë %¥ .!»®%£© &gt; $ G ©­ÃX® ÃX»ª%ª%©­ÃX®%¤¦» %.ª%»ª.¥ ì . ¤¦¥c®%£\ %® \ª.ª%©­¸H®c®%¨·¼¥t®%¨®t©×»½ ®%\£ ©#®tª.%©«®fëjÊ*.%¤p¨±¦±Åã® %£\%¤p¨± ² ¥t®%¨®t© ¨¥%%¥ ¤Å·   ì %\£ ©Æ¸\©XÎ&lt;®¯ª. °\· £l®«Ç H %»bÃX©­¥%¥Ñ®t©«ª.*\£ ©­¸A®%£\%»(Í©X²  ¨±¦±¦¥a®t»í¥t» ¹©^%©­¥t§$©­Ã«¤ , v©­¬Æ® H %%©­¥%£\» ±¦¬nÇ %©­¥t®%¤p¸\·Ð½©­¨®%%©ø»½Ä®%%» ¨Ã/£ï¤¦¥ %® *¥×ÃX©«ª%®%¨¤¦¸ ¤¦¸l®t©«ª.¨ÃX®%¤Å» $©«®ËÊN©«©­¸ ª.*¤¦®%£ §!» .¥ ¥%¤Å³h±Å©ò³$©­¸\ &gt; © vÃ«¤¦¨±© H &gt;G $©­ÃX®%¥«Ç È /\·©×© &gt; !"
G ©­ÃX®t©­¬ò³&lt;ãA» \¸ ©Kª.%©K®%£\ ©Ã ÃX» ¸l®t©XÎ&lt;®¿ª%© Â %! %\£ ©«ª5ª.$=© ¥% ¤¦¥  h©­¬7¨±¦±Å»SÊ*¤¦¸\·ø¤¦®^®t» vª
H %©Ç »ª#©XÎ\:· ¤ÅÍ©­¸ .ª °h±Å ©­¥ \ Ã.£h¨¸\·© » ^¤¦½:§hª%©«Í&lt;¤¦» =%® ¨·Ì¤¦¥  ] \
Ã/\·© º®t» K¤Å½#%©«Íb¤Å» %¨· ¤¦¥ &gt;]  ¥t© Â °\ ©  %\£ © WH hª.¥%®fª/ hª H %©Ì®t»7· ¤ÅÍ©  *¤¦¸\·×® %\£ ©7¥t©­ÃX» ¸h¬¼ª. H hª.©Ï· ¤¦Í&lt;  ÇÓÈ÷¥t ©­ÃX» ¸h¬ï½j»ª.¹ »½^¤¦¸H®t©«ª/¨ÃX®%¤Å» ¸º¤p¥Ê*£\©«ª.© ®%\£ ©Æ® %¨·c¥%%¤Å®%\° ®t©­¬7³lãÐ» \¸ ©íª.#¤Å®%¥t©­±Å½a¥%\°  .£h¨¸\· ©#&lt;³ ãÑ¨¸\»®%\£ ©«ªÒª. \ · ¤ÅÍ©­¸×ª/ \ .Ã © ®t» . ©«Í&lt;¤Å» °h¥¯®%¨·7¤¦¥ ]  \ /Ã \· © Æ®t» \ ©XÎb®#%® ¨·7¤¦¥  ] ¨¥t© Â \°   À5®%£\© hª H .¥t®fª. H hª%©í· ¤¦Í&lt;  Àv¨±p±Å»(Ê*¤p¸\%\£ ©¯¥t©­ÃX» .
H vª% ©À\®t»í¨· ¨¤¦¸ 1G ¨ $%£\©Ì¥t©­ÃX» !» ¥.¤Å® %¤Å»  ¤ÅÍb¤¦¸\· !
Ç ¶ .£ .ª °h±Å©K¤¦¸l®t©«ª.¨ÃX® %¤Å» ¸h¥Ï£h¨­Í©É³$©«©­¸&apos;¥% $§ »ª%%® ¨¸l®¿½j©­¨®%\° ª%©À\¨±¦±¦»(*Ê * *¨¹í¥% ûÏ¨ª. þ &quot; ì
Ã«¨±¦± ±Å©«Í©«ª.¨· \ÔÅÀ:¨¥#¤¦¸ \ ±Å©«Í©«ªt² ¨· *»½\§h¨ª%®%¤p¨± ¥t» ±¦°\%® ¤¦» $©«®ËÊN©«©­¸# £H³$»ª.¤p¸\· ¤¦¸h¥t®% U bÀ ] &lt;¤ÁÇõ©Ç ¥t»¯®%%%ª ©­ÃX®%¤Å»  &gt; $ G ©­ÃX®t©­¬í³&lt;ã ©­¨ª./ Â °\(%\ª.¨®t© »§$©«ª.¨®%¤Å» ¸»½$±p¨®t©«ª5ª.$»®%%® .\¸ ² \\®%¤Å» ¸ ì Ç \Í[¨¸l®%¨·©×»½í¾Nª.%» ¨Ã/£&apos;¤p¥Y®% ¤Å®%¥¿±Å©­¨ª.¸\&lt;
"Â *ÃX» ¹§h¨ÃX®«À&lt;ÃX» ¸h¥%¤¦¥t®% »½*¨c½j©«Êô£&lt;\ª.©­¬7ª. %® $©Æ¬h¤Åª%©­ÃX®%±Åã ¤¦¸h¥t§$©­ÃX®t©­¬òë ÃÇõ½ ,Ç %® £\©®%\£ » %#ÃX» ¸H®t©XÎb®% .»³h¨³h¤¦±¦¤¦®%¤Å ©­¥:»½$¨ ûÏû ®%¨··©«ª ì %%¥%©­¬ ¤¦¸ *¨¹¥%£h¨­ÊK¨¸h¬¯ûÏ¨ª.þ &quot; ì"
"À®%£\%» ¨Ã.£ ¤¦¥¯ª%©­¥%¤¦¥%%® ¨¸H®#®t»Y»(Í©«ª.®tª. &gt;G $©­ÃX®%¥«Ç J ¸4¾Nª.¤¦±¦±OÔÕ¥ ©XÎb§$©«ª.%¥Ïë ¾Nª.¤p±¦±ÁÀÒý­þþ  ì .\·7» ¸ $%# %# &amp; %\£ © ´ %¨··©­¬ (&apos; Yðßjß ) vâ + % * .Ù Ù­â -, \Ý $. / * Þvðß ÃX»ª%*\%\£ © Ã«±¦» %ãÑ¨¥%%¥   ®%¤Å» \©«ª%©É®%£\©«ª%©Ö¨ª.©Ö¸\\îb¸\»(Ê*¸òÊN»ª. ì · ¨(Í©*%® ¨·· ¤¦¸\^· .¨ÃXã#»½$þ"
"QP lÇ C 0/ Ç 2  1 %£\» \° ®z®% ¨¥%¥.%¤Å» ¸nÀ&lt;\ ©«ª%©Ò§$©«ª%½»ª/\©«§$ » ¸&apos;®%\£ \\îb¸\»(*Ê ¸ÓÊ¿»ª/^®%\£ ©×¥%ÃX»ª%© Êa¨¥Æþ  &lt; C Ç 0/ \ ©­¥t©Ñª%©­¥.%%©ø¥t®%¨®t©X²O»½²O®%\£ ©X² ¨ª%® ¨® ®%£\©a®%¤¦¹©À ³h°\.!.°\ª%§h¨¥%¥%©­¬ ³ ã¼¥t» %%® ¨®%¤¦¥t®%¤¦Ã«¨±#%» ¨Ã/\£ ©­¥c¨¸h¬&lt; Í»®%¤¦¸\·\Ô ¥tãb¥t®t©­¹¥Ò®%£h¨®ÒÃX» ¹#%¤¦§h±Å©¯®%¨··©«ª/¥íë  * %&lt;î   hÍ¨¸ ¨±Å®t©«ª%©­¸Ñ©«®*¨±OÇÅÀý­þþÿ ì Ç ´ %ª »³h±Å©­¹í¥?»½ ¾Nª.¤¦±p±ÁÔÕ¥?.» ¨Ã.£ \©À hª/¥t®%±ÅãÀb®%£\©^¥t§$©«©­¬ »½ ®%¨·· ¤p¸\\· *.£    ®t»º³! ©7¥.¤Å·  ,"
H vÃ«¨¸l® %±Åã % ¥ ±Å»(ÊN©«ªY®%£h¨¸ ûÏûY²O³h¨¥t©­¬ ÃX» $©«®%¤Å®t»ª/¥«Ç »SÊ¿©«Í©«ª­À  . ¶ Ã.£h¨³!©­¥   ì %¥ (Ê¼£\(»  #ª.%©­¬í®%¨·² ·©«ªÑ®t»É¨  
"H %®%¨®t©Ï®tª/%(À*· ¤ÅÍb¤¦¸\·KÍ©«ª%ã ½ ¨¥t®Ï©XÎ&lt;©­Ã«°\%® ¤¦»  ¶ ©­ÃX» ^®%£\©«ª%©¤¦¥ø®% ¥t® ¦¤ ¸ ®%¤¦¹©¼»½c®tª.\·\.£ ½j»ªï±¦¨ª.·©«ªt²Ë¥%¤ ©­¬ ®tª.¨¤¦¸h¤¦¸\·¼¥t©«®%¥ºëj©Çõ·\Ç   ì %¨î© $%# %# &amp; ¥t» %£h¤¦ ¸\%»  »ª*%©Ä®t»ÆÃX»  ë %¤p¸\·A¾Nª.¤¦±¦±ÁÔÕ¥Ï»SÊ*%¨®%¤Å» ¸ ì Ç * %¥ (Ê¨¸h¬¯ûÏ¨ª.þ &quot; ì §hª%»§$» #½j¨¥t®t©«ª ¤¦¸b² ÃXª%%¨±ÁÔa®tª.¨¤¦¸h¤¦¸\·4¨±Å·»ª/¤Å®% ëjÊ*/£º¨(Í»   %©­¥%\·É®%\£ ©ÃX»ª%.£&apos;ª.%£h¨®c¤¦¥ª ±Å©­¨ª.&lt;ãÆ°h¥%¤¦¸\%¥¿»½:!§ » ¤p¸H®t©«ª.. ®t» %® £\,© ¥%¤Å®t©­¥ *Ê \£ ©«ª.©&apos;®%£\ ì ÀÐ³h°\® \¸ »®t© %%.ãóª%© Â %©­¹©­¸H®%¥¨ª% £® ¨¥Ì®t»Ö±¦¤¦¹í¤Å®Æ¤Å®% ¶ ¨¹f°\ ì %»§$» ¥t©­¥Ñ¨ ±p¨ ãvÔ=Í©«ª.¥.¤Å» .¨¸h¥t½»ª/¹¨®%¤Å» ¸b² ³h¨¥t©­¬ ±Å©­¨ª/\\· ÀÐ¨ ûY» ¸l®t© üa¨ª.±¦» Í¨ª. ® \£ ©ø¥t®%.%£\[²% ±Å»· °\©f¨ÃX®Ò®%¨·· ¤¦¸\\· *\£ © $¾ ×¥tãb¥t®t©­¹ »½ ¨[²   ì © ÆÃ«¤¦©­¸H® ´ % ª » % [¨ ² %® ¤Å» ¸4»½N®tª/.¹¨®%¤Å» ¸b²O³v¨¥t©­¬7±Å©­¨ª.¸h¤¦¸\·\ÀnÊ*£h¤¦Ã.£ ®tª.¨¤¦¸h¥ø¤¦¸A¥%%®t©«ªY®%¤¦¹©7®%.¤p±¦±ÁÔÕ¥Ì»( §h±Å©­¹©­¸H®%¨®%¤Å» ¸ºëj³lã¨¸Ð»ª.¬\©«ª¯»½N¹¨· ¸h¤Å®%\ ©Àn½j»ª %\£ %¨¥tîb¥Äª%©«§$»ª%®t©­¬ ì Ç $¾ %¥® ±¦¨ ã!/\·\¬ ¨·©«ª­ÔÕ¥cª%©­¥%% ÃX» /%¨·· #¨Ã«Ã«°\ª.%*±¦¨  ¥t®%.%\£  ´Nµ^¶ ®%¨·· ¤¦¸\·\Ç å  L=çnF "
D æ  å[æ  JhF  ê ç æ&apos;å ¢ £\©Ï¥t®%¨ª%%® $» ¤¦¸l®í½j»ªÌ®%\£ ©cÊN»ª%îï¤p¸Ö®%[² § ©«ªÆ¤¦¥ Â °\©­¥t®%¤Å» \·Ð®%\£ ©¨¥%¥%%¤Å» ¸É®% ª.$ ¤¦¸l®t©«ª.¨ÃX®%¤Å» $»ª%®%¨¸H® %$©«ª%½j»ª. ¾Nª.%\£ »b¬ » ¸ ´
"Nµ^¶ %® ¨·· ¤¦¸\·\ÇNüa» %¤¦¬\©«ªN®%£\© ½ ¨ÃX®Ä®% ´  Yðßjß &apos; ! ) â + tÙ * /Ù«â \ , Ý $.   * ³h¨¥t©­±p¤¦¸\$©«ª%½j»ª.\©«ª¿®%£\ %¥ ©­¬íÍ»bÃ«¨³\² .ãÖ¨¥%%¥ %¤Å» .» ¹ ¨¥.¥%¤Å· %\£   %»³v¨³h±Å©f®%¨·ø®t»ø©­¨Ã.£4Ê¿»ª/%» þ \ &quot; Ç C 0/ Ç È  H &lt;®%¨·· ¤¦¸\.¨ÃXã#»½$¨ª%»  QP %  / %%»  ¤¦¸ &quot;0 c® # %¨· ¥ $©­¤¦¸\·ÒÃX»ª%%ª Ç % ¥t§v¨ª%©­¸\©­¥%¥«Ô»½!ÃX»ªt²³ % ª ©­ÃX®%¤Å»  .$ ©­ÃX® %® %*½jª%© Â  »½» .ª% ©­ÃX®%¤Å» ¸Ì³$©­¤¦¸\%/ ¥t©Ò®t» ^¨¸\»®%£\*¤p±¦±!!³ © Â ^±¦»(^Ê Ç  /^¬ ª%©­¥%°h±Å®%¥»½\.£¯Ê¿©a¨ª%©a¨­Êa¨ª%© %$©­¨ªY» ¸&apos;®%%¥%°\ ©4¨ª.©K· ¤ÅÍ©­¸&apos;³&lt;ã *® %(Ê  ûÏ¨ª. þ &quot; ì \»¼ª%©«§$»ª%®®%¥ ª.. ¸º¨ $#%&amp; %¨¹§h±Å©ø»½Ò®%£\© ¾Nª%(»  üN»ª% ±ÅÍ©­¬Ã/\·©­¥a¨®  þ  #¥%¤Å®t©­¥«À .£Æ½j»ª=»  þ   %¤Å®t©­¥=¬h¤¦¬®%£\©ÄÃ/\·©^¬\©X² $ ¸A®%%¤Å» . ©7®% .§ \©­¥%  © þ  #¥% °h¸H®N½»ªa¨³$» °\® bÇ # ÿ 0  / %\£ © ®tª.¨¤¦¸h¤¦¸\·K¥%\® %%\&lt;» ©­¥Ì®tª.  bÇ # 0 ÿ  / %£\%©­¥%%¤¦¸\·Ä®%¨·· ¤¦¸\\.ª ¨ÃXã ¥ *&lt;^^¨ ª.(%£\$» ¥%¤Å®%¤¦Í©À% ¸ ©«· ¨®%¤¦Í©¯»ª*\¸ .  "
"J ¸ ®%£\©Óª%%$©«ª­ÀYÊN©AÊ*¤p±¦± \¬ ©«Í©­±¦»§ ¥t»  ª/ ®%¨·· ¤¦¸\%&lt; ½j»ªÖÊ*.£ .ª .¨ÃX® %¤¦» ¸E¤¦¥×©XÎb§h±¦¤¦Ã«¤Å®%±¦ã ©XÎl² Ã«±¦°h¬\ \ ©ó§$©«ª%½j»ª.¹¨¸hÃX© »½É®%\£ &lt;¬\² %»(Íb¤¦¬\©­¥ .¤¦Ã«¨±c©«Í&lt;,ÃX» . %® £\©&apos;ª%$»ª%%® ./¨ÃX® %¤Å» ¸E®t» ®tª.¨¸h¥t½»ª/¹¨®%¤Å» ¸b²O³h¨¥%©­¬ ´Nµ^¶ %® ¨·· ¤p¸\·\Ç åSæ =J æ è5J æ ¡bJÉD æ   è Æç êO J æ  µ #./¨ÃX® %¤Å» ¸×¤¦¥¯ ©­¹#³$»&lt;¬h¤Å©­¬7³&lt;ã ®Ê¿»ï¨¥.¥%%¤¦» !ØHÙ"
K \Ù«ÞvØHÙ«ÞvÚ/ /Ú  Ý fÛ  â íÙXÞhâ ÀvÊ*/£í®t»·©«®%%\ ª.°h±Å©í¤¦¸l®t©«ª.¨ÃX®%¤Å» \©ÆÃX» ¹¹¤Å® %%¥% %¤Å» ¸ïª%©«·  \£ »SÊ .ª °h±Å©­¥¨ª%%© Â /¤¦¸\·® %*£\©«ª%©¨Ìª.°h±Å© hª H %©­¥Ä®t»YÃ/\·©í¨ø® %¨·\À %® ® %® ¨·7¹¨(ãK¸\% Â °\©­¸H®%±¦ã7³$© Ã/\·©­¬É¨· ¨¤¦¸ ë ¥t»KÊ¿©Ï¨ª%© ÃX» %£\©ÏÃ. ì Ç J ¸\² ¬\©«§$%\£ ©7¨¥%%¥ % ¤Å» %%%©X² Â \° %£Ê*£h¤¦Ã.£ ¨¸ ©­¨ª.±¦¤Å©«ª¿ª.*&lt; ®%\£ ©ÃX» ¸l®t©XÎ&lt;®Ñª%©­±¦©«Í[¨¸l®Æ®t»×®%\£ © H hª.¤p¸\·4»½¯¨±¦¨®t©«ª ª %°.
Ã«¤Å©­¸H®%±¦ãÆ±Å»SÊA®%*!©^¤¦· \¸ »ª%
"J &lt;%\£ ©­¥t©c¨¥%¥%%¤Å» %¤¦±p±=¨±¦±Å»(Ê ¨ÆÃX» %¤¦¬\©«ª/¨³h±Å©^\¬ ©«·ª%©«©#»½ ½ª.©«©­¬\» \»SÊò¨ %&lt;¬¹¤Å· £l®¯³$©Æ¥%!§ ©­Ã«¤ H  Ò°\.ª %¨·· ¤¦¸\·\À ½j»ª¿©XÎ\&lt;ÃX» ¹¹í¤Å®%($©­¸  Ê*%\£ ©«ªÑ¨Kª/%© Â \° ©­¸hÃX©¥.£\» $%©­¬ï³&lt;ã &lt;¤p¸\·Y©­¨Ã/£Öª.%\° ª.¸®t»Ð®%£\©Ñ©­¸l®%¤Åª%©Ñ¤¦¸h¤ ² %® ¤¦¨±z®%¨·c¥t®%¨®t©»ª¯¤Å½¿®%\£ ©«ã¥%£\» $ ¨×·ª%» %¨·Ö©­¨Ã.£&apos;®t»î©­¸Ó¤p¸º®%.Ç %£\©«ª %¤¦» ¸¨±p±Å»(Ê*#¥ %® £\©Ì§$» %¥ ¥%¤Å³v¤¦±¦¤Å®ãY®%.±¦¤¦©«ª#®%¨· ª #¥t®t ©«§h¥f¹¤Å· £l®#¨ 1G $#ª/ H hª%²%  ¥«Ç J ¸h¬\©«§$\\»SÊ¿©«Í©«ª­À . °hÃX®% ¤Å· %%/£¯¤¦¸l®t©«ª.¨ÃX® %¤Å» ¸h¥«À[¤p¸ÄÊ*/£^Ã«¨¥t©¿®%£\ %¤¦» .\£ »   ¤ÅÍ©Ò©­¥%¥t©­¸l®% Â % %¥«Ç 1 *\ª.¥.°\^© %® £\ %¤Å» ¸nÀvÊ*¤¦®%£ ª.°h±Å©­¥=$³ ©­¤¦¸\*¨í·ª%» °\§c®t»©­¨Ã. Ê*£h¤¦Ã.£c¤¦¸HÍ» ±¦Í©­¥*®t©­¥t®%¤p¸\·í©­¨Ã/£cª/%\° .ª %¤p± %\£  » \¸ ©=ª/ hª H %©­¥«À ¤p¸#.£® Ã«¨¥t©#%® \£ ©Äª%\.=¨ª%^© ¤Å· ¸\»ª%©­¬nÇ ¢* » %\£ &lt; °h¥%¤¦¸\^· /ª  ¨¹¤¦±Å² ¤¦¨ªÑ¤¦¸ ¹¨Ã/\©Ï±Å©­¨ª.%¨¸hÃX©Y»½#¨ ØHÙ.Ú«ÛÜ/ÛÁÝÞß \©­±5ë   QP ì ÇaûY»ª%©#¥t§$©X² Ã«¤ , !"
H Ã«¨±¦±ÅãÀ5· ¤ÅÍ©­¸Ö®%£\©øÃ./%£\©Ñª.\² Í»  K tÝ *  bÝ[ÜrÛjâ ÛÁÝÞvðß^¬\©­Ã«¤¦¥%¤Å» ¸×±¦¤p¥t® ¥tãb¥t®t©­¹ÑÇ
"Èù¥t®%.\©­Ã«¤p¥%¤Å» ¸×±¦¤¦¥t®%  3 ??  &lt;(&amp; (&amp; =&gt; + &amp;( +  ! $&quot; &amp;# %&apos;)(+*  , ½j»ª*Ã«±¦¨¥%%¥ ¤ vÃ«¨® H %¤Å» ¸Ì®%¨¥tîb¥«Àh¤ÁÇõ©Ç¿¨¥%¥%¤Å· \ \ ¸Ï®%£\ ¥t®%¨®%¤pÃ ì Ã/ ¨ÃX®t©«ª.¤¦¥%%® \© &lt;.£h¨ª.%¨·· ¤¦¸\· ¹¨î©­¥ ¤Å®í¥t» ¹©«Ê* G ! ©«ª%©­¸l®½jª%» ¹ Ã«±¦¨¥.¥%¤ ,H vÃ«¨[² %¤Å» *. £l®Æ§hª%©­¥t©­¸l®Ì¬h¤ %¤¦©­¥® ½j»ªí¬\©­Ã«¤¦¥%¤¦» .¸h¤ ¦¸\·Ê¿©«ª.©ø¤Å®¸\»®½j»ª®%£\© ! ©­¸h¬\%%¥ %¤Å»"
J %£\/¤¦±¦±\±Å©­¨ª.¸h¤¦¸\^· .» ¨Ã...
Â %©­¬Ï¤¦¸c® %\£ %.¬\ ©«ªÄ¨¥Ò®%\£ ©«ãÏ¨ª% \/ª ¤¦¸\·í®%¨·· ¤p¸\·\%¤¦¸\·Ñ¥%°hÃ.£Ð¨¸ .» ¨Ã.£×®t»Ð¨¬\©­Ã«¤¦¥%¤Å» ¸±¦¤¦¥%#®   *¥f· ¤ÅÍ©­¥ % hÙXÞhâ ..
Ý Ù * rÛÞ&lt;#ñ ðßõñHÝ % / * Û â [M  ¤ÁÇõ©ÇÐ® %£\© hª H /¥t®#/ª .¸\ ©­¬ ÃX»(Í©«ª.¥­Ô5¥t» %® »½f®%\£ ©Ð®tª.\·¬h¨®%¨Óëj®%\»SÊ ³$©­±Å» \¸ · ¥«Ôa®t» %. ì À¨¸h¬¯® %£\©­¸f®%£\©=\¸ ©XÎb®:ª..¸\(Íl²® ©«ª.¥ø¥%» %®Ñ»½#Ê*%  » ±¦±¦»(Ê*·  1 ¾Nª%  ì ÀÄÊ¿©ª%©«½j©«ª ® t»Ö®%(/¬\ ©«ª.¤¦¸\·×ª.°h±Å©­¥Ñ¨¥Kð [K \ÙXÞvØÛÞlñ À ¤ÁÇõ©Çù¸\.%$\%\£  ® £\©ÒÃ«°\ª%ª%©­¸l®Nª.*   %© Â °\©­¸l®%¤¦¨±% ª /\\·
"À$.ª /\¸ ©­¬©­¨ª.±¦¤Å©«ªÄ¨ª% . ·©­¸\©«ª.\^· ®t»¹í¨¸Hã %&lt;\©«ª%©­¨¥ .. %%$©­Ã«¤ ,H !&lt;ª %^· ©­¸\©«ª.¨± .ª *±Å©­¨ª.¸\©­¬ ©­¨ª..·©ÖÞ!Ù«â^!§ » .¥ ¤Å®%¤ÅÍ©Ð© &gt; $ G ©­ÃX®«À¯³h°\® ¹¨(%§!©­Ã«¤ vÃ H ^¥%°\³v¥t©«®%.» ¸\\·"
"Ç*\© ¾Nª.¤¦±¦±\%» ¨Ã.£Æ¨±¦±Å»SÊ*¥5±¦¨®t©«ª¿ª. ®t»#$³ .¸\ ©­¬ %ª%.ª%»ª.¥N¹¨¬\*© ³&lt;.±¦¤Å©«ª¿ª. Í&lt;¤p¨¯¥t ©X² Â \° ©­¸H®%¤p¨±5ª.%¤¦»   \©­Ã«¤¦¥%¤¦» ¸7±¦¤¦¥t® ®tª%©­¨®%!%® ¨·· ¤¦¸\\· À&lt;%® $» %¥ ¥.¤Å³h¤¦±¦¤¦®Ëã^¬\»&lt;©­¥5¸\»® ¨ª.¤¦¥%©Ç 1 %  ì ¨¬\Í»bÃ«¨®t©­¥Ñ¨7Í¨ª.¤ ² ¨¸l®N»½:¬\©­Ã«¤¦¥%¤¦» ¸í±¦¤¦¥%®¿±Å©­¨ª.¸h¤¦¸\#· ¤p¸íÊ*£h¤¦Ã.£Æª. Â %©­¬Ï¨ª% &quot ; © K % * Ù \ K Ù«ÞvØHÙ%ØÀ:¤ÁÇõ©Ç#\©­¬Ï®t»Ñ®%£\.» ¸H® »½*®%%%ª %.% ¤¦¸Ï®%\£ %©«Í©«ª.¥%%\£ .^.£Ï® %£\©«ã¨ª%© ±Å©­¨ª. ¶ ..» ¨Ã.£ .» ¹¤¦¥% \©­Ã«¤¦¥.¤Å» ¸4±¦¤p¥t®¯®tª%©­¨®%%¨··  Ê*¤¦±p±¨±¦±Å»(Ê½j»ª ¹¤¦¥%%® ¨î©­¥ &lt;ãÄ©­¨ª.±¦¤Å©«ª:±Å©­¨ª.\¸ ©­¬ ª $© (» Í©«ª%.ª \&lt;³lã ±¦¨®t©«ª=%©Ä¥t§$©­Ã«¤ ,H vÃ. ª #%® £h¨®í¨ª%%©«§$ J ¸×®%%©«· ¨ª.¬nÀN¤Å®. ¹¨(ãY³$©f°h¥t©«½j°h±®t»ø¨±¦±¦»(Ê ¤¦¬\©­¸l®%¤Å®"
"Ëã!$Ô ¢  *¥«À$Ê*£h¤¦Ã.£ hª%©7±¦©­¨­Íb¤¦¸\·É®%\£  %® ¨· /\\® H ¥t©«ª%%©«Í©­¸l® %® %¨¹©¿§$» %¥ ¤Å®%¤¦» ¸Ä³$©­¤¦¸\&lt;¬b² ,H ¤ h©­¬4³lãK¨¸Hã4¥.°\³h¥t© Â °\©­¸l®^.ª  ¤¦¸7®% ±¦¤p¥t®«Ç Ð© 1 / (&amp; ) &amp; &amp; &amp; % &amp;( 2  3 . 3 . % / =&gt; )/  &apos;,   *?? 243 / 3  &amp;(&apos; 0E&amp;  / @&lt;2 Y*+,*3 *#&amp;*?&quot;  % (&amp; (&amp; &amp;&quot;- *) *+   !    +, 3  #7&lt;((&amp; *@?@&lt;&gt;*+ / 3 *E8) 243 / 3 #&amp; )= -(:"
Ê*¤p±¦±nÃX» %¤p¬\©«ª*®tª.\·í¹©«®%\£ &lt;» ¬h¥*/ %® ¨·· %\£ » \° ^® /ª .¨ÃX® %¤Å» ¸×¤¦¸4³$»®%£K¨§\² $\%©«§$.¤¦¨¸l®%.£ïÊ¿©§ *¤p±¦±hª. J ü=Èôë ¤¦¸h¬\©«§$\ Ê ¹¤¦®%%$\\· Ô ì  J ü ´ ë Ç­Ç«Ç%Ê*¤ ¦®%£ . ©«§!\\·
"Ô ì À&lt;%ª ©­¥t§$©­ÃX®%¤ÅÍ©­±ÅãÇ &quot; V  Z V FHD ê æ ê æ X  M X nçnF êO Z ¢ \©«§$\ ¹í¹¤Å®%%¥% %® ¤Å» ¸h¥N£h¨­Í©Ä¨#ÃX» ¸h¥t© Â %=¤¦¥¿ÃXª/!¤¦¸í¨± ² ±Å»SÊ*¤¦¸\· ¨íª/%ãø© .\· ¨±Å·»ª/¤Å®% ^¤p¥Ä®%\.¨¤¦¸\² .% ©­¥«À ©­¨Ã..£¥t©«§\² ¨ª.\%ª ©­¥%¥t©­¥Ì» ¸h±Åãº®%.. %® &lt; ¤¦Í©­¸Ö¥%¤¦¸\· ±¦© ´aµ^¶ Ç &lt; ..%£h¨®^&lt;¬h¤Å½ãÑ®%¨ $ ·  # $§ \©­¸hÃX©#%®tª.%*¥ °h¥*®t»Ñ¤Å· \¸ »ª.©¯®%\£ ©f½j¨ÃX®Ò®% .°h±Å©­¥Ò±Å©­¨ª.¸\©­¬¤¦¸Ï¥t» .±p¤Å©«ªëj»ª^±¦¨®t©«ª ì %©ª ¹¤¦· % ¸H®t©XÎb® %¥Æ¨ª.» $» %¥ ¤Å®%¤Å» ¸h¥ %® ¨·· ©­¬ % / # \ÀN®%£\©ø§$» ¥%¥.¤Å³h¤¦±¦¤¦®Ëã®%. »®%£\©«ª* £H®= ¹©Ä»®%\£ %¨·®t» $³ ©­ÃX»  &amp; © ^ # Ã«¨¸4³$© ¤Å· \¸ »ª%%¤¦¸hÃX©ÆÃX» ¹¹¤¦®% . ©«Í©­¸H®%¥a®%%%» ¹ô³$©­¤¦¸\·¯½ \° ª%%®  ,H v©­¬ ³&lt;ãÆª/=±Å©­¨ª.%£\^© Ã«°\ª.ª%©­¸H®* ¿¨Ã.£×§h£h¨¥t©Æ»½*®tª.%©­¥t®tª.%¥# ®t©­¸l®%¤Å» ¸Ì®t»f» $»ª%%® ¤Å» ¸Æ»½n®%\£ ©Ò®tª.¨¤¦¸\² .\Ç±Å ©­¨ª/\· .ª %¨· # rÀ ÊN©*\¸   \ª.©­¥%¥zª%©­±Å©«Í¨¸H® ¬h¨®%¨Ä§$» ¤¦¸H®%¥­ÔÅÀ *£h¤¦Ã.£c¨ª%©#%®  ¥t©#§$» ¥%¤¦® %¤Å» (&lt;Í ¤p¸\·Æ¤¦¸h¤Å®%%¨· &amp;# Ê #±Å©­¨¥t®Ä» \¸ ©¨±Å®t©«ª/%¤ÅÍ©í±Å©XÎ\¤¦Ã«¨±:® %¨·\ ¢  %%\£ ©­¥t© $§ » %¥ ¤Å®%¤Å» %© ¨ ½jª.¨ÃX® %¤Å» %£\» ¥t©Ì®% &lt;*Ê £\©«ª%© ¤¦¸ ® \£ ©ÃX»ª%\.» ¨Ã.£¼¨±¦±Å»SÊ*¥ ½»ªÑ¨¸% ¤¦¸hÃXª%%¨±ÁÔa®tª./¤Å®% *Ê /\»l ©­¥ ¸ » ® %¥ ° $ G ©«ªn®%£\%»³h±¦©­¹¥$»½  %(\ ûÏ¨ª.*þ &quot; ì Ç J ¸h¤Å®%#%ÃX»ª%=©  ®tª.¨¸h¥t½»ª/¹¨®%¤Å»  %® %\£ *© ¬h¨®%¨Ä§$» ¤¦¸H® %¥ »½f®%£\%©Ç µ ¸¼¤p¬\©­¸H®%¤ ¦½ãb¤¦¸\\% ®%\£ ©³$©­¥t® .ª © (  &apos; \»×¸\»®Ñ¸\%©­ÃX»  #/^¥%ÃX»ª%*.Ã«¨¸Y® %\£ © %¨Ò§$» ¤¦¸l®%¥z®t» 9H %£\» ¥% *Ê /£ )&apos; H hª%©­¥«À  °  °\ªz©XÎ\¤¦¥t®%¤¦¸\%©­ÃX»ª..%ÃX»ª%©­¥5¤p¸¯ª%©X²\ · ¨ª.%$» ¥%.¥  *:¥ %®  H hª%©a¨® %® \£ ©­¥t© ±Å»bÃ«¨® %¤Å»   1 %£\\©«®t©«ª. %\£ ©^¸\.  .¹í¨±¥%î ©«®%Ã.£ »½º®%\£ ©à®tª. ² ·»ª.¤¦®% ¤¦¥Y· \» %»SÍ&lt;¤Å² ¥ ¤¦» ¸×½»ªí°h¸\î&lt;(Ê*¸ÊN»ª.%%£\©% Ã«±Å» %¥ %ãø¨¥%¥%%¤Å»  °\ªÒ©XÎb§!©«ª%² \©­¸l®%¤Å®"
"Ëã .ª $©ÄÃX» %%©­¬Ñ¨¥=$§ » ¥%%¥ ¤Å³v±Å© ª =\.ª %%¨±n°\. &apos; ¶ » ¹©c½ °\%ª %® \£ ©«ªÆ© %£h¨® ¨ª%©Ð¸\»®Ì¬\©X² ¥ ÃXª.¤¦³!©­¬ ¤¦¸% ¤Å· \° %ª  %%£\%©­¥%!§ ©X² Ã«¤ ,  H \%\£ ª%©­¥%\£ » ±p¬^½j»ª ®tª.%© &gt;H v±Å² ®t©«ª=ÊN»ª%îvÇ 2  1 \ ©­¸ ®%£\©ÒÃX»ª% . hª H .¥%®N±Å»  Ã«¨¸×©XÎb®tª. % ©«ª%%ª .¥­ÔNë   ì Ê \£ ©«ª.©Ö®%\£ ©ï¬\©«½ %¨· ô¥.£\» ,(Í©º³$©«©­¸* !Ç (1 Ð©Y¸\©«©­¬ï» %®%¨¤¦¸º¢  *¥íÃ.\· ¤p¸\· ®t» 7¤Å½a® %\£ © ÃX» °h¸H®f½»ªf®%\£ ©Æ§h¨¤Åª ë  ì ¤¦¥#¨³$(» Í© ® \£ ª%©­¥.\£ » &lt;¤ÁÇõ©Ç5¥%%\£ ©«ª.Ê*$» %¥ ¥%¤¦³h±Å©% %®  %¥ .£   ¢  %ÃX»ª%©¨³!»SÍ©Y®%£\.ª ©­¥%£\»  È $§ » %¥ ¤Å®%¤Å» ¸4Ê*¤Å®%%¤¦¨±z®%¨· É¤¦¥fÃX» %%©­¬4¨ %$» ¤¦¸H®«Ô­¤p¸^®tª.¨¤¦¸h¤¦¸\·=»  ¨¸#¨±Å®t©«ª.¸b² ¨®%%¨· c½»ª^¨¸Ð¨³$»SÍ©®%\£ %ª ©­¥% ±¦¬Y©«ª.%ª »ªÄ§h¨¤Åª ë  ì Ç ¶ ¤¦¹¤p±¦¨ª.±ÅãÀ:*Ê £\©­¸×ÊN©ø¤¦¬\©­¸l®%¤Å½jã7® %\£ %¨ $» ¤¦¸l®%^¥ %.\\· À Ê¿©ÌÃ«¨¸×ÃX» § %® £\©¿»&lt;%%ª :»½\ÊN»ª.:$©­¨ª.¤¦¸\=· Ê*¤¦®%%\£ © ÃX» ¸l®t©XÎ&lt;®ÏÊ*¤¦¸h¬\(» ^Ê .» %\£ ©­¥%©4§$» ¤¦¸l®%¥«Ç Ð© 1 \¸ ©«©­¬Y®%£\©­¸Ñ»  ÃX» .¤¦¬\%®%¨¸H®%¤¦¨®%¤¦» =»½z±Å©XÎl² ¤¦Ã«¨±$.¤¦¸\·fÊ¿»ª.¬h¥aÊ*\£ » $©­¨ª. ÃX» *¤¦¸Ì®% ¸l®t©XÎ&lt;® %$»(Í©¯®%£\ª%©­¥%£\»"
È ¥ø¸\»®t©­¬¼¨³$»SÍ©ÀÄ®%£\ ©7¬\©«®%¨¤¦±¦¥ »½f®%£\.©X² %$%®=»½z®%\£ ^ © ®tª.¨¤¦¸h¤¦¸\.¤Å®%  G ! ©«ª ½j»ª5®%£\© J üaÈ  J ü ´ Í¨ª.¤¦¨¸l®%¥«Ç »ª J üaÈ¯À %\£ ©N¥%ÃX»ª%%#£ ¥%£\» $ ª%©­ÃX»ª.\À® ¨®:©­¨Ã.%¨·©À®%£\ ©N¥%ÃX»ª%©­¥:½j»ª ©­¨Ã/£#/ª \¤Å® Ê¿©«ª%© ¨¬\»§h®t©­¬^®t» ÃX»SÍ©«ª­Ô¥t» $»ª%®%¤Å» ^¸ »½&lt;®%%©­¹¨¤¦¸\² . ¤¦¸h¥t®%  1 \©­¸7¥t» ¹©í¸\©«Ê .ª °h±Å© ï¤¦¥z¨¬\ ÊN©=%® \£ \©­¸l®%¤Å½jãÄ® %\£ ©=%¨Ò§$» ¤¦¸H®%¥ *%© &apos; H hª%©­¥«Ç:\©­¥t©¿§$» %¥ ¤Å®%¤Å» \»(Ê $³ ©­±Å» ¸\·\Ô­®t»Ê ¯À!%%ÃX» .\° ³h¥t©
Â \° ©­¸l®*%¥ ÃX»ª. ë %» ¹ ®%\£ ©*ª%©­ÃX»ª.¬»½ %¨^!§ » ¤p¸H®%¥«Ô ì À ##¹ %®*¨±¦¥t» \»\%ÃX»ª%©­¥ ¤¦¸Ï®%\£ . °h±Å©f¥.ÃX»ª%%£Ï®%^¬\©«§$ ¸®%\£ ©­¥t© $§ » ¥%¤Å®%¤¦» %ÃX»ª.¤¦¸\· ¯ýY¤Å½Ò®%\£ ©Yª. &gt; $ G ©­ÃX®%¥ ¨øÃX»ª.ª%©­ÃX® %¤Å » ¸4¨®¯» ¸\%\£ ©­¥t©§$» %¥ ¤Å®%¤¦»   ý % %%ª »ª­Ç »ª J ü ´ !
À %® £\%ÃX»ª%©­¥ .£^ª% ©­ÃX»ª.¬h¥«À[¨® ©­¨Ã.%¨·©À®%£\©N¥%ÃX»ª%©­¥ ½»ª ©­¨Ã.£ ª.°h±Å©f¤¦½¿¤Å®ÒÊN©«ª%©¨¬\ Ù * rÛ * ØHÙí®%\£ \ . *£\©­¸¨¯ª. &apos; ¤p¥5¨¬\ $¬h¨®t©Ñ® %\£ ©Y¥%ÃX»ª%%£É³&lt;ã×ÃX» %¤¦¬\©«ª%² #» %\£ %$» ¤¦¸H®%¥¿Ê*£\©«ª.© &apos; H hª%©­¥­À&lt;³v°\®N¤p¸ ®% %©Ñ®%\£ ©ÑÃ.\·©ø¹í¨¬\©Ì®t»Ð®%\£ ©Ñ¥%ÃX»ª% § » ¥%%¥ ¤¦³h±Å©fª.©$ %® ^ÃX»  hª H %^%® £\$» ¥%¤Å² %® ¤Å» !*¸\ ¸ÏÊ*\£ ©«®%£\©«ª ô» \° ®t§h°\®%%£\©
"X»ª%ª%©­ÃX®=®%\»®«À\³h°\®aª.¨® %£\©«ª=» \ ©«®%£\©«ª=%® \£ ©Ã $» ¥%¤Å®%¤Å» ¸øÊ¿» $©¯ÃX»ª%ª%\%%©«Íb¤Å» § ª.*\£ ©«®%\£ *$©fÃX»ª%ª%©­ÃX®Ò· ¤ÅÍ©­¸ &apos; %¤Å»  ¶ §$©­Ã«¤ ,H $» %¥ ¤Å®%¤¦» ¸KÊa¨¥ ÃX»ª%ª%©­ÃX®ëjª%©­¥%%ª%©­ÃX® ì  &apos;  % %%% ª ©­ÃX® ì À$®%\£ . ó® %ª hª%©^¨®*%® $» %¥ ¤Å®%¤Å» ¸ÌÊN©¯¥%ÃX»ª.© H ýëjª%©­¥t§Ç ¯ý ì Ç"
V Z  V YX4X D ê æ X  M  nçF êÁ Z »ª=¾Nª.%£\&lt;%® £\%.£Æ®ËÊN» .(ãÑØÛ % * %Ù (Ú â ß  1G $©­ÃX®¿©­¨Ã/£»®%\£ ª Å¤ ®t©­¬&apos;³lã¼®%*¤p¬\®%£¼»½íÃX» ¸l®t©XÎ&lt;®%¥Ð¤p¸Aª.  Ã/\.©­ÃX®a¤p¸H®t©«ª.¨ÃX®%¤¦»  ¤¦¸®%£\©«»ª%ã×¥t§h¨¸×»SÍ©«ª /£×·ª%% % ¨·· ¤¦¸\·¨±Å·»ª/¤Å®%*/%\£ © ª.:¬h¤Åª%©­ÃX®%\»® ÊN»ª%#î %% %ª »SÊÉÊ*¤¦¸b² \¬ »SÊK»SÍ©«ª ®%£\©¿®t©XÎ&lt;®«À³v°\%®t©­¨¬^#¹ °h¥t®ÊN»ª%îÒÊ*¤Å®%£ ®t©XÎb®*%¥a»½ ¨®*#%¥ ¤ ©Ç »ª:%® \£ %©­¥t©­¸l® %» ¨Ã.%\£ \©«§$\©­¸hÃX© ¨¥%¥%%¤Å»  %©5ª./¨ÃX®%¤Å»  ¥t»ºÊ¿©×Ã«¨¸&apos;°h¥t©K¨¸ò¨±Å·»ª.¤Å®%£h¹?Ê*.£Ó¥%®tª%©­¨¹¥ %® £\©Ð®t©XÎb®ø®%\£ %ª »  £¼¨×³h° $ G ©«ªÑ®% %*¤¦¬\© ½j»ªø¥%¤¦¸\· ±Å©cª. ®t» hª H % ©Ç 1 ¤Å® %£¼ÃX» ¸l®t©XÎb® %¥Ñª.¨¸b² · ¤¦¸\·Ð°\§h®t»Ï®%%%%\£ ©«ª¥%¤p¬\*¤¦¸b² ¬ »SÊºÊ*¤p¬\%® %¥¿¥%° ÆÃX©­¥«Ç Èa®¿©­¨Ã/£\ ¥t®t©«§f»½\¨¬\[Í ¨¸hÃ«¤¦¸\%\£ ©¿Ê*(^Ê ÀSÊ¿©¿©XÎb¨¹¤p¸\©5®%£\© %¤¦¨±&lt;%® ¨·¯¨¥%%¥ ¤Å· ^%® £\*© ÃX©­¸l®tª. ÃX©­¥%%£\©*%¥ °\.¥z»½vª.°h±Å©­¥z®%&lt;¬h¤Å½ã#%®  %® ¨·\/\£ ©­Ã%îÏ® %£\.
Â °\ ¤Å½$
J  \¸ ©# ¸\© ì »½$®%£\ ©­¹ H hª%©­¥­À\Ê¿©¯£h¨­Í© 5H v¸h¤¦¥%£\%=§$» ¥%¤Å®%¤Å» Ç    ê æ X  
"Jhè  D YX4X ê æ X êÁ 5ç Z é  HF é¿MOJ ê æ     ê ç æ 1 Ð©ÌÃX» %©­¬®%£\©Æ§!©«ª.½»ª.*¾Nª.¤¦±p±ÁÔÕ¥^ §h±Å©­¹©­¸H®%¨®%¤Å» ¸ëj½j»ª¼Ê*/£à®%\©,¤¦¥º§h°\\³ ² *(- /  &gt; &amp; ; /*(- 3/ @/ *3(&amp; &amp;0E&amp; /0 1&amp;(*?@?&lt;: ;+ *(&amp; #))(&amp; ;)3 2&amp;(%18&amp;()D7&amp;3 &lt;#&amp; 3= &lt; ! %1&amp;(6 ) -;: , / .   3   .&amp; &amp;(/ &amp; / ?@ (&amp; /3 ,+/ *- ), /  & amp;B) )  &amp;  / 2 *&amp; %&lt;(-  * 3 &amp; &amp;(;  &amp ;  &amp; , &amp; 2 ; 2   / ?5/*(- / ) [E + ()F&amp;  / *(- / ?  ;&amp; &amp;(/ &amp; &amp;/ ?@ , (&amp; )? 0/ *  ;&amp; 2 &amp;./ 2 = )% (&amp; 6;: % (&gt;- =)E)E* 3&gt;&amp;&amp; )(&amp;3  &amp ; / *? +,*- 2 *&apos;&lt;  @%&amp;3 ))&amp;&amp;(;) 2 )- E &amp; % ? / % , % ;&amp;:)=&gt;&lt;%3 - 2 %&lt;(;&amp;Y  /&amp;(3 ) @243 3 3 % 2 - ,3-  3 - 3  %#&amp; ;:  3 .3 *#+3 2&amp; &amp;)3(&amp; ;)&amp;B*&amp;( / 3 243/ &apos;&gt; &amp;() &amp; 3 %*( ;( *% #&lt;+,*6 &lt;*-/ / / &lt; 3   / (&amp; &amp;(% ?)  3 &amp; &lt; ,, )&amp; * 3 &amp; % + ¦± ¤pÃ«±Åãc¨(Í[¨¤¦±p¨³h±Å© ì Ê*¤¦®%%¨®%¤Å» ¸7¤¦¸Kü »½ø®% ¨±Å·»ª/¤Å® %%Ã. ¨³!   %® .% ©«®ïëjÊ*¤Å®%£ö³$»®%£ö±Å©XÎl² ¤¦Ã«¨±=¨¸h¬É¸\» ¸b²Ë±Å©XÎ\ ì &lt; %\£ ©øÃ«±¦» ¥t©­¬×Í»&lt;%ã¨¥%¥%%¤Å» %» ¥%® %¤¦» %» ¨Ã.%Ç 1 Ð©Ñ®t»&lt;»îK®%\£ © ©«Í©­¸A¸&lt;#$³ ©«ª%©­¬º¥%©­ÃX®%¤Å» %%¨··©­¬ Yðßjß &apos; ) +* vâ %Ù.Ù­â \ , Ý $. / * Þ!ðß¿ÃX» $» ¸\%\£ © ´ ¢*¾ ë ÃX»  /¤¦¥%¤¦¸\· ÿ $#%&amp;  ì  ª.\» /¤Å³\² °\®t©­¬Ö®%£\  %ª ©­¥%%¤p¸\· %¤ ©øÿ 5 $&amp; \©­¥t©ÑÃX» ±¦±Å©­Ãr² ® ¤Å» ¸h¥¨±¦±Å»SÊ ½»ªí¨¸Ö ©­¤Å·  ±¦¬ÖÃXª.» ¥%%¤Å» ¸% »½Nª%©­¥%%.£4®tª.%¨¥tî¤¦¥Òª%©«§$©­¨®t©­¬ ©­¤Å· £l®¯®%:*Ê ¤Å®%£7» =®%\£ ^¥t©­±¦©­ÃX®t©­¬K¨¥ %® \£ %¨bÀ:%\£ ©»®%\£ ©«ª#¥t©«Í©­¸4ÃX» ¹#³h¤¦¸\©­¬ ®t»Æ½»ª.¹à®%£\©¯®tª/\%¨bÀ!· ¤ÅÍb¤¦¸\.\·¥t©«® %¤¥ %»   þ 5 $  &amp; ÃX» ¹§h¨ª.¨³h±Å© ®t»f®%£\© $%# %#  &amp; .¤p±¦±\¤¦¸í£h¤¦¥5±¦¨ª%· ©«ª¿©XÎ&lt;$§ ©«ª.¤Å² %¥ ì *£\©Òª%©­¥%%¥a»³h®%¨¤¦¸\©­¬Ñ¨ª%^© (¨ Í©«ª.¨·©­¬ø®t» · %©aª%%¤Å» ¸#»½v§$©«ª%½j»ª.¹¨¸hÃX© %®  $©Ò· ¨¤¦¸\©­¬Ñ½jª%» ¹E¨¸lãÑ¥%¤¦¸\· / !"
Ç 1 Ð©Ñ®tª/¨¤¦¸\©­¬®%£\©Ì¾¿ª/¤¦±¦±N¥tã&lt;%¥ ®t©­¹¨®f®%% ©­¥%£\» ±¦¬h¥ ý ëj®%=[Í .\° ··©­¥t®t©­¬½j»ª ®% /.\· %¨Ì¤¦¸Ï®%£\\%® tª.%¤Å»  ì  &lt;Ç# ª ©­¥.%% ©f¥.£\»(*Ê ¸¤¦¸Y®%\£  ¨³v±Å©ÆýÇëjÊ*£\©«ª.©#®%\£ ©% ¥%ÃX»ª.©­¥a½»ª=¨Ã«Ã«°\.ª ¨ÃXãÀh©«®%%©Ä¨­Í©«ª.¨·©­¥*(» %£\© ©­¤Å· £l®Ñª. ì
"Çô¢*£\© H  #®%£\©Ð®%¨³h±Å© ¥ (Ê*%£\©f®%#®%¨î©­¸®t» %® ¨·Ðýf¹¤¦±¦±p¤Å» % .¤¦¸\*· %® .%.&lt;»  ¨· /¨· ©­¬ ì Ç %%\£ (»  ÃX»ª%ª.©­¥t§$» \=· ª.©­¥%%¥½»ª ® £\© J =ü È¨¸h¬ J ü ´ ®tª.\%&lt;[¨®:.¨¸\· ©% »½  , $ G ©«ª%©­¸l®=®tª.\·#%® %©­¥%£\» ±¦¬h¥«Ç  /¤Å®t© \ %£\»b¬ #&quot; S®%\£ ª%©­¥.\£ » ±¦¬ A] ëj©Çõ·\Ç ¾Nª.¤¦±p $ ± &lt; &quot; ý ì ®t»×ª%©«½j©«ªÌ®t»Ö¨K¹©«®%\£ »&lt;¬ ®tª.¨¤p¸\ · ¤ÅÍ©­¸í®%\£ % ª ©­¥%  µ ³h¥t©«ª%Í©*®%%\£ %¨·· ¤p¸\ #· ¨Ãr² Ã«°\ª/¨ÃXãK½j»ª J üaÈ % &lt; &quot; ý (ÊN©«ª­ÀN³lã bÇ¦ý # 0/"
"Àa®% ½j»ª5¾Nª.¤ ¦±¦± $ &lt; &quot; ý &lt;Ç »(ÊN©«Í©«ª­À J üaÈº£\ ©«ª%%%¥ ª.°h±Å©­¥z®%. # &quot; &lt;Í ¥«Çaý­ÿ  ì Àb¥t»^¤Å®5¤¦¥ ¤¦¸l®t©«ª%  ÃX» %© J ü=È % &lt; &quot; ý .¤¦±¦ &amp; ± &lt; &quot; ý &lt;À *£h¤¦Ã..&lt;» %/ %¥«ÇÊ ©«ª%©®%\£ © ¨Ã«Ã«°\/ª ¨ÃXã »½ J üaÈÓ¤¦¥=±¦»( ã bÇ¦ý # / Ç ¶ ¤¦¸hÃX© J üaÈ %\£ © [¨ª.¤p¨¸H®«Ô5»½Ä®%.¤Å· ¤¦¸h¨± ¾Nª.¤¦±p±:¹©«®%£\»&lt;¬nÀ®%£\©­¥t©íª%©­¥%%¥ÄÃ«¨¸4³$&lt;¤Å©«ÊN©­¬7¨¥ . (» \·^¬h¤Åª%.¤¦Ã«¨±b©«Íb¤¦¬\*%\£ &apos; 3 )( (- * &lt; ?@&amp; 2   -(+,? ) +   * @ (&amp; ., -0/ &lt; &gt;= - 3 = ? 2 1, /2/23 3 $ 54  , (-  % 64 Y&amp;  87 ! + [??*(- @: E 3   :9 (-  &lt; ?@&amp; % &amp;( 3 &amp; *?  / 3   [% @: ,3 % - ; ? 3 : 2  %  $ 3 )? %*- ) 3 =&gt; 1/0/13 3 $  4 (-  , &lt;4 &amp;0E %¨·· ¤¦¸\·® ª °h±Å©."
È&apos;¥t®tª.¤Åîb¤¦¸\#· ½©­¨®%\° ª. %® \£ ©Äª%©­¥%°h±Å®%¥¿¤p¸ ¢: ¤¦¥#%® \£ ©Ìª%©­±¦¨®%%¤Å®%¤¦Í&lt;¤Å®ãc»½ J ü= % È &quot; J ü ´ %® ¨·² · ¤¦¸\·Ä®%/=¥t©«®¿¥.¤  .¤p¸\·
J üaÈ &quot; &lt;ý  J üaÈ  &quot; &lt;%» °\· £h±Åã ½j» °\ªt²O½j» . ©­¨¥t©4¤¦¸¼ª/ ^³h°\®c¨¸Ó¤¦¸b² ÃXª%©­¨¥t©K¤¦¸ ®%¨·· ¤¦¸\·Ö®%%»  &quot;0# / ÇE¾Nã
Ã » ¸l®tª..%\£ »b¬Y¥%(*Ê %©­¨¥t©#¤p¸X ®%¨·· %¤¦¹©®%#¤p¥^ª%» °\· £h±Åãc§hª%»§$»ª%%® ¤Å»  ®%£\%©­¨¥t©=¤¦¸fª. :¤p¥:%°\ª%/ *¥% %® %¨·· ¤¦¸\.¤Å®%£h¹Aª%© Â %%¥ % \§v°\®=.£Ïª.°h±Å©¯¤¦¸Y®%£\©#¥t©X² Â \°   »ª ¬\©­Ã«¤¦¥%¤¦» ¸¯±¦¤¦¥%®n®%¨·· ¤¦¸\·\À» % ±Å© §h¨¥.#¥ %£\©Ñ¤¦¸\\.£×®t»î©­¸ » %\° ³v¥t © « ®a»½®%£\©Äª.°h±Å©­¥a¨ª%^© ¨Ã«ÃX©­¥% ¨¸lãÆª/  hª%©­¥«Àb®%\£ ©Äª.\©«ª=¨ª%^© ¤¦· \¸ »ª% 2 Ç Ð© 1 ¹¤¦· %.± ¦¤Å· £l® %±Åã .¥ ±Å»(%¨·· ¤p¸\·í½j»ª J ü ´ ª.°h±Å©­¥Ò¥t©«®%¥Ä®% J %¤¦¸hÃX©f®%  ·©­¸\©«ª/¨±ª.% hª
"The noisy channel model has been applied to a wide range of problems, including spelling correction."
These models consist of two components: a source model and a channel model.
Very little research has gone into improving the channel model for spelling correction.
"This paper describes a new channel model for spelling correction, based on generic string to string edits."
Using this model gives significant performance improvements compared to previously proposed models.
"The noisy channel model[REF_CITE]has been successfully applied to a wide range of problems, including spelling correction."
These models consist of two components: a source model and a channel model.
"For many applications, people have devoted considerable energy to improving both components, with resulting improvements in overall system accuracy."
"However, relatively little research has gone into improving the channel model for spelling correction."
"This paper describes an improvement to noisy channel spelling correction via a more powerful model of spelling errors, be they typing mistakes or cognitive errors, than has previously been employed."
"Our model works by learning generic string to string edits, along with the probabilities of each of these edits."
This more powerful model gives significant improvements in accuracy over previous approaches to noisy channel spelling correction.
This paper will address the problem of automatically training a system to correct generic single word spelling errors.[Footnote_1]
1 Two very nice overviews of spelling correction can be found[REF_CITE]and[REF_CITE].
"We do not address the problem of correcting specific word set confusions such as {to,too,two} (see[REF_CITE])."
"We will define the spelling correction problem abstractly as follows: Given an alphabet Σ, a dictionary D consisting of strings in Σ * and a string s, where s ∉ D and s ∈ Σ * , find the word w∈ D that is most likely to have been erroneously input as s."
"The requirement that s∉D can be dropped, but it only makes sense to do so in the context of a sufficiently powerful language model."
"In a probabilistic system, we want to find argmax w"
P(w | s) .
"Applying Bayes’ Rule and dropping the constant denominator, we get the unnormalized posterior: argmax w"
P(s | w)*P(w) .
"We now have a noisy channel model for spelling correction, with two components, the source model P(w) and the channel model P(s | w)."
"The model assumes that natural language text is generated as follows: First a person chooses a word to output, according to the probability distribution P(w)."
"Then the person attempts to output the word w, but the noisy channel induces the person to output string s instead, according to the distribution P(s | w)."
"For instance, under typical circumstances we would expect P(the | the) to be very high, P(teh | the) to be relatively high and P(hippopotamus | the) to be extremely low."
"In this paper, we will refer to the channel model as the error model."
Two seminal papers first posed a noisy channel model solution to the spelling correction problem.
"In (Mayes,[REF_CITE]), word bigrams are used for the source model."
"For the error model, they first define the confusion set of a string s to include s, along with all words w in the dictionary D such that s can be derived from w by a single application of one of the four edit operations: (1) Add a single letter. (2) Delete a single letter. (3) Replace one letter with another. (4) Transpose two adjacent letters."
Let C be the number of words in the confusion set of d.
"Then they define the error model, for all s in the confusion set of d, as:  α if s = d P(s | d) =  (1- α ) otherwise  (C −1) \  the prior on a typed word being correct, and the remaining probability mass is distributed evenly among all other words in the confusion set."
"Like Mayes,[REF_CITE], they consider as candidate source words only those words that are a single basic edit away from s, using the same edit set as above."
"However, two improvements are made."
"First, instead of weighing all edits equally, each unique edit has a probability associated with it."
"Second, insertion and deletion probabilities are conditioned on context."
The probability of inserting or deleting a character is conditioned on the letter appearing immediately to the left of that character.
The error probabilities are derived by first assuming all edits are equiprobable.
"They use as a training corpus a set of space-delimited strings that were found in a large collection of text, and that (a) do not appear in their dictionary and (b) are no more than one edit away from a word that does appear in the dictionary."
"They iteratively run the spell checker over the training corpus to find corrections, then use these corrections to update the edit probabilities."
"Previous error models have all been based on Damerau-Levenshtein distance measures[REF_CITE], where the distance between two strings is the minimum number of single character insertions, substitutions and deletions (and in some cases, character pair transpositions) necessary to derive one string from another."
Improvements have been made by associating probabilities with individual edit operations.
"We propose a much more generic   /    2XU model allows all edit operations of the form    , ∈ Σ * . 3   WKH probability that when users intends to type  WKH\ W\  WKDW the edit operations allowed[REF_CITE], Mayes,[REF_CITE]and[REF_CITE], are properly subsumed by our generic string to string substitutions."
"In addition, we condition on the position in the string that the edit occurs in, 3  _ 361    ^"
"RI word, middle of word, end of word}.[Footnote_2] The position is determined by the location of   GLFWLRQDU\  Positional information is a powerful conditioning feature for rich edit operations."
2 Another good PSN feature would be morpheme boundary.
"For instance, P(e | a) does not vary greatly between the three positions mentioned above."
"However, P(ent | ant) is highly dependent upon position."
"People rarely mistype antler as entler, but often mistype reluctant as reluctent."
"Within the noisy channel framework, we can informally think of our error model as follows."
"First, a person picks a word to generate."
Then she picks a partition of the characters of that word.
"Then she types each partition, possibly erroneously."
"For example, a person might choose to generate the word physical."
"She would then pick a partition from the set of all possible partitions, say: ph y s i c al."
"Then she would generate each partition, possibly with errors."
"After choosing this particular word and partition, the probability of generating the string fisikle with the partition f i s i k le would be P(f | ph) *"
P(i | y) *
P(s | s) *
P(i | i) * P(k | c) *P(le | al).[Footnote_3]
3 We will leave off the positional conditioning information for simplicity.
The above example points to advantages of our model compared to previous models based on weighted Damerau-Levenshtein distance.
Note that neither P(f | ph) nor P(le | al) are modeled directly in the previous approaches to error modeling.
"A number of studies have pointed out that a high percentage of misspelled words are wrong due to a single letter insertion, substitution, or deletion, or from a letter pair transpositi[REF_CITE]."
"However, even if this is the case, it does not imply that nothing is to be gained by modeling more powerful edit operations."
"If somebody types the string confidant, we do not really want to model this error as P(a | e), but rather P(ant | ent)."
"And anticedent can more accurately be modeled by P(anti | ante), rather than P(i | e)."
"By taking a more generic approach to error modeling, we can more accurately model the errors people make."
A formal presentation of our model follows.
Let Part(w) be the set of all possible ways of partitioning string w into adjacent (possibly null) substrings.
"For a particular partition R ∈ Part(w), where |R|=j (R consists of j contiguous segments), let R i be the i th segment."
"Under our model,"
P(s | w) = |R| ∑ P(R | w) ∑ ∏ P (T | R ) i i
R∈Part (w) T∈Part (s) i=1 |T |=|R|
"One particular pair of alignments for s and w induces a set of edits that derive s from w. By only considering the best partitioning of s and w, we can simplify this to:"
"P(s | w) = |R| max R ∈ Part(w),T ∈ Part(s) P(R|w) ∏"
P(T i |R i ) i=1
"We do not yet have a good way to derive P(R | w), and in running experiments we determined that poorly modeling this distribution gave slightly worse performance than not modeling it at all, so in practice we drop this term."
"To train the model, we need a training set consisting of {s i , w i } string pairs, representing spelling errors s i paired with the correct spelling of the word w i ."
"We begin by aligning the letters in s i with those in w i based on minimizing the edit distance between s i and w i , based on single character insertions, deletions and substitutions."
"For instance, given the training pair &lt;akgsual, actual&gt;, this could be aligned as: a c t u a l a k g s u a l"
This corresponds to the sequence of edit operations: aÈa  Èg tÈs uÈu aÈa lÈl
"To allow for richer contextual information, we expand each nonmatch substitution to incorporate up to N additional adjacent edits."
"For example, for the first nonmatch edit in the example above, with N=2, we would generate the following substitutions: c È k ac È ak c È kg ac È akg ct È kgs"
"We would do similarly for the other nonmatch edits, and give each of these substitutions a fractional count."
We can then calculate the probability      FRXQW È    \ of the counts derived from our training data as explained above.
Estimating FRXQW  bit tricky.
"If we took a text corpus, then extracted all the spelling errors found in the corpus and then used those errors for training, FRXQW  VLPSO\  WKH number of times    WKH text corpus."
"But if we are training from a set of {s i , w i } tuples and not given an associated corpus, we can do the following: (a)"
From a large collection of representative WH[ (b) Adjust the count based on an estimate of the rate with which people make typing errors.
"Since the rate of errors varies widely and is difficult to measure, we can only crudely approximate it."
"Fortunately, we have found empirically that the results are not very sensitive to the value chosen."
"Essentially, we are doing one iteration of the Expectation-Maximization algorithm (Dempster,[REF_CITE])."
"The idea is that contexts that are useful will accumulate fractional counts across multiple instances, whereas contexts that are noise will not accumulate significant counts."
"Given a string s, where s ∉ D , we want to return argmax w"
P(w | s)P(w | context) .
"Our approach will be to return an n-best list of candidates according to the error model, and then rescore these candidates by taking into account the source probabilities."
"We are given a dictionary D and a set of parameters P, where each parameter is 3    , ∈ Σ * , meaning the SUREDELOLW\  QRLV\   )LUVW note that for a particular pair of strings {s, w} we can use the standard dynamic programming algorithm for finding edit distance by filling a |s|*|w| weight matrix[REF_CITE], with only minor changes."
"For computing the Damerau-Levenshtein distance between two strings, this can be done in O(|s|*|w|) time."
"When we allow generic edit operations, the complexity increases to O(|s| 2 *|w| 2 )."
"In filling in a cell (i,j) in the matrix for computing Damerau-Levenshtein distance we need only examine cells (i,j-1), (i-1,j) and (i-1,j-1)."
"With generic edits, we have to examine all cells (a,b) where a ≤ i and b ≤ j."
"We first precompile the dictionary into a trie, with each node in the trie corresponding to a vector of weights."
"If we think of the x-axis of the standard weight matrix for computing edit distance as corresponding to w (a word in the dictionary), then the vector at each node in the trie corresponds to a column in the weight matrix associated with computing the distance between s and the string prefix ending at that trie node. :   of tries."
We have one trie corresponding to   of some substitution in our parameter set.
"At every node in this trie, corresponding to a  trie consisting of all        VLGH  on the left hand side."
"We store the substitution probabilities at the terminal   %\     LQ reverse order, we can efficiently compute edit distance over the entire dictionary."
"We process the dictionary trie from the root downwards, filling in the weight vector at each node."
"To find the substitution parameters that are applicable, given a particular node in the trie and a particular position in the input string s (this corresponds to filling in one cell in one vector of a dictionary trie node) we trace up from the node to the root, while tracing  trie from the root."
"As we trace  trie, if we encounter a terminal node, we follow the pointer to the  trie, and then trace backwards from the position in s while  ."
Note that searching through a static dictionary D is not a requirement of our error model.
"It is possible that with a different search technique, we could apply our model to languages such as Turkish for which a static dictionary is inappropriate[REF_CITE]."
"We ran experiments using a 10,000-word corpus of common English spelling errors, paired with their correct spelling."
"Our dictionary contained approximately 200,000 entries, including all words in the test set."
The results in this section are obtained with a language model that assigns uniform probability to all words in the dictionary.
"In Table 1 we show K-best results for different maximum context window sizes, without using positional information."
"For instance, the 2-best accuracy is the percentage of time the correct answer is one of the top two answers returned by the system."
"Note that a maximum window of zero corresponds to the set of single character insertion, deletion and substitution edits, weighted with their probabilities."
"We see that, up to a point, additional context provides us with more accurate spelling correction and beyond that, additional context neither helps nor hurts."
"In Table 1, the row labelled CG shows the results when we allow the equivalent set of edit operations to those used[REF_CITE]."
This is a proper superset of the set of edits where the maximum window is zero and a proper subset of the edits where the maximum window is one.
"The CG model is essentially equivalent to the Church and Gale error model, except (a) the models above can posit an arbitrary number of edits and (b) we did not do parameter reestimation (see below)."
"Next, we measured how much we gain by conditioning on the position of the edit relative to the source word."
These results are shown in Table 2.
"As we expected, positional information helps more when using a richer edit set than when using only single character edits."
"For a maximum window size of 0, using positional information gives a 13% relative improvement in 1-best accuracy, whereas for a maximum window size of 4, the gain is 22%."
Our full strength model gives a 52% relative error reduction on 1-best accuracy compared to the CG model (95.0% compared to 89.5%).
"We experimented with iteratively reestimating parameters, as was done in the original formulation[REF_CITE]."
Doing so resulted in a slight degradation in performance.
The data we are using is much cleaner than that used[REF_CITE]which probably explains why reestimation benefited them in their experiments and did not give any benefit to the error models in our experiments.
"Next, we explore what happens to our results as we add a language model."
"In order to get errors in context, we took the Brown Corpus and found all occurrences of all words in our test set."
"Then we mapped these words to the incorrect spellings they were paired with in the test set, and ran our spell checker to correct the misspellings."
We used two language models.
"The first assumed all words are equally likely, i.e. the null language model used above."
The second used a trigram language model derived from a large collection of on-line text (not including the Brown Corpus).
"Because a spell checker is typically applied right after a word is typed, the language model only used left context."
"We show the results in Figure 1, where we used the error model with positional information and with a maximum context window of four, and used the language model to rescore the 5 best word candidates returned by the error model."
"Note that for the case of no language model, the results are lower than the results quoted above (e.g. a 1-best score above of 95.0%, compared to 93.9% in the graph)."
"This is because the results on the Brown Corpus are computed per token, whereas above we were computing results per type."
One question we wanted to ask is whether using a good language model would obviate the need for a good error model.
"In Figure 2, we applied the trigram model to resort the 5-best results of the CG model."
"We see that while a language model improves results, using the better error model (Figure 1) still gives significantly better results."
Using a language model with our best error model gives a 73.6% error reduction compared to using a language model with the CG error model.
This paper introduces a statistical model for query-relevant summarization: succinctly characterizing the relevance of a document to a query.
"Learning parameter values for the proposed model requires a large collec-tion of summarized documents, which we do not have, but as a proxy, we use a col-lection of FAQ (frequently-asked question) documents."
"Taking a learning approach en-ables a principled, quantitative evaluation of the proposed system, and the results of some initial experiments—on a collection of Usenet FAQs and on a FAQ-like set of customer-submitted questions to several large retail companies—suggest the plausi-bility of learning for summarization."
"An important distinction in document summarization is between generic summaries, which capture the cen-tral ideas of the document in much the same way that the abstract of this paper was designed to distill its salient points, and query-relevant summaries, which reflect the relevance of a document to a user-specified query."
"This paper discusses query-relevant summa-rization, sometimes also called “user-focused summa-rization”[REF_CITE]."
"Query-relevant summaries are especially important in the “needle(s) in a haystack” document retrieval problem: a user has an information need expressed as a query (What countries export smoked salmon?), and a retrieval system must locate within a large collection of documents those documents most likely to fulfill this need."
"Many interactive retrieval systems—web search engines like Altavista, for instance—present the user with a small set of candi-date relevant documents, each summarized; the user must then perform a kind of triage to identify likely relevant documents from this set."
"The web page sum-maries presented by most search engines are generic, not query-relevant, and thus provide very little guid-ance to the user in assessing relevance."
Query-relevant summarization (QRS) aims to provide a more effective characterization of a document by accounting for the user’s information need when generating a summary. marization is large-scale document retrieval.
"Given a user query ! , search engines typically first (a) identify a set of documents which appear potentially relevant to the query, and then (b) produce a short characterization &quot;$&amp;# %(&apos;)+! * of each document’s relevance to ! ."
The purpose of $&quot; #&amp;%(&apos;)+! * is to as-sist the user in finding documents that merit a more detailed inspection.
"As with almost all previous work on summarization, this paper focuses on the task of extractive summariza-tion: selecting as summaries text spans—either com-plete sentences or paragraphs—from the original doc-ument."
"From a document , and query - , the task of query-relevant summarization is to extract a portion . from , which best reveals how the document relates to the query."
"To begin, we start with a collection / of 0 ,213-(1465. triplets, where . is a human-constructed sum-mary of , relative to the query - ."
"From such a collec- tion of data, we fit the best function VXWZY[-(13(, \^]_. mapping document/query pairs to summaries."
"The mapping we use is a probabilistic one, meaning the system assigns a value `ZYa.2bc,214-S\ to every possible summary .[REF_CITE]-S\ ."
The QRS system will summarize a[REF_CITE]-S\ pair by selecting
"V[REF_CITE]-S, \ dfe6gihkojlenm `pYa.pbi,213-S\ ( def"
"There are at least two ways to interpret `pYapbi. 213-S, \ ."
"First, one could view `pYa.2bc214-S, \ as a “degree of be-lief” that the correct summary of , relative to - is . ."
"Of course, what constitutes a good summary in any setting is subjective: any two people performing the same summarization task will likely disagree on which part of the document to extract."
"We could, in principle, ask a large number of people to perform the same task."
Doing so would impose a distribution `p[REF_CITE]-S\ over candidate summaries.
"Under the second, or “frequen-tist” interpretation, `pYa.pbi,213-S\ is the fraction of people who would select . —equivalently, the probability that a person selected at random would prefer . as the sum-mary."
"The statistical model `p[REF_CITE]-S, \ is parametric, the values of which are learned by inspection of the 0 214-, (13.n5 triplets."
The learning process involves maximum-likelihood estimation of probabilistic lan-guage models and the statistical technique of shrink-age[REF_CITE].
"This probabilistic approach easily generalizes to the generic summarization setting, where there 0 is no query."
"In that case, the training data consists of ,213.n5 pairs, where . is a summary of the document , ."
"The goal s WZ , t, in ]u this . fromcase,documentsis to learntoandsummariesapply a. mappingThat is, find s Y[(, \ df def e6gihkjlenmo `pYa.2bc(, \"
"We have proposed using statistical learning to con-struct a summarization system, but have not yet dis-cussed the one crucial ingredient of any learning pro-cedure: training data."
"The ideal training data would contain a large number of heterogeneous documents, a large number of queries, and summaries of each doc-ument relative to each query."
We know of no such publicly-available collection.
"Many studies on text summarization have focused on the task of summariz-ing newswire text, but there is no obvious way to use news articles for query-relevant summarization within our proposed framework."
"In this paper, we propose a novel data collection for training a QRS model: frequently-asked question documents."
Each frequently-asked question document (FAQ) is comprised of questions and answers about a specific topic.
We view each answer in a FAQ as a summary of the document relative to the question which preceded it.
"That is, an FAQ with  ques-tion/answer pairs comes equipped with  different queries and summaries: the answer to the  th ques-tion is a summary of the document relative to the  th question."
"While a somewhat unorthodox perspective, this insight allows us to enlist FAQs as labeled train-ing data for the purpose of learning the parameters of a statistical QRS model."
FAQ data has some properties that make it particu-larly attractive for text learning: 
There exist a large number of Usenet FAQs— several thousand documents—publicly available on the Web 1 .
"Moreover, many large compa-nies maintain their own FAQs to streamline the customer-response process.  FAQs are generally well-structured documents, so the task of extracting the constituent parts (queries and answers) is amenable to automation."
"There have even been proposals for standardized FAQ formats, such[REF_CITE]and the Minimal Digest Form[REF_CITE].  Usenet FAQs cover an astonishingly wide variety of topics, ranging from extraterrestrial visitors to mutual-fund investing."
"If there’s an online com-munity of people with a common interest, there’s likely to be a Usenet FAQ on that subject."
"There has been a small amount of published work involving question/answer data, including[REF_CITE]and[REF_CITE]."
"Sato and Sato used FAQs as a source of summarization corpora, although in quite a different context than that presented here."
"Lin used the datasets from a question/answer task within the T ipster project, a dataset of considerably smaller size than the FAQs we employ."
Neither of these paper focused on a statistical machine learning approach to summarization.
"Given a query - and document , , the query-relevant summarization task is to find. ^ e6gchjle6mo `pYa.pbi213-S, \1 i the a posteriori most probable summary[REF_CITE]-S, \ ."
"Using Bayes’ rule, we can rewrite this expression as d .  e6gch2jlenm `pYa-bc14. , \ `pYa.pbi(, \R1 e6gch2jlenmo `pYa-  bi.\ `pY[.pbi (, \ 1  (1) o relevance fidelity where the last line follows by dropping the dependence on , in `pY[-bc.13,(\ ."
Equation (1) is a search problem: find the summary .  which maximizes the product of two factors: 1.
"The relevance `pY[-bi. \ of the query to the sum-mary: A document may contain some portions directly relevant to the query, and other sections bearing little or no relation to the query."
"Con-sider, for instance, the problem of summarizing a survey on the history of organized sports relative to the query “Who was Lou Gehrig?”"
"A summary mentioning Lou Gehrig is probably more relevant to this query than one describing the rules of vol-leyball, even if two-thirds of the survey happens to be about volleyball. 2."
"The fidelity `pY[pbi. , \ of the summary to the document: Among a set of candidate sum-maries whose relevance scores are comparable, we should prefer that summary . which is most representative of the document as a whole."
Sum-maries of documents relative to a query can of-ten mislead a reader into overestimating the rel-evance of an unrelated document.
"In particular, very long documents are likely (by sheer luck) to contain some portion which appears related to the query."
"A document having nothing to do with Lou Gehrig may include a mention of his name in passing, perhaps in the context of amyotropic lateral sclerosis, the disease from which he suf-fered."
"The fidelity term guards against this occur-rence by rewarding or penalizing candidate sum-maries, depending on whether they are germane to the main theme of the document."
"More generally, the fidelity term represents a prior, query-independent distribution over candi-date summaries."
"In addition to enforcing fidelity, this term could serve to distinguish between more and less fluent candidate summaries, in much the same way that traditional language models steer a speech dictation system towards more fluent hy-pothesized transcriptions."
"In words, (1) says that the best summary of a doc-ument relative to a query is relevant to the query (ex-hibits a large `pYa-bi.\ value) and also representative of the document from which it was extracted (exhibits a large `pYa.pbi,(\ value)."
"We now describe the paramet-ric form of these models, and how one can determine optimal values for these parameters using maximum-likelihood estimation."
"The type of statistical model we employ for both `pY[-bc. \ and `pY[pbi. ,\ is a unigram probability distri-bution over words; in other words, a language model."
"Stochastic models of language have been used exten-sively in speech recognition, optical character recogni-tion, and machine translati[REF_CITE]."
Language models have also started to find their way into document retrieval[REF_CITE].
"The fidelity model `Y[pbi. ,\ document , One simple d statistical 0r 1   characterization [Footnote_1]  5 is the offrequencyan  -wordof each word in , —in other words, a marginal distribu-tion over words d ."
1 Two online sources for FAQ data are[URL_CITE]and rtfm.mit.edu.
"That ¥¤ is. ,Thisif wordis not appearsonly intuitive  times, butin , , then `¢¡Y[£\ also the maximum-likelihood estimate for `¢¡(Ya¦\ ."
"Now imagine that, when asked to summarize , rel-ative to - , a person generates a summary from , in the following way: "
Select a length § for the summary according to some distribution ¨ ¡ . 
"Do for © dtª 13«1&amp;§ : – Select a word at random according to the distribution ` ¡ . (That is, throw all the words in , into a bag, pull one out, and then re-place it.) – Set R¬. (­® . the summary ."
"In following d this 0r¯n procedure  1 ¯ 1 ¯R , the ° 5 withpersonprobabilitywill generate ° `pYa.pbi,(\ d ¨ ¡ Ya§±\ ²  ` ¡ Y ¯ ¬ \ (2) ¬´³"
"Denoting by µ the set of all known words, and by ¶ Y[X·¸, \ the number of times that word appears in , , one can also write (2) as a multinomial distribution: ¹ Ya2bc. ,(\ d ¨º¡Y[§±\ (» ² ¼6½ ¹ Ya¦\)¾4¿ ( » ¼ ¡ À  (3)"
"In the text classification literature, this characteriza-tion of , is known as a “bag of words” model, since the distribution `¢¡ does not take account of the order of the words within the document , , but rather views , as an unordered set (“bag”) of words."
"Of course, ignoring word order amounts to discarding potentially valuable information."
"In Figure 3, for instance, the second ques-tion contains an anaphoric reference to the preceding question: a sophisticated context-sensitive model of language might be able to detect that it in this context refers to amniocentesis, but a context-free model will not."
"To gauge how well our proposed summarization tech-nique performs, we applied it to two different real-world collections of answered questions:"
Usenet FAQs: A collection of «ßå ª frequently-asked question documents from the comp  ª .*
The documents contained åßå questions/answer pairs in total.
"Call-center data: A collection of questions submitted by customers to the companies Air Canada, Ben and Jerry, Iomagic, and Mylex, along with the answers supplied by company representatives ª å¥1 ò question. /answerThese pairsfour .documents contain"
"We conducted an identical, parallel set of experi-ments on both."
"First, we used a randomly-selected subset of 70% of the question/answer pairs to calcu-late the language models ` o 1)` è 1Ð` ¡ 1)` é —a simple matter of counting word frequencies."
"Then, we used this d same 0 ì o 1 set ì è 1 of ì ¡ data 1 ì é 1 to ì êZ5 estimateusing shrinkagethe model. weightsWe re- ìë served the remaining 30% of the question/answer pairs to evaluate the performance of the system, in a manner described below."
"Figure 5 shows the progress of the EM algo-rithm in calculating maximum-likelihood ì values for the smoothing coefficients ë , for the first of the three runs on the ì Usenet data."
The quick convergence and the final ë values were essentially identical for the other partitions of this dataset.
"The call-center data’s convergence ì behavior was similar, although the final ë values were quite differ-ent."
Figure 6 shows the final model weights for the first of the three experiments on both datasets.
"For the Usenet FAQ data, the corpus language model is the best predictor of the query and thus receives the high-est weight."
"This may seem counterintuitive; one might suspect that answer to the query ( . , that is) would be most similar to, and therefore the best predictor of, the query."
"But the corpus model, while certainly bi-ased away from the distribution of words found in the query, contains (by construction) no zeros, whereas each summary model is typically very sparse."
"In the call-center data, the corpus model weight is lower at the expense of a higher document model weight."
"We suspect this arises from the fact that the documents in the Usenet data were all quite similar to one another in lexical content, in contrast to the call-center documents."
"As a result, in the call-center data the document containing . will appear much more rel-evant than the corpus as a whole."
"To evaluate the performance of the trained QRS model, we used the previously-unseen portion of the FAQ data in the following way."
"For each test [Y 213-S, \ pair, we recorded how highly the system ranked the correct summary .  —the answer to - in , —relative to the other answers in , ."
We repeated this entire se-quence three times for both the Usenet and the call-center data.
"For these datasets, we discovered that using a uni-form fidelity term in place of the `pYa. bS(, \ model de-scribed above yields essentially the same result."
"This is not surprising: while the fidelity term is an important component of a real summarization system, our evalu-ation was conducted in an answer-locating framework, and in this context the fidelity term—enforcing that the summary be similar to the entire document from which of a QRS system using a uniform fidelity model, the fourth corresponds to a standard tfidf-based ranking method[REF_CITE], and the last column reflects the performance of randomly guessing the correct sum-mary from all answers in the document. it was drawn—is not so important 01 0  1 1  1 1*[Footnote_2] 5 ."
"2 By incorporating a ÔSÝ model into the relevance model, equation (6) has implicitly resurrected the dependence on % which we dropped, for the sake of simplicity, in deriving (1)."
"From a set of rankings , one can measure the the quality of a ranking algorithm using the harmonic mean rank: 3 d 2  &amp;³  4 Þ def ¬"
"A lower number indicates better performance; 3 dXª , which is optimal, means that the algorithm consis-tently assigns the first rank to the correct answer."
Ta-ble 1 shows the harmonic mean rank on the two col-lections.
The third column of Table 1 shows the result
The reader may by now have realized that our approach to the QRS problem may be portable to the problem of question-answering.
"By question-answering, we mean a system which automatically extracts from a poten-tially lengthy document (or set of documents) the an-swer to a user-specified question."
Devising a high-quality question-answering system would be of great service to anyone lacking the inclination to read an entire user’s manual just to find the answer to a sin-gle question.
The success of the various automated question-answering services on the Internet (such as AskJeeves) underscores the commercial importance of this task.
One can cast answer-finding as a traditional docu-ment retrieval problem by considering each candidate answer as an isolated document and ranking each can-didate answer by relevance to the query.
Traditional tfidf-based ranking of answers will reward candidate answers with many words in common with the query.
"Employing traditional vector-space retrieval to find an-swers seems attractive, since tfidf is a standard, time-tested algorithm in the toolbox of any IR professional."
What this paper has described is a first step towards more sophisticated models of question-answering.
"First, we have dispensed with the simplifying assump-tion that the candidate answers are independent of one another by using a model which explicitly accounts for the correlation between text blocks—candidate answers—within a single document."
"Second, we have put forward e6gch2jlenm a principled 5 `pYa. (b ,214-S\ statistical model for answer-ranking; has a probabilistic in-terpretation as the best answer to - within , is . ."
Question-answering and query-relevant summariza-tion are of course not one and the same.
"For one, the criterion of containing an answer to a question is rather stricter than mere relevance."
"Put another way, only a small number of documents actually contain the an-swer to a given query, while every document can in principle be summarized with respect to that query."
"Second, it would seem that the `pYa2bc. ,(\ term, which acts as a prior on summaries in (1), is less appropriate in a question-answering setting, where it is less impor-tant that a candidate answer to a query bears resem-blance to the document containing it."
"Although this paper focuses on the task of query-relevant summarization, the core ideas—formulating a probabilistic model of the problem and learning the values of this model automatically from FAQ-like data—are equally applicable to generic summariza-tion."
"In this case, one seeks the summary which best typifies the document."
"Applying Bayes’ rule as in (1), .i e6gchjle6m p` Y[pbi. ,\ e6gchjle6mo `pYañ,  bc.\ p` Ya.\ d (6) o generative prior"
"The first term on the right is a generative model of doc-uments from summaries, and the second is a prior dis-tribution over summaries."
One can think of this factor-ization in terms of a dialogue.
"Alice, a newspaper edi-tor, has an idea . for a story, which she relates to Bob."
"Bob researches and writes the story , , which we can view as a “corruption” of Alice’s original idea . ."
"The task of generic summarization is to recover . , given only the generated document , , a model `pYa,ñbc.\ of how the Alice generates summaries from documents, and a prior distribution `pY[.\ on ideas . ."
The central problem in information theory is reliable communication through an unreliable channel.
"We can interpret Alice’s idea . as the original signal, and the process by which Bob turns this idea into a document , as the channel, which corrupts the original message."
"The summarizer’s task is to “decode” the original, con-densed message from the document."
We point out this source-channel perspective be-cause of the increasing influence that information the-ory has exerted on language and information-related applications.
"For instance, the source-channel model has been used for non-extractive summarization, gen-erating titles automatically from news articles[REF_CITE]."
"The factorization in (6) is superficially similar to (1), but there is an important difference: ¹ Y[,lbi.\ is a gener-ative, from a summary to a larger document, whereas ¹ Ya-bi. \ is compressive, from a summary to a smaller query."
This distinction is likely to translate in prac-tice into quite different statistical models and training procedures in the two cases.
The task of summarization is difficult to define and even more difficult to automate.
"Historically, a re-warding line of attack for automating language-related problems has been to take a machine learning perspec-tive: let a computer learn how to perform the task by “watching” a human perform it many times."
This is the strategy we have pursued here.
"There has been some work on learning a probabilis-tic model of summarization from text; some of the ear-liest work on this was due[REF_CITE], who used a collection of manually-summarized text to learn the weights for a set of features used in a generic summarization system."
"More recently, there has been work on building more complex, structured models—probabilistic syntax trees—to compress sin-gle sentences[REF_CITE]."
"These previous approaches focus mainly on the generic summarization task, not query relevant summarization."
The language modelling approach described here does suffer from a common flaw within text processing systems: the problem of synonymy.
"A candidate an- swer containing the term Constantinople is likely to be relevant to a question about Istanbul, but rec-ognizing this correspondence requires a step beyond word frequency histograms."
"Synonymy has received much attention within the document retrieval com-munity recently, and researchers have applied a vari-ety of heuristic and statistical techniques—including pseudo-relevance feedback and local context analy-sis ([REF_CITE];"
Some recent work in statistical IR has extended the ba-sic language modelling approaches to account for word synonymy[REF_CITE].
This paper has proposed the use of two novel datasets for summarization: the frequently-asked questions (FAQs) from Usenet archives and ques-tion/answer pairs from the call centers of retail compa-nies.
"Clearly this data isn’t a perfect fit for the task of building a QRS system: after all, answers are not sum-maries."
"However, we believe that the FAQs represent a reasonable source of query-related document conden-sations."
"Furthermore, using FAQs allows us to assess the effectiveness of applying standard statistical learn-ing machinery—maximum-likelihood estimation, the EM algorithm, and so on—to the QRS problem."
"More importantly, it allows us to evaluate our results in a rig-orous, non-heuristic way."
"Although this work is meant as an opening salvo in the battle to conquer summa-rization with quantitative, statistical weapons, we ex-pect in the future to enlist linguistic, semantic, and other non-statistical tools which have shown promise in condensing text."
This paper presents an algorithm for text summarization using the the-matic hierarchy of a text.
"The algo-rithm is intended to generate a one-page summary for the user, thereby enabling the user to skim large vol-umes of an electronic book on a computer display."
The algorithm rst detects the thematic hierarchy of a source text with lexical cohe-sion measured by term repetitions.
"Then, it identi es boundary sen-tences at which a topic of appropri-ate grading probably starts."
"Finally, it generates a structured summary indicating the outline of the the-matic hierarchy."
"This paper mainly describes and evaluates the part for boundary sentence identi cation in the algorithm, and then brie y dis-cusses the readability of one-page summaries."
"This paper presents an algorithm for text summarization using the thematic hierarchy of a long text, especially for use by readers who want to skim an electronic book of sev-eral dozens of pages on a computer display."
"For those who want an outline to quickly understand important parts of a long text, a one-page summary is more useful than a quarter-size summary, such as that gener-ated by a typical automatic text summa-rizer."
"Moreover, a one-page summary helps users reading a long text online because the whole summary can appear at one time on the screen of a computer display."
"To make such a highly compressed sum-mary, topics of appropriate grading must be extracted according to the size of the sum-mary to be output, and selected topics must be condensed as much as possible."
"The pro-posed algorithm decomposes a text into an appropriate number of textual units by their subtopics, and then generates short extracts for each unit."
"For example, if a thirty-sentence summary is required to contain as many topics as possible, the proposed algo-rithm decomposes a source text into approxi-mately ten textual units, and then generates a summary composed of two- or three-sentence extracts of these units."
The proposed algorithm consists of three stages.
"In the rst stage, it detects the the-matic hierarchy of a source text to decom-pose a source text into an appropriate num-ber of textual units of approximately the same size."
"In the second stage, it adjusts each boundary between these textual units to iden-tify a boundary sentence , indicating where a topic corresponding to a textual unit proba-bly starts."
It then selects a lead sentence that probably indicates the contents of subsequent parts in the same textual unit.
"In the last stage, it generates a structured summary of these sentences, thereby providing an outline of the thematic hierarchy of the source text."
The remainder of this paper includes the following: an explanation of problems in one-page summarization that the proposed algo-rithm is intended to solve; brief explanations of a previously published algorithm for the-matic hierarchy detecti[REF_CITE]and a problem that must be solved to successfully realize one-page summarization; a description and evaluation of the algorithm for boundary sentence identi cation; a brief explanation of an algorithm for structured summary con-struction; and some points of discussion on one-page summarization for further research.
This section examines problems in one-page summarization.
The proposed algorithm is intended to solve three such problems.
The rst problem is related to text decom-position.
Newspaper editorials or technical papers can be decomposed based on their rhetorical structures.
"However, a long ag-gregated text, such as a long technical sur-vey report, cannot be decomposed in the same way, because large textual units, such as those longer than one section, are usually constructed with only weak and vague rela-tionships."
"Likewise, their arrangement may seem almost at random if analyzed accord-ing to their logical or rhetorical relationships."
"Thus, a method for detecting such large tex-tual units is required."
"Since a large textual unit often corresponds to a logical document element, such as a part or section, rendering features of logical ele-ments can have an important role in detecting such a unit."
"For example, a section header is distinguishable because it often consists of a decimal number followed by capitalized words."
"However, a method for detecting a large textual unit by rendering features is not expected to have wide range of applicability."
"In other words, since the process for render-ing features of logical elements varies accord-ing to document type, heuristic rules for de-tection must be prepared for every document type."
That is a problem.
"Moreover, the log-ical structure of a text does not always cor-respond to its thematic hierarchy, especially if a section consists of an overview clause fol-lowed by other clauses that can be divided into several groups by their subtopics."
"Since then, based on Hearst&apos;s work (1994), an algorithm for detecting the thematic hi- erarchy of a text using only lexical cohesi[REF_CITE]measured by term repetitions was developed[REF_CITE]."
"In comparison with some alternatives[REF_CITE], one of the features of the algorithm is that it can decompose a text into thematic textual units of approxi-mately the same size, ranging from units just smaller than the entire text to units of about one paragraph."
"In this paper, a summariza-tion algorithm based on this feature is pro-posed."
The second problem is related to the tex-tual coherence of a one-page summary itself.
"A three-sentence extract of a large text, which the proposed algorithm is designed to gener-ate for an appropriate grading topic, tend to form a collection of unrelated sentences if it is generated by simple extraction of important sentences."
"Furthermore, the summary should provides new information to a reader, so an introduction is necessary to help a reader un-derstand it."
Figure 4 shows a summary exam-ple of a technical survey report consisting of one hundred thousand characters.
It was gen-erated by extracting sentences with multiple signi cant terms as determined by the like-lihood ratio test of goodness-of- t for term frequency distribution.
"It seems to have sen-tences with some important concepts (key-words), but they do not relate much to one another."
"Moreover, inferring the contexts in which they appear is diÆcult."
"To prevent this problem, the proposed al-gorithm is designed to extract sentences from only the lead part of every topic."
The third problem is related to the read-ability of a summary.
"A one-page summary is much shorter than a very long text, such as a one-hundred-page book, but is too long to read easily without some breaks indicating segues of topics."
"Even for an entire exposi-tory text, for which a method for displaying the thematic hierarchy with generated head-ers was proposed to assist a reader to explore the content[REF_CITE], a good summary is required to help a user understand quickly."
"To improve readability, the proposed algo-rithm divides every one-page summary into severalheading-likepartssentence, each offollowedwhich byconsistssome para-of a graphs."
"In the rst stage, the proposed algorithm uses the previously published algorithm[REF_CITE]to detect the thematic hierarchy of a termtext basedrepetitionson lexical."
The cohesionoutput ofmeasuredthis stagebyis a set of lists consisting of thematic boundary candidate sections (TBCS).
The lists corre-spond individually to every layer of the hier-archy and are composed of TBCSs that sep-arate the source text into thematic textual units of approximately the same size.
"Algorithm scoreFirstat, thexed-widthalgorithmintervalscalculatesin a sourcea cohesiontext."
"According to Hearst&apos;s work (1994), a cohesion score is calculated based on the lexical sim-ilarity(which ofaretwoeightadjacenttimes largerxed-widththan thewindowsinterval width) set at a speci c point by the following formula: c ( b l ; b r ) = q t w t;bl w t;br t w t 2 ;bl t w 2 t;br whereleft and right windows, respectively, and b l and b r are the textual block in the w t;bl isis thethe frequencyfrequency of term 1 t for b l , and w t;br t for b r ."
"Hereafter, the point between the left and right windows is referred toTheas thealgorithmreference pointthenofdetectsa cohesionthematicscore. boundariesfour-item movingaccordingaverageto the(arithmeticminimal pointsmean ofof four consecutive scores) of the cohesion score series."
"After that, it selects the textual area contributingand identi estheit asmosta TBCSto every. minimal value tectionFigureexample[Footnote_1] shows, wherethe results of a TBCS de- FC is, Forward Co-hesion, a series of average values plotted at the reference point of the rst averaged score, and BC is, Backward Cohesion, a series of averaged values plotted at the reference point areaof thejustlastbeforeaveragedthe pointscoreat. whichSince the textual FC plotted isaveragedalways cohesionin the leftscoreswindowis calculatedwhen one, of the FC in-dicates the strength of forward (left-to-right) cohesion at a point."
"1All content words (i.e., verbs, nouns, and adjec-tives) extracted by a tokenizer for Japanese sentences."
"Conversely, BC indicates the strength of backward cohesion at a point."
"In the gure, EP is, Equilibrium Point, the point at which FC and BC have an identi-cal value."
"The algorithm checks for FC and BC starting from the beginning till the end of the source text; and it records a T BCS , as depicted by the rectangle, whenever an equi-librium point is detected (see[REF_CITE]for more information). sultingFor athematicsample texthierarchy, Figurethat2 showswas detectedthe re- by the aforementioned procedure using vary-ing window widths (the ordinates)."
Each hor-izontal sequence of rectangles depicts a list of TBCSs detected using a speci c window width.
"To narrow the width of candidate sections, the algorithm then uni es a TBCS with an-other TBCS in the layer immediate below."
"It continued the process until TBCSs in all lay-ers, from the top to the bottom, are uni ed."
"After that, it outputs the thematic hierarchy as a set of lists of TBCS data: i : layer index of the thematic hierarchy B ( i )[ j ]: TBCS data containing the following data members: ep : equilibrium point range : thematic boundary candidate section."
"In Figure 2, for example, B (1)[1] is uni ed with B (2)[1] ; B (3)[4] ; B ([Footnote_4])[6] ; : : : , and the val-ues of its data members (ep and range) are replaced by those of the uni ed TBCS in the bottom layer, which has been detected using the minimum window width (40 words)."
4 Monthly reports written for a Japanese company by a Japanese professor living in the U.S.A.
"Table 1 summarizes the accuracy of the-matic hierarchy detection in an experiment using the following three kinds of Japanese text as test data: a technical survey report [Footnote_2] that consists of three main sections and con-tains 17,816 content words; eight series of newspaper columns [Footnote_3] , each of which consists of 4 to 24 articles containing about 400 words; and twelve economic research reports 4 , each of which consists of about ten articles con-taining 33 to 2,375 words."
"2 \Progress Report of Technical Committee on Net-work Access&quot; in Survey on Natural Language Process-ing Systems by Japan Electronic Industry Develop-ment Association, chapter 4, pp. 117{197, Mar. 1997."
3 Obtained from the Daily Yomiuri On-line ([URL_CITE]
"In the table, cor. denotes the number of the correct data values composed of the starting points of sections that contain the same num-ber of words or more than the window width listed in the same row [Footnote_5] ."
"5 Only headings and intentional breaks, such as symbol lines inserted to separate a prologue or epi-logue from a main body, are used as correct bound-aries. As a result, the precision rates of using smaller window widths tend to degrade because of insuÆcient amounts of correct data."
"In addition, res. de-notes the number of TBCSs."
"The original TBCS columns list the recall and precision rates of detected TBCSs before TBCS uni ca-tion, and the uni ed TBCS columns list those rates after TBCS uni cation."
"On each layer, the width of candidate sections for original TBCS is about half of the window width; and that of uni ed[REF_CITE]words (about half of the minimum window width)."
The gures shown in parentheses are the baseline rates corresponding to random selection.
"That is, parts are randomly selected from the source text whose total size is equal to the total area size of TBCSs."
"As the boundary gures indicate, the pro-posed algorithm decomposes a text into tex-tual units of about equivalent window widths."
"In addition, the rates of detected TBCSs are clearly larger than their baselines."
"Further- more, for two relatively large series of news-paper columns, the major boundaries were detected properly."
"That is, using larger win-dow widths, those boundaries were selectively detected that separate groups of columns by their subtopics."
"For example, the starting point of a set of three consecutive columns identically entitled \The Great Cultural Rev-olution&quot; in the \Chinese Revolution&quot; series was detected using 1,280 word width window, as well as those of other three sets of consec-utive columns entitled identically."
"Thus, the proposed algorithm is expected to be e ec-tive for arbitrarily selecting the size of tex-tual units corresponding to di erent grading topics."
"However, there are problems about how to determine a boundary point in the range de-ned by a TBCS."
"Although the previously published algorithm[REF_CITE]deter-mines a boundary point with minimal points of cohesion scores for the smallest window width, the accuracy degrades substantially (see Table 3)."
The boundary sentence identi-cation algorithm given below is a solution to this problem.
"In the second stage, from sentences in a TBCS, the algorithm identi es a boundary sentence , indicating where a topic corre-sponding to a textual unit probably starts, and selects a lead sentence that probably in-dicates the contents of subsequent parts in the same textual unit."
Figure 3 shows the algo-rithm in detail.
"In steps 2 and 3, boundaries are identi ed and lead sentences are selected based on two kinds of relevance scores for a sentence: for-ward relevance indicating the sentence rele-vance to the textual unit immediately after the sentence, and backward relevance indicat-ing the sentence relevance to the textual unit immediately before the sentence."
The di er-ence between the forward and the backward relevance is referred to as relative forward rel- evance .
"Forward or backward relevance is calcu-lated using the formula below, where every textual unit is partitioned at the equilibrium points of two adjacent TBCSs in the target layer, the equilibrium point of each TBCS is initially set by the thematic hierarchy detec-tion algorithm, and the point is replaced by the location of the boundary sentence after the boundary sentence is identi ed (i.e., step 2b is completed)."
S;u = j S 1 j X tf log( j dfD j )) r t 2 S j u j t;u df total number of xed-width blocks t where term t appears
The use of this formula was proposed as an e ective and simple measure for term im-portance estimati[REF_CITE][Footnote_6] .
6 An experiment reported[REF_CITE]indi-
"It is a modi ed version of entropy, where informa-tion bit (log part of the formula) is calcu-lated by reducing the e ect of term repeti-tions in a short period."
"The modi cation was done to increase the scores for an important term higher, based on the reported observa-tion that content bearing words tend to occur in clumps[REF_CITE]."
"Table 2 summarizes an example of bound-ary sentence identi cation of a TBCS located just before the 12,000th word in Figure 2."
"Ev-ery row in the table except the rst row, which is marked with O:R: , shows a candidate sen-tence."
"The row marked B:S: shows a bound-ary sentence, which has positive relative for-ward relevance (0.016 in the fourth column of the row) and the greatest increment from the previous value (-0.017)."
"The row marked L:S: shows a lead sentence, which has the great-est forward relevance (0.022 in the third col-umn of the row) among all sentences after the boundary sentence."
Table 3 shows recall and precision rates of the boundary identi cation algorithm in the same format as Table 1.
"Compared with the results obtained using the previous version of the algorithm[REF_CITE], as shown in the minimal cohesion columns, the proposed al-gorithm identi es more accurate boundaries (the boundary sentence columns)."
"In ad-dition, boundary sentence identi cation was successful for 75% of the correct TBCSs, that is, TBCSs including correct boundaries [Footnote_7] (see uni ed TBCS in Table 1)."
"7 For the correct TBCSs, the average number of boundary sentence candidates is 4.4."
"Thus, the proposed boundary sentence identi cation algorithm is judged to be e ective."
Table 3 also summarizes a feature of the proposed algorithm that it tends to detect and identify headings as boundary sentences (the heading rate columns).
"For the part cor-responding to larger textual units, which the proposed algorithm mainly used, the gures in the overall columns indicate that half of boundary sentences or more are identical to headings in the original text; and the gures in the identi cation columns indicate that the proposed algorithm identi es headings as boundary sentences for more than 80% of the case where TBCSs including headings."
"In the third and last stage, the algorithm outputs the boundary and lead sentences of TBCSs on a layer that probably corresponds to topics of appropriate grading."
"Based on the ratio of source text size to a given summary size, the algorithm chooses a layer that con-tains an appropriate number of TBCSs, and generates a summary with some breaks to in-dicate thematic changes."
"For example, to generate a 1,000-character summary consisting of several parts of ap-proximately 200 characters for each topic, a text decomposition consisting of ve textual units is appropriate for summarization."
"Since the sample text used here was decomposed into ve textual units on the B (2) layer (see Figure 2), it outputs the boundary sentences and lead sentences of all TBCSs in B (2)."
"Figure 5 shows a one-page summary of a tech-nical survey report, where (a) is a part of the summary automatically generated, and (b) is its translation."
It corresponds to the part B (1)[of2] the(in Figuresource2text).
"Itbetweenis composed B (1)of[1]threeand parts corresponding to B (2)[1], B (2)[2], and B (3)[6]."
"Each part consists of a boundary sen-tence, presented as a heading, followed by a lead sentence."
"In comparison with the keyword-based summary shown in Figure 4, generated in the process described in Section 2, the one-page summary gives a good impression as being easy to understand."
"In fact, when we in-formally asked more than ve colleagues to state their impression of these summaries, they agreed with this point."
"As described in Section 2, one of the reasons for the good impression should be the di erence in coher-ence."
"The relationship among sentences in the keyword-based summary is not clear; con-versely, the second sentence of the one-page summary introduces the outline of the clause, and it is closely related to the sentences that follow it."
"The fact that the one-page sum-mary provides at least two sentences, includ-ing a heading, for each topic is also considered to make coherence strong."
"As shown in Table 3, the proposed algo-rithm is expected to extract headings e ec-tively."
"However, there is a problem that de-tected headings do not always correspond to topics of appropriate grading."
"For example, the second boundary sentence in the exam-ple is not appropriate because it is a heading of a subclause much smaller than the window width corresponding to B (2)[2], and its pre-vious sentence \4.3.2 Technical Trend of IR Techniques&quot; is more appropriate one."
This example is also related to another lim-itation of the proposed algorithm.
"Since there is no outline description in the subsequent part of the heading of clause 4.3.2, the pro-posed algorithm could not generate a coher-ent extract if it had identi ed the heading as a boundary sentence."
It is a future issue to develop more elab-orated algorithm for summarizing detected topics especially for the user who wants richer information than that can be provided in a extract consisting of two or three sentences.
This paper has proposed an algorithm for one-page summarization to help a user skim a long text.
It has mainly described and re-ported the e ectiveness of the boundary sen-tence identi cation part of the algorithm.
It has also discussed the readability of one-page summaries.
The e ectiveness of structured summaries using the thematic hierarchy is an issue for future evaluation.
_4 ¡t¡¢D£¤£ ¥§¦©¨4ª«£ ¬­ª­¬®¢¯2¨4°ª± ÁÃÂ ¬«´2¢¶µ·¨4¢¸¹£t&amp;¬«´2¢4°ª­¬«£ª­°d£?/º ¬­´2¢ztºS £t¡¦zÇ _ ¡¨4tÈ ¡ ´µÉ£ ¥Ètºut£t¡¦Ê£t´   ¨¦o¦o°d ºÇaË µ·£t&quot;¸¹´2¢D£t¡¢D£&quot;¹ÌS²     ¡ ¨4ª­£ Y¬­¢ £ ¥4 =ËÐ ¨4¦±¦o°d º ©¯¡¢u²  4 ´ÒÑS¬®Ï¡  £  £ÉÑe°ª­¨°d£ a£ ¥ µ·¡° ¬«Ö:¬­ª­¬«£?ºo´µ\£ ¥4È°d44  × Ø &gt;\502.Éb==Ù[REF_CITE]W.f&gt; Ú ¢7£ t£(Ï¡¸°ÏÆ&amp;°¨£t´2¦o°d£   ¬«´2¢ ´µ¼£t¹ÌS£  ¥4¶¦o°¬­¢ ÛætoÇç¯Ç&apos;Û(4;èé°¢4¬=:¨ ºÆ&quot;=tà 4òWô¹õö£t¹Ìu£´eít¡¸¹£ fÏ4¡°ª«£Ç÷ø¬«£Í ¬«£¥Ð£¥Y£¥=  ¬«´2¢Y´µLîÉïñðæòæót&amp; Ö°t¡Ï_tºut£t¡¦oÆ ¬­£Ã¬®Î°ª­t´¼&amp;´2 ¬«Öª­È£t´ 4 ´uÏ4¨4¸¹Î kµ·´ t&amp;´Þ¡¢7Ï¬­°ª«´¯2¨Ç
"Extractive summarization techniques cannot generate document summaries shorter than a single sentence, some-thing that is often required."
An ideal summarization system would under-stand each document and generate an appropriate summary directly from the results of that understanding.
A more practical approach to this problem re-sults in the use of an approximation: viewing summarization as a problem analogous to statistical machine trans-lation.
The issue then becomes one of generating a target document in a more concise language from a source docu-ment in a more verbose language.
"This paper presents results on experiments using this approach, in which statisti-cal models of the term selection and term ordering are jointly applied to pro-duce summaries in a style learned from a training corpus."
"Generating effective summaries requires the abil-ity to select, evaluate, order and aggregate items of information according to their relevance to a particular subject or for a particular purpose."
Most previous work on summarization has fo-cused on extractive summarization: selecting text spans - either complete sentences or paragraphs – from the original document.
These extracts are then arranged in a linear order (usually the same order as in the original document) to form a sum-mary document.
"There are several possible draw-backs to this approach, one of which is the fo-cus of this paper: the inability to generate co-herent summaries shorter than the smallest text-spans being considered – usually a sentence, and sometimes a paragraph."
"This can be a problem, because in many situations, a short headline style indicative summary is desired."
"Since, in many cases, the most important information in the doc-ument is scattered across multiple sentences, this is a problem for extractive summarization; worse, sentences ranked best for summary selection of-ten tend to be even longer than the average sen-tence in the document."
"This paper describes an alternative approach to summarization capable of generating summaries shorter than a sentence, some examples of which are given in Figure 1."
It does so by building sta-tistical models for content selection and surface realization.
"This paper reviews the framework, discusses some of the pros and cons of this ap-proach using examples from our corpus of news wire stories, and presents an initial evaluation."
"Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases[REF_CITE], positional indi-cators[REF_CITE], lexical occurrence statistics[REF_CITE], probabilistic mea-sures for token salience[REF_CITE], and the use of implicit discourse structure[REF_CITE]."
"Work on combining an information ex-traction phase followed by generation has also been reported: for instance, the FRUMP sys-tem[REF_CITE]used templates for both in- formation extraction and presentation."
"More recently, summarizers using sophisticated post-extraction strategies, such as revisi[REF_CITE], and sophisticated grammar-based gen-erati[REF_CITE]have also been presented."
"The work reported in this paper is most closely related to work on statistical machine transla-tion, particularly the ‘IBM-style’ work on CAN - DIDE[REF_CITE]."
"This approach was based on a statistical translation model that mapped between sets of words in a source lan-guage and sets of words in a target language, at the same time using an ordering model to con-strain possible token sequences in a target lan-guage based on likelihood."
"In a similar vein, a summarizer can be considered to be ‘translat-ing’ between two languages: one verbose and the other succinct[REF_CITE]."
"However, by definition, the translation during summarization is lossy, and consequently, somewhat easier to design and ex-periment with."
"As we will discuss in this paper, we built several models of varying complexity; [Footnote_1] even the simplest one did reasonably well at sum-marization, whereas it would have been severely deficient at (traditional) translation."
"1 We have very recently become aware of related work that builds upon more complex, structured models – syn-tax trees – to compress single sentences[REF_CITE]; our work differs from that work in (i) the level of compression possible (much more) and, (ii) accuracy possi-ble (less)."
"As in any language generation task, summariza-tion can be conceptually modeled as consisting of two major sub-tasks: (1) content selection, and (2) surface realization."
"Parameters for statistical models of both of these tasks were estimated from a training corpus of approximately 25,000 1997 Reuters news-wire articles on politics, technol-ogy, health, sports and business."
"The target docu-ments – the summaries – that the system needed to learn the translation mapping to, were the head-lines accompanying the news stories."
"The documents were preprocessed before training: formatting and mark-up information, such as font changes and SGML/HTML tags, was removed; punctuation, except apostrophes, was also removed."
"Apart from these two steps, no other normalization was performed."
"It is likely that further processing, such as lemmatization, might be useful, producing smaller and better lan-guage models, but this was not evaluated for this paper."
Content selection requires that the system learn a model of the relationship between the appearance of some features in a document and the appear-ance of corresponding features in the summary.
"This can be modeled by estimating the likelihood of some token appearing in a summary given that some tokens (one or more, possibly different to-kens) appeared in the document to be summa-rized."
"The very simplest, “zero-level” model for this relationship is the case when the two tokens in the document and the summary are identical."
This can be computed as the conditional proba-bility of a word occurring in the summary given that the word appeared in the document:   where and represent the bags of words that the headline and the document contain.
"Once the parameters of a content selection model have been estimated from a suitable doc-ument/summary corpus, the model can be used to compute selection scores for candidate summary terms, given the terms occurring in a particular source document."
"Specific subsets of terms, rep-resenting the core summary content of an article, can then be compared for suitability in generating a summary."
"This can be done at two levels (1) likelihood of the length of resulting summaries, given the source document, and (2) likelihood of forming a coherently ordered summary from the content selected."
The length of the summary can also be learned as a function of the source document.
The sim-plest model for document length is a fixed length based on document genre.
"For the discussions in this paper, this will be the model chosen."
Figure 2 shows the distribution of headline length.
"As can be seen, a Gaussian distribution could also model the likely lengths quite accurately."
"Finally, to simplify parameter estimation for the content selection model, we can assume that the likelihood of a word in the summary is inde-pendent of other words in the summary."
"In this case, the probability of any particular summary-content candidate can be calculated simply as the product of the probabilities of the terms in the candidate set."
"Therefore, the overall probability of  a candidate ! ##&quot; &quot;#&quot;  summary $% , , consisting of words, under the simplest, zero-level, summary model based on the previous assump-tions, can be computed as the product of the like-lihood of (i) the terms selected for the summary, (ii) the length of the resulting summary, and (iii) the most likely sequencing of the terms in the con-tent set. *( +)$  #&quot;##&quot; &quot; $&amp; ,+ $ /.10#/+ *()  ##&quot; #&quot; &quot; :"
"In general, the probability of a word appearing in a summary cannot be considered to be inde-pendent of the structure of the summary, but the independence assumption is an initial modeling choice."
The probability of any particular surface ordering as a headline candidate can be computed by mod-eling the probability of word sequences.
"The sim-plest model is a bigram language model, where the probability of a word sequence is approxi-mated by the product of the probabilities of seeing each term given its immediate left context."
Prob-abilities for sequences that have not been seen in the training data are estimated using back-off weights[REF_CITE].
"As mentioned earlier, in principle, surface linearization calculations can be carried out with respect to any textual spans from characters on up, and could take into ac-count additional information at the phrase level."
"They could also, of course, be extended to use higher order n-grams, providing that sufficient numbers of training headlines were available to estimate the probabilities."
"Even though content selection and summary structure generation have been presented sepa-rately, there is no reason for them to occur inde-pendently, and in fact, in our current implementa-tion, they are used simultaneously to contribute to an overall weighting scheme that ranks possible summary candidates against each other."
"Thus, the overall score used in ranking can be obtained as a weighted combination of the content and struc-ture model log probabilities."
"Cross-validation is used to learn weights ; , &lt; and = for a particular document genre. *)+$ ./H /K,  &gt; !"
"CBD@ F &gt;,E ; &lt; /."
H $ //./+ G = *) /.
"To generate a summary, it is necessary to find a sequence of words that maximizes the probability, under the content selection and summary struc-ture models, that it was generated from the doc-ument to be summarized."
"In the simplest, zero-level model that we have discussed, since each summary term is selected independently, and the summary structure model is first order Markov, it is possible to use Viterbi beam search[REF_CITE]to efficiently find a near-optimal summary. [Footnote_2] Other statistical models might require the use of a different heuristic search algorithm."
"2 In the experiments discussed in the following section, a beam width of three, and a minimum beam size of twenty states was used. In other experiments, we also tried to strongly discourage paths that repeated terms, by reweight-ing after backtracking at every state, since, otherwise, bi-grams that start repeating often seem to pathologically over-whelm the search; this reweighting violates the first order Markovian assumptions, but seems to to more good than harm."
An ex-ample of the results of a search for candidates of various lengths is shown in Figure 1.
It shows the set of headlines generated by the system when run against a real news story discussing Apple Com-puter’s decision to start direct internet sales and comparing it to the strategy of other computer makers.
"Zero level–Model: The system was trained on approximately 25,000 news articles from Reuters dated between 1/Jan/1997 and 1/Jun/1997."
"Af-ter punctuation had been stripped, these contained about 44,000 unique tokens in the articles and slightly more than 15,000 tokens in the headlines."
"Representing all the pairwise conditional proba-bilities for all combinations of article and head-line words [Footnote_3] added significant complexity, so we simplified our model further and investigated the effectiveness of training on a more limited vocab-ulary: the set of all the words that appeared in any of the headlines. [Footnote_4] Conditional probabilities for words in the headlines that also appeared in the articles were computed."
"3 This requires a matrix with 660 million entries, or about 2.6GB of memory. This requirement can be significantly re-duced by using a threshold to prune values and using a sparse matrix representation for the remaining pairs. However, in-ertia and the easy availability of the CMU-Cambridge Sta-tistical Modeling Toolkit – which generates the full matrix – have so far conspired to prevent us from exercising that option."
"4 An alternative approach to limiting the size of the map-pings R wordsthat, whereneed to R becouldestimatedhave a wouldsmall valuebe to inusetheonlyhundredsthe top, rather than the thousands, together with the words appear-ing in the headlines. This would limit the size of the model while still allowing more flexible content selection."
"As discussed earlier, in our zero-level model, the system was also trained on bigram transition probabilities as an approx-imation to the headline syntax."
Sample output from the system using this simplified model is shown in Figures 1 and 3.
"Zero Level–Performance Evaluation: The zero-level model, that we have discussed so far, works surprisingly well, given its strong inde-pendence assumptions and very limited vocabu-lary."
"There are problems, some of which are most likely due to lack of sufficient training data. [Footnote_5] Ide-ally, we should want to evaluate the system’s per-formance in terms both of content selection suc-cess and realization quality."
5 We estimate that approximately 100MB of training data would give us reasonable estimates for the models that we would like to evaluate; we had access to much less.
"However, it is hard to computationally evaluate coherence and phras-ing effectiveness, so we have, to date, restricted ourselves to the content aspect, which is more amenable to a quantitative analysis. (We have ex-perience doing much more laborious human eval- uation, and plan to do so with our statistical ap-proach as well, once the model is producing sum-maries that might be competitive with alternative approaches.)"
"After training, the system was evaluated on a separate, previously unseen set of 1000 Reuters news stories, distributed evenly amongst the same topics found in the training set."
"For each of these stories, headlines were generated for a variety of lengths and compared against the (i) the actual headlines, as well as (ii) the sentence ranked as the most important summary sentence."
The lat-ter is interesting because it helps suggest the de-gree to which headlines used a different vocabu-lary from that used in the story itself. [Footnote_6] Term over- lap between the generated headlines and the test standards (both the actual headline and the sum-mary sentence) was the metric of performance.
6 The summarizer we used here to test was an off-the-
"For each news article, the maximum overlap between the actual headline and the generated headline was noted; the length at which this overlap was maximal was also taken into ac-count."
"Also tallied were counts of headlines that matched completely – that is, all of the words in the generated headline were present in the actual headline – as well as their lengths."
These statis-tics illustrate the system’s performance in select-ing content words for the headlines.
"Actual head-lines are often, also, ungrammatical, incomplete phrases."
"It is likely that more sophisticated lan-guage models, such as structure models[REF_CITE], or longer n-gram models would lead to the system generating headlines that were more similar in phrasing to real headlines because longer range dependencies could be taken into account."
Table 1 shows the re-sults of these term selection schemes.
"As can be seen, even with such an impoverished language model, the system does quite well: when the gen-erated headlines are four words long almost one in every five has all of its words matched in the article s actual headline."
"This percentage drops, as is to be expected, as headlines get longer."
"Multiple Selection Models: POS and Position As we mentioned earlier, the zero-level model that we have discussed so far can be extended to take into account additional information both for the content selection and for the surface realiza-tion strategy."
"We will briefly discuss the use of two additional sources of information: (i) part of speech (POS) information, and (ii) positional in-formation."
POS information can be used both in content selection – to learn which word-senses are more likely to be part of a headline – and in surface re-alization.
"Training a POS model for both these tasks requires far less data than training a lexi-cal model, since the number of POS tags is much smaller."
We used a mixture model[REF_CITE]– combining the lexical and the POS probabilities – for both the content se-lection and the linearization tasks.
"Another indicator of salience is positional in-formation, which has often been cited as one of the most important cues for summarization by ex- (c) System generated output using a lexical + POS + posi-tional model."
Figure 4: Output generated by the system using augmented lexical models.
Numbers to the right are log probabilities of the generated strings un-der the generation model. tracti[REF_CITE].
We trained a content selection model based on the position of the tokens in the training set in their respective documents.
"There are several models of positional salience that have been proposed for sentence selection; we used the simplest possible one: estimating the probability of a token appear-ing in the headline given that it appeared in the 1st, 2nd, 3rd or 4th quartile of the body of the ar-ticle."
"We then tested mixtures of the lexical and POS models, lexical and positional models, and all three models combined together."
"Sample out-put for the article in Figure 3, using both lexi-cal and POS/positional information can be seen in Figure 4."
"As can be seen in Table 2, [Footnote_7] Al-though adding the POS information alone does not seem to provide any benefit, positional infor-mation does."
"7 Unlike the data in Table 1, these headlines contain only six words or fewer."
"When used in combination, each of the additional information sources seems to im-prove the overall model of summary generation."
"Problems with evaluation: Some of the statis-tics that we presented in the previous discus-sion suggest that this relatively simple statisti-cal summarization system is not very good com-pared to some of the extraction based summa-rization systems that have been presented else-where (e.g.,[REF_CITE])."
"However, it is worth emphasizing that many of the head-lines generated by the system were quite good, but were penalized because our evaluation met-ric was based on the word-error rate and the gen-erated headline terms did not exactly match the original ones."
"A quick manual scan of some of the failures that might have been scored as successes in a subjective manual evaluation indicated that some of these errors could not have been avoided without adding knowledge to the system, for ex-ample, allowing the use of alternate terms for re-ferring to collective nouns."
Some of these errors are shown in Table 3.
This paper has presented an alternative to ex-tractive summarization: an approach that makes it possible to generate coherent summaries that are shorter than a single sentence and that at-tempt to conform to a particular style.
"Our ap-proach applies statistical models of the term se-lection and term ordering processes to produce short summaries, shorter than those reported pre-viously."
"Furthermore, with a slight generaliza-tion of the system described here, the summaries need not contain any of the words in the original document, unlike previous statistical summariza-tion systems."
"Given good training corpora, this approach can also be used to generate headlines from a variety of formats: in one case, we experi-mented with corpora that contained Japanese doc-uments and English headlines."
This resulted in a working system that could simultaneously trans-late and summarize Japanese documents. [Footnote_8]
"8 Since our initial corpus was constructed by running a simple lexical translation system over Japanese headlines, the results were poor, but we have high hopes that usable summaries may be produced by training over larger corpora."
The performance of the system could be im-proved by improving either content selection or linearization.
"This can be through the use of more sophisticated models, such as additional language models that take into account the signed distance between words in the original story to condition the probability that they should appear separated by some distance in the headline."
"Recently, we have extended the model to gen-erate multi-sentential summaries as well: for in-stance, given an initial sentence such as “Clinton to meet visit MidEast.” and words that are related to nouns (“Clinton” and “mideast”) in the first sentence, the system biases the content selection model to select other nouns that have high mu-tual information with these nouns."
"In the exam-ple sentence, this generated the subsequent sen-tence “US urges Israel plan.”"
"This model cur-rently has several problems that we are attempt-ing to address: for instance, the fact that the words co-occur in adjacent sentences in the train-ing set is not sufficient to build coherent adjacent sentences (problems with pronominal references, cue phrases, sequence, etc. abound)."
"Further-more, our initial experiments have suffered from a lack of good training and testing corpora; few of the news stories we have in our corpora con-tain multi-sentential headlines."
"While the results so far can only be seen as in-dicative, this breed of non-extractive summariza-tion holds a great deal of promise, both because of its potential to integrate many types of informa-tion about source documents and intended sum-maries, and because of its potential to produce very brief coherent summaries."
We expect to im-prove both the quality and scope of the summaries produced in future work.
»2ÍAÎ&amp;¼Ï½¿¾ÁÐ +%Ã¾ÁÉÂFÊ..ÂQÑy× ½ØÊÚÙÊ+&lt;ÍAÛ?ÜÝÏ2ÜÞÐÜ *Ê )
Ó ^ÉfÂaÊ.  %.ÓaÄÒ×
Ï ÏSë ÆÇÓaÊ.+ ¼fÃßØÅì½ØÉîíïaïFï ÜçÏ ÂaÆ¼ &amp;Î Ï  .+½¿ÉfãÓaàSÂÁÊÓaÄÒÃ&amp;Ê...
Ã7¾ÇñyÓFÄ*Ã)ÃåÃÑy× ++.Ê ÃÇÄ^ËâÌMÀ6Ã)Óaà &amp;Î Ï  Ë+ Û?ÜÝÏ2Ü Ê.ÓyÅfÃß ;Ü ò Ã4Ë++ÂaÆË Ë+ Î Ï  +&lt; Ü ô _PBCV&gt;
This paper reports the first part of a project that aims to develop a knowledge extrac-tion and knowledge discovery system that extracts causal knowledge from textual da-tabases.
"In this initial study, we develop a method to identify and extract cause-effect information that is explicitly expressed in medical abstracts in the Medline database."
"A set of graphical patterns were constructed that indicate the presence of a causal rela-tion in sentences, and which part of the sentence represents the cause and which part represents the effect."
"The patterns are matched with the syntactic parse trees of sentences, and the parts of the parse tree that match with the slots in the patterns are extracted as the cause or the effect."
Vast amounts of textual documents and data-bases are now accessible on the Internet and the World Wide Web.
"However, it is very difficult to retrieve useful information from this huge disorganized storehouse."
"Programs that can identify and extract useful information, and re-late and integrate information from multiple sources are increasingly needed."
The World Wide Web presents tremendous opportunities for developing knowledge extraction and knowl-edge discovery programs that automatically ex-tract and acquire knowledge about a domain by integrating information from multiple sources.
New knowledge can be discovered by relating disparate pieces of information and by infer-encing from the extracted knowledge.
This paper reports the first phase of a project to develop a knowledge extraction and knowl- edge discovery system that focuses on causal knowledge.
A system is being developed to identify and extract cause-effect information from the Medline database – a database of ab-stracts of medical journal articles and conference papers.
"In this initial study, we focus on cause-effect information that is explicitly expressed (i.e. indicated using some linguistic marker) in sentences."
"We have selected four medical areas for this study – heart disease, AIDS, depression and schizophrenia."
The medical domain was selected for two reasons: 1.
"The causal relation is particular important in medicine, which is concerned with devel-oping treatments and drugs that can effect a cure for some disease 2."
"Because of the importance of the causal re-lation in medicine, the relation is more likely to be explicitly indicated using linguistic means (i.e. using words such as result, ef-fect, cause, etc.)."
"The goal of information extraction research is to develop systems that can identify the passage(s) in a document that contains information that is relevant to a prescribed task, extract the infor-mation and relate the pieces of information by filling a structured template or a database record ([REF_CITE]; Cowie &amp;[REF_CITE]; Gai-zauskas &amp;[REF_CITE])."
"Information extraction research has been influenced tremendously by the series of Mes-sage Understanding Conferences (MUC-5, MUC-6, MUC-7), organized by the U.S."
Ad-vanced Research Projects Agency (ARPA)[URL_CITE]ngs_index.html).
"Participants of the conferences develop systems to perform common informa-tion extraction tasks, defined by the conference organizers."
"For each task, a template is specified that indicates the slots to be filled in and the type of information to be extracted to fill each slot."
"The set of slots defines the various entities, aspects and roles relevant to a prescribed task or topic of interest."
"Information that has been extracted can be used for populating a database of facts about entities or events, for automatic summarization, for information mining, and for acquiring knowledge to use in a knowledge-based system."
Information extraction systems have been devel-oped for a wide range of tasks.
"However, few of them have focused on extracting cause-effect information from texts."
Previous studies that have attempted to ex-tract cause-effect information from text have mostly used knowledge-based inferences to infer the causal relations.
"Selfridge, Daniell &amp;[REF_CITE]and Joskowsicz, Ksiezyk &amp;[REF_CITE]developed prototype computer programs that extracted causal knowledge from short explanatory messages entered into the knowledge acquisition component of an expert system."
"When there was an ambiguity whether a causal relation was expressed in the text, the systems used a domain model to check whether such a causal relation between the events was possible."
"Kontos &amp;[REF_CITE]and Kaplan &amp;[REF_CITE]used linguistic patterns to identify causal relations in scientific texts, but the grammar, lexicon, and patterns for identify-ing causal relations were hand-coded and devel-oped just to handle the sample texts used in the studies."
Knowledge-based inferences were also used.
The authors pointed out that substantial domain knowledge was needed for the system to identify causal relations in the sample texts ac-curately.
"More recently,[REF_CITE]developed a computer program to extract cause-effect infor-mation from French technical texts without us-ing domain knowledge."
He focused on causative verbs and reported a precision rate of 85%.
"Khoo, Kornfilt, Oddy &amp;[REF_CITE]devel-oped an automatic method for extracting cause-effect information from Wall Street Journal texts using linguistic clues and pattern matching."
Their system was able to extract about 68% of the causal relations with an error rate of about 36%.
The emphasis of the current study is on ex-tracting cause-effect information that is explic-itly expressed in the text without knowledge-based inferencing.
It is hoped that this will result in a method that is more easily portable to other subject areas and document collections.
We also make use of a parser (Conexor’s FDG parser) to construct syntactic parse trees for the sentences.
Graphical extraction patterns are constructed to extract information from the parse trees.
"As a result, a much smaller number of patterns need be constructed."
200 abstracts were downloaded from the Med-line database for use as our training sample of texts.
"They are from four medical areas: depres-sion, schizophrenia, heart disease and AIDs (fifty abstracts from each area)."
The texts were analysed to identify: 1. the different roles and attributes that are in-volved in a causal situation.
"Cause and effect are, of course, the main roles, but other roles also exist including enabling conditions, size of the effect, and size of the cause (e.g. dos-age). 2. the various linguistic markers used by the writers to explicitly signal the presence of a causal relation, e.g. as a result, affect, re-duce, etc."
The various roles and attributes of causal situa-tions identified in the medical abstracts are structured in the form of a template.
"There are three levels in our cause-effect template, Level 1 giving the high-level roles and Level 3 giving the most specific sub-roles."
The first two levels are given in Table 1.
"A more detailed description is provided in Khoo, Chan &amp;[REF_CITE]."
"The information extraction system devel-oped in this initial study attempts to fill only the main slots of cause, effect and modality, without attempting to divide the main slots into subslots."
Causal relations are expressed in text in various ways.
Two common ways are by using causal links and causative verbs.
"Causal links are words used to link clauses or phrases, indicating a causal relation between them."
"He classified them into four main types: the adverbial link (e.g. hence, therefore), the prepositional link (e.g. because of, on account of), subordination (e.g. because, as, since, for, so) and the clause-integrated line (e.g. that’s why, the result was)."
Causative verbs are transi-tive action verbs that express a causal relation between the subject and object or prepositional phrase of the verb.
"For example, the transitive verb break can be paraphrased as to cause to break, and the transitive verb kill can be para-phrased as to cause to die."
We analyzed the 200 training abstracts to identify the linguistic markers (such as causal links and causative verbs) used to indicate causal relations explicitly.
The most common linguistic expressions of cause-effect found in the Depres-sion and Schizophrenia abstracts (occurring at least 10 times in 100 abstracts) are listed in Ta-ble 2.
The common expressions found in the AIDs and Heart Disease abstracts (with at least 10 occurrences) are listed in Table 3.
The ex-pressions listed in the two tables cover about 70% of the explicit causal expressions found in the sample abstracts.
"Six expressions appear in both tables, indicating a substantial overlap in the two groups of medical areas."
The most fre-quent way of expressing cause and effect is by using causative verbs.
The information extraction process used in this study makes use of pattern matching.
This is similar to methods employed by other research-ers for information extraction.
"Whereas most studies focus on particular types of events or topics, we are focusing on a particular type of relation."
"Furthermore, the patterns used in this study are graphical patterns that are matched with syntactic parse trees of sentences."
The pat-terns represent different words and sentence structures that indicate the presence of a causal relation and which parts of the sentence repre- sent which roles in the causal situation.
"Any part of the sentence that matches a particular pattern is considered to describe a causal situation, and the words in the sentence that match slots in the pattern are extracted and used to fill the appro-priate slots in the cause-effect template."
The sentences are parsed using Conexor’s Func-tional Dependency Grammar of English (FDG) parser[URL_CITE]which generates a representation of the syntactic structure of the sentence (i.e. the parse tree).
For the example sentence
Paclitaxel was well tolerated and resulted in a significant clinical response in this patient. a graphical representation of the parser output is given in Fig. 1.
"For easier processing, the syn-tactic structure is converted to the linear con-ceptual graph formalism[REF_CITE]given in Fig. 2."
A conceptual graph is a graph with the nodes representing concepts and the directed arcs representing relations between concepts.
"Although the conceptual graph formalism was developed primarily for semantic representation, we use it to represent the syntactic structure of sentences."
"In the linear conceptual graph nota-tion, concept labels are given within square brackets and relations between concepts are attr det det clinical a this attr significant"
Syntactic structure of a sentence given within parentheses.
Arrows indicate the direction of the relations.
We developed a set of graphical patterns that specifies the various ways a causal relation can be explicitly expressed in a sentence.
We call them causality patterns.
The initial set of pat-terns was constructed based on the training set of 200 abstracts mentioned earlier.
"Each abstract was analysed by two of the authors to identify the sentences containing causal relations, and the parts of the sentences representing the cause and the effect."
"For each sentence containing a causal relation, the words (causality identifiers) that were used to signal the causal relation were also identified."
These are mostly causal links and causative verbs described earlier.
Effect: a significant clinical response in this patient
Sentence structure and causality pattern in conceptual graph format
"We constructed the causality patterns for each causality identifier, to express the different sentence constructions that the causality identi-fier can be involved in, and to indicate which parts of the sentence represent the cause and the effect."
"For each causality identifier, at least 20 sentences containing the identifier were ana-lysed."
"If the training sample abstracts did not have 20 sentences containing the identifier, ad-ditional sentences were downloaded from the Medline database."
"After the patterns were con-structed, they were applied to a new set of 20 sentences from Medline containing the identi-fier."
Measures of precision and recall were cal-culated.
Each set of patterns are thus associated with a precision and a recall figure as a rough indication of how good the set of patterns is.
The causality patterns are represented in lin-ear conceptual graph format with some exten-sions.
The symbols used in the patterns are as follows: 1.
"Concept nodes take the following form: [concept_label] or [concept_label: role_indicator]. Concept_label can be: • a character string in lower case, represent-ing a stemmed word • a character string in uppercase, refering to a class of synonymous words that can occupy that place in a sentence • “*”, a wildcard character that can match any word • “T”, a wildcard character that can match with any sub-tree."
"Role_indicator refers to a slot in the cause-effect template, and can take the form: • role_label which is the name of a slot in the cause-effect template • role_label = “value”, where value is a character string that should be entered in the slot in the cause-effect template (if “value” is not specified, the part of the sentence that matches the concept_label is entered in the slot). 2. Relation nodes take the following form: (set_of_relations)."
"Set_of_relations can be: • a relation_label, which is a character string representing a syntactic relation (these are the relation tags used by Conexor’s FDG parser) • relation_label | set of relations (“|” indi-cates a logical “or”) 3. &amp;subpattern_label refers to a set of sub-graphs."
Each node can also be followed by a “+” indicating that the node is mandatory.
"If the mandatory nodes are not found in the sentence, then the pattern is rejected and no information is extracted from the sentence."
All other nodes are optional.
An example of a causality pattern is given in Fig. 2.
The information extraction process involves matching the causality patterns with the parse trees of the sentences.
The parse trees and the causality patterns are both represented in the linear conceptual graph notation.
"The pattern matching for each sentence follows the follow-ing procedure: 1. the causality identifiers that match with keywords in the sentence are identified, 2. the causality patterns associated with each matching causality identifier are shortlisted, 3. for each shortlisted pattern, a matching pro-cess is carried out on the sentence."
"The matching process involves a kind of spreading activation in both the causality pattern graph and the sentence graph, starting from the node representing the causality identifier."
"If a pattern node matches a sentence node, the matching node in the pattern and the sentence are activated."
"This activation spreads outwards, with the causality identifier node as the center."
"When a pattern node does not match a sentence node, then the spreading activation stops for that branch of the pattern graph."
Procedures are at-tached to the nodes to check whether there is a match and to extract words to fill in the slots in the cause-effect template.
The pattern matching program has been implemented in Java (JDK 1.2.1).
"An example of a sentence, matching pat-tern and filled template is given in Fig. 2."
A total of 68 patterns were constructed for the 35 causality identifiers that occurred at least twice in the training abstracts.
"The patterns were applied to two sets of new abstracts downloaded from Medline: 100 new abstracts from the origi-nal four medical areas (25 abstracts from each area), and 30 abstracts from two new domains (15 each) – digestive system diseases and respi- ratory tract diseases."
Each test abstract was analyzed by at least 2 of the authors to identify “medically relevant” cause and effect.
"A fair number of causal relations in the abstracts are trivial and not medically relevant, and it was felt that it would not be useful for the information extraction system to extract these trivial causal relations."
"Of the causal relations manually identified in the abstracts, about 7% are implicit (i.e. have to be inferred using knowledge-based inferenc-ing) or occur across sentences."
"Since the focus of the study is on explicitly expressed cause and effect within a sentence, only these are included in the evaluation."
The evaluation results are pre-sented in Table 4.
Recall is the percentage of the slots filled by the human analysts that are cor-rectly filled by the computer program.
Precision is the percentage of slots filled by the computer program that are correct (i.e. the text entered in the slot is the same as that entered by the human analysts).
"If the text entered by the computer program is partially correct, it is scored as 0.5 (i.e. half correct)."
"The F-measure given in Table 4 is a combination of recall and precision equally weighted, and is calculated using the formula (MUC-7): 2*precision*recall / (precision + recall)"
"For the 4 medical areas used for building the extraction patterns, the F-measure for the cause and effect slots are 0.508 and 0.578 respectively."
"If implicit causal relations are included in the evaluation, the recall measures for cause and effect are 0.405 and 0.481 respectively, yielding an F-measure of 0.47 for cause and 0.54 for ef-fect."
"The results are not very good, but not very bad either for an information extraction task."
"For the 2 new medical areas, we can see in Table 4 that the precision is about the same as for the original 4 medical areas, indicating that the current extraction patterns work equally well in the new areas."
The lower recall indicates that new causality identifiers and extraction patterns need to be constructed.
The sources of errors were analyzed for the set of 100 test abstracts and are summarized in Table 5.
Most of the spurious extractions (in-formation extracted by the program as cause or effect but not identified by human analysts) were actually causal relations that were not medically relevant.
"As mentioned earlier, the manual iden-tification of causal relations focused on medi-cally relevant causal relations."
"In the cases where the program did not correctly extract cause and effect information identified by the analysts, half were due to incorrect parser out-put, and in 20% of the cases, causality patterns have not been constructed for the causality iden-tifier found in the sentence."
"We also analyzed the instances of implicit causal relations in sentences, and found that many of them can be identified using some amount of semantic analysis."
"Some of them in-volve words like when, after and with that indi-cate a time sequence, for example: • The results indicate that changes to 8-OH-DPAT and clonidine-induced responses oc-cur quicker with the combination treatment than with either reboxetine or sertraline treatments alone. • There are also no reports of serious adverse events when lithium is added to a monoam-ine oxidase inhibitor. • Four days after flupenthixol administration, the patient developed orolingual dyskinetic movements involving mainly tongue biting and protrusion."
A. Spurious errors (the program identified cause or effect not identified by the hu-man judges)
The relations extracted are not relevant to medi-cine or disease. (84.1%)
Nominalized or adjectivized verbs are identified as causative verbs by the program because of parser error. (2.9%)
Some words and sentence constructions that are used to indicate cause-effect can be used to indi-cate other kinds of relations as well. (13.0%)
"B. Missing slots (cause or effect not ex-tracted by program), incorrect text ex-tracted, and partially correct extraction"
Complex sentence structures that are not in-cluded in the pattern. (18.8%)
The parser gave the wrong syntactic structure of a sentence. (49.2%)
Unexpected sentence structure resulting in the program extracting information that is actually not a cause or effect. (1.5%)
Patterns for the causality identifier have not been constructed. (19.6%)
The program extracts the relevant sub-tree (of the parse tree) to fill in the cause or effect slot.
"However, because of the sentence construction, the sub-tree includes both the cause and effect resulting in too much text being ex-tracted. (9.5%)"
Errors caused by pronouns that refer to a phrase or clause within the same sentence. (1.3%)
"In these cases, a treatment or drug is associated with a treatment response or physiological event."
"If noun phrases and clauses in sentences can be classified accurately into treatments and treat-ment responses (perhaps by using Medline’s Medical Subject Headings), then such implicit causal relations can be identified automatically."
"Another group of words involved in implicit causal relations are words like receive, get and take, that indicate that the patient received a drug or treatment, for example: • The nine subjects who received p24-VLP and zidovudine had an augmentation and/or broadening of their CTL response compared with baseline (p = 0.004)."
Such causal relations can also be identified by semantic analysis and classifying noun phrases and clauses into treatments and treatment re-sponses.
We have described a method for performing automatic extraction of cause-effect information from textual documents.
We use Conexor’s FDG parser to construct a syntactic parse tree for each target sentence.
The parse tree is matched with a set of graphical causality patterns that indicate the presence of a causal relation.
"When a match is found, various attributes of the causal relation (e.g. the cause, the effect, and the modality) can then be extracted and entered in a cause-effect template."
"The accuracy of our extraction system is not yet satisfactory, with an accuracy of about 0.51 (F-measure) for extracting the cause and 0.58 for extracting the effect that are explicitly ex-pressed."
"If both implicit and explicit causal rela-tions are included, the accuracy is 0.41 for cause and 0.48 for effect."
"We were heartened to find that when the extraction patterns were applied to 2 new medical areas, the extraction precision was the same as for the original 4 medical areas."
Future work includes: 1.
Constructing patterns to identify causal re-lations across sentences 2. Expanding the study to more medical areas 3.
"Incorporating semantic analysis to extract implicit cause-effect information 4. Incorporating discourse processing, includ-ing anaphor and co-reference resolution 5. Developing a method for constructing ex-traction patterns automatically 6. Investigating whether the cause-effect in-formation extracted can be chained together to synthesize new knowledge."
Two aspects of discourse processing is being studied: co-reference resolution and hypothesis confirmation.
Co-reference resolution is impor-tant for two reasons.
"The first is the obvious rea-son that to extract complete cause-effect infor-mation, pronouns and references have to be resolved and replaced with the information that they refer to."
"The second reason is that quite of-ten a causal relation between two events is ex-pressed more than once in a medical abstract, each time providing new information about the causal situation."
"The extraction system thus needs to be able to recognize that the different causal expressions refer to the same causal situation, and merge the information extracted from the different sentences."
The second aspect of discourse processing being investigated is what we refer to as hy-pothesis confirmation.
"Sometimes, a causal rela-tion is hypothesized by the author at the begin-ning of the abstract."
This hypothesis may be confirmed or disconfirmed by another sentence later in the abstract.
The information extraction system thus has to be able to link the initial hy-pothetical cause-effect expression with the con-firmation or disconfirmation expression later in the abstract.
"Finally, we hope eventually to develop a system that not only extracts cause-effect infor-mation from medical abstracts accurately, but also synthesizes new knowledge by chaining the extracted causal relations."
"In a series of studies,[REF_CITE]has demonstrated that logical connections between the published literature of two medical research areas can provide new and useful hypotheses."
"Suppose an article reports that A causes B, and another article reports that B causes C, then there is an implicit logical link between A and C (i.e. A causes C)."
This relation would not become explicit unless work is done to extract it.
"Thus, new discoveries can be made by analysing published literature automatically ([REF_CITE]; Swanson &amp;[REF_CITE])."
"In terms of both speed and mem-ory consumption, graph unification remains the most expensive com-ponent of unification-based gram-mar parsing."
"We present a tech-nique to reduce the memory usage of unification algorithms consider-ably, without increasing execution times."
"Also, the proposed algorithm is thread-safe, providing an efficient algorithm for parallel processing as well."
"Both in terms of speed and memory consump-tion, graph unification remains the most ex-pensive component in unification-based gram-mar parsing."
Unification is a well known algo-rithm.
"Prolog, for example, makes extensive use of term unification."
Graph unification is slightly different.
"Two different graph nota-tions and an example unification are shown in Figure 1 and 2, respectively."
"In typical unification-based grammar parsers, roughly 90% of the unifications fail."
"Any processing to create, or copy, the result graph before the point of failure is redundant."
"As copying is the most expensive part of unification, a great deal of research has gone in eliminating superfluous copying."
Examples of these approaches are given[REF_CITE]and[REF_CITE].
"In order to avoid superfluous copying, these algorithms incorporate control data in the graphs."
"This has several drawbacks, as we will discuss next."
"Memory Consumption To achieve the goal of eliminating superfluous copying, the aforementioned algorithms include adminis-trative fields—which we will call scratch fields—in the node structure."
"These fields do not attribute to the definition of the graph, but are used to efficiently guide the unifica-tion and copying process."
"Before a graph is used in unification, or after a result graph has been copied, these fields just take up space."
"This is undesirable, because memory usage is of great concern in many unification-based grammar parsers."
"This problem is especially of concern in Tomabechi’s algorithm, as it in-creases the node size by at least 60% for typ-ical implementations."
"In the ideal case, scratch fields would be stored in a separate buffer allowing them to be reused for each unification."
The size of such a buffer would be proportional to the maximum number of nodes that are involved in a single unification.
"Although this technique reduces memory usage considerably, it does not re-duce the amount of data involved in a single unification."
"Nevertheless, storing and loading nodes without scratch fields will be faster, be-cause they are smaller."
"Because scratch fields are reused, there is a high probability that they will remain in cache."
"As the difference in speed between processor and memory con-tinues to grow, caching is an important con-siderati[REF_CITE]. 1"
A straightforward approach to separate the scratch fields from the nodes would be to use a hash table to associate scratch structures with the addresses of nodes.
"The overhead of a hash table, however, may be significant."
"In general, any binding mechanism is bound to require some extra work."
"Nevertheless, considering the difference in speed between processors and memory, reducing the mem-ory footprint may compensate for the loss of performance to some extent."
Symmetric Multi Processing Small-scale desktop multiprocessor systems (e.g. dual or even quad Pentium machines) are be-coming more commonplace and affordable.
"If we focus on graph unification, there are two ways to exploit their capabilities."
"First, it is possible to parallelize a single graph unifica-tion, as proposed by e.g.[REF_CITE]."
"Suppose we are unifying graph a with graph b, then we could allow multiple processors to work on the unification of a and b simulta-neously."
We will call this parallel unifica-tion.
Another approach is to allow multiple graph unifications to run concurrently.
Sup-pose we are unifying graph a and b in addi-tion to unifying graph a and c. By assigning a different processor to each operation we ob-tain what we will call concurrent unifica-tion.
"Parallel unification exploits parallelism inherent of graph unification itself, whereas concurrent unification exploits parallelism at the context-free grammar backbone."
"As long as the number of unification operations in one parse is large, we believe it is preferable to choose concurrent unification."
"Especially when a large number of unifications termi-nates quickly (e.g. due to failure), the over-head of more finely grained parallelism can be considerable."
"In the example of concurrent unification, graph a was used in both unifications."
"This suggests that in order for concurrent unifica-tion to work, the input graphs need to be read only."
"With destructive unification al-gorithms this does not pose a problem, as the source graphs are copied before unifica-tion."
"However, including scratch fields in the node structure (as Tomabechi’s and Wrob-lewski’s algorithms do) thwarts the imple-mentation of concurrent unification, as differ-ent processors will need to write different val-ues in these fields."
One way to solve this prob-lem is to disallow a single graph to be used in multiple unification operations simultane-ously.
"In (van[REF_CITE]) it is shown, however, that this will greatly impair the abil-ity to achieve speedup."
Another solution is to duplicate the scratch fields in the nodes for each processor.
"This, however, will enlarge the node size even further."
"In other words, Tomabechi’s and Wroblewski’s algorithms are not suited for concurrent unification."
The key to the solution of all of the above-mentioned issues is to separate the scratch fields from the fields that actually make up the definition of the graph.
The result-ing data structures are shown in Figure 3.
"We have taken Tomabechi’s quasi-destructive graph unification algorithm as the starting point[REF_CITE], because it is often considered to be the fastest unification algo- rithm for unification-based grammar parsing (see e.g. (op den[REF_CITE]))."
We have separated the scratch fields needed for unification from the scratch fields needed for copying. [Footnote_2]
"2 The arc-list field could be used for permanent for-ward links, if required."
We propose the following technique to asso-ciate scratch structures with nodes.
We take an array of scratch structures.
"In addition, for each graph we assign each node a unique index number that corresponds to an element in the array."
Different graphs typically share the same indexes.
"Since unification involves two graphs, we need to ensure that two nodes will not be assigned the same scratch struc-ture."
We solve this by interleaving the index positions of the two graphs.
This mapping is shown in Figure 4.
"Obviously, the minimum number of elements in the table is two times the number of nodes of the largest graph."
"To reduce the table size, we allow certain nodes to be deprived of scratch structures. (For ex-ample, we do not forward atoms.)"
"We denote this with a valuation function v, which re-turns 1 if the node is assigned an index and 0 otherwise."
We can associate the index with a node by including it in the node structure.
"For struc-ture sharing, however, we have to use offsets between nodes (see Figure 4), because other-wise different nodes in a graph may end up having the same index (see Section 3)."
Off- sets can be easily derived from index values in nodes.
"As storing offsets in arcs consumes more memory than storing indexes in nodes (more arcs may point to the same node), we store index values and use them to compute the offsets."
"For ease of reading, we present our algorithm as if the offsets were stored instead of computed."
Note that the small index val-ues consume much less space than the scratch fields they replace.
The resulting algorithm is shown in Fig-ure 5.
"It is very similar to the algorithm[REF_CITE], but incorporates our in-dexing technique."
"Each reference to a node now not only consists of the address of the node structure, but also its index in the ta-ble."
This is required because we cannot derive its table index from its node structure alone.
The second argument of Copy indicates the next free index number.
"Copy returns references with an offset, allowing them to be directly stored in arcs."
"These offsets will be negative when Copy exits at line 2.2, resembling a reentrancy."
Note that only AbsArc explicitly defines operations on off-sets.
AbsArc computes a node’s index using its parent node’s index and an offset. d Note that we are multiplying the offset by 2 to account for the interleaved offsets of the left and right graph. e
"We assume it is known at this point whether the new node requires an index number. f Note that ref contains an index, whereas ref1 contains an offset. g If the node was already copied (in which case it is &lt; 0), we need not reserve indexes."
Figure 5: The memory-efficient and thread-safe unification algorithm.
"Note that the arrays fwtab and cptab—which represent the forward table and copy table, respectively—are defined as global variables."
"In order to be thread safe, each thread needs to have its own copy of these tables."
"Contrary to Tomabechi’s implementation, we invalidate scratch fields by simply reset-ting them after a unification completes."
This simplifies the algorithm.
We only reset the table up to the highest index in use.
"As table entries are roughly filled in increasing order, there is little overhead for clearing unused el-ements."
"A nice property of the algorithm is that indexes identify from which input graph a node originates (even=left, odd=right)."
"This information can be used, for example, to selectively share nodes in a structure shar-ing scheme."
We can also specify additional scratch fields or additional arrays at hardly any cost.
Some of these abilities will be used in the enhancements of the algorithm we will discuss next.
Structure Sharing Structure sharing is an important technique to reduce memory us-age.
We will adopt the same terminology as Tomabechi[REF_CITE].
"That is, we will use the term feature-structure sharing when two arcs in one graph converge to the same node in that graph (also refered to as reentrancy) and data-structure sharing when arcs from two different graphs converge to the same node."
The conditions for sharing mentioned[REF_CITE]are: (1) bottom and atomic nodes can be shared; (2) complex nodes can be shared unless they are modified.
We need to add the following condition: ([Footnote_3]) all arcs in the shared subgraph must have the same offsets as the subgraph that would have resulted from copying.
"3 This can easily be accomplished by fixing the or-der in which arcs are stored in memory. This is a good idea anyway, as it can speedup the ComplementArcs and IntersectArcs operations."
A possible violation of this constraint is shown in Figure 6.
"As long as arcs are processed in increasing order of index number, 3 this condition can only be violated in case of reentrancy."
"Basically, the condition can be violated when a reentrancy points past a node that is bound to a larger subgraph."
"Contrary to many other structure sharing schemes ([REF_CITE]), our algo-rithm allows sharing of nodes that are part of the grammar."
"As nodes from the different in-put graphs are never assigned the same table entry, they are always bound independently of each other. (See the footnote for line 3 of Unify1.)"
The sharing version of Copy is similar to the variant[REF_CITE].
The extra check can be implemented straightforwardly by comparing the old offset with the offset for the new nodes.
"Because we derive the offsets from index values associated with nodes, we need to compensate for a difference between the index of the shared node and the index it should have in the new graph."
We store this information in a specialized share arc.
We need to adjust Unify1 to handle share arcs accordingly.
"Deferred Copying Just as we use a table for unification and copying, we also use a ta-ble for subsumption checking."
Tomabechi’s algorithm requires that the graph resulting from unification be copied before it can be used for further processing.
This can result in superfluous copying when the graph is sub-sumed by an existing graph.
Our technique allows subsumption to use the bindings gener-ated by Unify1 in addition to its own table.
This allows us to defer copying until we com-pleted subsumption checking.
"Packed Nodes With a straightforward im-plementation of our algorithm, we obtain a node size of 8 bytes. [Footnote_4] By dropping the con-cept of a fixed node size, we can reduce the size of atom and bottom nodes to 4 bytes."
4 We do not have a type hierarchy.
Type information can be stored in two bits.
We use the two least significant bits of point-ers (which otherwise are 0) to store this type information.
"Instead of using a pointer for the value field, we store nodes in place."
Only for reentrancies we still need pointers.
"Com-plex nodes require 8 bytes, as they include a pointer to the first node past its children (necessary for unification)."
"This scheme re-quires some extra logic to decode nodes, but significantly reduces memory consumption."
We have tested our algorithm with a medium-sized grammar for Dutch.
The system was implemented in Objective-C using a fixed ar-ity graph representation.
We used a test set of 22 sentences of varying length.
"Usually, ap-proximately 90% of the unifications fails."
"On average, graphs consist of 60 nodes."
"The ex-periments were run on a[REF_CITE]EB (256 KB L2 cache) box, with 128 MB mem-ory, running Linux."
We tested both memory usage and execu-tion time for various configurations.
The re-sults are shown in Figure 7 and 8.
It includes a version of Tomabechi’s algorithm.
The node size for this implementation is 20 bytes.
"For the proposed algorithm we have included several versions: a basic implementation, a packed version, a version with deferred copy-ing, and a version with structure sharing."
"The basic implementation has a node size of 8 bytes, the others have a variable node size."
"Whenever applicable, we applied the same op-timizations to all algorithms."
We also tested the speedup on a dual[REF_CITE]Mhz. [Footnote_5] Each processor was assigned its own scratch tables.
5 These results are scaled to reflect the speedup rel-ative to the tests run on the other machine.
"Apart from that, no changes to the algorithm were required."
"For more details on the multi-processor implementation, see (van[REF_CITE])."
The memory utilization results show signif-icant improvements for our approach. [Footnote_6] Pack-ing decreased memory utilization by almost 40%.
"6 The results do not include the space consumed by the scratch tables. However, these tables do not consume more than 10 KB in total, and hence have no significant impact on the results."
Structure sharing roughly halved this once more. 7 The third condition prohibited sharing in less than 2% of the cases where it would be possible in Tomabechi’s approach.
Figure 7 shows that our algorithm does not increase execution times.
Our algorithm even scrapes off roughly [Footnote_7]% of the total parsing time.
"7 Because the packed version has a variable node size, structure sharing yielded less relative improve-ments than when applied to the basic version. In terms of number of nodes, though, the two results were identical."
This speedup can be attributed to im-proved cache utilization.
We verified this by running the same tests with cache disabled.
This made our algorithm actually run slower than Tomabechi’s algorithm.
Deferred copy-ing did not improve performance.
The addi-tional overhead of dereferencing during sub-sumption was not compensated by the savings on copying.
Structure sharing did not sig-nificantly alter the performance as well.
"Al-though, this version uses less memory, it has to perform additional work."
"Running the same tests on machines with less memory showed a clear performance ad-vantage for the algorithms using less memory, because paging could be avoided."
We reduce memory consumption of graph uni-fication as presented[REF_CITE](or[REF_CITE]) by separating scratch fields from node structures.
Pereira’s[REF_CITE]algorithm also stores changes to nodes separate from the graph.
"However, Pereira’s mechanism incurs a log(n) overhead for accessing the changes (where n is the number of nodes in a graph), resulting in an O(n log n) time algorithm."
Our algorithm runs in O(n) time.
"With respect to over and early copying (as defined[REF_CITE]), our algorithm has the same characteristics as Tomabechi’s algorithm."
"In addition, our algorithm allows to postpone the copying of graphs until after subsumption checks complete."
This would re-quire additional fields in the node structure for Tomabechi’s algorithm.
"Our algorithm allows sharing of grammar nodes, which is usually impossible in other implementations[REF_CITE]."
A weak point of our structure sharing scheme is its extra condition.
"However, our experi-ments showed that this condition can have a minor impact on the amount of sharing."
We showed that compressing node struc-tures allowed us to reduce memory consump-tion by another 40% without sacrificing per-formance.
"Applying the same technique to Tomabechi’s algorithm would yield smaller relative improvements (max. 20%), because the scratch fields cannot be compressed to the same extent."
One of the design goals of Tomabechi’s al-gorithm was to come to an efficient imple-mentation of parallel unificati[REF_CITE].
"Although theoretically parallel uni-fication is hard[REF_CITE], Tomabechi’s algorithm provides an elegant solution to achieve limited scale parallelism[REF_CITE]."
"Since our algorithm is based on the same principles, it allows paral-lel unification as well."
"Tomabechi’s algorithm, however, is not thread-safe, and hence cannot be used for concurrent unification."
We have presented a technique to reduce memory usage by separating scratch fields from nodes.
We showed that compressing node structures can further reduce the mem-ory footprint.
"Although these techniques re-quire extra computation, the algorithms still run faster."
The main reason for this was the difference between cache and memory speed.
"As current developments indicate that this difference will only get larger, this effect is not just an artifact of the current architectures."
We showed how to incoporate data- structure sharing.
"For our grammar, the ad-ditional constraint for sharing did not pose a problem."
"If it does pose a problem, there are several techniques to mitigate its effect."
"For example, one could reserve additional in-dexes at critical positions in a subgraph (e.g. based on type information)."
These can then be assigned to nodes in later unifications with-out introducing conflicts elsewhere.
Another technique is to include a tiny table with re-pair information in each share arc to allow a small number of conflicts to be resolved.
"For certain grammars, data-structure shar-ing can also significantly reduce execution times, because the equality check (see line 3 of Unify1) can intercept shared nodes with the same address more frequently."
"We did not ex-ploit this benefit, but rather included an offset check to allow grammar nodes to be shared as well."
"One could still choose, however, not to share grammar nodes."
"Finally, we introduced deferred copying."
"Although this technique did not improve per-formance, we suspect that it might be benefi-cial for systems that use more expensive mem-ory allocation and deallocation models (like garbage collection)."
"Since memory consumption is a major con-cern with many of the current unification-based grammar parsers, our approach pro-vides a fast and memory-efficient alternative to Tomabechi’s algorithm."
"In addition, we showed that our algorithm is well suited for concurrent unification, allowing to reduce ex-ecution times as well."
h²._¤´³µ©«\¦__ \ _&quot; .³À¥j¡®ªs_\.Ê Ë_¦ ©«¯Â·k_.!Ð`_k¾j¡¢ÑA«±Ê k_k§Óº¼Ô¦;__¤ _ k§ QÆwQ ¦Q¥j &quot;&lt;¨®ªs¦Q¥j )_¦  \ k ­w©«\__Æ\¨®ªj­ ·k&lt;­wQªsQ.ÊV¾ _¤Í©«·.Ê  k$È ¨VÄÞ&quot;Ä®È.³µ©« ß à @?&quot;w&gt; áCHC\wâÛA&gt; @ ¡Rk¾j¡¢ÑÕ¦Úªãº!_)s_©«;ÐVº¼8ÑhÈ4¾[$©k³Â¦_
Dominance constraints are logical descriptions of trees that are widely used in computational linguistics.
Their general satisfiability problem is known to be NP-complete.
Here we identify the natural fragment of normal dominance constraints and show that its satisfiability problem is in deterministic polynomial time.
Dominance constraints are used as partial descriptions of trees in problems through-out computational linguistics.
"They have been applied to incremental parsing[REF_CITE], grammar formalisms[REF_CITE], discourse[REF_CITE], and scope un-derspecificati[REF_CITE]."
"Logical properties of dominance constraints have been studied e.g.[REF_CITE], and computational properties have been addressed[REF_CITE]."
"Here, the two most important operations are satisfia-bility testing – does the constraint describe a tree? – and enumerating solutions, i.e. the described trees."
"Unfortunately, even the sat-isfiability problem has been shown to be NP-complete[REF_CITE]."
This has shed doubt on their practical usefulness.
"In this paper, we define normal domi-nance constraints, a natural fragment of dom-inance constraints whose restrictions should be unproblematic for many applications."
We present a graph algorithm that decides sat-isfiability of normal dominance constraints in polynomial time.
Then we show how to use this algorithm to enumerate solutions ef-ficiently.
"An example for an application of normal dominance constraints is scope underspecifi-cation: Constraints as in Fig. 1 can serve as underspecified descriptions of the semantic readings of sentences such as (1), considered as the structural trees of the first-order rep-resentations."
"The dotted lines signify domi-nance relations, which require the upper node to be an ancestor of the lower one in any tree that fits the description. (1) Some representative of every department in all companies saw a sample of each product."
"The sentence has 42 readings[REF_CITE], and it is easy to imagine how the number of readings grows exponen-tially (or worse) in the length of the sen-tence."
Efficient enumeration of readings from the description is a longstanding problem in scope underspecification.
Our polynomial algorithm solves this problem.
"Moreover, the investigation of graph problems that are closely related to normal constraints allows us to prove that many other underspecification formalisms – e.g. Minimal Recursion Seman-tics[REF_CITE]and Hole Seman-tics[REF_CITE]– have NP-hard satisfiability problems."
"Our algorithm can still be used as a preprocessing step for these approaches; in fact, experience shows that it seems to solve all encodings of descriptions in Hole Seman-tics that actually occur."
"In this section, we define the syntax and se-mantics of dominance constraints."
The vari-ant of dominance constraints we employ de-scribes constructor trees – ground terms over a signature of function symbols – rather than feature trees.
"So we assume a signa-ture Σ function symbols f • g • ranged over by f,g,..., a • a • each of which is equipped with an arity ar(f) ≥ 0."
"Constants – function Fig. 2: f(g(a,a)) symbols of arity 0 – are ranged over by a,b."
We assume that Σ contains at least one con-stant and one symbol of arity at least 2.
"Finally, let Vars be an infinite set of vari-ables ranged over by X,Y,Z."
The variables will denote nodes of a constructor tree.
"We will consider constructor trees as directed la-beled graphs; for instance, the ground term f(g(a, a)) can be seen as the graph in Fig. 2."
"We define an (unlabeled) tree to be a fi-nite directed graph (V, E)."
"V is a finite set of nodes ranged over by u, v, w, and E ⊆ V × V is a set of edges denoted by e. The indegree of each node is at most [Footnote_1]; each tree has exactly one root, i.e. a node with indegree 0."
1 The symbol L is overloaded to serve both as a node and an edge labeling.
We call the nodes with outdegree 0 the leaves of the tree.
"A (finite) constructor tree τ is a pair (T, L) consisting of a tree T = (V, E), a node labeling L : V → Σ, and an edge labeling L : E → N, such that for each node u ∈ V and each 1 ≤ k ≤ ar(L(u)), there is exactly one edge (u,v) ∈ E with L((u,v)) = k. 1"
"We draw constructor trees as in Fig. 2, by annotating nodes with their labels and ordering the edges along their labels from left to right."
"If τ = ((V,E),L), we write V τ = V , E τ = E, L τ ="
"Now we are ready to define tree structures, the models of dominance constraints: Definition 2.1."
The tree structure M τ of a constructor tree τ is a first-order structure with domain V τ which provides the dominance relation ∗τ and a labeling relation for each function symbol f ∈ Σ.
"Let u, v, v 1 , . . . v n ∈ V τ be nodes of τ."
"The dominance relationship  ∗τ v holds iff there is a path from u to v in E τ ; the labeling rela-tionship u:f τ (v 1 , . . . , v n ) holds iff u is labeled by the n-ary symbol f and has the children v 1 ,... ,v n in this order; that is, L τ (u) = f, ar(f) = n, {(u, v 1 ), . . . , (u, v n )} ⊆ E τ , and L τ ((u, v i )) = i for all 1 ≤ i ≤ n."
"A dominance constraint ϕ is a conjunction of dominance, inequality, and labeling literals of the following form where ar(f) = n: ϕ ::= ϕ ∧ ϕ 0 |  ∗ Y | X6=Y | X:f(X 1 , . . . , X n )"
Let Var(ϕ) be the set of X f variables of ϕ. A pair of
X 1 X 2 a tree structure M τ and a variable assignment α :
Var(ϕ) → V τ satisfies ϕ Fig. 3: An unsat-iff it satisfies each literal isfiable constraint in the obvious way.
"We say that (M τ ,α) is a solution of ϕ in this case; ϕ is satisfiable if it has a solution."
We usually draw dominance constraints as constraint graphs.
"For instance, the con-straint graph for X:f(X 1 ,X 2 ) ∧ X 1 ∗ Y ∧"
X 2 ∗ Y is shown in Fig. 3.
"As for trees, we annotate node labels to nodes and order tree edges from left to right; dominance edges are drawn dotted."
The example happens to be unsatisfiable because trees cannot branch up-wards.
Let ϕ be a dominance con-straint that does not contain two labeling con-straints for the same variable. 2
"Then the con-straint graph for ϕ is a directed labeled graph G(ϕ) = (Var(ϕ),E,L)."
It contains a (par-tial) node labeling L : Var(ϕ) Σ and an edge labeling L : E → N ∪ { ∗ }.
The sets of edges E and labels L of the graph G(ϕ) are defined in dependence of the literals in ϕ:
"The labeling literal X:f(X 1 ,... ,X n ) belongs to ϕ iff L(X) = f and for each 1 ≤ i ≤ n, (X,X i ) ∈ E and L((X, X i )) = i."
The dominance literal  ∗ Y is in ϕ iff (
"X, Y ) ∈ E and L((X, Y )) = ∗ ."
Note that inequalities in constraints are not represented by the corresponding constraint graph.
We define (solid) fragments of a con-straint graph to be maximal sets of nodes that are connected over tree edges.
"Satisfiability of dominance constraints can be decided easily in non-deterministic polyno-mial time; in fact, it is NP-complete[REF_CITE]."
Y f proof relies on the X f fact that solid frag-
Y 1 Y 2 ments can “overlap” X 1 X 2 properly.
"For illustra-tion, consider the con- Fig. 4: Overlap straint X:f(X 1 , X 2 ) ∧ Y :f(Y 1 ,Y 2 ) ∧ Y ∗ X ∧  ∗ Y 1 , whose con-straint graph is shown in Fig. 4."
"In a solu-tion of this constraint, either Y or Y 1 must be mapped to the same node as X; if X = Y , the two fragments overlap properly."
"In the applications in computational linguistics, we typically don’t want proper overlap; X should never be identified with Y , only with Y 1 ."
The subclass of dominance constraints that ex-cludes proper overlap (and fixes some minor inconveniences) is the class of normal domi-nance constraints.
"3 Allowing more inequality literals does not make satisfiability harder, but the pathological case X 6= X invalidates the simple graph-theoretical characteriza-tions below."
"A dominance constraint ϕ is called normal iff for all variables X, Y, Z ∈ Var(ϕ), 1. X 6= Y in ϕ iff both X:f(. . .) and Y:g(...) in ϕ, where f and g may be equal (no overlap); 3 [Footnote_2]. X only appears once as a parent and once as a child in a labeling literal (tree-shaped fragments); 3. if  ∗ Y in ϕ, neither X:f(...) nor Z:f(. . ."
2 Every constraint can be brought into this form by introducing auxiliary variables and expressing X=Y as ∗ Y ∧ Y ∗ X.
"Y . . .) are (dominances go from holes to roots); 4. if  ∗ Y in ϕ, then there are Z,f such that Z:f(. . ."
X . . .) in ϕ (no empty frag-ments).
"Fragments of normal constraints are tree-shaped, so they have a unique root and leaves."
We call unlabeled leaves holes.
"If X is a vari-able, we can define R ϕ (X) to be the root of the fragment containing X. Note that by Condition 1 of the definition, the constraint graph specifies all the inequality literals in a normal constraint."
All constraint graphs in the rest of the paper will represent normal constraints.
"The main result of this paper, which we prove in Section 4, is that the restriction to normal constraints indeed makes satisfiability polynomial: Theorem 3.2."
"Satisfiability of normal domi-nance constraints is O((k+1) 3 n 2 log n), where n is the number of variables in the constraint, and k is the maximum number of dominance edges into the same node in the constraint graph."
"In the applications, k will be small – in scope underspecification, for instance, it is bounded by the maximum number of argu-ments a verb can take in the language if we disregard VP modification."
So we can say that satisfiability of the linguistically relevant dominance constraints is O(n 2 log n).
Now we derive the satisfiability algorithm that proves Theorem 3.2 and prove it correct.
"In Section 5, we embed it into an enumera-tion algorithm."
An alternative proof of The-orem 3.2 is by reduction to a graph problem discussed[REF_CITE]; this more indirect approach is sketched in Section 6.
"Throughout this section and the next, we will employ the following non-deterministic choice rule (Distr), where X,Y are different variables. (Distr) ϕ ∧  ∗ Z ∧ Y ∗ Z → ϕ ∧  ∗ R ϕ (Y ) ∧ Y ∗ Z ∨ ϕ ∧ Y ∗ R ϕ (X) ∧  ∗ Z"
"In each application, we can pick one of the disjuncts on the right-hand side."
"For instance, we get Fig. 5b by choosing the second disjunct in a rule application to Fig. 5a."
"The rule is sound if the left-hand side is nor-mal:  ∗ Z ∧ Y ∗ Z entails  ∗ Y ∨ Y ∗ X, which entails the right-hand side disjunction because of conditions 1, 2, 4 of normality and X =6 Y ."
"Furthermore, it preserves normality: If the left-hand side is normal, so are both possible results."
A normal dominance con-straint ϕ is in solved form iff (Distr) is not applicable to ϕ and G(ϕ) is cycle-free.
Constraints in solved form are satisfiable.
"In a first step, we characterize the unsatisfia-bility of a normal constraint by the existence of certain cycles in the undirected version of its graph (Proposition 4.4)."
Recall that a cy-cle in a graph is simple if it does not contain the same node twice.
A cycle in an undirected constraint graph is called hypernormal if it does not contain two adjacent dominance edges that emanate from the same node. f • g • • X • Y • f • g • • X • Y • a • Z b • a • Z b • (a) (b)
"Fig. 5: (a) A constraint that entails  ∗ Y , and (b) the result of trying to arrange Y above X. The cycle in (b) is hypernormal, the one in (a) is not."
"For instance, the cycle in the left-hand graph in Fig. 5 is not hypernormal, whereas the cycle in the right-hand one is."
A normal dominance constraint whose undirected graph has a simple hyper-normal cycle is unsatisfiable.
Let ϕ be a normal dominance con-straint whose undirected graph contains a simple hypernormal cycle.
Assume first that it contains a simple hypernormal cycle C that is also a cycle in the directed graph.
There is at least one leaf of a fragment on C; let Y be such a leaf.
"Because ϕ is normal, Y has a mother X via a tree edge, and X is on C as well."
"That is, X must dominate Y but is properly dominated by Y in any solution of ϕ, so ϕ is unsatisfiable."
"In particular, if an undirected constraint graph has a simple hypernormal cycle C with only one dominance edge, C is also a directed cycle, so the constraint is unsatisfiable."
Now we can continue inductively.
"Let ϕ be a con-straint with an undirected simple hypernor-mal cycle C of length l, and suppose we know that all constraints with cycles of length less than l are unsatisfiable."
"If C is a directed cycle, we are done (see above); otherwise, the edges in C must change directions some-where."
"Because ϕ is normal, this means that there must be a node Z that has two incoming dominance edges (X, Z), (Y, Z) which are ad-jacent edges in C. If X and Y are in the same fragment, ϕ is trivially unsatisfiable."
"Other-wise, let ϕ 1 and ϕ 2 be the two constraints ob-tained from ϕ by one application of (Distr) to X,Y,Z. Let C 1 be the sequence of edges we obtain from C by replacing the path from X to R ϕ (Y ) via Z by the edge (X,R ϕ (Y ))."
"C is hypernormal and simple, so no two dom-inance edges in C emanate from the same node; hence, the new edge is the only dom-inance edge in C 1 emanating from X, and C 1 is a hypernormal cycle in the undirected graph of ϕ 1 ."
"C 1 is still simple, as we have only removed nodes."
"But the length of C 1 is strictly less than l, so ϕ 1 is unsatisfiable by induction hypothesis."
An analogous ar-gument shows unsatisfiability of ϕ 2 .
"But be-cause (Distr) is sound, this means that ϕ is unsatisfiable too."
A normal dominance con-straint is satisfiable iff its undirected con-straint graph has no simple hypernormal cy-cle.
The direction that a normal constraint with a simple hypernormal cycle is unsatisfi-able is shown in Lemma 4.3.
"For the converse, we first define an ordering ϕ 1 ≤ ϕ 2 on normal dominance constraints: it holds if both constraints have the same vari-ables, labeling and inequality literals, and if the reachability relation of G(ϕ 1 ) is a subset of that of G(ϕ 2 )."
"If the subset inclusion is proper, we write ϕ 1 &lt; ϕ 2 ."
We call a con-straint ϕ irredundant if there is no normal constraint ϕ 0 with fewer dominance literals but ϕ ≤ ϕ 0 .
"If ϕ is irredundant and G(ϕ) is acyclic, both results of applying (Distr) to ϕ are strictly greater than ϕ."
Now let ϕ be a constraint whose undirected graph has no simple hypernormal cycle.
"We can assume without loss of generality that ϕ is irredundant; otherwise we make it irre-dundant by removing dominance edges, which does not introduce new hypernormal cycles."
"If (Distr) is not applicable to ϕ, ϕ is in solved form and hence satisfiable."
"Otherwise, we know that both results of applying the rule are strictly greater than ϕ."
It can be shown that one of the results of an application of the distribution rule contains no simple hypernor-mal cycle.
We omit this argument for lack of space; details can be found in the proof of Theorem 3[REF_CITE].
"Further-more, the maximal length of a &lt; increasing chain of constraints is bounded by n 2 , where n is the number of variables."
"Thus, appli-cations of (Distr) can only be iterated a fi-nite number of times on constraints without simple hypernormal cycles (given redundancy elimination), and it follows by induction that ϕ is satisfiable."
We can test an undirected constraint graph for the presence of simple hypernormal cycles by solving a perfect weighted matching prob-lem on an auxiliary graph A(G(ϕ)).
"Perfect weighted matching in an undirected graph G = (V,E) with edge weights is the prob-lem of selecting a subset E 0 of edges such that each node is adjacent to exactly one edge in E 0 , and the sum of the weights of the edges in E 0 is maximal."
The auxiliary graph A(G(ϕ)) we consider is an undirected graph with two types of edges.
"For every edge e = (v,w) ∈ G(ϕ) we have two nodes e v ,e w in A(G(ϕ))."
The edges are as follows: (Type A)
"For every edge e in G(ϕ) we have the edge {e v , e w }. (Type B)"
"For every node v and distinct edges e,f which are both incident to v in G(ϕ), we have the edge {e v ,f v } if ei-ther v is not a leaf, or if v is a leaf and either e or f is a tree edge."
We give type A edges weight zero and type B edges weight one.
Now it can be shown ([REF_CITE]Lemma 2) that A(G(ϕ)) has a perfect matching of positive weight iff the undirected version of G(ϕ) contains a sim-ple hypernormal cycle.
"The proof is by con-structing positive matchings from cycles, and vice versa."
Perfect weighted matching on a graph with n nodes and m edges can be done in time
The match-ing algorithm itself is beyond the scope of this paper; for an implementation (in C++) see e.g.[REF_CITE].
"Now let’s say that k is the maximum number of dominance edges into the same node in G(ϕ), then A(G(ϕ)) has O((k + 1)n) nodes and O((k + 1) 2 n) edges."
This shows: Proposition [Footnote_4].5.
"4 In the literature, solved forms with respect to the NP saturation algorithms can contain additional la-beling literals. Our notion of an irredundant solved form corresponds to a minimal solved form there."
"A constraint graph can be tested for simple hypernormal cycles in time O((k + 1) 3 n 2 log n), where n is the number of variables and k is the maximum number of dominance edges into the same node."
"This completes the proof of Theorem 3.2: We can test satisfiability of a normal con-straint by first constructing the auxiliary graph and then solving its weighted match-ing problem, in the time claimed."
It is even easier to test the satisfiability of a hypernormal dominance constraint – a nor-mal dominance constraint in whose constraint graph no node has two outgoing dominance edges.
A simple corollary of Prop. 4.4 for this special case is: Corollary [Footnote_4].6.
"4 In the literature, solved forms with respect to the NP saturation algorithms can contain additional la-beling literals. Our notion of an irredundant solved form corresponds to a minimal solved form there."
A hypernormal constraint is satisfiable iff its undirected constraint graph is acyclic.
This means that satisfiability of hypernor-mal constraints can be tested in linear time by a simple depth-first search.
Now we embed the satisfiability algorithms from the previous section into an algorithm for enumerating the irredundant solved forms of constraints.
"A solved form of the normal constraint ϕ is a normal constraint ϕ 0 which is in solved form and ϕ ≤ ϕ 0 , with respect to the ≤ order from the proof of Prop. 4.4. 4"
"Irredundant solved forms of a constraint are very similar to its solutions: Their con-straint graphs are tree-shaped, but may still 1. Check satisfiability of ϕ. If it is unsatis-fiable, terminate with failure. 2. Make ϕ irredundant. 3."
"If ϕ is in solved form, terminate with suc-cess. 4."
"Otherwise, apply the distribution rule and repeat the algorithm for both results."
Fig. 6: Algorithm for enumerating all irre-dundant solved forms of a normal constraint. contain dominance edges.
Every solution of a constraint is a solution of one of its irre-dundant solved forms.
"However, the number of irredundant solved forms is always finite, whereas the number of solutions typically is not: X:a ∧ Y :b is in solved form, but each so-lution must contain an additional node with arbitrary label that combines X and Y into a tree (e.g. f(a, b), g(a, b))."
"That is, we can ex-tract a solution from a solved form by “adding material” if necessary."
"The main workhorse of the enumeration al-gorithm, shown in Fig. 6, is the distribution rule (Distr) we have introduced in Section 4."
"As we have already argued, (Distr) can be ap-plied at most n 2 times."
Each end result is in solved form and irredundant.
"On the other hand, distribution is an equivalence transfor-mation, which preserves the total set of solved forms of the constraints after the same itera-tion."
"Finally, the redundancy elimination in Step 2 can be done in time O((k +1)n 2 )[REF_CITE]."
This proves: Theorem 5.1.
"The algorithm in Fig. 6 enu-merates exactly the irredundant solved forms of a normal dominance constraint ϕ in time O((k + 1) 4 n 4 N log n), where N is the number of irredundant solved forms, n is the number of variables, and k is the maximum number of dominance edges into the same node."
"Of course, the number of irredundant solved forms can still be exponential in the size of the constraint."
"Note that for hypernor- mal constraints, we can replace the quadratic satisfiability test by the linear one, and we can skip Step 2 of the enumeration algorithm because hypernormal constraints are always irredundant."
This improves the runtime of enumeration to O((k + 1)n 3 N).
"Instead of proving Theorem 4.4 directly as we have done above, we can also reduce it to a configuration problem of dominance graphs[REF_CITE], which provides a more general perspective on related problems as well."
"Dominance graphs are unlabeled, di-rected graphs G = (V, E ] D) with tree edges E and dominance edges D. Nodes with no in-coming tree edges are called roots, and nodes with no outgoing ones are called leaves; dom-inance edges only go from leaves to roots."
"A configuration of G is a graph G 0 = (V, E ] E 0 ) such that every edge in D is realized by a path in G 0 ."
The following results are proved[REF_CITE]: 1.
"Configurability of dominance graphs is in O((k + 1) 3 n 2 log n), where k is the max-imum number of dominance edges into the same node. 2."
"If we specify a subset V 0 ⊆ V of closed leaves (we call the others open) and re-quire that only open leaves can have outgoing edges in E 0 , the configurability problem becomes NP-complete. (This is shown by encoding a strongly NP-complete partitioning problem.) 3."
"If we require in addition that every open leaf has an outgoing edge in E 0 , the prob-lem stays NP-complete."
Satisfiability of normal dominance constraints can be reduced to the first problem in the list by deleting all labels from the constraint graph.
The reduction can be shown to be correct by encoding models as configurations and vice versa.
"On the other hand, the third problem can be reduced to the problems of whether there is a plugging for a description in Hole Seman-tics[REF_CITE], or whether a given MRS de-scription can be resolved[REF_CITE], or whether a given normal dominance constraints has a constructive solution. [Footnote_5] This reduction is by deleting all labels and making leaves that had nullary labels closed."
"5 A constructive solution is one where every node in the model is the image of a variable for which a labeling literal is in the constraint. Informally, this means that the solution only contains “material” “mentioned” in the constraint."
This means that (the equivalent of) deciding satis-fiability in these approaches is NP-hard.
The crucial difference between e.g. satisfi-ability and constructive satisfiability of nor-mal dominance constraints is that it is pos-sible that a solved form has no constructive solutions.
"This happens e.g. in the example from Section 5, X:a ∧ Y : b. The constraint, which is in solved form, is satisfiable e.g. by the tree f(a, b); but every solution must con-tain an additional node with a binary label, and hence cannot be constructive."
"For practical purposes, however, it can still make sense to enumerate the irredundant solved forms of a normal constraint even if we are interested only in constructive solution: It is certainly cheaper to try to find construc-tive solutions of solved forms than of arbitrary constraints."
"In fact, experience indicates that for those constraints we really need in scope underspecification, all solved forms do have constructive solutions – although it is not yet known why."
"This means that our enumera-tion algorithm can in practice be used without change to enumerate constructive solutions, and it is straightforward to adapt it e.g. to an enumeration algorithm for Hole Semantics."
"We have investigated normal dominance con-straints, a natural subclass of general dom-inance constraints."
"We have given an O(n 2 logn) satisfiability algorithm for them and integrated it into an algorithm that enu-merates all irredundant solved forms in time O(Nn 4 log n), where N is the number of irre-dundant solved forms."
This eliminates any doubts about the computational practicability of dominance constraints which were raised by the NP-completeness result for the general language[REF_CITE]and expressed e.g.[REF_CITE].
First experi-ments confirm the efficiency of the new algo-rithm – it is superior to the NP algorithms especially on larger constraints.
"On the other hand, we have argued that the problem of finding constructive solutions even of a normal dominance constraint is NP-complete."
"This result carries over to other underspecification formalisms, such as Hole Semantics and MRS."
"In practice, however, it seems that the enumeration algorithm pre-sented here can be adapted to those problems."
"We would like to thank Ernst Althaus, Denys Duchier, Gert Smolka, Sven Thiel, all members of the[REF_CITE]project CHORUS at the University of the Saarland, and our reviewers."
This work was supported by the DFG in the[REF_CITE].
", _ÜvÅÉº&apos;ÖeßKÎÒ h  © ;Ã¾Àº&apos;¾ÀºvØ¥¿BÅ»â9ø ô, . 9ý © $½&apos;ÅÉº~ ;:Ú);) ;E ù ~Ð±Î_|ÎÆ6Ð±ÅÆ©½&apos;Ã¿   »BÅÉÖ;¹ ½&apos;º&apos;¾º ø __ß   {    î __Î_ËEÕgéQÜvÆ6¾ÙºvØ Æ ª¾«  ^­ ø W __Î _ß   ) ý ½ â &lt;A  "
OøB W | ø .?- &gt; î . ø - C ¯ ±  ³dª¾«^ EÚÄÎÆ5¾À¿ ­ ø W &quot;åÄÃÖ6ÍÅÉËKÎlì~_å  OøV W | Oø .?- |&gt; { :C î . ø - C ý±¾ÀºïÁ&apos;Æ6ÎÁ&amp; { ÿ  ² ø W Oø .?- &gt; îø .- C ! ý  ÖeÎº&apos;Ë&apos;¾ÞÕ Ð ¿
"W Oø .-?&gt; îø .- C Ú&apos;¾ÞÒ ª{ ³d Eâ )ð Î¤ÇAÅÏÅÆÉÚ&amp; _ø 7Æ6Å   ø -?&gt; î . ø - C ý;ÚnÇAÅ î y , &amp;ø W . ø ?- &gt; . ø - T ý î"
K8 ÁÀ?z 8  t f Å ¬ª ! ù ^«
Æ ­ ø W % ( Ê î C % 65( % ( Oø| W Oø /-?&gt;|{ C:î . ø - C ý % ±
T º   î fÇÆ î ËÍÌ r T  :â ;ÃºvØÅ-ÎÒ À ×¾À¿&quot; ¿ À 8
Îd  H! ==ç: ç ~ÃºÄË ÀÈÏ0]  & lt;
Ð8Ñ   K mÒ ¡ }IJ¼; ^Ó ¹ CÔ9ÕÖ 7 § ¥ ¿  ¾À¿&quot;ÃÑÀ¿BÎ±ËÄ¾   _Ü&apos;ÃnÕ  ÎÒ =
Qâ À & quot;½vÅãÁÄÃlÆ Ý ;&quot;ÎÒMÃºäð5Ì3ÌæÐ±Ã¤ß¥½ÄÃÉÏÅ©Ë&apos;¾Þì&amp;ÅÆBÕ   
Ó  ¬Q wâ=ØªÙªâ4Ø &quot;  é !    )
ÎÒ |ÎÑÙÑÞÎ¤Ç Õ
Oø .?- &gt; îø .- C )~¿    :¿ Y . 
Ë Y ¿ Í ø - Oî s . ø - C Ãº&apos;Ë~ ø ¿ W ø - &lt; x î . ø - C â ø .- w î &amp; ¿ =   ;Ë&apos;¿ =Y . ¹   ø ËvÅÉºÄÖeÅÚø ¹ ä Ìý;:â &amp;ÅÉºÄËvÅÉº&apos;ÖeÅ-Ð±Î_ËvÅÉÑ át.
"Ð  !ÃÑÀÑQ¿BÅÉº»BÅÉºÄÖeÅÉ¿)Ç) K,  lâ K¾ÀºÃº~ð5Ì³Ì×ÚQ¾Þ»Ûå9ÅeÕ  ßd"
Ú =  ä lâ ~  ß ã¾ÀºC d Ã  ä &quot; !
Ãk¿BÅÉº»BÅÉºÄÖeÅâ)ð)Î¤ÇAÅÏÅÆÉÚ: _ÜvÅÉº&apos;ÖeÅ)|Ò      ä  ÇAÅÉÑÀÑ â !ð5Ì3Ì
ÖÃºHå;&amp; t .Ø  ä &amp;Ç)½&apos;¾ÀÖ6½³¾À¿ &quot;ÒÎÑÀÑÞÎ¤Ç)¿ ø ê ë = &quot; ì [ ÄRç·j[ _:  .ÎÒ  ¨0   $ÇAÎÆ6Ë&apos;¿.Ãº&apos;Ë_Ú   í!   î ½ 0 )   ¿BÅ»¥ÎÒ ; ;     ed¾ï.
ÅeáE»BÆ6ÃÖe»BÅÉË7Ò|   ed  nÕzÒ|)¿BÅ»1ÒÎÆ   ed  ed :â  Ãº&apos;ÃÑÞß Å×ÅÉÃÖ;½ ÑÞß  :&quot;ÅÅÚ. = = _
Ó  it Ãº&apos;ÃÑÀß_¿BÅÉ¿&quot;Å á &amp;Á VÕ !
"Ê¾ÀÑÀÑÙÜ&apos;¿B»BÆ6Ãl»BÅñØÆ&amp;ÅÉÖeÎÐ±ÅÉ¿;_â lâ =  Ã¤ÏÅÆ6ÃlØÅ³ÃÖÖÜ&apos;Æ6ÃÖeßïÆ6Ãl»BÅÉ¿±ÎÒãð5Ì3Ì³¿HÃº&apos;~&quot;;ËEÕg¿BÁÄÃÖ¾Àº&apos;ØvÚQÇ)Ë v  ä , ¤ ÇAÃlåÄÎ¾ÙÆ6ÑÀËEÕg¾Þ¿BÅÉ¿ÚNÃÖÆ6¾ÀÅÉºv¿BÁ9ÅÉ¾Àº©å9¾ÞÏÎÅÉÑÞ½ÛßâÔÅÃlØãÚAÃº&apos;ÑÀÃlË-å9ÅÉ.ÑÞÅeÑÀ¿7áE¾Ù¾ÀÖºÃÑÁ&apos;ÕgÃnÎávå&apos;¾À¿Õ ¿BÁ9ÅÉÖ¾ÞÒßÐ±Î_ &gt;w s ËvÅÉºvÎ»BÅÉ¿ &quot;½&apos;ÅXÐ±Îø ml n  _ &gt; ËvÅÉÑÙ¿QÃlÆ6Å u n  &quot;ý:ÎÆ å _ø ml  n  &gt; u n  ;ý â î î x k é î  ç î xHt î  ç î xHt &amp;_º :ÁÄÃlÆ;!é&quot;½&apos;Å êÄÆ;;¿)_&quot;_.*Õ ÖÜvÆ;;½ØÆ6ÃlÁÄ½  &quot;_;½&apos;º&apos;¾ _ &amp;  5¿?"
Ú ñ ¹ ä Ì³¿?ÃÖ;½&apos;¾ÞÅÏÅÉ¿ Ãº&apos;Ë ð   ä Ð ð5Ì3Ì³¿Z¾Ùº  _ ¾Àº&apos;Ø$Æ;_ $;ËEÕg¿BÁÄÃÖ¾Àº&apos;Ø &amp; )&amp;ËvÅÉº&apos;ÖeÅð5Ì3Ì³¿äÃº&apos;Ë Á ¹ ä Ì³¿â ¹ ».¾Ù¿. _
ËvÅÉÑzÚ ø ml p2r u p2r ýÂø )
"C ò # ËvÅÁ9ÅÉº&apos;Ë&apos;ÅÉº&apos;ÖeÅ&quot;å9Å»gÇAÅÅÉº.Æ6Ãº&apos;ËvÎÐ_»BÅÉºÄËvÅÉË?ÏnÃlÆ6¾ÙÃlåÄÑÞÅÉ¿ÚÇð5Ì³Ì3¿HÒ&quot;)½&apos;¾ÀÖ6½|ÎÆ E ; ¿B»BÆ6Ü&apos;ÖÃº&apos;Ë*Õ Æ6¾ÙÖ6½×¾ÙºvÒ|ÎÆ;~  ~$ ?&amp;ÅÆ;; )½&apos;Ã¤ÏÅ)ÎåvÕ &amp;)Ì³Ì³¿1Ãº&apos;_Ë b  ä , ó ø}õ ö) =ÏKE êÄÅÉË ~»BÅÉÖ;½&apos;º&apos;¾ _ ;   _»BÅÉº&apos;Ë&apos;ÅÉË þ  ä  ) ¾ÀºvØkÏnÃlÆð5Ì3Ì ;  &amp;__$»BÅÉÖ6½&apos;ºÄ¾   î  ä .5º&apos;Ë  _|ÎÆ; ß Åeá_ÁÄ½&apos;Æ6Ã¿BÅ¶Ö6½»BÆ;_ 5 Ð ¦¶[ &apos;¸[ [ [Äj"
"P* ÿ % ú &amp;(*)+-, . ^ 0 ù / (3  ÿ (*, ×û !&quot;(# &amp;:$,&lt;=; &gt;; A@ &quot;(  Îü F&gt;[]Z *\ P_^` ? a *J *&quot;fi\*=S&quot;`&gt;`kf-l! (qWr*(s**v  ? $ / (y 9z+{&gt;}` | &gt;b!~l  \ &quot;&gt;f `kfWPYfiK~J2| &quot;`"
"Nc ù I *Z]\PY{ I*\ }( &quot;*\ bA&lt;},O{ \ (l  &quot;S ? ,  &quot;, *, }( ÿ n} $$&amp; / &quot;!, &quot;;   | { &quot; Üû ) *bQP=&quot;( *\ *o WefK \ b\ b|9 ¬ÿ n ¾ù n&lt;\*,2`k_W2/fW!1 ^   û (+\PFS&quot;}IWb&quot;(a `}\* W ? @  ]x, éÿ ¡&lt;, ¢ *, }(%&apos;&amp; @  , %&amp;:*; ü   /21( n  &amp; @ 5 *,  (&quot;&quot;}$ ¢&quot;, &quot; £ ü F~fiJ*¤ S&quot;PYIWf}¥(*\ (u §i3*;=u!¨c© 3!s2 ? = % ú ?A*@ ,. ¬$/ (!t ,sA vÿ (&lt;,, ,  Íú &amp;$#&quot;# }}, ; @(\ KcPgfiJ&quot; *!, *?A@ { +&quot;}.R¶&quot; 9ü * \ F¯j|%°$$° [° *\ ±+` *\ *o &gt;bAl _ ! 3  §Y3(¨W ©u(¦(¦_vAu(¦ /( ü  (  Mû ;  ](«· ! ¯;!«·}¢ /( ;"
"W; ]¸:&amp; &lt;, ; &gt;; ? @ ( &lt; âú E ~}¢e5 ¢ ÿ !, @ ú  &quot;g  F+J&lt;NP&gt;\*PgfiJ*\*` ¡a &quot; ¼J*\ *IW¤ *\"
PgfiJ*b~&gt;!b  *1 v/ i ý i )ó }ô õ ô i ý i ý ) i Æ ó }ô õ ö
Æ i ýi ý ý i i i i ý i i ý i ý ý i ý i ý ý
"PgfiJ¾ VYXUZ*]\*PgfiJ**\ &gt;` !b a J*(3_vAy&quot;( *\ b³jWb2NcJ*IW¤ *\ o û  ;  E~½&amp;:&lt;; , ;&gt;; A? @¢&quot;/!] (}  ü ¢¯F,  ¤J&lt;N»P8R*\ PgfiJ*~S Ze\*&quot;PgfiJ*\S*` a &quot;J*fiScPg&quot;*&quot;?A@ !ª(\*(bÁjWb2NcJ¦ *IWo û F ; ] ú ($«·@ &quot;? ;;&lt;ÂD:-9,&quot;&amp;(?  ~  ù ](&lt;;  ~ E$+~ ú ¢¶&quot;]/ ; PgfiJ*\*` a ;&quot;?@ &quot;}  ü"
"F\*P8R \~\**oo  Wf&gt;* ? ;  &lt;J N¼J&quot;*(Q\/ *| a |*\*fiK&quot;PFS=PgfiJl(*bÁ*\ &quot;Ã| f&gt;&quot;bAl*\ ¤ &quot;²ZeSc\ { \ { \*` nZeScPgÈ¡J}( &quot;*&lt;}, (, ( !&quot;, ,  ?  ,  ?  ; ]a *J&quot;(| &quot;ÉIWf&gt;b!/![¼J**:ÈbHÄ9J\*|*  ~\ &lt;&gt;,!b l½_WP=S&quot;¤ ((}( * ü ?  ü F(&lt;, }, ,  ? &quot;} ; ? ;"
"OÂ&apos;+ ¢ èû /F; (]½«·} :&amp; ,&lt;;  ;]=&gt;½; A? +@ &quot;[ ù E; }   +5  ( @  ú ? , *, &lt;, &quot; âü F \*PgfiJ*\*` a *J  NcS&quot;*\&quot;{ \ (*Sc*b m8j aaa*J ¤nµ{&quot;*(t(&gt;(b! ¦lÉJ&lt;NÏÊIWfiS&quot;\*` ? &lt;ÂD&quot;/(MZ]&quot;S ÈÇP=\ \ $»J2|&quot;*{ ; PFJ*¤ *\"
ps0©ª¢«¢¡Q0¬]£¥¤lZ]&apos;j£¢¤p¦K§l£¢¦K K¤¨ sl ¦¢«u£T¡Q¯p ¨ b°±¡Q¯l¢¤l ¨ _¯ ·j­¹K¤l&amp;B«ºl_«»¢© ¨ £¢¦¢¤l·¼²º§pl&amp;­ ¨ £Bj _ « ²º¯l£¢¤&lt; ¨ Yp0 ­¾¿£¢  l_s ¢¡rÀ£B¤À£Bj· ¦K¤ ¨   ¨ K©Â¦B«u£T¡Q¯l ¨ !­8£B¤lÃ¡Q¯pK¤pÆÇ²ºbÈm²É£¢¤l2Ê3ËÌÍ¢§p²Î°¨ 0­?©ÄB«  _ÈY¡r«º· ¨  ¡Q¯l¢¤l ¨ # Å¤p¦¢«u£ ¨²º¯pÁ¸ ¨ ®Q· ¤Q£T²º· ¨ mp0¹£B¤l¨ _ ° ²º¯pÑ¦¢«º£B¡l¯l£¢ ¦KB«º·¼²º¯ ¨ ¨ _ °±¡Q¯pK¤p ¨ Ñ£¢ ·· ¦K¤¨ ¨ 0¤s² ¸b¯Q£B«º£¢¸_²º_« «º¸0¢¦K¤p·  K¤ £B¸0¸§p«u£B¸Æ  ¦¢«º£B¡l¯l ¨ Ô£¢¤p]0¤A¾ ¡Q¯pK¤p ¨  Õ ÖZ×  ×
"The definitions of the basic concepts, rules, and constraints of centering the-ory involve underspecified notions such as ‘previous utterance’, ‘realization’, and ‘ranking’."
"We attempted to find the best way of defining each such notion among those that can be annotated reli-ably, and using a corpus of texts in two domains of practical interest."
"Our main result is that trying to reduce the num-ber of utterances without a backward-looking center ( CB ) results in an in-creased number of cases in which some discourse entity, but not the CB , gets pronominalized, and viceversa."
"Centering Theory[REF_CITE]is best characterized as a ‘parametric’ theory: its key definitions and claims involve no-tions such as ‘utterance’, ‘realization’, and ‘rank-ing’ which are not completely specified; their pre-cise definition is left as a matter for empirical re-search, and may vary from language to language."
"A first goal of the work presented in this paper was to find which way of specifying these param-eters, among the many proposed in the literature, would make the claims of centering theory most accurate as predictors of coherence and pronomi-nalization for English."
"We did this by annotating a corpus of English texts with the sort of informa-tion required to implement some of the most pop-ular variants of centering theory, and using this corpus to automatically check two central claims of the theory, the claim that all utterances have a backward looking center ( CB ) (Constraint 1), and the claim that if any discourse entity is pronomi-nalized, the CB is (Rule 1)."
"In doing this, we tried to make sure we would only use information that could be annotated reliably."
"Our second goal was to evaluate the predic-tions of the theory in domains of interest for real applications–natural language generation, in our case."
"For this reason, we used texts in two gen-res not yet studied, but of interest to developers of NLG systems: instructional texts and descriptions of museum objects to be displayed on Web pages."
The paper is organized as follows.
We first re-view the basic notions of the theory.
We then dis-cuss the methods we used: our annotation method and how the annotation was used.
In Section 4 we present the results of the study.
A discussion of these results follows.
Centering theory[REF_CITE]is an ‘object-centered’ theory of text coherence: it attempts to characterize the texts that can be considered coherent on the basis of the way discourse entities are introduced and dis-cussed. [Footnote_1]
"1 For a discussion of ‘object-centered’ vs. ‘relation-centered’ notions of coherence, see[REF_CITE]."
"At the same time, it is also meant to be a theory of salience: i.e., it attempts to pre-dict which entities will be most salient at any given time (which should be useful for a natural language generator, since it is these entities that are most typically pronominalized[REF_CITE])."
"According to the theory, every UTTERANCE in a spoken dialogue or written text introduces into the discourse a number of FORWARD - LOOKING CENTERS ( CF s)."
"CF s correspond more or less to discourse entities in the sense[REF_CITE], and can be linked to CF s introduced by previous or suc-cessive utterances."
"Forward-looking centers are RANKED , and because of this ranking, some CF s acquire particular prominence."
"Among them, the so-called BACKWARD - LOOKING CENTER ( CB ), defined as follows:"
"Backward Looking Center (CB) CB (U i +1 ), the BACKWARD - LOOKING CENTER of utter-ance U i +1 , is the highest ranked element of CF (U i ) that is realized in U i +1 ."
Utterance U i +1 is classified as a CONTINUE if CB (U i +1 ) =
"CB (U i ) and CB (U i +1 ) is the most highly ranked CF of U i +1 ; as a RETAIN if the CB remains the same, but it’s not any longer the most highly-ranked CF ; and as a SHIFT if CB (U i +1 ) 6="
CB (U i ).
The main claims of the theory are articulated in terms of constraints and rules on CF s and CB .
Constraint 1: All utterances of a segment except for the 1st have exactly one CB .
"Rule 1: if any CF is pronominalized, the CB is."
"Rule2: (sequences of) continuations are pre-ferred over (sequences of) retains, which are preferred over (sequences of) shifts"
Constraint 1 and Rule 2 express a preference for utterances in a text to talk about the same ob-jects; Rule 1 is the main claim of the theory about pronominalization.
In this paper we concentrate on Constraint 1 and Rule 1.
"One of the most unusual features of centering theory is that the notions of utterance, previous utterance, ranking, and realization used in the def-initions above are left unspecified, to be appropri-ately defined on the basis of empirical evidence, and possibly in a different way for each language."
"As a result, centering theory is best viewed as a cluster of theories, each of which specifies the parameters in a different ways: e.g., ranking has been claimed to depend on grammatical functi[REF_CITE], on the-matic roles[REF_CITE], and on the discourse sta-tus of the CF s[REF_CITE]; there are at least two definitions of what counts as ‘previ-ous utterance’[REF_CITE]; and ‘realization’ can be interpreted either in a strict sense, i.e., by taking a CF to be realized in an utterance only if an NP in that utter-ance denotes that CF , or in a looser sense, by also counting a CF as ‘realized’ if it is referred to in-directly by means of a bridging reference[REF_CITE], i.e., an anaphoric expression that refers to an object which wasn’t mentioned before but is somehow related to an object that already has, as in the vase . . . the handle (see, e.g., the discussion[REF_CITE])."
The fact that so many basic notions of centering theory do not have a completely specified def-inition makes empirical verification of the the-ory rather difficult.
"Because any attempt at di-rectly annotating a corpus for ‘utterances’ and their CB s is bound to force the annotators to adopt some specification of the basic notions of the the-ory, previous studies have tended to study a par-ticular variant of the theory[REF_CITE]."
"A notable exception is[REF_CITE], which used an annotated cor-pus to compare the performance of two variants of centering theory."
"The work discussed here, like Tetreault’s, is an attempt at using corpora to compare different ver-sions of centering theory, but considering also pa-rameters of centering theory not studied in this earlier work."
"In particular, we looked at different ways of defining the notion of utterance, we stud-ied the definition of realization, and more gener-ally the role of semantic information."
"We did this by annotating a corpus with information that has been claimed by one or the other version of cen-tering theory to play a role in the definitions of its basic notions - e.g., the grammatical function of an NP , anaphoric relations (including infor-mation about bridging references) and how sen-tences break up into clauses and subclausal units– and then tried to find out the best way of specify-ing these notions automatically, by trying out dif-ferent configurations of parameters, and counting the number of violations of the constraints and rules that would result by adopting a particular parameter configuration."
The principle we used to evaluate the different configurations of the theory was that the best def-inition of the parameters was the one that would lead to the fewest violations of Constraint 1 and Rule 1.
We discuss the results for each principle.
Constraint 1: All utterances of a segment except for the 1st have precisely one CB
Our first set of figures concerns Constraint 1: how many utterances have a CB .
"This con-straint can be used to evaluate how well cen-tering theory predicts coherence, in the follow-ing sense: assuming that all our texts are co-herent, if centering were the only factor behind coherence, all utterances should verify this con-straint."
"The first table shows the results obtained by choosing the configuration that comes clos-est to the one suggested[REF_CITE]: utterance=finite, prev=kameyama, rank=gf, real-ization=direct."
"The first column lists the number of utterances that satisfy Constraint 1; the second those that do not satisfy it, but are segment-initial; the third those that do not satisfy it and are not segment-initial."
"The previous table shows that with this config-uration of parameters, most utterances do not sat-isfy Constraint 1 in the strict sense even if we take into account text segmentation (admittedly, a very rough one)."
"If we take sentences as utterances, instead of finite clauses, we get fewer violations, although about 25% of the total number of utter-ances are violations:"
"Using Suri and McCoy’s definition of previous utterance, instead of Kameyama’s (i.e., treating adjuncts as embedded utterances) leads to a slight improvement over Kameyama’s proposal but still not as good as using sentences:"
What about the finite clause types not consid-ered by Kameyama or Suri and McCoy?
"It turns out that we get better results if we do not treat as utterances relative clauses (which anyway always have a CB , under standard syntactic assumptions about the presence of traces referring to the modi-fied noun phrase), parentheticals, clauses that oc-cur in subject position; and if we treat as a single utterance matrix clauses with empty subjects and their complements (as in it is possible that John will arrive tomorrow)."
"But by far the most significant improvement to the percentage of utterances that satisfy Constraint 1 comes by adopting a looser definition of ’real-izes’, i.e., by allowing a discourse entity to serve as CB of an utterance even if it’s only referred to indirectly in that utterance by means of a bridg-ing reference, as originally proposed[REF_CITE]for her discourse focus."
The following se-quence of utterances explains why this could lead to fewer violations of Constraint 1: (1) (u1)
"These “egg vases” are of exceptional quality: (u2) basketwork bases support egg-shaped bodies (u3) and bundles of straw form the handles, (u4) while small eggs resting in straw nests serve as the finial for each lid. (u5)"
Each vase is decorated with inlaid decoration: ...
"In (1), u1 is followed by four utterances."
"Only the last of these directly refers to the set of egg vases introduced in u1, while they all contain im-plicit references to these objects."
"If we adopt this looser notion of realization, the figures improve dramatically, even with the rather restricted set of relations on which our annotators agree."
Now the majority of utterances satisfy Constraint 1:
And of course we get even better results by treat-ing sentences as utterances:
"It is important, however, to notice that even un-der the best configuration, at least 17% of utter-ances violate the constraint."
"The (possibly, obvi-ous) explanation is that although coherence is of-ten achieved by means of links between objects, this is not the only way to make texts coherent."
"So, in the museum domain, we find utterances that do not refer to any of the previous CF s be-cause they express generic statements about the class of objects of which the object under discus-sion is an instance, or viceversa utterances that make a generic point that will then be illustrated by a specific object."
"In the following example, the second utterance gives some background con-cerning the decoration of a particular object."
"Coherence can also be achieved by explicit coherence relations, such as EXEMPLIFICA-TION in the following example: (3) (u1)"
Jewelry is often worn to signal membership of a particular social group. (u2)
The Beatles brooch shown previously is another case in point:
"Rule 1: if any NP is pronominalized, the CB is In the previous section we saw that allowing bridging references to maintain the CB leads to fewer violations of Constraint 1."
"One should not, however, immediately conclude that it would be a good idea to replace the strict definition of ’realizes’ with a looser one, because there is, unfortunately, a side effect: adopting an in-direct notion of realizes leads to more viola-tions of Rule 1."
Figures are as follows.
"Us-ing utterance=s, rank=gf, realizes=direct 22 pro-nouns violating Rule 1 (9 museum, 13 pharmacy) (13.4%), whereas with realizes=indirect we have 38 violations (25, 13) (23%); if we choose utter-ance=finite, prev=suri, we have 23 violations of rule 1 with realizes=direct (13 + 10) (14%), 32 with realizes=indirect (21 + 11) (19.5%)."
"Using functional centering[REF_CITE]to rank the CF s led to no improvements, because of the almost perfect correlation in our domain be-tween subjecthood and being discourse-old."
One reason for these problems is illustrated by ([Footnote_4]). (4) (u1)
"4 This separation among a ‘center of coherence’ and a ‘center of salience’ is independently motivated by consid-erations about the division of labor between the text planner and the sentence planner in a generation system; see, e.g.,[REF_CITE]."
A great refinement among armorial signets was to reproduce not only the coat-of-arms but the correct tinctures; (u2) they were repeated in colour on the reverse side (u3) and the crystal would then be set in the gold bezel.
"They in u2 refers back to the correct tinctures (or, possibly, the coat-of-arms), which however only occurs in object position in a (non-finite) com-plement clause in (u1), and therefore has lower ranking than armorial signets, which is realized in (u2) by the bridge the reverse side and there-fore becomes the CB having higher rank in (u1), but is not pronominalized."
"In the pharmaceutical leaflets we found a num-ber of violations of Rule 1 towards the end of texts, when the product is referred to."
"A possi-ble explanation is that after the product has been mentioned sentence after sentence in the text, by the end of the text it is salient enough that there is no need to put it again in the local focus by mentioning it explicitly."
"E.g., it in the following example refers to the cream, not mentioned in any of the previous two utterances. (5) (u1)"
A child of 4 years needs about a third of the adult amount. (u2)
A course of treatment for a child should not normally last more than five days (u3) unless your doctor has told you to use it for longer.
Our main result is that there seems to be a trade-off between Constraint 1 and Rule 1.
Allowing for a definition of ’realizes’ that makes the CB be-have more like Sidner’s Discourse Focus[REF_CITE]leads to a very significant reduction in the number of violations of Constraint 1. [Footnote_3]
"3 Footnote 2, page 3 of the intro[REF_CITE]suggests a weaker interpretation for the Constraint: ‘there is no more than one CB for utterance’. This weaker form of the Constraint does hold for most utterances, but it’s almost vacuous, especially for grammatical function ranking, given that utterances have at most one subject."
"We also noted, however, that interpreting ‘realizes’ in this way results in more violations of Rule 1. (No differences were found when functional center-ing was used to rank CF s instead of grammati- cal function.)"
"The problem raised by these re-sults is that whereas centering is intended as an account of both coherence and local salience, dif-ferent concepts may have to be used in Constraint 1 and Rule 1, as in Sidner’s theory."
"E.g., we might have a ‘Center of Coherence’, analogous to Sid-ner’s discourse focus, and that can be realized in-directly; and a ‘Center of Salience’, similar to her actor focus, and that can only be realized directly."
"Constraint 1 would be about the Center of Coher-ence, whereas Rule 1 would be about the Center of Salience."
"Indeed, many versions of centering theory have elevated the CP to the rank of a sec-ond center. 4"
"We also saw that texts can be coherent even when Constraint 1 is violated, as coherence can be ensured by other means (e.g., by rhetorical re-lations)."
"This, again, suggests possible revisions to Constraint 1, requiring every utterance either to have a center of coherence, or to be linked by a rhetorical relation to the previous utterance."
"Finally, we saw that we get fewer violations of Constraint 1 by adopting sentences as our notion of utterance; however, again, this results in more violations of Rule 1."
"If finite clauses are used as utterances, we found that certain types of finite clauses not previously discussed, including rela-tive clauses and matrix clauses with empty sub-jects, are best not treated as utterances."
We didn’t find significant differences between Kameyama and Suri and McCoy’s definition of ‘previous ut-terance’.
We believe however more work is still needed to identify a completely satisfactory way of breaking up sentences in utterance units.
"We wish to thank Kees van Deemter, Barbara di Eugenio, Nikiforos Karamanis and Donia Scott for comments and suggestions."
Massimo Poesio is supported by an EPSRC Advanced Fellowship.
"Hua Cheng, Renate Henschel and Rodger Kib-ble were in part supported by the EPSRC project GNOME, GR/[REF_CITE]/01."
Janet Hitzeman was in part supported by the EPSRC project SOLE.
"Existing software systems for automated essay scor-ing can provide NLP researchers with opportunities to test certain theoretical hypotheses, including some derived from Centering Theory."
"In this study we em-ploy ETS&apos;s e-rater essay scoring system to examine whether local discourse coherence, as de ned by a measure of Rough-Shift transitions, might be a sig-ni cant contributor to the evaluation of essays."
"Our positive results indicate that Rough-Shifts do indeed capture a source of incoherence, one that has not been closely examined in the Centering literature."
"These re-sults not only justify Rough-Shifts as a valid transition type, but they also support the original formulation of Centering as a measure of discourse continuity even in pronominal-free text."
The task of evaluating student&apos;s writ-ing ability has traditionally been a labor-intensiveeral di erenthumansoftwareendeavorsystems.
"However, e.g.,,PEGsev-[REF_CITE], Intelligent Essay Assessor [URL_CITE] and e-rater [URL_CITE] , are now being used to perform this task fully automatically."
"Fur-thermore, by at least one measure, these soft-ware systems evaluate student essays with the same degree of accuracy as human experts."
"That is, computer-generated scores tend to match human expert scores as frequently as two human scores match each other[REF_CITE]. provideEssay"
NLPscoringresearcherssystems withsuch opportunitiesas these can to test certain theoretical hypotheses and to explore a variety of practical issues in compu-tational linguistics.
"In this study, we employ the e-rater essay scoring system to test a hy- pothesis related to Centering Theory ([REF_CITE]in-ter alia)."
We focus on Centering Theory&apos;s Rough-Shift transition which is the least well studied among the four transition types.
"In particular, we examine whether the discourse coherence found in an essay, as de ned by a measure of relative proportion of Rough-Shift transitions, might be a signi cant contributor to the accuracy of computer-generated essay scores."
Our positive nding validates the role of the Rough-Shift transition and suggests a route for exploring Centering Theory&apos;s prac-tical applicability to writing evaluation and instruction.
One goal of automatic essay scoring systems such as e-rater is to represent the criteria that human experts use to evaluate essays.
The writing features that e-rater evaluates were speci cally chosen to re ect scoring criteria for the essay portion of the Graduate Manage-ment Admissions Test (GMAT).
These cri-teria are articulated in GMAT test prepara-tion materials[URL_CITE]
"In e-rater , syntactic variety is represented by features that quantify occurrences of clause types."
Logical organization and clear transi-tions are represented by features that quan-tify cue words in certain syntactic construc-tions.
The existence of main and supporting points is represented by features that detect where new points begin and where they are developed.
E-rater also includes features that quantify the appropriateness of the vocabu-laryOnecontentfeatureof anof writingessay. valued by writing experts that is not explicitly represented in the current version of e-rater is local coher-ence.
Centering Theory provides an algo-rithm for computing local coherence in writ-ten discourse.
Our study investigates the ap-plicability of Centering Theory&apos;s local coher-ence measure to essay evaluation by determin-ing the e ect of adding this new feature to e-rater &apos;s existing array of features.
A synthesis of two di erent lines of work[REF_CITE]and[REF_CITE]yielded the formulation of Centering Theory as a model for moni-toring local focus in discourse.
The Cen-tering model was designed to account for those aspects of processing that are respon-sible for the di erence in the perceived co-herence of discourses such as those demon-strated in (1) and (2) below (examples from Hudson-D&apos;[REF_CITE]). (1) a. Johnbuy awentpiano.to his favorite music store to b. Heyearshad. frequented the store for many c. Hepianowas. excited that he could nally buy a d. Hethearrivedday. just as the store was closing for (2) a. Johnbuy awentpiano.to his favorite music store to b. Itmanywasyearsa store.
John had frequented for c. Hepianowas. excited that he could nally buy a d.
It was closing just as John arrived. thanDiscoursediscourse(1)(2is).intuitivelyThis di erencemore coherentmay be seen to arise from the di erent degrees of con-tinuity in what the discourse is about.
"Dis-course (1) centers a single individual ( John ) whereas discourse (2) seems to focus in and out on di erent entities ( John, store, John, store )."
Centering is designed to capture these uctuations in continuity.
"In this section, we present the basic def-initions and common assumptions in Cen-tering as discussed in the literature (e.g.,"
We present the as-sumptions and modi cations we made for this study in Section 6.1.
"Discourse consists of a sequence of textual segments and each segment consists of a se-quence of utterances. ory, utterances are designatedIn Centeringby U , The-"
"Each utterance U i evokes a set i of dis- n course entities, the FORWARD-LOOKING CENTERS, designated by Cf ( U i ). members of the Cf set are ranked accord-The ing to discourse salience. (Ranking is de-scribed in Section 4.4.)The highest-ranked member of the Cf set is the PREFERRED CENTER, Cp."
"A BACKWARD-LOOKING CENTER, Cb,is also identi ed for utterance U i ."
"The highest ranked entity in the pre-vious utterance, Cf ( U i , 1 ), that is realized in the current utterance, U i , is its des-ignated BACKWARD-LOOKING CENTER, Cb."
"TER isThea specialBACKWARD-LOOKINGmember of the Cf set becauseCEN-it represents the discourse entity that U i is about, what in the literature is often called the &apos;topic&apos;[REF_CITE]. ticalThewithCp itsfor Cba given, bututterancenot necessarilymay besoiden-."
It is precisely this distinction between looking back in the discourse with the Cb and pro-jecting preferences for interpretations in the subsequent discourse with the Cp that pro-vides the key element in computing local co-herence in discourse.
"Four types of transitions, re ecting four de-grees of coherence, are de ned in Centering."
They are shown in transition ordering rule (1).
The rules for computing the transitions are (1 shown )
"Transition in Table ordering 1. rule: Continue is preferred to Retain, which is preferred to Smooth-Shift, which is preferred to Rough-Shift. nounCenteringrule whichde neswe onewill morediscussrulein, thedetailPro-in Section 5."
"In early formulations of Centering Theory, the &apos;utterance&apos; was not de ned explicitly."
"In subsequent work[REF_CITE], the ut-terance was de ned as, roughly, the tensed clause with relative clauses and clausal com-plements as exceptions."
"Based on crosslin-guistic studies,[REF_CITE]de ned the utterance as the traditional &apos;sentence&apos;, i.e., the main clause and its accompanying subor-dinate and adjunct clauses constitute a single utterance."
"As mentioned earlier, the PREFERRED CENTER of an utterance is de ned as the highest ranked member of the Cf set."
The ranking of the Cf members is determined by the salience status of the entities in the utterance and may vary crosslinguistically.
"OBJ &gt; OBJ &gt; OTHERS piricalLaterworkcrosslinguistic([REF_CITE];based[REF_CITE]m-;[REF_CITE]) determined the following detailed ranking, with QIS standing for quan-ti ed inde nite subjects (people, everyone etc) and PRO-ARB (we, you) for arbitrary plural pronominals. forward-looking(3)Revised rulecentersfor the: ranking SUBJ &gt; IND of ."
"OBJ &gt; OBJ &gt; OTHERS &gt; QIS, PRO-ARB. 4.4.1 Complex NPs theInpropertythe caseofofevokingcomplexmultipleNPs,discoursewhich haveen-tities (e.g. his mother, software industry), the working hypothesis commonly assumed (e.g.[REF_CITE]) is ordering from left to right. [Footnote_3]"
3 But see also[REF_CITE]for the treatment of complex NPs in Italian.
"As mentioned brie y earlier, the Centering model includes one more rule, the Pronoun Rule (4) given Pronoun in (4)."
"Rule: If some element of Cf(Ui-1) is realized as a pronoun in Ui, then so is the Cb(Ui). thatThepronominalsPronoun"
Ruleare felicitouslyre ects theusedintuitionto re-fer to discourse-salient entities.
"As a result, Cbs are often pronominalized, or even deleted (if the grammar allows it)."
"Rule (4) then predicts that if there is only one pronoun in an utterance, this pronoun must realize the Cb."
The Pronoun Rule and the distribution of forms (de nite/inde nite NPs and pronom-inals) over transition types plays a signi cant role in the development of anaphora resolu-tion algorithms in NLP.
Note that the utility of the Pronoun Rule and the Centering transi-tions in anaphora resolution algorithms relies heavily on the assumption that the texts un-der consideration are maximally coherent.
"In maximally coherent texts, however, Rough-Shifts transitions are rare, and even in less infrequently.than maximallyForcoherentthis reasontextsthetheydistinc-occur tion between Smooth-Shifts and Rough-Shifts was collapsed in previous work ([REF_CITE]inter alia)."
"The status of Rough-Shift transitions in the Centering model was therefore unclear, receiving only negative evidence: Rough-Shifts are valid be-cause they are found to be rare in coherent discourse. toInthethisnaturestudyofwethegainRough-Shiftsinsights pertainingprecisely because we are forced to drop the coherence assumption."
Our data consist of student es-says whose degree of coherence is under eval-uation and therefore cannot be assumed.
"Us-ing students&apos; paragraph marking as segment boundaries, we &apos;centered&apos; 100 GMAT essays."
The average length of these essays was about 250 words.
"In the next section we show that Rough-Shift transitions provide a reli-able measure of incoherence , correlating well with scores provided by writing experts. ourOnedataof, thethe crucialincoherenceinsightsdetectedwas thatby ,thein Rough-Shift measure is not due to violations of the Pronominal Rule or infelicitous use of pronominal forms in general."
"In Table 2, we report the results of the distribution of theforms211overRough-ShiftRough-Shifttransitionstransitions, found."
"Outin theof set of 100 essays, in 195 occasions the Cp was a nominal phrase, either de nite or indef-inite."
Pronominals occurred in only 16 cases of which 6 cases instantiated the pronominals &apos;we&apos; or &apos;you&apos; in their generic sense.
Table 2 strongly indicates that student essays were not incoherent in terms of the processing load imposed on the reader to resolve anaphoric references.
"Instead, the incoherence in the es-says was due to discontinuities in students&apos; essays caused by their introducing too many undeveloped topics within what should be a conceptually uniform segment, i.e. their para-graphs."
"This is, in fact, what Rough-Shift pickedTheseupresults. not only justify Rough-Shifts as a valid transition type but they also sup-port the original formulation of Centering as a measure of discourse continuity even when anaphora resoluion is not an issue."
It seems that Rough-Shifts are capturing a source of incoherence that has been overlooked in the Centering literature.
The processing load in the Rough-Shift cases reported here is not increased by the e ort required to resolve anaphoric reference but instead by the e ort required to nd the relevant topic connections in a discourse bombarded with a rapid suc-cession of multiple entities.
"That is, Rough-Shifts are the result of absent or extremely short-lived Cbs."
We interpret the Rough-Shift transitions in this context as a re ection of the incoherence perceived by the reader when s/he is unable to identify the topic (fo-cus) structure of the discourse.
This is a signi cant insight which opens up new av-enues for practical applications of the Cen-tering model.
"In an earlier preliminary study, we applied the Centering algorithm manually to a sample of 36 GMAT essays to explore the hypothesis that the Centering model provides a reason-able measure of coherence (or lack of) re ect-ing the evaluation performed by human raters with respect to the corresponding require-ments described in the instructions for human raters."
We observed that essays with higher scores tended to have signi cantly lower per-centages of ROUGH-SHIFTs than essays with lower scores.
"As expected, the distribution of the other types of transitions was not signif-icant."
"In general, CONTINUEs, RETAINs, and SMOOTH-SHIFTs do not yield incoher-ent discourses (in fact, an essay with only CONTINUE transitions might sound rather boring!). a Inpredictorthis studyvariablewe testderivedthe hypothesisfrom"
Centeringthat can signi cantly improve the performance of e-rater .
"Since we are in fact proposing Cen-tering&apos;s ROUGH-SHIFTs as a predictor vari-able, our model, strictly speaking, measures incoherence . poolTheof corpusessays writtenfor ourbystudystudentscametakingfromthea GMAT test."
"We randomly selected a total of 100 essays, covering the full range of the scoring scale, where 1 is lowest and 6 is high-est (see appendix)."
"We applied the Center-ing algorithm to all 100 essays, calculated the percentage of ROUGH-SHIFTs in each essay and then ran multiple regression to evaluate the contribution of the proposed variable to the e-rater &apos;s performance."
We assumed the Cf ranking given in (3). tusAofmodithe cationpronominalwe made involved the sta-
I . [Footnote_4]
"4 In fact, a similar modi cation has been proposed[REF_CITE]and[REF_CITE]observed that the use of I in sentences such as &apos;I believe that...&apos;, &apos;I think that...&apos; do not a ect the focus structure of the text."
"We observed that in low-scored essays the rst person pronom-inal I was used extensively, normally present-ing personal narratives."
"However, personal narratives were unsuited to this essay writing task and were assigned lower scores by ex-pert readers."
The extensive use of I in the subject position produced an unwanted e ect of high coherence.
We prescriptively decided to penalize the use of I &apos;s in order to better re ect the coherence demands made by the particular writing task.
The way to penal-ize was to omit I &apos;s.
"As a result, coherence was measured with respect to the treatment of the remaining entities in the I -containing utterances."
This gave us the desired result of being able to distinguish those I -containing utterances which made coherent transitions with respect to the entities they were talking about and those that did not.
Segment boundaries are ex- tremely hard to identify in an accurate and principled way.
"Furthermore, existing algo-rithms[REF_CITE]rely heavily on the assumption of textual coherence."
"In our case, textual coher-ence cannot be assumed."
"Given that text or-ganization is also part of the evaluation of the essays, we decided to use the students&apos; para-graph breaks to locate segment boundaries."
"For this study, we decided to manually tag coreferring expressions despite the availabil-ity of coreference algorithms."
We made this decision because a poor performance of the coreference algorithm would give us distorted results and we would not be able to test our hypothesis.
"For the same reason, we manu-ally tagged the Preferred centers as Cp."
We only needed to mark all the other entities as OTHER.
This information was adequate for the computation of the Cb and all of the tran-sitionsDiscourse. segmentation and the implemen-tation of the Centering algorithm for the com-putation of the transitions were automated.
Segments boundaries were marked at para-graph breaks and the transitions were calcu-lated according to the instructions given in Table 1.
"As output, the system computed the percentage of Rough-Shifts for each es-say."
The percentage of Rough-Shifts was cal-culated as the number of Rough-Shifts over the total number of identi ed transitions in the essay.
"In the appendix, we give the percentages of Rough-Shifts (ROUGH) for each of the actual student essays (100) on which we tested the ROUGH variable in the regression discussed below."
The HUMAN (HUM) column con-tains the essay scores given by human raters and the EARTER (E-R) column contains the corresponding score assigned by the e-rater .
"Comparing HUMAN and ROUGH, we ob-serve that essays with scores from the higher end of the scale tend to have lower percent- ages of Rough-Shifts than the ones from the lower end."
"To evaluate that this observa-performancetion can be ,utilizedwe regressedto improveX=E-RATERthe and e-rater &apos;s X=ROUGH (the predictors) by Y=HUMAN."
The results of the regression are shown in Ta-blecients3.
"Thethe coef-coef-ingcientoccurrencesfor ROUGHof Rough-Shiftsis negative, thusin thepenaliz-essays."
The t-test (&apos;t-ratio&apos; in Table 3) for ROUGH has a highly signi cant p-value (p &lt; 0.0013) for these 100 essays suggesting that the added variable ROUGH can contribute to the ac-curacy of the model.
"The magnitude of the contribution indicated by this regression is approximately 0.5 point, a reasonalby siz-able e ect given the scoring scale (1-6)."
Ad-ditional work is needed to precisely quan-tify the contribution of ROUGH.
"That would involve incorporating the ROUGH variable into the building of a new e-rater model and comapring the results of the new model to the original e-rater model. of Astheamodelpreliminary, we jacknifedtest ofthethedatapredictability."
We per-formed 100 tests with ERATER as the sole variable leaving out one essay each time and recorded the prediction of the model for that essay.
We repeated the procedure using both variables.
The predicted values for ERATER alone and ERATER+ROUGH are shown in columns PrH/E and PrH/E+R respectively in Table 4.
"In comparing the predictions, we observe that, indeed, 57 % of the predicted values shown in the PrH/E+R column are better approximations of the HUMAN scores, especially in the cases where the ERATER&apos;s score is discrepant by 2 points from the HU-MAN score."
"Our positive nding, namely that Centering Theory&apos;s measure of relative proportion of Rough-Shift transitions is indeed a signi -cant contributor to the accuracy of computer-generated essay scores, has several practical and theoretical implications."
"Clearly, it in-dicates that adding a local coherence feature to e-rater could signi cantly improve e-rater &apos;s scoring accuracy."
"Note, however, that over-all scores and coherence scores need not be strongly correlated."
"Indeed, our data contain several examples of essays with high coher-ence scores but low overall scores and vice versa."
ETSWewritingbrie y reviewedassessmenttheseexpertscasestowithgainseveraltheir insights into the value of pursuing this work further.
"In an e ort to maximize the use of their time with us, we carefully selected three pairs of essays to elicit speci c information."
"One pair included two high-scoring (6) essays, one with a high coherence score and the other with a low coherence score."
"Another pair in-cluded two essays with low coherence scores butnaldipaireringwasoverallcarefullyscoreschosen(a 5toandincludea 6).oneA essay with an overall score of 3 that made several main points but did not develop them fully or coherently, and another essay with an overall score of 4 that made only one main point but did develop it fully and coherently. herenceAfter briemeasurey describingand withoutthe Rough-Shiftrevealing eitherco-the overall scores or the coherence scores of the essay pairs, we asked our experts for their comments on the overall scores and coherence of the essays."
"In all cases, our experts pre-cisely identi ed the scores the essays had been given."
"In the rst case, they agreed with the high Centering coherence measure, but one expert disagreed with the low Centering co-herence measure."
"For that essay, one expert noted that &quot;coherence comes and goes&quot; while another found coherence in a &quot;chronological organization of examples&quot; (a notion beyond the domain of Centering Theory)."
"In the sec-ond case, our experts&apos; judgments con rmed the Rough-Shift coherence measure."
"In the third case, our experts speci cally identi ed both the coherence and the development as-pects as determinants of the essays&apos; scores."
"In general, our experts felt that the development of an automated coherence measure would be a useful instructional aid. overTheotheradvantagequanti ofedthecomponentsRough-Shiftof themetric e- rater is that it can be appropriately translated into instructive feedback for the student."
"In an interactive tutorial system, segments con-taining Rough-Shift transitions can be high-lighted and supplementary instructional com-ments will guide the student into revising the relevant section paying attention to topic dis-continuities."
Our study prescribes a route for several fu-ture research projects.
"Some, such as the need to improve on fully automated tech-niques for noun phrase/discourse entity iden-ti cation and coreference resolution, are es-sential for converting this measure of local co-herence to a fully automated procedure."
"Oth-ers, not explicitly discussed here, such as the status of discourse deictic expressions, nom-inalization resolution, and global coherence studies are fair game for basic, theoretical re-search."
"In this paper, we outline a theory of referential accessibility called Veins Theory (VT)."
"We show how VT addresses the problem of &quot;left satellites&quot;, currently a problem for stack-based models, and show that VT can be used to significantly reduce the search space for antecedents."
"We also show that VT provides a better model for determining domains of referential accessibility, and discuss how VT can be used to address various issues of structural ambiguity."
"In this paper, we outline a theory of referential accessibility called Veins Theory (VT)."
"We compare VT to stack-based models based on Grosz and Sidner&apos;s (1986) focus spaces, and show how VT addresses the problem of &quot;left satellites&quot;, i.e., subordinate discourse segments that appear prior to their nuclei (dominating segments) in the linear text."
"Left-satellites pose a problem for stack-based models, which remove subordinate segments from the stack before pushing a nuclear or dominating segment, thus rendering them inaccessible."
"The percentage of such cases is typically small, which may account for the fact that their treatment has been largely overlooked in the literature, but the phenomenon nonetheless persists in most texts."
We also show how VT can be used to address various issues of structural ambiguity.
Veins Theory (VT) extends and formalizes the relation between discourse structure and reference proposed[REF_CITE].
VT identifies “veins” over discourse structure trees that are built according to the requirements put forth in Rhetorical Structure Theory (RST)[REF_CITE].
"RST structures are represented as binary trees, with no loss of information."
"Veins are computed based on the RST-specific distinction between nuclei and satellites; therefore, RST relations labeling nodes in the tree are ignored."
Terminal nodes in the tree represent discourse units and non-terminal nodes represent discourse relations.
"The fundamental intuition underlying VT is that the distinction between nuclei and satellites constrains the range of referents to which anaphors can be resolved; in other words, the nucleus-satellite distinction induces a domain of referential accessibility (DRA) for each referential expression."
"More precisely, for each anaphor a in a discourse unit u, VT hypothesizes that a can be resolved by examining referential expressions that were used in a subset of the discourse units that precede u; this subset is called the DRA of u. For any elementary unit u in a text, the corresponding DRA is computed automatically from the text&apos;s RST tree in two steps: 1."
Heads for each node are computed bottom-up over the rhetorical representation tree.
Heads of elementary discourse units are the units themselves.
"Heads of internal nodes, i.e., discourse spans, are computed by taking the union of the heads of the immediate child nodes that are nuclei."
"For example, for the text in Figure 1, 1 with the rhetorical structure shown in Figure 2, [Footnote_2] the head of span [5,7] is unit 5."
2 The rhetorical structure is represented using the conventions proposed[REF_CITE].
"Note that the head of span [6,7] is the list &lt;6,7&gt; because both immediate children are nuclei. 2 ."
"Using the results of step 1, Vein"
"The DRA of a unit u is given by the units in the vein that precede u. For example, for the text and RST tree in Figures 1 and 2, the vein expression of unit 3, which contains units 1 and 3, suggests that anaphors from unit 3 should be resolved only to referential expressions in units 1 and 3."
"Because unit 2 is a satellite to unit 1, it is considered to be “blocked” to referential links from unit 3."
"In contrast, the DRA of unit 9, consisting of units 1, 8, and 9, reflects the intuition that anaphors from unit 9 can be resolved only to referential expressions from unit 1, which is the most important unit in span [1,7] and to unit 8, a satellite that immediately precedes unit 9."
Figure 2 shows the heads and veins of all internal nodes in the rhetorical representation.
"In general, co-referential relations (such as the identity relation) induce equivalence classes over the set of referential expressions in a text."
"When hierarchical adjacency is considered, an anaphor may be resolved to a referent that is not the closest in a linear interpretation of a text."
"However, because referential expressions are organized in equivalence classes, it is sufficient that an anaphor is resolved to some member of the set."
This is consistent with the distinction between &quot;direct&quot; and &quot;indirect&quot; references discussed[REF_CITE].
"Veins Theory claims that references from a given unit are possible only in its DRA, i.e., that discourse structure constrains the areas of the text over which references can be resolved."
"In previous work, we compared the potential of hierarchical and linear models of discourse--i.e., approaches that enumerate potential antecedents in an undifferentiated window of text linearly preceding the anaphor under scrutiny--to correctly establish co-referential links in texts, and hence, their potential to correctly resolve anaphors[REF_CITE]."
"Our results showed that by exploiting the hierarchical discourse structure of texts, one can increase the potential of natural language systems to correctly determine co-referential links, which is a requirement for correctly resolving anaphors."
"In general, the potential to correctly determine co-referential links was greater for VT than for linear models when one looks back 4 elementary discourse units."
"When looking back more than four units, the linear model was equally effective."
"Here, we compare VT to stack-based models of discourse structure based on Grosz and Sidner&apos;s (1986) (G&amp;S) focus spaces (e.g.,[REF_CITE])."
"In these approaches, discourse segments are pushed on the stack as they are encountered in a linear traversal of the text."
"Before a dominating segment is pushed, subordinate segments that precede it are popped from the stack."
Antecedents for REs appearing in the segment on the top of the stack are sought in discourse segments in the stack below it.
"Therefore, in cases where a subordinate segment a precedes a dominating segment b, a reference to an entity in a by an RE in b is not resolvable."
"Special provision could be made in order to handle such cases—e.g., subsequently pushing a on top of b—but this would violate the overall strategy of resolving REs appearing in segments currently on the top of the stack."
The special status given to left satellites in VT addresses this problem.
"For example, one RST analysis of (1) proposed[REF_CITE]is given in Figure 3."
Moser and Moore note that the relation of an RST nucleus to its satellite is analogous to the dominates relation proposed by G&amp;S (see also[REF_CITE]).
"As a subordinate segment preceding the segment that dominates it, the satellite is popped from the stack before the dominant segment (the nucleus) is pushed in the stack-based model, and therefore it is not included among the discourse segments that are searched to resolve co-references. [Footnote_3] Similarly, the text in (2), taken from the MUC annotated corpus[REF_CITE], was assigned the RST structure in Figure 4, which presents the same problem for the stack-based approach: the referent for this in C2 is to the Clinton program in A2, but because it is a subordinate segment, it is no longer on the stack when C2 is processed. (1) A1."
"3 Note th[REF_CITE]also propose an informational RST structure for the same text, in which a « volitional-cause » relation holds between the nucleus a and the satellite b, thus providing for a to be on the stack when b is processed."
George Bush supports big business.
He&apos;s sure to ve[REF_CITE].
"To validate our claim, we examined 23 newspaper texts with widely varying lengths (mean length = 408 words, standard deviation 376)."
The texts were annotated manually for co-reference relations of identity[REF_CITE].
The co-reference relations define equivalence relations on the set of all marked references in a text.
The texts were also annotated manually with discourse structures built in the style[REF_CITE].
Each analysis yielded an average of 52 elementary discourse units.
Details of the annotation process are given[REF_CITE].
Six percent of all co-references in the corpus are to left satellites.
"If only co-references pointing outside the unit in which they appear (inter-unit references) are considered, the rate increases to 7.76%."
"Among these cases, two possibilities exist: either the reference is unresolvable using the stack-based method because the unit in which the referent appears has been popped from the stack, or the stack-based algorithm finds a correct referent in an earlier unit that is still on the stack."
Twenty-two percent (2.38% of all co-referring expressions in the corpus) of the referents that VT finds in left satellites fall into the first category.
"For example, in text fragment (3), taken from the MUC corpus, the co-referential equivalence class for the pronoun he in C3 includes Saloman Brothers analyst Jeff Canin in B3 and he in A3."
The RST analysis of this fragment in Figure 5 shows that both A3 and B3 are left satellites.
"A stack-based approach would not find either antecedent for he in C3, since both A3 and B3 are popped from the stack before C3 is processed. (3) A3."
"Although the results were a little lighter than the 49 cents a share he hoped for,"
Salomon Brothers analyst Jeff Canin said
C3. he was pleased with Sun&apos;s gross margins for the quarter.
"In cases where stack-based approaches find a co-referent (although not the most recent antecedent) elsewhere in the stack, it makes sense to compare the effort required by the two models to establish correct co-referential links."
"That is, we assume that from a computational perspective (and, presumably a psycholinguistic one as well), the closer an antecedent is to the referential expression to be resolved, the better."
"We have shown elsewhere[REF_CITE]that VT, compared to linear models, requires significantly less effort for DRAs of any size."
We use a similar strategy here to compute the effort required by VT and stack-based models.
DRAs for both models are treated as ordered lists.
"For example, text fragment (4) reflects the set of units on the stack at a given point in processing one of the MUC texts; units D4 and E4, in brackets, are left satellites and therefore not available using the stack-based model, but visible using VT."
"To determine the correct antecedent of Mr. Clinton in F4 using the stack- based model , it is necessary to search back through 3 units (C4, B4, A4) to find the referent President Clinton."
"In contrast, using VT, we search back only 1 unit to D4. (4) A4."
"A group of top corporate executives urged Congress to pass President Clinton&apos;s deficit-reduction plan,"
B4. declaring that it is superior to the only apparent alternative: more gridlock.
Some of the executives who attended yesterday&apos;s session weren&apos;t a surprise. [ D4.
"Tenneco Inc. Chairman Michael Walsh, for"
"We compute the effort e(M,a,DRA k ) of a model M to determine correct co-referential links with respect to a referential expression a in unit u, given a DRA of size k (DRA k (u)) is given by the number of units between u and the first unit in DRA k that contains a co-referential expression of a."
"The effort e(M,C,k) of a model M to determine correct co-referential links for all referential expressions in a corpus of texts C using DRAs of size k is computed as the sum of the efforts e(M,a,DRA k ) of all referential expressions a where VT finds the co-reference of a in a left satellite."
"Since co-referents found in units that are not left satellites will be identical for both VT and stack-based models, the difference in effort between the two models depends only on co-referents found in left satellites."
Figure 6 shows the VT and stack-based efforts computed over referential expressions resolved by VT in left satellites and k = 1 to 12.
"Obviously, for a given k and a given referent a, that no co-reference exists in the units of the corresponding DRA k"
"In these cases, we consider the effort to be equal to k. As a result, for small k the effort required to establish co-referential links is similar for both models, because both can establish only a limited number of links."
"However, as k increases, the effort computed over the entire corpus diverges, with VT performing consistently better than the stack-"
"Note that in some cases, the stack-based model performs better than VT, in particular for small k."
"This occurs when VT searches back through n adjacent left satellites, where n &gt; 1, to find a co-reference, but a co-referent is found using the stack-based method by searching back m non-left satellite units, where m &lt; n."
"This would be the case, if for instance, VT first found a co-referent for Mr. Clinton In text (4) in D4 (2 units away), but the stack-based model found a co-referent in C4 (1 unit away since the left satellites are not on the stack)."
"In our corpus, 15% of the co-references found in left satellites by VT required less effort using the stack-based method, whereas VT out-performed the stack-based method 23% of the time."
"In the majority of cases (62%), the two models required the same level of effort."
"However, all of the cases in which the stack-based model performed better are for small k (k&lt;4), and the average difference in distance (in units) is 1.25."
"In contrast, VT out-performs the stack-based model for cases ranging over all values of k in our experiment (1 to 12), and the average difference in distance is 3.8 units."
"At k=4, VT can determine all the co-referents in our corpus, whereas the stack-based model requires DRAs of up to 12 units to resolve them all."
This accounts for the marked divergence in effort shown in Figure 6 as k increases.
"So, despite the minor difference in the percentage of cases where VT out-performs the stack-based model, VT has the potential to significantly reduce the search space for co-referential links."
"We have also examined the exceptions, i.e., co-referential links that VT and stack-based models cannot determine correctly."
"Because of the equivalence of the stack contents for left-balanced discourse trees, there is no case in which the stack-based model finds a referent where VT does not."
"There is, however, a number of referring expressions for which neither VT nor the stack-based model finds a co-referent."
"In the corpus of MUC texts we consider, 12.3% of inter-unit references fall into this category, or 9.3% of the references in the corpus if we include intra-unit references."
"Table 1 provides a summary of the types of referring expressions for which co-referents are not found in our corpus—i.e., no antecedent exists, or the antecedent appears outside the DRA. [Footnote_4]"
"4 Our calculations are made based on the RST analysis of the MUC data, in which we detected a small number of structural errors. Therefore, the values given here are not absolute but rather provide an indication of the relative distribution of RE types."
"We show the percentage of REs in our corpus for which VT (and the stack-based model as well, since all units in the DRA computed according to VT are in the DRA computed using the stack-based model) fails to find an antecedent, and the percentage of REs for which VT finds a co-referent (in a left satellite) but the stack-based model does not."
"We consider four types of REs: (1) Pragmatic references, which refer to entities that can be assumed part of general knowledge, such as the Senate, the key in the phrase lock them up and throw away the key, or our in the phrase our streets. (2) Proper nouns, such as Mr. Gerstner or Senator Biden. (3) Common nouns, such as the steelmaker, the proceeds, or the top job. (4) Pronouns"
"Following[REF_CITE], we consider that the evoking power of each of these types of REs decreases as we move down the list."
"That is, pragmatic references are easily understood without an antecedent; proper nouns and noun phrases less so, and are typically resolved by inference over the context."
"On the other hand, pronouns have very poor evoking power, and therefore a message emitter employs them only when s/he is certain that the structure of the discourse allows for easy recuperation of the antecedent in the message receiver&apos;s memory. [Footnote_5]"
"5 Ideally, a psycho-linguistic study of reading times to verify the claim that referees outside the DRA are more difficult to process would be in order."
"Except for the cases where a pronoun can be understood without an antecedent (e.g., our in our streets), it is virtually impossible to use a pronoun to refer to an antecedent that is outside"
"The alignment of the evoking power of referential expressions with the percentage of exceptions for both models shows that the predictions made by VT relative to DRAs are fundamentally correct--that is, their prevalence corresponds directly to their respective evoking powers."
"On the other hand, the almost equal distribution of exceptions over RE types for the stack-based model shows that it is less reliable for determining DRAs."
"Note that in all VT exceptions for pronouns, the RST attribution relation is involved."
Text fragment (5) and the corresponding RST tree (Figure 7) shows the typical case: (5) A5.
"A spokesman for the company said,"
Mr. Bartlett’s promotion reflects the current emphasis at Mary Kay on international expansion.
"Mr. Bartlett will be involved in developing the international expansion strategy,"
D5. he said
"The antecedent for he in D5 is a spokesman for the company in A5, which, due to the nuclear-satellite relations, is inaccessible on the vein."
"Our results suggest that annotation of attributive relations needs to be refined, possibly by treating X said and the attributed quotation as a single unit."
"If this were done, the vein expression would allow appropriate access."
"In sum, VT provides a more natural account of referential accessibility than the stack-based model."
"In cases where the discourse structure is not left-polarized, at least one satellite precedes its nucleus in the discourse and is therefore its left sibling in the binary discourse tree."
"The vein definition formalizes the intuition that in a sequence of units a b c, where a and c are satellites of b, b can refer to entities in a (its left satellite), but the subsequent right satellite, c, cannot refer to a due to the interposition of nuclear unit b--or, if such a reference exists, it is harder to process."
"In stack-based approaches to referentiality, such configurations pose problems: because b dominates a, in order to resolve potential references from b to a, b must appear below a on the stack even though it is processed after a. Even if the processing difficulties are overcome, this situation leads to the postulation of cataphoric references when a satellite precedes its nucleus, which is counter-intuitive."
"The fact that VT considers only the nuclear-satellite distinction and ignores rhetorical labeling has practical ramifications for anaphora resolution systems that rely on discourse structure to determine the DRA for a given RE.[REF_CITE]show that over a corpus of texts drawn from MUC newspaper texts, the Wall Street Journal corpus, and the Brown Corpus, reliable agreement among annotators is consistently obtained for discourse segmentation and assignment of nuclear-satellite status, while agreement on rhetorical labeling was less reliable (statistically significant for only the MUC texts)."
"This means that even when there exist differences in rhetorical labeling, vein expressions can be computed and used to determine DRAs."
"VT also has ramifications for evaluating the viability of different structural representations for a given text, at least for the purposes of reference resolution."
"Like syntactic parsing, discourse parsing typically yields several interpretations, and one of the a priori tasks for further analysis of the parsed texts is to choose one from among potentially several alternative structures."
Considering intention-based relations can yield even more alternatives.
"For anaphora resolution, the choice of one structure over another may have significant impact."
"For example, an RST tree for (6) using rhetorical relations is given in Figure 8; Figure 9 shows another RST tree for the same text, using intention-based relations."
"If we compute the vein expressions for both representations, we see that the vein for segment C6 in the intentional representation is &lt;A6 B6 C6&gt;, whereas in the rhetorical representation, the vein is &lt;(B6), C6&gt;."
"That is, under the constraints imposed by VT, John is not available as a referent for he in C6 in the rhetorical version, although J o h n is clearly the appropriate antecedent."
"Interestingly, the intention-based analysis is skewed to the right and thus is a &quot;better&quot; representation according to the criteria outlined[REF_CITE]; it also eliminates the left-satellite that was shown to pose problems for stack-based approaches."
It is therefore likely that the intention-based analysis is &quot;better&quot; for the purposes of anaphora resolution.
"Veins Theory is based on established notions of discourse structure: hierarchical organization, as in the stack-based model and RST&apos;s tree structures, and dominance or nuclear/satellite relations between discourse segments."
"As such, VT captures and formalizes intuitions about discourse structure that run through the current literature."
"VT also explicitly recognizes the special status of the left satellite for discourse structure, which has not been adequately addressed in previous work."
"In this paper we have shown how VT addresses the left satellite problem, and how VT can be used to address various issues of structural ambiguity."
"VT predicts that references not resolved in the DRA of the unit in which it appears are more difficult to process, both computationally and cognitively; by looking at cases where VT fails we determine that this claim is justified."
"By comparing the types of referring expressions for which VT and the stack-based model fail, we also show that VT provides a better model for determining DRAs."
Building a bilingual dictionary for transfer in a machine translation system is conventionally done by hand and is very time-consuming.
"In order to overcome this bottleneck, we propose a new mechanism for lexical transfer, which is simple and suitable for learning from bilingual corpora."
It exploits a vector-space model developed in information retrieval research.
We present a preliminary result from our computational experiment.
Many machine translation systems have been developed and commercialized.
"When these systems are faced with unknown domains, however, their performance degrades."
"Although there are several reasons behind this poor performance, in this paper, we concentrate on one of the major problems, i.e., building a bilingual dictionary for transfer."
A bilingual dictionary consists of rules that map a part of the representation of a source sentence to a target representation by taking grammatical differences (such as the word order between the source and target languages) into consideration.
These rules usually use case-frames as their base and accompany syntactic and/or semantic constraints on mapping from a source word to a target word.
"For many machine translation systems, experienced experts on individual systems compile the bilingual dictionary, because this is a complicated and difficult task."
"In other words, this task is knowledge-intensive and labor-intensive, and therefore, time-consuming."
"Typically, the developer of a machine translation system has to spend several years building a general-purpose bilingual dictionary."
"Unfortunately, such a general-purpose dictionary is not almighty, in that (1) when faced with a new domain, unknown source words may emerge and/or some domain-specific usages of known words may appear and (2) the accuracy of the target word selection may be insufficient due to the handling of many target words simultaneously."
"Recently, to overcome these bottlenecks in knowledge building and/or tuning, the automation of lexicography has been studied by many researchers: (1) approaches using a decision tree: the ID3 learning algorithm is applied to obtain transfer rules from case-frame representations of simple sentences with a thesaurus for generalization (Akiba et. al., 1996 and[REF_CITE]); (2) approaches using structural matching: to obtain transfer rules, several search methods have been proposed for maximal structural matching between trees obtained by parsing bilingual sentences ([REF_CITE]; Meyers et. al., 1998; and Kaji et. al.,1992)."
"In this paper, we concentrate on lexical transfer, i.e., target word selection."
"In other words, the mapping of structures between source and target expressions is not dealt with here."
We assume that this structural transfer can be solved on top of lexical transfer.
We propose an approach that differs from the studies mentioned in the introduction section in that:
It use not structural representations like case frames but vector-space representations.
The weight of each element for constraining the ambiguity of target words is determined automatically by following the term frequency and inverse document frequency in information retrieval research.
III) A word alignment that does not rely on parsing is utilized.
The background for the decisions made in our approach is as follows:
We would like to reduce human interaction to prepare the data necessary for building lexical transfer rules.
We do not expect that mature parsing systems for multi-languages and/or
This section explains our trial for applying a vector-space model to lexical transfer starting from a basic idea.
"We can select an appropriate target word for a given source word by observing the environment including the context, world knowledge, and target words in the neighborhood."
The most influential elements in the environment are of course the other words in the source sentence surrounding the concerned source word.
Suppose that we have translation examples including the concerned source word and we
IV) Bilingual corpora are clustered in terms of target equivalence. spoken languages will be available in the near future.
We would like the determination of the importance of each feature in the target selection to be automated.
We would like the problem caused by errors in the corpora and data sparseness to be reduced. know in advance which target word corresponds to the source word.
"By measuring the similarity between (1) an unknown sentence that includes the concerned source word and (2) known sentences that include the concerned source word, we can select the target word which is included in the most similar sentence."
"This is the same idea as example-based machine translation ([REF_CITE]and Furuse et. al., 1994)."
Listed in Table 1 are samples of English-Japanese sentence pairs of our corpus including the source word “dry.”
The upper three samples of group 1 are translated with the target word “辛口 (not sweet)” and the lower two samples of group 2 are translated with the target word “乾燥 (not wet).”
The remaining portions of target sentences are hidden here because they do not relate to the discussion in the paper.
The underlined words are some of the cues used to select the target words.
"They are distributed in the source sentence with several different grammatical relations such as subject, parallel adjective, modified noun, and so on, for the concerned word “dry.”"
"We propose representing the sentence as a sentence vector, i.e., a vector that lists all of the words in the sentence."
"The sentence vector of the first sentence of Table 1 is as follows: &lt;this, beer, is, dry, and, full-body&gt;"
Figure 1 outlines our proposal.
Suppose that we have the sentence vector of an input sentence I and the sentence vector of an example sentence E from a bilingual corpus.
We measure the similarity by computing the cosine of the angle between I and E.
We output the target word of the example sentence whose cosine is maximal.
The naïve implementation of a sentence vector that uses the occurrence of words themselves suffers from data sparseness and unawareness of relevance.
"To reduce the adverse influence of data sparseness, we count occurrences by not only the words themselves but also by the semantic categories of the words given by a thesaurus."
"For example, the “辛口 (not sweet)” sentences of"
"Table 1 have the different cue words of “beer,” “sherry,” and “wine,” and the cues are merged into a single semantic category alcohol in the sentence vectors."
"The previous subsection does not consider the relevance to the target selection of each element of the vectors; therefore, the selection may fail due to non-relevant elements."
We exploit the term frequency and inverse document frequency in information retrieval research.
"Here, we regard a group of sentences that share the same target word as a document.”"
Vectors are made not sentence-wise but group-wise.
The relevance of each dimension is the term frequency multiplied by the inverse document frequency.
The term frequency is the frequency in the document (group).
A repetitive occurrence may indicate the importance of the word.
The inverse document frequency corresponds to the discriminative power of the target selection.
It is usually calculated as a logarithm of N divided by df where N is the number of the documents (groups) and df is the frequency of documents (groups) that include the word.
"Before generating vectors, the given bilingual corpus is pre-processed in two ways (1) words are aligned in terms of translation; (2)"
We need to have source words and target words aligned in parallel corpora.
We use a word alignment program that does not rely on parsing[REF_CITE].
"This is not the focus of this paper, and therefore, we will only describe it briefly here. sentences are clustered in terms of target equivalence to reduce problems caused by data sparseness."
"First, all possible alignments are hypothesized as a matrix filled with occurrence similarities between source words and target words."
"Second, using the occurrence similarities and other constraints, the most plausible alignment is selected from the matrix."
We adopt a clustering method to avoid the sparseness that comes from variations in target words.
The translation of a word can vary more than the meaning of the target word.
"For example, the English word “bill” has two main meanings: (1) a piece of paper money, and (2) an account."
"In Japanese, there is more than one word for each meaning."
"For (1), “札” and “紙 幣” can correspond, and for (2), “勘定,” “会 計,” and “料金” can correspond."
"The most frequent target word can represent the cluster, e.g., “紙幣” for (1) a piece of paper money; “勘定” for (2) an account."
We assume that selecting a cluster is equal to selecting the target word.
"If we can merge such equivalent translation variations of target words into clusters, we can improve the accuracy of lexical transfer for two reasons: (1) doing so makes the mark larger by neglecting accidental differences among target words; (2) doing so collects scattered pieces of evidence and strengthens the effect."
"Furthermore, word alignment as an automated process is incomplete."
We therefore need to filter out erroneous target words that come from alignment errors.
Erroneous target words are considered to be low in frequency and are expected to be semantically dissimilar from correct target words based on correct alignment.
Clustering example corpora can help filter out erroneous target words.
"By calculating the semantic similarity between the semantic codes of target words, we 4 Experiment"
"To demonstrate the feasibility of our proposal, we conducted a pilot experiment as explained in this section. perform clustering according to the simple algorithm in subsection 3.2.2."
Suppose each target word has semantic codes for all of its possible meanings.
"In our thesaurus, for example, the target word “札” has three decimal codes, 974 (label/tag), 829 (counter) and 975 (money) and the target word “紙幣” has a single code 975 (money)."
We represent this as a code vector and define the similarity between the two target words by computing the cosine of the angle between their code vectors.
"We adopt a simple procedure to cluster a set of n target words X = {X 1 , X 2 ,…, X n }."
X is sorted in the descending order of the frequency of X n in a sub-corpus including the concerned source word.
"We repeat (1) and (2) until the set X is empty. (1) We move the leftmost X l from X to the new cluster C(X l ). (2) For all m (m&gt;l) , we move X m from X to C(X l ) if the cosine of X l and X m is larger than the threshold T."
"As a result, we obtain a set of clusters {C(X l )} for each meaning as exemplified in Table 2."
The threshold of semantic similarity T is determined empirically.
T in the experiment was 1/2.
"For our sentence vectors and code vectors, we used hand-made thesauri of Japanese and English covering our corpus (for a travel arrangement task), whose hierarchy is based on that of the Japanese commercial thesaurus Kadokawa Ruigo Jiten[REF_CITE]."
We used our English-Japanese phrase book (a collection of pairs of typical sentences and their translations) for foreign tourists.
The statistics of the corpus are summarized in Table 3.
We word-aligned the corpus before generating the sentence vectors.
"We focused on the transfer of content words such as nouns, verbs, and adjectives."
"We picked out six polysemous words for a preliminary evaluation: bill, dry, call in English and “ 熱 ,” “ 悪 い ,” “ 飲 む ” in Japanese."
We confined ourselves to a selection between two major clusters of each source word using the method in subsection 3.2
We compared the accuracy of our proposal using the vector-space model (vsm system) with that of a decision-by-majority model (baseline system).
The results are shown in Table 4.
"Here, the accuracy of the baseline system is #1 (the number of target sentences of the most major cluster) divided by #1&amp;2 (the number of target sentences of clusters 1 &amp; 2)."
The accuracy of the vsm system is #correct (the number of vsm answers that match the target sentence) divided by #1&amp;2.
Judging was done mechanically by assuming that the aligned data was 100% correct. 1
Our vsm system achieved an accuracy from about 60% to about 80% and outperformed the baseline system by about 5% to about 20%.
"One reason why we clustered the example database was to filter out noise, i.e., wrongly aligned words."
We skimmed the clusters and we saw that many instances of noise were filtered out.
"At the same time, however, a portion of correctly aligned data was unfortunately discarded."
"We think that such discarding is not fatal because the coverage of clusters [Footnote_1]&amp;2 was relatively high, around 70% or 80% as shown in Table 5."
"1 This does not necessarily hold, therefore, performance degrades in a certain degree."
"Here, the coverage is #1&amp;2 (the number of data not filtered) divided by #all (the number of data before discarding)."
"An experiment was done for a restricted problem, i.e., select the appropriate one cluster (target word) from two major clusters (target words), and the result was encouraging for the automation of the lexicography for transfer."
"We plan to improve the accuracy obtained so far by exploring elementary techniques: (1) Adding new features including extra linguistic information such as the role of the speaker of the sentence[REF_CITE](also, the topic that sentences are referring to) may be effective; and (2) Considering the physical distance from the concerned input word, which may improve the accuracy."
"A kind of window function might also be useful; (3) Improving the word alignment, which may contribute to the overall accuracy."
"In our proposal, deficiencies in the naïve implementation of vsm are compensated in several ways by using a thesaurus, grouping, and clustering, as explained in subsections 2.3 and 3.2."
We showed only the translation of content words.
"Next, we will explore the translation of function words, the word order, and full sentences."
Our proposal depends on a handcrafted thesaurus.
"If we manage to do without craftsmanship, we will achieve broader applicability."
"Therefore, automatic thesaurus construction is an important research goal for the future."
This paper describes a language independent method for alignment of parallel texts that makes use of homograph tokens for each pair of languages.
"In order to filter out tokens that may cause misalignment, we use confidence bands of linear regression lines instead of heuristics which are not theoreti-cally supported."
"This method was originally inspired on work done by Pascale Fung and Kathleen McKeown, and Melamed, provid-ing the statistical support those authors could not claim."
"Human compiled bilingual dictionaries do not cover every term translation, especially when it comes to technical domains."
"Moreover, we can no longer afford to waste human time and effort building manually these ever changing and in-complete databases or design language specific applications to solve this problem."
"The need for an automatic language independent task for equivalents extraction becomes clear in multi-lingual regions like Hong Kong, Macao, Quebec, the European Union, where texts must be translated daily into eleven languages, or even in the U.S.A. where Spanish and English speaking communities are intermingled."
Parallel texts (texts that are mutual transla-tions) are valuable sources of information for bilingual lexicography.
"However, they are not of much use unless a computational system may find which piece of text in one language corre-sponds to which piece of text in the other lan-guage."
"In order to achieve this, they must be aligned first, i.e. the various pieces of text must be put into correspondence."
This makes the translations extraction task easier and more reli-able.
"Alignment is usually done by finding correspondence points – sequences of characters with the same form in both texts (homographs, e.g. numbers, proper names, punctuation marks), similar forms (cognates, like Region and Região in English and Portuguese, respectively) or even previously known translations."
"However, although the heuristics both ap-proaches use to filter noisy points may be intui-tively quite acceptable, they are not theoretically supported by Statistics."
"The former approach considers a candidate correspondence point reliable as long as, among some other constraints, “[...] it is not too far away from the diagonal [...]” ([REF_CITE]p.72) of a rectangle whose sides sizes are proportional to the lengths of the texts in each language (henceforth, ‘the golden translation diagonal’)."
"The latter ap-proach uses other filtering parameters: maxi-mum point ambiguity level, point dispersion and angle deviation ([REF_CITE]pp. 115–116)."
The method avoids heuristic filters and they claim high precision alignments.
"In this paper, we will extend this work by de-fining a linear regression line with all points generated from homographs with equal frequen-cies in parallel texts."
We will filter out those points which lie outside statistically defined confidence bands[REF_CITE].
Our method will repeatedly use a standard linear regression line adjustment technique to filter unreliable points until there is no misalignment.
Points resulting from this filtration are chosen as correspondence points.
The following section will discuss related work.
The method is described in section 2 and we will evaluate and compare the results in sec-tion 3.
"Finally, we present conclusions and fu-ture work."
There have been two mainstreams for parallel text alignment.
One assumes that translated texts have proportional sizes; the other tries to use lexical information in parallel texts to generate candidate correspondence points.
Both use some notion of correspondence points.
"Early work[REF_CITE]and[REF_CITE]aligned sentences which had a proportional number of words and characters, respectively."
Pairs of sentence delimiters (full stops) were used as candidate correspondence points and they ended up being selected while aligning.
"However, these algorithms tended to break down when sentence boundaries were not clearly marked."
"Full stops do not always mark sentence boundaries, they may not even exist due to OCR noise and languages may not share the same punctuation policies."
"Using lexical information,[REF_CITE]showed that cheap alignment of text segments was still possible exploiting ortho-graphic cognates[REF_CITE], instead of sentence delimiters."
They became the new candidate correspondence points.
"During the alignment, some were discarded because they lied outside an empirically estimated bounded search space, required for time and space reasons."
Words with similar distributions became the candidate correspondence points.
Two sentences were aligned if the number of correspondence points associating them was greater than an empirically defined threshold: “[...] more than some mini-mum number of times [...]” ([REF_CITE]p.128).
In[REF_CITE]noisy points were filtered out by deleting frequent words.
"Instead, they used vectors that stored distances between consecutive occurrences of a word (DK-vec’s)."
Candidate correspondence points were identified from words with similar distance vectors and noisy points were filtered using some heuristics.
"Later,[REF_CITE], the algorithm used extracted terms to compile a list of reliable pairs of translations."
Those pairs whose distribution similarity was above a threshold became candidate correspon-dence points (called potential anchor points).
These points were further constrained not to be “too far away” from the ‘translation diagonal’.
"Some were filtered out if they either lied outside an empirically defined search space, named a corridor, or were “not in line” with their neighbours."
"A maximum point ambiguity level filters points outside a search space, a maximum point dispersion filters points too distant from a line formed by candidate correspondence points and a maximum angle deviation filters points that tend to slope this line too much."
"Whether the filtering of candidate correspon-dence points is done prior to alignment or during it, we all want to find reliable correspondence points."
They provide the basic means for ex-tracting reliable information from parallel texts.
"However, as far as we learned from the above papers, current methods have repeatedly used statistically unsupported heuristics to filter out noisy points."
"For instance, the ‘golden transla-tion diagonal’ is mentioned in all of them but none attempts filtering noisy points using statis-tically defined confidence bands."
The basic insight is that not all candidate corre-spondence points are reliable.
"Whatever heuris-tics are taken (similar word distributions, search corridors, point dispersion, angle deviation,...), we want to filter the most reliable points."
We assume that reliable points have similar charac-teristics.
"For instance, they tend to gather some-where near the ‘golden translation diagonal’."
Homographs with equal frequencies may be good alignment points.
We worked with a mixed parallel corpus con-sisting of texts selected at random from the Offi-cial Journal of the European Communities [Footnote_1][REF_CITE]and from The Court of Justice of the European Communities [Footnote_2] in eleven lan-guages [Footnote_3] .
"1 Danish (da), Dutch (nl), English (en), French (fr), German (de), Greek (el), Italian (it), Portuguese (pt) and Spanish (es)."
2 Webpage address: curia.eu.int
3 The same languages as those in footnote 1 plus Finnish (fi) and Swedish (sv).
"For each language, we included: • five texts with Written Questions asked by members of the European Parliament to the European Commission and their corre-sponding answers (average: about 60k words or 100 pages / text); • five texts with records of Debates in the European Parliament (average: about 400k words or more than 600 pages / text)."
These are written transcripts of oral discussions; • five texts with judgements of The Court of Justice of the European Communities (aver-age: about 3k words or 5 pages / text).
"In order to reduce the number of possible pairs of parallel texts from 110 sets (11 lan-guages×10) to a more manageable size of 10 sets, we decided to take Portuguese as the kernel language of all pairs."
We generate candidate correspondence points from homographs with equal frequencies in two parallel texts.
"Homographs, as a naive and par-ticular form of cognate words, are likely transla-tions (e.g. Hong Kong in various European lan-guages)."
Here is a table with the percentages of occurrences of these words in the used texts:
"For average size texts (e.g. the Written Ques-tions), these words account for about 5% of the total (about 3k words / text)."
This number varies according to language similarity.
"For instance, on average, it is higher for Portuguese–Spanish than for Portuguese–English."
These words end up being mainly numbers and names.
"Here are a few examples from a parallel Portuguese–English text: 2002 (num-bers, dates), ASEAN (acronyms), Patten (proper names), China (countries), Manila (cities), apartheid (foreign words), Ltd (abbreviations), habitats (Latin words), ferry (common names), global (common vocabulary)."
"In order to avoid pairing homographs that are not equivalent (e.g. ‘a’, a definite article in Por-tuguese and an indefinite article in English), we restricted ourselves to homographs with the same frequencies in both parallel texts."
"In this way, we are selecting words with similar distri-butions."
"Actually, equal frequency words helped Jean-François Champollion to decipher the Ro-setta Stone for there was a name of a King (Ptolemy V) which occurred the same number of times in the ‘parallel texts’ of the stone."
Each pair of texts provides a set of candidate correspondence points from which we draw a line based on linear regression.
Points are de-fined using the co-ordinates of the word posi-tions in each parallel text.
"For example, if the first occurrence of the homograph word Patten occurs at word position 125545 in the Portuguese text and at 135787 in the English parallel text, then the point co-ordinates are (125545,135787)."
The generated points may adjust themselves well to a linear regression line or may be dispersed around it.
"So, firstly, we use a simple filter based on the histogram of the distances between the expected and real posi-tions."
"After that, we apply a finer-grained filter based on statistically defined confidence bands for linear regression lines."
We will now elaborate on these filters.
The points obtained from the positions of homo-graphs with equal frequencies are still prone to be noisy.
Here is an example: line’) candidate correspondence points.
The linear regression line equation is shown on the top right corner.
The figure above shows noisy points because their respective homographs appear in positions quite apart.
We should feel reluctant to accept distant pairings and that is what the first filter does.
It filters out those points which are clearly too far apart from their expected positions to be considered as reliable correspondence points.
"We find expected positions building a linear regression line with all points, and then deter-mining the distances between the real and the expected word positions:"
"Expected positions are computed from the lin-ear regression line equation y = ax + b, where a is the line slope and b is the Y-axis intercept (the value of y when x is 0), substituting x for the Portuguese word position."
"For Table 3, the ex-pected word position for the word I at pt word position 3877 is 0.9165 × 3877 + 141.65 = 3695 (see the regression line equation in Figure 1) and, thus, the distance between its expected and real positions is | 3695 – 24998 | = 21303."
"If we draw a histogram ranging from the smallest to the largest distance, we get:"
"In order to build this histogram, we use the Sturges rule (see ‘Histograms’[REF_CITE])."
"The number of classes (bars or bins) is given by 1 + log 2 n, where n is the total number of points."
The size of the classes is given by (maximum distance – minimum distance) / number of classes.
"For example, for Figure 1, we have 3338 points and the distances between expected and real positions range from 0 to 35997."
"Thus, the number of classes is 1 + log 2 3338 ≅ 12.7 → 13 and the size of the classes is (35997 – 0) / 13 ≅ 2769."
"In this way, the first class ranges from 0 to 2769, the second class from 2769 to 5538 and so forth."
"With this histogram, we are able to identify those words which are too far apart from their expected positions."
"In Figure 2, the gap in the histogram makes clear that there is a discontinu-ity in the distances between expected and real positions."
"So, we are confident that all points above 22152 are extreme points."
We filter them out of the candidate correspondence points set and proceed to the next filter.
"Confidence bands of linear regression lines ([REF_CITE]p. 384) help us to identify reliable points, i.e. points which belong to a regression line with a great confidence level (99.9%)."
The band is typically wider in the extremes and narrower in the middle of the regression line.
The figure below shows an example of filter-ing using confidence bands:
"We start from the regression line defined by the points filtered with the Histogram technique, described in the previous section, and then we calculate the confidence band."
Points which lie outside this band are filtered out since they are credited as too unreliable for alignment (e.g. Point A in Figure 3).
"We repeat this step until no pieces of text belong to different translations, i.e. until there is no misalignment."
The confidence band is the error admitted at an x co-ordinate of a linear regression line.
"A point (x,y) is considered outside a linear regres-sion line with a confidence level of 99.9% if its y co-ordinate does not lie within the confidence interval [ ax + b – error(x); ax + b + error(x)], where ax + b is the linear regression line equa-tion and error(x) is the error admitted at the x co-ordinate."
The upper and lower limits of the confidence interval are given by the following equation (see Thomas Wonnacott &amp;[REF_CITE]p. 385): y =(ax + b) ± t 0.005 s 1 + (x − X ) 2n n ∑ (x − X ) 2 i i=1 where: • t 0.005 is the t-statistics value for a 99.9% con-fidence interval.
"We will use the z-statistics instead since t 0.005 = z 0.005 = 3.27 for large samples of points (above 120); • n is the number of points; • s is the standard deviation from the expected value ŷ at co-ordinate x (see Thomas Won-nacott &amp;[REF_CITE]p. 379): n ∑ (y − yˆ) i s = i=1 , where ˆy = ax + b n − 2 • X is the average value of the various x i : n X = 1 ∑ x i n i=1"
We ran our alignment algorithm on the parallel texts of 10 language pairs as described in section 2.2.
The table below summarises the results:
"On average, we end up with about 2% of the initial correspondence points which means that we are able to break a text in about 90 segments (ranging from 70 words to 12 pages per segment for the Debates)."
An average of just three filtra-tions are needed: the Histogram filter plus two filtrations with the Confidence Bands.
The figure below shows an example of a mis-aligning correspondence point.
"Had we restricted ourselves to using homo-graphs which occur only once (hapaxes), we would get about one third of the final points[REF_CITE]."
Hapaxes turn out to be good candidate correspondence points because they work like cognates that are not mistaken for others within the full text scope[REF_CITE].
"When they are in similar positions, they turn out to be reliable correspondence points."
"To compare our results, we aligned the BAF Corpus[REF_CITE]which consists of a collection of parallel texts (Canadian Parliament Hansards, United Nations, literary, etc.)."
"The table above shows that, on average, we got about 1.5% of the total segments, resulting in about 10k characters per segment."
This num-ber ranges from 25% (average: 500 characters per segment) for a small text (tao3.fr-en) to 1% (average: 15k characters per segment) for a large text (ilo.fr-en).
"Although these are small num- bers, we should notice that, in contrast with Mi-chel[REF_CITE], we are not including: • words defined as cognate “if their four first characters are identical”; • an ‘isolation window’ heuristics to reduce the search space; • heuristics to define a search corridor to find candidate correspondence points;"
We should stress again that the algorithm re-ported in this paper is purely statistical and re-curs to no heuristics.
"Moreover, we did not re-apply the algorithm to each aligned parallel segment which would result in finding more correspondence points and, consequently, fur-ther segmentation of the parallel texts."
"Besides, if we use the methodology presented in Joaquim da[REF_CITE]for extracting relevant string patterns, we are able to identify more sta-tistically reliable cognates."
"However, the algorithm does not as-sure 100% alignment precision and discards some good correspondence points which end up in bad clusters."
"Our main critique to the use of heuristics is that though they may be intuitively quite accept-able and may significantly improve the results as seen with Jacal alignment for the BAF Corpus, they are just heuristics and cannot be theoreti-cally explained by Statistics."
"At the moment, we are working on alignment of sub-segments of parallel texts in order to find more correspondence points within each aligned segment in a recursive way."
We are also plan-ning to apply the method to large parallel Portu-guese–Chinese texts.
"We believe we may sig-nificantly increase the number of segments we get in the end by using a more dynamic ap-proach to the filtering using linear regression lines, by selecting candidate correspondence points at the same time that parallel texts tokens are input."
"This approach is similar[REF_CITE]but, in contrast, it is statistically sup-ported and uses no heuristics."
Another area for future experiments will use relevant strings of characters in parallel texts instead of using just homographs.
"For this pur-pose, we will apply a methodology described in Joaquim da[REF_CITE]."
This method was used to extract string patterns and it will help us to automatically extract ‘real’ cognates.
ª« +(&lt;² [²Y«@¬}{·T¸/¹&amp;º ° //¿¼²Tº]&quot;[²Y¶+±¿® ¼¾/«^ºn ¹ &amp;¸^¶K²Y¿Á¯ÃÂ¸³+[¯ [&lt;±·&quot;&lt; [/«Ä¬ Å &quot; ªÈÊÉË±¿Á®¼¾/&amp;²Y«¬Ì¹&amp;¸^¶K²Y¿Á¯g´@%^º *±¿Á®Á¾/&amp;²Y«@¬Î¹&amp;^¸ &amp;^º ®«K¾¡¬[²Y·&quot;@&quot;®Á¸/&amp; / +² [²Y«@@&amp;^ ¬ ¸{·T¸/¹}/&amp;²Y«@ [ /«Ö·T³Q®¼¬[²g³&quot;®¼¸/[{¸Â (&quot;%Ù ®Á¬[/&amp;²Y«@¬·T¸/¹&amp;º [¸&amp;±y¹&lt;^[º %±¿Á®¼¾/&amp;²Y«¬gÄWÅ &amp;^¸ ^¶ º +&quot;[¯ ¬º] +&amp;^¸ [/± [¬[&amp;°n¿¼²l¹&amp;¸^¶^º ²Y¿¯+ªÈÌÉ&gt;º&quot;Û¸³}%&quot; ±Àn¿¼²%¬[¸&amp;¾¸&amp;/]^º  á â 0¨-/ ä D¨= /å[REF_CITE]ª«Ü¯[[&quot; ¬[/+¯[&gt;± [[/«¹&amp;^¸ ê ìí©îëbï &quot;­ ¯ {/[/¸ ½K³&quot;·T² ¿
Á±«K¾/]^ô õ ï [&quot;³ ®Á«K¾ñéëê &quot;¾²g¬ß¿Á±«^º ¾  ï [&quot;³ ®Á«K¾ í ëî ÄGªú«óèù¯[[ ï ±¿Á®¼¾/«^º / ¹ &amp;²Y«@+¬ &amp;¹ ^¸ ¶K²Y¿Á¯ ëWê  êë ìíYîë ï /^« º ¹
This paper presents a restricted version of Set-Local Multi-Component TAGs generative([REF_CITE]apacity) whichofretainsTree-Localthe strongMulti-Component TAG (i.e. produces the same derived structures) but has a greater derivational generative capacity (i.e. can derive those structures in more ways).
This formalism is then applied as a framework for integrating dependency and constituency based linguistic repre-sentations.
"An aim of one strand of research in gener-ative grammar is to nd a formalism that has a restricted descriptive capacity  to describe natural language, but no more powerful than necessary, so that the reasons some constructions are not legal in any nat-ural language is explained by the formalism rather than stipulations in the linguistic the-ory."
"Several mildly context-sensitive grammar formalisms, all characterizing the same string languages, are currently possible candidates for adequately describing natural language; however, they di er in their capacities to as-sign appropriate linguistic structural descrip-tions to these string languages."
"The work in this paper is in the vein of other work[REF_CITE]in extracting as much structural de-scriptive power given a xed ability to de-scribe strings, and uses this to model depen-dency as well as constituency correctly. scriptiveOne waypowerto characterizeis by the theasetformalism&apos;sof string lan-de-guages it can generate, called its weak gener-ative capacity ."
"For example, Tree Adjoining Grammars (TAGs)[REF_CITE]can generate the language a n b n c n d n and Context-Free Grammars (CFGs) cannot[REF_CITE]."
"S S S : : : theHowevercapacity, weakof a grammargenerativeformalismcapacitytoignoresgener-ate derived trees."
This is known as its strong generative capacity .
"For example, CFGs and TAGs can both generate the language a n b n , but CFGs can only associate the a &apos;s and b &apos;s by making them siblings in the derived tree, as shown in Figure 1, whereas a TAG can gen-erate the in nite set of trees for the language a n b n that have a &apos;s and b &apos;s as siblings, as well as the in nite set of trees where the a &apos;s dom-inate the b &apos;s in each tree, shown in Figure 2[REF_CITE]; thus TAGs have more strong generativeIn additioncapacityto thethantree CFGssets and. string lan-guages a formalism can generate, there may also be linguistic reasons to care about how these structures are derived."
"For this reason, multi-component TAGs (MCTAGs)[REF_CITE]have been adopted to model some linguistic phenomena."
"In multi-component TAG, elementary trees are grouped into tree sets , and at each step of the derivation all the trees of a set adjoin simultaneously."
In tree-local MCTAG (TL-MCTAG) all the trees of a set are required to adjoin into the same elementary tree; in set-local MCTAG (SL-MCTAG) all the trees of a set are required to adjoin into the same elementary tree set.
"TL-MCTAGslanguages andcanderivedgeneratetree setsthe sameas ordinarystring TAGs, so they have the same weak and strong generative capacities, but TL-MCTAGs can derivethan TAGsthese samecan."
Onestringsmotivationand trees forin moreTL-MCTAG as a linguistic formalism[REF_CITE]is that it can generate a functional head (such as does ) in the same derivational step as the lexical head with which it is associated (see Figure 3) without violating any assump-tions about the derived phrase structure tree { something TAGs cannot do in every case.
S seem : S sleep sleep : does S seem
S ... does S
John VP VP VP John seem VP seem VP to sleep to sleep
Figure 3: TL-MCTAG generable derivation marThisformalismnotion ofasthetheyderivationsrelate toofthea struc-gram-tures they derive has been called the deriva-tional generative capacity (1992).
"Somewhat more formally (for a precise de nition, see[REF_CITE]): we annotate each ele-ment of a derived structure with a code indi-cating which step of the derivation produced that element."
This code is simply the address of the corresponding node in the derivation tree. [Footnote_1]
"1[REF_CITE]the derived structures were always strings, and the codes were not addresses but unordered identi ers. We trust that our de nition is in the spirit of theirs."
"Then a formalism&apos;s derivational gener-ative capacity is the sets of derived structures, thus annotated, that it can generate. formalismThe derivationalalso describesgenerativewhat partscapacityof aofde-a rived structure combine with each other."
"Thus if we consider each derivation step to corre-spond to a semantic dependency, then deriva-tional generative capacity describes what other elements a semantic element may de-pend on."
"That is, if we interpret the derivation trees of TAG as dependency structures and the derived trees as phrase structures, then the derivational generative capacity of TAG limits the possible dependency structures that can be assigned to a given phrase structure."
"We have seen that TL-MCTAGs can gener-ate some derivations for \Does John seem to sleep&quot; that TAG cannot, but even TL-MCTAG cannot generate the string, \Does John seem likely to sleep&quot; with a derived tree that matches some linguistic notion of correct constituency and a derivation that matches some notion of correct dependency."
"This is because the components for `does&apos; and `seem&apos; would have to adjoin into di erent compo-nents of the elementary tree set for `likely&apos; (see Figure 4), which would require a set-local multi-component TAG instead of tree-local. componentUnfortunatelyTAGs, unrestrictednot only haveset-localmore deriva-multi-tional generative capacity than TAGs, but they also have more weak generative capac-ity: SL-MCTAGs can generate the quadru-ple copy language[URL_CITE], for example, which does not correspond to any known linguis-tic phenomenon."
"Other formalisms aiming to model dependency correctly similarly expand weak generative capacity, notably D-tree Sub-stitution Grammar[REF_CITE], and consequently end up with much greater parsingThe workcomplexityin this. paper follows another line of research which has focused on squeez-ing as much strong generative capacity as possible out of weakly TAG-equivalent for-malisms."
"Tree-local multicomponent TAG[REF_CITE], nondirectional compositi[REF_CITE], and seg-mented adjuncti[REF_CITE]are exam-ples of this approach, wherein the constraint on weak generative capacity naturally limits the expressivity of these systems."
"We discuss the relation of the formalism of this paper, Restricted MCTAG (R-MCTAG) with some of these in Section 5."
"componentThe way weadjunctionpropose tois dealrst towithlimitmulti-the numberroughly speakingof components, to treatto twotwo-component, and then, adjunction as one-component adjunction by temporarily removing the material between the two adjunction sites."
"The reasons behind this scheme will be explained in subsequent sections, but we mention it now because it tionsmotivateson possiblethe somewhatadjunctioncomplicatedsites: restric- One adjunction site must dominate the otherthe set."
"Ifofthenodestwo dominatedsites are h byandone l node, call but not strictly dominated by the other the site-segment h h ; l i ."
Removing a site-segment must not de-prive a tree of its foot node.
"That is, no site-segment h h ; l i may contain a foot node unless l is itself the foot node."
"If two tree sets adjoin into the same tree, the two site-segments must be simulta-neouslysegmentsremovablemust be.disjointThat is,, theor onetwomustsite-contain the other. treeBecausesets withof thethe componentsrst restrictionconnected, we depictby a dominance link (dotted line), in the man-ner[REF_CITE]."
"As written, the above rules only allow tree-local adjunction; we can generalize them to allow set-local ad-junction by treating this dominance link like an ordinary arc."
But this would increase the weak generative capacity of the system.
"For present purposes it is  just to allow one type of set-local adjunction: adjoin the upper tree to the upper foot, and the lower treeThisto thedoeslowernot increaseroot (seetheFigureweak5generative). capacity, as will be shown in Section 2.3."
Ob-serve that the set-local TAG given in Figure 5 obeys the above restrictions.
"For the following section, it is useful to think of TAG in a manner other than the usual."
"Instead of it being a tree-rewriting system whose derivation history is recorded in a derivation tree, it can be thought of as a set of trees (the `derivation&apos; trees) with a yield function (here, reading o the node labels of derivation trees, and composing correspond-ing elementary trees by adjunction or sub-stitution as appropriate) applied to get the TAG trees."
TAGsMore h G precisely ; G 0 i = hh : a ; NT [Footnote_2]LTAG ; I; A; S is i ; h a I [ pair A; I of [ A; I 0 ;
"2 The Gorn address of a root node is ; if a node has Gorn address , then its i th child has Gorn address"
A 0 ; S 0 ii .
G 0 Wethecall meta-level G the object-level grammar.
"Thegrammarobject-level, and grammar is a standard TAG: its terminal and nonterminal alphabetsand NT ,"
"A are its initial and auxiliary trees, and S 2 I contains the trees which derivations may start with. thatTheit meta-levelderives treesgrammarthat look G 0 likeis dederivationned so trees of G :"
Nodes are labeled with (the names of) elementary trees of G .
Foot nodes have no labels.
Arcs are labeled with Gorn addresses. 2
An auxiliary tree may adjoin anywhere.
"When a tree is adjoined at a node , is rewritten as , and the foot of inherits the label of . f G The[ T ( G tree 0 )], where set of f h is G; the G 0 i ,yield T ( h G function ;G 0 i ), ofis G and T ( G 0 ) is the G tree set of G 0 ."
"Thus, the elementary trees of G 0 are combined to form a derived tree, which is then interpreted as a derivation tree for G , which gives instructions fornalcombiningderived treeelementary. trees of G into the meta-levelIt was showngrammarin Drasis in(1999the ) regularthat whenformthe[REF_CITE]the formalism is weakly equiv-alent to TAG."
Consider the case of a multicomponent tree set f 1 ; 2 g adjoining into an initial tree (Figure 6).
Recall that we de ned a site-segment of a pair of adjunction sites to be all the nodes which are dominated by the upper site-segmentsite but not theislowerexcisedsitefrom.
"Imagine, andthatthatthe and 2 are fused into a single elementary tree 1 ."
"Now we can simulate the multi-component adjunction by ordinary adjunction: adjoin the replacefused 1 andby adjoining 2 into whatit betweenis left of 1 and; then 2 . indeThenitelyreplacement: some ofother can(fusedbe)postponedtree set f 1 0 ; 2 0 g can adjoin between 1 and 2 , and pair of trees."
"This willadjoinsproducebetweenthe samethe lastre-so on, and then sultMoreas aformallyseries of: set-local adjunctions. 1."
Fusegrammarall theby elementaryidentifying treethe uppersets offootthe i . with the lower root.
Designate this fused node the meta-foot . 2.
"Forbinationeach treeof site-segments, and for every, excisepossibleallcom-the site-segments and add all the trees thus produced (the excised auxiliary trees and the remainders) to the grammar. toNowpiecesthat, weourmustgrammarmake surehas thatbeen thesmashedright pieces go back in the right places."
"We could do this using features, but the resulting grammar would only be strongly equivalent, not deriva-tionally equivalent, to the original."
Therefore we use a meta-level grammar instead: 1.
"Forsibleeachcombinationinitial treeof, site-segmentsand for every, pos-con-struct the derivation tree that will re-assemble the pieces created in step (2) above and add it to the meta-level gram-mar. 2."
"Forsibleeachcombinationauxiliary treeof site-segments, and for every, con-pos-struct a derivation tree as above, and for the node which corresponds to the piece containing the meta-foot, add a child, la-bel its arc with the meta-foot&apos;s address (within the piece), and mark it a foot node."
Add the resulting (meta-level) aux-iliary tree to the meta-level grammar. spondsObserveto meta-levelthat set-localadjunctionadjunctionalongcorre-the (meta-level) spine.
Recall that we restricted set-local adjunction so that a tree set can only adjoin at the foot of the upper tree and the root of the lower tree.
"Since this pair of nodesrestatecorrespondsour restrictionto thein meta-footterms of ,thewecon-can verted grammar: no meta-level adjunction is allowed along the spine of a (meta-level) aux-iliary tree except at the (meta-level) foot. adjunctionThen all inmeta-levelthe senseadjunctionof (Rogersis,regular1994)."
"Therefore this converted 2LTAG produces derivation tree sets which are recognizable, and therefore our formalism is strongly equiv-alent to TAG. thanNoteRogers&apos;that thisregularrestrictionform isrestrictionmuch stronger."
This was done for two reasons.
"First, the de ni-tion of our restriction would have been more complicated otherwise; second, this restric- tion overcomes some computational ties with RF-TAG which we discuss below."
"In cases where TAG models dependencies cor-rectly, the use of R-MCTAG is straightfor-ward: when an auxiliary tree adjoins at a site pair which is just a single node, it looks just like conventional adjunction."
"However, in problematic cases we can use the extra expres-sive power of R-MCTAG to model dependen-cies correctly."
Two such cases are discussed below.
"tainConsiderboth bridgethe caseandof sentencesraising verbswhich, notedcon-[REF_CITE]."
"In most TAG-based analyses, bridge verbs adjoin at S (or C 0 ), and raising verbs adjoin at VP (or I 0 )."
"Thus the derivation for a sentence like (1) John thinks that Mary seems to sleep. will have the trees for thinks and seems si-multaneously adjoining into the tree for like , which, when interpreted, gives an incorrect dependencyBut understructurethe present. view we can ana-lyze sentences like (1) with derivations mir-roring dependencies."
The desired trees for (1) are shown in Figure 7.
"Since the tree for that seems can meta-adjoin around the subject, thetreetreefor seems for thinks rathercorrectlythan eat adjoins. into the theAlsocorrect, althoughdependencythe abovelinksanalysis, the directionsproduces arevantageinvertedcomparedin someto, forcasesexample."
"This ,isDSGa disad-; but since the directions are consistently inverted, for applications like translation or statistical modeling, the particular choice of direction is usually immaterial."
"Tree-local MCTAG is able to derive (2a), but unable to derive (2b) except by adjoining the auxiliary tree for to be likely at the foot of the auxiliary tree for seem[REF_CITE]. (2) a. Does John seem to sleep? b. Does John seem to be likely to sleep?"
"The derivation structure of this analysis does not match the dependencies, however| seem adjoins into to sleep . tionDSGmatchingcan derivethethisdependenciessentence with, buta itderiva-loses some of the advantage of TAG in that, for example, cases of super-raising (where the verb is raised out of two clauses) must be ex-plicitly ruled out by subsertion-insertion con-straints."
"It turns out that the anal-ysis of raising from the previous section, de-signed for a translation problem, has both of these properties as well."
The grammar is shown back in Figure 4.
Figure 8 shows a CKY-style parser for our restriction of MCTAG as a system of inference rules.
"It is limited to grammars whose trees are at most binary-branching. oneTheof theparserfollowingconsistsformsof ,ruleswhereover w items w ofis the input; , h , and l specify nodes 1 of n the grammar; i, j, k, and l are integers between 0 and n inclusive; and code is either + or , : [ ; code ; i; , ; , ; l; , ; , ] [ ; code ; i; j; k; l; , ; , ] function as andin a CKY-style parser for standard TAG rooted by(Vijay-Shanker 2 , T 1987derives): athetreesubtreewhose fringe w is Fww i w l if T is initial, or auxiliary j tree k of w a l setifand T F is isthethelowerlabel w i of its foot node."
"In all four item forms, code = + i adjunction has taken place at . [ ; code ; i; j; k; l; , ; l ] speci es that the fringe is w i segment h ; l i w derives Lw w a , treewherewhose L is the label of l ."
"Intuitively j k , it l means that a potential site-segment has been recog-nized. [ ; code ; i; j; k; l; h ; l ] speci es, if longs to the upper tree of a set, thatbe-the subtree rooted by , the segment h h ; l i , and the lower tree concatenated w i together w"
"Fw derive w a ,treewherewhose F isfringethe la-is bel of the j lower k foot l node."
"Intuitively, it meansrecognizedthat, witha treeasetsite-segmenthas been partiallyinserted between the two components. parserThe andruleshencewhichexplanationrequire di areer fromPseudopoda"
"TAG, Push, Pop, and Pop-push."
"Pseudopod applies to any potential lower adjunction site and is so called because the parser essentially views everytree (seepotentialSectionsite-segment2.[Footnote_3]), and theasPseudopodan auxiliaryax-iom recognizes the feet of these false auxiliary treesThe."
3 Thanks to Anoop Sarkar for pointing out the rst
"Push rule performs the adjunction of oneplacesof athesesite-segmentfalse auxiliarybetweentreesthe|twothattreesis, ofit an elementary tree set."
It is so called because the site-segment is saved in a \stack&quot; so that the rest of its elementary tree can be recog-nized later.
"Of course, in our case the \stack&quot; hasTheat mostPop ruleone doeselementthe. reverse: every com-pleted elementary tree set must contain a site-segment, and the Pop rule places it back where the site-segment came from, emptying the \stack.&quot; The Pop-push rule performs set-local adjunction: a completed elementary tree set is placed between the two trees of yet an-other elementary tree set, and the \stack&quot; is unchanged. pensivePop-pushrule;issincecomputationallyit involves sixtheindicesmostandex-three di erent elementary trees, its running timeIt wasis O (noted n 6 G 3 )in.[REF_CITE]that for synchronous RF-2LTAG, parse forests could not be transferred in time O ( n 6 )."
This fact turns out to be connected to several prop-erties of RF-TAG[REF_CITE]. 3
"TAGThedescribedCKY-stylein (parserRogers,for1994regular) essentiallyform keeps track of adjunctions using stacks, and the regular form constraint ensures that the stack depth is bounded."
"The only kinds of ad-junction that can occur to arbitrary depth are root and foot adjunction, which are treated similarly to substitution and do not a ect the stacks."
"The reader will note that our parser works in exactly the same way. andAfootproblemadjunctionarises, howeverif we allow."
It is well-knownboth root that allowing both types of adjunction creates derivational ambiguity[REF_CITE]: adjoining 1 at the foot of 2 produces the same derived tree that adjoining 1 at the root of 2 would.
"The problem is not the am-biguity per se , but that the regular form TAG parser, unlike a standard TAG parser, does not always distinguish these multiple deriva-tions, because root and foot adjunction are both performed by the same rule (analogous to our Pop-push)."
"Thus for a given application of this rule, it is not possible to say which tree is adjoining into which without examining the rest of the derivation. formButcertainthis knowledgetasks onlineis: fornecessaryexample,toenforc-per-ing adjoining constraints, computing proba-bilities (and pruning based on them), or per-forming synchronous mappings."
Therefore we arbitrarily forbid one of the two possibilities. [Footnote_4] The parser given in Section 4 already takes this into account.
"4 Against tradition, we forbid root adjunction, be-cause adjunction at the foot ensures that a bottom-up traversal of the derived tree will encounter elementary trees in the same order as they appear in a bottom-up traversal of the derivation tree, simplifying the calcu-lation of derivations."
Our version of MCTAG follows other work in incorporating dependency into a constituency-based approach to modeling natural language.
"One such early integra-tion involved work[REF_CITE], which showed that projective dependency grammars could be represented by CFGs."
"However, it is known that there are common phenom-ena which require non-projective dependency grammars, so looking only at projective de-such connection. pendency grammars is inadequate."
"Follow-ing the observation of TAG derivations&apos; sim-ilarity to dependency relations, other for-malisms have also looked at relating depen-dency and constituency approaches to gram-mar formalisms. tutionA moreGrammarsrecent instance(DSG) (isRambowD-Tree Substi-et al., 1995), where the derivations are also inter-preted as dependency relations."
"Thought of in the terms of this paper, there is a clear parallel with R-MCTAG, with a local set ultimately representing dependencies having some yield function applied to it; the idea of non-immediate dominance also appears in both formalisms."
The di erence between the two is in the kinds of languages that they are restrictiveable to describethan :R-MCTAGDSG is both.
"DSGlesscanandgener-more ate the language count - k for some arbitrary k (that is, f a 1n a 2n :::a kn g ), which makes it extremely powerful, whereas R-MCTAG cancannotonlygenerategeneratethe count copy -4language."
"However(that, DSGis, f ww j w 2 g with some terminal al- phabet), whereas R-MCTAG can; this may be problematic for a formalism modeling nat-ural language, given the key role of the copy language in demonstrating that natural lan-guage is not context-free[REF_CITE]."
R-MCTAG is thus a more constrained relaxation of the notion of immediate dominance in fa-vor of non-immediate dominance than is the case for DSG. hereAnotheris the Segmentedformalism Adjoiningof particularGrammarinterest[REF_CITE].
"This generalization of TAG is characterized by an extension of the adjoining operation, motivated by evidence in scram-bling, clitic climbing and subject-to-subject raising."
"Most interestingly, this extension to TAGned,byproposeda compositionon empiricaloperationgroundswith, iscon-de-strained non-immediate dominance links that looks quite similar to the formalism described in this paper, which began from formal con-siderations and was then applied to data."
This con uence suggests that the ideas described here concerning combining dependency and constituency might be reaching towards some deeper connection.
"From a theoretical perspective, extracting more derivational generative capacity and stituencythereby integratinginto a commondependencyframeworkandis ancon-in-teresting exercise."
"It also, however, proves to be useful in modeling otherwise problematic constructions, such as subject-auxiliary inver-sion and bridge and raising verb interleaving."
"Moreoveroretical considerations, the formalism, presenteddevelopedinfromthisthe-pa-per, has similar properties to work developed on empirical grounds, suggesting that this is worth further exploration."
"We discuss the advantages of lexical-ized tree-adjoining grammar as an al-ternative to lexicalized PCFG for sta-tistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its pars-ing performance."
"We nd that this in-duction method is an improvement over the EM-based method[REF_CITE], and that the induced model yields re-sults comparable to lexicalized PCFG."
Why use tree-adjoining grammar for statisti-cal parsing?
"Given that statistical natural lan-guage processing is concerned with the proba-ble rather than the possible, it is not because TAG can describe constructions like arbitrar-ily large Dutch verb clusters."
"Rather, what makes TAG useful for statistical parsing are the structural descriptions it assigns to bread-and-butterThe approachsentencesof Chelba. and[REF_CITE]to language modeling is illustrative: even though the probability estimate of w appear-ing as the k th word can be conditioned on the entire history w 1 ;::: ;w k , 1 , the quantity of availabletext to abouttrainingtwodatawordslimits|butthewhichusabletwocon-?"
"A trigram model chooses w k , 1 and w k , 2 and works quite well; a model which chose w k , 7 and w k , 11 would probably work less well."
"But[REF_CITE]chooses the lexical heads of the two previous constituents as de-termined by a shift-reduce parser, and works better than a trigram model."
"Thus the (vir-tual) grammar serves to structure the history so that the two most useful words can be cho- sen, even though the structure of the problem itself is entirely linear. lemSimilarlyrequires, nothingthat weaboutconstructthe parsingany struc-prob-ture other than phrase structure."
But be-ginning[REF_CITE]statistical parsers have used bilexical dependencies with great success.
"Since these dependencies are not encoded in plain phrase-structure trees, the standard approach has been to let the lex-ical heads percolate up the tree, so that when one lexical head is immediately dominated by another, it is understood to be dependent on it."
"E ectively, a dependency structure is made parasitic on the phrase structure so that they can be generated together by a context-free model. fromHowevercases,wherethis solutioncontext-freeis notderivationsideal."
"Asideare incapable of encoding both constituency and dependency (which are somewhat isolated and not of great interest for statistical pars-ing) there are common cases where percola-tion of single heads is not  to encode dependencies correctly|for example, relative clause attachment or raising/auxiliary verbs (see Section 3)."
More complicated grammar transformations are necessary. a grammarA more suitableformalismapproachwhich producesis to employstruc-tural descriptions that can encode both con-stituency and dependency.
"Lexicalized TAG is such a formalism, because it assigns to each sentence not only a parse tree, which is built out of elementary trees and is inter-preted as encoding constituency, but a deriva-tion tree , which records how the various el-ementary trees were combined together and is commonly intepreted as encoding depen-dency."
The ability of probabilistic LTAG to model bilexical dependencies was noted early on[REF_CITE]. contextualIt turns outinformationthat therethatareneedothertopiecesbe ex-of plicitly accounted for in a CFG by gram-mar transformations but come for free in a TAG.
We discuss a few such cases in Sec-tion 3.
In Sections 4 and 5 we describe an experiment to test the parsing accuracy of a probabilistic TAG extracted automati-cally from the Penn Treebank.
"We nd that the automatically-extracted grammar gives an improvement over the EM-based induction method[REF_CITE], and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for im-provement. becauseWe emphasizeit can dothatthingsTAGthatis attractiveCFG cannotnot, but because it does everything that CFG can, only more cleanly. (This is where the anal-ogy[REF_CITE]breaks down.)"
Thus certain possibilities which were not apparent in a PCFG framework or pro-hibitively complicated might become simple to implement in a PTAG framework; we con-clude by o ering two such possibilities.
"The formalism we use is a variant of lexical-ized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG (Schabes and"
"In this variant there are three kinds of elementary tree: initial, (predicative) auxiliary, and modi er, and three composi-tion operations: substitution, adjunction, and sister-adjunction. strictedAuxiliaryas intreesTIG:andessentiallyadjunction, no wrappingare re-adjunction or anything equivalent to wrap-ping adjunction is allowed."
"Sister-adjunction is not an operation found in standard de ni-tions of TAG, but is borrowed from D-Tree Grammar[REF_CITE]."
In sister-adjunction the root of a modi er tree is added as a new daughter to any other node. (Note thatpletelyasunconstrainedit stands sister-adjunction; it will be constrainedis com-by the probability model.)
We introduce this operation simply so we can derive the at structures found in the Penn Treebank.
"Fol-lowing[REF_CITE], multiple modi er trees can be sister-adjoined at a sin-gle site, but only one auxiliary tree may be adjoined at a single node. theFigurederivation[Footnote_1] showsof thean examplesentence grammar\John shouldand leave tomorrow.&quot; The derivation tree encodes this process, with each arc corresponding to a composition operation."
"1 A Gorn address is a list of integers: the root of a tree has address , and the j th child of the node with"
Arcs corresponding to substitution and adjunction are labeled with the Gorn address 1 of the substitution or ad- junction site.
"An arc corresponding to the sister-adjunction of a tree between the i th and i + 1th children of (allowing for two imagi-nary children beyond the leftmost and right-most children) is labeled ; i . byThisthe grammarparser, is,lexicalizedas well as thein grammarthe sense usedthat every elementary tree has exactly one termi-nal node, its lexical anchor . bySinceordinarysister-adjunctionadjunction, thiscanvariantbe simulatedis, like TIG (and CFG), weakly context-free and O ( n 3 )-time parsable."
"Rather than coin a new acronym for this particular variant, we will simply refer to it as \TAG&quot; and trust that no confusion will arise. ([REF_CITE]; Schabesof ,a1992 X probabilistic) are: TAG X P s ( P i ( ) = 1 j ) = 1 X P a ( j ) +"
"P a ( NONE j ) = 1 where ranges over initial trees, over aux-iliary trees, over modi er trees, and over anodesderivation."
"P i ( ) withis the ;probability P s ( of beginning j ) is the prob-abilitythe probabilityof substitutingof adjoiningat ; P at a ( ; nally, j ) is adjoining P a ( NONE at .[REF_CITE]sug-j ) is the probability of nothing gest other parameterizations worth exploring as well."
X Our P sa variant( adds another set of parameters: j ; i; f ) +
"P sa ( STOP j ; i; f ) = 1 This is the probability of sister-adjoining between the i th and i + 1th children of (as before, allowing for two imaginary children beyond the leftmost and rightmost children)."
"Since multiple modi er trees can adjoin at the sameag f locationwhich ,indicates P sa ( ) iswhetheralso conditionedis the onrsta modi er tree (i.e., the one closest to the head) to adjoin at that location. the individual operations of the derivation."
Thus the probability of the example deriva-tion of Figure 1 would be
P i ( 2 ) P a ( NONE j 2 ( ))
P s ( 1 j 2 (1))
P a ( j 2 (2))
P sa ( j 2 (2) ; 1 ; true )
P sa ( STOP j 2 (2) ; 1 ; false )
"P sa ( STOP j 2 ( ) ; 0 ; true ) : : : where ( i ) is the node of with address i . estimateWe wantof theseto obtainparametersa maximum-likelihood, but cannot es-timate them directly from the Treebank, be-cause the sample space of PTAG is the space of TAG derivations, not the derived trees that are found in the Treebank."
"One approach, taken[REF_CITE], is to choose some gram-mar general enough to parse the whole corpus and obtain a maximum-likelihood estimate by EM."
"Another approach, taken[REF_CITE]and others for lexicalized PCFGs and[REF_CITE]for LTAGs, is to use heuristics to reconstruct the derivations, and directly es-timate the PTAG parameters from the recon-structed derivations."
"We take this approach as well. (One could imagine combining the two approaches, using heuristics to extract a grammar but EM to estimate its parameters.)"
"In a lexicalized TAG, because each compo-sition brings together two lexical items, ev-ery composition probability involves a bilex-ical dependency."
"Given a CFG and head-percolation scheme, an equivalent TAG can be constructed whose derivations mirror the dependency analysis implicit in the head-percolationFurthermorescheme, there. are some dependency analyses encodable by TAGs that are not en-codable by a simple head-percolation scheme."
"For example, for the sentence \John should have left,&quot; Magerman&apos;s rules make should and have the heads of their respective VPs, so that there is no dependency between left and its subject John (see Figure 2a)."
"Since nearly a quarter of nonempty subjects appear in such a con guration, this is not a small problem. (We could make VP the head of VP instead, but this would generate auxiliaries indepen-dently of each other, so that, for example, P (John leave) &gt; 0.) (b)TAGeasilycan, usingproducethe thegrammardesiredofdependenciesFigure 1."
"A more complex lexicalization scheme for CFG could as well (one which kept track of two heads at a time, for example), but the TAG account is simpler and cleaner. nonlocalBilexicaldependenciesdependenciesthatarecannotbetheusedonlyto improve parsing accuracy."
"For example, the attachment of an S depends on the presence or absence of the embedded subject[REF_CITE]; Treebank-style two-level NPs are mis-modeled by PCFG[REF_CITE]; the generation of a node depends on the label of its grandparent[REF_CITE]."
"In order to capture such dependencies in a PCFG-based model, they must be localized either by transforming the data or modifying the parser."
"Such changes are not always obvious a priori and often must be devised anew for each language or each corpus. specialBut nonetreatmentof thesein acasesPTAGreallymodelrequires, be-cause each composition probability involves not only a bilexical dependency but a \biarbo-real&quot; (tree-tree) dependency."
"That is, PTAG generates an entire elementary tree at once, conditioned on the entire elementary tree be-ing modi ed."
Thus dependencies that have to be stipulated in a PCFG by tree transforma-tions or parser modi cations are captured for free in a PTAG model.
"Of course, the price that the PTAG model pays is sparser data; the backo model must therefore be chosen carefully."
We want to extract from the Penn Tree-bank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of ([REF_CITE]; classify exactly one child[REF_CITE]).
"For each node as, atheseheadrulesand the rest as either arguments or adjuncts."
Us-ing this classi cation we can construct a TAG derivation (including elementary trees) from a derived tree as follows: 1.
"Ifrootedisatan adjunctto form,aexcisemodi erthetreesubtree. 2."
"Ifrootedis atan argumentto form an, initialexcise treethe,subtreeleaving behind a substitution node. 3."
"Ifgumenthaswitha rightthecornersame labelwhichas is(andan ar-all segment fromintervening nodesdownare headsto )to, exciseformthean auxiliary tree. sultRules; rule(1()3)andchanges(2) producethe analysisthe desiredsomewhatre-by making subtrees with recursive arguments into predicative auxiliary trees."
"It produces, among other things, the analysis of auxiliary verbs described in the previous section."
"It is applied in a greedy fashion, with potential s considered top-down and potential s bottom-up."
The complicated restrictions on are sim-ply to ensure that a well-formed TIG deriva-tion is produced.
"Now that we have augmented the training data to include TAG derivations, we could try to directly estimate the parameters of the model from Section 2."
"But since the number of (tree, site) pairs is very high, the data would be too sparse."
"We therefore generate an ele-mentary tree in two steps: rst the tree tem-plate (that is, the elementary tree minus its anchor), then the anchor."
"The probabilities 



 are decomposed as follows:"
P s ( P i ( ) = P i ( ) P i (
"P sa 2 ( w j ; t ; w ; f ) part-of-speechis thetagtreeoftemplatethe anchorof , and, t is w theis where the anchor itself. backoThe generationlevels: at ofthethe treerst leveltemplate, the hasanchortwo of POSistagignoredof the, anchorand at theas wellsecondas thelevel,agthe f are ignored."
"The generation of the anchor has three backo levels: the rst two are as before, and the third just conditions the anchor on its POS tag."
"The backed-o models are combined by linear interpolation, with the weights cho-sen as[REF_CITE]."
We ran the algorithm given in Section 4.1 on sections 02{21 of the Penn Treebank.
"The ex-tracted grammar is large (about 73,000 trees, with words seen fewer than four times re-placed with the symbol *UNKNOWN* ), but if we consider elementary tree templates, the gram-mar is quite manageable: 3626 tree templates, of which 2039 occur more than once (see Fig-ure 4). account[REF_CITE]% offrequenttree-templatetree-templatetokens intypesthe training data."
Removing all but these trees from the grammar increased the error rate by about 5% (testing on a subset of section 00).
"A few of the most frequent tree-templates are shown in Figure 3. pactSo, butthe extractedhow completegrammaris it? isIf wefairlyplotcom-the growth of the grammar during training (Fig-ure 5), it&apos;s not clear the grammar will ever converge, even though the very idea of a grammar requires it."
Three possible explana-tions are:
New constructions continue to appear.
Oldneouslyconstructions) annotatedcontinuein new waysto be. (erro-
"Old constructions continue to be com-bined in new ways, and the extraction heuristics fail to factor this variation out. mentaryIn a randomtree templatessample,ofwe100foundonce-seen(by casualele-inspection) that 34 resulted from annotation errors, 50 from de ciencies in the heuristics, and four apparently from performance errors."
Only twelve appeared to be genuine. grammarThereforeis nottheascontinuedrapid as Figuregrowth 5ofmightthe indicate.
"Moreover, our extraction heuristics ityevidentlyof treeshaveresultingroom tofromimprovede ciencies."
"The major-in the heuristics involved complicated coordination structures, which is not surprising, since co-ordination has always been problematic for TAG. convergeTo see iswhat, we ranthetheimpactgrammarof thisextractorfailure onto some held-out data (section 00)."
This amounts to about one unseen tree template every 20 sen-tences.
"When we consider lexicalized trees, this gure of course rises: out of the same 450824%, hadtreenottokensbeen ,seen1828inlexicalizedtraining. trees, or goodSo."
"Notethe coveragethat evenofinthecasesgrammarwhere theisparserquite encounters a sentence for which the (fallible) extraction heuristics would have produced an unseen tree template, it is possible that the parser will use other trees to produce the cor-rect bracketing."
"We used a CKY-style parser similar to the one described[REF_CITE], with a modi cation to ensure completeness (be-cause foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions."
"We also extended the parser to simulate sister-adjunction as regular ad-junction and compute the ag f which dis-tinguishes the rst modi er from subsequent modi ers. of Wean useitema beam[ ; i; j search] by multiplying, computingitthebyscorethe prior probability P ( )[REF_CITE]; any item with score less than 10 , 5 times that of the best item in a cell is pruned. ringFollowingfewer than(Collinsfour,times1997)in, wordstrainingoccur-were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described[REF_CITE]."
"Tree templates occurring only once in training were ignored entirely. 1998We): wersttrainedcomparedthe themodelparseron sentenceswith (Hwaof, length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23, parsing from part-of-speech tag se-quences to fully bracketed parses."
The metric used was the percentage of guessed brackets which did not cross any correct brackets.
"Our parser scored 84.4% compared with 82.4%[REF_CITE], an error reduction of 11%. icalizedNext wePCFGcomparedparsersour, trainingparser againston sectionslex- 02{21 and testing on section 23."
The results are shown in Figure 6. theThesemiddleresultsof theplacelexicalizedour parserPCFGroughlyparsersin.
"While the results are not state-of-the-art, they do demonstrate the viability of TAG as a framework for statistical parsing."
"With 



 improvements in smoothing and cleaner han-dling of punctuation and coordination, per-haps these results can be brought more up-to-date."
They present a more detailed discussion of various grammar extraction pro-cesses and the performance of supertagging models (B.[REF_CITE]) based on the ex-tracted grammars.
"They do not report parsing results, though their intention is to evaluate how the various grammars a ect parsing ac-curacy and how k -best supertagging a fects parsing speed. 1997Srinivas&apos;s) also usesworkTAGon supertagsfor statistical(B. Srinivasparsing,, but with a rather di erent strategy: tree tem-plates are thought of as extended parts-of-speechon local, and(e.gthese., n -gramare )assignedcontextto. words based tiesAsmadefor futureavailableworkby, thereTAGarewhichstillremainpossibili-to be explored."
"One, also suggested[REF_CITE], is to group elemen-tary trees into families and relate the trees of a family by transformations."
"For example, one would imagine that the distribution of active verbs and their subjects would be similar to the distribution of passive verbs and their no-tional subjects, yet they are treated as inde-pendentgurationsin couldthe currentbe relatedmodel, then."
If thethetwosparse-con-ness of verb-argument dependencies would be reduced. anchoredAnothertreespossibility.
"Nothingisaboutthe usePTAGof multiply-requires that elementary trees have only a single an-chor (or any anchor at all), so multiply-anchored trees could be used to make, for example, the attachment of a PP dependent not only on the preposition (as in the cur-rent model) but the lexical head of the prepo-sitional object as well, or the attachment of a relative clause dependent on the embed-ded verb as well as the relative pronoun."
"The smoothing method described above would have to be modi ed to account for multiple anchors. videsIn summarya cleaner, wewayhaveofarguedlookingthatatTAGstatisti-pro-cal parsing than lexicalized PCFG does, and demonstrated that in practice it performs in the same range."
"Moreover, the greater ex-ibility of TAG suggests some potential im-provements which would be cumbersome to implement using a lexicalized CFG."
Further research will show whether these advantages turn out to be signi cant in practice.
"¶ @¦¨§R§ %@% ¬t­_ ª¯¤µ´%¬®¶m°¨¦±·j¬ .¶»£ ¶f´%  ¦ÇÆ_&lt;­¶m¥¯¬®¼È¥¯ª¯¤oÉ%£ ¶m¥¯¹ .Ë{¤m´!°±¬²¦¨Éj¶m° É%£¤E¦¨É¬m¾ÍÌ,¬¹  ¦±ÎÏÉj¸ °±ªÑÐU¤mËt¤m­  ¦¨§© ¶¥¯¬®§ &lt; %¶m°Ê¦±·j¬     &lt;­O¶m¥¯¬®¼. %£ ¦¨¬jÃm¬ .¶"
Ôy¤m´r¼o¹&lt;%@Ó ´%¬jÒ   Ãm¬j´  ¬Ö­O¶m¥¯¬® °¨¦¨§ ¬m¾ Þ ß :
C9 :¡o=-BÖå@  É¬ÖÒO°¨¶m§O§$ Éj¶m§Õ­¦¨§©¦¨§_¬cÉ@%§£O¶f´%¶mÉª¯¬j´%¶m°-°¨¶m§%©E¸ ¶f©m¬©m¬®§¬j´¯¹¬tÉ¤E°Ç¹
The paper proposes an information-theory-based method for feature types analysis in probabilistic evaluation modelling for statistical parsing.
The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction.
Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions.
"In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]]."
How to evaluate the different feature types’ effects for syntactic parsing?
"The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure."
"In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-theory-based feature type analysis model ; Section 4 introduces several experimental issues; Section 5 quantitatively analyses the different contextual feature types or feature types combination in the view of information theory and draws a series of conclusion on their predictive powers for syntactic structures."
"Given a sentence, the task of statistical syntactic parsing is to assign a probability to each candidate parsing tree that conforms to the grammar and select the one with highest probability as the final analysis result."
"T best = arg max P(T | S) (1) T where S denotes the given sentence, T denotes the set of all the candidate parsing trees that conform to the grammar, P(T|S) denotes the probability of parsing tree T for the given sentence S."
The task of probabilistic evaluation model in syntactic parsing is the estimation of P(T|S).
"In the syntactic parsing model which uses rule-based grammar, the probability of a parsing tree can be defined as the probability of the derivation which generates the current parsing tree for the given sentence."
P(T | S) =
"P(r 1 , r 2 , , r n | S) n = ∏"
"P(r i | r 1 , r 2 , , r i−1 , S) (2) i=1 n = ∏"
"P(r i | h i , S) i=1"
"Where, r 1 ,r 2 , ,r i−1 denotes a derivation rule sequence, h i denotes the partial parsing tree derived from r 1 ,r 2 , ,r i−1 ."
"In order to accurately estimate the parameters, we need to select some feature types F 1 , F 2 , , F m , depending on which we can divide the contextual condition h i , S for predicting rule r i into some equivalence classes, that is, h i , S  F 1 ,F  2 ,  ,F  m →[h i , S] , so that n n ∏"
"P(r i | h i ,S) ≈ ∏ P(r i | [h i , S]) ([Footnote_3]) i=1 i=1"
3 The information-theory-based feature type analysis model for statistical syntactic parsing
"According to the equation of (2) and (3), we have the following equation: n P(T | S) ≈ ∏ P(r i | [h i , S]) (4) i=1"
"In this way, we can get a unite expression of probabilistic evaluation model for statistical syntactic parsing."
The difference among the different parsing models lies mainly in that they use different feature types or feature type combination to divide the contextual condition into equivalent classes.
Our ultimate aim is to determine which combination of feature types is optimal for the probabilistic evaluation model of statistical syntactic parsing.
"Unfortunately, the state of knowledge in this regard is very limited."
"Many probabilistic evaluation models have been published inspired by one or more of these feature types [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]] [[REF_CITE]], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively."
"In the paper, we propose an information-theory-based feature type analysis model by which we can quantitatively analyse the predictive power of different feature types or feature type combinations for syntactic structure in a systematic way."
The conclusion is expected to provide reliable reference for feature type selection in the probabilistic evaluation modelling for statistical syntactic parsing.
"In the prediction of stochastic events, entropy and conditional entropy can be used to evaluate the predictive power of different feature types."
"To predict a stochastic event, if the entropy of the event is much larger than its conditional entropy on condition that a feature type is known, it indicates that the feature type grasps some of the important information for the predicted event."
"According to the above idea, we build the information-theory-based feature type analysis model, which is composed of four concepts: predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation."
"PIQ(F;R) , the predictive information quantity of feature type F to predict derivation rule R, is defined as the difference between the entropy of R and the conditional entropy of R on condition that the feature type F is known."
PIQ(F; R) =
"H (R) − H(R | F) = ∑ P( f , r) log P( f , r) f∈F,r∈R P( f )⋅ P(r) (5)"
Predictive information quantity can be used to measure the predictive power of a feature type in feature type analysis.
"For the prediction of rule R, PIG(F x ;R|F 1 ,F 2 ,...,F i ), the predictive information gain of taking F x as a variant model on top of a baseline model employing F 1 ,F 2 ,...,F i as feature type combination, is defined as the difference between the conditional entropy of predicting R based on feature type combination F 1 ,F 2 ,...,F i and the conditional entropy of predicting R based on feature type combination F 1 ,F 2 ,...,F i ,F x ."
"PIG(F x ;R | F 1 , ,F i ) ="
"H(R | F 1 , ,F i ) − H(R | F 1 , ,F i ,F x ) = ∑ P( f 1 , , f i , f x ,r)log PP((f 1 f, , f i , f x ,r) ⋅ P( f 1 , , f i ) (6) 1 , , f i , f x ) P( f 1 , , f i ,r) f 1 ∈F 1 f i ∈F i f x ∈F x r∈R"
"If PIG(F x ;R | F 1 ,F 2 , ,F i ) &gt; PIG(F y ;R | F 1 ,F 2 , ,F i ) , then F x is deemed to be more informative than F y for predicting R on top of F 1 ,F 2 ,...,F i , no matter whether PIQ(F x ;R) is larger than PIQ(F y ;R) or not. z Predictive Information Redundancy(PIR)"
"Based on the above two definitions, we can further draw the definition of predictive information redundancy as follows."
"PIR(F x ,{F 1 ,F 2 ,...,F i };R) denotes the redundant information between feature type F x and feature type combination {F 1 ,F 2 ,...,F i } in predicting R, which is defined as the difference between"
"PIQ(F x ;R) and PIG(F x ;R|F 1 ,F 2 ,...,F i )."
"That is, PIR(F x ,{F 1 , F 2 , , F i }; R) = PIQ(F x ; R) − PIG(F x ; R | F 1 , F 2 , , F i ) (7)"
Predictive information redundancy can be used as a measure of the redundancy between the predictive information of a feature type and that of a feature type combination.
"PIS(F 1 ,F 2 ,...,F m ;R), the predictive information summation of feature type combination F 1 ,F 2 ,...,F m , is defined as the total information that F 1 ,F 2 ,...,F m can provide for the prediction of a derivation rule."
"PIS(F 1 , F 2 , , F m ; R) = PIQ(F 1 ; R) + ∑ PIG(F i ; R | F 1 , , F i−1 ) (8) m i=2"
The predicted event of our experiment is the derivation rule to extend the current non-terminal node.
"The feature types for prediction can be classified into two classes, history feature types and objective feature types."
"In the following, we will take the parsing tree shown in Figure-1 as the example to explain the classification of the feature types."
"In Figure-1, the current predicted event is the derivation rule to extend the framed non-terminal node VP, the part connected by the solid line belongs to history feature types, which is the already derived partial parsing tree, representing the structural environment of the current non-terminal node."
"The part framed by the larger rectangle belongs to the objective feature types, which is the word sequence containing the leaf nodes of the partial parsing tree rooted by the current node, representing the final objectives to be derived from the current node."
The experimental corpus is derived from Penn TreeBank[[REF_CITE]].
"We semi-automatically assign a headword and a POS tag to each non-terminal node. 80% of the corpus (979,767 words) is taken as the training set, used for estimating the various co-occurrence probabilities, 10% of the corpus (133,814 words) is taken as the testing set, used to calculate predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation."
"The grammar rule set is composed of 8,126 CFG rules extracted from Penn TreeBank."
"In the information-theory-based feature type analysis model, we need to estimate joint probability P( f 1 , f 2 , , f i , r) ."
"Let F 1 ,F 2 ,...,F i be the feature type series selected till now, f 1 ∈ F 1 , f 2 ∈ F 2 , , f i ∈ F i , r ∈ R , we use a ~ blended toprobability P( f 1 , f 2 , , f i , r) approximate probability P( f 1 , f 2 , , f i , r) in order to solve the sparse data problem[[REF_CITE]]. c(r)"
P 0 (r) = ∑ c(rˆ) (11) rˆ∈R where c(r) is the total number of time that r has been seen in the corpus.
"According to the escape mechanism in [[REF_CITE]], we define the weights w k (−1&lt; k ≤i) in the formula (9) as follows. i w k = (1− e k ) ∏ e s , −1 ≤ k ≤ i (12) s=k+1 w i = 1− e i where e k denotes the escape probability of context ( f 1 , f 2 , , f k ) , that is, the probability in which (f 1 , f 2 , ... , f k , r) is unseen in the corpus."
"In such case, the blending model has to escape to the lower contexts to approximate P( f 1 , f 2 , , f k , r) ."
"Exactly, escape probability is defined as  ∑ d( f 1 , f 2 ,..., f k , rˆ)  rˆ∈R e k =  ∑ c( f 1 , f 2 ,..., f k , rˆ) , 0 ≤ k ≤ i (13)  rˆ∈R 0 , k = −1 where d( f 1 , f 2 ,..., f k ,rˆ) = 1, if c( f 1 , f 2 ,..., f k ,rˆ) &gt; 0 (14) 0, if c( f 1 , f 2 ,..., f k ,rˆ) = 0"
"In the above blending model, a special 1 probability P −1 (r) = ∑ c(rˆ) is used, where all rˆ∈R derivation rules are given an equal probability."
"As a result, P~( f 1 , f 2 , , f i ,r) &gt; 0 as long as ∑ c(rˆ) &gt; 0 . rˆ∈R"
"The experiments led to a number of interesting conclusions on the predictive power of various feature types and feature type combinations, which is expected to provide reliable reference for the modelling of probabilistic parsing."
"information quantities of lexical feature types, part-of-speech feature types and constituent label feature types z Goal"
One of the most important variation in statistical parsing over the last few years is that statistical lexical information is incorporated into the probabilistic evaluation model.
Some statistical parsing systems show that the performance is improved after the lexical information is added.
"Our research aims at a quantitative analysis of the differences among the predictive information quantities provided by the lexical feature types, part-of-speech feature types and constituent label feature types from the view of information theory."
The experiment is conducted on the history feature types of the nodes whose structural distance to the current node is within 2.
"In Table-1, “Y” in PIQ(X of Y; R) represents the node, “X” represents the constitute label, the headword or POS of the headword of the node."
"In the following, the units of PIQ are bits."
"Among the feature types in the same structural position of the parsing tree, the predictive information quantity of lexical feature type is larger than that of part-of-speech feature type, and the predictive information quantity of part-of-speech feature type is larger than that of the constituent label feature type."
Y= the first left brother of the parent 5.2
The analysis to the influence of the structural relation and the structural distance to the predictive information quantities of the history feature types z Goal:
"In this experiment, we wish to find out the influence of the structural relation and structural distance between the current node and the node that the given feature type related to has to the predictive information quantities of these feature types."
"In Table-2, SR represents the structural relation between the current node and the node that the given feature type related to."
SD represents the structural distance between the current node and the node that the given feature type related to.
"Among the history feature types which have the same structural relation with the current node (the relations are both parent-child relation, or both brother relation, etc), the one which has closer structural distance to the current node will provide larger predictive information quantity; Among the history feature types which have the same structural distance to the current node, the one which has parent relation with the current node will provide larger predictive information quantity than the one that has brother relation or mixed parent and brother relation to the current node (such as the parent&apos;s brother node). 5.3 The analysis to the predictive information quantities of the history feature types and the objective feature types z Goal"
Many of the existing probabilistic evaluation models prefer to use history feature types other than objective feature types.
"We select some of history feature types and objective feature types, and quantitatively compare their predictive information quantities."
"The history feature type we use here is the headword of the parent, which has the largest predictive information quantity among all the history feature types."
"The objective feature types are selected stochastically, which are the first word and the second word in the objective word sequence of the current node (Please see 4.1 and"
Figure-1 for detailed descriptions on the selected feature types). z Conclusion Either of the predictive information quantity of the first word and the second word in the objective word sequence is larger than that of the headword of the parent node which has the largest predictive information quantity among all of the history feature type candidates.
"That is to say, objective feature types may have larger predictive power than that of the history feature type."
"information quantities of the objective features types selected respectively on the physical position information, the heuristic information of headword and modifier, and the exact headword information z Goal"
"Not alike the structural history feature types, the objective feature types are sequential."
"Generally, the candidates of the objective feature types are selected according to the physical position."
"However, from the linguistic viewpoint, the physical position information can hardly grasp the relations between the linguistic structures."
"Therefore, besides the physical position information, our research try to select the objective feature types respectively according to the exact headword information and the heuristic information of headword and modifier."
"Through the experiment, we hope to find out what influence the exact headword information, the heuristic information of headword and modifier, and the physical position information have respectively to the predictive information quantities of the feature types. z Data: (Y= the headword of the current constitute) z Conclusion The predictive information quantity of the headword of the current node is larger than that of a feature type selected according to the selected heuristic information of headword or modifier, and larger than that of a feature type selected according to the physical positions; The predictive information quantity of a feature type selected according to the physical positions is larger than that of a feature types selected according to the selected heuristic information of headword or modifier."
combination which has the optimal predictive information summation z Goal: We aim at proposing a method to select the feature types combination that has the optimal predictive information summation for prediction. z Approach
We use the following greedy algorithm to select the optimal feature type combination.
"In building a model, the first feature type to be selected is the feature type which has the largest predictive information quantity for the prediction of the derivation rule among all of the feature type candidates, that is,"
F 1 = arg max PIQ(F i ; R) (15) F i ∈Ω Where Ω is the set of candidate feature types.
"Given that the model has selected feature type combination F 1 , F 2 , , F j , the next feature type to be added into the model is the feature type which has the largest predictive information gain in all of the feature type candidate except F 1 , F 2 , , F j , on condition that F 1 , F 2 , , F j is known."
"F j+1 = arg max PIG(F i ; R | F 1 , F 2 , , F j ) (16) Fi∈Ω"
"Fi∉{F1,F2, ,Fj} z Data:"
"Among the feature types mentioned above, the optimal feature type combination (i.e. the feature type combination with the largest predictive information summation) which is composed of 6 feature types is, the headword of the current node (type1), the headword of the parent node (type2), the headword of the grandpa node (type3), the first word in the objective word sequence(type4), the first word in the objective word sequence which have the possibility to act as the headword of the current constitute(type5), the headword of the right brother node(type6)."
The cumulative predictive information summation is showed in Figure-2
"The paper proposes an information-theory-based feature type analysis method, which not only presents a series of heuristic conclusion on the predictive power of the different feature types and feature type combination for syntactic parsing, but also provides a guide for the modeling of syntactic parsing in the view of methodology, that is, we can quantitatively analyse the different contextual feature types or feature types combination&apos;s effect for syntactic structure prediction in advance."
"Based on these analysis, we can select the feature type or feature types combination that has the optimal predictive information summation to build the probabilistic parsing model."
"However, there are still some questions to be answered in this paper."
"For example, what is the beneficial improvement in the performance after using this method in a real parser?"
Whether the improvements in PIQ will lead to the improvement of parsing accuracy or not?
"In the following research, we will incorporate these conclusions into a real parser to see whether the parsing accuracy can be improved or not."
"Another work we will do is to do some experimental analysis to find the impact of data sparseness on feature type analysis, which is critical to the performance of real systems."
"The proposed feature type analysis method can be used in not only the probabilistic modelling for statistical syntactic parsing, but also language modelling in more general fields [[REF_CITE]] [[REF_CITE]]."
We present a new approach to stochastic modeling of constraint-based grammars that is based on log-linear models and uses EM for esti-mation from unannotated data.
The techniques are applied to an LFG grammar for German.
"Evaluation on an exact match task yields 86% pre-cision for an ambiguity rate of 5.4, and 90% precision on a subcat frame match for an ambiguity rate of 25."
Experimental comparison to train-ing from a parsebank shows a 10% gain from EM training.
"Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models."
Stochastic parsing models capturing contex-tual constraints beyond the dependencies of probabilistic context-free grammars (PCFGs) are currently the subject of intensive research.
An interesting feature common to most such models is the incorporation of contextual de-pendencies on individual head words into rule-based probability models.
"Such word-based lexicalizations of probability models are used successfully in the statistical parsing mod-els of, e.g.,[REF_CITE],[REF_CITE], or[REF_CITE]."
"However, it is still an open question which kind of lexicaliza-tion, e.g., statistics on individual words or statistics based upon word classes, is the best choice."
"Secondly, these approaches have in common the fact that the probability models"
"Mark_Johnson@brown.edu are trained on treebanks, i.e., corpora of man-ually disambiguated sentences, and not from corpora of unannotated sentences."
"In all of the cited approaches, the Penn Wall Street Jour-nal Treebank[REF_CITE]is used, the availability of which obviates the standard  required for treebank annotating large corpora of  domains of  languages with  parse types."
"Moreover, common wisdom is that training from unannotated data via the expectation-maximization (EM) algorithm[REF_CITE]yields poor results unless at least partial annotation is applied."
"Experi-mental results  this wisdom have been presented, e.g.,[REF_CITE]and[REF_CITE]for EM training of Hidden Markov Models and PCFGs. stochasticIn this papermodel, weforpresentconstraint-baseda new lexicalizedgram-mars that employs a combination of head-word frequencies and EM-based clustering for grammar lexicalization."
"Furthermore, we make crucial use of EM for estimating the parameters of the stochastic grammar from unannotated data."
Our usage of EM was ini-tiated by the current lack of large based treebanks for German.
"However, our ex-perimental results also show an exception to the common wisdom of the  of EM for highly accurate statistical modeling. elingOurisapproachbased ontothelexicalizedparametricstochasticfamily ofmod-log-linear probability models, which is used to de-  a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German."
"In previous work on log-linear mod-els for LFG[REF_CITE], pseudo- likelihood estimation from annotated corpora has been introduced and experimented with on a small scale."
"However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available."
"For-tunately, algorithms exist for statistical infer-ence of log-linear models from unannotated data[REF_CITE]."
We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text.
"In our largest ex-periment, we used 250,000 parses which were produced by parsing 36,000 newspaper sen-tences with the German LFG."
Experimental evaluation of our models on an exact-match task (i.e. percentage of exact match of most probable parse with correct parse) on 550 manually examined examples with on average 5.4 analyses gave 86% precision.
Another eval-uation on a verb frame recognition task (i.e. percentage of agreement between subcatego-rization frames of main verb of most proba-ble parse and correct parse) gave 90% pre-cision on 375 manually disambiguated exam-ples with an average ambiguity of 25.
"Clearly, a direct comparison of these results to state-of-the-art statistical parsers cannot be made because of  training and test data and other evaluation measures."
"However, we would like to draw the following conclusions from our experiments:"
The problem of chaotic convergence be-haviour of EM estimation can be solved for log-linear models.
"EM does help constraint-based gram-mars, e.g. using about 10 times more sen-tences and about 100 times more parses for EM training than for training from an automatically constructed parsebank can improve precision by about 10%."
"Class-based lexicalization can yield a gain in precision of about 10%. duceIn incomplete-datathe rest of thisestimationpaperforwelog-linearintro-models (Sec. 2), and present the actual design ofmentalour modelsresults ((SecSec.. 34)).and report our experi- 2 Incomplete-Data Estimation for"
"Log-Linear Models 2.1 Log-Linear Models A log-linear distribution p ( x ) on the set of analyses X of a constraint-based grammar can be  as follows: p ( x ) = Z ; 1 e ( x ) p 0 ( x ) where Z = P izing constant, x 2X = e ( 1( x ; ) :p:: 0 ( ;x ) n ) is 2 a IR normal- n is a a vector of property-functions = ( i : 1 ; X : : ! : ; IR n ) foris vector of log-parameters, uct i ="
"P 1 ; ni : = : 1 : ; n i , i ( x ) , ( and x ) is p 0 theis vectora dotreferenceprod-distributionThe task of. probabilistic modeling with log-linear distributions is to build salient proper-ties of the data as property-functions i into the probability model."
"For a given vector of ference is to tune the parametersproperty-functions, the task of statisticalto bestin- the empirical distribution of the train-ing data."
Standard numerical methods for statis-tical inference of log-linear models from fully annotated  complete  the iterative scaling meth-ods of Darroch and  (1972) and[REF_CITE].
For data consisting of unannotated  incom-plete  iterative method of the EM algorithm[REF_CITE]has to be employed.
"However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative."
A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically  estimation method for log-linear models from incomplete data is the IM algorithm[REF_CITE].
"Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: A training sample of unannotated sen-tences y from a set Y , observed with empirical probability p ~( y ) , a constraint-based grammar yielding a set X ( y ) of parses for each sentence y , and P a log-linear model p ( ) on the parses X the = training y 2Yj p ~( corpus y ) &gt; 0 X , ( y with ) for knownthe sentencesvalues inof property-functions of ."
"The aim of incomplete-dataand unknownmaximumvalues likelihoodthat maximizesestimationthe(MLEincomplete-data) is to  a valuelog-likelihood L = P y 2Y p ~( y )ln P x 2 X ( y ) p ( x ) , i.e., = arg 2 max IR n L ( ) :"
"Closed-form parameter-updates for this prob-lem can be computed P n by the ( x ) algorithm, and k ( x of j y Fig ) = . [Footnote_1], where P # ( x ) = p ( x ) = x 2 X ( y ) p ( x ) i = is 1 the i conditional prob-ability of a parse x given the sentence y and the current parameter value . enforcedThe constancyby addingrequirementa on # property-can be function l :"
1 If the training sample consists of complete data
Choose K = max x 2X # ( x ) and Then l ( x ) =
P K li =1 ; i ( # x () x =) for K forall x all 2 x X2 .
"Note that because of the restriction of X to the parses obtainable by a grammar from the training corpus, we have a log-linear probabil-ity measure only on those parses and not on all possible parses of the grammar."
We shall therefore speak of mere log-linear measures in our application of disambiguation.
"For incomplete-data estimation, a sequence of likelihood values is guaranteed to converge to a critical point of the likelihood function L ."
This is shown for the IM algorithm[REF_CITE].
"The process of  likeli-hood maxima is chaotic in that the  likeli-hood value is extremely sensitive to the start-ing values of , i.e. limit points can be lo-cal maxima (or saddlepoints), which are not necessarily also global maxima."
A way to search for order in this chaos is to search for starting values which are hopefully attracted by the global maximum of L .
"This problem can best be explained in terms of the mini-mum divergence paradigm[REF_CITE], which is equivalent to the maximum likeli-hood paradigm P by the following theorem."
Let a function x 2X f with p ( x ) f respect ( x ) betothea distributionexpectation p of: p [ f ] =
The probability distribution p that minimizes the divergence
D ( p jj p 0 ) to a reference model p 0 subject to the constraints p [ i ] = q [ i ] ; i = 1 ; : : : ; n is the model in the parametric fam-ily of log-linear distributions p that maximizes the likelihood L ( ) = q [ln p ] of the training data 1 .
Reasonable starting values for minimum di-vergence estimation is to set i = 0 for i = 1 ; : : : ; n .
"This yields a distribution which minimizes the divergence to p 0 , over the set of models p to which the constraints p [ i ] = q [ i ] ;i = 1 ;::: ;n have yet to be ap-plied."
"Clearly, this argument applies to both complete-data and incomplete-data estima-tion."
"Note that for a uniformly distributed reference model p 0 , the minimum divergence model is a maximum entropy model[REF_CITE]."
"In Sec. 4, we will demonstrate that a uniform initialization of the IM algorithm shows a  improvement in likelihood maximization as well as in linguistic perfor-mance when compared to standard random initialization."
They refer to both the c(onstituent)-structure and the f(eature)-structure of the LFG parses.
"Examples are properties for c-structure nodes, corresponding to stan-dard production properties, c-structure subtrees, indicating argument versus adjunct attachment, f-structure attributes, corresponding to grammatical functions used in LFG, atomic attribute-value pairs in f-structures, complexity of the phrase being attached to, thus indicating both high and low at-tachment, non-right-branching behavior of nonter-minal nodes, non-parallelism of coordinations."
Our approach to grammar lexicalization is class-based in the sense that we use class-based estimated frequencies f c ( v;n ) of head-verbs v and argument head-nouns n in-stead of pure frequency statistics or class-based probabilities of head word dependen-cies.
"Class-based estimated frequencies are in-troduced[REF_CITE]as the fre-quency f ( v;n ) of a ( v;n ) -pair in the train-ing corpus, weighted by the best estimate of the class-membership probability p ( c j v;n ) of an EM-based clustering model on ( v; n ) -pairs, i.e., f c ( v; n ) = max c 2 C p ( c j v; n )( f ( v; n ) + 1) ."
"As is shown[REF_CITE]in an evaluation on lexical ambiguity resolution, a gain of about 7% can be obtained by using the class-based estimated frequency f c ( v;n ) as disambiguation criterion instead of class-based probabilities p ( n j v ) ."
"In order to make the most direct use possible of this fact, we incorporated the decisions of the disambigua-tor directly into 45 additional properties for the grammatical relations of the subject, di-rect object, indirect object,  object, oblique and adjunctival dative and accusative preposition, for active and passive forms of the  three verbs in each parse."
"Let v r ( x ) be the verbal head of grammatical relation r in parse x , and n r ( x ) the nominal head of grammatical relation r in x ."
Then a lexicalized property r for grammatical relation r is  as 8 r ( x ) = &lt;: 01 otherwiseif ( ( 0 ) f c ( v r ( x ) ; n r ( x )) f c v r x ;n r ( x 0 )) 8 x 0 2 X ( y ) ; :
"The disambiguatesproperty-functionthe parses x r 2 thus X ( y ) ofpre-a sentence y according to f c ( v;n ) , and stores the best parse directly instead of taking the actual estimated frequencies as its value."
"In Sec. 4, we will see that an incorporation of this pre-disambiguation routine into the mod-els improves performance in disambiguation by about 10%."
"In our experiments, we used an LFG grammar for German [Footnote_2] for parsing unrestricted text."
"2 The German LFG grammar is being imple-mented in the Xerox Linguistic Environment (XLE, see[REF_CITE]) as part of the Paral-lel Grammar (ParGram) project at the IMS Stuttgart. The coverage of the grammar is about 50% for unre-stricted newspaper text. For the experiments reported here, the coverage was lower, since the cor-pus preprocessing we applied was minimal. Note that for the disambiguation task we were interested in, the overall grammar coverage was of subordinate rel-evance."
"Since training was faster than parsing, we parsed in advance and stored the resulting packed c/f-structures."
The low ambiguity rate ofrestrictthe Germanthe trainingLFG datagrammarto sentencesallowed uswithto at most 20 parses.
"The resulting training cor-pus of unannotated, incomplete data consists of approximately 36,000 sentences of online available German newspaper text, comprising approximately 250,000 parses. ambiguousIn order toandcompareambiguousthe sentencescontributionto theof un-es-timation results, we extracted a subcorpus of 4,000 sentences, for which the LFG grammar produced a unique parse, from the full train- ing corpus."
"The average sentence length of 7.5 for this automatically constructed parse-bank is only slightly smaller than that of 10.5 for the full set of 36,000 training sen-tences and 250,000 parses."
"Thus, we conjec-ture that the parsebank includes a representa-tive variety of linguistic phenomena."
Estima-tion from this automatically disambiguated parsebank enjoys the same complete-data es-timation properties [Footnote_3] as training from manu-ally disambiguated treebanks.
"3 For example, convergence to the global maximum of the complete-data log-likelihood function is guar-anteed, which is a good condition for highly precise statistical disambiguation."
This makes a comparison of complete-data estimation from this parsebank to incomplete-data estimation from the full set of training data interesting.
"To evaluate our models, we constructed two  test corpora."
We  parsed with the[REF_CITE]sentences which are used for illustrative purposes in the foreign language learner&apos;s grammar[REF_CITE].
"In a next step, the correct parse was indicated by a human dis-ambiguator, according to the reading intended[REF_CITE]."
Thus a precise indication of correct c/f-structure pairs was possible.
"However, the average ambiguity of this corpus is only 5.4 parses per sentence, for sentences with on average 7.5 words."
"In order to evaluate on sentences with higher ambigu-ity rate, we manually disambiguated further 375 sentences of LFG-parsed newspaper text."
The sentences of this corpus have on average 25 parses and 11.2 words. tionWetaskstested.
"Theourstatisticalmodelsdisambiguatoron two evalua-was tested on an   task, where ex-act correspondence of the full c/f-structure pairthe mostof theprobablehand-annotatedparse is checkedcorrect .parseAnotherand evaluation was done on a   task, where exact correspondence only of the sub-categorization frame of the main verb of the most probable parse and the correct parse is checked."
"Clearly, the latter task involves a smallerto be interpreted ambiguityas an evaluationrate, andof theis com-thus bined system of highly-constrained symbolic parsing and statistical disambiguation. wasPerformanceassessed accordingon thesetotwotheevaluationfollowing evalu-tasks ation measures: # correct Precision = # incorrect , correct #+#  = # correct +# incorrectcorrect +# don&apos;t know .   and   a suc-cess/failure on the respective evaluation tasks;   cases are cases where the system is unable to make a decision, i.e. cases with more than one most probable parse."
"For each task and each test corpus, we cal-culated a random baseline by averaging over several models with randomly chosen pa-rameter values."
This baseline measures the disambiguation power of the pure symbolic parser.
The results of an exact-match evalu-ation on the Helbig-Buscha corpus is shown in Fig. 2.
The random baseline was around models33% foraccordingthis case.
Theto theircolumnsproperty-vectorslist . propertiesmodelsas describedconsist ofin190Sec. 3.1.  models are extended by 45 lexical pre-disambiguation properties as described in Sec. 3.2.   +  models result from a simple property selection procedure where a  on the number of parses with non-negative value of the property-functions was set.
"Estimation of basic models from com-plete data gave 68% precision (P), whereas training lexicalized and selected models from incomplete data gave 86.1% precision, which is an improvement of 18%."
Comparing lex-icalized models in the estimation method shows that incomplete-data estimation gives an improvement of 12% precision over train-ing from the parsebank.
A comparison of mod-els trained from incomplete data shows that lexicalization yields a gain of 13% in preci-sion.
Note also the gain in  (E) due to the pre-disambigution routine included in the lexicalized properties.
The gain due to property selection both in precision and tiveness is minimal.
A similar pattern of per-formance arises in an exact match evaluation on the newspaper corpus with an ambiguity rate of 25.
"The lexicalized and selected model trained from incomplete data achieved here 60.1% precision and 57.9% , for a random baseline of around 17%. formanceAs shownduein toFigboth. 3, thelexicalizationimprovementandin per-EM training is smaller for the easier task of frame evaluation."
Here the random baseline is 70% for frame evaluation on the newspaper corpus with an ambiguity rate of 25.
An overall gain of roughly 10% can be achieved by going from unlexicalized parsebank models (80.6% preci-sion) to lexicalized EM-trained models (90% precision).
"Again, the contribution to this im-provement is about the same for lexicalization and incomplete-data training."
"Applying the same evaluation to the Helbig-Buscha corpus shows 97.6% precision and 96.7%  for the lexicalized and selected incomplete-data model, compared to around 80% for the random baseline. repeatedOptimalevaluationiteration numbersof the modelswere decidedat everyby  iteration."
Fig. 4 shows the precision of lexicalized and selected models on the exact match task plotted against the number of it-erations of the training algorithm.
"For parse-bank training, the maximal precision value is obtained at 35 iterations."
Iterating fur-ther shows a clear overtraining .
For incomplete-data estimation more iterations are necessary to reach a maximal precision value.
A comparison of models with random or uniform starting values shows an increase in precision of 10% to 40% for the latter.
"In terms of maximization of likelihood, this corresponds to the fact that uniform starting values immediately push the likelihood up to nearly its  value, whereas random starting values yield an initial likelihood which has to be increased by factors of 2 to 20 to an often lower  value."
The most direct points of compar-proachesof ofour Johnsonmethodet alare. (1999the) andap- is[REF_CITE].
"In the  ap-proach, log-linear models on LFG grammars using about 200  properties were trained on treebanks of about 400 sentences by maximum pseudo-likelihood estimation."
"Precision was evaluated on an exact match task in a 10-way cross valida-tion paradigm for an ambiguity rate of 10, and achieved 59% for the  approach."
"Our best models clearly outperform these results, both in terms of precision relative to ambiguity and in terms of relative gain due to lexicalization."
A comparison of performance is more  for the lexicalized PCFG[REF_CITE]which was trained[REF_CITE]000 sen-tences of German newspaper text.
"There, a 70.4% precision is reported on a verb frame recognition task on 584 examples."
"However, the gain achieved[REF_CITE]due to grammar lexicalizaton is only 2%, compared to about 10% in our case."
"A comparison is  also for most other state-of-the-art PCFG-based statistical parsers, since  training and test data, and most importantly,  evaluation criteria were used."
A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported[REF_CITE].
We have presented a new approach to stochas-tic modeling of constraint-based grammars.
Our experimental results show that EM train-ing can in fact be very helpful for accurate stochastic modeling in natural language pro-cessing.
"We conjecture that this result is due partly to the fact that the space of parses produced by a constraint-based grammar is only  , i.e. the ambiguity rate can be kept relatively low."
"Another rea-son may be that EM is especially useful for log-linear models, where the search space in maximization can be kept under control."
"Fur-thermore, we have introduced a new class- based grammar lexicalization, which again uses EM training and incorporates a pre-disambiguation routine into log-linear models."
An impressive gain in performance could also be demonstrated for this method.
"Clearly, a central task of future work is a further explo-ration of the relation between complete-data and incomplete-data estimation for larger, manually disambiguated treebanks."
"An inter-esting question is whether a systematic vari-ation of training data size along the lines of the EM-experiments[REF_CITE]for text  will show similar results, namely a systematic dependence of the rela-tive gain due to EM training from the relative sizes of unannotated and annotated data."
"Fur-thermore, it is important to show that EM-based methods can be applied successfully also to other statistical parsing frameworks."
"^¬r­ ®S¯O®Z°²±´³SµG°E®g±·¶L®Z¶¦­t°1¯, ?? ½¼«g©^¬¿Ä  ­1ÂÆ±t¶Ç¸5ÈV°´ÉÊ®S¯OË¦°´­; ªt¶$Ì^ª·¬ÀÅr¬¿Í²°Vª·«^°vÈÎ¶¦±·År¹ÏÈÐ¬À¹^°6È{°´ÉÑ¯¦­ ¯¦©0°?^© ½²ÒX½´Å¿¶¦®¯¦©^~¹Õ°´¹g¬À¯*^ÖØ×ÚÙÓ0ÈÎ°ÔÌg­t°{ªt°²»Xª&quot;+ÅÀ¬r©ËÇÌg¬À­tª·¬r½?­ ?©|ª·­6½²¶¦©| ¬À©^^{,° ¯¦ÅÀ­t¶VÌ^­t° ¯vÅr¯L©^^°´Å  © °²¶ÇÌ^?^¹Ú¯,½´ÅÀÌg­tªt°´±·¬r©Ë^ ^{¹  ,±·°?­·Ì^.^¹ °²Ä ­·½²±·¬À®^{­·«^¶OµÞª·«^°V°²ß~?°  ^°´­·­ ¶LÂ2¶¦Ì^^¶¹&amp;ÉXÒ;µá¯?Ò;¶LÂ2°²»X®~°´±tÄ© ? â æØ°²çg°?^ ^ª·¬ÀÅr¬¿Í´¯Lª·¬À¶¦©Á¶LÂ,¸ ¯OÄ"
"In recent years, statistical approaches on ATR (Automatic Term Recognition) have achieved good results."
"However, there are scopes to improve the performance in extracting terms still further."
"For example, domain dictionaries can improve the performance in ATR."
This paper focuses on a method for extracting terms using a dictionary hierarchy.
Our method produces relatively good results for this task.
"In recent years, statistical approaches on ATR (Automatic Term Recognition)[REF_CITE]have achieved good results."
"However, there are scopes to improve the performance in extracting terms still further."
"For example, the additional technical dictionaries can be used for improving the accuracy in extracting terms."
"Although, the hardship on constructing an electronic dictionary was major obstacles for using an electronic technical dictionary in term recognition, the increasing development of tools for building electronic lexical resources makes a new chance to use them in the field of terminology."
"From these endeavour, a number of electronic technical dictionaries (domain dictionaries) have been acquired."
"Since newly produced terms are usually made out of existing terms, dictionaries can be used as a source of them."
"For example, ‘distributed database’ is composed of ‘distributed’ and ‘database’ that are terms in a computer science domain."
"Further, concepts and terms of a domain are frequently imported from related domains."
"For example, the term ‘Geographical Information System (GIS)’ is used not only in a computer science domain, but also in an electronic domain."
"To use these properties, it is necessary to build relationships between domains."
The hierarchical clustering method used in the information retrieval offers a good means for this purpose.
A dictionary hierarchy can be constructed by the hierarchical clustering method.
The hierarchy helps to estimate the relationships between domains.
Moreover the estimated relationships between domains can be used for weighting terms in the corpus.
"For example, a domain of electronics may have a deep relationship to that of computer science."
"As a result, terms in the dictionary of electronics domain have a higher probability to be terms of computer science domain than terms in the dictionary of others do[REF_CITE]."
The recent works on ATR identify the candidate terms using shallow syntactic information and score the terms using statistical measure such as frequency.
The candidate terms are ranked by the score and are truncated by the thresholds.
"However, the statistical method solely may not give accurate performance in case of small sized corpora or very specialized domains, where the terms may not appear repeatedly in the corpora."
"In our approach, a dictionary hierarchy is used to avoid these limitations."
"In the next section, we describe the overall method description."
"In section 2, section 3, and section 4, we describe primary methods and its details."
"In section 5, we describe experiments and results"
The description of the proposed method is shown in figure 1.
There are three main steps in our method.
"In the first stage, candidate terms that are complex nominal are extracted by a linguistic filter and a dictionary hierarchy is constructed."
"In the second stage, candidate terms are scored by each weighting scheme."
"In dictionary weighing scheme, candidate terms are scored based on the kind of domain dictionary where terms appear."
"In statistical weighting scheme, terms are scored by their frequency in the given corpus."
"In transliterated word weighting scheme, terms are scored by the number of transliterated foreign words in the terms."
"In the third stage, each weight is normalized and combined to Term weight (W term ), and terms are extracted by Term weight."
A dictionary hierarchy is constructed using bi-lingual dictionaries (English to Korean) of the fifty-seven domains.
Table 1 lists the domains that are used for constructing the dictionary hierarchy.
The dictionaries belong to domains of science and technology.
"Moreover, terms that do not appear in any dictionary (henceforth we call them unregistered terms) are complemented by a domain tagged corpus."
"We use a corpus, called ETRI-KEMONG test collection, with the documents of seventy-six domains to complement unregistered terms and to eliminate common term."
The clustering method is used for constructing a dictionary hierarchy.
The clustering is a statistical technique to generate a category structure using the similarity between documents[REF_CITE].
"Among the clustering methods, a reciprocal nearest neighbor (RNN) algorithm[REF_CITE]based on a hierarchical clustering model is used, since it joins the cluster minimizing the increase in the total within-group error sum of squares at each stage and tends to make a symmetric hierarchy[REF_CITE]."
The algorithm to form a cluster can be described as follows: 1. Determine all inter-object (or inter-dictionary) dissimilarity. 2. Form cluster from two closest objects (dictionaries) or clusters. 3.
"Recalculate dissimilarities between new cluster created in the step2 and other object (dictionary) or cluster already made. (all other inter-point dissimilarities are unchanged). 4. Return to Step2, until all objects (including cluster) are in the one cluster."
"In the algorithm, all objects are treated as a vector such as D i = (x i1 , x i2 , ... , x iL )."
"In the step 1, inter-object dissimilarity is calculated based on the Euclidian distance."
"In the step2, the closest object is determined by a RNN."
"For given object i and object j, we can define that there is a RNN relationship between i and j when the closest object of i is object j and the closest object of j is object i."
This is the reason why the algorithm is called a RNN algorithm.
"A dictionary hierarchy is constructed by the algorithm, as shown in figure 2."
There are ten domains in the hierarchy – this is a fragment of whole hierarchy.
The main idea for scoring terms using the hierarchy is based on the premise that terms in the dictionaries of the target domain and terms in the dictionary of the domain related to the target domain act as a positive indicator for recognizing terms.
Terms in the dictionaries of the domains that are not related to the target domain act as a negative indicator for recognizing terms.
We apply the premise for scoring terms using the hierarchy.
There are three steps to calculate the score. 1. Calculating the similarity between the domains using the formula (2.1)[REF_CITE]similarity ij = 2 × Common ij (2.1) depth i + depth j where
Depth i : the depth of the domain i node in the hierarchy Common ij : the depth of the deepest node sharing between the domain i and the domain j in the path from the root.
"In the formula (2.1), the depth of the node is defined as a distance from the root – the depth of a root is 1."
"For example, let the parent node of C1 and C8 be the root of hierarchy in figure 2."
The similarity between “Chemistry” and “Chemical engineering” is calculated as shown below in table 2: 3. Complementing unregistered terms and common terms by domain tagged corpora.
Consider two exceptional possible cases.
"First, there are unregistered terms that are not contained in any dictionaries."
"Second, some commonly used terms can be used to describe a special concept in a specific domain dictionary."
"Since an unregistered term may be a newly created term of domains, it should be considered as a candidate term."
"In contrast with an unregistered term, common terms should be eliminated from candidate terms."
"Therefore, the score calculated in the step 2 should be complemented for these purposes."
"In our method, the domain tagged corpus[REF_CITE]is used."
Each word in the candidate terms – they are composed of more than one word – can appear in the domain tagged corpus.
We can count the number of domains where the word appears.
"If the number is large, we can determine that the word have a tendency to be a common word."
"If the number is small, we can determine that the word have a high probability to be a valid term."
"In this paper, the score calculated by the dictionary hierarchy is called Dictionary Weight (W Dic )."
The statistical method is divided into two elements.
"The first element, the Statistical Weight, is based on the frequencies of terms."
"The second element, the Transliterated word Weight, which is based on the number of transliterated foreign word in the candidate term."
This section describes the above two elements. 3.1.
Statistical Weight: Frequency Based Weight
"In the Statistical Weight, not only abbreviation pairs and translation pairs in a parenthetical expression but also frequencies of terms are considered."
Abbreviation pairs and translation pairs are detected using the following simple heuristics:
"For a given parenthetical expression A(B), 1. Check on a fact that A and B are abbreviation pairs."
"The capital letter of A is compared with that of B. If the half of the capital letter are matched for each other sequentially, A and B are determined to abbreviation pairs (Hisamitsu et. al, 1998)."
"For example, ‘ISO’ and ‘International Standardization Organization’ is detected as an abbreviation in a parenthetical expression ‘ISO (International Standardization Organization)’. 2. Check on a fact that A and B are translation pairs."
"Using the bi-lingual dictionary, it is determined."
"After detecting abbreviation pairs and translation pairs, the Statistical Weight (W Stat ) of the terms is calculated by the formula (3.1). where α : a candidate term | α |: the length of a term’ α ’ S ( α ): abbreviation and translation pairs of ‘ α ’ T( α ): The set of candidate terms that nest ‘ α ’ f( α ): the frequency of ‘ α ’ C(T( α )): The number of elements in T( α )"
"In the formula (3.1), the nested relation is defined as follows: let A and B be a candidate term."
"If A contains B, we define that A nests B."
"The formula implies that abbreviation pairs and translation pairs related to ‘α’ is counted as well as ‘α’ itself and productivity of words in the nested expression containing ‘α’ gives more weight, when the generated expression contains ‘α’."
"Moreover, formula (1) deals with a single-word term, since an abbreviation such as GUI (Graphical User Interface) is single word term and English multi-word term usually translated to Korean single-word term – (e.g. distributed database =&gt; bunsan deitabeisu) 3.2 Transliterated word Weight:"
By Automatic Extraction of Transliterated words
Technical terms and concepts are created in the world that must be translated or transliterated.
Transliterated terms are one of important clues to identify the terms in the given domain.
We observe dictionaries of computer science and chemistry domains to investigate the transliterated foreign words.
"In the result of observation, about 53% of whole entries in a dictionary of a computer science domain are transliterated foreign words and about 48% of whole entries in a dictionary of a chemistry domain are transliterated foreign words."
"Because there are many possible transliterated forms and they are usually unregistered terms, it is difficult to detect them automatically."
"In our method, we use HMM (Hidden Markov Model) for this task[REF_CITE]."
"The main idea for extracting a foreign word is that the composition of foreign words would be different from that of pure Korean words, since the phonetic system for the Korean language is different from that of the foreign language."
"Especially, several English consonants that occur frequently in English words, such as ‘p’, ’t’, ’c’, and ‘f’, are transliterated into Korean consonants ‘p’, ‘t’, ‘k’, and ‘p’ respectively."
"Since these consonants of Korean are not used in pure Korean words frequently, this property can be used as an important clue for extracting a foreign word from Korean."
"For example, in a word, ‘si-seu-tem’ (system), the syllable ‘tem’ have a high probability to be a syllable of transliterated foreign word, since the consonant of ‘t’ in the syllable ‘tem’ is usually not used in a pure Korean word."
"Therefore, the consonant information which is acquired from a corpus can be used to determine whether a syllable in the given term is likely to be the part of a foreign word or not."
"Using HMM, a syllable is tagged with ‘K’ or ‘F’."
A syllable tagged with ‘K’ means that it is part of a pure Korean word.
A syllable tagged with ‘F’ means that it is part of a transliterated word.
"For example, ‘si-seu-tem-eun (system is)’ is tagged with ‘si/F + seu/F + tem/F + eun/K’."
We use consonant information to detect a transliterated word like lexical information in part-of-speech-tagging.
The formula (3.2) is used for extracting a transliterated word and the formula (3.3) is used for calculating the Transliterated Word Weight (W Trl ).
The formula (3.3) implies that terms have more transliterated foreign words than common words do.
"P(T | S)P(S) = p(t 1 ) p(t 2 | t 1 )  ∏ p(t | t ,t ) ∏ p(s | t )  n n i i−1 i−2  (3.2)  i=3 i i  i=1 where s i : i-th consonant in the given word. t i : i-th tag (‘F’ or ‘K’) of the syllable in the given word."
W Trl ( α ) = trans α ( α ) (3.3) where | α | is the number of words in the term α trans( α ) is the number of transliterated words in the term α 4.Term
The three individual weights described above are combined according to the following formula (4.1) called Term Weight (W Term ) for identifying the relevant terms.
W term ( ϕ ) = α × f (W Dic ( ϕ )) + β × g(W Trl ( ϕ )) + γ × h(W
Stat ( ϕ )) (4.1)
"Where ϕ : a candidate term ‘ ϕ ’ f,g,h : normalization function α + β + γ = 1"
"In the formula (4.1), the three individual weights are normalized by the function f, g, and h respectively and weighted parameter α , β , and γ ."
"The parameter α , β , and γ are determined by experiment with the condition α + β + γ = 1."
"Each value which is used in this paper is α =0.6, β =0.1, and γ =0.3 respectively."
"The proposed method is tested on a corpus of computer science domains, called the KT test collection."
"The collection contains 4,434 documents and 67,253 words and contains documents about the abstract of the paper (Park. et al., 1996)."
It was tagged with a part-of-speech tagger for evaluation.
We examined the performance of the Dictionary Weight (W Dic ) to show its usefulness.
"Moreover, we examined both the performance of the C-value that is based on the statistical method (Frantzi. et al., 1999) and the performance of the proposed method."
Two domain experts manually carry out the assessment of the list of terms extracted by the proposed method.
The results are accepted as the valid term when both of the two experts agree on them.
"This prevents the evaluation from being carried out subjectively, when one expert assesses the results."
The results are evaluated by a precision rate.
A precision rate means that the proportion of correct answers to the extracted results by the system.
"In this section, the evaluation is performed using only W Dic to show the usefulness of a dictionary hierarchy to recognize the relevant terms The Dictionary Weight is based on the premise that the information of the target domain is a good indicator for identifying terms."
The term in the dictionaries of the target domain and the domain related to the target domain acts as a positive indicator for recognizing terms.
"The term in the dictionaries of the domains, which are not related to the target domain acts as a negative indicator for recognizing terms."
The dictionary hierarchy is constructed to estimate the similarity between one domain and another.
"The result, depicted in table 4, can be interpreted as follows: In the top 10% of the extracted terms, 94% of them are the valid terms and 6% of them are non-terms."
"In the bottom 10% of the extracted terms, 54.8% of them are the valid terms and 45.2% of them are non-terms."
"This means that the relevant terms are much more than non-terms in the top 10% of the result, while non-terms are much more than the relevant terms in the bottom 10% of the result."
"The results are summarized as follow: !&quot;According as a term has a high Dictionary Weight (W Dic ), it is apt to be valid. ! &quot;More valid terms have a high Dictionary Weight (W Dic ) than non-terms do"
Table 5 and figure 3 show the performance of the proposed method and of the C-value method.
"By dividing the ranked lists into 10 equal sections, the results are compared."
Each section contains the 1291 terms and is evaluated independently. proposed method :[REF_CITE]terms and precision is evaluated independently.
"For example, in section 1, since there are 1291 candidate terms and 1241 relevant terms by the proposed method, the precision rate in section 1 is 96.13% ."
The result can be interpreted as follows.
"In the top sections, the proposed method shows the higher precision rate than the C-value does."
"The distribution of valid terms is also better for the proposed method, since there is a downward tendency from section 1 to section 10."
This implies that the terms with higher weight scored by our method have a higher probability to be valid terms.
"Moreover, the precision rate of our method shows the rapid decrease from section 6 to section 10."
This indicates that most of valid terms are located in the top sections.
The results can be summarized as follow : !&quot;The proposed method extracts a valid term more accurate than C-value does. !&quot;Most of the valid terms are in the top section extracted by the proposed method.
"In this paper, we have described a method for term extraction using a dictionary hierarchy."
It is constructed by clustering method and is used for estimating the relationships between domains.
Evaluation shows improvement over the C-value.
"Especially, our approach can distinguish the valid terms efficiently – there are more valid terms in the top sections and less valid terms in the bottom sections."
"Although the method targets Korean, it can be applicable to English by slight change on the Tweight (W Trl )."
"However, there are many scopes for further extensions of this research."
"The problems of non-nominal terms[REF_CITE], term variati[REF_CITE], and relevant contexts[REF_CITE], can be considered for improving the performance."
"Moreover, it is necessary to apply our method to practical NLP systems, such as an information retrieval system and a morphological analyser."
"®¢¡©o¤°¯²±³¦s´f±µ¦p¶·¨T¯µ¤¡£s¤x¡Y¥2¡,¦V§ ¨ ,¨a¸s¹º¡Y»o±²¥t§2±µ¦p¶¼¯²¡Y»o±¾½¤x©pªs«p¥t§ ¨V£s£s¤x©o¨a¬Y­ ¬ ¨T¯µ¿T¥t¡,ÀÁ¨V¦f§t±²¬Â­p±²¡,¤t¨V¤x¬Y­p±²¡Y¥,¢¡Ä«p¥2¡Y¸, ¨Å¬Y©o¦p¥t§`¤t¨T±µ¦f§*¥`¨T§t±²¥ ® ¨T¬Y§t±²©o¦Æ¨a¯²¶V©p¤x±²§`­sÀ Ç ¤x¡Y¯³¨a»È¨T§t±²©o¦É¯³¨Vªs¡Y¯²±³¦p¶oÊË§2©·¥t¡Y¯²¡Y¬Y§ Ì &amp; ® ¬, ¦p©p¸s¡ ÀO¨a§t¬Y­p¡Y¥&amp;,¡   ©p¦p©oÀc¹VÃ}{Ñ ¦Ò£s¨V¤x§t±²¬&amp;,«p¯³¨V¤YÓ0Ô0¡Õ«p¥t¡I±Ö§O§t©£s¨V¤x§Á© ® * Ù ÃÛÚI©o¦V§2©u Ù ,¤x¹ , ÀO¨a±³¦p±µ¦p¶&lt; ß à á ^A C :9 :8 paH C á, ¶V¡â ,­p¡¦p¡,,¤x¡±²¥ã¨V¦w±³¦p¬¤t¨T¯äÓ0¨a¬Y¬,«s¤t¨T§t¡å¨f¦p¸,¤x¡,}ªs¤x©o¨a¸×¬Y©aÝV¡® ­s¨Ýo±³¦p¶,&amp;¨,Ýa¨a±²¯³¨Vªp¯²¡"
"Weing thepresentsemantica systemrelationshipsfor identify-, or se-mantic roles , lled by constituents of a sentence within a semantic frame."
Various lexical and syntactic fea-tures are derived from parse trees and used to derive statistical clas-si ers from hand-annotated training data.
Identifying the semantic roles lled by con-stituents of a sentence can provide a level of shallow semantic analysis useful in solving a number of natural language processing tasks.
Semantic roles represent the participants in an action or relationship captured by a se-mantic frame.
"For example, the frame for one sense of the verb \crash&quot; includes the roles Agent , Vehicle and To-Location . tionThiscanshallowbe usedsemanticfor manylevelpurposesof interpreta-."
"Cur-rent information extraction systems often use domain-speci c frame-and-slot templates to extract facts about, for example, nancial news or interesting political events."
"A shal-low semantic level of representation is a more domain-independent, robust level of represen-tation."
"Identifying these roles, for example, could allow a system to determine that in the sentence \The rst one crashed&quot; the sub-jectrst isonethecrashedvehicleit, &quot;buttheinsubjectthe sentenceis the agent\The, thiswhichdomainwould. helpAnotherin informationapplicationextractionis in word-in sense disambiguation, where the roles associ- ated with a word can be cues to its sense."
"For example,[REF_CITE]and others have shown that the di erent syntactic sub-catgorization frames of a verb like \serve&quot; can be used to help disambiguate a particular in-stance of the word \serve&quot;."
Adding seman-tic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge.
Semantic roles could also act as an important inter-mediate representation in statistical machine translation or automatic text summarization and in the emerging eld of Text Data Mining (TDM)[REF_CITE].
"Finally, incorporat-ing semantic roles into probabilistic models of language should yield more accurate parsers and better language models for speech recog-nition. tomaticThis papersemanticproposesanalysisan ,algorithmassigningfora au-se-mantic role to constituents in a sentence."
"Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambigua-tion."
"We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classi ca-tion."
Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database[REF_CITE].
"The FrameNet database de nes a tagset of semantic roles 50called,000 frame sentences elements from ,theandBritishincludesNationalroughly Corpus which have been hand-labeled with these frame elements."
The next section de-scribes the set of frame elements/semantic roles used by our system.
"In the rest of this paper we report on our current system, as well as a number of preliminary experiments on extensions to the system."
"Historically, two types of semantic roles have been studied: abstract roles such as Agent and Patient , and roles speci c to individual verbsThe FrameNetsuch as Eater project andproposes Eaten rolesforat\aneatin-&quot;. termediate level, that of the semantic frame."
"Frames are de ned as schematic representa-pantstions ,ofpropssituations, and otherinvolvingconceptualvariousrolespartici-[REF_CITE]."
"For example, the frame \conver-sation&quot;, shown in Figure 1, is invoked by the semantically related verbs \argue&quot;, \banter&quot;, \debate&quot;, \converse&quot;, and \gossip&quot; as well as the nouns \argument&quot;, \dispute&quot;, \discus-sion&quot; and \ti &quot;."
"The roles de ned for this frame, and shared by all its lexical entries, include Protagonist1 and Protagonist2 or simply Protagonists for the participants in the conversation, as well as Medium , and Topic ."
Example sentences are shown in Ta-ble 1.
"De ning semantic roles at the frame level avoids some of the  of at-tempting to nd a small set of universal, ab-stract thematic roles, or case roles such as Agent , Patient , etc (as in, among many others,[REF_CITE](Jackendo , 1972))."
Abstract thematic roles can be thought of as being frame elements de ned in abstract frames such as \action&quot; and \motion&quot; which are at the top of in inheritance hierarchy of semantic frames[REF_CITE]. corpusThe preliminaryused for our experimentsversion of thecontained[REF_CITE]frames from 12 general semantic domains cho-sen for annotation.
"Examples of domains (see Figure 1) include \motion&quot;, \cognition&quot; and \communication&quot;."
"Within these frames, ex-amples of a total of 1462 distinct lexical pred-icates, or target words , were annotated: 927 verbs, 339 nouns, and 175 adjectives."
"There are a total of 49,013 annotated sentences, and 99,232 annotated frame elements (which do not include the target words themselves)."
"Assignment of semantic roles is an impor-tant part of language understanding, and has been attacked by many computational sys-tems."
"Traditional parsing and understand-ing systems, including implementations of uni cation-based grammars such as HPSG[REF_CITE], rely on hand-developed grammars which must anticipate each way in which semantic roles may be real-ized syntactically."
"Writing such grammars is time-consuming, and typically such systems have limited coverage. appliedData-drivento template-basedtechniques havesemanticrecentlyinterpre-been tation in limited domains by \shallow&quot; sys-tems that avoid complex feature structures, and often perform only shallow syntactic analysis."
"For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue,[REF_CITE]com-puted the probability that a constituent such as \Atlanta&quot; lled a semantic slot such as Destination in a semantic frame for air travel."
"In a data-driven approach to infor-mation extraction,[REF_CITE]builds a dic-tionary of patterns for lling slots in a spe-ci c domain such as terrorist attacks, and[REF_CITE]extend this technique to automatically derive entire case frames for words in the domain."
These last systems make use of a limited amount of hand labor to accept or reject automatically gen-erated hypotheses.
They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks.
"More recently, a do-main independent system has been trained on general function tags such as Manner and Temporal[REF_CITE]."
"We divide the task of labeling frame elements into two subtasks: that of identifying the boundaries of the frame elements in the sen-tences, and that of labeling each frame ele-ment, given its boundaries, with the correct role."
"We rst give results for a system which labels roles using human-annotated bound-aries, returning to the question of automat-ically identifying the boundaries in Section 5.3."
"The system is a statistical one, based on train-ing a classi er on a labeled training set, and testing on an unlabeled test set."
"The sys-tem is trained by rst using the Collins parser[REF_CITE]to parse the 36,995 train-ing sentences, matching annotated frame el-ements to parse constituents, and extracting various features from the string of words and the parse tree."
"During testing, the parser is run on the test sentences and the same fea-tures extracted."
Probabilities for each possi-ble semantic role r are then computed from the features.
The probability computation will be described in the next section; the fea-tures include:
"Phrase syntactic Type type : Thisof featurethe phraseindicatesexpressingthe the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S)."
"Phrase types were derived au-tomatically from parse trees generated by the parser, as shown in Figure 2."
"The wordsparse constituentannotated asspanninga frame elementeach setwasof foundlabel was, andtakenthe constituent&apos;sas the phrasenonterminaltype."
"As an example of how this feature is useful, in communication frames, the Speaker is likely appear a a noun phrase, Topic as a prepositional phrase or noun phrase, and Medium as a prepostional phrase, as in: \We talked about the proposal over the phone.&quot; When no parse constituent was found with boundaries matching those of a frame element during testing, the largest constituent beginning at the frame element&apos;s left boundary and lying calculateentirely withinthe featuresthe element. was used to Grammatical tempts to indicate Function a constituent&apos;s :"
"This featuresyntac-at-tic relation to the rest of the sentence, for example as a subject or object of a verbwas .readAs withfrom phraseparse treestype, returnedthis featureby the parser."
"After experimentation with various versions of this feature, we re-stricted it to apply only to NPs, as it was found to have little e ect on other phrase tortypeswas."
EachfoundNP&apos;sin thenearestparseStreeor;VPNPsances-with ancal Sfunctionancestor subject were givenand thosethe grammati-with a VP ancestor were labeled object .
"In general, agenthood is closely correlated with sub-jecthood\He drove. theForcarexampleover the, inclithe&quot;,sentencethe rst NP is more likely to ll the Agent role than the second or third."
"Position whether : Thisthe constituentfeature simplyto be labeledindicatesoc-cursing thebeforesemanticor afterframethe.predicateWe expectedde n-thisgrammaticalfeature tofunctionbe highly, sincecorrelatedsubjectswithwill generally appear before a verb, and objects after. may overcome theMoreovershortcomings, this offeatureread-ing grammatical function from a con-stituent&apos;s ancestors in the parse tree, as well as errors in the parser output."
"Voice passive : The verbsdistinctionplaysbetweenan importantactive androle in the connection between semantic role and grammatical function, since direct objects of active verbs correspond to sub-jects of passive verbs."
"From the parser output, verbs were classi ed as active or passive by building a set of 10 passive-identifying patterns."
Each of the pat-terns requires both a passive auxiliary (some form of \to be&quot; or \to get&quot;) and a past participle.
"Head pected Word lexical : As previouslydependenciesnotedto, webe ex-ex-tremely important in labeling semantic roles, as indicated by their importance in related tasks such as parsing."
"Since the parser used assigns each constituent a head word as an integral part of the parsing model, we were able to read the headparserwordsoutputof. theFor exampleconstituents, in afromcommu-the nication frame, noun phrases headed by \Bill&quot;, \brother&quot;, or \he&quot; are more likely to be the Speaker , while those headed by \proposal&quot;, \story&quot;, or \question&quot; are more likely to be the Topic ."
"FrameNetFor ourcorpusexperimentsas follows, :weone-tenthdividedof thethe annotated sentences for each target word were reserved as a test set, and another one-tenth ourweresystemset aside."
Aasfewa tuningtarget setwordsfor withdevelopingfewer than ten examples were removed from the cor-pus.
"In our corpus, the average number of sentences per target word is only 34, and the number of sentences per frame is 732 | both relatively small amounts of data on which to train frame element classi ers. actAlthoughin variouswewaysexpect, theourdatafeaturesare tootosparseinter-to calculate probabilities directly on the full set of features."
"For this reason, we built our classi er by combining probabilities from dis-tributions conditioned on a variety of combi-nationsAn importantof featurescaveat. in using the FrameNet database is that sentences are not chosen for annotation at random, and therefore are not necessarily statistically representative of the corpus as a whole."
"Rather, examples are cho-sen to illustrate typical usage patterns for each word."
We intend to remedy this in fu-ture versions of this work by bootstrapping ourTablestatistics2 showsusingtheunannotatedprobability distributionstext. used in the nal version of the system.
Cov-erage indicates the percentage of the test data for which the conditioning event had been seen in training data.
"Accuracy is the propor-tion of covered test data for which the correct role is predicted, and Performance , simply the product of coverage and accuracy, is the overall percentage of test data for which the correct role is predicted."
"Accuracy is some-what similar to the familiar metric of pre-cision in that it is calculated over cases for which a decision is made, and performance is similar to recall in that it is calculated over all true frame elements."
"However, unlike a tradi-tional precision/recall trade-o , these results have no threshold to adjust, and the task is a multi-way classi cation rather than a binary decision."
The distributions calculated were simply the empirical distributions from the training data.
"That is, occurrences of each role and each set of conditioning events were counted in a table, and probabilities calcu-lated by dividing the counts for each role by the total number of observations for each con-ditioning event."
"For example, the distribution P ( r j pt; t ) was calculated sas follows:"
P ( r j pt; t ) = ##( r ( pt; pt; ;t ) t ) theSometrainingsampleare shownprobabilitiesin Tablecalculated3. from
Results for di erent methods of combining the probability distributions described in the previous section are shown in Table 4.
"The thelinearprobabilitiesinterpolationgivenmethodby eachsimplyof theaveragesdistri-butions P ( r j constituent in Table 2:) = 1 P ( r j t ) ++ 4 P ( r j pt; position + ; voice ) 3 P ( r j pt; gf; t ) 2 P ( r j pt; t ) 5 P ( r j pt; position; voice; t ) + 6 P ( r j h ) ++ 7 P ( r j h; P t ) + 8 P ( r j h; pt; t ) pressedwherein the i i log= 1.domainThe geometric, is similarmean: , ex-"
P ( r j constituent ) =
Z 1 exp f 1 logP ( r j t ) ++ 4 logP ( r j pt; position + ; voice 3 logP ) ( r j pt; gf; t ) 2 logP ( r j pt; t ) + 5 logP ( r j pt; position; voice; t ) + 6 logP ( r j h ) 7 logP ( r j h; t ) 8 logP ( r j h; pt; t ) g + + thatwhere P r PZ ( r is j constituent a normalizing) =constant1. ensuring valuesThe ofresultsforshowneach indistributionTable 4 redeectnedequalfor the relevant conditioning event (but exclud-ing distributions for which the conditioning event was not seen in the training data).
The variable gf is only de ned for noun phrases.
"The roles de ned for the removing frame in the motion domain are: Agent , Theme , CoTheme (\... had been abducted with him &quot;) and Manner ."
"Other schemes for choosing values of , in-cluding giving more weight to distributions for which more training data was available, were found to have relatively little e ect."
"We attribute this to the fact that the evaluation depends only the the ranking of the probabil-ities rather than their exact values. latticeIn thewas\backoconstructed&quot; combinationover the distributionsmethod, a in Table 2 from more speci c conditioning events to less speci c, as shown in Figure 3."
The less speci c distributions were used only when no data was present for any more speci c distribution.
"As before, probabilities were combined with both linear interpolation and a geometric mean. curacyThe, whichnal systemcan beperformedcompared atto the80.4%40.9ac-% achieved by always choosing the most prob-able role for each target word, essentially chance performance on this task."
"Results for this system on test data, held out during de-velopment of the system, are shown in Table 5."
It is interesting to note that looking at a con-stituent&apos;s position relative to the target word along with active/passive information per-formed as well as reading grammatical func-tion o the parse tree.
"A system using gram-matical function, along with the head word, phrase type, and target word, but no passive information, scored 79.2%."
A similar system using position rather than grammatical func-tion scored 78.8% | nearly identical perfor-mance.
"However, using head word, phrase type, and target word without either position or grammatical function yielded only 76.3%, indicating that while the two features accom-plish a similar goal, it is important to include some measure of the constituent&apos;s syntactic relationship to the target word."
"Our nal sys-tem incorporated both features, giving a fur-ther, though not signi cant, improvement."
"As a guideline for interpreting these results, with 8176 observations, the threshold for statisti-cal signifance with p &lt; : 05 is a 1.0% absolute di erence in performance. furtherUse ofimprovementthe active/:passiveour systemfeatureusingmadepo-a sition but no grammatical function or pas-sive information scored 78.8%; adding passive information brought performance to 80.5%."
"Roughly 5% of the examples were identi ed as passive uses. dicatorsHead wordsof a constituent&apos;sproved to besemanticvery accuraterole whenin-data was available for a given head word, con rming the importance of lexicalization tributionshown in various P ( r j h;t )othercan onlytasksbe."
"Whileevaluatedthe dis-for 56.0% of the data, of those cases it gets 86.7% correct, without use of any of the syntactic features."
"In order to address the sparse coverage of lex-ical head word statistics, an experiment was carried out using an automatic clustering of head words of the type described[REF_CITE]."
A soft clustering of nouns was per-formed by applying the co-occurrence model[REF_CITE]to a large corpus of observed direct object relationships between verbs and nouns.
"The clustering was computed from an automatically parsed ver-sion of the British National Corpus, using the parser[REF_CITE]."
The ex-periment was performed using only frame el-ements with a noun as head word.
"This al-be computed aslowed a smoothed P estimate P ( r j c; nt of ; tP ) P ( r ( j ch j ;hnt ), ; sum- t ) to ming over the automatically c derived clusters c to which a nominal head word h might belong."
This allows the use of head word statistics even when the headword h has not been seen in conjunction was the target word t in the training data.
"While the unclustered nominal head word feature is correct for 87.6% of cases where data for P ( r j h; nt; t ) is available, such data was available for only 43.7% of nominal head words."
The clustered head word alone correctly classi ed 79.7% of the cases where the head word was in the vocabulary used for clustering; 97.9% of instances of nominal head words were in the vocabulary.
Adding clustering statistics for NP constituents into the full system increased overall performance from 80.4% to 81.2%.
The experiments described above have used human annotated frame element boundaries | here we address how well the frame ele-ments can be found automatically.
Exper-iments were conducted using features simi-larstituentsto thosein adescribedsentence&apos;saboveparsetotreeidentifythat werecon-likely to be frame elements.
"The system was given the human-annotated target word and the frame as inputs, whereas a full lan-guage understanding system would also iden-tify which frames come into play in a sen-tence | essentially the task of word sense disambiguation."
"The main feature used was the path from the target word through the parse tree to the constituent in question, rep-resented as a string of parse tree nonterminals linked by symbols indicating upward or down-ward movement through the tree, as shown in Figure 4."
"Figure 4: In this example, the path from the recall frame element \He&quot; to the target word \ate&quot; can be represented as NP &quot; S # VP # V, with &quot; indicating upward movement in the parse tree and # downward movement. tityTheof theothertargetfeatureswordusedand werethe identitythe iden-of the constituent&apos;s head word."
"The probabil-ity distributions calculated from the train-ing data were P ( fe j path ), P ( fe j path; t ), and P ( fe j h; t ), where fe indicates an event where the parse constituent in question is a frame el-fromementthe, path targetthewordpathtothroughthe parsetheconstituentparse tree, t the identity of the target word, and h the head word of the parse constituent."
"By vary-ing the probability threshold at which a deci-sion is made, one can plot a precision/recall curve as shown in Figure 5."
P ( fe j path;t ) performs relatively poorly due to fragmenta-tion of the training data (recall only about 30 sentences are available for each target word).
"While the lexical statistic P ( fe j h;t ) alone is not useful as a classi er, using it in linear in-terpolation with the path statistics improves results."
Note that this method can only iden-tify frame elements that have a correspond-ing constituent in the automatically gener- ated parse tree.
"For this reason, it is inter-esting to calculate how many true frame el-ements overlap with the results of the sys-tem, relaxing the criterion that the bound-aries must match exactly."
"Results for partial matching are shown in Table 6. stituentsWhen werethe automaticallyfed through theidentirole edlabelingcon-system described above, 79.6% of the con-stituents which had been correctly identi ed in the rst stage were assigned the correct role formancein the secondwhen, roughlyassigningequivalentroles to constituentsto the per-identi ed by hand."
Figure 5: Precison/Recall plot for various methods of identifying frame elements.
Recall is calculated over only frame elements with matching parse constituents.
"Our preliminary system is able to automati-cally label semantic roles with fairly high ac-curacy, indicating promise for applications in various natural language tasks."
Lexical statis-tics computed on constituent head words were found to be the most important of the fea-tures used.
"While lexical statistics are quite accurate on the data covered by observations in the training set, the sparsity of the data when conditioned on lexical items meant that allcombiningperformancefeatures."
"Whilewas thethekeycombinedto high over-sys-tem was far more accurate than any feature taken alone, the speci c method of combina-tion used was less important. ingWesemanticplan torolecontinueidentithiscationworkwithby integrat-parsing, by bootstrapping the system on larger, and more representative, amounts of data, and by attempting to generalize from the set of pred-icates chosen by FrameNet for annotation to general text."
In this paper we revisit Puste-jovsky&apos;s proposal to treat ontologi-cally complex word meaning by so-called dotted pairs.
"We use a higher-order feature logic based on Ohori&apos;s record -calculus to model the se-mantics of words like book and li-brary , in particular their behavior in the context of quanti cation and cardinality statements."
The treatment of lexical ambiguity is one of the main problems in lexical semantics and in the modeling of natural language understand-ing.
"Pustejovsky&apos;s framework of the \Gen-erative Lexicon&quot; made a contribution to the discussion by employing the concept of type coercion, thus replacing the enumeration of readings by the systematic context-dependent generation of suitable interpretations, in the case of systematic polysemies[REF_CITE]."
"Also, Pustejovsky pointed to a frequent and important phe-nomenon in lexical semantics, which at rst sight looks as another case of polysemy, but is signi cantly di erent in nature. (1) The book is blue/on the shelf. (2) Mary burned the book. (3) The book is amusing."
"Examples (1)-(4) suggest an inherent ambi-guity of the common noun book : blue , on the shelf , and burn subcategorize for a physical object, while amusing and understand require an informational object as argument. (5) and (6) are in fact ambiguous:"
The statements may refer either to the shape or the content of the book.
"However, a thorough analysis of the situation shows that there is a third read-ing where the beauty of the book as well as Mary&apos;s positive attitude are due to the har-mony between physical shape and informa-tional content."
"The action of reading, nally, is not carried out on a physical object alone, nor on a pure informational object as argu-ment, but requires an object which is essen-tially a combination of the two."
"This indi-cates a semantic relation which is conjunctive or additive in character, rather than a dis-junction between readings as in the ambiguity case."
"In addition to the more philosophical ar-gument, the assumption of a basically di er-ent semantic relation is supported by observa-tions from semantic composition."
"If the physi-cal/informational distinction in the semantics of book were just an ambiguity, (8) and (9) would not be consistently interpretable, since the sortal requirements of the noun modi er ( amusing and on the shelf , resp.) are incom-patible with the selection restrictions of the verbs burn and understand , respectively. (8) Mary burned an amusing book. (9) Mary understands the book on the shelf."
"Pustejovsky concludes that ontologically complex objects must be taken into account to describe lexical semantics properly, and he represents them as \dotted pairs&quot; made up form two (or more) ontologically simple ob-jects, and being semantically categorized as \dotted types&quot;, e.g., P I in the case of book ."
"He convincingly argues that complex types are omnipresent in the lexicon, the phys-ical/informational object distinction being just a special case of a wide range of dotted types, including container/content ( bottle ), aperture/panel ( door ) building/institution ( library )."
"The part of the Generative Lexicon con-cept which was not concerned with onto-logically complex objects, i.e., type coer-cion and co-composition mechanisms using so-called qualia information, has triggered a line of intensive and fruitful research in lexi-cal semantics, which led to progress in repre-sentation formalisms and tools for the com-putational lexicon (see e.g. ([REF_CITE]; Dolling, 1995; Busa and Bouil-lon, forthcoming;[REF_CITE]))."
"In contrast, a problem with Pustejovsky&apos;s proposal about the complex objects is that the dotted-pair notation has been formally and semantically not clear enough to form a starting point for meaning representation and processing."
"In this paper, we present a formally sound semantic reconstruction of complex objects, using a higher-order feature logic based on Ohori&apos;s record -calculus (1995) which has been originally developed for functional- and object-oriented programming."
"We do not claim that our reconstruction provides a full theory of the of the peculiar kind of ontolog-ical objects, but it appears to be useful as a basis for representing lexical entries for these objects and modeling the composition pro-cess in which they are involved."
"We will not only show that the basic examples above can be treated, but also that our treatment pro-vides a straightforward solution to some puz-zles concerning the behavior of dotted pairs in quanti cational, cardinality and identity statements. (10) Mary burned every book in the library. (11) Mary understood every book in the library. (12)[REF_CITE]books in the library. (13) All new books are on the shelf. (14) The book on your book-shelf is the one I saw in the library."
"The respective readings in (10) and (11) appear to be triggered by the sortal requirements of the verbal predicate, as the ambiguity in (12) is due to the lack of a se-lection restriction."
"In order to reduce the complexity of the calcu-lus, we will rst introduce a feature -calculus F and then extend it to F ."
"F , is an exten-sion of the simply typed -calculus by feature structures (which we will call records)."
See Figure 1 for the syntactical categories of the raw terms.
"We assume the base types e (for individu-als) and t (for truth values), and a set L = f ` 1 ; ` 2 ; : : : g of features."
The set of well-typed terms is de ned by the inference rules in Fig-ure 2 for the typing judgment ` A : .
The meaning of this judgment is that term A has type 2 T relative to the (global) type as-sumptions in the signature cal) type assumptions (the context and )thefor (thelo-variables.
"As usual, we say that a term A (and often ` A simply: is derivablewrite A byto is of type indicate this), i these rules."
"We will call a type a record type (with features ` i ), i it is of the form ff ` 1 : 1 ; : : : ; ` n : n gg ."
"Similarly, we call an F -term"
"A a record , i it has a record type."
Note that record selection operator \.&quot; can only be applied to records.
"In a slight abuse of notation, we will also use it on record types and have A :` : :` ."
"It is well-known that type inference with these rules is decidable (as a consequence we will sometimes refrain from explicitly mark-ing types in our examples), that well-typed terms have unique types, and that the calcu-lus admits subject reduction, i.e that the set of well-typed terms is closed under well-typed substitutions."
"The calculus F is equipped with an (op-erational) equality theory, given by the rules in Figure 3 (extended to congruence relations on F -terms in the usual way)."
The rst two are just the well-known equality rules from -calculus (we assume alphabetic renaming of bound variables wherever necessary).
The second two rules specify the semantics of the record dereferencing operation \ : &quot;.
"Here we know that these rules form a canonical (i.e. terminating and con uent), and type-safe (re-duction does not change the type) reduction system, and that we therefore have unique  -normal forms."
"The semantics of F is a straightforward extention of that of the sim-ply typed -calculus: records are interpreted as partial functions from features to objects, and dereferencing is only application of these functions."
"With this semantics it is easy to show that the evaluation mapping is well-typed ( I &apos; ( A ) 2 D ) and that the equalities in Figure 3 are sound (i.e. if A =  B , then I &apos; ( A ) = I &apos; ( B ))."
"Up to now, we have a calculus for so-called closed records that exactly pre-scribe the features of a record."
"The se-mantics given above also licenses a slightly di erent interpretation: a record type ff ` 1 : n ; : : : ; ` n : n gg is descriptive , i.e. an F =-term of type would only be required to have at least the features ` 1 ;::: ` n , but may actually have more."
"This makes it neces-sary to introduce a subtyping relation , since a record ff ` ="
"A gg will now have the ff ` : gg types ff ` : ffgggg , andsince ffgg the. latterOf courseis lesswerestric-have tive."
The higher-order feature logic F we will use for the linguistic analysis in section 3 is given as F extended by the rules in Fig-ure 4.
"The rst rule speci es that record types that prescribe more features are more speci c, and thus describe a smaller set of objects."
The second rule is a standard weak-ening rule for the subtype relation.
"We need the re exivity rule for base types in order to keep the last rule, which induces the subtype relation on function types from that of its do-main and range types simple."
"It states that function spaces can be enlarged by enlarg-ing the range type or by making the domain smaller (intuitively, every function can be re-stricted to a smaller domain)."
We say that is covariant (preserving the direction) in the range and contravariant in the domain type (inverting the direction).
"For F , we have the same meta-logical re-sults as for F (the type-preservations, sub-ject reduction, normal forms, soundness,. . . ) except for the unique type property, which cannot hold by construction."
"Instead we have the principal type property, i.e. every F -term has a unique minimal type."
"To fortify our intuition about F , let us take a look at the following example: It should be possible to apply a function F of type ff ` 1 : gg ! features ` 1 ; ` 2 , since F toonlya recordexpectswith ` 1 ."
The type derivation in Figure 5 shows that F ff ` 1 = A 1 1 ; ` 2 = A 2 2 gg is indeed well-typed.
"In the rst block, we use the rules from Fig-ure 4 (in particular contravariance) to estab-lish a subtype relation that is used in the sec-ond block to weaken the type of F , so that it (in the third block) can be applied to the ar-gument record that has one feature more than the feature ` 1 required by F &apos;s type."
"We start with the standard Montagovian analysis[REF_CITE], only that we base it on F instead of the simply typed -calculus."
"For our example, it will be suÆcient to take the set L of features as a superset of f P ; I ; H g (where the rst stand for physical, and informational facets of an object)."
In our fragment we use the extension F to struc-ture type e into subsets given by types of the form ff ` 1 : e; : : : ; ` n : e gg .
"Note that throw-ing away all feature information and mapping each such type to a type E in our examples will yield a standard Montagovian treatment of NL expressions, where E takes the role that e has in standard Montague grammar."
"Linguistic examples are the proper name Mary , which translates to mary0 : ff H : e gg , shelf which translates to shelf0 : ff P : e gg ! t , and the common noun book which translates to book0 : ff P : e; I : e gg ! t ."
A predicate like blue requires a physical ob-ject as argument.
"To be precise, the argument need not be an object of type ff P : e gg , like a shelf or a table. blue can be perfectly ap-plied to complex objects as books, libraries, and doors, if they have a physical realization, irrespective of whether it is accompanied by an informational object, an institution, or an aperture."
"At rst glance, this seems to be a signi cant di erence from kind predicates like shelf and book ."
"However, it is OK to interpret the type assignment for kind predicates along with property denoting expressions: In both cases, the occurrence of a feature ` means that ` occurs in the type of the argument object."
"Thus, ff ` : e gg ! t is a sortal characterization for a predicate A with the following impact: 1."
"A has a value for feature ` , possibly among other features, 2. the semantics of A is projective, i.e., the applicability conditions of A and ac-cordingly the truth value of the result-ing predication is only dependent of the value of ` ."
Note that 1. is exactly the behavior that we have built the extension F for and that we have discussed with the example in Figure 5.
We will now come to 2.
"Although type e never occurs as argument type directly in the translation of NL expres-sions, representation language constants with type- e arguments are useful in the de nition of the semantics of lexical entries."
"E.g., the semantics of book can be de ned using the basic constant book of type e ! e ! t , as : ( book ( x: P ; x: I )), where book expresses the book-speci c relation holding between physical and informational objects [Footnote_1] ."
1 Pustejovsky conjectures that the relation holding among di erent ontological levels is more than just a set of pairs. We restrict ourselves to the extensional level here.
"The fragment in Figure 6 provides represen-tations for some of the lexical items occurring in the examples of Section 1, in terms of the basic expressions mary : e; shelf ; blue ; amusing : e ! t on ; book ; burn ; understand : e ! e ! t; read : e ! e ! e ! t"
"Observe that the representations nicely re-ect the distinction between linguistic arity of the lexical items, which is given by the -pre x (e.g., two-place in the case of read ), and the \ontological arity&quot; of the underlying ba-sic relations (e.g., the 3-place-relation holding between a person, the physical object which is visually scanned, and the content which is acquired by that action)."
"In particular, all of the meanings are projective, i.e. they only pick out the features from the complex argu-ments and make them available to the basic predicate."
"Therefore, we can reconstruct the meaning term R = :read ( x: H ;y: P ; y: I ) of read if we only know the relevant features (we call them selection restrictions) of the ar-guments, and write R as read [ f H gf P ; I g ]."
The interpretation of sentence (2) via basic predicates is shown in (15) to (17).
"For sim-plicity, the de nite noun phrase is translated by an existential quanti er here. (15) shows the result of the direct one-to-one-translation of lexical items into representation language constants."
"The representations show that the functors select there appropriate ontological level locally, thereby avoiding global inconsis-tency. (18) 9 v ( book ( v: P ; v: I )) ^ ( understand ( mary ;v: I )) (19) 9 v ( book ( v: P ; v: I )) ^ ( read ( mary ; v: P ; v: I )) (20) 9 v ( book ( v: P ; v: I )) ^ amusing ( v: I ) ^ ( burn ( mary ;v: P )) (21) 9 v ( book ( v: P ; v: I )) ^ 9 ushelf ( v: P ) ^ on ( v: P ; u: P ) ^ ( understand ( mary ;v: I ))"
"The lexical items beautiful and like in (5) and (6), resp., are polysemous because of the lack of strict sortal requirements."
"They can be represented as relational expressions contain-ing a parameter for the selection restrictions which has to be instantiated to a set of fea-tures by context. like , e.g., can be translated to like [ S ] 0 , with like [ f P g ] 0 , like [ f"
"I g ] 0 , and like [ f P ; I g ] 0 as (some of the) possible readings."
Of course this presupposes the availability of a set of basic predicates like i of di erent on-tological arities.
We now turn to the behavior of non-existential quanti ers and cardinality oper-ators in combination with complex objects.
"The choice of the appropriate ontological level for an application of these operators may be guided by the sortal requirements of the predicates used (as in (10)-(12)), but as (13) demonstrates it is not determined by the lexical semantics."
"We represent quanti ers and cardinality operators as second-order re-lations, according to the theory of gener-alized quanti ers ([REF_CITE];"
Barwise rameterized by a context variable S and[REF_CITE]) and take them to be L pa-for selection restrictions in the same manner as the predicates like and beautiful .
The value of S may depend on the general context as well as on semantic properties of lexical items in the utterance.
"We de ne the semantics of a parameter-ized quanti er Qj S by applying its respec-tive basic, non-parameterized variants to the S -projections of their argument predicates P and Q to features in S , which we write as P j S and Q j S , respectively."
"Formally P j f ` 1 ;:::;` n g is  1 : : : x n : 9 u:P ( u ) ^ x 1 = u:` 1 ^ : : : ^ x n = u:` n A rst proposal is given in (22). (23) gives the representation of sentence (13) in the \bookstore reading&quot; (omitting the seman-tics of new and representing on the shelf as an atomic one-place predicate, for simplicity), (24) the reduction of (23) to ordinary quan-ti cation on the S -projections, which is equiv-alent to the rst-order formula (25), which in turn can be spelled out as (26) using basic predicates. (22) Qj S ( P; Q ) , Q ( P j S ; Q j S ) (23) every j f"
I g ( book 0 ;on shelf 0 ) (24) every book 0 j f I g ;on shelf 0 j f
I g (25) 8 x: 9 u: ( x = u: I ^ book 0 ( u )) = ) 9 v:x = v:
I ^ on shelf 0 ( v ) (26) 8 x: 9 u: ( x = u: I ^ book ( u: P ; u: I )) = ) 9 v:x = v:
I ^ on shelf ( v: P )
"As one can easily see, the instantiation of S to f I g triggers the wanted 89 reading (\for all books (as informational objects) there is a physical object on the shelf&quot;), where the in-stantiation to f P g would have given the 88 reading, since on shelf 0 is projective for P only, and as a consequence we have on shelf 0 j f P g = : 9 u:on shelf 0 ( u ) ^ x = u: P , = : 9 : 9 uu::ononshelfshelf (( ux: ) P ^ ) ^ xx == u:u P : P , :on shelf ( x )"
The extension to cases (10)-(12) is straight-forward.
The proposed interpretation may be too permissive.
"Take a situation, where new pub-lications are alternatively available as book and on CD-ROM."
"We therefore slightly modify the general scheme (22) by (27), where the re-striction of the quanti er is repeated in the nuclear scope."
"For ordinary quanti cation, this does not cause any change, because of the monotonic-ity of NL quanti ers."
"In our case of level-speci c quanti cation, it guarantees that the second argument covers only projections orig-inating from the right type of complex ob-jects."
We give the revised rst-order repre-sentation corresponding to (26) in (28). (28) 8 x: 9 u: ( x = u:
I ^ book ( u: P ; u: I )) v: I ^ book = ) ( 9 vv: P :x; v = : I ) ^ on shelf ( v: P )
"Our higher-order feature logic F provides a framework for the simple and straightfor-ward modeling of ontologically complex ob-jects, including the puzzles of quanti cation and cardinality statements."
"In this frame-work, a number of interesting empirical ques-tions can be further pursued:"
The ontology for complex objects can be in-vestigated.
"So far, we constrained ourselves to the simplest case of \dotted pairs&quot;, and may even have taken over a wrong classi cation from the literature, talking about the dualism of physical and informational objects, where a type/token distinction might have been more adequate."
"The reality about books (as well as bottles and libraries) might be more complex, however, including both the P / I distinction as well as hierarchical type/token structures."
"The linguistic selection restrictions are probably more complex than we assumed in this paper: As[REF_CITE], we may have to take distinguish exocentric and endocentric cases of dotted pairs, as well as projective and non-projective verbal pred-icates."
"Another fruitful question might be whether the framework could be used to reconsider the mechanism of type coercion in general: It may be that at least some cases of reinterpretation may be better described by adding an onto-logical level, and thus creating a complex ob-ject, rather than by switching from one level to another."
"We would like to conclude with a very gen-eral remark: The data type of feature struc-tures as employed in our formalism has been widely used in grammar formalisms, among other things to incorporate semantic informa-tion."
"In this paper, a logical framework for semantics is proposed, which itself has fea-ture structures as a part of the meaning rep-resentation."
It may be worthwhile to consider whether this property can be used to tell a new story about treating syntax and seman-tics in a uniform framework.
"With the rapid development of the Internet, writing English becomes daily work for computer users all over the world."
"However, for Chinese users who have significantly different culture and writing style, English writing is a big barrier."
"Therefore, building a machine-aided English writing system, which helps Chinese users not only on spelling checking and grammar checking but also on writing in the way of native-English, is a very promising task."
"Statistics shows that almost all Chinese users who need to write in English [Footnote_1] have enough knowledge of English that they can easily tell the difference between two sentences written in Chinese-English and native-English, respectively."
"1 Now Ting Liu is an associate professor in Harbin Institute of Technology, P.R.C."
"Thus, the machine-aided English writing system should act as a consultant that provide various kinds of help whenever necessary, and let users play the major role during writing."
"These helps include: 1) Spelling help: help users input hard-to-spell words, and check the usage in a certain context simultaneously; 2) Example sentence help: help users refine the writing by providing perfect example sentences."
Several machine-aided approaches have been proposed recently.
"They basically fall into two categories, 1) automatic translation, and 2) translation memory."
Both work at the sentence level.
"While in the former, the translation is not readable even after a lot of manually editing."
"The latter works like a case-based system, in that, given a sentence, the system retrieve similar sentences from translation example database, the user then translates his sentences by analogy."
To build a computer-aided English writing system that helps Chinese users on writing in the way of native-English is a challenging task.
"Machine translation is widely used for this purpose, but how to find an efficient way in which human collaborates well with computers remains an open issue."
"Although the quality of fully automatic machine translation at the sentence level is by no means satisfied, it is hopeful to provide relatively acceptable quality translations at the word or short phrase level."
"Therefore, we can expect that combining word/phrase level automatic translation with translation memory will achieve a better solution to machine-aided English writing system [[REF_CITE]]."
"In this paper, we propose an approach to machine aided English writing system, which consists of two components: 1) a statistical approach to word spelling help, and 2) an information retrieval based approach to intelligent recommendation by providing suggestive example sentences."
"Both components work together in a unified way, and highly improve the productivity of English writing."
"We also develop a pilot system, namely PENS."
Preliminary experiments show very promising results.
The rest of this paper is structured as follows.
"In section 2 we give an overview of the system, introduce the components of the system, and describe the resources needed."
"In section 3, we discuss the word spelling help, and focus the discussion on Chinese pinyin to English word translation."
"In addition, we describe various kinds of word level help functions, such as automatic translation of Chinese word in the form of either pinyin or Chinese characters, and synonym suggestion, etc."
We also describe the user interface briefly.
"In section 4, an effective retrieval algorithm is proposed to implement the so-called intelligent recommendation function."
"In section 5, we present preliminary experimental results."
"Finally, concluding remarks is given in section 6."
There are two modules in PENS.
The first is called the spelling help.
"Given an English word, the spelling help performs two functions, 1) retrieving its synonym, antonym, and thesaurus; or 2) automatically giving the corresponding translation of Chinese words in the form of Chinese characters or pinyin."
"Statistical machine translation techniques are used for this translation, and therefore a Chinese-English bilingual dictionary (MRD), an English language model, and an English-Chinese word- translation model (TM) are needed."
"The English language model is a word trigram model, which consists of 247,238,396 trigrams, and the vocabulary used contains 58541 words."
"The MRD dictionary contains 115,200 Chinese entries as well as their corresponding English translations, and other information, such as part-of-speech, semantic classification, etc."
"The TM is trained from a word-aligned bilingual corpus, which occupies approximately 96,362 bilingual sentence pairs."
The second module is an intelligent recommendation system.
It employs an effective sentence retrieval algorithm on a large bilingual corpus.
"The input is a sequence of keywords or a short phrase given by users, and the output is limited pairs bilingual sentences expressing relevant meaning with users’ query, or just a few pairs of bilingual sentences with syntactical relevance."
"We have collected bilingual texts extracted from World Wide Web bilingual sites, dictionaries, books, bilingual news and magazines, and product manuals."
"The size of the corpus is 96,362 sentence pairs."
The corpus is used in the following three cases: 1)Act as translation memory to support the Intelligent Recommendation Function; 2) To be used to acquire English-Chinese translation model to support translation at word and phrase level; 3) To be used to extract bilingual terms to enrich the Chinese-English MRD;
"To construct a sentence aligned bilingual corpus, we first use an alignment algorithm doing the automatic alignment and then the alignment result are corrected."
There have been quite a number of recent papers on parallel text alignment.
Lexically based techniques use extensive online bilingual lexicons to match sentences [[REF_CITE]].
"In contrast, statistical techniques require almost no prior knowledge and are based solely on the lengths of sentences, i.e. length-based alignment method."
We use a novel method to incorporate both approaches [[REF_CITE]].
"First, the rough result is obtained by using the length-based method."
Then anchors are identified in the text to reduce the complexity.
An anchor is defined as a block that consists of n successive sentences.
Our experiments show best performance when n=3.
"Finally, a small, restricted set of lexical cues is applied to obtain for further improvement."
"Chinese sentences must be segmented before word translation training, because written Chinese consists of a character stream without space between words."
"Therefore, we use a wordlist, which consists of 65502 words, in conjunction with an optimization procedure described in [[REF_CITE]]."
The bilingual training process employs a variant of the model in [[REF_CITE]] and as such is based on an iterative EM (expectation-maximization) procedure for maximizing the likelihood of generating the English given the Chinese portion.
"The output of the training process is a set of potential English translations for each Chinese word, together with the probability estimate for each translation."
A domain-specific term is defined as a string that consists of more than one successive word and has certain occurrences in a text collection within a specific domain.
"Such a string has a complete meaning and lexical boundaries in semantics; it might be a compound word, phrase or linguistic template."
We use two steps to extract bilingual terms from sentence aligned corpus.
"First we extract Chinese monolingual terms from Chinese part of the corpus by a similar method described in [[REF_CITE]], then we extract the English corresponding part by using the word alignment information."
A candidate list of the Chinese-English bilingual terms can be obtained as the result.
Then we will check the list and add the terms into the dictionary.
The spelling help works on the word or phrase level.
"Given an English word or phrase, it performs two functions, 1) retrieving corresponding synonyms, antonyms, and thesaurus; and 2) automatically giving the corresponding translation of Chinese words in the form of Chinese characters or pinyin."
We will focus our discussion on the latter function in the section.
"To use the latter function, the user may input Chinese characters or just input pinyin."
It is not very convenient for Chinese users to input Chinese characters by an English keyboard.
Furthermore the user must switch between English input model and Chinese input model time and again.
These operations will interrupt his train of thought.
"To avoid this shortcoming, our system allows the user to input pinyin instead of Chinese characters."
The pinyin can be translated into English word directly.
Let us take a user scenario for an example to show how the spelling help works.
"Suppose that a user input a Chinese word “  ” in the form of pinyin, say “wancheng”, as shown in figure1-1."
PENS is able to detect whether a string is a pinyin string or an English string automatically.
"For a pinyin string, PENS tries to translate it into the corresponding English word or phrase directly."
"The mapping from pinyin to Chinese word is one-to-many, so does the mapping from Chinese word to English words."
"Therefore, for each pinyin string, there are alternative translations."
PENS employs a statistical approach to determine the correct translation.
"PENS also displays the corresponding Chinese word or phrase for confirmation, as shown in figure 1-2."
"If the user is not satisfied with the English word determined by PENS, he can browse other candidates as well as their bilingual example sentences, and select a better one, as shown in figure 1-3."
"Suppose that a user input two English words, say EW 1 and EW 2 , and then a pinyin string, say PY."
"For PY, all candidate Chinese words are determined by looking up a Pinyin-Chinese dictionary."
"Then, a list of candidate English translations is obtained according to a MRD."
"These English translations are English words of their original form, while they should be of different forms in different contexts."
"We exploit morphology for this purpose, and expand each word to all possible forms."
"For instance, inflections of “go” may be “went”, and “gone”."
"In what follows, we will describe how to determine the proper translation among the candidate list."
Figure 2-1: Word-level Pinyin-English
"As shown in Figure 2-1, we assume that the most proper translation of PY is the English word with the highest conditional probability among all leaf nodes, that is According to Bayes’ law, the conditional probability is estimated by"
"For simplicity, we assume that a Chinese word doesn’t depends on the translation context, so we can get the following approximate equation:"
"P(CW i | EW ij , EW 1 , EW 2 ) ≈ P(CW i | EW ij )"
"We can also assume that the pinyin of a Chinese word is not concerned in the corresponding English translation, namely:"
"P(PY | CW i , EW ij , EW 1 , EW 2 ) ≈ P(PY | CW i )"
"It is almost impossible that two Chinese words correspond to the same pinyin and the same English translation, so we can suppose that:"
"P(CW i | PY , EW ij , EW 1 , EW 2 ) ≈ 1"
"Therefore, we get the approximation of (2-3) as follows:"
"P(PY | EW ij , EW 1 , EW 2 ) = (2-4) P(CW i | EW ij )× P(PY | CW i )"
"According to formula (2-2) and (2-4), we get:"
"P(EW ij | PY,EW 1 ,EW 2 ) = (2-5) P(CW i | EW ij )×P(PY|CW i )×P(EW ij | EW 1 ,EW 2 ) where P(CW i |EW ij ) is the translation model, and can be got from bilingual corpus, and P(PY | CW i ) is the polyphone model, here we suppose P(PY|CW i ) = 1, and P(EW ij | EW 1 , EW 2 ) is the English trigram language model."
"To sum up, as indicated in (2-6), the spelling help find the most proper translation of PY by retrieving the English word with the highest conditional probability."
EW ij (2-6) arg max P(CW i | EW ij )×
"P(EW ij | EW 1 , EW 2 )"
The intelligent recommendation works on the sentence level.
"When a user input a sequence of Chinese characters, the character string will be firstly segmented into one or more words."
The segmented word string acts as the user query in IR.
"After query expansion, the intelligent recommendation employs an effective sentence retrieval algorithm on a large bilingual corpus, and retrieves a pair (or a set of pairs) of bilingual sentences related to the query."
All the retrieved sentence pairs are ranked based on a scoring strategy.
"Suppose that a user query is of the form CW 1 , CW 2 , … , CW m ."
"We then list all synonyms for each word of the queries based on a Chinese thesaurus, as shown below."
We can obtain an expanded query by substituting a word in the query with its synonym.
"To avoid over-generation, we restrict that only Let us take the query “  ” for an example. one word is substituted at each time."
The  =  synonyms list is as follows: …… = …….
The query consists of two words.
"By substituting the first word, we get expanded queries, such as “  ” “  ” “  ”, etc, and by ” “  substituting the second word, we get other expanded queries, such as “  ” “  ”, etc."
"Then we select the expanded query, which is used for retrieving example sentence pairs, by estimating the mutual information of words with the query."
"It is indicated as follows m arg max ∑ MI (CW ,CW ) k ij i, j k =1 k ≠ i where CW k is a the kth Chinese word in the query, word."
"In the above example, “   ” is and CW ij is the jth synonym of the i-th Chinese selected."
The selection well meets the common sense.
"Therefore, bilingual example sentences containing “   ” will be retrieved as well."
"The input of the ranking algorithm is a query Q, as described above, Q is a Chinese word string, as shown below"
"Q= T 1 ,T 2 ,T 3 ,…T k"
"The output is a set of relevant bilingual example sentence pairs in the form of, &gt;  Relevance(Q,Engsent) &gt;  S={(Chinsent, Engsent) | Relevance(Q,Chinsent) Engsent is an English sentence, and  where Chinsent is a Chinese sentence, and"
"For each sentence, the relevance score is computed in two parts, 1) the bonus which represents the similarity of input query and the target sentence, and 2) the penalty, which represents the dissimilarity of input query and the target sentence."
The bonus is computed by the following formula:
Bonusi = ∑ log(W × tfij ) × log(n / df j ) / L j i j = 1
"W j is the weight of the jth word in query Q, which will be described later, tf ij is the number of the jth word occurring in sentence i, n is the number of the sentences in corpus, df j is the number of sentence which contains Wj, and L i is the number of word in the ith sentence."
The above formula contains only the algebraic similarities.
"To take the geometry similarity into consideration, we designed a penalty formula."
The idea is that we use the editing distance to compute that geometry similarity.
R i =
Bonus i − Penalty i Suppose the matched word list between query Q and a sentence are represented as A and B respectively
"A 1 , A 2 , A 3 , … , A l B 1 , B 2 , B 3 , … , B m"
The editing distance is defined as the number of editing operation to revise B to A.
"The penalty will increase for each editing operation, but the score is different for different word category."
"For example, the penalty will be serious when operating a verb than operating a noun h Penalty i = ∑ log(W &apos; × E j ) × log( n / df j ) /"
L j i j = 1 where W j ’ is the penalty of the jth word E j the editing distance We define the score and penalty for each kind of part-or-speech
"In this section, we will report the primary experimental results on 1) word-level pinyin-English translation, and 2) example sentences retrieval."
"Firstly, we built a testing set based on the word aligned bilingual corpus automatically."
"Suppose that there is a word-aligned bilingual sentence pair, and every Chinese word is labelled with Pinyin."
See Figure 4-1.
If we substitute an English word with the piny
"If we substitute an English word with the pinyin of the Chinese word which the English word is aligned to, we can get a testing example for word-level Pinyin-English translation."
"Since the user only cares about how to write content words, rather than function words, we should skip function words in the English sentence."
"In this example, suppose EW 1 is a function word, EW 2 and EW 3 are content words, thus the extracted testing examples are:"
"EW 1 PY 2 (CW 2 , EW 2 ) EW 1 EW 2 PY 4 (CW 4 , EW 3 )"
The Chinese words and English words in brackets are standard answers to the pinyin.
We can get the precision of translation by comparing the standard answers with the answers obtained by the Pinyin-English translation module.
"The standard testing set includes 1198 testing sentences, and all the pinyins are polysyllabic."
The experimental result is shown in Figure 4-2.
We built a standard example sentences set which consists of 964 bilingual example sentence pairs.
We also created 50 Chinese-phrase queries manually based on the set.
Then we labelled every sentence with the 50 queries.
"For instance, let’s say that the example sentence is !&quot;#$%&amp;&apos;()* +,-./ (He drew the conclusion by building on his own investigation.)"
"After ( ) labelling * ”, and,“the + corresponding -. ”, that queriesis, whenarea user“ &apos; input these queries, the above example sentence should be picked out."
"After we labelled all 964 sentences, we performed the sentence retrieval module on the sentence set, that is, PENS retrieved example sentences for each of the 50 queries."
"Therefore, for each query, we compared the sentence set retrieved by PENS with the sentence labelled manually, and evaluate the performance by estimating the precision and the recall."
"Let A denotes the number of sentences which is selected by both human and the machine, B denotes the number of sentences which is selected only by the machine, and C denotes the number of sentences which is selected only by human."
"The precision of the retrieval to query i, say Pi, is estimated by Pi = A / B and the recall Ri, is estimated by Ri = A/C. The average precision"
"The experimental results are P = 83.3%, and R = 55.7%."
"The user only cares if he could obtain a useful example sentence, and it is unnecessary for the system to find out all the relevant sentences in the bilingual sentence corpus."
"Therefore, example sentence retrieval in PENS is different from conventional text retrieval at this point."
"In this paper, based on the comprehensive study of Chinese users requirements, we propose a unified approach to machine aided English writing system, which consists of two components: 1) a statistical approach to word spelling help, and 2) an information retrieval based approach to intelligent recommendation by providing suggestive example sentences."
"While the former works at the word or phrase level, the latter works at the sentence level."
"Both components work together in a unified way, and highly improve the productivity of English writing."
"We also develop a pilot system, namely PENS, where we try to find an efficient way in which human collaborate with computers."
"Although many components of PENS are under development, primary experiments on two standard testing sets have already shown very promising results."
"As an application of NLP to computer-assisted language learn-ing(CALL) , we propose a diag-nostic processing of Japanese be-ing able to detect errors and inap-propriateness of sentences composed by the students in the given situ-ation and the context of the exer-cise texts."
"Using LTAG(Lexicalized Tree Adjoining Grammar) formal-ism, we have implemented a proto-type of such a diagnostic parser as a component of a CALL system being developed."
"In the recent classroom of second language learning, communicative approach(H.G.[REF_CITE]) is promoted in which it mat-ters for the students to become aware of the language use, i.e. the functionality of lan-guage usage and it’s dependence on the sit-uations and the contexts of communication."
"In order to achieve the objective according to “constructivistic” point of view of learning (T.M.[REF_CITE]), the students are en-couraged to produce sentences by themselves in various situations and contexts and guided to recognize by themselves the erroneous or inappropriate functions of their misused ex-pressions."
"We have already proposed a Computer-Assisted Language Learning(CALL) system (N.[REF_CITE]) which provides the stu-dents with sample texts promoting their re- flection on the errors and inappropriateness, detected by a diagnostic parser, of the sen-tences composed by the students filling the blanks set up in the given contexts and situ-ations."
In this paper we report on prototyp-ing the diagnostic parser implemented using LTAG formalism as a component of the sys-tem.
Tree Adjoining Gram-mar) is a lexicalized grammatical formalism (XTAG[REF_CITE]).
"For ease of diagnosing the erroneous sentences com-posed by the students, lexicalized type of grammars seemed most suitable."
"Comparing HPSG(Head-driven Phrase Structure Gram-mar) (C.[REF_CITE]) and LTAG, the well-known two (almost-)lexicalized gram-mars, LTAG looked more simple and espe-cially convenient for sentence generation nec-essary in diagnosis."
LTAG systematically as-sociates an elementary tree structure with a lexical anchor and the structure is embedded in the corresponding lexical item.
"Associated with each of the external nodes of the embed-ded tree structure are feature structures such as inflection, case information, head symbol, semantic constraints as well as a difference list for surface expressions."
These features have their origin in the anchored lexical item.
"The feature information can, moreover, in-clude the knowledge of situated language use."
Appearance of the features at the external nodes of the lexical items greatly facilitates generation of local phrases which is indispens-able in diagnostic parsing.
These are the rea-son why we employed LTAG.
Preference of unification to all-procedural handling excluded the so-called “ dependency grammar ”(M.[REF_CITE]).
Japanese phrases are classified in the first place into two categories: Yougen phrase(YP) and Taigen phrase(TP).
"A YP or TP has a Yougen or a Taigen, respectively, as it’s head word."
Yougen along with Taigen as categories belong to the category of se-mantically self-contained (called autonomous) words.
"The words, e.g. verbs, adjectives, be-longing to Yougen have inflections, whereas the words. e.g. nouns, pronouns, demonstra-tives, belonging to Taigen have no inflection."
A YP or TP consists of a head word and its sibling phrases on it’s left semantically modi-fying the head word.
"And such a phrase in its turn can semantically modify an autonomous word by way of attaching a connective to it’s right, forming a phrase, or inflecting the head word of the modifier."
"In general, a sentence is constructed by at-taching to a phrase a few (or void of) func-tional words expressing the attitude of the lo-cutor to the proposional part of the phrase ( modality ) and intention of the locution af-fecting the listener ( illocutionary-act marking )."
Fig.1 shows Elementary Trees of LTAG we defined for Japanese.
"Each node is expressed by a predicate for-malism, in general, as following,"
"For example, “ ” is a self-contained (autonomous) word and its lexical item, com-prising an initial tree, is expressed by,"
"Note that tense, aspect, polite expressions, “Ren-you (te)” are dealt with as inflections just as in the classes teaching Japanese as Sec-ond Language."
"The lexical items are classi-fied into several categories such as auto, link, prio, post, compo, according to the embed-ded tree structures."
"In LTAG, 2 tree operations are defined(See Fig. 2)."
A node of a tree is said to be substi-tuted by another tree if the root node of the latter is successfully unified with the node.
"A tree is said to be adjoined with another tree if it is successfully inserted into the lat-ter by unifying the root node and the foot node(marked ∗) of the former, respectively, with the separated nodes of the latter, all with a same syntactic category."
"In Japanese, a Yougen requires as ad- joined modifiers Taigen phrases with connec-tives(e.g."
"Fig. 2 (1)) corresponding to the mandatory “ cases ” ( e.g. Fig.2 (2) ), and it also require have those corresponding to the optional “cases”."
The default order of the case phrases may be changed for the purpose of stress-ing or avoiding unintended modification.
The change can be dealt with by way of permuta-tion in unification.
"Another type of phrase to modify the Yougen is YP plus one of the connectives de-noting cause, reason-why, condition etc.(e.g. Fig.3 (4))."
A Yougen may be modified by a YP (Yougen Phrase) with its head Yougen in-flection in Ren-you form without any connec-tive(e.g. Fig.3 (3)).
A Taigen is mostly modified by a YP (Yougen Phrase) with its head Yougen in-flected in Rentai form with no connective(e.g. Fig.3 (2)).
"For ease and uniformity of processing, es-pecially in the diagnostic parser, null connec-tives λ-Ren-you and λ-Rentai are introduced when a YP modifies Yougen and Taigen, re-spectively, by way of inflection(e.g. Fig.3 (3), (2) )."
"The other type of phrase to modify the Taigen is TP plus connective “ (no)” de-noting proprietary, kinship or whole-part re-lationship(e.g. Fig.3 (1))."
"By incorporating into the feature structure an additional item expressing situational con-straints, the parser has the capability of diag-nosing usage of situation-dependent Japanese expressions such as giving and receiving ben-efits as well as demonstratives."
"As for demon-stratives, e.g. “ (kono-hon) ”, “ (sono-hon) ”, “ (ano-hon) ” indi-cates a book located either in the territory of the locuter, the listener, or outside the both, respectively."
"In the case of expression for giving and re-ceiving benefits, for example as shown in Ta- ble 1, the empathy relational constraints are embedded in each of the lexical items for the underlined word along with the case informa-tion for “ (ga)”, “ (ni)”"
"Though the indicated three expressions have the same propositional function of ex-pressing giving-benefit whose giver is x and givee is y, “camera” is placed on the side of x, y, y with “angles” towards y, x, x respec-tively."
It is seen that the camera angle deter-mines the requirement to the empathy rela-tions(S.[REF_CITE]).
"Suppose the situation E(X|Z) &lt; E(Y |Z) is given, where X, Y , Z stand for “the nurse”, “the locutor’s son”, “the locutor”, respec-tively, for instance, the parser can diagnose the following."
"The above-mentioned expressions for giving and receiving, e.g. “ ” yonde-morau , is an example of “composite verbs” in Japanese."
Many composite verbs can be produced with a considerable number of auxiliary verbs preceded by different main verbs.
"Because of the modification of the sense and the case control due to the auxiliary compo-nent, as illustrated in the case information column in Table1, we are forced to generate the composite tree (See Fig.4), carrying out modification of the meaning and the case con-trol, before adjoining of modifiers to the com-posite verb takes place."
"In Japanese, “modality words”are func-tional words expressing the attitude of the lo-cutor towards the propositional part of the ut-terance, “illocutionary-act markers” demands answer from the listener or expresses other in-tention of the locution affecting the listener."
Some combinations of certain adverbs and a “modality word” co-occur in the position interposing that part of the proposition in which the locutor has concern.
"The example shown in Fig.5, “ ”(darou) is a modal-ity word expressing locutor’s supposition, and “ ”(osoraku) expresses the extent of his confidence on the supposition."
The lexi-cal item for the latter includes the demand for the modality semantics of the locutor’s sup-position.
"English : It will probably rain tomorrow, I’m sure."
"In Japanese, TP plus connective “ ”(wa) is frequently used."
"It is said that there are two kinds of usage of connective “ ” ; the one in-troduces the theme of the sentence, the other discriminatorily presents one of the cases of the head Yougen as shown, respectively, in the following cases."
"In distinguishing between usage1. and us-age2. , we focus on the head Yougen of YP."
"If it has any unfilled-case, and the semantic con-straint of the Taigen before connective “ ” corresponds to that of one of the unfilled-cases, then our processor regards “ ” as discriminatory."
"Otherwise, “ ” is considered as introduc-ing the theme of the sentence."
"For implementing a parser for Japanese, a stack memory can be conveniently employed. ]"
"In processing the sentence from left to right, the candidate modifier phrases are kept in a stack memory until a possible Yougen or Taigen word appears and inspected if they can modify the word."
"The tree-structured features of the candidate modifier phrases popped up one by one from the stack are tried to be unified with those of the word, and the features of the phrases as far as the tree adjoining unification succeeds are inte-grated with the features of the modified word, to make a Saturated Initial Tree(SIT)."
The rest of the phrases of the stack are left there to be tested on the next Yougen or Taigen word which will appear later on.
Any ordering of modifiers is syntactically permitted except when an undesired modification takes place. \
"If a connective is found by reading one word ahead, the thus-far made SIT substi-tutes the left external node of the tree of the connective to make a Saturated Auxiliary Tree(SAT) provided unification succeeds(e.g."
"If the read ahead is a modality word, its yp node is substituted by the yp root of the SIT, and after interposing modality modifiers having been processed, the resulting phrase is considered SIT anew and the procedure goes to \ ."
"If the read ahead is an illocutionary-act marker or the ending sentence symbol, and the inflection of SIT is appropriate, parsing terminates."
Otherwise either of the λ-Ren-you or λ-Rentai connectives is attached de-pending on the inflection of the head of the SIT to make a SAT ( See Fig. 3 and Fig. 7).
"In either cases as well as the case with a non-null connective, the SAT is pushed into the stack and the procedure recurs to ]."
"We describe here our algorithm for gener-ating a sentence when the semantic relation- ship, for example as in Fig8, is given."
The generation process progresses as illustrated in Fig9.
The main stream of our generation algo-rithm follows.
"At first, from the lexical database, an au-tonomous word is fetched, whose semantic re-lationship term is unifiable with the root of the given semantic relationship."
"Letting the root and terminal node of the word be the first and the second arguments, respectively. generate2 is called • If the first argument can be unified with the second argument, generation is termi-nated."
"Otherwise, the process, carrying over the second argument, searches for a prio or link word whose root node can be unified with the first argument. • If a prio word is found, letting its right ( foot ) node be the first argument and re-taining the second argument, generate2 is called. • If a link word is found, an autonomous word is searched for whose root node can be unified with the left ( substitution ) node of the link word."
"Letting the word’s root and the terminal node be the first argument and the second argument, respectively, generate2 is called."
"Let-ting the right ( foot ) node of the link word be the first argument and retain-ing the second argument, generate2 is called."
"In the following, searching of the au-tonomous word and handing their 2 nodes off to generate2 are dealt with by generate1 predicates. generate2(Root,Terminal):-link(W,Root,Left,Right), generate1(Left), generate2(Right,Terminal)."
"In the case of generation including modality words, illocutionary-act markers or composite verbs, the algorithm needs a little more com-plicated procedures. 4 Case and Semantic Pro-cessing in Parsing and Generation"
"In parsing and generation, case and seman-tic processing occurs by unification without any procedural programming."
The initial tree structure of the lexical item of an autonomous word consists of a root node and a terminal node.
"Especially in the YP initial tree, the root node has a filled used-case slot and a variable unused-case slot as well as a variable semantic slot whose head part is filled."
The terminal node has the null used-case slot and the filled unused-case slot as well as the semantic slot consisting only of the head predicate.
"In parsing, following the process as illus-trated in Fig. 9 bottom up, when the foot YP node of a YP SAT ( e.g. ) is unified with the terminal node of a Yougen autonomous word ( e.g. ) , the case data, if any, ( e.g. [Y,[ (Y)], ]) corresponding to the SAT is moved from the unused-case slot to the used-case slot in the SAT root node."
The semantic data from the SAT is integrated with that of the word and transferred to the SAT root.
"The foot YP node of another YP SAT if any, ( e.g. ) is unified with the said root node the corresponding case data, if any, ( e.g. [Z,[ (Z)], ]) is further moved from the unused-case slot to the used-case slot."
The semantic data from the new SAT is joined with that in the previous SAT root in the root of the new SAT.
"Likewise proceeding, finally, by unifying the concatenated SAT with the root of the origi-nal autonomous word ( e.g. ), there re-mains in the unused case slot those case datas with no corresponding SAT which may be ex-plained by omitted SATs or the slash case whose entity will be found in the Taigen word to be modified by the thus-constructed mod-ifying YP."
The whole semantic data from the SATs is integrated in the root node of the original au-tonomous word.
"The process of adjoining TP SATs ( e.g. ) to modify a Taigen au-tonomous word ( e.g. ) is similar to that for YP SATs to a Yougen word, except that no case data processing occurs."
"In generation, following the process as il-lustrated in Fig. 9 top-down, when the whole given semantic relationship is unified into the semantic slot of a Yougen autonomous word ( e.g. ), and if e.g. a link word ( e.g. ) is found with its root unifiable with the root of the Yougen initial tree, the semantic expression is di-vided into two parts thanks to the case data ( e.g. [Z,[ ] ), the one(Z)], part ( e.g. [ ( ,Z, , )]) is trans-ferred to the right node, and the other part ( e.g. [ (X,Z,Y),[ (Y),[ (U,Y), ( ,U)] ]] ) transferred to the left ( foot ) node."
"From the case data of the used-case slot of the original yougen ( e.g. ), the case data corresponding to the link word ( e.g. ) is moved from the used-case slot to the unused-case slot in the left ( foot ) node."
That part of semantics transferred to the right node is processed to find the correspond-ing surface expression ( e.g. ) by con-structing an SIT.
"The other part of seman-tics sent to the left ( foot ) node along with the remaining used-case slot ( e.g. [[Y,[ (Y)], ]] ) are made use of for finding a link word ( e.g. ) whose root node is unifiable with the said left ( foot ) node."
"The semantics sent to the new link root node is divided into two parts; the one part ( e.g. [ (Y),[ (U,Y), ( ,U)]]) sent to the right node to form SIT and con-struct the corresponding surface expression ( e.g. ), the other part ( e.g. [ (X,Z,Y)]) sent to the left ( foot ) node."
"Likewise proceeding, when all the used-case data is transferred into the unused-case slot in the foot node, it may be unified with the terminal node of the original yougen ( e.g. ) , terminating the generation."
"In our CALL system, the students are asked to fill in the blanks for composition in the given situation and context, using words from a given list."
Therefore no morphological anal-ysis is needed.
"In diagnosing the students’ sentence, we assume that the following data is available for constraining processing. • Semantic elements and their relation-ships, which should be expressed by the sentence with which the students are asked to fill the blanks. • The list of words, to be used in the com-position, corresponding to the semantic elements."
Fig.10 is an example of relationships of se-mantic elements represented by a tree struc-ture.
"Modifying elements are placed as the children of the parent, the modified elements."
The list of the words to be used for expressing an element is linked to the element.
"After an SIT has been constructed, the di-agnostic parser consults the lexicon with the succeeding word."
"If it is a connective, the parser tries substitution operation with SIT and, if successful, appends it to the SIT to form the temporary SAT."
"In case the parser fails to append the connective to the SIT, only the surface expression of the connective along with the SIT is recorded in the provisional SAT."
Suppose the succeeding word was not a connective.
"If it was a Taigen or Yougen and the SIT is yp and its inflections is Rentai or Ren-you, respectively, then λ-Rentai or λ- Ren-you is appended to the SIT to form an SAT, even though the inflection might be in-correct."
"If the inflection of the SIT is incon-sistent with the succeeding word or the SIT is tp, as no reasonable interpretation is possible, “Pending Connective” µ is appended to the SIT to make an SAT."
"In all of the above-mentioned cases, the obtained SAT is pushed into the stack."
"When the parser encounters a Yougen word[]] or a Taigen word, it pops up one SAT after another from the stack and examines, locally generating surface expres-sions, if it conforms with one of the semantic children to the parent corresponding to the target Yougen/Taigen word."
"If it does, the parser adjoins the SAT to the word, after, if necessary, having corrected wrong/missing connective or wrong inflection of the SAT, thus making an SIT, including error correc-tion messages if any."
"If the popped SAT does not conform with any of the semantic chil-dren, it is pushed into a temporary stack, recording the SAT as a false modifier if SAT can be falsely adjoined to the Yougen/Taigen word."
"In case of SAT accompanying µ, the parser, consulting the semantic relationship tree data, generating a related phrase, either replaces µ with a suitable missing connec-tive and/or corrects the wrong inflection if necessary."
"When an SAT is popped up which conforms with one of the semantic children, the SATs held in temporary stack at that in-stance, if any, should have been obstacles for the popped up SAT to modify the target word."
And they are marked “?”.
"After all the SATs in the main stack have been examined, the SATs recorded in the temporary stack are returned into the main stack."
And then the SAT constructed as explained in the above is pushed into the main stack.
"If, later on, the SATs marked “?” are found to modify a tar-get word, conforming to the semantic relation-ship, they are commented as causing modifi-cation crossover."
"Finally, if the semantic relationship requires modality expression(s) and/or illocutionary-act marker(s), the thus-far-made Yougen SIT is (recursively if neces-sary) substituted into the yp node of the ex-pression(s) and, at the same time, correspond-ing modifiers of the expression(s) are looked for in the main stack to be popped making an SIT."
"If at []], the found Yougen word is a part of a composite verb the semantic relationship requires, the rest is looked for, supplemented if lacking, the case information is modified if necessary, and the same procedures follow as described after []]."
"For example, supposing the student had in-put the sentence shown in Fig.11, the parser could detect the errors by using the seman-tic relationship aforementioned in Fig.10 and the relation of the degrees of empathy in the given situation."
The detected errors are listed in the follow-ing.
"Inappropriate placing “ ”(watashi no), causing the phrase to modify “ ”(hobo-san)."
Missing connective “ ”(ga) which “ ”(hobo-san) must have for the phrase to be adjoined to “ ”(yo-nde kureru). obstacle for modification : “ ” (hobo-san) is in the place of obstacle for “ ”(watashi no) to mod-ify “ ”(musuko). wrong inflection : “ ”(yo-mi) has to be replaced by “ ”(yo-nde) for the verb to form a composite verb together with auxiliary verb “ ”(kureru) expressing giv-ing benefit. wrong connective :
Wrong connective “ ”(de) has to be replaced by “ ”(wo) which “ ”(hon) must have for the phrase to be adjoined to “ ”(yo-nde kureru). modification crossover :
The sentence has a modification crossover between “ ”(watashi no musuko) and “ ”(hobo-san ga yo-nde kureru). inappropriate situational expression :
Use of “ ”(ageru) in the given sit-uation designates empathy relation
E(nurse|locutor) &gt; E(the locutor 0 s son|locutor) which contradict with the given empa-thy relation.
It requires less number of corrections for “ ” to be re-placed by “ ”(kureru) for conform-ing with the relation and retaining “ ”(musuko ni) than to be replaced by “ ”(morau).
We proposed a diagnostic processing of Japanese and described its procedures in de-tail.
"The parser makes use of LTAG formalism introducing several additional data structure such as SIT, SAT, null/pending connectives."
The diagnosis we reported here is local in principle.
"Referring to the given relationship of semantic elements, the error is detected and corrected locally."
The correction mes-sages are generated and recorded locally in SITs.
"The undesired modifications in the stu-dent sentence, however, can be detected and commented on."
"Our CALL system, based on the detected errors and inappropriateness, provides the students with sample texts which will enable the students to correct their sen-tence by themselves."
"The tasks to be achieved are: 1. to establish ontology of semantic rela-tionship description, 2. efficient methodology for preparing the lexical items comprising semantic con-straints, 3. to communicate semantic contexts and situations to the students through assist-ing reading the texts by way of bidirec-tionally linking the text words with an electronic dictionary, 4. to deal with anaphora."
Most corpus-based approaches to natural language processing su er from lack of training data.
This is because acquiring a large num-ber of labeled data is expensive.
This paper describes a learning method that exploits unlabeled data to tackle data sparseness problem.
The method uses committee learn-ing to predict the labels of unla-beled data that augment the exist-ing training data.
Our experiments on word sense disambiguation show that predictive accuracy is signi -cantly improved by using additional unlabeled data.
The objective of word sense disambiguation (WSD) is to identify the correct sense of a word in context.
"It is one of the most critical tasks in most natural language applications, including information retrieval, information extraction, and machine translation."
"The availability of large-scale corpus and various machine learning algorithms enabled corpus-based approach to WSD[REF_CITE],but a large scale sense-tagged corpus or aligned bilingual corpus is needed for a corpus-based approach."
"However, most languages except English do not have a large-scale sense-tagged cor-pus."
"Therefore, any corpus-based approach to WSD for such languages should consider the following problems:"
There&apos;s no reliable and available sense-tagged corpus.
Most words are sense ambiguous.
"Annotating the large corpora requires human experts, so that it is too expen-sive."
"Because it is expensive to construct sense-tagged corpus or bilingual corpus, many re-searchers tried to reduce the number of ex-amples needed to learn WSD ([REF_CITE];"
Atsushi et al.[REF_CITE]adopted a selec-tive sampling method to use small number of examples in training.
"They de ned a train-ing utility function to select examples with minimum certainty, and at each training it-eration the examples with less certainty were saved in the example database."
"However, at each iteration of training the similarity among word property vectors must be calculated due to their k -NN like implementation of training utility."
"While labeled examples obtained from a sense-tagged corpus is expensive and time-consuming, it is signi cantly easier to ob-tain the unlabeled examples."
"Yarowsky[REF_CITE]presented, for the rst time, the possibility that unlabeled examples can be used for WSD."
He used a learning algo-rithm based on the local context under the assumption that all instances of a word have the same intended meaning within any xed document and achieved good results with only a few labeled examples and many unlabeled ones.
Nigam et al.[REF_CITE]also showed the unlabeled examples can enhance the accuracy of text categorization.
"In this paper, we present a new approach to word sense disambiguation that is based on selective sampling algorithm with commit-tees."
"In this approach, the number of train-ing examples is reduced, by determining by weighted majority voting of multiple classi-ers, whether a given training example should be learned or not."
The classi ers of the com-mittee are rst trained on a small set of la-beled examples and the training set is aug-mented by a large number of unlabeled exam-ples.
One might think that this has the pos-sibility that the committee is misled by unla-beled examples.
"But, the experimental results con rm that the accuracy of WSD is increased by using unlabeled examples when the mem-bers of the committee are well trained with labeled examples."
"We also theoretically show that performance improvement is guaranteed by a mild requirement, i.e., the base classi-ers need to guess better than random selec-tion."
This is because the possibility misled by unlabeled examples is reduced by integrating outputs of multiple classi ers.
One advantage of this method is that it e ectively performs WSD with only a small number of labeled ex-amples and thus shows possibility of building word sense disambiguators for the languages which have no sense-tagged corpus.
The rest of this paper is organized as fol-lows.
Section 2 introduces the general proce-dure for word sense disambiguation and the necessity of unlabeled examples.
Section 3 ex-plains how the proposed method works using both labeled and unlabeled examples.
Section 4 presents the experimental results obtained by using the KAIST raw corpus.
Section 5 draws conclusions.
Let S 2 f s 1 ;:::;s k g be the set of possible senses of a word to be disambiguated.
"To determine the sense of the word, we need to consider the contextual properties."
Let x = &lt; x 1 ;:::;x n &gt; be the vector for rep-resenting selected contextual features.
"If we have a classi er f ( x; ) parameterized with , then the sense of a word with property vec-tor x can be determined by choosing the most probable sense s : s = arg max s 2 S f ( x; ) :"
"The parameters are determined by training the classi er on a set of labeled examples, L = f ( x 1 ; s 1 ) ; : : : ; ( x N ; s N ) g ."
"In general, the rst step of WSD is to extract a set of contextual features."
"To select particu-lar properties for Korean, the language of our cencern, the following characteristics should be considered:"
Korean is a partially free-order language.
"The ordering information on the neigh-bors of the ambiguous word, therefore, does not give signi cantly meaningful in-formation in Korean."
"In Korean, ellipses appear very often with a nominative case or objective case."
"Therefore, it is diÆcult to build a large scale database of labeled examples with case markers."
"Considering both characteristics and re-sults of previous work, we select eight prop-erties for WSD of Korean nouns (Table 1)."
"Three of them ( PARENT, NMODWORD, ADNWORD ) take morphological form as their value, one ( GFUNC ) takes 11 values of grammatical functions 1 , and others take only true or false ."
Many researchers tried to develop automated methods to reduce training cost in language learning and found out that the cost can be reduced by active learning which has control over the training examples[REF_CITE].
"Though the number of labeled exam-ples needed is reduced by active learning, the label of the selected examples must be given by the human experts."
"Thus, active learn-ing is still expensive and a method for auto-matic labeling unlabeled examples is needed to have the learner automatically gather in-formati[REF_CITE]."
As the unlabeled examples can be obtained with ease without human experts it makes WSD robust.
Yarowsky[REF_CITE]presented the possibility of automatic label-ing of training examples in WSD and achieved good results with only a few labeled exam-ples and many unlabeled examples.
"On the other hand, Blum and Mitchell tried to clas-sify Web pages, in which the description of each example can be partitioned into distinct views such as the words occurring on that page and the words occurring in hyperlinks[REF_CITE]."
"By using both views together, they augmented a small set of labeled examples with a lot of unlabeled examples."
The unlabeled examples in WSD can pro-vide information about the joint probability distribution over properties but they also can mislead the learner.
"However, the possibility of being misled by the unlabeled examples is reduced by the committee of classi ers since combining or integrating the outputs of sev-eral classi ers in general leads to improved performance."
This is why we use active learn-ing with committees to select informative un-labeled examples and label them.
The algorithm for active learning using unla-beled data is given in Figure 1.
It takes two sets of examples as inputs.
A Set L is the one with labeled examples and D = f x 1 ; : : : ; x T g is the one with unlabeled examples where x i is a property vector.
"First of all, the training set L ( j 1) ([Footnote_1] j M ) of labeled examples is constructed for each base classi er C j ."
"1[REF_CITE]grammatical functions are from the parser, KEMTS (Korean-to-English Machine Translation System) developed in Seoul National Uni-versity, Korea."
This is done by random resampling as in Bagging[REF_CITE].
"Then, each base classi er C j is trained with the set of labeled examples L ( j 1) ."
"After the classi ers are trained on labeled examples, the training set is augmented by the unlabeled examples."
"For each unlabeled example x t 2 D , each classi er computes the sense y j 2 S which is the label associated with it, where S is the set of possible sense of x t ."
The distribution W over the base classi-ers represents the importance weights.
"As the distribution can be changed each iter-ation, the distribution in iteration t is de-noted by W t ."
The importance weight of clas-si er C j under distribution W t is denoted by W t ( j ).
"Initially, the base classi ers have equal weights, so that W t ( j ) = 1 =M ."
The sense of the unlabeled example x t is de-termined by majority voting among C j &apos;s with weight distribution W .
"Formally, the sense y t of x t is predicted by"
X y t ( x t ) = arg max y 2 S j W t ( j ) : : C j ( x t )= y
"If most classi ers believe that y t is the correct sense of x t , they need not learn x t because this example makes no contribution to reduce the variance over the distribution of exam-ples."
"In this case, instead of learning the ex-ample, the weight of each classi er is updated in such a way that the classi ers whose pre-dictions were correct get a higher importance weight and the classi ers whose predictions were wrong get a lower importance weight under the assumption that the correct sense of x t is y t ."
This is done by multiplying the weight of the classi er whose prediction is y t by certainty t .
"To ensure the updated W t +1 form a distribution, W t +1 is normalized by constant Z t ."
"Formally, the importance weight is updated as follows: t if y j = y t ; W t +1 = WZ t ( t j ) 1 otherwise."
The certainty t is computed from error t .
"Because we trust that the correct sense of x t is y t , the error t is the ratio of the number of classi ers whose predictions are not y t ."
"That is, t is computed as t = 1 t t where t is given as t ="
No. of C j &apos;s whose M predictions are not y t :
"Note that the smaller t , the larger the value of t ."
"This implies that, if the sense of x t is certainly y t and a classi er predicts it, a higher weight is assigned to the classi er."
We assume that most classi ers believe that y t is the sense of x t if the value of y t is larger than a certainty threshold which is set by trial-and-error.
"However, if the certainty is below the threshold, the classi ers need to learn the ex-ample x t yet with belief that the sense of it is y t ."
"Therefore, the set of training examples, L ( jt ) , for the classi er C j is expanded by"
L ( jt +1) =
L ( jt ) + f ( x t ; y t ) g :
"Then, each classi er C j is restructured with L ( jt +1) ."
This process is repeated until the unlabeled examples are exhausted.
The sense of a new example x is then determined by weighted majority voting among X the trained classi ers: f ( x ) = arg max y 2 S j W T ( j ) ; : C j ( x )= y where W T ( j ) is the importance weight of clas-si er C j after the learning process.
Previous studies show that using multiple classi ers rather than a single classi er leads to improved generalizati[REF_CITE]and learning algorithms which use weak classi ers can be boosted into strong algorithms[REF_CITE].
"In addition, Littlestone and Warmuth[REF_CITE]showed that the error of the weighted majority algorithm is linearly bounded on that of the best mem-ber when the weight of each classi er is de-termined by held-out examples."
The performance of the proposed method depends on that of initial base classi ers.
This is because it is highly possible for unla-beled examples to mislead the learning algo-rithm if they are poorly trained in their initial state.
"However, if the accuracy of the initial majority voting is larger than 12 , the proposed method performs well as the following theo-rem shows."
Theorem 1 Assume that every unlabeled data x t is added to the set of training ex-amples for all classi ers and the importance weights are not updated.
Suppose that p 0 be the probability that the initial classi ers do not make errors and t (0 1) be the t probability by which the accuracy is increased in adding one more correct example or de-creased in adding one more incorrect example at iteration t .
"If p 0 1 , the accuracy does 2 not decrease as a new unlabeled data is added to the training data set."
"The probability for the classi ers to predict the correct sense at iteration t = 1, p 1 , is p 1 = p 0 ( p 0 + 0 ) + (1 p 0 )( p 0 0 ) = p 0 (2 0 + 1) 0 because the accuracy can be increased or de-creased by 0 with the probability p 0 and 1 p 0 , respectively."
"Therefore, without loss of generality, at iteration t = i + 1, we have p i +1 = p i (2 i + 1) i :"
"To ensure the accuracy does not decrease, the condition p i +1 p i should be satis ed. p i +1 p i = p i (2 i + 1) i p i = p i (2 i ) 0 i 1 ) p i 2"
The theorem follows immediately from this result.
"Although any kind of learning algorithms which meet the conditions for Theorem 1 can be used as base classi ers, Quinlan&apos;s C4.5 re-lease 8[REF_CITE]is used in this paper."
The main reason why decision trees are used as base classi ers is that there is a fast restruc-turing algorithm for decision trees.
Adding an unlabeled example with a predicted label to the existing set of training examples makes the classi ers restructured.
"Because the re-structuring of classi ers is time-consuming, the proposed method is of little practical use without an eÆcient way to restructure."
Ut-go et al.[REF_CITE]presented two kinds of eÆcient algorithms for restructuring decision trees and showed experimentally that their methods perform well with only small restructuring cost.
We modi ed C4.5 so that word match-ing is accomplished not by comparing mor-phological forms but by calculating similar-ity between words to tackle data-sparseness problem.
The similarity between two Ko-rean words is measured by averaged distance in WordNet of their English-translated words[REF_CITE].
We used the KAIST Korean raw corpus 2 for the experiments.
The entire corpus consists of 10 million words but we used in this pa-per the corpus containing one million words excluding the duplicated news articles.
Ta-ble 2 shows various senses of ambiguous Ko-rean nouns considered and their sense distri-butions.
The percentage column in the table denotes the ratio that the word is used with the sense in the corpus.
"Therefore, we can regard the maximum percentage as a lower bound on the correct sense for each word."
"For the experiments, 15 base classi ers are used."
"If there is a tie in predicting senses, the sense with the lowest order is chosen as[REF_CITE]."
"For each noun, 90% of the examples are used for training and the remaining 10% are used for testing."
Table 3 shows the 10-fold cross validation result of WSD experiments for nouns listed in Table 2.
The accuracy of the proposed method shown in Table 3 is measured when the accuracy is in its best for various ratios of the number of labeled examples for base clas-si ers to total examples.
The results show that WSD by selective sampling with com-mittees using both labeled and unlabeled ex-amples is comparable to a single learner us-ing all the labeled examples.
"In addition, the method proposed in this paper achieves 26.3% improvement over the lower bound for ` bae &apos;, 41.5% for ` bun &apos;, 22.1% for ` jonja &apos;, and 4.[Footnote_2]% for ` dari &apos;, which is 23.6% improvement on the average."
2 This corpus is distributed by the Korea Termi-nology Research Center for Language and Knowledge Engineering.
"Especially, for ` jonja &apos; the proposed method shows higher accuracy than the single C4.5 trained on the whole labeled examples."
Figure 2 shows the performance improved by using unlabeled examples.
This gure demonstrates that the proposed method out-performs the one without using unlabeled ex-amples.
"The initial learning in the gure means that the committee is trained on la-beled examples, but is not augmented by un-labeled examples."
The di erence between two lines is the improved accuracy obtained by using unlabeled examples.
"When the accu-racy of the proposed method gets stabilized for the rst time, the improved accuracy by using unlabeled examples is 20.2% for ` bae &apos;, 9.9% for ` bun , 13.5% ` jonja &apos;, and 13.4% for ` dari &apos;."
"It should be mentioned that the results also show that the accuracy of the proposed method may be dropped when the classi ers are trained on too small a set of labeled data, as is the case in the early stages of Figure 2."
"However, in typical situations where the clas-si ers are trained on minimum training set size, this does not happen as the results of other nouns show."
"In addition, we can nd in this particular experiment that the accuracy is always improved by using unlabeled exam-ples if only about 22% of training examples, on the average, are labeled in advance."
"In Figure 2(a), it is interesting to observe jumps in the accuracy curve."
"The jump ap-pears because the unlabeled examples mislead the classi ers only when the classi ers are poorly trained, but they play an important role as information to select senses when the classi ers are well trained on labeled exam-ples."
Other nouns show similar phenomena though the percentage of labeled examples is di erent when the accuracy gets at.
"In this paper, we proposed a new method for word sense disambiguation that is based on unlabeled data."
Using unlabeled data is especially important in corpus-based natural language processing because raw corpora are ubiquitous while labeled data are expensive to obtain.
In a series of experiments on word sense disambiguation of Korean nouns we ob-served that the accuracy is improved up to 20.2% using only 32% of labeled data.
"This implies, the learning model trained on a small number of labeled data can be enhanced by using additional unlabeled data."
We also the-oretically showed that the predictive accuracy is always improved if the individual classi ers do better than random selection after being trained on labeled data.
"As the labels of unlabeled data are es-timated by committees of multiple decision trees, the burden of manual labeling is min- imized by using unlabeled data."
"Thus, the proposed method seems especially e ective and useful for the languages for which a large-scale sense-tagged corpus is not available yet."
"Another advantage of the proposed method is that it can be applied to other kinds of language learning problems such as POS-tagging, PP attachment, and text classi ca-tion."
These problems are similar to word sense disambiguation in the sense that unla-beled raw data are abundant but labeled data are limited and expensive to obtain.
The main aim of this paper is pronominalto analyse theanaphorae ectsresolutionof applyingto
Question Answering (QA) systems.
For this task a complete QA system has been implemented.
System eval-uation measures performance im-provements obtained when informa-tion that is referenced anaphorically in documents is not ignored.
Open domain QA systems are de ned as tools capable of extracting the answer to user queries directly from unrestricted do-main documents.
"Or at least, systems that can extract text snippets from texts, from whose content it is possible to infer the an-swer to a speci c question."
"In both cases, these systems try to reduce the amount of time users spend to locate a concrete infor-mation. palThisobjectiveswork is. intendedFirst, we toanalyseachieveseveraltwo princi-docu-ment collections to determine the level of in-formation referenced pronominally in them. amountThis studyof informationgives us anthatoverviewis discardedaboutwhenthe these references are not solved."
"As second ob-jective, we try to measure improvements of solving this kind of references in QA systems."
"With this purpose in mind, a full QA system has been implemented."
Bene ts obtained by solving pronominal references are measured by comparing system performance with and without taking into account information ref-erenced pronominally.
"Evaluation shows that solving these references improves QA perfor-mance. artInoftheopenfollowingdomain sectionQA systems, the state-of-the-will be sum-marised."
"Afterwards, importance of pronom-inal references in documents is analysed."
"Next, our approach and system components are described."
"Finally, evaluation results are presented and discussed."
Interest in open domain QA systems is quite recent.
We had little information about this kind of systems until the First Question An-swering Track was held in last TREC confer-ence[REF_CITE].
"In this conference, nearly twenty di erent systems were evaluated with very di erent success rates."
We can clas-sify current approaches into two groups: text-snippet extraction systems and noun-phrase extraction systems. basedText-snippeton locatingextractionand extractingapproachesthe most rel-are evant sentences or paragraphs to the query by supposing that this text will contain the cor-rect answer to the query.
This approach has been the most commonly used by participants in last TREC QA Track.
"Examples of these systems are[REF_CITE]. noticeAfterthatreviewingthere theseis a approachesgeneral agreement, we can about the importance of several Natural Lan-guage Processing (NLP) techniques for QA task."
"Pos-tagging, parsing and Name En- tity recognition are used by most of the sys-tems."
"However, few systems apply other NLP techniques."
"Particularly, only four systems model some coreference relations between en-tities in the query and documents[REF_CITE]."
"As example, Mor-ton approach models identity, de nite noun-phrases and non-possessive third person pro-nouns."
"Nevertheless, bene ts of applying these coreference techniques have not been analysed and measured separately. tractionThe secondsystemsgroup."
Theseincludesapproachesnoun-phrasetryex-to questionsnd the whosepreciseanswerinformationis de nedrequestedtypically byby a noun phrase. 1999MURAX).
"It canis oneuse informationof these systemsfrom (diKupiecerent, sentences, paragraphs and even di erent doc-uments to determine the answer (the most rel-evant noun-phrase) to the question."
"However, this system does not take into account the information referenced pronominally in docu-ments."
"Simply, it is ignored. beneWithts ourof applyingsystem, pronominalwe want to determineanaphora res-the olution techniques to QA systems."
"Therefore, we apply the developed computational sys-tem, Slot Uni cation Parser for Anaphora res-olution (SUPAR) over documents and queries ture(consistsetofal.three, 1999independent)."
"SUPAR&apos;s modulesarchitec-: lexical analysis, syntactic analysis, and a reso-lution module for natural language processing problems, such as pronominal anaphora. andForaevaluationsentence-extraction, a standardQAbasedsystemIR systemhave been implemented."
Both are based[REF_CITE].
"After IR system retrieves theserelevantdocumentsdocumentswith, our andQA systemwithoutprocessessolving pronominal references in order to compare -nal performance. resolutionAs resultsimproveswill showgreatly, pronominalQA systemsanaphoraper-formance."
"So, we think that this NLP tech-nique should be considered as part of any open domain QA system."
"Trying to measure the importance of informa-tion referenced pronominally in documents, we have analysed several text collections used for QA task in TREC-8 Conference as well as others used frequently for IR system test-ing."
"These collections were the following: Los Angeles Times (LAT), Federal Register (FR), Financial Times (FT), Federal Bureau Infor-mation Service (FBIS), TIME, CRANFIELD, CISI, CACM, MED and LISA."
"This analy-sis consists on determining the amount and type of pronouns used, as well as the number of sentences containing pronouns in each of them."
"As average measure of pronouns used in a collection, we use the ratio between the quantity of pronouns and the number of sen-tences containing pronouns."
This measure ap-proximates the level of information that is ig-nored if these references are not solved.
"Fig-ure 1 shows the results obtained in this anal-ysis. nounsAs weusedcaninseeanalysed, the amountcollectionsand typevaryof pro-de-pending on the subject the documents talk about."
"LAT, FBIS, TIME and FT collections are composed from news published in di er-ent newspapers."
"The ratio of pronominal ref-erence used in this kind of documents is very high (from 35,96% to 55,20%)."
"These doc-uments contain a great number of pronomi-nal references in third person (he, she, they, his, her, their) whose antecedents are mainly people&apos;s names."
"In this type of documents, pronominal anaphora resolution seems to be very necessary for a correct modelling of rela-tions between entities."
CISI and MED collec-tions appear ranked next in decreasing ratio level order.
"These collections are composed by general comments about document man-aging, classi cation and indexing and doc-uments extracted from medical journals re-spectively."
"Although the ratio presented by these collections (24,94% and 22,16%) is also high, the most important group of pronominal references used in these collections is formed by &quot;it&quot; and &quot;its&quot; pronouns."
"In this case, antecedents of these pronominal references are mainly concepts represented typically by noun phrases."
It seems again important solv-ing these references for a correct modelling of relations between concepts expressed by noun-phrases.
"The lowest ratio results are presented by CRANFIELD collection with a 9,05%."
The reason of this level of pronominal use is due to text contents.
This collection is composed by extracts of very high technical subjects.
"Between the described percentages we nd the CACM, LISA and FR collections."
"These collections are formed by abstracts and terdocuments, from theextractedCACM journalfrom theandFederalfrom LibraryRegis-and Information Science Abstracts, respec-thattivelyas."
"Asmoregeneraltechnicalbehaviourdocument, wecontentscan noticebe-come, the pronouns &quot;it&quot; and &quot;its&quot; become the most appearing in documents and the ratio of pronominal references used decreases."
An-other observation can be extracted from this analysis.
Distribution of pronouns within sen-tences is similar in all collections.
Pronouns ingappearonescatteredor two pronounsthrough. sentencesUsing morecontain-than two pronouns in the same sentence is quite infrequent. questionAfter analysingmay arisethese.
Is resultsit worthan enoughimportantto solve pronominal references in documents?
It would seem reasonable to think that resolu-tion of pronominal anaphora would only be accomplished when the ratio of pronominal occurrence exceeds a minimum level.
"How-ever, we have to take into account that the cost of solving these references is proportional to the number of pronouns analysed and con-sequentlyformation, aproportionalsystem willtoignorethe amountif these ofrefer-in-ences are not solved. ableAstoresultssolve pronominalabove statereferences, it seemsinreason-queries and documents for QA tasks."
"At least, when the ratio of pronouns used in documents rec-ommend it."
"Anyway, evaluation and later analysis (section 5) contribute with empiri-cal data to conclude that applying pronom-inal anaphora resolution techniques improve QA systems performance."
Ourrst onesystemis a isstandardmade upIRofsystemthree modulesthat retrieves.
The relevant documents for queries.
"The second module will manage with anaphora resolution in both, queries and retrieved documents."
For this purpose we use SUPAR computational system (section 4.1).
And the third one is a sentence-extraction QA system that inter-acts with SUPAR module and ranks sentences from retrieved documents to locate the an-swer where the correct answer appears (sec-tion 4.2). temForhasthebeenpurposeimplementedof evaluation.
ThisansystemIR sys-is based on the standard information retrieval approach to document ranking described[REF_CITE].
"For QA task, the same ap-proach has been used as baseline but using sentences as text unit."
Each term in the query and documents is assigned an inverse docu-ment frequency ( idf ) score based on the same corpus.
This measure is computed as: idf ( t ) = log ( dfN ( t )) (1) inwherethe collection N is theandtotal df number (t) is theof documentsnumber of pansiondocumentsconsistswhichofcontainsstemmingtermtermst.
Queryusingex-a version of the Porter stemmer.
Document and sentence similarity to the query was computed using the cosine similarity measure.
The LAT corpus has been selected as test collection due to his high level of pronominal references.
"ParserIn thisforsectionAnaphora, the NLPResolutionSlot Uni(SUPARcation) is brie y described ( et al., 1999;  et al., 1998)."
SUPAR&apos;s architec-ture consists of three independent modules that interact with one other.
"These modules are lexical analysis, syntactic analysis, and a resolution module for Natural Language Pro-cessing Lexical problems analysis . module."
"This module takes each sentence to parse as input, along with a tool that provides the system with all the lexical information for each word of the sentence."
This tool may be either a dictio-nary or a part-of-speech tagger.
"In addition, this module returns a list with all the neces-assaryoutputinformation."
"SUPARfor worksthe remainingsentencemodulesby sen-tence from the input text, but stores informa-tion from previous sentences, which it uses in other modules, (e.g. the list of antecedents of previous Syntactic sentences analysis for anaphora module. resolutionThis mod-). ule takes as input the output of lexical analy-sis module and the syntactic information rep-resented by means of grammatical formalism whatSlot Uniis calledcationslotGrammarstructure(,SUGwhich)."
Itstoresreturnsall necessary information for following modules.
One of the main advantages of this system is that it allows carrying out either partial or full parsing of the text. lemsModule.
"In of this resolution module, of NLP NLP problems prob- (e.g. anaphora, extra-position, ellipsis or PP-attachment) are dealt with."
It takes the slot structure (SS) that corresponds to the parsed sentence as input.
The output is an SS in which all the anaphors have been resolved.
"In this paper, only pronominal anaphora resolu-tion has been applied. beTheusedkindsin pronominalof knowledgeanaphorathat resolutionare going toin this paper are: pos-tagger, partial parsing, statistical knowledge, c-command and mor-phologic agreement as restrictions and several heuristics such as syntactic parallelism, pref-erencethe pronounfor noun-phrasespreference forin propersame sentencenouns. as unrestrictedWe shouldtextsremark(asthatit occurswheninwethisworkpaperwith) we do not use semantic knowledge (i.e. a tool such as WorNet)."
"Presently, SUPAR re-solves both Spanish and English pronominal anaphora with a success rate of 87% and 84% respectively. diSUPARers frompronominalthose basedanaphoraon restrictionsresolutionand preferences, since the aim of our preferences is not to sort candidates, but rather to dis-card candidates."
"That is to say, preferences are considered in a similar way to restrictions, except when no candidate satis es a prefer-ence, in which case no candidate is discarded."
For example in sentence: &quot;Rob was asking us about John.
I replied that Peter saw John yes-terday.
"James also saw him.&quot; After applying the restrictions, the following list of candi-dates is obtained for the pronoun him : [ John, Peter, Rob ], which are then sorted according to their proximity to the anaphora."
"If pref-erence for candidates in same sentence as the anaphoraes it, so theis appliedfollowing, thenpreferenceno candidateis appliedsatis-on the same list of candidates."
"Next, preference for candidates in the previous sentence is ap-plied and the list is reduced to the following candidates: [ John, Peter ]."
"If syntactic par-allelism preference is then applied, only one candidate remains, [ John ], which will be the antecedent chosen. restrictionsEach kindandof preferencesanaphora has, althoughits owntheyset allof follow the same general algorithm: rst come the restrictions, after which the preferences are applied."
"For pronominal anaphora, the set of restrictions and preferences that apply are described in Figure 2."
"The following restrictions are rst applied mentto the, c-commandlist of candidatesconstraints: morphologicand semanticagree-consistency."
This list is sorted by proximity to tionsthe anaphorthere is.
"Nextstill more, if afterthanapplyingone candidaterestric-, the preferences are then applied, in the order shown in this gure."
"This sequence of prefer-ences (from 1 to 7 ) stops when, after having applied a preference, only one candidate re- mains."
"If after applying preferences there is still more than one candidate, then the most repeated candidates [Footnote_1] in the text are extracted from the list after applying preferences."
"1 Here, we mean that rstly we obtain the maxi-mum number of repetitions for an antecedent in the remaining list. After that, we extract from that list the antecedents that have this value of repetition."
"After this is done, if there is still more than one can-didate, then those candidates that have ap-peared most frequently with the verb of the anaphor are extracted from the previous list."
"Finally, if after having applied all the previ-ous preferences, there is still more than one candidate left, the rst candidate of the re-sulting list, (the closest one to the anaphor), is selected."
Our QA approach provides a second level of processing for relevant documents: Analysing matching Analysing documents Matching and Sentence Documents ranking .
This. step is applied over the best matching docu-ments retrieved from the IR system.
These documents are analysed by SUPAR module and pronominal references are solved.
"As re -sult, each pronoun is associated with the noun phrase it refers to in the documents."
"Then, documents are split into sentences as basic text unit for QA purposes."
This set of sen-tences Sentence is sent Ranking to the sentence .
Eachrankingterm stagein the. query is assigned a weight.
This weight is the sum of inverse document frequency mea-sure of terms based on its occurrence in the LAT collection described earlier.
Each docu-ment sentence is weighted the same way.
The only di erence with baseline is that pronouns are given the weight of the entity they refer to.
"As we only want to analyse the e ects of pronominal reference resolution, no more changes are introduced in weighting scheme."
"For sentence ranking, cosine similarity is used between query and document sentences."
"For this evaluation, several people unac-quainted with this work proposed 150 queries whose correct answer appeared at least once into the analysed collection."
"These queries thewereuser&apos;salso selectedinformationbasedneedonclearlytheir expressingand their being likely answered in a single sentence. wereFirstretrieved, relevantusingdocumentsthe IR systemfor eachdescribedquery mentsearlier.wereOnlyselectedthe best[REF_CITE]matchingevaluationdocu-."
"As the document containing the correct answer was included into the retrieved sets for only 93 queries (a 62% of the proposed queries), thethis remainingevaluation.57 queries were excluded for wasOnceaccomplishedretrieval offorrelevanteach querydocument, the setssys-tem applied anaphora resolution algorithm to andtheserankingdocumentswas .accomplishedFinally, sentenceas describedmatchingin section 4.2 and the system presented a ranked list containing the 10 most relevant sentences to each query. sultsFor, queriesa betterwereunderstandingclassi ed intoof evaluationthree groupsre-depending on the following characteristics: erencesGroup Ain."
Therethe targetare nosentencepronominal(sentenceref-containing the correct answer).
Group B. The information required as answer is referenced via pronominal anaphora in the target sentence.
Group C. Any term in the query is ref-erenced pronominally in the target sen-tence.
GroupsGroupBAandwasC containedmade up 25byand37 31questionsqueries. respectively.
"Figure 3 shows examples of queries classi ed into groups B and C. 4 asEvaluationthe numberresultsof targetare presentedsentencesinappear-Figure ing into the 10 most relevant sentences re-turned by the system for each query and also, the number of these sentences that are con-sideredsidered correcta correctif answerit can .beAnobtainedanswerbyis con-sim-ply looking at the target sentence."
Results are classi ed based on question type intro-duced above.
The number of queries pertain-ing to each group appears in the second col-umn.
Third and fourth columns show base-line results (without solving anaphora).
Fifth and sixth columns show results obtained when pronominal references have been solved. takeResultsinto accountshow several.
Bene tsaspectsobtainedwe fromhaveap-to plying pronominal anaphora resolution vary depending on question type. group A and B queries show us thatResultsrelevancefor to the query is the same as baseline system.
"So, it seems that pronominal anaphora res-olution does not achieve any improvement."
This is true only for group A questions.
"Al-though target sentences are ranked similarly, for group B questions, target sentences re-turned by baseline can not be considered as correct because we do not obtain the an-swer by simply looking at returned sentences."
The correct answer is displayed only when pronominal anaphora is solved and pronom-inal references are substituted by the noun phrase they refer to.
"Only if pronominal ref-erences are solved, the user will not need to read more text to obtain the correct answer."
For noun-phrase extraction QA systems the improvement is greater.
"If pronominal ref-erences are not solved, this information will not be analysed and probably a wrong noun-phrase will be given as answer to the query."
C Resultsqueries improveperformanceagain.
Theseif we analysequeriesgrouphave the following characteristic: some of the query terms were referenced via pronominal anaphora in the relevant sentence.
"When this situation occurs, target sentences are re-trieved earlier in the nal ranked list than in the baseline list."
This improvement is because similarity increases between query and target sentence when pronouns are weighted with the same score as their referring terms.
"The percentage of target sentences obtained in-creases 38,71 points (from 29,03% to 67,74%). measureAggregateimprovementresults presentedobtainedinconsideringFigure 4 the system as a whole."
"General percentage of target sentences obtained increases 12,90 points (from 41,94% to 54,84%) and the level of correct answers returned by the system in-creases 25,81 points (from 29,03% to 54,84%). ingAtquestionthis point: Willwe needthesetoresultsconsiderbethethefollow-same for any other question set?"
We have analysed test questions in order to determine if results obtained depend on question test set.
We ar-gue that a well-balanced query set would have a percentage of target sentences that contain pronouns (PTSC) similar to the pronominal reference ratio of the text collection that is being queried.
"Besides, we suppose that the probability of nding an answer in a sentence is the same for all sentences in the collec-tion."
"Comparing LAT ratio of pronominal reference (55,20%) with the question test set aPTSCect resultswe can. measureOur questionhow a setquestionPTSCsetvaluecan is a 60,22%."
"We obtain as target sentences containing pronouns only a 5,02% more than expected when test queries are randomly se-lected."
"In order to obtain results according to a well-balanced question set, we discarded ve questions from both groups B and C. Figure 5 shows that results for this well-balanced ques-tion set are similar to previous results."
"Aggre-gate results show that general percentage of target sentences increases 10,84 points when solving pronominal anaphora and the level of correct answers retrieved increases 22,89 points (instead of 12,90 and 25,81 obtained in previous evaluation respectively). inalAsanaphoraresults showresolution, we canimprovessay that pronom-QA sys-tems performance in several aspects."
"First, precision increases when query terms are ref-erenced anaphorically in the target sentence."
"Second, pronominal anaphora resolution re-duces the amount of text a user has to read when the answer sentence is displayed and pronominal references are substituted with their coreferent noun phrases."
"And third, for noun phrase extraction QA systems it is essential to solve pronominal references if a good performance is pursued."
The analysis of information referenced pronominally in documents has revealed to be important to tasks where high level of recall is required.
"We have analysed and anaphora resolution in QA systems.measured the e ects of applying pronominalAs results show, its application improves greatly QA performance and seems to be essential in some cases. pearedThreewhilemain investigationareas of futurehasworkbeenhavedevel-ap-oped."
"First, IR system used for retrieving relevant documents has to be adapted for QA tasks."
"The IR used, obtained the document containing the target sentence only for 93 of the 150 proposed queries."
"Therefore, its preci-sion needs to be improved."
"Second, anaphora resolution algorithm has to be extended to di erent types of anaphora such as de nite descriptions, surface count, verbal phrase and one-anaphora."
"And third, sentence ranking approach has to be analysed to maximise the percentage of target sentences included into the 10 answer sentences presented by the sys-tem."
2~|~+¡ /¥/s¢|~^¥ ¦ ¢§sJ¨F©J lZg¢ª©¡J¡s«¬~§s­ª¢©J®s¢J¯¡£~« ° ª¢±¢£³µr¶¸·¹§£~| ¢©¡»º-¡s ° l| ¦ ¯¡£¼D| ½F| |¢ ~ «£l¿J~­ª©J~3~« ¡³¢£/À-g¢g¢|©J½§F¦ |~~­ª~-©¡ÊË¥U©¾a®2¡sg¢ª©¡©JÌv|½F¡F¦¢g¢RÃFÄiÅ) ° l/l¨È¢£i|½FÉ¦ ~g/¥/DÌd©Ja¢£ÐJ¡s~¾ÍJ¡Î¢¯¥È¢° |~ /¥ s¡2Ï+§s|~~ ¼JZg2±¡s«£UÑ£¡£¼£Æ¸ºÒ~­vÌH©J/¾n©JÌ4¡2«£UÑF¯¡£¼b¥lJ­­/~/_¦ ÌH©JJ¡2&lt;° lZ ° JÈJ¥° /l2ªl¿J/~_Ö£ÆÓÔ¤Ìd ©J&apos;­ª©¡£¼ ÚÍ¥U©¾?
Û Ü 3ÎÝMÞ ßOà&apos;á&lt;â ã  ° lM¢|;© ¹Ï+§£~¢ª©¡S®+½;/l¢§£/¡s¡s¼
¤¬«n­ ¦¯®°g­n¤§°±¨²Nª  ²°  ­n²¹«n²³¯ª¾ °g­ °¿­ ¸¶¤º­¾ ¦©¼2¤¬¨«² »³ ©¤¬¼¦¯²¨´¶² ¤½z«nµa°g¶¾ À·]­ «n¤ ÂL¦©¨¤¬°g¶z° ³©® ¤| ° |° ³i³¯¤ ¾ ¶ÂOÅ6ÆÈÇ¤¬³©°g­ ¸¦¯µ«O²½?«nµa°g¶¾ «n¤%&quot;Î­n² ² ¶ ¾ Ì¨«¬ÁOÏ¾ %¸¶·]¤²gÍ|¤ ªg°g¶ ¦©°   |­ ¸² ¾ «&quot;°g¶ ¤Ð« À®gÍ ¤¬« ­  ¦©¨®+|¤¬¨® ¤¬« ­n¤ ¾ ° ¨ ¾ ­ |ÑÒ°g¶ ¤¿¤|ªc° ¤|¶ » ¾ ½²·]²gÍ¶ ²6·||· À¶ ¶ ] ² »a°g»¦©³©¦¯­;Ñ /Á  ¾ ¤¬³ « ¸²Î%«µ ²« « ¦©»¦©³©¦¯­;Ñ´­n²Õ»[¤Ö° ³¯­n¤|¶ ¨°g­ ¦¯ª ¤ µ¶ ² »°g»¦©³i¦¯­EÑ×«  ­ ¸¦©¨®2¼2¤|­ ¸² ¾ Á Ø Ù A   &gt; @ ¢H 8E&gt;?A ­Û ¦©«n­¨¤Ü²¦i·|° ) ½³B³©°­  À6Þ²6·]¤|¤¬«¶ « ¦©¨®à¦©«¾ ¦©ß×·«n²gÍE·||¦¯¤¬«l¦©¨Ö«n­° ³©³¯¤  °g­°cÍ°
We propose a distribution-based pruning of n-gram backoff language models.
"Instead of the conventional approach of pruning n-grams that are infrequent in training data, we prune n-grams that are likely to be infrequent in a new document."
Our method is based on the n-gram distribution i.e. the probability that an n-gram occurs in a new document.
Experimental results show that our method performed 7-9% (word perplexity reduction) better than conventional cutoff methods.
"Statistical language modelling (SLM) has been successfully applied to many domains such as speech recogniti[REF_CITE], information retrieval[REF_CITE], and spoken language understanding[REF_CITE]."
"In particular, n-gram language model (LM) has been demonstrated to be highly effective for these domains."
"N-gram LM estimates the probability of a word given previous words, P(w n |w 1 ,…,w n-1 )."
"In applying an SLM, it is usually the case that more training data will improve a language model."
"However, as training data size increases, LM size increases, which can lead to models that are too large for practical use."
"To deal with the problem, count cut[REF_CITE]is widely used to prune language models."
The cutoff method deletes from the LM those n-grams that occur infrequently in the training data.
"The cutoff method assumes that if an n-gram is infrequent in training data, it is also infrequent in testing data."
"But in the real world, training data rarely matches testing data perfectly."
"Therefore, the count cutoff method is not perfect."
"In this paper, we propose a distribution-based cutoff method."
This approach estimates if an n-gram is “likely to be infrequent in testing data”.
"To determine this likelihood, we divide the training data into partitions, and use a cross-validation-like approach."
Experiments show that this method performed 7-9% (word perplexity reduction) better than conventional cutoff methods.
"In section 2, we discuss prior SLM research, including backoff bigram LM, perplexity, and related works on LM pruning methods."
"In section 3, we propose a new criterion for LM pruning based on n-gram distribution, and discuss in detail how to estimate the distribution."
"In section 4, we compare our method with count cutoff, and present experimental results in perplexity."
"Finally, we present our conclusions in section 5."
"One of the most successful forms of SLM is the n-gram LM. N-gram LM estimates the probability of a word given the n-1 previous words, P(w n |w 1 ,…,w n-1 )."
"In practice, n is usually set to 2 (bigram), or 3 (trigram)."
"For simplicity, we restrict our discussion to bigram, P(w n |w n-1 ), which assumes that the probability of a word depends only on the identity of the immediately preceding word."
But our approach extends to any n-gram.
Perplexity is the most common metric for evaluating a bigram LM.
"It is defined as,"
N − 1 ∑ logP(w i |w i−1 )
PP = 2 N i=1 (1) where N is the length of the testing data.
The perplexity can be roughly interpreted as the geometric mean of the branching factor of the document when presented to the language model.
"Clearly, lower perplexities are better."
One of the key issues in language modelling is the problem of data sparseness.
"To deal with the problem,[REF_CITE]proposed a backoff scheme, which is widely used in bigram language modelling."
Backoff scheme estimates the probability of an unseen bigram by utilizing unigram estimates.
"It is of the form: c(w i−1 ,w i ) &gt; 0"
P(w i | w i−1 ) = P d (w i | w i−1 ) (2)  α (w i−1 )
"P(w i ) otherwise where c(w i-1 ,w i ) is the frequency of word pair (w i-1 ,w i ) in training data, P d represents the Good-Turing discounted estimate for seen word pairs, and α (w i-1 ) is a normalization factor."
"Due to the memory limitation in realistic applications, only a finite set of word pairs have conditional probabilities P(w n |w n-1 ) explicitly represented in the model, especially when the model is trained on a large corpus."
The remaining word pairs are assigned a probability by back-off (i.e. unigram estimates).
"The goal of bigram pruning is to remove uncommon explicit bigram estimates P(w n |w n-1 ) from the model to reduce the number of parameters, while minimizing the performance loss."
The most common way to eliminate unused count is by means of count cutoffs[REF_CITE].
"A cutoff is chosen, say 2, and all probabilities stored in the model with 2 or fewer counts are removed."
"This method assumes that there is not much difference between a bigram occurring once, twice, or not at all."
"Just by excluding those bigrams with a small count from a model, a significant saving in memory can be achieved."
"In a typical training corpus, roughly 65% of unique bigram sequences occur only once."
"Recently, several improvements over count cutoffs have been proposed.[REF_CITE]proposed a different pruning scheme for backoff models, where bigrams are ranked by a weighted difference of the log probability estimate before and after pruning."
Bigrams with difference less than a threshold are pruned.[REF_CITE]proposed a criterion for pruning based on the relative entropy between the original and the pruned model.
The relative entropy measure can be expressed as a relative change in training data perplexity.
All bigrams that change perplexity by less than a threshold are removed from the model.
"Stolcke also concluded that, for practical purpose, the method[REF_CITE]is a very good approximation to this method."
"All previous cutoff methods described above use a similar criterion for pruning, that is, the difference (or information loss) between the original estimate and the backoff estimate."
"After ranking, all bigrams with difference small enough will be pruned, since they contain no more information."
"As described in the previous section, previous cutoff methods assume that training data covers testing data."
"Bigrams that are infrequent in training data are also assumed to be infrequent in testing data, and will be cutoff."
"But in the real world, no matter how large the training data, it is still always very sparse compared to all data in the world."
"Furthermore, training data will be biased by its mixture of domain, time, or style, etc."
"For example, if we use newspaper in training, a name like “Lewinsky” may have high frequency in certain years but not others; if we use Gone with the Wind in training, “Scarlett O’Hara” will have disproportionately high probability and will not be cutoff."
We propose another approach to pruning.
We aim to keep bigrams that are more likely to occur in a new document.
"We therefore propose a new criterion for pruning parameters from bigram models, based on the bigram distribution i.e. the probability that a bigram will occur in a new document."
All bigrams with the probability less than a threshold are removed.
"We estimate the probability that a bigram occurs in a new document by dividing training data into partitions, called subunits, and use a cross-validation-like approach."
"In the remaining part of this section, we firstly investigate several methods for term distribution modelling, and extend them to bigram distribution modelling."
"Then we investigate the effects of the definition of the subunit, and experiment with various ways to divide a training set into subunits."
"Experiments show that this not only allows a much more efficient computation for bigram distribution modelling, but also results in a more general bigram model, in spite of the domain, style, or temporal bias of training data."
"In this section, we will discuss in detail how to estimate the probability that a bigram occurs in a new document."
"For simplicity, we define a document as the subunit of the training corpus."
"In the next section, we will loosen this constraint."
"Term distribution models estimate the probability P i (k), the proportion of times that of a word w i appears k times in a document."
"In bigram distribution models, we wish to model the probability that a word pair (w i-1 ,w i ) occurs in a new document."
The probability can be expressed as the measure of the generality of a bigram.
"Thus, in what follows, it is denoted by P gen (w i-1 ,w i )."
"The higher the P gen (w i-1 ,w i ) is, for one particular document, the less informative the bigram is, but for all documents, the more general the bigram is."
"We now consider several methods for term distribution modelling, which are widely used in Information Retrieval, and extend them to bigram distribution modelling."
"These methods include models based on the Poisson distributi[REF_CITE], inverse document frequency[REF_CITE], and Katz’s K mixture[REF_CITE]."
"The standard probabilistic model for the distribution of a certain type of event over units of a fixed size (such as periods of time or volumes of liquid) is the Poisson distribution, which is defined as follows:"
P i (k) = P(k; λ i ) = e − λ λ ki (3) i k!
"In the most common model of the Poisson distribution in IR, the parameter λ i &gt;0 is the average number of occurrences of w i per document, that is λ i = cf i , where cf i is the"
"N number of documents containing w i , and N is the total number of documents in the collection."
"In our case, the event we are interested in is the occurrence of a particular word pair (w i-1 ,w i ) and the fixed unit is the document."
We can use the Poisson distribution to estimate an answer to the question: what is the probability that a word pair occurs in a document.
"Therefore, we get"
"P gen (w i−1 , w i ) =1− P(0; λ i) =1− e − λ (4) i"
"It turns out that using Poisson distribution, we have P gen (w i-1 , w i ) ∝ c(w i-1 ,w i )."
This means that this criterion is equivalent to count cutoff.
IDF is a widely used measure of specificity[REF_CITE].
It is the reverse of generality.
Therefore we can also derive generality from IDF.
IDF is defined as follows:
"IDF i = log( N ) (5) df i where, in the case of bigram distribution, N is the total number of documents, and df i is the number of documents that the contain word pair (w i-1 ,w i )."
"The formula log = N gives full df i weight to a word pair (w i-1 ,w i ) that occurred in one document."
"Therefore, let’s assume,"
"P gen (w i −1 , w i ) ∝"
"C(w i −1, w i ) (6) IDF i"
"It turns out that based on IDF, our criterion is equivalent to the count cutoff weighted by the reverse of IDF."
"Unfortunately, experiments show that using (6) directly does not get any improvement."
"In fact, it is even worse than count cutoff methods."
"Therefore, we use the following form instead,"
"P gen (w i−1 , w i ) ∝ C(w i−1, w i ) (7) IDF i α where α is a weighting factor tuned to maximize the performance."
"As stated[REF_CITE], the Poisson estimates are good for non-content words, but not for content words."
Several improvements over Poisson have been proposed.
These include two-Poisson Model[REF_CITE]and Katz’s K mixture model[REF_CITE].
The K mixture is the better.
It is also a simpler distribution that fits empirical distributions of content words as well as non-content words.
"Therefore, we try to use K mixture for bigram distribution modelling."
"According[REF_CITE], K mixture model estimates the probability that word w i appears k times in a document as follows:"
"P i (k) = (1− α ) δ k,0 + βα +1 ( ββ +1) k (8) where δ k,0 =1 iff k=0 and δ k,0 =0 otherwise. α and β are parameters that can be fit using the observed mean λ and the observed inverse document frequency IDF as follow: λ = cf (9) N IDF = log N (10) df β = λ × 2 IDF −1 = cf − df (11) df α = λ (12) β where again, cf is the total number of occurrence of word w i in the collection, df is the number of documents in the collection that w i occurs in, and N is the total number of documents."
"The bigram distribution model is a variation of the above K mixture model, where we estimate the probability that a word pair (w i-1 ,w i ) , occurs in a document by:"
"K P gen (w i −1 , w i ) = 1− ∑ P (k) i (13) k =1 where K is dependent on the size of the subunit, the larger the subunit, the larger the value (in our experiments, we set K from 1 to 3), and P i (k) is the probability of word pair (w i-1 ,w i ) occurs k times in a document."
"P i (k) is estimated by equation (8), where α , and β are estimated by equations (9) to (12)."
"Accordingly, cf is the total number of occurrence of a word pair (w i-1 ,w i ) in the collection, df is the number of documents that contain (w i-1 ,w i ), and N is the total number of documents."
Our experiments show that K mixture is the best among the three in most cases.
Some partial experimental results are shown in table 1.
"Therefore, in section 4, all experimental results are based on K mixture method."
"The bigram distribution model suggests a simple thresholding algorithm for bigram backoff model pruning: 1. Select a threshold θ . 2. Compute the probability that each bigram occurs in a document individually by equation (13). 3. Remove all bigrams whose probability to occur in a document is less than θ , and recomputed backoff weights."
"In this section, we report the experimental results on bigram pruning based on distribution versus count cutoff pruning method."
"In conventional approaches, a document is defined as the subunit of training data for term distribution estimating."
"But for a very large training corpus that consists of millions of documents, the estimation for the bigram distribution is very time-consuming."
"To cope with this problem, we use a cluster of documents as the subunit."
"As the number of clusters can be controlled, we can define an efficient computation method, and optimise the clustering algorithm."
"In what follows, we will report the experimental results with document and cluster being defined as the subunit, respectively."
"In our experiments, documents are clustered in three ways: by similar domain, style, or time."
"In all experiments described below, we use an open testing data consisting of 15 million characters that have been proofread and balanced among domain, style and time."
Training data are obtained from newspaper (People’s Daily) and novels.
Figure 1 shows the results when we define a document as the subunit.
"We used approximately 450 million characters of People’s Daily training data (1996), which consists of 39708 documents."
Figure 2 shows the results when we define a domain cluster as the subunit.
We also used approximately 450 million characters of
People’s Daily training data (1996).
"To cluster the documents, we used an SVM classifier developed by Platt[REF_CITE]to cluster documents of similar domains together automatically, and obtain a domain hierarchy incrementally."
"We also added a constraint to balance the size of each cluster, and finally we obtained 105 clusters."
It turns out that using domain clusters as subunits performs almost as well as the case of documents as subunits.
"Furthermore, we found that by using the pruning criterion based on bigram distribution, a lot of domain-specific bigrams are pruned."
It then results in a relatively domain-independent language model.
"Therefore, we call this pruning method domain subtraction based pruning."
Figure 3 shows the results when we define a style cluster as the subunit.
"For this experiment, we used 220 novels written by different writers, each approximately 500 kilonbytes in size, and defined each novel as a style cluster."
"Just like in domain clustering, we found that by using the pruning criterion based on bigram distribution, a lot of style-specific bigrams are pruned."
It then results in a relatively style-independent language model.
"Therefore, we call this pruning method style subtraction based pruning."
"In practice, it is relatively easier to collect large training text from newspaper."
"For example, many Chinese SLMs are trained from newspaper, which has high quality and consistent in style."
But the disadvantage is the temporal term phenomenon.
"In other words, some bigrams are used frequently during one time period, and then never used again."
Figure 4 shows the results when we define a temporal cluster as the subunit.
"In this experiment, we used approximately 9,200 million characters of People’s Daily training data (1978--1997)."
We simply clustered the document published in the same month of the same year as a cluster.
"Therefore, we obtained 240 clusters in total."
"Similarly, we found that by using the pruning criterion based on bigram distribution, a lot of time-specific bigrams are pruned."
It then results in a relatively time-independent language model.
"Therefore, we call this pruning method temporal subtraction based pruning."
"In our research lab, we are particularly interested in the problem of pinyin to Chinese character conversion, which has a memory limitation of 2MB for programs."
"At 2MB memory, our method leads to 7-9% word perplexity reduction, as displayed in table 2."
"As shown in figure 1-4, although as the size of language model is decreased, the perplexity rises sharply, the models created with the bigram distribution based pruning have consistently lower perplexity values than for the count cutoff method."
"Furthermore, when modelling bigram distribution on document clusters, our pruning method results in a more general n-gram backoff model, which resists to domain, style or temporal bias of training data."
"In this paper, we proposed a novel approach for n-gram backoff models pruning: keep n-grams that are more likely to occur in a new document."
"We then developed a criterion for pruning parameters from n-gram models, based on the n-gram distribution i.e. the probability that an n-gram occurs in a document."
All n-grams with the probability less than a threshold are removed.
Experimental results show that the distribution-based pruning method performed 7-9% (word perplexity reduction) better than conventional cutoff methods.
"Furthermore, when modelling n-gram distribution on document clusters created according to domain, style, or time, the pruning method results in a more general n-gram backoff model, in spite of the domain, style or temporal bias of training data."
This is a paper that describes computational linguistic activities on Philippines languages.
The Philippines is an archipelago with vast numbers of islands and numerous languages.
"The tasks of understanding, representing and implementing these languages require enormous work."
"An extensive amount of work has been done on understanding at least some of the major Philippine languages, but little has been done on the computational aspect."
Majority of the latter has been on the purpose of machine translation.
"Within the 7,200 islands of the Philippine archipelago, there are about one hundred and one (101) languages that are spoken."
This is according to the nationwide 1995 census conducted by the National Statistics Office of the Philippine Government[REF_CITE].
"The languages that are spoken by at least one percent of the total household population include Tagalog, Cebuano, Ilocano, Hiligaynon, Bikol, Waray, Pampanggo or Kapangpangan, Boholano, Pangasinan or Panggalatok, Maranao, Maguin-danao, and Tausug."
"Aside from these major languages, there are other Philippine dialects, which are variants of these major languages."
"Linguistics information on Philippine languages are extensive on the languages mentioned above, except for Maranao, Maguin-danao, and Tausug, which are some of the languages spoken in Southern Philippines."
"But as of yet, extensive research has already been done on theoretical linguistics and little is known for computational linguistics."
"In fact, the computational linguistics researches on Philippine languages are mainly focused on Tagalog. [Footnote_1]"
1 Tagalog (or Pilipino) has the most number of speakers in the country. This may be due to the fact that it was officially declared the national language of the[REF_CITE].
There are also notable work done on Ilocano.
This issue was further discussed in the[REF_CITE]which is on the problem of voice and grammatical functions in Western Austronesian Languages.
"Rubino (1997, 1996) provided an in-depth analysis of Ilocano."
"Among the major contributions of the work include an extensive treatment of the complex morphology in the language, a thorough treatment of the discourse particles, and the reference grammar of the language."
"Currently, most of the empirical endeavours in computational linguistics are in machine translation."
"There are several commercially available translation software, which include Philippine language, but translation is done word-for-word."
One such software is the[REF_CITE]which includes[REF_CITE]other languages.
"Although omni-directional, trans-lation involving Tagalog excludes morpho-logical and syntactic aspects of the language"
"Another software is the Filipino Language Software, which includes Tagalog, Visayan, Cebuano, and Ilocano languages."
IsaWika! is an English to Filipino machine translator that uses the augmented transition network as its computational architecture[REF_CITE].
It translates simple and compound declarative statements as well as imperative English statements.
"To date, it is the most serious research undertaking in machine translation in the Philippines."
"The computational architecture of the system is based on LFG, which differs from IsaWika’s ATN implementation."
Part of the research was describing a possible set of semantic information on every grammar category to establish a semantically-close translation.
"There are various theoretical linguistic studies on Philippine languages, but computational linguistics research is currently limited."
CL activities in the Philippines had yet to gain acceptance from its computing science community.
"The emergence of Internet as a global information repository, where information of all kind is stored, requires intelligent information processing tools (i.e., computer applications) to help the information seeker to retrieve the stored information."
"To build these intelligent information processing tools, we need to build computer applications that understand human language since most of those information is represented in human language."
"This is where computational linguistics becomes important, especially for countries like Indonesia that hosts more than 200 million people."
We need to develop a systematic understanding of the Bahasa Indonesia (the Indonesian national language) to enable us develop the needed computer applications that will help us manage information intelligently.
"However, until recently, there is only a few research activities in computational linguistics conducted in Indonesia."
The establishment of Computer Science departments in Indonesian universities that did not start until the beginning of 1980’s may be partially responsible for this [Footnote_1] .
1 Bandung Institute of Technology was the first among public universities that established Computer Science department in 1980.
"In addition, the Indonesian linguists seem to be keen on working “manually” instead of using computers in conducting their linguistics researches as stated[REF_CITE], only few of them really make use of the technology."
"While, on the other hand, most computer scientists tend to use the practical approach rather than constructing a complete framework to understand the language when building related applications such as a specific information retrieval system."
"In the following, I will describe past research activities in computational linguistics on Bahasa Indonesia."
This description is by no means exhaustive since it is very difficult to find out research activities in computational linguistics in Indonesia.
Corpus analysis is an important means as a way to understand the evolution of language usage by its people.
"In the case of Bahasa Indonesia, research activities on corpus analysis were almost none."
"There was one work by R. R.[REF_CITE]from Monash University, who conducted word frequency analysis of Indonesian newspapers."
"There was also similar work conducted the MMTS project (will be described later, in the following section); however, the result of the group’s corpus analysis was not made public."
"Given this condition, with a group of colleague both from the Faculty of Computer Science and the Faculty of Letters, I conducted an Indonesian corpus analysis using newspapers as the text source."
Each of the 52 editions corresponds to a particular week of the year and was taken randomly from the 7 daily editions of that given week.
"From this collection, we constructed a corpus consisting of 2.200.818 words that were formed by 74.559 unique words."
"Of these more than 2 million words, 1.826.740 words that were formed by 27.738 unique words are actually words that matched with the KBBI [Footnote_2] entries, while the rest are either names or foreign words."
"2 KBBI (Kamus Besar Bahasa Indonesia), the standard word dictionary for Bahasa Indonesia, contains a little more than 70.000 word entries."
Detailed analysis can be found[REF_CITE].
Everyone who has used a word processor understands the importance of a spelling checker in helping him/her to produce an error-free document.
"To develop a spelling checker, we need to understand the morphological structure of words especially how derived-words are constructed from their root-words and the addition of affixes."
We have conducted research to analyze the morphological structure of Indonesian words and based on this analysis we have developed a stemming algorithm suitable for those words.
"Unlike English, where the role of suffix dominates the generation of derived-words, Bahasa Indonesia depends on both prefix and suffix to derive new words."
"Therefore, to stem a derived Indonesian word in order to obtain its root-word, we have to look at the presence of both prefix and suffix in that derived-word[REF_CITE]."
"In addition, similar to English, multiple suffixes can also be present on a given derived-word."
"Based on this stemming algorithm, we have developed a spelling checker and spelling-error corrector utilities as part of the Lotus Smartsuite [Footnote_3] package."
"3 Lotus Smartsuite is an office automation package consisting word processor, spreadsheet, presentation editor, and database applications developed by Lotus Development Corporation."
"One notable research activity among the few computational linguistics research activities in Indonesia is the Multilingual Machine Translation System (MMTS) project conducted by the Agency for Assessment and Application of Technology (BPPT) as part of multi-national research project between China, Indonesia, Malaysia, Thailand, and lead by Japan[URL_CITE]"
"Unfortunately, there are very few publications about this work that could have benefited the computational linguistic community in the country."
"One of the few publications that the MMTS project made available for public is the Indonesian Word Electronic Dictionary (KEBI), which could be accessed on-line[URL_CITE]The dictionary contains 22.500 root-word and 43.500 derived-word entries."
"Currently, I am concentrating my work on developing syntax analyzer for sentences written in Bahasa Indonesia."
The approach taken initially was to use the context free grammar with restriction such as that used in the linguistic string analysis[REF_CITE].
"Using this approach, we have developed grammar that understands declarative sentences[REF_CITE]."
"However, our experience shows that we need to have a more detailed word categories than is currently available in the standard Indonesian word dictionary (KBBI) before the grammar can be used effectively."
This finding really shows us the importance of collaborating with the linguists who understand this field better.
"But before we do this, we need to educate our linguist-fellows the importance of computer in their fields."
"I would like to thank Mirna, bu Multamia, pak Muhadjir, bu Kiswartini, and all of my students who have collaborated with me in these efforts to understand Bahasa Indonesia better."
There are many challenging problems for Vietnamese language processing.
It will be a long time before these challenges are met.
Even some apparently simple problems such as spelling correction are quite difficult and have not been approached systematically yet.
"In this paper, we will discuss one aspect of this type of work: designing the so-called Vietools to detect and correct spelling of Vietnamese texts by using a spelling database based on TELEX code."
Vietools is also extended to serve many purposes in Vietnamese language processing.
"For the past two decades computational linguistics (CL) has progressed substantially in Vietnam, mainly in these basic aspects: data acquisition from the keyboard, encoding, and restitution through an output device for Vietnamese diacritic characters, updates on the fonts in Microsoft DOS/Windows, standardization for Vietnamese (James Do, Ngo Thanh Nhan), automatic translation of English documents into Vietnamese and vice versa (Phan Thi Tuoi, Dinh Dien), recognition of handwriting (Hoang Kiem, Nguyen Van Khuong), speech processing (Nguyen Thanh Phuc, Quach Tuan Ngoc), building bilingual dictionaries such as English-Vietnamese and V-E, French-Vietnamese and V-F dictionaries (Lac Viet), archives of old Sino-Vietnamese documents (Ngo Trung Viet, Cong Tam), etc."
Some of these works have been presented in Informatics and IT workshops organized in Vietnam.
These efforts are modest and do not yet show our full potential.
There are many reasons for this weakness.
The major reasons that the different efforts are quite isolated and there is not enough coordination.
Some coordinated workshops held from time to time would be very helpful.
At the IT Dept. DaNang University we are building a lexical database based on TELEX code for accomplishing the following tasks: - Converting Vietnamese texts from any font to any other font. - Putting texts in alphabetical order independently of the font in use. - Looking up words up in the monolingual and / or multilingual dictionary. - Building specialized monolingual dictionaries.
"At present, we are taking part in the GETA, CLIPS, IMAG, France, in the FEV project: for a multilingual dictionary: French-Vietnamese via English."
"In fact, inputting Vietnamese texts still encounters many problems, not yet solved properly."
"The most common mistakes in detecting and correcting spelling errors are: - wrong intonation or misspelling, - not following spelling specialization, not using syllables systematically in the same texts, etc."
"Winword, a commercial text processor, is not able to detect and correct spelling mistakes."
The program designed by Ngo Thanh Nhan (without an associated spelling dictionary) and other software packages for Vietnamese still do not offer adequate solutions.
We propose here a general solution for building the so-called Vietools for detecting and correcting spelling errors.
"Vietools is designed for office application such as Winword, Excel, Acess, PowerPoint, etc. in Microsoft Windows."
"Vietools has also been extended for converting and rearranging Vietnamese words in the dictionaries and consulting the Vietnamese dictionaries, including multilingual dictionaries."
"In the spelling dictionary[REF_CITE], there are 6760 syllables in the writing system (6616 syllables in the phonology system) to compose single words or complex words."
Each syllable has two parts: initial consonant (optional) and rhyme pattern (including rhyme and tone).
"Altogether, there are 27 initial consonants, and 1160 rhyme patterns (including 6 tones)."
"Based on Vietnamese syllable structure, the spelling database is built in a tabular form."
"Each element of the table helps to check the correction of a syllable based on the column position of initial consonants and the row position of rhyme patterns, for example, the syllable lamf (work) in the TELEX form, is composed of the initial consonant l and rhyme pattern am with by low falling tone (or grave accent) f."
"Each element of the table can be understood as: - syllables used in Vietnamese. - elements between tone sign positions (on o: oja or on a: oaj), pronunciation or dialect with spelling (z is equivalent to d or gi, y is equivalent to i...) and borrowings such as karaoke, photocopy, fax... - Sino-Vietnamese word: coongj (addition) → congj, quoocs (country) → nuwowcs... - being unable to form syllables: quts, quoon, coan , cuee..."
"Techniques have been developed to recognize the compound words from two syllables, such as baor damr or damr baor (guarantee), chung chung (vague), etc., from three syllables, such as howpj tacs xax (cooperative), etc., from four syllables, such as coong awn vieecj lamf (work, job), etc."
The error detecting program reads one syllable at a time from the text.
"The syllable is divided into an initial consonant and a rhyme pattern, paying attention to solving initial consonants such as: gi containing vowel i; the consonant qu has vowel u, but it is easy to separate it from the syllable for it does not have the consonant q; the other combined initial consonants have the length of 2, or 3."
The error-correcting unit checks the conformity of initial consonants (if present) and the rhyme pattern.
"At present, there are many Vietnamese fonts built on different codes (different in number of bytes used: 1 byte or 2 bytes, order of tones, letter arrangements, etc.)."
"Because there has not been a unified code for Vietnamese text, we selected a pivot code and TELEX code."
"There are many codes to convert from such[REF_CITE]VISCII, VietKey, VietWare, VNI, TCVN3, Unicode, etc."
Vietools works on syllables converted to TELEX.
Vietools analyses syllables to detect initial consonants and rhyme pattern in TELEX code.
My thanks go to my students for the realization of Vietools and my colleagues for their opinions.
"In particular, I thank Professor Aravind Joshi, University of Pennsylvania, Philadelphia, USA, for his helpful suggestions I am grateful to Christian Boitet, Professor, Joseph Fourier University, GETA, CLIPS, IMAG, France, for his comments on this paper."
Computational linguistics activities in India are being carried out at many institutions.
The activities are centred around development of machine translation systems and lexical resources.
Four major efforts on machine translation in India are presented below.
"The first one is from one Indian language to another, the next three are from English to Hindi."
"In the anusaaraka systems, the load between the human reader and the machine is divided as follows: language-based analysis of the text is carried out by the machine, and knowledge-based analysis or interpretation is left to the reader."
"The machine uses a dictionary and grammar rules, to produce the output."
"Most importantly, it does not use world knowledge to interpret (or disambiguate), as it is an error prone task and involves guessing or inferring based on knowledge other than the text."
Anusaaraka aims for perfect &quot;information preservation&quot;.
We relax the requirement that the output be grammatical.
"In fact, anusaaraka output follows the grammar of the source language (where the grammar rules differ, and cannot be applied with 100 percent confidence)."
This requires that the reader undergo a short training to read and understand the output.
"Among Indian languages, which share vocabulary, grammar, pragmatics, etc. the task (and the training) is easier."
"For example, words in a language are ambiguous, but if the two languages are close, one is likely to find a one to one correspondence between words such that the meaning is carried across from the source language to target language."
"For example, for 80 percent of the Kannada words in the anusaaraka dictionary of 30,000 root words, there is a single equivalend Hindi word which covers the senses of the original Kannada word."
"Similarly, wherever the two languages differ in grammatical constructions, either an existing construction in the target language which expresses the same meaning is used, or a new construction is invented (or an old construction used with some special notation)."
"For example, adjectival participial phrases in the south Indian languages are mapped to relative clauses in Hindi with the ’*’ notati[REF_CITE]."
"Similarly, existing words in the target language may be given wider or narrower meaning[REF_CITE]."
"Anusaarakas are available for use as email servers (anusaaraka, URL)."
The Mantra system translates appointment letters in government from English to Hindi.
It is based on synchronous Tree Adjoining Grammar and uses tree-transfer for translating from English to Hindi.
The system is tailored to deal with its narrow subject-domain.
The grammar is specially designed to accept analyze and generate sentential constructions in &quot;officialese&quot;.
"Similarly, the lexicon is suitably restricted to deal with meanings of English words as used in its subject-domain."
The system is ready for use in its domain.
The Matra system is a tool for human aided machine translation from English to Hindi for news stories.
"It has a text categorisation component at the front, which determines the type of news story (political, terrorism, economic, etc.) before operating on the given story."
"Depending on the type of news, it uses an appropriate dictionary."
"For example, the word ’party’ is usually a ’politicalentity’ and not a ’social event’, in political news."
The text categorisation component uses word-vectors and is easily trainable from pre-categorized news corpus.
"The parser tries to identify chunks (such as noun phrases, verb groups) but does not attempt to join them together."
It requires considerable human assistance in analysing the input.
"Another novel component of the system is that given a complex English sentence, it breaks it up into simpler sentences, which are then analysed and used to generate Hindi."
The system is under development and expected to be ready for use so[REF_CITE].
The English to Hindi anusaaraka system follows the basic principles of information preservation.
It uses XTAG based super tagger and light dependency analyzer developed at University of Pennsylvania [[REF_CITE]] for performing the analysis of the given English text.
It distributes the load on man and machine in novel ways.
The system produces several outputs corresponding to a given input.
"The simplest possible (and the most robust) output is based on the machine taking the load of lexicon, and leaving the load of syntax on man."
"Output based on the most detailed analysis of the English input text, uses a full parser and a bilingual dictionary."
The parsing system is based on XTAG (consisting of super tagger and parser) wherein we have modified them for the task at hand.
"A user may read the output produced after the full analysis, but when he finds that the system has &quot;obviously&quot; gone wrong or failed to produce the output, he can always switch to a simpler output."
"Text[REF_CITE]Indian languages has been prepared with funding from Ministry of Information Technology, Govt. of India."
"Each corpus is of about 3-million words, consisting of randomly chosen text-pieces published from 1970 to 1980."
"The texts are categorized into: literature (novel, short story), science, social science, mass media etc."
"The corpus can be used remotely over the net or obtained on CDs (Corpora, URL)."
"A number of bilingual dictionaries among Indian languages have been developed for the purpose of machine translation, and are available &quot;freely&quot; under GPL."
Collaborative creation of a very large English to Hindi lexical resource is underway.
"As a first step, dictionary with 25000 entries with example sentences illustrating each different sense of a word, has been released on the web (Dictionary, URL)."
"Currently work is going on to refine it and to add contextual information for use in the anusaaraka system, by involving volunteers."
"Morphological analyzers for 6 Indian languages developed as part of Anusaaraka systems are available for download and use (Anusaaraka,URL)."
Sanskrit morphological analyzers have been developed with reasonable coverage based on the Paninian theory by Ramanujan and Melkote.
"Besides the parsers mentioned above, a parsing formalism called UCSG identifies clause boundaries without using sub-categorization information."
Some work has also started on building search engines.
"However, missing are the terminological databases and thesauri."
Spelling checkers are available for many languages.
There is substantial work based on alternative theoretical models of language analysis.
Most of this work is based on Paninian model[REF_CITE].
"In conclusion, there is a large computational linguistic activity in Indian languages, mainly centred around machine translation and lexical resources."
"Most recently, a number of new projects have been started for Indian languages with Govt. funding, and are getting off the ground."
"Bharati, Akshar, and Vineet Chaitanya and Rajeev Sangal, Natural Language Processing: A Paninian Perspective, Prentice-Hall of India,[REF_CITE]"
"Bharati, Akshar, et.al, Anusaaraka: Overcoming the Language Barrier in India, To appear in &quot;Anuvad”. (Available from anusaaraka URL.)"
"Narayana, V. N, Anusarak: A Device to Overcome the Language Barrier, PhD thesis, Dept. of CSE, IITKanpur,[REF_CITE]."
"Rao, Durgesh, Pushpak Bhattacharya and Radhika Mamidi, &quot;Natural Language Generation for English to Hindi Human-Aided Machine Translation&quot;, pp. 179- 189, in KBCS-98, NCST, Mumbai."
"Joshi, A.K. Tree Adjoining Grammar, In D. Dowty et.al. (eds.)"
"Natural Language Parsing, Cambridge[REF_CITE]."
This paper reviews the current state of tech-nology and research progress in the Thai language processing.
It resumes the charac-teristics of the Thai language and the ap-proaches to overcome the difficulties in each processing task.
Processing It is obvious that the most fundamental semantic unit in a language is the word.
Words are ex-plicitly identified in those languages with word boundaries.
"In Thai, there is no word boundary."
"Thai words are implicitly recognized and in many cases, they depend on the individual judgement."
This causes a lot of difficulties in the Thai language processing.
"To illustrate the problem, we employed a classic English exam-ple."
The segmentation of “GODISNOWHERE”.
Segmentation Meaning (1) God is here.
God is now here. (2) God is no where.
God doesn’t exist. (3) God is nowhere.
God doesn’t exist.
"With the different segmentations, (1) and (2) have absolutely opposite meanings. (2) and (3) are ambiguous that nowhere is one word or two words."
And the difficulty becomes greatly ag-gravated when unknown words exist.
"As a tonal language, a phoneme with differ-ent tone has different meaning."
Many unique approaches are introduced for both the tone gen-eration in speech synthesis research and tone recognition in speech recognition research.
"These difficulties propagate to many levels in the language processing area such as lexical ac-quisition, information retrieval, machine trans-lation, speech processing, etc."
Furthermore the similar problem also occurs in the levels of sen-tence and paragraph.
The first and most obvious problem to attack is the problem of word identification and segmen- tation.
"For the most part, the Thai language processing relies on manually created dictionar-ies, which have inconsistencies in defining word units and limitation in the quantity. [1] proposed a word extraction algorithm employing C4.5 with some string features such as entropy and mutual information."
They reported a result of 85% in precision and 50% in recall measures.
"For word segmentation, the longest matching, maximal matching and probabilistic segmenta-tion had been applied in the early research [2], [3]."
"However, these approaches have some limitations in dealing with unknown words."
"More advanced techniques of word segmenta-tion captured many language features such as context words, parts of speech, collocations and semantics [4], [5]."
These reported about 95-99 % of accuracy.
"For sentence segmentation, the tri-gram model was adopted and yielded 85% of accuracy [6]."
"Currently, there is only one machine translation system available to the public, called ParSit[URL_CITE]links.nectec.or.th/services/parsit), it is a service of English-to-Thai webpage translation."
"ParSiT is a collaborative work of NECTEC, Thailand and NEC, Japan."
This system is based on an in-terlingual approach MT and the translation accu-racy is about 80%.
Other approaches such as generate-and-repair [7] and sentence pattern mapping have been also studied [8].
The only Thai text corpus available for research use is the ORCHID corpus.
"ORCHID is a 9-MB Thai part-of-speech tagged corpus initiated by NECTEC, Thailand and Communications Re-search Laboratory, Japan."
ORCHID is available[URL_CITE]
"Frequently used Thai characters are about 80 characters, including alphabets, vowels, tone marks, special marks, and numerals."
"Thai writ-ing are in 4 levels, without spaces between words, and the problem of similarity among many patterns has made research challenging."
"Moreover, the use of English and Thai in general Thai text creates many more patterns which must be recognized by OCR."
"For more than 10 years, there has been a con-siderable growth in Thai OCR research, especially for “printed character” task."
"The early proposed approaches focused on structural matching and tended towards neural-network-based algorithms with input for some special characteristics of Thai characters e.g., curves, heads of characters, and placements."
"At least 3 commercial products have been launched in-cluding “ArnThai” by NECTEC, which claims to achieve 95% recognition performance on clean input."
Recent technical improvement of ArnThai has been reported in [9].
"Recently, fo-cus has been changed to develop system that are more robust with any unclean scanning input."
"The approach of using more efficient features, fuzzy algorithms, and document analysis is re-quired in this step."
"At the same time, “Offline Thai handwritten character recognition” task has been investigated but is only in the research phase of isolated characters."
"Almost all proposed engines were neural network-based with several styles of in-put features [10], [11]."
There has been a small amount of research on “Online handwritten character recognition”.
"One attempt was pro-posed by [12], which was also neural network-based with chain code input."
"Regarding speech, Thai, like Chinese, is a tonal language."
The tonal perception is important to the meaning of the speech.
"The research cur-rently being done in speech technology can be divided into 3 major fields: (1) speech analysis, (2) speech recognition and (3) speech synthesis."
Most of the research in (1) done by the linguists are on the basic study of Thai phonetics e.g. [13].
"In speech recognition, most of the current research [14] focus on the recognition of isolated words."
"To develop continuous speech recogni-tion, a large-scale speech corpus is needed."
The status of practical research on continuous speech recognition is in its initial step with at least one published paper [15].
"In contrast to western speech recognition, topics specifying tonal lan-guages or tone recognition have been deeply researched as seen in many papers e.g., [16]."
"For text-to-speech synthesis, processing the idiosyncrasy of Thai text and handling the tones interplaying with intonation are the topics that make the TTS algorithm for the Thai language differrent from others."
"In the research, the first successful system was accomplished by [14] and later by NECTEC [15]."
Both systems employ the same synthesis technique based on the con-catenation of demisyllable inventory units.
& amp;HDUO\    \
D  DQDO\   0DOD\  
"D      0DOD\[REF_CITE]%H[H[*URXSHZDV G¶(    $XWRQDWLTXH *(7$ * )  YHU:\ WHUPDV   *(7$  860       SURWRW\SH (\   07 V\*(7$¶V $5,$1(\       YDULRXV       "
GD\  WKH      )  &amp;$ NQRZQ  870.        
DQG     WKH 0DOD\  870.   
"GRZQ FRQVLGHUDEO\      RQ     FRPSXWDWLRQDO       SURFHVVLQJ  $0DOD\ 8708QLYHUVLWL WKH     &amp;,&amp;&amp; SURMHFW \ )"
"HQG     $,, V\ $ \ $  0DOD\    DQG % &apos;%3 0DOD\  $FDGHP\ 0DOD.\VLDQ /"
A wide range of natural language problems can be viewed as disambiguating between a small set of alternatives based upon the string context surrounding the ambiguity site.
In this paper we demonstrate that classification accuracy can be improved by invoking a more descriptive feature set than what is typically used.
We present a technique that disambiguates by learning regular expressions describing the stnng contexts in which the ambiguity sites appear. & apos;C Introduction
Most previous work applying machine learning to linguistic disambiguafion has used as features very local collocational information as well as the presence of a word within a fixed window of an ambiguity site.
"Indeed, one of the great insights in both speech recognition and natural language processing is the realization that fixed local cues provide a great deal of useful information."
"While the n-gram reins supreme in language modeling, there has been some interesting work done building language models based on linguistically richer features."
"Bahl,[REF_CITE]describe a language model that builds a decision tree that is allowed to ask questions about the history up to twenty words back."
The language model described[REF_CITE]similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords.
"Samuellson,[REF_CITE]describe a method for learning a particular useful type of pattern, which they call a barrier."
"Given two symbols X and Y, and a set of symbols S, they learn conditions of the form: take an action if there is an X preceded by a Y, with no intervening symbols from S. In their paper they demonstrate how such patterns can be useful for part of speech tagging."
"Below we provide the standard definition for regular expressions, and then define a less expressive language formafism, which we will refer to as reduced regular expressions."
The learning method we describe herein can learn rules conditioned on any reduced regular expression.
"Regular Expression (RE): 1 Given a finite alphabet E, the set of regular expressions over that alphabet is defined[REF_CITE]: (1) Va~ E, a is a regular expression and denotes the set {a} (2) if r and s are regular expressions denoting the languages R and S, respectively, then (r+s), (rs), and (r*) are regular expressions that denote the sets R ~_~S, RS and R* respectively."
"Reduced Regular Expression (R.RE): Given a finite alphabet ~, the set of reduced regular expressions over that alphabet is defined as: (1) Va~ E: a is an RRE and denotes the set {a} a+ is an RRE and denotes the positive closure of the set {a} a* is an RRE and denotes the Kleene closure of the set {a} -a is an RRE and denotes the set ~ - a ~a+ is an RRE and denotes the positive closure of the set Z - a ~a* is an RRE and denotes the Kleene closure of the set E - a (2) . is an RRE denoting the set E (3) .+ is an RRE denoting the positive closure of the set E (4) . * is an RRE denoting the Kleene closure of the set (5) if r and s are RREs denoting the languages R and S, respectively, then rs is an RRE denoting the set RS."
"Some examples of strings that are regular expressions but not reduced regular expressions include: (ab)*, a(b]c)d, (a (bc)+)*"
"Next, we need some definitions to allow us to make reference to particular positions in a collection of strings."
Corpus: A corpus is an ordered set of strings.
We will notate the jth String of a corpus C as C[j]. ]C] is the number of strings in the corpus.
IqJ]l is the number of symbols in the jt~ string of the corpus.
"A corpus position for a corpus C is a tuple (j,k), meaning the kthsymbol in the jtb string in the corpus, with the restrictions: 1_&lt;j _&lt;[C[and 0 _&lt;k _&lt;[CU] [."
A Corpus Position Set is a set of corpus positions.
"Next, we define an RRE-Tree, the data structure we will use in learning RREs."
"RRE-Tree: An RRE-Tree over E is a tree (V,E), where V is a set of tuples &lt;v,S&gt;, v being a unique vertex identifier and S being a Corpus Position Set, and E is a set of labeled directed edges &lt;vi,vj,label&gt;, where vi and vj are vertex identifiers, label ~"
"LABEL_SET and LABEL SET = {dot, dot+, dot*} U {a,a+,a*,~a,~a+,~a* I &apos;Va ~ E }.2"
Our implemented learner is based upon the transformation-based learning paradigm[REF_CITE].
In this section we briefly review transformation-based learning.
"I In all of our formulations, we ignore expressions denoting the empty set O and the set {e}. 2We use &quot;dot&quot; for &quot;.&quot; 2"
"In string classification, the goal is to assign the proper label to a string, from a prespecified set of labels L. A transformation-based system consists of: (1) A start-state annotator, which assigns an initial label to a string. (2) A sequence of rules of the form: Change the label of a string from m to n if C(string), where C is a predicate over strings and m,n ~ L."
"A string is labelled by first applying the start-state annotator to it, and then applying each rule, in order."
"To learn a transformation sequence, the system begins with a properly labelled training set."
It then removes the labels and applies the start-state annotator to each string.
Then the learner iteratively does the following: (1) Find the best rule to apply to the training set. (2) Append that rule to the end of the learned transformation sequence. (3) Apply that rule to the training set. : until the stopping criterion is met.
Below we will demonstrate how to learn transformation sequences where the predicate C(stfing) is of the form &quot;Does RRE R apply to the stnng?&quot; We will show this for the binary classification case (where ILl = 2).
"In each learning iteration, we will construct an RRE-Tree in a particular way, find the best node in that RRE-Tree, and then return the edge labels on the path from root to best node as the learned RRE."
The learner will learn a sequence of rules of the form: Change the label of a string from li to lj if the string matches reduced regular expression R.
"Before proceeding, we need to specify two things: the start-state annotator and the goodness measure for determining what rule is best."
"The system will use a start-state annotator that initially labels all strings with the most frequent label in the training set, and the goodness measure will simply be the number of good label changes minus the number of bad label changes when a rule is applied to the training set."
Take the following training set:
String # String True Label Init.
Guess 1 abc 0 1 2 abb 1 1 3 baa 1 1
"Since 1 is the most frequent label in the training set, the start-state annotator would initially assign all three training set strings the label 1, meaning stnng 1 would be incorrectly labelled and strings 2 and 3 would be correct."
Now we want to learn a rule whose appfication will best improve our labelling of the training set.
"We will first present an algorithm for constructing an RRE-Tree for a training corpus, and then trace through the appficadon of this algorithm to our example training corpus above."
"To simplify the presentation, we will limit ourselves to learning rules for a weaker language type, which we call Very Reduced Regular Expressions (VRREs)."
The extension to RRE learning is straightforward.
"Very Reduced Regular Expression (VRRE): Given a finite alphabet E, the set of very reduced regular expressions over that alphabet is defined as: (1) &apos;v&apos;a~ E: a is a VRRE and denotes the set {a} (2) . is a VRRE denoting the set g (3) . * is a VRRE denoting the Kleene closure of the set E (4) if r and s are VRREs denoting the languages R and S, respectively, then rs is a VRRE denoting the set RS."
"Say we have a training corpus C. For every string C[j]~ C, Tmth[C[j]] ~ {0,1} is the true label of C[j] and Guess[C[j]] is the current guess of the label of C[j]."
"The algorithm for one iteration of rule learning follows. (1) Create root node with corpus position set S = {0,0) [j = 1 .. ICI)."
Push this node onto processing stack (STACK). (2) While (STACK not empty) { STATE = pop(STACK);
"Push(dotstarexpand (STATE), STACK);"
"Push(atomexpand(a, STATE),STACK) } (3) Find best state S in the RRE-tree."
"Let R be the RRE obtained by following the edges from the root to S, outputting each edge label as the edge is traversed."
"Return either the rule &quot;0--~1 if R&quot; or &quot;1--)0 if R&quot; depending on which is appropriate for state S. } dotexpand(STATE) { create new state STATE&quot; let P be the corpus position set of STATE P&apos; = {0,k) I(j,k-1) E P and k-1 ~"
ICorpusfj]l} If (P&apos; not empty) { Make P&apos; the corpus position set of
"STATE&apos; Add (STATE,STATE&apos; ,DOT) to tree edges return STATE&apos; } Else return NULL } dotstarexpand(STATE) { create new state STATE&apos; let P be the corpus position set of STATE P&apos; = {(j,k) [(j,m) ~ P, m_&lt; k, and k _&lt; ICorpusU]l} If(P&apos;¢ P) { Make P&apos; the corpus position set of STATE&apos; Add (STATE,STATE&apos;,DOT*) to tree edges return STATE&apos; }"
"Else return NULL } atomexpand(a, STATE) { create new state STATE&apos; let P be the corpus position set of STATE"
"P&apos; = {(j,k) I(j,k-1) E P, k-1 ¢ Icorpusfj]l, and the k-1st symbol in Corpus[j] is a}"
If (P&apos; not empty) {
Make P&apos; the corpus position set of STATE
"Add (STATE,STATE&apos;,a) to tree edges return STATE&apos; }"
Else return NULL }
"Each state S in the RRE-tree represents the RRE corresponding to the edge labels on the path from root to S. For a state S with corpus position set P and corresponding RRE R, the goodness of the rule: 0~1 if R, is computed asp"
"Similarly, we can compute the score for the rule : 1@0 if R. We then define Goodness(S) = max(Goodness0_to l(S),Goodness_l_to_0(S))"
"Returning to our example, for the [Footnote_3] strings in this training corpus, the root node of the RRE-Tree would have the corpus position set: {(1,0),(2,0),(3,0)}."
3 This assumes an RRE must match the entire string in order to accept it.
"The root node corresponds to the null RRE, and so the position set consists of the beginning of each string in the training set."
In figure 1 (at the end of the paper) we show a partial RRE-Tree.
"If we follow the edge labelled &quot;dot&quot; from the root node, we see it leads to a state with position set {(1,1),(2,1),(3,1)}, as a dot advances all positions by one."
The square state in figure 1 represents the RRE: &quot;dot* c&quot; and the triangular state represents &quot;a dot c&quot;.
"Both the square and triangular states have a corpus position set consisting of only one corpus position, namely the end of stnng 1, and both would have a goodness score of 1 for the corresponding l&amp;0 rule."
"If Weprefer shorter rules, we will learn as our first rule in the rule list: l&amp;0 if dot* c. After applying this rule to the training corpus, all strings will be correctly labelled and training will terminate."
"If the stopping criterion were not met, we would apply the learned rule to change the values of our Guess array, then create a new RRE-tree, find the best state in that tree, and so on."
It is easy to extend the above algorithm to learn RREs instead of VRREs.
"Note, for instance, that the corpus position set for a state S with incoming edge labelled ~a can be found by taking the position set for the sibling of S with incoming edge labelled dot and deleting those corpus positions that are found in the position set for the sibling of S with incoming edge labelled a. ..? 5 Optimizations"
The algorithm above is exponential.
There are some opfirnizations we can perform that make it feasible to apply the learning algorithm.
Optimization 1: Pruning states we know cannot be on the path from root to the best state.
"Define GoodPotential 0 to I(S) as the number of sentences s in the training corpus for which Guess[s]=0, Truth[s]=1 and 3k :(s, k) ~ corpus_position_set(S)."
"We can similarly define GoodPotential 1 to0(S), and then define"
"GoodPotential(S)= max(GoodPotential 0 to_l(S), GoodPotential 1 to O(S))"
"As we construct the RRE-tree, we keep track of the largest Goodness(S) we have encountered."
"If that value is X, then for a state S&apos;, if GoodPotential(S&apos;)_&lt;X, it is impossible for any path through S&apos; to reach a state with a better goodness score than the best found thus far."
"We can check this condition when pushing states onto the stack, and when popping off the stack to be processed, and if the pruning condition is met, the state is discarded."
Optimization 2: Merging states with identical corpus position sets.
"If we are going to push a state onto the stack when a state already exists with an identical corpus position set, we do not need to retain both states."
We may use heuristics to decide which of the states with identical corpus position sets we should keep (such as choosing the one with the shortest path to the root).
"To test whether learning RREs can improve disambiguafion accuracy, we explored the task of confusion set disambiguati[REF_CITE]."
"We trained and applied two different rule sequence learners, one which used the standard feature set for this problem (e.g. the identical feature set to that used[REF_CITE]and[REF_CITE]and described in the introduction, and one which learned RR.Es.[Footnote_4] Because we wanted to deterinine what could be gained by using RREs, we ran an ablation study where we kept everything else constant across the two runs, and did not use performance enhancing techniques such as parameter tuning on held out data or classifier combination."
4 The set of RREs is a superset of what can be learned using the standard feature set.
Both learners were given a window of +/- 5 words surrounding the ambiguity site.
Context was not allowed to cross sentence boundaries.
"The training and test set were derived by finding all instances of the confusable words in the Brown Corpus, using the Penn Treebank parts of speech and tokenization (Marcus,[REF_CITE]), and then dividing this set into 80% for training and 20% for testing."
"For the RRE-based system, we mapped the +/- 5 word window of context into a string as follows (where wi is a word and ti is a part of speech tag):"
Wi.[Footnote_5] ti.5 Wi-4 ti.4 Wi.3 ti.3
5 Note that this does not imply a bound on the length of a string to which an RRE can apply.
Wi+l ti+l wi+2 ti+2 wi+3 ti+3 wi+4 ti+4 wi+5 ti+5 where MIDDLE is the ambiguity site.
"Both for execution time and space considerations for the learner and for fear of overtraining, we put a bound on the length of the RRE that could be learned,s"
We define an atomic RRE as any RRE derived without any concatenation operations.
Then the length of an RRE is defined as the number of atomic RREs which that RRE is made up of.
The atom &quot;MIDDLE&quot; is not counted in length.
Below we give two examples of rules that were learned for one confusion set: [Footnote_6] (1) past ~ passed if .* ~DT MIDDLE DOT IN (2) past ~ passed if (~to)*
"6 DT= determiner, IN = preposition, biN = singular noun."
The first rule says to change the disambiguation guess to &lt;&lt;passed &gt;&gt;if the word before is not a determiner and the word after is a the accuracy attained on the test set by always picking the word that appears more frequently in the training set. preposition.
This matches contexts such as : &lt;&lt; ... they passed by ... &gt;&gt; while not matching In Table 2 we see that the RRE-based contexts such as : &lt;&lt;... made in the past by ... &gt;&gt; system outperforms the standard system on 9 of
"The second rule captures contexts such as : &lt;&lt;... the hike passed the campground ... &gt;~while not matching contexts such as : &lt;&lt;... want to take a hike past the campground... &gt;&gt; In Table 1, we show test set results from running the rule sequence learner with both the standard set of features and with RRE-based features.[Footnote_7] The results are sorted by training corpus size, with the raise/rise training corpus being the smallest and the then/than training corpus being the largest."
7 While these results look worse than those achieved
"Baseline accuracy is by other systems, as reported[REF_CITE], we used different data splits and tokenization."
Our baseline accuracies are significantly lower than the baselines for their test sets.
"If we account for this by instead measuring percent error reduction compared to baseline accuracy, then our average reduction is better than that reported for the BaySpell system, but worse than that of WinSpell."
"If we add voting to our system (WinSpell employs voting), then we attain results on par with WinSpell. the confusion sets, the standard system outperforms the RRE-based system on 2 and the two systems attain identical results on 3."
We see that the relative performance of the RRE-based learner is better overall on the larger training sets than on the smaller sets.
"This is to be expected, as more data is needed to support learning the more expressive RRE-based rules. overall accuracy of 88.4%, compared to 86.9% for the standard learner."
We believe that identifyingthe structure of scien- BACKGROUND: tific argumentation in articles can help in tasks Researchers in knowledge representa-such as automatic summarization or the auto- tion agree thatone of the hard problems of mated construction of citation indexes.
One par- understanding narrativeis the representation ticularly important aspect of this structure is the oftemporal information.
"Certain factsofnat-question of who a given scientific statement is at- urallanguage make ithard to capture tempo-tributed to: other researchers, the field in general, ralinformation [...] or the authors themselves."
We present the algorithm and a systematic eval-
"Recently, Researcher-4 has suggested the uation of a system which can recognize the most followingsolutionto thisproblem [...]. salient textual properties that contribute to the"
The verb conclude/propose&quot;.
Such phrases can be useful classes are based on semantic concepts such as indicators of overall importance.
"However, for similarity,contrast, competition, presentation, ar-our task, more flexible meta-diiscourse expressions gumentation and textual structure."
For ex-need to be determined.
"The ,description of a re- ample, PRESENTATION.."
"ACTIONSinclude commu-search tradition, or the stateraent that the work nication verbs like &quot;present&quot;, &quot;report&quot;, &quot;state&quot; described in the paper is the continuation of some[REF_CITE], RE-other work, cover a wide range of syntactic and SEARCH_ACTIONS include &quot;analyze&quot;, &quot;conduct&quot; lexical expressions and are too hard to find for a and &quot;observe&quot;, and ARGUMENTATION_ACTIONS mechanism like simple pattern matching."
Agent Type Example US-AGENT we THEM_AGENT his approach GENERAL_AGENT traditional methods US_PREVIOUS.
AGENT the approach given in X (99) OUR_AIM_AGENT the point o] this study REF_US_AGENT thia paper REF._AGENT the paper THEM_PRONOUN_AGENT they AIM_I:
LEF_AGENT its goal GAP_AGENT none of these papers PROBLEM_AGENT these drawbacks SOLUTION_AGENT a way out o] this dilemma
TEXTSTRUCTURE_AGENT the concluding chap- ter
Figure 4: Agent Lexicon: 168[REF_CITE]Classes We suggest that the robust recognition of pro-totypical agents and actions is one way out of this dilemma.
The agents we propose to recognize de-scribe fixed role-players in the argumentation.
"In Figure 1, prototypical agents are given in bold-face (&quot;Researchers in knowledge representation, &quot;Researcher-4&quot; and &quot;we&quot;)."
"We also propose pro-totypical actions frequently occurring in scientific discourse (shown underlined in Figure 1): the re-searchers &quot;agree&quot;, Researcher-4 &quot;suggested&quot; some-thing, the solution &quot;cannot be used&quot;."
We will now describe an algorithm which rec-ognizes and classifies agents and actions.
"We use a manually created lexicon for patterns for agents, and a manually clustered verb lexicon for the verbs."
Figure 4 lists the agent types we dis-tinguish.
"The main three types are US_aGENT, THEM-AGENT and GENERAL.AGENT."
"A fourth type is US.PREVIOUS_AGENT(the authors, but in a previous paper)."
"Additional agent types include non-personal agents like aims, problems, solutions, absence of solution, or textual segments."
"There are four equivalence classes of agents with ambiguous reference (&quot;this system&quot;), namely REF_US_AGENT, THEM-PRONOUN_AGENT, AIM.-REF-AGENT, REF_AGENT."
"The total of 168 patterns in the lexicon expands to many more as we use a replace & quot;argue&quot;, &quot;disagree&quot;, &quot;object to&quot;."
"Domain-specific actions are contained in the classes indicating a problem ( &quot;.fail&quot;, &quot;degrade&quot;, &quot;overestimate&quot;), and solution-contributing actions (&quot; &quot;circumvent&apos;, solve&quot;, &quot;mitigate&quot;)."
"The main reason for using a hand-crafted, genre--specific lexicon instead of a general resource such as WordNet or Levin&apos;s (1993) classes (as used[REF_CITE]), was to avoid polysemy problems without having to perform word sense disambiguation."
"Verbs in our texts often have a specialized meaning in the domain of scientific ar-gumentation, which our lexicon readily encodes."
"We did notice some ambiguity problems (e.g. &quot;fol-low&quot; can mean following another approach, or it can mean follow in a sense having nothing to do with presentation of research, e.g. following an arc in an algorithm)."
"In a wider domain, however, ambiguity would be a much bigger problem."
"Processing of the articles includes transforma-tion from I~TEX into XML format, recognition of formal citations and author names in running text, tokenization, sentence separation and POS-tagging."
The pipeline uses the TTT software pro-vided by the HCRC Language Technology Gro[REF_CITE].
The algorithm for deter-mining agents in subject positions (or By-PPs in passive sentences) is based on a finite automaton which uses POS-input; cf.
"In the case that more than one finite verb is found in a sentence, the first finite verb which has agents and/or actions in the sentences is used as a value for that sentence."
We carried out two evaluations.
"Evaluation A tests whether all patterns were recognized as in-tended by the algorithm, and whether patterns were found that should not have been recognized."
Evaluation B tests how well agent and action recognition helps us perform argumentative zon-ing automatically.
"We first manually evaluated the error level of the POS-Tagging of finite verbs, as our algorithm cru-cially relies on finite verbs."
"In a random sample of 100 sentences from our corpus (containing a total of 184 finite verbs), the tagger showed a recall of 1. Start from the first finite verb in the sentence. 2. Check right context of the finite verb for verbal forms of interest which might make up more complex tenses."
Remain within the assumed clause boundaries; do not cross commas or other finite verbs.
"Once the main verb of that construction (the &quot;semantic&quot; verb) has been found, a simple morphological analysis determines its lemma; the tense and voice of the construction follow from the succession of auxiliary verbs encountered. 3. Look up the lemma of semantic verb in Action Lexicon; return the associated Action Class if successful."
"Else return Action 0. 4. Determine if one of the 32 fixed negation words contained in the lexicon (e.g. &quot;not, don&apos;t, neither&quot;) is present within a fixed window of 6 to the right of the finite verb. 5. Search for the agent either as a by-PP to the right, or as a subject-NP to the left, depending on the voice of the construction as determined in step 2."
Remain within assumed clause boundaries. 6.
"If one of the Agent Patterns matches within that area in the sentence, return the Agent Type."
Else return Agent 0. 7. Repeat Steps 1-6 until there are no more finite verbs left. 95% and a precision of 93%.
"At the point where John &lt;ACTION We found that for the 174 correctly determined TENSE=Pi~SENT VOICE=ACTIVE MODAL=NOMODAL NEGATION=0 finite verbs (out of the total 184), the heuristics for"
ACTIONTYPE=0&gt; knows &lt;/ACTION&gt; the truth negation worked without any errors (100% accu- has been &lt;FINITE TENSE=PRESENT_PERFECT racy).
The correct semantic verb was determined
VOICE=PASSIVE MODAL=NOMODAL[REF_CITE]% percent of all cases; errors are mostly due
TION=0 ACTIONTYPE=0&gt; processed to misrecognition of clause boundaries.
"Action &lt;/ACTION&gt; , a complete clause will have Type lookup was fully correct, even in the case been &lt;ACTION TENSE=FUTURE.PERFECT of phrasal verbs and longer idiomatic expressions VOICE=ACTIVE MODAL=NOMODAL NEGA- (&quot;have to&quot; is a NEED..ACTION;&quot;be inspired by&quot; is TION=0 ACTIONTYPE=0&gt; built &lt;/ACTION&gt; a, CONTINUE_ACTION)."
"There were 7 voice errors, 2 of which were due to POS-tagging errors (past participle misrecognized)."
The remaining 5 voice Figure 7: Sample Output of Action Detection errors correspond to a 98% accuracy.
Figure 7 gives an example for a voice error (underlined) in the output of the action/agent determination.
"Correctness of Agent Type determination was tested on a random sample of 100 sentences con-taining at least one agent, resulting in 111 agents."
No agent pattern that should have been identi- of a subject NP (typically the NP in a postmodify-fied was missed (100% recall).
"There was one no grave errors, as they still give an indication complete error, caused by a POS-tagging error."
"In which type of agents the nominal phrase is associ- 5 of the 111 agents, the pattern covered only part ated with. 4.2 Evaluation B: Usefulness for Baseline 2, the most frequent category (OWN),"
Argumentative Zoning is a particularly bad baseline: its recall on all cate- We evaluated the usefulness of the Agent and Ac- gories except OWN is zero.
"We cannot see this bad tion features by measuring if they improve the performance in the percentage accuracy values, classification results of our stochastic classifier for but only in the Kappa values (measured against argumentative zones. one human annotator, i.e. k=2)."
As Kappa takes (K=0).
"Nevertheless, if taken together with the by sentence."
"The ngram model combines evidence other features, it still improves results. from the context (Cm-1,Cm-2) and from I senten-tiai features (F,~,o...Fmj-t), assuming that those Building on the idea that intellectual attribu-two factors are independent of each other."
"It uses tion is a segment-based phenomena, we improved the same likelihoodestimation as the Naive Bayes, the Agent feature by including history (feature but maximises a context-sensitive prior using the SAgent)."
The assumption is that in unmarked sen-
"We received best results for tences the agent of the previous attribution is still n=2, i.e. a bigram model. active."
"K=.21, which makes SAgent the best single fea-ing a total of 12422 sentences (classified items). tures available in the entire feature pool."
"Inclusion As the first baseline, we use a standard text cat- of SAgent to the final model improved results to egorization method for classification (where each K=.43 (bigram model). sentence is considered as a document*) Baseline 1[REF_CITE]also shows that different features are has an accuracy of 69%, which is low considering better at disambiguating certain categories."
"The that the most frequent category (OWN) also coy- Formulaic feature, which is not very strong on errs 69% of all sentences."
"Worse still, the classifier its own, is the most diverse, as it contributes to classifies almost all sentences as OWN and OTHER the disambiguation of six categories directly."
Both segments (the most frequent categories).
"Recall on Agent and Action features disambiguate cate-, the rare categories but important categories AIM, gories which many of the other 12 features cannot TEXTUAL, CONTRAST and BASIS is zero or very disambiguate (e.g. CONTRAST),and SAgent addi-low."
Text classification is therefore not a solution. tionally contributes towards the determination of *
"We used the Rainbow implementation of a Naive Bayes BACKGROUND zones (along with the Fo~ulaic tf/idf method, 10-foldcross-validation. and the Absolute Location feature)."
Another factor which decreases results are in-
"The result for automatic classification is in agree- consistencies in the training data: we discovered ment with our previous experimental results for that 4% of the sentences with the same features human classification: humans, too, recognize the were classified differently by the human annota-categories AIM and TEXTUAL most robustly (cf. tion."
This points to the fact that our set of fea-[REF_CITE]).
"AIM and TEXTUAL sentences, stating tures could be made more distinctive."
"In most knowledge claims and organizing the text respec-of these cases, there were linguistic expressions tively, are conventionalized to a high degree."
"The present, such as subtle signs of criticism, which system&apos;s results for AIM sentences, for instance, humans correctly identified, but for which the fea-compares favourably to similar sentence extraction tures are too coarse."
"Therefore, the addition of experiments (cf."
"Kupiec et al.&apos;s (1995) results of &quot;deeper&quot; features to the pool, which model the se- 42%/42% recall and precision for extracting &quot;rel-mantics of the meta-discourse shallowly, seemed evant&quot; sentences from scientific articles)."
BASIS a promising avenue.
"We consider the automatic and CONTRAST sentences have a less prototypical and robust recognition of agents and actions, as syntactic realization, and they also occur at less presented here, to be the first incarnations of such predictable places in the document."
"Therefore, it features. is far more difficult for both machine and human to recognize such sentences."
"While the system does well for AIM and TEX-TUALsentences, and provides substantial improve-"
"Argumentative zoning is the task of breaking a ment over both baselines, the difference to human text containing a scientific argument into linear performance is still quite large (cf. figure 11)."
"We zones of the same argumentative status, or zones attribute most of this difference to the modest size of the same intellectual attribution."
We plan to of our training corpus: 80 papers are not much for use argumentative zoning as a first step for IR and machine learning of such high-level features.
"It is shallow document understanding tasks like sum-possible that a more sophisticated model, in com- marization."
"In contrast to hierarchical segmenta-bination with more training material, would im- tion (e.g. Marcu&apos;s (1997) work, which is based on prove results significantly."
"However, when we ran RST[REF_CITE]), this type of them on our data as it is now, different other sta- segmentation aims at capturing the argumentative tistical models, e.g. Ripper[REF_CITE]and a status of a piece of text in respect to the overall Maximum Entropy model, all showed similar nu- argumentative act of the paper."
It does not deter-
Sub- with set-valued features.
"In Proceedingsof AAAL zone structure is most likely related to domain- 96. specific rhetorical relations which are not directly Grocer, Claire, Andrei Mikheev, and Colin Mathe-relevant to the discourse-level relations we wish to son. 1999."
LT TTT Version 1.0:
Text Tokenisa-recognize. tion Software.
"Technical report, Human Commu-We have presented a fully implemented proto- nication Research Centre, University of Edinburgh. type for argumentative zoning."
"Its main inno- http ://~w. ltg. ed. ac. uk/software/ttt/. vation are two new features: prototypical agents Hearst, Marti A. 1997."
TextTiling: Segmenting text and actions -- semi-shallow representations of the into multi-paragraph subtopic passages.
Computa-overall scientific argumentation of the article.
"For tional[REF_CITE](1): 33---64. agent and action recognition, we use syntactic Hyland, Ken. 1998."
Persuasion and context: The prag-heuristics and two extensive libraries of patterns. matics of academic metadiscourse.
Journal o]Prag-Processing is robust and very low in error.
Linear Segmentation and Segment prove results for automatic argumentative zoning Significance.
In Proceedingso~the Sixth Workshop considerably.
"History-aware agents are the best on Very Large Corpora (COLIN G/ACL-98), 197-single feature in a large, extensively tested feature 205. pool."
"In general classification problems, there are y = sgn (i;x~~esVs oLiyiK(xi,x)+b). (10) cases in which it is unable to separate the training data linearly."
"In such cases, the train-ing data could be separated linearly by ex- 3 Dependency Analysis using panding all combinations of features as new SVMs ones, and projecting them onto a higher- 3.1 The Probability Model dimensional space."
"However, such a naive ap-proach requires enormous computational over-"
"This section describes a general formulation of head. the probability model and parsing techniques Let us consider the case where we project for Japanese statistical dependency analysis. the training data x onto a higher-dimensional First of all, we let a sequence of space by using projection function • 1 As chunks be {bz,b2...,bm} by B, and we pay attention to the objective function (5) the sequence dependency pattern be and the decision function (6), these functions {Dep(1),Dep(2),...,Dep(m - 1)} by D, depend only on the dot products of the in- where Dep(i) = j means that the chunk b~ put training vectors."
"If we could calculate the depends on (modifies) the chunk bj. dot products from xz and x2 directly without In this framework, we suppose that the de-considering the vectors ~(xz) and ¢(x2) pro- pendency sequence D satisfies the following jected onto the higher-dimensional space, we constraints. can reduce the computational complexity con-siderably."
"Namely, we can reduce the compu- 1."
"Except for the rightmost one, each chunk tational overhead if we could find the function depends on (modifies) exactly one of the K that satisfies: chunks appearing to the right. 2."
Dependencies do not cross each other. ~(xl) &quot;¢(x2) ----K(Xl~ x2). (8)
"Statistical dependency structure analysis On the other hand, since we do not need is defined as a searching problem for the itself for actual learning and classification, dependency pattern D that maximizes the 1In general, ,It(x) is a mapping into Hilbert space. conditional probability P(DIB ) of the in- 20 put sequence under the above-mentioned con- (11) shows that the distance between test data straints. frO and the separating hyperplane is put into the sigmoid function, assuming it represents Dbest = argmax P(D[B) the probability value of the dependency rela-"
"We adopt this method in our experiment If we assume that the dependency probabil-to transform the distance measure obtained ities are mutually independent, P(DIB ) could in SVMs into a probability function and an-be rewritten as: alyze dependency structure with a fframework rn-1 of conventional probability model 2 P(DIB) = ~I P(Dep(i)=j Ifit) i=1 3.3 Static and Dynamic Features fit = {fl,...,fn} e Rn."
"Features that are supposed to be effective in Japanese dependency analysis are: head P(Dep(i) = J If0) represents the probability words and their parts-of-speech, particles and that bi depends on (modifies) bt. fit is an n di- inflection forms of the words that appear mensional feature vector that represents var- at the end of chunks, distance between two ious kinds of linguistic features related with chunks, existence of punctuation marks."
"As the chunks bi and bt. those are solely defined by the pair of chunks,"
Weobtain Dbest taking into all the combina- we refer to them as static features. tion of these probabilities.
"Generally, the op- Japanese dependency relations are heavily timal solution Dbest Can be identified by using constrained by such static features since the bottom-up algorithm such as CYK algorithm. inflection forms and postpositional particles Sekine suggests an efficient parsing technique constrain the dependency relation."
"However, for Japanese sentences that parses from the when a sentence is long and there are more end of a sentence(Sekineet al., 2000)."
"We ap- than one possible dependents, static features, ply Sekine&apos;s technique in our experiments. by themselves cannot determine the correct ..? dependency."
Let us look at the following ex- 3.2 Training with SVMs ample.
"In order to use SVMs for dependency analysis, we need to prepare positive and negative ex-amples since SVMs is a binary classifier."
We adopt a simple and effective method for our watashi-ha kono-hon-wo motteim josei-wo sagasiteiru
"I-top, be looking forthisbook-acc, have, lady-acc, purpose: Out of all combination of two chunks in the training data, we take a pair of chunks that axe in a dependency relation as a positive In this example, &quot;kono-hon-wo(this book-example, and two chunks that appear in a sen- acc)&quot; may modify either of &quot;motteiru(have)&quot; tence but are not in a dependency relation as or &quot;sagasiteiru(be looking for)&quot; and cannot a negative example. be determined only with the static features."
"However, &quot;josei-wo (lady-acc)&quot; can modify LJ (f J,y t) = {(f12,y12), (f23,v23), the only the verb &quot;sagasiteiru,&quot;."
"Knowing such information is quite useful for resolv- ~l_&lt;j&lt;m ing syntactic ambiguity, since two accusative ---, (fro-1m, Ym-1m)} noun phrses hardly modify the same verb."
"It is possible to use such information if we add fij = {fl,---, fn) e R n new features related to other modifiers."
"In the above case, the chunk &quot;sagasiteiru&quot; can Yij E (Depend(q-l), Not-Depend(-1)} receive a new feature of accusative modifica-tion (by &quot;josei-wo&quot;) during the parsing pro-"
"Then, we define the dependency probability cess, which precludes the chunk &quot;kono-hon- P (Dep( i) = j lf&apos;ij ): wo&quot; from modifying &quot;sagasiteiru&quot; since there is a strict constraint about double-accusative P(Dep(i) =j If&apos;ij) = 2Experimentally, itisshown that tliesigmoid func- (11) tion gives a good approximation of probability func-tion from the decision function of SVMs[REF_CITE]. modification that will be learned from train-"
Head ing examples.
"We decided to take into consid- (surface-form, POS, eration all such modification information by POS-subcategory, using functional words or inflection forms of inflection-type, modifiers. inflection-form),"
"Using such information about modifiers in Type the training phase has no difficulty since they Left/ (surface-form, POS,"
Static are clearly available in a tree-bank.
"On the Right POS-subcategory,"
"Features other hand, they are not known in the parsing Chunks inflection-type, phase of the test data."
"This problem can be inflection-form), easily solved if we adopt a bottom-up parsing brackets, algorithm and attach the modification infor- quotation-marks, mation dynamically to the newly constructed punctuation-marks, phrases (the chlmks that become the head of position in sentence the phrases)."
"As we describe later we apply a (beginning, end) beam search for parsing, and it is possible to distance(I,2-5,6-), keep several intermediate solutions while sup- Between case-particles, pressing the combinatorial explosion."
"Chunks brackets, We refer to the features that are added in- quotation-marks, crementally during the parsing process as dy- punctuation-marks namic features."
Form of functional words or in- Dynamic flection that modifies the right 4 FeaturesExperiments and Discussion chunk 4.1 Experiments Setting
"Table 1: Features used in experiments We use Kyoto University text corpus (Ver-sion 2.0) consisting of articles of Mainichi newspaper annotated with dependency struc-ture[REF_CITE]. 7,958 sen- apply a simple filtering based on the part-of-tences from the articles on January 1st to Jan- speech of functional words: We use the lexical uary 7th are used for the training data, and form if the word&apos;s POS is particle, adverb, ad- 1,246 sentences from the articles on January nominal or conjunction."
We use the inflection 9th are used for the test data.
For the kernel form if the word has inflection.
"We use the function, we used the polynomial function (9)."
POS tags for others.
We set the soft margin parameter C to be 1.
The feature set used in the experiments are shown in Table 1.
The static features are ba-
"Table 2 shows the result of parsing accuracy sically taken from Uchimoto&apos;s list(Uchimoto under the condition k = 5 (beam width), and et al., 1999) with little modification."
"In Table d = 3 (dimension of the polynomial functions 1, &apos;Head&apos; means the rightmost content word used for the kernel function). in a chunk whose part-of-speech is not a func-"
"This table shows two types of dependency tional category. &apos;Type&apos; means the rightmost accuracy, A and B. The training data size is functional word or the inflectional form of the measured by the number of sentences."
The ac-rightmost predicate if there is no functional curacy A means the accuracy of the entire de-word in the chunk.
The static features in- pendency relations.
"Since Japanese is a head-clude the information on existence of brack- final language, the second chunk from the end ets, question marks and punctuation marks of a sentence always modifies the last chunk. etc."
"Besides, there are features that show The accuracy B is calculated by excluding this the relative relation of two chunks, such as dependencyrelation."
"Hereafter, we use the ac-distance, and existence of brackets, quotation curacy A, if it is not explicitly specified, since marks and punctuation marks between them. this measure is usually used in other litera- For dynamic features, we selected func- ture. tional words or inflection forms of the right- 4.3 Effects of Dynamic Features most predicates in the chunks that appear be-tween two chunks and depend on the modi- Table3 shows the accuracy when only static flee."
"Considering data sparseness problem, we features are used."
"Generally, the results with"
Training Dependency Accuracy Sentence Dimension Dependency Sentence
The parser achieves 86.52% accuracy for 6756 45.04%88.55% 87.13% test data even with small training data (1172 7958 88.77% 87.38% 45.04% sentences).
This is due to a good character-
"Table 3: Result without dynamic features istic of SVMs to cope with the data sparse- (d = 3, k = 5) ness problem."
"Furthermore, it achieves almost 100% accuracy for the training data, showing that the training data are completely sepa-rated by appropriate combination of features. dynamic feature set is better than the results Generally, selecting those specific features of without them."
"The results with dynamic fea-the training data tends to cause overfitting, tures constantly outperform that with static and accuracy for test data may fall."
"However, features only."
"In most of cases, the improve-the SVMs method achieve a high accuracy not ments is significant."
"In the experiments, we only on the training data but also on the test restrict the features only from the chunks that data."
We claim that this is due to the high appear between two chunks being in consider-generalization ability of SVMs.
"In addition, ation, however, dynamic features could be also observing at the learning curve, further im-taken from the chunks that appear not be-provement will be possible if we increase the tween the two chunks."
"For example, we could size of the training data. also take into consideration the chunk that is modified by the right chunk, or the chunks Kernel Function vs. Accuracy4.5"
Table 4 shows the relationship between the di-mension of the kernel function and the parsing W accuracy under the condition k -- 5.
"As a result, the case of d ----4 gives the best accuracy."
We could not carry out the training m.5 in realistic time for the case of d = 1.
"This result supports our intuition that we need a combination of at least two features. 8,r"
"In other words, it will be hard to confirm a dependency relation with only the features of / the modifier or the modfiee."
It is natural that a dependency relation is decided by at least ~0 ~o0 4c00 $000 ~00 7Oo0
"Nt~IberofTriL~img0,11~a(l~&apos;sl~c~) the information from both of two chunks."
"In addition, further improvementhas been pos- Figure 1: Training Data vs Accuracy sible by considering combinations of three or more features."
"Beam Dependency Sentence viding all training data (7958 sentences) by Width Accuracy Accuracy 4, the final dependency score is given by a 1 88.66% 45.76% weighted average of each scores."
"This simple 3 88.74% 45.20% voting approach is shown to achieve the ac- 5 88.77% 45.36% curacy of 88.66%, which is nearly the same 7 88.76% 45.36% accuracy achieved 5540 training sentences."
"In this experiment, we simply give an equal 15 45.28%88.65% weight to each classifier."
"However, if we op-timized the voting weight more carefully, the Table 5: Beam width vs. Accuracy (6756 sen-further improvements would be achieved (Inui tences, d = 3) and[REF_CITE])."
Sekine[REF_CITE]gives an interest-ing report about the relationship between the beam width and the parsing accuracy.
"Gener-ally, high parsing accuracy is expected when a large beam width is employed in the depen-dency structure analysis."
"However, the result is against our intuition."
"They report that a beam width between 3 and 10 gives the best parsing accuracy, and parsing accuracy falls down with a width larger than 10."
This result suggests that Japanese dependency structures may consist of a series of local optimization processes.
We evaluate the relationship between the beam width and the parsing accuracy.
"Table 5 shows their relationships under the condition d = 3, along with the changes of the beam width from k = 1 to 15."
The best parsing accuracy is achieved at k ----5 and the best sentence accuracy is achieved at k = 5 and k=7.
We have to consider how we should set the beam width that gives the best parsing accu-racy.
"We believe that the beam width that gives the best parsing accuracy is related not only with the length of the sentence, but also with the lexical entries and parts-of-speech that comprise the chunks."
"Instead of learning a single classier using all training data, we can make n classifiers di-viding all training data by n, and the final result is decided by their voting."
This ap-proach would reduce computational overhead.
The use of multi-processing computer would help to reduce their training time considerably since all individual training can be carried out in parallel.
"To investigate the effectiveness of this method, we perform a simple experiment: Di-"
"Uchimo[REF_CITE]and Sekine[REF_CITE]report that using Kyoto University Corpus for their training and test-ing, they achieve around 87.2% accuracy by building statistical model based on Maximum Entropy framework."
"For the training data, we used exactly the same data that they used in order to make a fair comparison."
"In our ex-periments, the accuracy of 89.09% is achieved using same training data."
Our model outper-forms Uchimoto&apos;s model as far as the accura-cies are compared.
"Although Uchimoto suggests that the im-portance of considering combination of fea-tures, in ME framework we must expand these combination by introducing new fea-ture set."
Uchimoto heuristically selects &quot;effec-tive&quot; combination of features.
"However, such a manual selection does not always cover all relevant combinations that are important in the determination of dependency relation."
"We believe that our model is better than others from the viewpoints of coverage and consistency, since our model learns the combi-nation of features without increasing the com-putational complexity."
"If we want to recon-sider them, all we have to do is just to change the Kernel function."
The computational com-plexity depends on the number of support vec-tors not on the dimension of the Kernel func-tion.
The simplest and most effective way to achieve better accuracy is to increase the training data.
"However, the proposed method that uses all candidates that form dependency re-lation requires a great amount of time to com-pute the separating hyperplaneas the size of the training data increases."
"The experiments given in this paper have actually taken long training time 3 training data and outperforms existing meth- To handle large size of training data, we ods based on Maximum Entropy Models."
The have to select only the related portion of ex- result shows that Japanese dependency anal-amples that are effectivefor the analysis.
This ysis can be effectively performed by use of will reduce the training overhead as well as SVMs due to its good generalization and non-the analysis time.
The committee-based ap- overfitting characteristics. proach discussed section 4.7 is one method of coping with this problem.
"For future research, References to reduce the computational overhead, we will Eugene Charniak. 2000."
A maximum-entropy-work on methods for sample selection as fol- inspired parser.
In Processing of the NAACL lows: • Introduction of constraints on non- dependency Some pairs of chunks need not consider since there is no possibility of depen-dency between them from grammatical constraints.
Such pairs of chunks are not necessary to use as negative examples in the training phase.
"For example, a chunk within quotation marks may not modify a chunk that locates outside of the quo-tation marks."
"Of course, we have to be careful in introducing such constraints, and they should be learned from existing corpus. • Integration with other simple models Suppose that a computationally light and moderately accuracy learning model is obtainable (there are actually such sys-tems based on probabilistic parsing mod-els)."
We can use the system to output some redundant parsing results and use only those results for the positive and negative examples.
This is another way to reduce the size of training data. • Error-driven data selection We can start with a small size of train-ing data with a small size of feature set.
"Then, by analyzing held-out training data and selecting the features that affect the parsing accuracy."
This kind of grad-ual increase of training data and feature set will be another method for reducing the computational overhead.
This paper proposes Japanese dependency achieves a high accuracy even with a small
"In Proceedings of the EA CL, pages 196-203. 3[REF_CITE](617Mhz),it took 15days Vladimir N. Vapnik. 1998."
Statistical Learning to train with 7958sentences.
The central idea of transformation-based learn-
"The system thus learns a listof rulesin a greedy ing is to learn an ordered list of rules which fashion,according to the objective function."
When progressively improve upon the current state of no rule that improves the current state of the the training set.
"An initial assignment is made training set beyond the pre-set threshold can based on simple statistics, and then rules are be found, the training phase ends."
"During the greedily learned to correct the mistakes, until no evaluation phase, the evaluation set is initialized net improvement can be made. with the same initialclass assignment."
"Each rule These definitions and notation will be used isthen applied, in the order it was learned, to the throughout the paper: evaluation set."
The finalclassificationis the one • X denotes the sample space; attained when allrules have been applied. • C denotes the set of possible classifications of the samples; 3 Probability estimation with transformation rule lists • The state space is defined as 8 = X x C.
"Rule lists are infamous for making hard decisions, • 7r will usually denote a predicate defined on decisions which adhere entirely to one possibility, X; excluding all others."
"These hard decisions are • A rule r is defined as a predicate - class label often accurate and outperform other types of - time tuple, (~r,c,t), c E C,t E N, where t is classifiers in terms of exact-match accuracy, but the learning iteration in which when the rule because they do not have an associated proba-was learned, its position in the list. bility, they give no hint as to when they might • A rule r = (~r,c,t) applies to a state (z, y) if fail."
"In contrast, probabilistic systems make soft 7r(z) = true and c # y. decisions by assigning a probability distribution over all possible classes."
Using a TBL framework to solve a problem as- There are many applications where soft deci-sumes the existence of: sions prove useful.
"In situations such as active • An initial class assignment (mapping from X learning, where a small number of samples are to ,.9)."
"This can be as simple as the most selected for annotation, the probabilities can be common class label in the training set, or it used to determine which examples the classifier can be the output from another classifier. was most unsure of, and hence should provide the most extra information."
A probabilistic system • A set of allowable templates for rules.
"These can also act as a filter for a more expensive templates determine the predicates the rules system or a human expert when it is permitted will test, and they have the biggest influence to reject samples."
Soft decision-making is also over the behavior of the system. useful when the system is one of the components • An objective function for learning.
"Unlike in in a larger decision-malting process, as is the case many other learning algorithms, the objective in speech recognition systems[REF_CITE], function for TBL will typically optimize the or in an ensemble system like AdaBoost (Freund evaluation function."
An often-used method is and[REF_CITE]).
"There are many other the difference in performance resulting from applications in which a probabilistic classifier is applying the rule. necessary, and a non-probabHistic classifier cannot be used instead."
"At the beginning of the learning phase, the training set is first given an initial class assign- 3.1 Estimation via conversion to decision ment."
The system then iteratively executes the tree followingsteps:
The method we propose to obtain probabilis-tic classificationsfrom a transformation rule list 1. Generate all productive rules. involves dividing the samples into equivalence 2.
For each rule: classes and computing distributions over each (a) Apply to a copy of the most recent state equivalence class.
"At any given point in time i, of the training set. each sample z in the training set has an associated state si(z) = (z,~l)."
"Let R(z) to be the set of rules (b) Score the result using the objective func-r~ that applies to the state el(z), tion. 3. Select the rule with the best score."
"R(z) = {ri ~ 7~Ir~applies to si(z)} 4. Apply the rule to the current state of the An equivalence class consists of all the samples training set, updating it to reflect this change. z that have the same R(z)."
Class probability 5. Stop if the score is smaller than some pre-set assignments are then estimated using statistics threshold T. computed on the equivalence classes.
"Initial label = A (a) Create a new internal node, n; If Q1 and label=A then label+-B (b) Set the question: q(n) = 7rj; If Q2 and label=A then labele-B (c) Create the left child of n using a recursive If Q3 and label=B then label~A call to RLTDT(BL, 7~); Table I: Example of a Transformation Rule List. (d) Create the right child of n using a recur-sive call to RLTDT(BR, 7~); (e) Return node n."
"Otherwise, no split is performed using rj."
Repeat from Step 1.
"The parameter K is a constant that determines the minimum weight that a leaf is permitted to have, effectively pruning the tree during construction."
"In all the experiments, K was set to 5."
"When a rule list is converted into a decision tree, there are often leaves that are inordinately heavy because they contain a large number of samples. from Table 1 to a decision tree, Examples of such leaves are those containing samples which were never transformed by any The conversion from a transformation rule list of the rules in the rule list."
These populations to a decision tree is presented as a recursive exist either because they could not be split up procedure.
"The set of samples in the training set during the rule list learning without incurring a is transformed to a set of states by applying the net penalty, or because any rule that acts on them initial class assignments."
A node n is created for has an objective function score of less than the each of the initial class label assignments c and all threshold
"This is sub-optimal for estimation states labeled c are assigned to n. because when a large portion of the corpus falls The following recursive procedure is invoked into the same equivalence class, the distribution with an initial &quot;root&quot; node, the complete set of assigned to it reflects only the mean of those states (from the corpus) and the whole sequence samples."
The undesirable consequence is that all of rules learned during training: of those samples are given the same probability distribution.
"Algorithm: RuleListToDecisionTree To ameliorate this problem, those samples are (RLTDT) partitioned into smaller equivalence classes by Input: further growing the decision tree."
Since a decision *
"A set/3 of N states ((Zl, Yl) --- (ZN, YN)) with tree does not place all the samples with the same labels Yi E C; current label into a single equivalence class, it does not get stuck in the same situation as a rule list • A set 7~ of M rules (ro,rl ...rM) where ri = m in which no change in the current state of corpus can be made without incurring a net loss Do: in performance."
Continuing to grow the decision tree that was Word POS tag Chunk Tag converted from a rule list can be viewed from A.P. NNP B-NP another angle.
A highly accurate prefix tree Green NNP I-NP for the final decision tree is created by tying currently RB B-ADVP questions together during the first phase of the has VBZ B-VP growth process (TBL).
"Unlike traditional decision 2,664,098 CD B-NP trees which select splitting questions for a node shares I-NPNNS by looking only at the samples contained in the outstanding B-ADJPJJ local node, this decision tree selects questions by O looking at samples contained in all nodes on the frontier whose paths have a suM&lt; in common."
"An Table 2: Example of a sentence with chunk tags illustration of this phenomenon can be seen in Figure 1, where the choice to split on Question 3 was made from samples which tested false secondly, it belongs to the same class of classifiers on the predicate of Question 1, together with as our converted transformation-based rule list samples which tested false on the predicate of (henceforthTBLDT)."
"The result of this is that questions To perform a fair evaluation, extra care was are chosen based on a much larger population than taken to ensure that both C4.5 and TBLDT in standard decision tree growth, and therefore explore as similar a sample space as possible."
"The have a much greater chance of being useful and systems were allowed to consult the word, the generalizable."
"This alleviates the problem of over- part-of-speech, and the chunk tag of all examples partitioning of data, which is a widely-recognized within a window of 5 positions (2 words on either concern during decision tree growth. side) of each target example.2 Since multiple The decision tree obtained from this conversion features covering the entire vocabulary of the can be grown further."
"When the rule list 7~ is training set would be too large a space for C4.5 exhausted at Step 1, instead of creating a leaf to deal with, in all of experiments where TBLDT node, continue splitting the samples contained in is directly compared with C4.5, the word types the node with a decision tree induction algorithm. that both systems can include in their predicates The splitting criterion used in the experiments is are restricted to the most &quot;ambiguous&quot; 100 words the information gain measure. in the training set, as measured by the number of chunk tag types that are assigned to them."
The 4 Experiments initial prediction was made for both systems using Three experiments that demonstrate the effec- a class assignment based solely on the part-of-tiveness and appropriateness of our probability speech tag of the word. estimates are presented in this section.
"The Considering chunk tags within a contextualwin-experiments are performed on text chunking, a dow ofthe target word raises a problem with C4.5. subproblem of syntactic parsing."
"Unlike full pars- A decision tree generally trains on independent ing, the sentences are divided into non-overlapping samples and does not take into account changes phrases, where each word belongs to the lowest of any features in the context."
"In our case, the parse constituent that dominates it. samples are dependent; the classificationof sample The data used in all of these experiments is i is a feature for sample i + 1, which means that the[REF_CITE]hrase chunking corpus (CoNLL, changing the classification for sample i affects 2000)."
The corpus consists of sections 15-18 and the context of sample i + 1.
"To address this section 20 of the Penn Treebank (Marcus et al., problem, the C4.5 systems are trained with the 1993), and is pre-divided into a 8936-sentence correct chlmk~ in the left context."
"When the (211727 tokens) training set and a 2012-sentence system is used for classification, input is processed (47377 tokens) test set."
"The chunk tags are in a left-to-right manner;and the output of the derived from the parse tree constituents, and the system is fed forward to be used as features part-of-speech tags were generated by the Brill in the left context of following samples."
"C4.5 generates probabilities for each classification decision, they can be redirected into the input for As was noted by Ramshaw &amp;[REF_CITE], text chunking can be mapped to a tagging task, the next position."
Providing the decision treewith this confidence information effectivelyallows it to where each word is tagged with a chunk tag perform a limited search over the entire sentence. representing the phrase that it belongs to.
"An example sentence from the corpus is shown in C4.5 does have one advantage over TBLDT, Table 4."
"As a contrasting system, our results however."
"A decision tree can be trained using the are compared with those produced by a C4.5 subsetting feature, where questions asked are of decision tree system (henceforth C4.5). the form: &quot;does feature f belong to the set FT&apos;.The reason for using C4.5 is twofold: firstly, it is a This is not something that a TBL can do readily, widely-usedalgorithmwhich achievesstate-.of-the- 2The TBL templates are similar to those used in art performance on a broad variety of tasks; and l~am.~haw and[REF_CITE]. weight to precision and recall."
"The reported performances are all measured with the evaluation tool provided with the CoNLL corpus[REF_CITE]. where H(UIS, i) is the entropy of the chllnk 4.2 Active Learning probability distribution associated with the word"
"To demonstrate the usefulness of obtaining proba- index i in sentence S. bilities from a transformation rule list, this section Figure 2 displays the performance (F-measure describes an application which utilizes these prob-and chllnk accuracy) of a TBLDT system trained abilities, and compare the resulting performance on samples selected by active learning and the of the system with that achieved by C4.5. same system trained on samples selected sequen- Natural language processing has traditionally tially from the corpus versus the number of words required large amounts of annotated data from in the annotated tralniug set."
At each step of which to extract linguistic properties.
"However, the iteration, the active learning-trained TBLDT not all data is created equal: a normal distribu-system achieves a higher accuracy/F-measure, or, tion of aunotated data contains much redundant conversely,is able to obtain the same performance information."
"Overall, our system al. (1997) proposed a theoretical active learning can yield the same performance as the sequential approach, where samples are intelligentlyselected system with 45% less data, a significant reduction for annotation."
"By eliminating redundant infor-in the annotation effort. mation, the same performance can be achieved Figure 3 shows a comparison between two active while using fewer resources."
"Empirically, active learning experiments: one using TBLDT and the learning has been applied to various NLP tasks other using C4.5.4 For completeness, a sequential such as text categorization ([REF_CITE]; run using C4.5 is also presented."
"C4.5 examines a larger space than[REF_CITE]) , part-of-speech tagging[REF_CITE], and base SThetrue (reference or gold standard) classificationis noun phrase chunbiug[REF_CITE], available in this experiment."
"In an annotation situation, the samplesare sentto human annotators for labeling. resulting in significantly large reductions in the 4Asmentionedearlier,both the TBLDTand C4.5 were quantity of data needed to achieve comparable limited to the same 100 most ambiguouswordsin the performance. corpusto ensurecomparability. utilizing the feature subset predicates, TBLDT established in machine learning and optimization still performs better."
"The difference in accuracy at research ([REF_CITE]0 words (at the end of the active learning run al., 1999). for TBLDT) is statistically significant at a 0.0003 Since non-probabilistic classifiers do not offer level. any insights into how sure they are about a As a final remark on this experiment, note that particular classification, it is not easy to obtain at an annotation level of 19000 words, the fully confidence scores from them."
"A probabilistic lexicalized TBLDT outperformed the C4.5 system classifier, in contrast, offers information about the by making 15% fewer errors. class probability distribution of a given sample."
Two measures that can be used in generating 4.3 Rejection curves confidence scores are proposed in this section.
It is often very useful for a classifier to be able
"The first measure, the entropy H of the class to offer confidence scores associated with its deci-probability distribution of a sample z, C(z) = sions."
"Confidence scores are associated with the {p(CllZ),p(c2[z)...p(cklZ)}, is a measure of the probability P(C(z) correct[z) where C(z) is the uncertainty in the distribution: classification of sample z."
"These scores can be used in real-life problems to reject samples that k the the classifier is not sure about, in which case HCCCz)) = - I=) log2 pC Iz) a better observation, or a human decision, might i=I be requested."
"The performance of the classifier is then evaluated on the samples that were not The higher the entropy of the distributionof rejected. class probability estimates, the more uncertain theThis experiment framework is well- classifier is of its classification."
The samples se-
"Cross Entropy lected for rejection are chosen by sorting the data TBLDT 1.2944 0.2580 using the entropies of the estimated probabilities, DT+probs 1.4150 0.3471 and then selecting the ones with highest entropies."
DT 1.4568 0.3763 The resulting curve is a measure of the correlation between the true probability distribution and the Table 3: Cross entropy and perplexities for two one given by the classifier.
C4.5 systems and the TBLDT system Figure 4(a) shows the rejection curves for the
TBLDT system and two C4.5 decision trees - one is below the accuracy level.
"The resulting curve which receives a probability distribution as input is a measure of the correlation between the true (&quot;soft&quot; decisions on the left context) , and one distribution probability and the probability of the which receives classifications (&quot;hard&quot; decisions on most likely chunk tag, i.e. how appropriate those all fields)."
"At the left of the curve, no samples probabilities are as confidence measures."
"Unlike are rejected; at the right side, only the samples the first measure mentioned before, a threshold about which the classifiers were most certain are obtained using this measure can be used in an kept (the samples with minimum entropy)."
"Note online manner to identify the samples of whose that the y-values on the right side of the curve are classification the system is confident. based on less data, effectively introducing wider Figure 4(b) displays the rejection curve for variance in the curve as it moves right. the second measure and the same three systems."
"As shown in Figure 4(a), the C4.5 classifier TBLDT again outperforms both C4.5 systems, at that has access to the left context chunk tag all levels of confidence. probability distributions behaves better than the In summary, the TBLDT system outperforms other C4.5 system, because this information about both C4.5 systems presented, resulting in fewer re-the surrounding context allows it to effectively jections for the same performance, or, conversely, perform a shallow search of the classification better performance at the same rejection rate. space."
"The TBLDT system, which also receives 4.4 Perplexity and Cross Entropy a probability distribution on the chunk tags in the left context, clearly outperforms both C4.5 Cross entropy is a goodness measure for probabil-systems at all rejection levels. ity estimates that takes into account the accuracy of the estimates as well as the classification accu-"
The second proposed measure is based on the racy of the system.
It measures the performance probability of the most likely tag.
The assumption of a system trained on a set of samples distributed here is that this probability is representative of according to the probability distribution p when how certain the system is about the classifica-tested on a set following a probability distribution tion.
"The samples are put in bins based on q. More specifically, we utilize conditional cross the probability of the most likely chnnk tag, and entropy, which is defined as accuracies are computed for each bin (these bins are cumulative, meaning that a sample will be n (ClX) = - q(=)- q(cl=) •log2 pC@:) included in all the bins that have a lower threshold zEX ¢EC than the probability of its most likely chnnl¢ tag)."
"At each accuracy level, a sample will be where X is the set of examples and C is the set of rejected if the probability of its most likely chnn~ chnnlr tags, q is the probability distribution on the testdocument and p isthe probability distribution on the train corpus."
The crossentropy metric failsifany outcome is given zero probability by the estimator.
"To avoid this problem, estimators are &quot;smoothed&quot;, ensuring that novel events receive non-zero probabilities."
A very simple smoothing technique (interpolation with a constant) was used for allof these systems.
"A closelyrelated measure is perplexity, defined as P = 2~(clx)"
The cross entropy and perplexity resultsfor the various estimation schemes are presented in Table •3.
"The TBLDT outperforms both C4.5 systems, obtaining better cross-entropy and chunk tag per-plexity."
This shows that the overall probability distribution obtained from the TBLDT system better matches the true probability distribution.
This strongly suggests that probabilities generated thisway can be used successfullyin system com-bination techniques such as voting or boosting.
"It is worth noting that the transformation-based system used in the comparative graphs in Figure 3 was not r,uning at fullpotential."
"As described earlier,the TBLDT system was only allowed to consider words that C4.5 had access to."
"However, a comparison between the corresponding TBLDT curves in Figures 2 (where the system is given access to all the words) and 3 show that a transformation-based system given access to all the words performs better than the one with a restrictedlexicon,which in turn outperforms the best C4.5 decisiontree system both in terms of accuracy and F-measure."
"Table 4 shows the performance of the TBLDT system on the fullCoNLL test set,broken down by chunk type."
"Even though the TBLDT results could not be compared with other published re-sults on the same task and data (CoNLL will not take place until[REF_CITE]), our system significantly outperforms a similar system trained with a C4.5 decision tree, shown in Table 5, both"
"In this paper we presented a novel way to convert transformation rule lists, a common paradigm in natural language processing, into a form that is equivalent in its classification behavior, but is capable of providing probability estimates."
"Using this approach, favorable properties of transfor-mation rule lists that makes them popular for language processing are retained, while the many advantages of a probabilistic system axe gained."
"To demonstrate the efficacy of this approach, the resulting probabilities were tested in three ways: directly measuring the modeling accuracy on the test set via cross entropy, testing the goodness of the output probabilities in a active learning algorithm, and observing the rejection curves attained from these probability estimates."
"The experiments clearly demonstrate that the resulting probabilities perform at least as well as the ones generated by C4.5 decision trees, resulting in better performance in all cases."
This proves that the resulting probabilistic classifier is as least as good as other state-of-the-art probabilistic models.
"The positive results obtained suggest that the probabilistic classifier obtained from transforma-tion rule lists can be successfully used in machine learning algorithms that require soft-decisionclas-sifters,such as boosting or voting."
Future research will include testing the behavior of the system under AdaBoost[REF_CITE].
We also intend to investigate the effectsthat other decision tree growth and smoothing techniques may have on continued refinement ofthe converted rule list.
"We thank Eric Brill, Fred Jelinek and David Yaxowsky for their invaluable advice and sugges-tions."
"In addition we would like to thank David Day, Ben Weliner and the anonymous reviewers for their useful comments and suggestions on the paper. . . ."
The views expressed in this paper are those of in chunk accuracy and F-measure. the authors and do not necessarily reflect the views
T. G. Dietterich and G. Bakiri. 1995.
Solving multi- by rule sequences.
"In International Conference on class learning problems via error-correcting output Computational Linguistics, pages 274-279, Copen-codes."
"Journal of Artificial Intelligence Research, hagen, Denmark, August. 2:263-286."
S. Engelson and I. Dagan. 1996.
"We address the issue of &apos;topic analysis,&apos; by The purpose of tMs paper is to provide a which is determined a text&apos;s topic structure, single framework for conducting topic analy-which indicates what topics are included in a sis, i.e., performing both topic identification text, and how topics change within the text. and text segmentation."
"We propose a novel approach to this issue, one The key characteristics of our framework based on statistical modeling and learning. are 1) representing a topic by means of a clus- We represent topics by means ofword clusters, ter of words that are closely related to the and employ a finite mixture model to repre- topic, and 2) employing a stochastic model, sent a word distribution within a text."
"Our called a .finite mixture model (e.g., (Everitt experimental results indicate that our method and[REF_CITE])), to represent a word dis-significantly outperforms a method that com- tribution within a text."
The finite mixture bines existing techniques. model has a hierarchical structure of probabil-
"Let W denote a set of words, and K a set of topics."
We first define a distribution of topics (clusters) P(k) : ~kEIK P(k) = 1.
"Then, for each topic k E K, we define a probability dis-tribution of words P(wik) : ~,ew P(wlk) = 1."
Here the value of P(wik) will be zeroif w is not included in k.
"We next define a Stochas-tic Topic Model (STM) as a finite mixture model, which is a linear combination of the word probability distributions P(w[k), with the topic distribution P(k) being used as the coefficient vector."
"The probability of word w word sequence) as having been generated by some &apos;true&apos; STMs, which we then seek to esti-mate as closely as possible."
"A text may have a number of blocks, and each block is assumed to be generated by an individual STM."
"The STMs within a text are assumed to have the same set of topics, but have different param-eter values."
"From the linguistic viewpoint, a text gener-ally focuses on a single main topic, but it may discuss different subtopics in different blocks."
"While a text is discussing any one topic, it will more frequently use words strongly related to that topic."
"Hence, STM is a natural represen-tation of statistical word occurrence based on topics."
"Before conducting topic analysis, we create word clusters using a large data corpus."
"More precisely, we treat all words in a vocabulary as seed words, and for each seed word we collect from the data those words which frequently co-occur with it and group them into a cluster."
"As one example, the word-cluster in Figure 1 has been constructed with the word &apos;trade&apos; as the seed word."
"We have developed a new method for reli-ably collecting frequently co-occurring words on the basis of stochastic complexity, or the MDL principle."
"For a given data sequence zm = xl...zm and for a fixed probability model M, 1 the stochastic complexity of xm relative to M, which we denote as SC(x m :"
"M), is defined as the least code length re-quired to encode xrn with M[REF_CITE]."
"SC(x m : M) can be interpreted as the amount information included in xn relative to M. The in W is, then, 1Here, we use &apos;model&apos; to refer to aprobability dis- P(w) = ~ P(k)P(wlk ) tnbution which has specified paxameters but unspeci- we W. kEK fied parameter values."
"For a fixed seed word s, we take a word w as (I) a frequently co-occurring word if the presence According to the MDL principle, the larger of s is a statistically significant indicator of the 6SC value, the more likely that the pres-the presence of w. ence or absence of w is dependent on those of w TM relative to a model I in which the presence tering algorithm is more efficient than con-or absence of w is independent from those of ventional ones."
"For example, Hofmann&apos;s is s (i.e., a Bernoulli model), is calculated as of order O(]DIIWI2), while ours is only of O(ID I+ ]WI2), where IDI denotes the number of texts and IW] the number of words."
"That SC(w TM :I) = mH means that our method is more practical when+ ~ log ~ + log 7r, a large amount of text data is available. where m+ denotes the number of l&apos;s in wm. [Footnote_4].1 Input and Output Here, log denotes the logarithm to the base"
4 Topic Analysis
"In topic analysis, we use STM to parse a 2, ~- the circular constant, and H(z) deJ given text and output a topic structure which -zlogz - (1 - z)log(1 - z), when 0 &lt; z &lt; 1; consists of segmented blocks and their top- H(z) des = 0, whenz=0orz= 1. ics."
Figure 3 shows an example topic struc- Let wm&quot; be the sequence of all wi&apos;s (wi E ture as output with our method.
"The text has wrn) such that its corresponding si is 1, where been segmented into five blocks, and to each ms denotes the number of l&apos;s in s~. Let wrn&apos;&apos; block, a number of topics having high prob-be the sequence ofaU wi&apos;s (wi E wm) such that ability values have been assigned (topics axe its corresponding si is 0, where rn.~s denotes represented by their seed words)."
The topic the number O&apos;s in sm.
The SC value of wm structure clearly represents what topics are in-relative to a model D in which the presence cluded in the text and how the topics change or absence of w is dependent on those of s is within the text. then calculated as
Our topic analysis consists of three processes:
"SC(w : ( s.u + ~logT~ + log) a pre-process called &apos;topic spotting,&apos; text seg- = mentation, and topic identification."
"In topic + (m&quot;sH (-m&apos;-&apos;~&apos;~&apos;~W ½1°g-m-2~=&apos;~Wl°gr) SNote that the quantity within [---] in (1) is (em-pirical) mutual inyormation, which is an effective mea-sure for word co-occurrence calculation (cf.,(Brown et where ms+ denotes the number of l&apos;s in wm&apos;, al., 1992))."
"When the sample size is small, mutual and w~+sthe number of l&apos;s in wm~,. information values tend to be undesirably large."
"The quantity within {-..} in (1) can help avoid this unde-sirable tendency because its value will become large 2For an introduction to MDL, see[REF_CITE]. when data size is small. spotting, we select topics discussed in a given Shannon information can be justified on the text."
"We can then construct STMs on the basis of information theory, but that of tf-idf basis of the topics."
"In text segmentation, we cannot."
"Our preliminary experimental results segment the text on the basis of the STMs, indicate that Shannon information performs assuming that each block is generated by an better than or at least as well as tf-idf in key individual STM."
"In topic identification, we es- word extraction.4 timate the parameters of the STM for each From the results ofword clustering, we next segmented block and select topics with high select any cluster (topic) whose seed word is probabilities for the block."
"In this way, we included among the selected key words. obtain a topic structure for the text."
"We next merge any two clusters if one of their seed words is included in the other&apos;s clus- j = i-1 ; for the purposes of our explanation here, all while (j &gt; 0 &amp; S(j - 1) &gt; S(j)) sentence-ending periods will be candidates. j--; For each candidate, we create two pseudo- P[Footnote_1] = S(j); texts, one consisting of the h sentences pre- j=i+ l; ceding it, and the other of the h sentences while(j &lt; n &amp; S(j + 1) &gt; S(j)) following it (when fewer than h exist in any j++; ..:direction, we simply use those which do exist)."
1 They told Router correspondents in Asian capitals a U.S. move against Japan might boost prote©tionist sentiment in she U.S. ~nd lead to ...
P2 = S(j);
"We use the EM algorithm ((Dempster et al., if(P1 - S(i) &gt; ~ &amp; P2- S(i) &gt; 8) 1977), cL, Figure 5) to separately estimate the Conduct segmentation at i. parameters of an STM from each of the two }) pseudo texts."
It is theoretically guaranteed that the EM algorithm converges to a local maximum of the likelihood.
"We next calculate Figure 6: Algorithm: segment the similarity (i.e., essentially the converse no-tion of distances) between the STM based on the preceding pseudo-text, and the STM text shown in Figure 3. &apos;Valleys&apos; (i.e., low-based on the following pseudo-text."
These similarity values) in the graph suggest points
"STMs axe denoted, respectively, as PL(W) and for reasonable segmentations."
In actual prac- PR(w).
"The similarity between PL(W) and tice, segmentation is performed for each valley PR(w) is defined as whose similarity values is lower to a predeter-mined degree 0 than each of the values of its S(LI[R) = 1 - E~w [PL(w) - PR(w)[ left &apos;peak&apos; and right &apos;peak&apos; (cf., Figure 6)"
"For 2 example, for the text in Figure 3, segmenta-tion was performed at candidates (i.e., end of The numerator is referred to in statistics as sentences) 6, 14, 18, and 22, with 8 = 0.05. variational distance and has good properties as a distance between two probability dis- 4.5 Topic Identification tributions (cf.,[REF_CITE],"
"After segmentation, we separately estimate p.299). the parameters of the STM for each block,"
"Figure 7 shows a graph of calculated simi- again using the EM algorithm, and obtain laxity values for each of the candidates in the a topic (cluster) probability distribution for each block."
We then choose those topics (dus- 5We use similarity rather than distance here in or-der to simplify comparison between our method and ters) in each block having.high probability val-
"In this way, we construct a topic struc- only in individual blocks as subtopics."
"In similarity the cosine value between the word the text in Figure 3, the topic represented frequency vectors of the two pseudo texts."
"It by seed-words &apos;trade-export-tariff-import&apos; is then conducts segmentation at valley points the main topic, and &apos;Japan-Japanese,&apos; &apos;Hong in a similar way to that of our method."
"Since Kong,&apos; etc., are subtopics. the problem setting of TextTiling (in general the former group) is most close to that of our 5 Applications study, we use TextTiling for comparison in our Our method can be used in a variety of text experiments. processing applications."
"Our method by its nature performs topic For example, given a collection of texts identification and segmentation within a sin- (e.g., home pages), we can automatically con- gle framework."
While it is possible with a struct an index of the texts on the basis of the combination of existing methods to extract extracted topics.
"We can indicate which topic key words from a given text by using tf-idf, is from which text or even which block of a view the extracted key words as topics, seg-text."
"Furthermore, we can indicate which top- ment the text into blocks by employing Text-ics are main topics of texts and which topics Tiling, estimate distribution of topics in each are subtopics (e.g., by displaying main topics block, and identify topics having high prob-in boldface, etc)."
"In this way, users can get a abilities in each block."
Our method outper: fair sense of the contents of the texts simply forms such a combination (referred to here-by looking through the index.
"For a specific after as &apos;Corn&apos;) for topic identification, be-text, users can get a rough sense of the con- cause it utilizes word duster information."
"It tent by looking at the topic structure as, for also performs better than Com in text seg-example, it is shown in Figure 3. mentation because it is based on a well-defined Our method can also be useful for text min- probability framework."
"Most importantly is ing, text summarization, information extrac- that our method is able to output an easily tion, and other text processing, which require understandable topic structure, which has not one to first analyze the structure of a text. been proposed so far."
"Note that topic analysis is different from 6 Related Work text classification (e.g., ([REF_CITE]; Li To the best of our knowledge, no previous and[REF_CITE]; Weiss study has so far dealt with topic identification et al., 1999;[REF_CITE]))."
"While text and text segmentation within a single frame- classification uses a number of pre-determined work. categories, topic analysis includes no notion"
A widely used method for key word extrac- of category.
"The output of topic analysis is a tion calculates the tf-idf value of each word in topic structure, while the output of text clas- sification is a label representing a category. split of the data &apos;Apte split,&apos; which consists Furthermore, text classification is generally of 9603 texts for training and 3299 texts for based on supervised learning, which uses la- test."
All of the texts had already been classi-beled text data6.
"By way of contrast, topic fied into 90 categories by human subjects. analysis is based on unsupervised learning, For each text, we used the Oxford Learner&apos;s which uses only unlabeled text data."
"Dictionarys to conduct stemming, and re- Finite mixture models have been used in moved &apos;stop words&apos; (e.g., &apos;the,&apos; &apos;and&apos;) that we a variety of applications in text processing had included on a previously prepared list. (e.g., ([REF_CITE]; Nigam et al., The average length of a text was about 115 2000;[REF_CITE])), indicating that they words. (We did not use phrases, however, are essential to text processing."
"We should which would further improve experimental re-note, however, that their definitions and the sults.) ways they use them axe different from those 7.2 Word Clustering for STM in this paper."
"For example, Li and"
Yamanishi propose to employ in text classi-
"We conducted word clustering with 9603 fication a mixture model (Li and Yamanishi, training texts. 7340 individual words had a 1997) defined over categories: total frequency of more than 5, and we used them as seeds with which to collect frequently P(WIC) = ~ P(klc)P(wlk),w e W,c e C, co-occurring words."
"The threshold for clus-kEK tering 7 was set at 0.005, and this yielded 970 word clusters having more than one word where W denotes a set of words, and C a (i.e., not simply containing a seed word alone). set of categories."
"In their framework, a new Note that the category labels of the training text d is assigned into a category c* such that texts need not be used in clustering. c* = argmaxeee P(c]d) is satisfied."
I-Iofmann We next conducted a topic analysis on all proposes using in information retrieval a joint the 3299 texts.
"The thresholds of l, h, and 0 distribution which he calls &apos;an aspect model,&apos; were set at 20, 3, and 0.05, respectively, on .Aefined[REF_CITE]the basis of preliminary experimental results."
"P(w,d) = P(d)P(wld) 7.3 Topic Structure = P(d) EkeK P(kld)P(wlk), We looked at the topic structures of the 3299 wEW, dED texts obtained by our method to determine how well they conformed to human intuition. where D denotes a set of texts."
"Furthermore, For topic identification in this experiment, he proposes extracting in retrieval those texts clusters in each block were sorted in descend-whose estimated word distributions P(w[d) ing order of their probabilities, and the top are similar to the word distribution of a query. 7 seed words were extracted to represent the topics of the block. 7 Experimental Results Figure 3 show results for the text with ID We have evaluated the performance of our 14826; they generally agree well with human topic analysis method (STM) in terms of three intuition."
"The text has been segmented into aspects: topic structure adequacy, text seg- 5 blocks and the topics of each block is rep-mentation accuracy, and topic identification resented by 7 seed words."
"The main topic is accuracy. represented by the seed-words &apos;trade-export-tariff-import.&apos; The subtopics are represented 7.1 Data Set by &apos;Japan-Japanese,&apos; &apos;Taiwan,&apos; &apos;Hong Kong,&apos;"
We know of no data available for the pur-etc.
"There were, however, a small number pose of evaluation of topic analysis."
We thus of errors.
"For example, the text should also utilized Reuters news articles referred to as have been segmented after sentences 11 and &apos;[REF_CITE]&apos; which has been widely used 13, but, due to limited sentence content, it was in text classificationv."
We used a prepared not.
"Furthermore, assigning subtopic of &apos;But-SAn exception is the method proposed in (McCal- ton&apos; (from &apos;Mr."
"Button&apos;) into block 3 (due lure and[REF_CITE]), which, instead oflabeled texts, to the high Shannon information value of the uses unlabeled texts, pre-determined categories, and word &apos;Button&apos;) was also undesirable. keywords defined by humans for each category. rAvailable[URL_CITE]SAvailable[URL_CITE]earn earning, share, profit, dividend category STM Com acq acquisition, acquire, sell, buy rec. pre. rec. pre. money-fx currency, dollar, yen, stg 0.790 0.971earn 0.526 0.976 grain grain, cereal, crop 0.245acq 0.854 0.184 0.841 crude oil, crude, gas money-fx 0.436 0.456 0.285 0.421 trade trade, export, import, tariff 0.322grain interest interest &amp; rate crude 0.487 ship ship, vessel, ferry, tanker 0.667trade wheat wheat interest 0.107 corn cori1, maize 0.247ship wheat 0.620 corn 0.429 micro-average 0.515 7.4"
Main Topic Identification We conducted an evaluation to determine whether or not the main topics in the topic structures obtained for the 3299 test texts 0.750 0.174 0.650 0.676 0.407 0.664 0.473 0.590 0.356 0.700 0.084 0.733 0.957 0.270 0.828 0.936 0.408 0.967 0.960 0.446 1.00 0.824 0.365 0.774
"Note that here labels are used only for eval- category CornSTM uation, not for training."
"This is in contrast rec. to the situation in most text classification ex- earn 0.742 periments, in which labels are generally used acq 0.184 both for training and for evaluation."
"It is not money-fx 0.413 particularly meaningful, then, to compare the grain 0.295 results for main topic identification obtained crude 0.471 here with those for text classification. trade 0.479 With STM, clusters in each block were interest 0.053 sorted in descending order of their probabil- ship 0.169 ities, and the top k seed words were extracted wheat 0.577 to represent the topics of the block."
"Further- corn 0.357 more, a seed word appearing in all the blocks micro-average 0.461 of the text was considered to represent a main topic."
"When a text had not been segmented (i.e., has only one block), all top k seed words pre. rec. pre. 0.971 0.348 0.977 0.868 0.120 0.869 0.503 0.268 0.471 0.759 0.121 0.600 0.718 0.333 0.656 0.505 0.513 0.403 0.700 0.069 0.818 1.000 0.180 0.762 0.953 0.282 0.952 0.952 0.321 1.000 0.850 0.257 0.767 of decisions which should have been correctly were considered to represent main topics."
Table 1 lists the largest 10 categories in the Reuters data.
"On the basis of the definition of each of the 10 categories, we assigned based on our intuition to each of them the identification words that are listed in Table 1."
"For the evaluation, when the seed words for main topics contained at least one of the iden-tification words, we considered our method to have identified the corresponding main topic equivalent to a human-determined category. made."
"We also looked at the performance of Corn (cf., Section 6)."
"For Corn, we extracted from a text the key words with the 20 largest Shan-non information values, segmented the text using TextTiling, and extracted in each block the key words having the largest k probabil-ity values."
Any key word extracted in all blocks was considered to represent a main topic.
When the key words for main top-
"Title: g[REF_CITE]RHSATFLOU!- U.S. YRADSItS &quot;Body:ggyp1 bought 125,723 sonnet o~"
"U.S. shoat ~lour in its PL Table 5: Subtopic identification results 480 tender yesterdxy, trade ooQrceo said."
"The purchase included $t,880 tonnes ~er"
"Say shipn~nt ~d 73,843 tennes for June sbipnent."
Price details gere not available.
Content Words (Freq.): tone(S) shipment(2) buy(t) detail(I) category of CornSTM ggypt(1) tlour(l) include(I) June(l) PL(I) price(t) purchase(l) source(l) trade(l) US(l) 9heat(l) seed text rec. rec. pre.pre.
Icy Bordo (ShLn.
Ing.): tonne(17.5) ohi~4nent(1S.3) PL(IO.6) flour(9.8) eaxn 0.430 0.945 0.324 0.973
Sgypt(9.3) detail(7.S) Juno(7.2) uheat(6.8) purchas¢(6.6) source(S.S)
U$(6.1) buy(6.0) inclnde(6.O) trade(B.3) price (S.l) acq 0.237 0.939 0.217 0.959 money-fx 0.950 0.533 0.9610.585
Con Yopics (Prob.): tonn¢(O.17) shipnent(O.ll) price(O.06) June(O.O6) in¢lude(O.00) purcbaoe(O.06) source(O.O6) grain 0.947 0.222 0.9380.276
BIB Iopico (Prob.) : ilour~sheat(O.IB) tonn4(0.12) shipmont(O.tl) crude 0.572 0.979 0.557 O.990 purchaoe-buy(O.tl) Egypt(O.O6) trade 0.634 0.627 0.8990.951 0.211
Cluster: (Sieur-wheat: ghent tonne ~lour) (purchase-buy: purchase bny) interest 0.937 0.136 1.000 ship 0.260 1.000 0.340 0.994 wheat 0.970 0.395 0.9800.500
Figure 8: Topic Identification Example 0.317 1.000 0.441 0.882 corn Average 0.962 0.379 0.958 0.402 shows the results in the case of k = 5.
"The comparison may be considered fair in that it requires each of the two methods to provide in terms of recall, precision, and error prob-the same number of words to represent top- ability."
Table 5 shows the results of subtopic ics.
"Results indicate that STM significantly identification as evaluated in terms of recall outperforms Corn, particularly in terms of re- and precision."
"Error probability is a metric call. for evaluating segmentation results proposed The main reason for the higher performance in (Allan et ai., 1998; Beeferman etal., 1999). achieved by STM is that it utilizes word clus- It is defined here as the probability that a ran-ter information."
"Figure 8 shows topic analysis domly chosen pair of sentences a distance of k results for the text[REF_CITE]labeled with sentence apart is incorrectly segmented.1° & apos;wheat.&apos; The text contains only 15 content Experimental results indicate that STM words (word types), thus all of the 15 words outperforms Corn in both segmentation and were extracted as key words and the text was identification,n not segmented by either method."
Many learning problems in the domain of natural language processing need supervised We show that sample selection can be training.
"For instance, it is difficult to induce applied to grammar induction to produce a grammar from a corpus of raw text; but the high quality grammars with fewer annotated task becomes much easier when the training training sentences."
Our approach is to use sentences are supplemented with their parse uncertainty-based evaluation functions that trees.
"However, appropriate supervised train- estimate the TUV of a sentence by quantify-ing data may be difficult to obtain."
Existing ing the grammar&apos;s uncertainty about assign-corpora might not contain the relevant type ing a parse tree to this sentence.
"We have of supervision, and the data might not be considered two functions."
The first is a sire-in the domain of interest.
"For example, one •ple heuristic that approximates the grammar&apos;s might need morphological analyses of the lex- uncertainty in terms of sentence lengths."
"The icon in addition to the parse trees for inducing second computes uncertainty in terms of the a grammar, or one might be interested in pro- tree entropy of the sentence."
This metric is cessing non-English languages for which there described in detail later. is no annotated corpus.
Because supervised training typically demands significant human
"This paper presents an empirical study involvement (e.g., annotating the parse trees measuring the effectiveness of our evaluation of sentences by hand), building a new corpus is functions at selecting training sentences from a labor-intensive task."
"Therefore, it is worth-the Wall Street Journal (WSJ) corpus (Mar-while to consider ways of minimizing the size cuset al., 1993) for inducing grammars."
"Con-of the corpus to reduce the effort spent by an-ducting the experiments with training pools notators. of different sizes, we have found that sample 2 Sample Selection U is a Set of unlabeled candidates."
"Unlike traditional learning systems that re- L is a set of labeled training examples. ceive training examples indiscriminately, a C is the current hypothesis. learning system that uses sample selection Initialize: actively influences its progress by choosing C +--Train(L). new examples to incorporate into its training Repeat set."
"Sample selection works with two types N ~-- Select(n, U,C, f). of learning systems: a committee of learners U~-U-N. or a single learner."
"The committee-based se- L ~--L t2Label(N). lection algorithm works with multiple learn- C ~ Train(L). ers, each maintaining a different hypothesis Until (C ----Ctrue)Or (U = O) or (human stops) (perhaps pertaining to different aspects of the problem)."
The candidate examples that led Figure 1: The pseudo-code for the sample se-to the most disagreements among the differ- lection learning algorithm ent learners are considered to have the high-est TUV[REF_CITE].
"For computationally intensive prob-lems such as grammar induction, maintaining ural language processing."
Sample selection multiple learners may be an impracticality.
"In presents an attractive solution to offset this this work, we explore sample selection with a labeled data sparsity problem."
"Thus far, it single learner that keeps just one working hy- has been successfully applied to several classi-pothesis at all times."
Figure 1 outlines the single-learner sample selection training loop in pseudo-code.
"Ini-tially, the training set, L, consists of a small number of labeled examples, based on which the learner proposes its first hypothesis of the target concept, C. Also available to the learner is a large pool of uulabeled training candidates, U. In each training iteration, the selection algorithm, Select(n, U,C, f), ranks the candidates of U according to their ex-pected TUVs and returns the n candidates with the highest values."
"The algorithm com-putes the expected TUV of each candidate, u E U, with an evaluation function, f(u, C)."
This function may possibly rely on the hy- fication applications.
"Some examples include text categorizati[REF_CITE], part-of-speech tagging[REF_CITE], word-sense disambiguati[REF_CITE], and prepositional-phrase attachment[REF_CITE]."
"More difficult are learning problems whose objective is not classification, but generation of complex structures."
"One example in this di-rection is applying sample selection to seman-tic parsing[REF_CITE], in which sentences are paired with their semantic rep-resentation using a deterministic shift-reduce parser."
Our work focuses on another complex natural language learning problem: inducing pothesis concept C to estimate the utility of a stochastic context-free grammar that can a candidate u.
The set of the n chosen candi- generate syntactic parse trees for novel test dates are then labeled by human and added sentences. to the existing training set.
"Rnnning the learning algorithm~ Train(L), on the updated training set, the system proposes a new hy-pothesis consistent with all the examples seen thus far."
"The loop continues until one of three stopping conditions is met: the hypothesis is considered close enough to the target concept, all candidates are labeled, or all human re-sources are exhausted."
Sample selection may be beneficial for many learning tasks in natural language process-
"Although abstractly, parsing with a gram-mar can be seen as a classification task of de-termining the structure of a sentence by se-lecting one tree out of a set of possible parse trees, there are two major distinctions that differentiate it from typical classification prob-lems."
"First, a classifier usually chooses from a fixed set of categories, but in our domain, every sentence has a different set of possible parse trees."
"Second, for most classification ing."
"Although there exist abundant collec- problems, the the number of the possible cate-tions of raw text, the high expense of man- gories is relatively small, whereas the number ually annotating the text sets a severe lim- of potential parse trees for a sentence is expo-itation for many learning algorithms in nat- nential with respect to the sentence length. 3 tic Lexicalized Tree Insertion Grammar rep-Grammar Induction The degree of difficulty of the task of learning resentation ([REF_CITE]; Hwa, a grammar from data depends on the quantity and quality of the training supervision."
"When the training corpus consists of a large reservoir of fully annotated parse trees, it is possible to directly extract a grammar based on these parse trees."
The success of recent high-quality parsers[REF_CITE]relies on the availability of such treebank corpora.
"To work with smaller training corpora, the learning system would require even more in-formation about the examples than their syn-tactic parse trees."
"For instance,[REF_CITE]have described a learning system that can build a deterministic shift-reduce parser from a small set of training examples with the aid of detailed morpho-logical, syntactical, and semantic knowledge databases and step-by-step guidance from hu-man experts."
The induction task becomes more chal-lenging as the amount of supervision in the training data and background knowledge de-creases.
"To compensate for the missing infor-mation, the learning process requires heuristic search to find locally optimal grammars."
One form of partially supervised data might spec-ify the phrasal boundaries without specify-ing their labels by bracketing each constituent unit with a pair of parentheses[REF_CITE].
"For example, the parse tree for the sen-tence &apos;~Severalfund managers expect a rough market this morning before prices stablize.&quot; is labeled as &quot;((Several fund managers) (ex-pect ((a rough market) (this morning)) (be-fore (prices stabilize))).)&quot; As shown[REF_CITE], an essentially unsuper-vised learning algorithm such as the Inside-Outside re-estimation process[REF_CITE]can be modified to take advantage of these bracketing constraints."
"For our sample selection experiment, we chose to work under the more stringent con-dition of partially supervised training data, as described above, because our ultimate goal is to minimize the amount of annotation done by humans in terms of both the number of sentences and the number of brackets within the sentences."
"Thus, the quality of our in-duced grammars should not be compared to 1998)."
This formalism&apos;s Context-free equiva-lence and its lexicalized representation make the training process efficient and computa-tionally plausible.
"In this paper, we propose two uncertainty-based evaluation functions for estimating the training utilities of the candidate sentences."
The first is a simple heuristic that uses the length of a sentence to estimate uncertain-ties.
The second function computes uncer-tainty in terms of the entropy of the parse trees that the hypothesis-grammar generated for the sentence.
"Let us first consider a simple evaluation function that estimates the training utility of a candidate without consulting the cur-rent hypothesis-grammar, G. The function ften(s,G) coarsely approximates the uncer-tainty of a candidate sentence s with its length: flen(S, G) = length(s)."
The intuition behind this function is based on the general observation that longer sen-tences tend to have complex structures and introduce more opportunities for ambiguous parses.
"Since the scoring only depends on sentence lengths, this naive evaluation func-tion orders the training pool deterministically regardless of either the current state of the grammar or the annotation of previous train-ing sentences."
This approach has one major advantage: it is easy to compute and takes negligible processing time.
Sentence length is not a very reliable indi-cator of uncertainty.
"To measure the un-certainty of a sentence more accurately, the evaluation function must base its estimation on the outcome of testing the sentence on the hypothesis-grammar."
"When a stochastic grammar parses a sentence, it generates a set of possible trees and associates a likelihood value with each."
"Typically, the most likely those extracted from a fully annotated train- tree is taken to be the best parse for the sen-ing corpus."
The learning algorithm we use is tence. a variant of the Inside-Outside algorithm that We propose an evaluation function that induces grammars expressed in the Probabilis- considers the probabilities of all parses.
"The ~,cv Pr(v l G) log2Pr(v l G) of 100 sentences."
"In every iteration, n = 100 Pr(s IG) new sentences are picked from U to be added to L, and a new C is induced from the updated E sv P (v Ia) + logs P (s I -, i b) L."
"After the hypothesis-grammar is updated, it is tested."
The quality of the induced gram- ~vev Pr(v ]G) l°g 2Pr(vIG ) max is judged by its ability to generate cor- Pr(s 1 a) rect parses for unseen test sentences.
"We use + log2 Pr(s IG) the consistent bracketing metric (i.e., the per-centage of brackets in the proposed parse not Using the bottom-up, dynamic program- crossing brackets of the true parse) to mea-ming technique of computing Inside Proba- sure parsing accuracy1."
"To ensure the staffs-bilities[REF_CITE], we can ef- tical significance of the results, we report the ficiently compute the probability of the sen- average of ten trials for each experiment2. tence, Pr(s I G)."
"Similarly, the algorithm"
To determine the effectiveness of selecting functions.
"The learning rate relatesthe qual-training examples with the two proposed eval- ity of the induced grammars to the amount of uation functions, we compare them against supervised trainingdata available."
"In order a baseline of random selection (frand(S,G) = for the induced grammar to parse test sen-rand())."
"The task is to induce grammars from tences with higher accuracy (x-axis),more su-selected sentences in the Wall Street Journal pervision (y-axis) is needed."
"The amount of (WSJ) corpus, and to parse unseen test sen- supervision ismeasured in terms of the num-tences with the trained gr~.mmars."
"Because ber of brackets rather than sentences because the vocabulary size (and the grammar size itmore accurately quantifiesthe effortspent by extension) is very large, we have substi- by the human annotator."
"Longer sentences tuted the words with their part-of-speech tags tend to require more brackets than short ones, to avoid additional computational complexity and thus take more time to analyze."
We deem in training the grammar.
"After replacing the one evaluation function more effectivethan words with part-of-speech tags, the vocabu- another ifthe smallest set of sentences it se-lary size of the corpus is reduced to 47 tags. lected can train a grammar that performs at"
We repeat the study for two different leastas well as the grammar trained under the candidate-pool sizes.
"For the first experiment, other function and if the selected data con-we assume that there exists an abundant sup-- tains considerably fewer brackets than that of ply of unlabeled data."
"Based on empirical ob- the other function. servations (as will be shown in Section 6), for Figure 2(a) presents the outcomes of the the task we are considering, the induction al- firstexperiment, in which the evaluation func-gorithm typically reaches its asymptotic limit tions select training examples out of a large after training with 2600 sentences; therefore, candidate-pool."
"We see that overall, sample it is sufficient to allow for a candidate-pool size selection has a positive effect on the learning of U = 3500 unlabeled WSJ sentences."
"In the second experiment, we restrict the size of the IThe unsupervised induction algorithm induces candidate-pool such that U contains only 900 grammars that generate binary branching treesso that unlabeled sentences."
This experiment studies thenumber of proposed brackets in a sentence is al-ways one fewer than the length of the sentence.
"The how the paucity of training data affects the WSJ corpus, on the other hand, favors a more fiat-evaluation functions. tened tree structure with considerably fewer brackets For both experiments, each of the three per sentence."
"The consistent bracketing metric does evaluation functions: frand, ften, and fte, is not unfairly penalize a proposed parse tree for being binary branching. applied to the sample selection learning algo- 2We generate differentcandidate-pools by moving rithm shown in Figure 1, where concept C is a fixed-sizewindow across[REF_CITE]through the current hypothesis-grammar G, and L, the 05, advancing 400 sentences for each trial."
"Sec~n 23 set of labeled training data; initially consists isalways used fortesting. evaluation functions for (a) when the candidate-pool is large, and (b) when the candidate-pool is small. grammar set 11avg. training brackets [ t-test on bracket.avg. avg. score t-test on score avg baseline-26 33355 N/A 80.3 N/A length-17 30288 not sig. worsebetter 80.3 tree entropy-!4 better 80.421236 not sig. worse"
"Table 1: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars induced with the baseline (after 26 selection iterations) to the sets of grammars induced under the proposed evaluation functions (ften after 17 iterations, fte after 14 iterations). rate of the induction process."
"For the base- line if its mean test score is at least as high line case, the induction process uses frand, as that of the baseline and if the difference of in which training sentences are randomly se- the means is not statistically significant (us-lected."
The resulting grammars achieves an ing pair-wise t-test at 95% confidence).
Ta-average parsing accuracy of 80.3% on the test ble 1 summarizes the statistical significance of sentences after seeing an average of 33355 comparing the best set of baseline grammars brackets in the training data.
"The learning with those of of f~en and ffte. rate of the tree entropy evaluation function, Figure 2(b) presents the results of the sec-fte, progresses much faster than the baseline. ond experiment, in which the evaluation func- To induce a grammar that reaches the same tions only have access to a small candidate 80.3% parsing accuracy with the examples se-pool."
"Similar to the previous experiment, lected by fte, the learner requires, on average, grammars induced from training examples se- 21236 training brackets, reducing the amount lected by fte require significantly less annota-of annotation by 36% comparing to the base-tions than the baseline."
"Under the baseline, line."
"While the simplistic sentence length frand, to train grammars with 78.5% parsing evaluation function, f~en, is less helpful, its accuracy on test data, an average of 11699 learning rate still improves slightly faster than brackets (in 900 sentences) is required."
In con-the baseline.
"A grammar of comparable qual-trast, fte can induce a comparable grammar ity can be induced from a set of training exam-with an average of 8559 brackets (in 600 sen-ples selected by fzen containing an average of tences), providing a saving of 27% in the num- 30288 brackets."
This provides a small reduc-ber of training brackets.
The simpler evalua-tion of 9% from the baseline 3.
We consider a tion function f~n out:performs the baseline set of grammars to be comparable to the base-as well; the 600 sentences it selected have an average of 9935 brackets.
"Table 2 shows the 3In terms of the number of sentences, the baseline statistical significance of these comParisons. f~d used 2600 randomly chosen training sentences; .fze,~ selected the 1700 longest sentences as training A somewhat surprising outcome of the sec-data; and fte selected 1400 sentences. ond study is that the grammars induced from grammar set II avg. training brackets t-test on bracket avg. avg. score t-t-test~est on score aavg~ . baseline-9 N/[REF_CITE].5 N/A length-6 9936 better 78.5 not sig. worse tree entropy-6 8559 better 78.5 not sig. worse tree entropy-8 11242 better 79.1 better"
"Table 2: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars induced with the baseline (after 9 selection iterations) to the sets of grammars induced under the proposed evaluation functions (ften after 6 iterations, fte after 6 and 8 iterations). the three methods did not parse with the same the evaluation functions could estimate the accuracy when all the sentences from the un- training utilities of constituent units rather labeled pool have been added to the training than full sentences."
Another area of interest set.
Presenting the training examples in dif- is to experiment with committee-based sam-ferent orders changes the search path of the ple selection using multiple learners.
"Finally, induction process."
"Trained on data selected we are interested in applying sample selection by fte, the induced grammar parses the test to other natural language learning algorithms sentences with 79.1% accuracy, a small but that have been limited by the sparsity of an-statistically significant improvement over the notated data. baseline."
"This suggests that, when faced with a dearth of training candidates, fte can make References good use of the available data to induce gram- James K. Baker. 1979."
Trainable grammars for mars that are comparable to those directly in- speech recognition.
"In Proceedings of the Spring duced from more data. Conference of the Acoustical Society of Amer-ica, pages 547-550, Boston, MA, June."
Eugene Charniak. 1997.
Statistical parsing with 7 Conclusion and Future Work a context-freegrammar and word statistics.
"In This empirical study indicates that sample se- Proceedings of the AAAI, pages 598-603, Prov-lection can significantly reduce the human ef- idence, RI."
AAAI Press/MIT Press.
"David Cohn, Les Atlas, and Richard Ladner. 1994. fort in parsing sentences for inducing gram- Improving generalization with active learning. mars."
Our proposed evaluation function using[REF_CITE](2):201-221. tree entropy selects helpful training examples.
Michael Collins. 1997.
"Three generative, lexi-"
Choosing from a large pool of unlabeled can-calised models for statistical parsing.
"In Pro-didates, it significantly reduces the amount of ceedings of the 35th Annual Meeting of the A CL, training annotations needed (by 36% in the pages 16-23, Madrid, Spain. experiment)."
Although the reduction is less Thomas M. Cover and Joy A. Thomas. 1991.
El-dramatic when the pool of candidates is small ements of Information Theory.
"John Wiley. (by 27% in the experiment), the training ex- Sean P. Engelson and Ido Dagan. 1996."
Mhaimiz-amples it selected helped to induce slightly ing manual annotation cost in supervised train-ing from copora.
"In Proceedings of the 34th An-better grammars. nual Meeting of the ACL, pages 319-326."
"The current work suggests many potential Yoav Freund, H. Sebastian Seung, Eli Shamir, and research directions on selective sampling for Naftali Tishby. 1997."
Selective sampling using grammar induction.
"First, since the ideas be- the query by committee algorithm."
"Machine hind the proposed evaluation fimctions are[REF_CITE](2-3):133-168. general and independent of formalisms, we Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, would like to empirically determine their ef- and Hozumi Tanaka. 1998."
Selective sampling fect on other parsers.
"Next, we shall explore for example-based word sense disambiguation. alternative formulations of evaluation func-[REF_CITE](4):573-598, De-cember. tions for the single-learner system."
Ulf Hermjakob and Raymond J. Mooney. 1997. rent approach uses uncertainty-based evalua-
Learning parse and translation decisions from tion functions; we hope to consider other fac-examples with rich context.
"In Proceedings o/ tors such as confidence about the parameters the Association for Computational Linguistics, of the grammars and domain knowledge."
An empiric~alevaluation within a sentence as training examples.
"Thus, of probabilistic lexicaiized tree insertion gram-"
"For illustrative purposes, we describe the computation process using a PCFG grammar terminal symbols."
"Moreover, let the sym-bol S be the start symbol of the grammar G."
"Following the notation of Lari and Young, we denote the inside probability as e(X, i,j), which represents the probability that a non-terminal X :~ wi...wj."
"Similarly, we define a new function h(X, i,j) to represent the cor-responding entropy for the set of subtrees. h(X,i,j) =- Pr(."
"Ia)log (Pr(, IV)). vEX~wi...w~"
"Therefore, ~vev Pr(v [G)log2 Pr(v l G) can be expressed as h(S, 1, n)."
"We compute all possible h(X,i,j) re-cursively."
"The base case is h(X,i,i) = -e(X, i, i) log2 (e(X, i, i)) since a non-terminal X can generate the symbol wi in exactly one way."
"For the more general case, h(X, i,j), we consider all the possible rules with X on the left hand side that might have contributed to build X =~ wi . . . wj. j-1 hr, k=i (x~YZ)"
"The function hy,z,k(X,i,j) is a portion of h(X,i,j) where Y =~ wi...wk and Z ~ Wk+l... wj."
"The non-terminals Yand Z may, in turn, generate their substrings with mul-tiple parses."
Let there be a parses for Y wi...
Wk and f~ parses for Z ~ Wk+l...wi.
"Let x denote the event of X --r YZ; y E Yl,...,Ya; and z E zl,...,zz."
"The proba-bility of one of the a x fl possible parses is Pr(x)Pr(y)Pr(z), and hY,z,k is computed by summing over all possible parses: hy,z,k(X, i,j) = -- ~ , z Pr(x)Pr(y)Pr(z)x log2 (Pr (x)Pr (y)Pr (z) ) = - Z~,~Pr(x)Pr(y)Pr(z)× [log2 Pr(x) + log2 Pr(y) + log2Pr(z)] = - P r ( x ) log2 Pr(x)e(Y, i, k)e(Z, k+l, j) +Pr(x)h(Y,i, k)e(Z,k + 1,j) +"
"Pr(x)e(Y, i, k)h(Z, k + 1,j)."
"These equations can be modified to compute the tree entropy of sentences using a Prob-abilistic Lexicalized Tree Insertion Grammar expressed in Chomsky Normal Form, in which[REF_CITE]. each rule can have two forms: X ~ YZ or X ---ra, where X, Y,Z are variables over"
There are various grammar frameworks pro-posed for natural languages.
We take Lexi-calized Tree-adjoining Grammars (LTAGs) as representative of a class of lexicalized gram-mars.
LTAGs[REF_CITE]are ap-pealing for representing various phenomena in natural languages due to its linguistic and computational properties.
"In the last decade, LTAG has been used in several aspects of natural language understanding (e.g., pars-ing[REF_CITE], semantics[REF_CITE], and discourse[REF_CITE]) and a number of NLP applica-tions (e.g., machine translati[REF_CITE], information retrieval[REF_CITE], and generati[REF_CITE]."
"This paper describes a system that extracts LTAGs from annotated corpora (i.e., Treebanks)."
There has been much work done on extract-ing Context-Free grammars (CFGs)[REF_CITE].
"However, extracting LTAGs is more complicated than extracting CFGs because of the differences between LTAGs and CFGs."
"First, the primitive elements of an LTAG are"
LTAG from all other formalisms.
"Third, un-like in CFGs, the parse trees (also known as derived trees in the LTAG) and the derivation trees (which describe how elementary trees are combined to form parse trees) are differ-ent in the LTAG formalism in the sense that a parse tree can be produced by several dis-tinct derivation trees."
"Therefore, to provide training data for statistical LTAG parsers, an LTAG extraction algorithm should also build derivation trees."
"For each phrase structure in a Treebank, our system creates a fully bracketed phrase structure, a set of elementary trees and a derivation tree."
The data produced by our system have been used in several NLP tasks.
We report experimental results on two of those applications and compare our ap-proaches with related work.
The primitive elements of an LTAG are ele-mentary trees (etrees).
Each etree is associ-ated with a lexical item (called the anchor of the tree) on its frontier.
"We choose LTAGs as our target grammars (i.e., the grammars to be extracted) because LTAGs possess many de-sirable properties, such as the Extended Do-main of Locality, which allows the encapsula-tion of all arguments of [he anchor associated with an etree."
There are two types of etrees:. initial trees and auxiliary trees.
"An auxiliary tree represents recursive structure and has a unique leaf node, called the foot node, which has the same syntactic category as the root lexicalized tree structures (called elementary node."
"Leaf nodes other than anchor nodes trees), not context-free rules (which can be and foot nodes are substitutionnodes."
Etrees seen.as trees with depth one).
"Therefore, an are combined by two operations: substitution LTAG extraction algorithm needs to examine and adjunction, as in Figure 1 and 2."
Figure 5: A Treebank example 3 System Overview
Our system recognizes three types of rela-
"We have built a system, called LexTract, for tion (namely, predicate-argument, modifica-grammar extraction."
"The architecture of Lex-tion, and coordination relations) between the Tract is shown in Figure 4 (the parts that will anchor of an etree and other nodes in the etree, be discussed in this paper are in bold)."
"The and imposes the constraint that all the etrees core of LexTract is an extraction algorithm to be extracted should fall into exactly one of that takes a Treebank sentence such as the the three patterns in Figure 6. one in Figure 5 and produces the trees (el-ementary trees, derived trees and derivation • The spine-etrees for predicate-argument trees) such as the ones in Figure 3. relations."
X ° is the head of X m and the 3.1 anchor of the etree.
"The etree is formedThe Form of Target Grammars by a spine X m --+ X m-1 -~ .. --+ X ° and Without further constraints, the etrees in the the arguments of X °. target grammar could be of various shapes. • The mod-etrees for modification rela- #, .4&quot;--. &quot; VP -- ~, #3 &apos; ."
S ~: tions.
"The root of the etree has two chil- l: NP A &quot;- ~I -- NP I dren, one is a foot node with the label ADVP VP* &quot;."
"NP I VP &apos; s~s &quot; .... &quot;~:~&apos;-. i Ntis I etrees and conj-etrees are auxiliary trees. 3.3.1 Fully bracketing ttrees 3.2 Treebank-specific information As just mentioned, the ttrees in the Tree-The phrase structures in the Treebank (ttrees bank do not explicitly distinguish arguments for short) are partially bracketed in the sense and modifiers, whereas etrees do."
"To account that arguments and modifiers are not struc- for this difference, we first fully bracket the turally distinguished."
"In order to construct ttrees by adding intermediate nodes so that the etrees, which make such distinction, Lex- at each level, one of the following relations Tract requires its user to provide additional holds between the head and its siblings: (1) information in the form of three tables: a head-argument relation, (2) modification re-"
"Head Percolation Table, an Argument Table, lation, and (3) coordination relation."
Lex-and a Tagset Table.
Tract achieves this by first choosing the head-
"A Head Percolation Table has previ- child at each level and distinguishing argu-ously been used in several statistical parsers ments from adjuncts with the help of the three[REF_CITE]to find heads tables mentioned in Section 3.2, then adding of phrases."
Our strategy for choosing heads is intermediate nodes so that the modifiers and similar to the one[REF_CITE].
An Ar- arguments of a head attach to different levels. gument Table informs LexTract what types of Figure 7 shows the fully bracketed ttree.
The arguments a head can take.
"The Tagset Table nodes inserted by LexTract are in bold face. specifies what function tags always mark ar- 3.3.2 Building etrees guments (adjuncts, heads, respectively)."
"Lex-In this step, LexTract removes recursive struc-"
Tract marks each sibling of a head as an argu-tures - which will become mod-etrees or conj-ment if the sibling can be an argument of the etrees - from the fully bracketed ttrees and head according to the Argument Table and builds spine-etrees for the non-recursive struc-none of the function tags of the sister indi-tures.
Starting from the root of a fully brack-cates that it is an adjunct.
"For example, in eted ttree, LexTract first finds a unique path Figure 5, the head of the root S is the verb from the root to its head."
"It then checks each draft, and the verb has two siblings: the noun node e on the path."
"If a sibling of e in the ttree phrase policies is marked as an argument of is marked as a modifier, LexTract marks e and the verb because from the Argument Table we e&apos;s parent, and builds a mod-etree (or a conj-know that verbs in general can take an NP ob-etree if e has another sibling which is a con-ject; the clause is marked as a modifier of the junction) with e&apos;s parent as the root node, e as verb because, although verbs in general can the foot node, and e&apos;s siblings as the modifier. take a sentential argument, the Tagset Table Next, LexTract creates a spine-etree with the informs LexTract that the function tag -MNR remaining unmarked nodes on the path and (manner) always marks a modifier. their siblings."
"Finally, LexTract repeats this process for the nodes that are not on the path."
"In Figure 8, which is the same as the one in Algorithm"
"Figure 7 except that some nodes are numbered The extraction process has three steps: First, and split into the top and bottom pairs,1 the LexTract fully brackets each ttree; Second, LexTract decomposes the fully bracketed ttree 1When a pair of etrees are combined during parsing, of VP2 and $3 is a modifier of VP3, and the corresponding structures form mod-etrees #4 3.4 Uniqueness of decomposition and #7."
"On the path from the root to VBP, To summarize, LexTract is a language- Sl.t and S2.b are merged (and so are VPi.t independent grammar extraction system, and VP3.b) to from the spine-etree #5."
"Re- which takes Treebank-specific information peating this process for other nodes will gen- (see Section 3.2) and a ttree T, and creates erate other trees such as trees #2, #3 and #6."
"The whole ttree yields twelve etrees as shown 2Without this additional constraint, the derivation tree sometimes is not unique."
"For example, in Figure in Figure 9. 8, both #4 and #7 modify the etree#5."
"If adjunc- 3.3.3 tion were allowed at foot nodes, ~4 could adjoin toBuilding derivation trees ~7 at VP2.b, and #7 would adjoin to #5 at VPs.b."
The fully bracketed ttree is in fact a derived An alternative is for #4 to adjoin to #5 at VPs.b and tree of the sentence if the sentence is parsed for ~7 to adjoin to ~4 at VP2.t.
The no-adjunction-with the etrees extracted by LexTract.
"In ad- at-foot-node constraint would rule out the latter al-dition to these etrees and the derived tree, we ternative and make the derivation tree unique."
Note that this constraint has been adopted by several hand-the root ofone etreeis merged with a node in the other crafted grammars such as the XTAG grammar for En-etree.
"Splitting nodes into top and bottom pairs during glish[REF_CITE], because it eliminates this the decomposition of the derived tree is the reverse source of spurious ambiguity. process of merging nodes during parsing."
"For the sake SThis decision may affect parsing accuracy of an of simplicity, we showthe top and the bottom parts of LTAGparser which uses the derivation trees for train-a node only when the two parts willend up in different ing, but it will not affect the results reported in this etrees. paper. (1) a fully bracketed ttree T*, (2) a set Eset we use an example to illustrate how the con-of etrees, and (3) a derivation tree D for T*. ditions (C1)--(C4) rule out all the decompo-"
"Furthermore, Eset is the only tree set that sitions except the one produced by LexTract. satisfies all the following conditions:[REF_CITE]the ttree T* has 5 nodes (i.e., S, NP, N, VP, and V)."
"Out of these 32 decom-generated if the trees in the set were com-positions, only five (i.e., E2 -- E6) are fully bined via the substitution and adjunction lexicalized -- that is, each tree in these tree operations. sets is anchored by a lexical item."
"The rest, (C2) LTAG formalism: Each tree in the including El, are not fully lexicalized, and are set is a valid etree, according to the LTAG therefore ruled out by the condition (C2)."
"For instance, each tree should the remaining five etree sets, E2 -- E4 are be lexicalized and the arguments of the ruled out by the condition (C3), because each anchor should be encapsulated in the of these tree sets has one tree that violates one same etree. constraint which says that in a spine-etree an argument of the anchor should be a substitu- (C3) Target grammar: Each tree in the tion node, rather than an internal node."
"For set falls into one of the three types as the remaining two, E5 is ruled out by (C4) specified in Section 3.1. because according to the Head Table provided by the user, the head of the S node should be (C4) Treebank-specific information: V, not N. Therefore, E6, the tree set that is The head/argument/adjunct distinction produced by LexTract, is the only etree set for in the trees is made according to the T* that satisfies (C1)--(C4)."
Treebank-specific information provided by the user as specified in Section 3.2.
We have ran LexTract on the one-million-word English Penn Treebank[REF_CITE]and got two Treebank grammars.
"The first one, G1, uses the Treebank&apos;s tagset."
"The second Treebank grammar, G2, uses a reduced tagset, where some tags in the Treebank tagset are merged into a single tag."
"For example, the tags for verbs, MD/VB/VBP/VBZ/VBN/VBD/VBG, are merged into a single tag V. The reduced [ &amp;~m t lea lc~hn [ Icft tagset is basically the same as the tagset (E) (E,) (Es) (E6) used in the XTAG grammar[REF_CITE]."
"G2 is built so that we can compare[REF_CITE]: Tree sets for a fully bracketed ttree it with the XTAG grammar, as will be discussed in the next section."
"We also ran the This uniqueness of the tree set may be quite system on the 100-thousand-word Chinese surprising at first sight, considering that the"
"Penn Treebank[REF_CITE]and on a number of possible decompositions of T* is 30-thousand-word Korean Penn Treebank. ~(2n), where n is the number of nodes in T*.4"
"The sizes of extracted grammars are shown in Instead of giving a proof of the uniqueness,"
Table 1. (For more discussion on the Chinese 4Recall that the process of building etrees has two and the Korean Treebanks and the compar-steps.
"First, LexTract treats each node as a pair of ison between these Treebank grammars, see the top and bottom parts."
The ttree is cut into pieces[REF_CITE]).
The second column of along the boundaries of the top and bottom parts of the table lists the numbers of unique tem-some nodes.
"The top and the bottom parts of each plates in each grammar, where templates are node belong to either two distinct pieces or one piece, as a result, there are 2~ distinct partitions."
"Second, etrees with the lexical items removed,s The some non-adjacent pieces in a partition can be glued third column shows the numbers of unique together to form a bigger piece."
"Therefore, each par-tition will result in one or more decompositions of the 5Forinstance, #3, #6 and #9 in Figure 9 are three ttree."
"In total, there are at least 2n decompositions of different etrees but they share the same template."
"An the ttree. etree can be seen as a (word, template) pair. etrees."
The average numbers of etrees for each • We have used LexTract to retrieve the word type in G1 and G2 are 2.67 and-2.38 data from Treebanks to test theoret-respectively.
"Because frequent words often ical linguistic hypotheses such as the anchor many etrees, the numbers increase by Tree-locality Hypothesis (Xia and Bleam, more than 10 times when we consider word 20O0). token, as shown in the fifth and sixth columns • LexTract has a filter that checks the of the table."
G3 and G4 are much smaller plausibility of extracted etrees by decom-than G1 and G2 because the Chinese and the posing each etree into substructures and Korean Treebanks are much smaller than the checking them.
Implausible etrees are of- English Treebank. ten caused by Treebank annotation er-
"In addition to LTAGs, by reading context-rors."
"Because LexTract maintains the free rules off the etrees of a Treebank LTAG, mappings between etree nodes and ttree LexTract also produces CFGs."
"The numbers nodes, it can detect certain types of an-of unlexicalized context-free rules from G1--notation errors."
We have used LexTract G4 are shown in the last column of Table 1. for the final cleanup of the Penn Chinese Comparing with other CFG extraction algo-
"Treebank. rithms such as the one[REF_CITE], the CFGs produced by LexTract have sev-"
"Due to space limitation, in this paper we eral good properties."
"For example, they allow will only discuss the first two tasks. unary rules and epsilon rules, they are more compact and the size of the grammar remains 4.1 Evaluating the coverage of monotonic as the Treebank grows. hand-crafted grammars[REF_CITE]shows the log frequency of tem-"
"The XTAG grammar[REF_CITE]plates and the percentage of template tokens is a hand-crafted large-scale grammar for En-covered by template types in G1.6 In both glish, which has been developed at University cases, template types are sorted according to of Pennsylvania in the last decade."
It has been their frequencies and plotted on the X-axes. used in many NLP applications such as gen-
The figure indicates that a small portion of erati[REF_CITE].
"Evaluating template types, which can be seen as the core the coverage of such a grammar is important of the grammar, cover majority of template for both its developers and its users. tokens in the Treebank."
"For example, the first Previous evaluations ([REF_CITE]; 100 (500, 1000 and 1500, resp.) templates[REF_CITE]) of the XTAG grammar cover 87.1% (96.6~o, 98.4% and 99.0% resp.) use raw data (i.e., a set of sentences with-of the tokens, whereas about half (3411) of out syntactic bracketing)."
"The data are first the templates each occur only once, account- parsed by an LTAG parser and the coverage ing for only 0.29% of template tokens in total. of the grammar is measured as the percent-age of sentences in the data that get at least 4 Applications of LexTract one parse, which is not necessarily the correct parse."
"For more discussion on this approach, In addition to extract LTAGs and CFGs, Lex-see[REF_CITE]."
"Tract has been used to perform the following We propose a new evaluation method that tasks: takes advantage of Treebanks and LexTract. • We use the Treebank grammars produced The idea is as follows: given a Treebank T and by LexTract to evaluate the coverage of a hand-crafted grammar Gh, the coverage of hand-crafted grammars."
"Gh on T can be measured by the overlap of Gh and a Treebank grammar Gt that is produced • We use the (word, template) sequence by LexTract from T. In this case, we will esti-produced by LexTract to re-train Srini-mate the coverage of the XTAG grammar on vas&apos; Supertaggers[REF_CITE]. the English Penn Treebank (PTB) using the • The derivation trees created by LexTract Treebank grammar G2. are used to train a statistical LTAG"
There are obvious differences between these parser[REF_CITE].
LexTract output two grammars.
"For example, feature struc-has also been used to train an LR LTAG tures and multi-anchor etrees are present only parser[REF_CITE]. in the XTAG grammar, whereas frequency in-formation is available only in G2."
"When we 6Similar results hold for G2, G3 and G4. match templates in two grammars, we disre- template word etree types etree types CFG rules etree types types types per word type i per word token (unlexicalized)"
"Ch G3 9.131140 1.9621,125 10,772 515 Kor[REF_CITE]1.45 2.76 177634 9,787 gard the type of information that is present mars do not match according to our defini-only in one grammar."
"As a result, the map- tion. ping between two grammars is not one-to-one. (T4) Constructions not covered by XTAG:"
"Some of such constructions are the unlike matched unmatched total coordination phrase (UCP), parenthetical templates templates (PRN), and ellipsis."
"For (T1)--(T3), the XTAG grammar can ~equency 82.1% I 17.9% [100% handle the corresponding constructions al-"
Table 2: Matched templates in two grammars though the templates used in two grammars look very different.
"To find out what construc- Table 2 shows that 497 templates in the tions are not covered by XTAG, we manually XTAG grammar and 215 templates[REF_CITE]of the most frequent unmatched match, and the latter accounts for 82.1% of templates in G2 according to the reason why the template tokens in the PTB."
The remain- they are absent from XTAG.
"The results because of one of the following reasons: are shown in Table 3, where the percentage is with respect to all the tokens in the Treebank. (T1) Incorrect templates in G2:"
"These tem- From the table, it is clear that the most com-plates result from Treebank annotation er-mon reason for mis-matches is (T3)."
"Combin-rors, and therefore, are not in XTAG. ing the results in Table 2 and 3, we conclude (T2) Coordination in XTAG: the templates that 97.2% of template tokens in the Treebank for coordinations in XTAG are generated are covered by XTAG, while another 1.7% are on the fly while parsing (Sarkar and Joshi, not."
"For the remaining 1.1% template tokens, 1996), and are not part of the 1004 templates. we do not know whether or not they are cov-"
"Therefore, the conj-etrees in G2, which ac-ered by XTAG because we have not checked count for 3.4% of the template tokens in the"
"Treebank, do not match any templates in the remaining 2416 unmatched templates in"
"To summarize, we have just showed that, (T3)"
Alternative analyses: XTAG and PTB sometimes choose different analyses for the 7[REF_CITE].2% is the sum of two numbers: same phenomenon.
"For example, the two the first one is the percentage of matched template to-grammars treat reduced relative clauses dif- kens (82.1% from Table 2)."
The secb-ndnumber is the ferently.
"As a result, the templates used to percentage of template tokens which fall under (T1)--handle those phenomena in these two gram- (T3), i.e., 16.8%-1.7%=15.1% from Table 3. evaluation approach, this :method has several advantages."
"First, the whole process is semi-automatic and requires little human effort."
"Second, the coverage can be calculated at ei-ther sentence level or etree level, which is more fine-grained."
"Third, the method provides a list of etrees that can be added to the gram-mar to improve its coverage."
"Fourth, there is no need to parse the whole corpus, which could have been very time-consuming."
A Supertagger[REF_CITE]assigns an etree template to each word in a sentence.
The templates are also called Supertags because they in-clude more information than Part-of-Speech tags.
"Srinivas implemented the first Supertag-ger, and he also built a Lightweight Depen-dency Analyzer that assembles the Supertags of words to create an almost-parse for the sen-tence."
"Supertaggers have been found useful for several applications, such as information retrieval[REF_CITE]."
"To use a Treebank to train a Supertagger, the phrase structures in the Treebank have to be converted into (word, Supertag) sequences first."
"Producing such sequences is exactly one of LexTract&apos;s main functions, as shown previ-ously in Section 3.3.2 and Figure 9."
"Besides LexTract, there are two other at-tempts in converting the English Penn Tree-bank to train a Supertagger."
His method is different from LexTract in that the set of Supertags in his method is chosen from the pre-existing XTAG grammar before the con-
The method is similar to ours in that both work use Head Percolation Tables to find the head and both distinguish adjuncts from modifiers using syntactic tags and func-tional tags.
"Nevertheless, there are several differences: only LexTract explicitly creates fully bracketed ttrees, which are identical to the derived trees for the sentences."
"As a re-sult, building etrees can be seen as a task of decomposing the fully bracketed ttrees."
The mapping between the nodes in fully bracketed ttrees and etrees makes LexTract a useful tool for &apos;IYeebank annotation and error detection.
The two approaches also differ in how they distinguish arguments from adjuncts and how they handle coordinations.
Table 4 lists the tagging accuracy of the same trigram Supertagger[REF_CITE]trained and tested on the same original PTB data.s
"The difference in tagging accuracy is caused by different conversion algorithms that convert the original PTB data into the (word, template) sequences, which are fed to the Supertagger."
The results of Chen &amp; Vijay-Shanker&apos;s method come from their pa-per[REF_CITE].
They built eight grammars.
We just list two of them which seem to be most relevant: C4 uses a re-duced tagset while C3 uses the PTB tagset.
"As for Srinivas&apos; results, we did not use the re-sults reported[REF_CITE]and[REF_CITE]because they are based on different training and testing data. 9 Instead, we re-ran 22."
"Chen &amp; Vijay-Shauker&apos;s results~e[REF_CITE]Supertag set, and it is not very-easy to port only. it to another Supertag set."
"A third difference 9He used Section 0-24 minus[REF_CITE]fortraining is that the Supertags in his converted data do and the[REF_CITE]for testing. his Supertagger using his data on the sections POS tags may improve the Supertagging ac-that we have chosen.1° We have calculated curacy,n Third, the Supertagging accuracy two baselines for each seg of data."
The first using G2 is 1.3-1.9% lower than the one using one tags each word in testing data with the Srinivas&apos; data.
This is not surprising since the most common Supertag w.r.t the word in the size of G2 is 6 times that of Srinivas&apos; grammar. training data.
"For an unknown word, just use Notice that G1 is twice the size of G2 and its most common Supertag."
For the second the accuracy using G1 is 2% lower.
"Fourth, baseline, we use a trigram POS tagger to tag higher Supertagging accuracy does not neces-the words first, and then for each word we use sarily means the quality of converted data are the most common Supertag w.r.t, the (word, better since the underlying grammars differ a POS tag) pair. lot with respect to the size and the coverage."
A few observations are in order.
"First, the 5 Conclusion baselines for Supertagging are lower than the"
"We have presented a system for grammar ex-one for POS tagging, which is 91%, indicat- traction that produces an LTAG from a Tree-ing Supertagging is harder than POS tagging. bank."
"The output produced by the system Second, the second baseline is slightly bet-has been used in many NLP tasks, two of ter than the first baseline, indicating using which are discussed in the paper."
"In the first ~°Noticeably,the results wereport on Srinivas&apos; data, task, by comparing the XTAG grammar with 85.78%[REF_CITE]and 85.53%[REF_CITE]axe a Treebank grammar produced by LexTract, lower than 92.2% reported[REF_CITE]and we estimate that the XTAG grammar covers 91.37%[REF_CITE]."
There axe several 97.2% of template tokens in the English Tree-reasons for the difference.
"First, the size of training data in our report is smaller than the one for his pre- bank."
"We plan to use the Treebank grammar vious work; Second, we treat punctuation marks as to improve the coverage of the XTAG gram-normal words during evaluation because, likeother mar."
"We have also found constructions that words, punctuation marks can anchor etrees, whereas are covered in the XTAG grammar but do not he treats the Supertags for punctuation marks as al-appear in the Treebank."
"In the second task, ways correct."
"Third, he used some equivalent classes during evaluations."
"If a word is mis-tagged as x, while LexTract converts the Treebank into a format the correct Supertag is y, he considers that not to be that can be used to train Supertaggers, and an error if x and y appear in the same equivalent class. the Supertagging accuracy is compatible to, if Wesuspect that the reason that those Supertagging er-not better than, the ones based on other con-rors axe disregarded is that those errors might not af-fect parsing results when the Supertags are combined. version algorithms."
"For future work, we plan For example, both adjectives and nouns can modify to use derivation trees to train LTAG parsers other nouns."
The two templates (i.e. Supertags) rep- directly and use LexTract to add semantic in-resenting these modification relations look the same formation to the Penn Treebank. except for the POS tags of the anchors.
If a word which should be tagged with one Supertag is mis-
"References tagged with the other Supertag, it is likelythat the wrong Supertag can still fit with other Supertags in R. Chandrasekar and B. Srinivas. 1997."
Glean-the sentence and produce the right parse.
We did not ing information from the Web: Using Syntax use these equivalent classes in this experiment because to Filter out Irrelevant Information.
In Proc. of we are not aware of a systematic way to find all the cases in which Supertagging errors do not affect the nThe baselines and results[REF_CITE]for (Chen final parsing results. and[REF_CITE]) are not available to us.
No. Feature Type Template The model was trained and tested on the part-of-
"General feature templates can be instantiated by The testing procedure uses a beam search to arbitrary contexts, whereas rare feature tem- find the tag sequence with maximal probability plates are instantiated only by histories where given a sentence."
In our experiments we used a the current word wi is rare.
Rare words are beam of size 5.
"Increasing the beam size did not defined to be words that appear less than a result in improved accuracy. certain number of times in the training data The preceding tags for the word at the (here, the value 7 was used). beginning of the sentence are regarded as"
In order to be able to throw out features that having the pseudo-tag NA.
"In this way, the would give misleading statistics due to sparse- information that a word is the first word in a ness or noise in the data, we use two different sentence is available to the tagger."
"We do not cutoff values for general and rare feature have a special end-of-sentence symbol. templates (in this implementation, 5 and 45 We used a tag dictionary for known words respectively)."
As seen in Table 1 the features are in testing.
This was built from tags found in the conjunctions of a boolean function on the training data but augmented so as to capture a history h and a boolean function on the tag t. few basic systematic tag ambiguities that are Features whose first conjuncts are true for more found in English.
"Namely, for regular verbs the than the corresponding threshold number of -ed form can be either a VBD or a VBN and histories in the training data are included in the similarly the stem form can be either a VBP or model."
Hence for words that had occurred with
"The feature templates[REF_CITE]only one of these tags in the training data the that were left out were the ones that look at the other was also included as possible for previous word, the word two positions before assignment. the current, and the word two positions after the The results on the test set for the Baseline current."
These features are of the same form as model are shown in Table 3. but lower for unknown words.
This may stem Tagger errors are of various types.
"Some are the from the differences between the two models&apos; result of inconsistency in labeling in the training feature templates, thresholds, and approxi- data[REF_CITE], which usually reflects mations of the expected values for the features, a lack of linguistic clarity or determination of as discussed in the beginning of the section, or the correct part of speech in context."
"For may just reflect differences in the choice of instance, the status of various noun premodifiers training and test sets (which are not precisely specified[REF_CITE])."
The differences are not great enough to justify any definite statement about the different use of feature templates or other particularities of the model estimation.
One conclusion that we can draw is that at present the additional word features used[REF_CITE]- looking at words more than one position away from the current - do not appear to be helping the overall performance of the models.
"A large number of words, including many of the most common words, can have more than one syntactic category."
This introduces a lot of ambiguities that the tagger has to resolve.
Some of the ambiguities are easier for taggers to resolve and others are harder.
Some of the most significant confusions that the Baseline model made on the test set can be seen in Table 5.
"The row labels in Table 5 signify the correct tags, and the column labels signify the assigned tags."
"For example, the num-ber 244 in the (NN, JJ) position is the number of words that were NNs but were incorrectly assigned the JJ category."
"These particular confu-sions, shown in the table, account for a large percentage of the total error (2652/3651 = 72.64%)."
"Table 6 shows part of the Baseline (whether chief or maximum is NN or JJ, or whether a word in -ing is acting as a JJ or VBG) is of this type."
"Some, such as errors between NN/NNP/NNPS/NNS largely reflect difficulties with unknown words."
"But other cases, such as VBN/VBD and VB/VBP/NN, represent syste-matic tag ambiguity patterns in English, for which the fight answer is invariably clear in context, and for which there are in general good structural contextual clues that one should be able to use to disarnbiguate."
"Finally, in another class of cases, of which the most prominent is probably the RP/IN/RB ambiguity of words like up, out, and on, the linguistic distinctions, while having a sound empirical basis (e.g., see Baker (1995: 198-201), are quite subtle, and often require semantic intuitions."
"There are not good syntactic cues for the correct tag (and further-more, human taggers not infrequently make errors)."
"Within this classification, the greatest hopes for tagging improvement appear to come from minimizing errors in the second and third classes of this classification."
"In the following sections we discuss how we include additional knowledge sources to help in the assignment of tags to forms of verbs, capitalized unknown words, particle words, and in the overall accuracy of part of speech assignments. model&apos;s confusion matrix for just unknown Improving the Unknown Words Model2 words."
The accuracy of the baseline model is markedly lower for unknown words than for previously assignment accuracies for different parts of seen ones.
This is also the case for all other speech.
"For example, the accuracy on nouns is taggers, and reflects the importance of lexical greater than the accuracy on adjectives."
"The information to taggers: in the best accuracy accuracy on NNPS (plural proper nouns) is a figures punished for corpus-based taggers, surprisingly low 41.1%. known word accuracy is around 97%, whereas unknown word accuracy is around 85%."
"Tag Accuracy Tag Accuracy In following experiments, we examined[REF_CITE].3%[REF_CITE].0% ways of using additional features to improve the[REF_CITE].5%[REF_CITE].2%"
NNP accuracy of tagging unknown words.
"As previ-96.2%[REF_CITE].4% VBD ously discussed[REF_CITE], it is possible95.2%[REF_CITE].5%[REF_CITE].0% to improve the accuracy on capitalized words[REF_CITE].1% VBP that might be proper nouns or the first word in a93.4% sentence, etc."
JJ NN NNP NNPS R.B RP IN VB VBD VBN VBP Total
Capitalization Verb forms Particles Accuracy[REF_CITE].72% 96.76% 96.83% 96.86% Unknown Words Accuracy[REF_CITE].50% 86.76% 86.87% 86.91% Accuracy[REF_CITE].53% 96.55% 96.58% 96.62% Unknown Words Accuracy[REF_CITE].48% 86.03% 86.03% 86.06%
"For example, the error on the proper noun preceding tag."
"Even though-our maximum category (NNP) accounts :for a significantly entropy model does not require independence larger percent of the total error for unknown among the predictors, it provides for free only a words than for known words."
"In the Baseline simple combination of feature weights, and model, of the unknown word error 41.3% is due additional &apos;interaction terms&apos; are needed to model to words being NNP and assigned to some other non-additive interactions (in log-space terms) category, or being of other category and assigned between features."
The percentage of the same type of error for known words is 16.2%. 3 Features for Disambiguating Verb Forms The incorporation of the following two Two of the most significant sources of classifier feature schemas greatly improved NNP accuracy: errors are the VBN/VBD ambiguity and the (1) A feature that looks at whether all the letters VBP/VB ambiguity.
"As seen in Table 5, of a word are uppercase."
The feature that VBN/VBD confusions account for 6.9% of the looked at capitalization before (cf.
"Table 1, total word error."
The VBP/VB confusions are a feature No. 8) is activated when the word smaller 3.7% of the errors.
In many cases it is contains an uppercase character.
"This turns easy for people (and for taggers) to determine the out to be a notable distinction because, for correct form."
"For example, if there is a to example, in titles in the WSJ data all words infinitive or a modal directly preceding the are in all uppercase, and the distribution of VB/VBP ambiguous word, the form is certainly tags for these words is different from the non-finite."
"But often the modal can be several overall distribution for words that contain an positions away from the current position - still uppercase character. obvious to a human, but out of sight for the (2) A feature that is activated when the word baseline model."
"To help resolve a VB/VBP ambiguity in such contains an uppercase character and it is not cases, we can add a feature that looks at the at the start of a sentence."
"These word tokens preceding several words (we have chosen 8 as a also have a different tag distribution from the threshold), but not across another verb, and distribution for all tokens that contain an activates if there is a to there, a modal verb, or a uppercase character. form of do, let, make, or help (verbs that Conversely, empirically it was found that the frequently take a bare infinitive complement). prefix features for rare words were having a net Rather than having a separate feature look at negative effect on accuracy."
"We do not at present each preceding position, we define one feature have a good explanation for this phenomenon. that looks at the chosen number of positions to The addition of the features (1) and (2) and the left."
This both increases the scope of the the removal of the prefix features considerably available history for the tagger and provides a improved the accuracy on unknown words and better statistic because it avoids fragmentation. the overall accuracy.
The results on the test set We added a similar feature for resolving after adding these features are shown below: VBD/VBN confusions.
"It activates if there is a have or be auxiliary form in the preceding several Overall Accuracy Unknown Word Accuracy [ positions (again the value 8 is used in the I implementation). 96.76% 86.76% The form of these two feature templates was Table 9 Accuracy when adding capitalization features and removing prefix features. motivated by the structural rules of English and not induced from the training data, but it should Unknown word error is reduced by 15% as be possible to look for &quot;predictors&quot; for certain compared to the Baseline model. parts of speech in the preceding words in the It is important to note that (2) is composed of sentence by, for example, computing association information already &apos;known&apos; to the tagger in strengths. some sense."
"This feature can be viewed as the The addition of the two feature schemas conjunction of two features, one of which is helped reduce the VB/VBP and VBD/VBN con-already in the baseline model, and the other of fusions."
"Below is the performance on the test set which is the negation of a feature existing in the of the resulting model when features for disam-baseline model - since for words at the beginning biguating verb forms are added to the model of of a sentence, the preceding tag is the pseudo-tag Section 2."
"The number of VB/VBP confusions NA, and there is a feature looking at the was reduced by 23.1% as compared to the base- to be still considerable room to improve these line."
"The number of VBD/VBN confusions was results, though the attainable accuracy is limited reduced by 12.3%. by the accuracy with which these distinctions are marked in the Penn Treebank (on a quick Overall Accuracy Unknown Word Accuracy informal study, this accuracy seems to be around 96.83% 86.87% 85%)."
The next table shows the final performance on the test set.
"OverallAccuracy Unknown Word Accuracy [ 4 Features[REF_CITE].86% 86.91% As discussed in section 1.3 above, the task[REF_CITE]Accuracyof the final model determining RB/RP/IN tags for words like down, out, up is difficult and in particular examples, For ease of comparison, the accuracies of all there are often no good local syntactic indicators. models on the test and development sets are"
"For instance, in (2), we find the exact same shown in Table 7."
We note that accuracy is lower on the development set.
"This presumably corre- sequence of parts of speech, but (2a) is a particle use of on, while (2b) is a prepositional use."
"Consequently, the accuracy on the rarer RP (particles) category is as low as 41.5% for the Baseline model (cf."
Table 4). (2) a. Kim took on the monster. b. Kim sat on the monster.
"We tried to improve the tagger&apos;s capability to resolve these ambiguities through adding infor-mation on verbs&apos; preferences to take specific words as particles, or adverbs, or prepositions."
"There are verbs that take particles more than others, and particular words like out are much more likely to be used as a particle in the context of some verb than other words ambiguous between these tags."
"We added two different feature templates to capture this information, consisting as usual of a predicate on the history h, and a condition on the tag t."
The first predicate is true if the current word sponds with Charniak&apos;s (2000: 136) observation th[REF_CITE]of the Penn Treebank is easier than some others.
Table 8 shows the different number of feature templates of each kind that have been instantiated for the different models as well as the total number of features each model has.
"It can be seen that the features which help disambiguate verb forms, which look at capital-ization and the first of the feature templates for particles are a very small number as compared to the features of the other kinds."
The improvement in classification accuracy therefore comes at the price of adding very few parameters to the maximum entropy model and does not result in increased model complexity.
"Even when the accuracy figures for corpus-based part-of-speech taggers start to look extremely similar, it is still possible to move performance is often used as a particle, and if there is a verb at levels up."
"The work presented in this paper most 3 positions to the left, which is &quot;known&quot; to explored just a few information sources in have a good chance of taking the current word as addition to the ones usually used for tagging. a particle."
"The verb-particle pairs that are known While progress is slow, because each new feature by the system to be very common were collected applies only to a limited range of cases, through analysis of the training data in a nevertheless the improvement in accuracy as preprocessing stage. compared to previous results is noticeable, The second feature template has the form: particularly for the individual decisions on which The last verb is v and the current word is w and w we focused. has been tagged as a particle and the current tag"
The potential of maximum entropy methods is t. The last verb is the pseudo-symbol NA if has not previously been fully exploited for the there is no verb in the previous three positions. task of assignment of parts of speech.
These features were some help in reducing porated into a maximum entropy-based tagger the RB/IN/RP confusions.
"The accuracy on the more linguistically sophisticated features, which RP category rose to 44.3%."
"Although the overall are non-local and do not look just at particular confusions in this class were reduced, some of the positions in the text."
"We also added features that errors were increased, for example, the number of model the interactions of previously employed INs classified as RBs rose slightly."
There seems predictors.
All of these changes led to modest increases in tagging accuracy.
This paper has thus presented some initial experiments in improving tagger accuracy through using additional information sources.
In the future we hope to explore automatically discovering information sources that can be profitably incorporated into maximum entropy part-of-speech prediction.
This paper proposes a new error-driven HMM-based text chunk tagger with context-dependent lexicon.
"Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry."
"Moreover, an error-driven learning approach is"
Text chunking is to divide sentences into non-overlapping segments on the basis of fairly superficial analysis.
Text chunking typically relies on fairly simple and efficient processing algorithms.
"Recently, many researchers have looked at text chunking in two different ways: Some researchers have applied rule-based methods, combining lexical data with finite state or other rule constraints, while others have worked on inducing statistical models either directly from the words and/or from automatically assigned part-of-speech classes."
"On the statistics-based approaches,[REF_CITE]proposed a HMM-based approach to recognise the syntactic structures of limited length."
"Buchholz,[REF_CITE], and[REF_CITE]explored memory-based learning method to fred labelled chunks."
"On the rule-based approaches,[REF_CITE]used some heuristics and a grammar to extract &quot;terminology noun phrases&quot; from French text."
"In this paper, we will focus on statistics-based methods."
"The structure of this paper is as follows: In section 1, we will briefly describe the new error-driven HMM-based chunk tagger with context-dependent lexicon in principle."
"In section 2, a baseline system which only includes the current part-of-speech in the lexicon is given."
"In section 3, several extended systems with different context-dependent lexicons are described."
"In section 4, an error=driven learning method is used to decrease memory requirement of the lexicon by keeping only positive lexical entries and make it possible to further improve"
Tin and the given token sequence G~. By the accuracy by merging different context-assuming that the mutual information between dependent lexicons into one after automatic analysis of the chunking errors.
"Finally, the G~ and T1~ is equal to the summation of mutual conclusion is given. information between G~ and the individual tag"
"The data used for all our experiments is ti(l_&lt;i_&lt;n) : extracted from the PENN&quot; WSJ Treebank n[REF_CITE]by the program provided log P(TI&quot;&apos;G?) = ~ log P(t,, G~) by Sabine Buchholz from Tilbug University e(Tln)."
P(G? ) .
We use sections 00-19 as the training data and 20-24 as test data.
"Therefore, the performance is or on large scale task instead of small scale task on"
"MI(T~~, G~ n ) = ~ MI(t,, G? ) , i=l"
"For evaluation of our results, we use the precision and recall measures."
Precision is the we have: percentage of predicted chunks that are actually correct while the recall is the percentage of log P(T~n IG~) correct chunks that are actually found.
"For convenient comparisons of only one value, we = log P(T1n ) + ~, log P(ti&apos; G? )_ also list the F~=I value[REF_CITE]: P(ti)."
"P(G?) (/32 + 1). precision, recall , with/3 = 1. /32. precision + recall 1 HMM-based Chunk Tagger rl n = log P(T1~) - Z log P(t, ) + ~ log P(t, [G?) i=1 i=1"
The first item of above equation can be solved by using chain rules.
"Normally, each tag is assumed to be probabilistic dependent on the"
The idea of using statistics for chunking goes
N-1 previous tags.
"Here, backoff bigram(N=2) back[REF_CITE], who used corpus model is used."
The second item is the frequencies to determine the boundaries of summation of log probabilities of all the tags. simple non-recursive noun phrases.
Skut and Both the first item and second item correspond[REF_CITE]modified Church&apos;s approach in a to the language model component of the tagger way permitting efficient and reliable recognition while the third item corresponds to the lexicon of structures of limited depth and encoded the component of the tagger.
Ideally the third item structure in such a way that it can be recognised can be estimated by using the forward-backward by a Viterbi tagger.
This makes the process run algorithm[REF_CITE]recursively for the in time linear to the length of the input string. first-order[REF_CITE]or second-order HMMs[REF_CITE].
Our approach follows Skut and Brants&apos; way several approximations on it will be attempted by employing HMM-based tagging method to later in this paper instead.
The stochastic model the chunking process. optimal tag sequence can be found by
"Given a token sequence G~ =g~g2 &quot;&quot;g,, maxmizing the above equation over all the possible tag sequences."
"This is implemented by the goal is to fred a stochastic optimal tag theViterbialgorithm. sequence Tin = tlt2...t n which maximizes log P(T~&quot; IOf ) : e(:q&quot;,G?)"
The main difference between our tagger and other standard taggers lies in our tagger has a context-dependent lexicon while others use a context-independent lexicon. log P(Tin [G? ) = logP(Tin)+ log P(Tin)&quot;P(G? )
"For chunk tagger, we haveg1= piwi where"
The second item in the above equation is the W~n = w~w2---wn is the word-sequence and mutual information between the tag sequence
P~ = PiP2 &quot;&quot; P~ part-of-speech is the sequence.
"Here, we use structural tags to representing chunking(bracketing and labelling) structure."
The basic idea of representing the structural tags is similar[REF_CITE]and the structural tag consists of three parts: 1) Structural relation.
The basic idea is simple: structures of limited depth are encoded using a finite number of flags.
"Given a sequence of input tokens(here, the word and part-of-speech pairs), we consider the structural relation between the previous input token and the current one."
"For the recognition of chunks, it is sufficient to distinguish the following four different structural relations which uniquely identify the sub-structures of depth l(Skut and Brants used seven different structural relations to identify the sub-structures of depth 2). 00 the current input token and the previous one have the same parent 90 one ancestor of the current input token and the previous input token have the same parent 09 the current input token and one ancestor of the previous input token have the same parent 99 one ancestor of the current input token and one ancestor of the previous input token have the same parent"
"For example, in the following chunk tagged sentence(NULL represents the beginning and end of the sentence):"
NULL [NP He/PRP] [VP reckons/VBZ] [ NP the/DT current/JJ account/NN deficit/NN] [VP will/MD narrow/VB] [PP to/TO] [NP only/RB #/# 1.8/CD billion/CD] [PP in/IN] [NP September/NNP] [O ./.]
NULL the corresponding structural relations between two adjacent input tokens are: 90(NULL He/PRP) 99(He/PRP reckons/VBZ) 99(reckons/VBZ the/DT) 00(the/DT current/JJ) 00(current/
JJ account/NN) 00(account/
NN deficit/NN) 99(deficit/NN will/MD) 00(will/MD narrow/VB) 99(narrow/VB to/TO) 99(to /TO only/RB) O0(only/RB #/#) 00(#/# 1.8/CD) 00(1.8/CD billion/CD) 99(billion/CD in/IN) 99(in/IN september/NNP) 99(september/NNP ./.) 09(./. NULL)
"Compared with the B-Chunk and I-Chunk used[REF_CITE], structural relations 99 and 90 correspond to B-Chunk which represents the first word of the chunk, and structural relations 00 and 09 correspond to I-Chunk which represnts each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence. 2)Phrase category."
This is used to identify the phrase categories of input tokens. 3)Part-of-speech.
"Because of the limited number of structural relations and phrase categories, the part-of-speech is added into the structural tag to represent more accurate models."
"For the above chunk tagged sentence, the structural tags for all the corresponding input tokens are: 90 PRt~NP(He/PRP) 99_VBZ_VP(reckons/VBZ) 99 DT NP(the/DT)"
"O0 JJ NP(currentJJJ) 00_N/&apos;~NP(account/NN) 00 N1NNP(deficiffNN) 99_MDSVP(will/MD) 00 VB_VP(narrow/VB) 99_TO PP(to/TO) 99_RB~,IP(only/RB) oo_# NP(#/#) 00 CD_NP(1.8/CD) 0(~CD~qP(billion/CD) 99_IN PP(in/IN) 99~lNP~,lP(september/NNP) 99_._0(./.) 2 The Baseline System"
"As the baseline system, we assume P(ti"
P(ti I pi ).
"That is to say, only the current part-of-speech is used as a lexical entry to determine the current structural chunk tag."
"Here, we define: • • is the list of lexical entries in the chunking lexicon, • In this case, the current part-of-speech and[@[ is the number of lexical entries(the size of the chunking lexicon) • C is the training data."
"For the baseline system, we have : • word pair is also used as a lexical entry to determine the current structural chunk tag and we have a total of about 49563 lexical entries([ • ]=49563)."
"Actually, the lexicon used here can be regarded as context-independent. @={pi,p~3C}, where Pi is a part-of-"
The reason we discuss it in this section is to speech existing in the tra]Lningdata C • distinguish it from the context-independent lexicon used in the baseline system.
Table 2 ]@ [=48 (the number of part-of-speech tags give an overview of the results of the chunking in the training data). experiments on the test data.
Table 1 gives an overview of the results of Type [Precision Recall Fa~.l the chunking experiments.
"For convenience,[REF_CITE].32 92.18 9i.24 precision, recall and F#_1 values are given[REF_CITE].75 92.14 91.44 seperately for the chunk types NP, VP, ADJP,[REF_CITE].88 92.78 91.82"
ADVP and PP.
Type Precision Recall Fa__~
"Table 2 shows that incorporation of current word information improves the overall F~=~ value by 2.9%(especially for the ADJP, ADVP and PP chunks), compared with Table 1 of the baseline system which only uses current part-of-speech information."
This result suggests that current word information plays a very important
"In the last section, we only use current part-of- role in determining the current chunk tag. speech as a lexical entry."
"In this section, we will attempt to add more contextual information to 3.2 Context of previous part-of-speech and current part-of-speech approximate P(ti/G~)."
"This can be done by adding lexical entries with more contextual Here, we assume : information into the lexicon ~."
"In the following, we will discuss five context- P(ti / G~) dependent lexicons which consider different I P(ti / pi-lPi )"
Pi-lPi E contextual information. = [ P(ti I Pi)
Pi-!Pi ~ ~ 3.1 Context of current part-of-speech and where current word
"Here, we assume: e(t i I G~) ="
"I P(ti I p~wi) piwi ~ dp [ P(tl I Pi) PiWi ~ dp where ={Pi-l Pi, P~-1Pi3C} +{Pi, pi3C} and Pi-lPi is a pair of previous part-of-speech and current part-of-speech existing in the training data C."
"In this case, the previous part-of-speech and current part-of-speech pair is also used as a lexical entry to determine the current structural ~={piwi,piwi3C}+{pi,pi3C} and piwi is a chunk tag and we have a total of about 1411 part-of-speech and word pair existing in the lexical entries(l~]=1411)."
Table 3 give an training data C. overview of the results of the chunking experiments.
"Compared with Table 1 of the baseline system, Table 3 shows that additional contextual information of previous part-of-speech improves the overall F/~_~ value by 0.5%."
"Especially, F/3_~ value for VP improves by 1.25%, which indicates that previous part-of-speech information has a important role in determining the chunk type VP."
Table 3 also shows that the recall rate for chunk type ADJP decrease by 3.7%.
It indicates that additional previous part-of-speech information makes ADJP chunks easier to merge with neibghbouring chunks.
"Here, we assume :"
"P(t, / G~) IP(ti / pi_lwi_lpi) pi_lwi_lpl~ dp"
"I [ P(ti [ Pi ) Pi-lWi-IPi ~ ~ where ={Pi-i wi-l Pi, Pi-l wi-I Pi3 C} +{Pi, Pi 3 C }, where pi_lwi_lp~ is a triple pattern existing in the training corpus."
"In this case, the previous part-of-speech, previous word and current part-of-speech triple is also used as a lexical entry to determine the current structural chunk tag and }• 1=136164."
Table 4 gives the results of the chunking experiments.
"Compared with Table 1 of the baseline system, Table 4 shows that additional 136116 new lexical entries of format Pi-lw~-lPi improves the overall F#=l value by 3.3%."
"Compared with Table 3 of the extended system 2.2 which uses previous part-of-speech and current part-of-speech as a lexical entry, Table 4 shows that additional contextual information of previous word improves the overall Fa=1 value by 2.8%."
"Here, we assume :"
P(ti I G~) IP(tt I Pi-i PiWi)
"Pi-Ipiwi E dp [ P(ti / Pi ) Pi-IPiWi ~ 1I) where ={Pi-lPiWi, Pi-lP~W~3C} +{Pi, Pi3C}, where pi_lpiw~ is a triple pattern existing in the training and ]• [=131416."
Table 5 gives the results of the chunking experiments.
Type Precision Recall F/3=1
"Table 5: Results of chunking experiments with the lexical entry list : ={Pi-lPiWi, P,-iP,w,3C} +{pi , Pi3C}"
"Compared with Table 2 of the extended system which uses current part-of-speech and current word as a lexical entry, Table 5 shows that additional contextual information of previous part-of-speech improves the overall Fa=1 value by 1.8%."
"previous word, current part-of-speech and current word"
"Here, the context of previous part-of-speech, current part-of-speech and current word is used as a lexical entry to determine the current"
"Here, we will examine the effectiveness of lexical entries to reduce the size of lexicon and make it possible to further improve the chunking accuracy by merging several context-dependent lexicons in a single lexicon."
F~ (ei) is measured by the reduction in error which results from adding the lexical entry to the lexicon :
F~ (e i) =
"F: rr°r(e i ) -- ~ - o E + rr Ao or (e,)."
"Here, F,~r~°r(el) is the chunking error number of the lexical entry ei for the old lexicon r~ Error / x and r~,+~ tei) is the chunking error number of the lexical entry ei for the new lexicon + AO where e~e A~ (A~ is the list of new lexical entries added to the old lexicon ~ )."
"Compared with Table 2 of the extended If Fo (ei ) &gt; 0, we define the lexical entry ei as system which uses current part-of-speech and positive for lexicon ~."
"Otherwise, the lexical current word as a lexical entry, Table 6 shows that additional contextual information entry ei is negative for lexicon ~. of previous part-of-speech improves the overall Tables 7 and 8 give an overview of the"
"Ft3=l value by 1.8%. 3.6 Conclusion effectiveness distributions for different lexicons applied in the extended systems, compared with the lexicon appfied in the baseline system, on the test data and the training data, respectively."
Above experiments shows that adding more contextual information into lexicon significantly Tables 7 and 8 show that only a minority of improves the chunking accuracy.
"However, this lexical entries are positive."
This indicates that improvement is gained at the expense of a very discarding non-positive lexical entries will large lexicon and we fred it difficult to merge all largely decrease the lexicon memory the above context-dependentlexicons in a single requirement while keeping the chunking lexicon to further improve the chunking accurracy. accurracy because of memory limitation.
"In order to reduce the size of lexicon effectively, Context Positive Negative Total an error-driven learning approach is adopted to 1800 314 49515 examine the effectiveness of lexical entries and 209 1363136 make it possible to further improve the 2876 229 136116 chunking accuracy by merging all the above 2895 193 131368 context-dependent lexicons in a single lexicon 98441. 4083"
This will be discussed in the next section.
Table 7 : The effectiveness of lexical entries on the test data .....
Tables 9-13 give the performances of the five error-driven systems which discard all the non-positive lexical enrties on the training data.
"Here, ~&apos; is the lexicon used in the baseline system, dP&apos;={pi,pi3C } and"
A ~ = ~ - ~ &apos; .
It is found that Ffl_~ values of error driven systems for context of current part-of-speech and word pak and for context of previous part-of-speech and current part-of-speech increase by 1.2% and 0.6%.
"Although F~=1 values for other three cases slightly decrease by 0.02%, 0.02% and 0.19%, the sizes of lexicons have been greatly reduced by 85% to 97%."
Type i Precision Recall Fa=l
"After discussing the five context-dependent lexicons separately, now we explore the merging of context-dependent lexicons by assuming :"
"CI~.~{Pi-lWi-I PiWi, Pi-lWi-I PiwigC &amp; Fa,.(pi-lwi-t piwi ) &gt; 0} +{Pi-IPiW~,Pi-l piwi ~C &amp; Fa&quot;(Pi-l piwi ) &gt; O} + {Pi-lWi-I Pi&quot; Pi-lWi-1Pi3C &amp; F~. (pi_lWi_l Pi ) &gt; 0} +{Pi-1Pi, Pi-I Pii~C &amp; F~,(Pi-l Pi )&gt; O} +{piw~, Piw~3C &amp; F~,.(PiWi) &gt; 0} + {Pi, p~3C} and P(ti /G~) is approximatl~ by the following For comparison with other chunk taggers, order : 1. if Pi_lWi_iPiWi E fI~, by assuming MI(Tqn,G~)= 2 M l ( t , , G f ) . i=1"
For the relationship between the training
"Moreover, an error-driven learning approach is corpus size and error driven learning performance,[REF_CITE]shows that the adopted to drease the memeory requirement and performance of error-driven learning improves further improve the accuracy by including more stably when the training corpus size increases."
Training Sections I~ I
Accuracy i FB 1 0-1 14384 context-dependent information into lexicon.
It is found that our new chunk tagger 94.78% 91.95 singnificantly outperforms other reported chunk 0-3 24507 95.19% i 92.51 taggers on the same training data and test data.
This paper presents a novel nonlocal lmlguage model which utilizes contextual information.
A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors.
"The sum of word co-occurrence vectors represents tile context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the ]ong-distmlce lexical de-pendencies."
Experiments on the Mainichi Newspaper corpus show significant improve-ment in perplexity (5.070 overall and 27.2% on target vocabulary)
Human pattern recognition rarely handles iso-lated or independent objects.
"We recog-nize objects in various spatiotemporal circum-stances such as an object in a scene, a word in an uttermlce."
"These circumstances work as conditions, eliminating ambiguities and en-abling robust recognition."
The most challeng-ing topics in machine pattern recognition are in what representation and to what extent those circumstances are utilized.
"In laalguage processing, a context--that is; a portion of the utterance or the text before the object--is ml important circumstmlce."
"One way of representing a context is statis-tical language nmdels which provide a word sequence probability, P(w~), where w~ de-notes the sequence wi...wj."
"In other words, they provide the conditional probability of a word given with the previous word sequence, P( wilw~-l ), which shows the prediction of a word in a given context."
The most conmmn laalguage models used nowadays are N-granl models based on a (N- 1)-th order Markov process: event pre-dictions depend on at most (N- 1) previous events.
"Therefore, they offer the following ap- proximation:"
P(w.ilw -1) wiJwi_N+l) (I)
A common value for N is 2 (bigram language model) or 3 (trigram language model); only a short local context of one or two words is considered.
Even such a local context is effective in some cases.
"For example, in Japanese, after the word kokumu &apos;state affairs&apos;, words such as daijin &apos;minister&apos; mad shou &apos;department&apos; likely follow; kaijin &apos;monster&apos; and shou &apos;priZe&apos; do not."
"After dake de &apos;only at&apos;, you cml often find wa (topic-marker), but you hardly find ga (nominative-marker) or wo (accusative-marker)."
These examples show behaviors of compound nouns and function word sequences are well handled by bigram mad trigraan mod-els.
"These models are exploited in several ap-plications such as speech recognition, optical character recognition and nmrphological anal-ysis."
"Local language models, however, cannot predict nmch in some cases."
"For instance, the word probability distribution after de wa &apos;at (topic-marker)&apos; is very flat."
"However, even if the probability distribution is flat in local lan-guage models, the probability of daijin &apos;min-ister&apos; and kaijin &apos;monster&apos; must be very differ-ent in documents concenfing politics."
"Bigram and trigram models are obviously powerless to such kind of nonlocal, long-distmlce lexical dependencies."
This paper presents a nonlocal language model.
The important information concern-ing long-distance lexical dependencies is the word co-occurrenceinformation.
"For example, words such as politics, govermnent, admin-istration, department, tend to co-occur with daijin &apos;minister&apos;."
"It is easy to measure co-occurrences of word pairs from a training cor-pus, but utilizing them as a representation of context is the problem."
We present a vector
D1 D2 D3 D4 D~ D6 D7 Ds
Wl 1 0 1 0 1 0 1 0
W2 1 O 1 1 0 0 0 0 w3 0 1 0 0 1 1 10 w4 1 1 1 0 0 0 0 0 w5 0 0 0
O 1 0 1 0 w6 0 0 0 0 1 0 0 1
Wl W2 w3 W4 w5 w 6 Wl 4 2 1 2 2 1 w2 3 2 000 w3 4 1 1 2 w4 3 0 0 w5 2 1 w6 2
"As such a matrix reduction, we utilized a representation of word co-occurrence informa- learning method developed by HNC Software tion; and show that the context can be repre-[REF_CITE]. 1 sented as a sum of word co-occurrence vectors in a docmnent and it is incorporated in a non- [Footnote_1]."
"1The goal of HNC was the enhancement of text In the reduction of a matrix, angles of two retrieval. The reduced word vectors were regarded as row-vectors in the original matrLx should be semantic representation of words and used to represent maintained in the reduced matrLx. documents and queries."
Not the word-docmnent co-occurrence local language model.
Word co-occurrences are directly represented in a matrix whose rows correspond to words and whose columns correspond to documents (e.g. a newspaper article).
The element of the matrix is 1 if the word of the row ap-pears in the document of the colunm (Figure 1).
Wre call such a matrix a word-document co-occurrence matrix.
The row-vectors of a word-document co-occurrence matrix represent the co-occurrence information of words.
"If two words tend to ap-pear in the same documents, that is: tend to co-occur, their row-vectors are similar, that is, they point in sinfilar directions."
"The more document is considered, the more reliable and realistic the co-occurrence infor-mation will be."
"Then, the row size of a word-document co-occurrence matrix may become very large."
"Since enormous amounts of online text are available these days, row size can be-come more than a million documents."
"Then, it is not practical to use a word-docmnent co- 2. occurrence matrix as it is."
It is necessary to reduce row size and to simulate the tendency in the original matrix by a reduced matrix.
"The aim of a word-document co-occurrence matrix is to measure co-occurrence of two words by the angle of the two row-vectors. matrix is constructed from tile learning corpus, but a word-word co-occurrence matrix."
"In this matrix: the rows and colunms correspond to words and the i-th diagonal element denotes the number of documents in which the word wl ap-pears, F(wi)."
"The i:j-th element denotes the number of documents in which both words w,: and wj appear, F(wi, wj) (Fig-ure 2)."
"The importmlt information in a word-document co-occurrence matrix is the co-sine of the angle of the row-vector of wi and that of wj, which can be calculated by the word-word co-occurrence matrix as follows:"
"F(w,:, wj) (2)"
"This is because x/F(wi) corresponds to the magnitude of the row-vector of wl, and F(wl, wi) corresponds to the dot product of the row-vector of wl and that of wj in the word-docmnent co-occurrence matrix."
"Given a reduced row size, a matrix is ini-tialized as follows: matrix elements are chosen from a normal distribution ran-domly, then each row-vector is normal-ized to magnitude 1.0."
"The random refit row-vector of the word wl is denoted as ,WCiRand."
Random unit row-vectors in high di-mensional floating point spaces have a property that is referred to a &quot;qnasi-orthogonality&apos;.
"That is; the expected ~¢alue of the dot product between an3&quot; pair of random row-vectors, wciRand and wet and, is approximately equal to zero (i.e. all vectors are approximately or-thogonal). 3."
"The trained row-vector, wai is calculated as follows:"
WCi -~ ~13C~and + &quot;q~ O&apos;ij&apos;T.ll4and
J (3) wc - (4) aij corresponds to the degree of the co-occurrence of two words.
"By adding wc~ and to wet a&apos;d depending on aij, th.e learning formula (3) achieves that two words that, tend to co-occur will have trained vectors that point in shnilar di-rections, r/is a design parameter chosen to optimize performance."
The formula (4) is to normalize vectors to magnitude 1.0.
We call the trained row-vector we/of the word wi a word co-occurrence vector.
The background of the above method is a stochastic gradient descent procedure for min-imizing the cost function: 1 J = ~ .~(aij -- we/&quot; wcj) 2 (5) %3 subject to the constraints [[we/I[ = 1.
The procedure iterates the following calcu-lation:
OJ wen e~&apos; = wcl - q Owe/ = + rl (a j - we~. wcj)wc (6) new -- W C7e~: ilwcF wl I (7)
"The learning method by HNC is a rather simple approximation of the procedure, doing just one step of it."
Note that wci.wcj is approximately zero for the initialized random vectors.
The next question is how to represent the context of a document based on word co-occurrence vectors.
We propose a simple model which represents the context as the sum of the word co-occurrence vectors associated with content words ill a document so far.
It should be noted that the vector is normalized to unit length.
V~recall the resulting vector a context co-occurrence vector.
W&apos;ord co-occurrence vectors have the prop-erty that words which tend to co-occur have vectors that. point in similar directions.
Con-text co-occurrence vectors are expected to have the sinfilar property.
"That is, if a word tends to appear in a given context, the word co-occurrence vector of the word and the con-text co-occurrence vector of the context will point in similar directions ......"
"Such a context co-occurrence vector can be seen to predict the occurrence of words in a given context, mad is utilized as a component 4.2 of statistical language modeling, as shown in the next section."
Language Modeling using Context Co-occurrence Probability
Context co-occurrence probabilities can ham 4 Language Modeling using dle long-distance lexical dependencies while a Context Co-occurrence standard trigram model can handle local con-Vector texts more clearly: in this way they comple- 4.1 Context Co-occurrence ment each other.
"Therefore, language model- Probability ing of their linear interpolation is employed."
"The dot product of a context co-occurrence Note that tile linear interpolation of unigram, vector and a word co-occurrence vector shows bigram and trigram models is simply referred the degree of affinity of the context m:d the to &apos;trigxan: model&apos; in this paper. word."
"The probability of a content word based The proposed language model, called a con-on such dot products, called a context co- text language model, computes probabilities occurrence probability, can be calculated as as follows: shown in Figure 4."
"Since context co- occurrence probabilities are considered only for content words (Cc), probabilities are cal- Pc(wilw~_lcc) = culated separately for content words (Co) andf(cc~-1 &quot;~cl) ~wjEcc f(cc~-1&quot; ~vcj) function words (C/). (S) P(Cc[w~-1) denotes the probability that a where cc~-1 denotes the context co-occurrence content word follows w~-:, which is approx-vector of the left context, Wl ... wi-1, and Cc imated by a trigrmn nmdel."
P(.wi[w~-lcc) denotes a content word class.
"Pc(wilw~-lcc) denotes the probability that wi follows w~-: metals the conditional probability of wi given given that a content word follows w~-:, which that a content word follows wj-:. is a linear interpolation of a standard trigram One choice for the function .f(x) is the iden-model and the context co-occurrence proba-tity."
"However, a linear contribution of dot bilities. products to the probability results in poorer estimates, since the differences of dot prod- In the case of a function word, since the ucts of related words (tend to co-occur) and context co-occurrence probability is not con-unrelated words are not so large."
"Experiments sidered, P(wdw~-lCi) is just a standard tri-showed that x2 or x3 is a better estimate. granl model."
"An example of context co-occurrence prob- X&apos;s adapt using an EM re-estimation proce-abilities is shown in Figure 3. dure on the held-out data. wagayonoharu wo ~a~ shire iru. [shoukenl &apos;prosperity&apos; &apos;enjoy&apos; &apos;do&apos; &apos;stock&apos; halite ka o saiko lko shi ] &apos;enter&apos; &apos;past&apos; &apos;maximum&apos; &apos;profit&apos; &apos;renew&apos; ni I.tsuzuki] kyushin . mata ]kab.uka] & apos;continue&apos; &apos;rapid increase&apos; kaisha, ~h~ ginkou wa 1996 nen ni &apos;company&apos; &apos;investment&apos; &apos;bank&apos; &apos;year&apos; &apos;96 ne, Ik b shiki l so.ha &apos;95 &apos;year&apos; &apos;stock&apos; &apos;market&apos; &apos;year&apos; kyushin wo ni ~u~ no & apos;stock price&apos; &apos;rapidly increase&apos; &apos;background&apos; &apos;corporation&apos ; Ishinkabul hakkou ga ~ saikou to natta. &apos;new stock&apos; &apos;issue&apos; &apos;past&apos; &apos;maximum&apos; &apos;become&apos;"
"By using the Mainichi Newspaper corpus (from 1991 to 1997, 440,000 articles), test set perplexities of a standard trigrmn/bigram model and the proposed context language model are compared."
"The articles of six years were used for the leanfing of word co-occurrence vectors, unigrams, bigrmns and trigrams; the articles of half a year were used as a held-out data for EM re-estimation of A&apos;s; the remaining articles (half a year) for com-puting test set perplexities."
"Word co-occurrence vectors were computed for the top 50,000 frequent content words (ex-cluding pronouns, numerals, temporal nouns, mad light verbs) in the corpus, and unigrmn: bigrmn and trigrmn were computed for the top 60,000 frequent words."
The upper part of Table 1 shows thecom-parison results of the stmldard trigram model and the context language model.
"For the best parameters (marked by *), the overall per-plexity decreased 5.0% and the perplexity on target vocabulary (50,000 content words) de-creased 27.270 relative to the standard trigram model."
"For the best parameters, A&apos;s were adapted as follows:"
"A1 = 0.08, A2 = 0.50, A3 = 0.42"
"Acl = 0.03, ~c2 = 0.50, Xc3 = 0.30, Xcc = 0.17"
"Afl = 0.06, ~f2 = 0.57, Af3 = 0.37"
"As for parazneter settings, note that per-formance is decreased by using shorter word co-occurrence vector size."
The vaxiation of ~/does not change the performance so much. f(x) = x 2 and f(x) = x3 are alnmst the same; better thaaa f(x) = x.
The lower part of Table 1 shows the compar-ison results of the standard bigram model and 2. the context language model.
"Here, the context language model is based on the bigrana model, that is, the terms concerning trigrmn in Fig-ure 4 were eliminated."
"The result was similar, but the perplexity decreased a bit more; 5.7% overall and 28.9% on target vocabulary."
"Figure 5 shows a test article in which the probabilities of content words by the trigram common in the Latent Semaaltic Analysis (Deerwester et ai.; 1990), and context co-occurrence probabilities were computed for all words, and the degree of combination of context co-occurrence probabilities and N-gram probabilities was computed for each word, depending on its distribution over the set of docu-lnents."
"As for the first point, we utilized the lnodel aald the context model are compared."
"If computationally-light, iteration-based proce-that by the context model is bigger (i.e. the dure."
"One reason for this is that the com-context model predicts better), the word is putational cost of SVD is very high when boxed; if not, the word is underlined. millions or more documents are processed."
"The figure shows that the context model Furthermore, considering an extension of our usually performs better after a function word, nmdel with a cognitive viewpoint, we believe where the trigram model usually has little pre-an iteration-based model seems more reason-diction."
"On the other hand, the trigram model able than an algebraic model such as SVD. performs better after a content word (i.e. in As for the second point, we doubt the ap-a compound noun) because a clear prediction propriateness to use the word&apos;s distribution by the trigram model is reduced by paying as a measure of combination of two models. attention to the relatively vague context co-"
What we need to do is to distinguish words occurrence probability (Acc is 0.17). to which semantics should be considered and The proposed model is a constant interpo-other words.
We judged the distinction of con-lation of a trigram model and the context co-tent words and function words is good enough .0ccurrence probabilities.
"More adaptive inter-for that purpose, and developed their trigram-polation depending on the N-gram probabil-based distinction as shown in Figure 4. ity distribution may improve the performance."
"Several topic-based models have been pro- 5 posed based on the observation that certainRelated Work words tend to have different probability dis- Cache language models (Kuhn mad de Mori, tributions in different topics."
"While the basic idea of our model is similar to trigger models, they handle co-occurrences of word pairs independently t (9) where t denotes a topic id. Topics are and do not use a representation of the whole obtained by hierarchical clustering from a context."
"This omission is also done in ap-training corpus, and a topic-specific language plications such as word sense dismnbiguation model, Pt, is learned from the clustered docu- (Yarowsky: 1994;[REF_CITE]). ments."
"Reductions in perplexity relative to a Our model is the most related to Coccaro bigrmn model were 10.5% for the entire text mad[REF_CITE], in that a reduced vec-and 33.5% for the target vocabulary. tor space approach was taken and context is represented by the accumulation of word co- Topic-based models capture long-distance occurrence vectors."
"Their model was reported lexical dependencies via intermediate topics. to decrease the test set perplexity by 12%, In other words, the estimated distribution of compared to the bigram nmdel."
"The major topics, P(t]w~), is the representation of a con-differences are: text."
"Our model does not use such interme-diate topics, but accesses word cg-occurrence 1."
SVD (Singular Value Decomposition) information directly aald represents a context was used to reduce the matrix which is as the accumulation of this information.
In this paper we described a novel language model of incorporating long-distance lexical dependencies based on context co-occurrence vectors.
Reduced vector representation of word co-occurrences enables rather simple but effective representation of the context.
Sig-nificant reductions in perplexity are obtained relative to a staaldard trigram model: both on the entire text. (5.0~) and on the target vo-cabulary (27.2%).
"The bigram language models are popular, in much language processing applications, in both Indo-European and Asian languages."
"However, when the language model for Chinese is applied in a novel domain, the accuracy is reduced significantly, from 96% to 78% in our evaluation."
"We apply pattern recognition techniques (i.e. Bayesian, decision tree and neural network classifiers) to discover language model errors."
We have examined 2 general types of features: model-based and language-specific features.
"In our evaluation, Bayesian classifiers produce the best recall performance of 80% but the precision is low (60%)."
Neural network produced good recall (75%) and precision (80%) but both Bayesian and Neural network have low skip ratio (65%).
The decision tree classifier produced the best precision (81%) and skip ratio (76%) but its recall is the lowest (73%).
"Language models are important post-processing modules to improve recognition accuracy of a wide variety of input, namely speech recogniti[REF_CITE], handwritten recogniti[REF_CITE]and printed character recogniti[REF_CITE], for many human languages."
They can also be used for text correcti[REF_CITE]and part-of-speech tagging.
"For Indo-European languages, the word-bigram language model is used in speech recogniti[REF_CITE]and handwriting recogniti[REF_CITE]."
Various ways to improve language models were reported.
"First, the model has been extended with longer dependencies (e.g. trigram)[REF_CITE]and using non-contiguous dependencies, like trigger pairs[REF_CITE]or long distance n-gram language models[REF_CITE]."
"For better probability estimation, the model was extended to work with (hidden) word classes ([REF_CITE])."
"A more error-driven approach is the use of hybrid language models, in which some detection mechanism (e.g. perplexity measures [Keene and O&apos;[REF_CITE]] or topic detection [[REF_CITE]]) selects or combines with a more appropriate language model."
"For Asian languages (e.g. Chinese, Japanese and Korean) represented by ideographic characters, language models are widely used in computer entry because these Asian languages have a large set of characters (in thousands) that the conventional keyboard is not designed for."
"Apart from using speech and handwriting recognition for computer entry, language models for Asian languages can be used for sentence-based keyboard input (e.g.[REF_CITE]), as well as detecting improper writing (e.g. dialect-specific words or expressions)."
"Unlike Indo-European languages, words in these Asian languages are not delimited by space and conventional approximate string matching techniques[REF_CITE]in handwriting recognition are seldom used in Asian language models."
"Instead, a widely used and reported Asian language model is the character-bigram language model[REF_CITE]because it (1) achieved high recognition accuracy (around 90-96%) (2) is easy to estimate model parameters (3) can be processed quickly and (4) is relatively easy to implement."
Improvement of these language models for Indo-European languages can be applied for the Asian languages but words need to be identified.
"For Asian languages, the model was integrated with syntactic rules (Chien,[REF_CITE])."
Class based language model[REF_CITE]was also examined but the classes are based on semantically related words.
"A-new approach[REF_CITE]is reported using segments expressed by prefix and suffix trees but language model selection (Keene and O&apos;Kane,the comparison is based on perplexity measures, 1996) can be applied to those area to find a more which may not correlate well with recognition appropriate language model because usually improvement[REF_CITE]. topic-dependent words are those causing errors."
"While attempts to improve the;(bigram) language Another (integrative) approach improves the models were (quite) successful, the high language model accuracy using more recognition accuracy (about 96%) is not adequate sophisticated recognizers, instead of a for professional data entry services, which complementary language model."
"The more typically require an error rate lower than 1 in sophisticated recognizer may give a set of 1,000."
"As part of the quality control exercises, different results that the bigram language model these services estimate their error rate by can re-apply on or this recognizer simply gives sampling, and they identify and correct the errors the recognized character."
This integrates well with manually to achieve the required quality.
"Faced the coarse-fine recognition architecture proposed with a large volume of text, the ability automatically identify where the errors are to[REF_CITE]back in the 1960s."
Coarse is recognition provides the candidates for the perhaps more important than automatically language model to select.
"Fine, expensive correcting errors, in post-editing because ( )1 recognition is carried out only where the language manual correction is more reliable than automatic models failed."
"Finally, it is possible to combine all correction, (2) manual error sampling can carried out and (3) more manual efforts be the different approaches (i.e. adaptive, hybrid and are integrative). required in error identification than correction due Given the significance in detecting errors of to the large volume of text."
"For example, if the language models, there is little work in this area. identification of errors is 97% and there are no Perhaps, it was considered that these errors were errors in error correction, then the accuracy of the language model is improved from 96% to 99.9 after error correction. random and therefore hard to detect."
"However, % users can detect errors quickly."
We suspect that some of these errors may be systematic due to the
"In typical applications, the accuracy of the bigram properties of the language model used or due to language model may not be as high as those language specific properties. reported in the literature because the data may be We adopt a pattern recognition app~&apos;~z,ch to in a different genre than that of the training data. detecting errors of the bigram language rnoaei for For evaluation, we tested a bigram language model with text from a novel domain and the Chinese language."
"Each output is assigned to its either the class of correct output or the class of accuracy dropped significantly from 96% to 78%, errors."
The assignment of a class to an output is which is similar to English[REF_CITE]. based on a set of features.
"We explore a number Improvement in the robustness of the bigram language model across different genre of features to detect errors, which are classified is into model-based features and language-specific necessary and several approaches are available, features. based on detecting errors of the language model."
The proposed approach can work with Indo-One (adaptive) approach is to automatically European languages at the word-bigram level. identify the errors and manually correcting them.
"However, language-specific features have to be The information about the correction of errors is discovered for the particular language."
"In addition, used to improve the bigram language model."
"For this approach can be adopted for n-gram language example, the bigram probabilities of the language models."
"In principal, the model-based features can model may be estimated and updated with the corrected data."
"In this way, future occurrences these errors are reduced. of be found or evaluated similar to the bigram language model."
"For example, if the trigram probability (instead of bigram probability) is low,"
Another (hybrid) approach uses another language then the likelihood of a language model error is model to correct the identified errors.
This high. language model can be computationally more This paper is organized as follows.
"Section 1 expensive than the bigram language model because it is applied only to the identified errors. discusses various features and some preliminary evaluation of their suitability for error Also, topic detecti[REF_CITE]and identification."
Section 2 describes 3 types of classifiers used.
"In section 3, our evaluation is reported."
"Finally, we conclude."
We evaluate individual features for error detection because they are important to the success of detection.
Articles from Yazhou Zhoukan (YZZK) magazine (4+ Mbytes)/PH corpus[REF_CITE](7+ Mbytes) are used for evaluation.
We use the recall and precision measurements for evaluation.
The recall is the number of errors identified by a particular feature divided by the total number of errors.
The precision is the number of errors identified by a particular feature divided by the total number of times the feature indicate that there are errors.
"In the first subsection, we describe some model-based features."
"Next, we describe the language-based features."
"In the last subsection, we discuss the combined use of both types of features."
The bigram language model selects the most likely path Pm~ out of a set S.
"The probability of a path s in S is simply the product of the conditional probabilities of one character c, after the other c~.l where s = Co.C+..c:s:, after making the Markov assumption."
"Pm~ = arg max {p(s)} s~S =arg~ax{p(Co)I~IP(cilC,_l)[coC,...c,,== s}"
The set s is generated by the set of candidate characters for each recognition output.
The recognizer may supply the set of candidate characters.
"Alternatively, a coarse recogniser may simply identify the best-matched group or class of characters."
"Then, members of this class are the candidate characters."
"Formally, we use a function h(.), that maps the recognition position to a set of candidate characters, i.e. h(i) = {ciJ. We can also define the set of sentences in terms of h(.), i.e. S = {s Is = cocz... c,, ~, c, ~ h(i)}."
"One feature to detect errors is to count the number of conditional probabilities p(cilc~.l) that are zero, between 2 consecutive positions."
Zero conditional probabilities may be due to insufficient training data or may be because they represent the language properties.
Figure 1 shows the likelihood of an error occurring against the percentage of the conditional probabilities that are zero.
"When there are insufficient data, the conditional probabilities that are small are not reliable."
"IfPm~ have selected some conditional probabilities that are low, then probably there are no other choices from the candidate sets."
"Hence, the insufficient data problem may occur in that particular Pm~."
"In Figure 2, we plot the likelihood of errors identified against the different logarithmic conditional probability values."
"When the recall increases, unfortunately, the precision drops."
"Figure 2: The precision, recall and accuracy (i.e. recall x precision) of detecting language model errors by examining the logarithm conditional probabilities on the maximum likelihood path."
The language-specific features are based on applying the word segmentation algorithm[REF_CITE]to the maximum likelihood path.
"The ROCLING[REF_CITE]word list used for segmentation has 78,000+ entries. length is not equals to 2) have almost 100% recall."
One possible reason why 2 single-character sequences achieved low precision is that there are many spurious bigrams and therefore false match.
We carried out a preliminary study using the features mentioned in subsection 1.1 and 1.2.
"Our Bayesian classifier (Section 2.1) achieved 83% recall but 35% precision, which can be achieved using language specific features only (Fz2)."
"Therefore, we try to combine the use of these sequences of different lengths. speech tags."
"Similar to matched words in the maximum Figure 5 shows that the accuracy of the language likelihood path, the error detection performance of model for these single characters with part-of- speech tags related to exclamations are low."
"For error detection, a feature is assigned to each part-of-speech tag."
The language model accuracy for single characters may depend on the availability of the left and right context to form high probability bigrams.
"Therefore, we expect that language model accuracy of single characters at the beginning (70%) and end (70%) of a sentence is lower than those in the middle (85%) of the sentence."
"The worst case occurs when the sentence has only a single character, where the measured accuracy is only 8.75% (i.e. no bigram context)."
Figure 6 shows that language model output accuracy increases as the bigram probability of single-character sequences of length 2 increases.
"Hence, the bigram probabilities can be used as a feature for detection. 1.2 &quot;~ ................... il ./&lt; . ~ /\!"
Another feature for 2-single-characters sequences is to examine whether the characters in the two candidate sets can form words that match with the dictionary.
These matched words are called hidden words.
"Figure 7 shows that if there are hidden words, the language model accuracy dropped from 60% to 25%."
"Since there are not many cases with 6-8 hidden words, the accuracy for these cases are not reliable."
"For 2 character words, the bigram probability (Figure 8) can be used as a feature similar to the single-character sequences."
The position of these 2 character words in the sentence does not relate to the language model accuracy.
"Our measured accuracy is 91%, 89% and 91% for the beginning, the end and the middle of the sentence, respectively."
Even sentences with a single 2-character word achieved 90% accuracy.
"Hence, there is no need to assign features for the position of the 2 character words in a sentence."
"Similar to 2-single-characters sequences, the language model accuracy (Figure 9) decreases as the humber of hidden words increase in the corresponding 2 sets of candidate characters."
Figure 6: The bigram (logarithm) probability of the single-character sequence of length 2.
"Similar to single characters, the language model accuracy for 2-single-characters sequences at the start, middle and end of a sentence are 48%, 47% and 30%, respectively."
"The accuracy is 33% if the sentence is the 2-single-characters sequence. 0,4 0,3 m 0.2- 0,[Footnote_1] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . those in the hidden layer."
"1 ............. o.7 ~ ............. 0.6 ....................................... ~.---2.. ::-&apos;=-*--. = 0,5 ............."
Likewise nodes in the
One of the problems witlh using individual hidden layer are fully connected to the output features is that the recall and precision are not layer.
"For our application, one input node very high, except the language-specific features."
It corresponds to a feature in section 1.3.
The value is also difficult to set the threshold for detection of the feature is the input value of the node.
"Two because of the precision-recall trade-off. addition, there may be some improvement"
In output nodes indicate whether the current in character is correct or erroneous.
"The number of detection performance if features are combined hidden nodes is 2-4, calculated according to for detection."
"Therefore, we adopt a pattern[REF_CITE]. recognition approach to detect errors."
The output of each node in the MLP is the
"Several classifiers are used to decide for error weighted sum of its input, which is transformed identification because we do not know whether by a sigmoidal function."
"Initially, the weights are particular features work well with particular assigned with small random numbers, which are classifiers, which make different assumptions adjusted by the gradient descend method with about classification."
"Three types of classifers will learning rate 0.05 and momentum 0.1. be examined: Bayesian, decision tree and neural network."
"In the evaluation, the training data is the PH corpus and the test data is the YZZK magazine"
"The Bayesian classifier is simple to implement articles (4+ Mbytes), downloaded from the and is compatible with the model-based features."
"In handwritten character recognition, the Given the feature vector x, the Bayesian detection optimal size of the number of candidates is 6 scheme assigns the correct class wc and the error class we, using the following rule: g~(x) &gt; ge(X) assign wc .Otherwise assign we where go(.) and ge(.) are:[REF_CITE]."
"For robustness, each recognized character in our evaluation is selected from 10 candidates."
"We measured the performance in terms of recall, precision and the manual effort reduction in scanning the text for errors."
"The recall is the number of identified errors over the total number gc(x) = -(x -/~c)r y.c-.(x _/~,)- logl•, I+[Footnote_2]logp(w )c of errors."
2 Classifiers Nodes in the input layer are fully connected with
"The precision is the number of identified g, (x)= -(x-lee) r Z,-~(x-/~,) - log]Z~ ]+2log p(w ), errors over the total number of cases classified as Pc and ,ue are the mean vectors of the class wc and errors."
"The amount of saving in manual scanning we, respectively, ~ and ~ are the covariance for errors is called the skip ratio, which is the matrices of the class wc and we, respectively, and number of blocks classified as correct over the 1-I is the determinant. 2.2 Decision Tree total number of blocks."
The recall and the skip ratio are more important than the precision because post error correction (manual or automatic) can improve the recognition accuracy.
"Originally, we tried to use the support vector It is possible to combine the recall and precision machine (SVM)[REF_CITE]but it could not into one, using the F measures (Van Rijsbergen, converge."
"Instead, we used the decision tree 1979) but the value for rating the relative algorithm C4.5[REF_CITE]."
Decision trees importance is subjective. are known to produce good classification if clusters can be bounded by some hyper-rectilinear Table 1 shows the classification performance of regions.
We trained C4.5 with a set of feature the Bayesian classifier.
"The recall of errors by the vectors, described in Section 1.3."
Bayesian classifier has reduced slightly from 83% using a single classifier to 79% using 3 classifiers but the precision improved from 51% to 60%.
"Also, the skip ratio is 65%, which is much higher"
We use the multi-layer perceptron (MLP) because than the skip ratio of 0.1% if we did not use the it can perform non-linear classification.
The MLP classifier.
"Although the MLP has a higher has 3 layers of nodes: input, hidden and output. precision (80%), its recall is slightly lower than the Bayesian classifier."
The skip ratio of the both Bayesian and MLP classifiers are about the same.
We have evaluated both model-based and language-specific features for detecting language model errors.
"Individual model-based features did not yield good detection accuracy, suffering from the precision-recall trade-off."
The language-specific features detect errors better.
"In particular, matched multi-character words are usually correct."
"If the model-based and language-specific features are aggregated as a single feature vector, the recall and precision of errors are 83% and 35%, respectively, which are the same if we just use language-specific features."
"Hence, instead of a single classifier, we separated 3 situations identified by the language-specific features and 3 classifiers are used to detect these errors individually."
"The Bayesian classifier (simpliest) achieved an overall 79% recall, 60% precision and 65% skip ratio and the MLP achieved an overall 75% recall, 80% precision and a 66% skip ratio."
"Similar recall and precision performances are achieved using decision trees, which are preferred since their skip ratio is higher (i.e. 76%)."
"Although the precision (so far) is not high (60% - 80%), it is not the most important result because (1) this only represents a minor waste of checking effort, compared with scanning the entire text, and (2) the identified errors will be checked further or corrected either manually or automatically."
This paper presents empirical results in cross-lingual information retrieval using English queries to access Chinese documents (TREC-5 and TREC-6) and Spanish documents (TREC-4).
"Since our interest is in languages where resources may be minimal, we use an integrated probabilistic model that requires only a bilingual dictionary as a resource."
We explore how a combined probability model of term translation and retrieval can reduce the effect of translation ambiguity.
"In addition, we estimate an upper bound on performance, if translation ambiguity were a solved problem."
We also measure performance as a function of bilingual dictionary size.
Cross-language information retrieval (CLIR) can serve both those users with a smattering of knowledge of other languages and also those fluent in them.
"For those with limited knowledge of the other language(s), CLIR offers a wide pool of documents, even though the user does not have the skill to prepare a high quality query in the other language(s)."
"Once documents are retrieved, machine translation or human translation, if desired, can make the documents usable."
"For the user who is fluent in two or more languages, even though he/she may be able to formulate good queries in each of the source languages, CLIR relieves the user from having to do so."
Most CLIR studies have been based on a variant of tf-idf; our experiments instead use a hidden Markov model (HMM) to estimate the probability that a document is relevant given the query.
"We integrated two simple estimates of term translation probability into the mono-lingual HMM model, giving an estimate of the probability that a document is relevant given a query in another language."
"In this paper we address the following questions: • How can a combined probability model of term translation and retrieval minimize the effect of translation ambiguity? (Sections 3, 5, 6, 7, and 10) • What is the upper bound performance using bilingual dictionary lookup for term translation? (Section 8) • How much does performance degrade due to omissions from the bilingual dictionary and how does performance vary with size of such a dictionary? (Sections 8-9)"
"All experiments were performed using a common baseline, an HMM-based (mono-lingual) indexing and retrieval engine."
"In order to design controlled experiments for the questions above, the IR system was run without sophisticated query expansion techniques."
Our experiments are based on the Chinese materials of TREC-5 and TREC-6 and the Spanish materials of TREC-4.
"Using Bayes Rule, and the fact that P(Q) is constant for a given query, and our initial assumption of a uniform a priori probability that a document is relevant, ranking collection is the sum of the document documents according to P(Q[D is R) is the same lengths. as ranking them according to P(D is RIQ)."
"The approach therefore estimates the probability that 3 HMM for Cross-lingual IR a query Q is generated, given the document D is relevant. (A glossary of symbols used appears For CLIR we extend the query generation below.)"
We use x to represent the language (e.g.
English) for which retrieval is carried out.
"According to that model of monolingual retrieval, it can be shown that process so that a document Dywritten in language y can generate a query Qxin language x. We use Wxto denote a word in x and Wyto denote a word in y. As before, to model general query words from language x, we estimate P(Wx ]"
Gx)by using a large corpus Cxin language x. p(Q [D is R) =
"I I (aP(W [Gx)+ (1- a)e(w I D)), Also as before, we estimate P(WyIDy) to be the"
W inQ where W&apos;s are query words in Q. Miller et al. estimated probabilities as follows: sample distribution of Wy in Dy.
We use P(Wx[Wy)to denote the probability that
Wy is translated as Wx.
Though terms often * The transition probability a is 0.7 using the should not be translated independent of their
"EM algorithm[REF_CITE]on the TREC4 context, we make that simplifying assumption ad-hoc query set. number of occurrences of W • e0e IGx)= length of Cx here."
We assume that the possible translations are specified by a bilingual lexicon BL.
"Since in Cx the event spaces for Wy&apos;s in P(WyIDy) are mutually exclusive, we can compute the output which is the general language probability for probability P(WxIDy): word Win language x. number of occurrences of W • e(WlD) = length of D"
"In principle, any large corpus Cxthat is representative of language x can be used in in D"
P(WxIDy)= ~ P ( WylDy ) P ( WxIWy )
W i n B L y
We compute P(Q~IDyis R) as below: computing the general language probabilities.
P(Qx IDr /sR) =
"I~I(aetwx IG,)+O-a)P(W~ IDy))"
"In practice, the collection to be searched is used for that purpose."
The length of a
Q a query
QX English query
D a document
Dr a document in foreign language y
DisR document is relevant
W a word
Gx an English corpus
Cx a corpus in language x
Wx an English word Wy a word in foreign language y
BL a bilingual dictionary
"A Glossary of Notation used in Formulas w.~,o."
"The above model generates queries from documents, that is, it attempts to determine how likely a particular query is given a relevant document."
"The retrieval system, however, can use either query translation or document translation."
"We chose query translation over document translation for its flexibility, since it allowed us to experiment with a new method of estimating the translation probabilities without changing the index structure. 4 Experimental Set-up"
For retrieval using English queries to search
"Chinese documents, we used the TREC5 and"
"TREC6 Chinese data which consists of 164,789 documents from the Xinhua News Agency and People&apos;s Daily, averaging 450 Chinese characters/document."
"Each of the TREC topics has three Chinese fields: title, description and narrative, plus manually translated, English versions of each."
"We corrected some of the English queries that contained errors, such as &quot;Dali Lama&quot; instead of the correct &quot;Dalai Lama&quot; and &quot;Medina&quot; instead of &quot;Medellin.&quot; Stop words and stop phrases were removed."
"We created three versions of Chinese queries and three versions of English queries: short (title only), medium (title and description), and long (all three fields)."
"For retrieval using English queries to search Spanish documents, we used the TREC4 Spanish data, which has 57,868 documents."
We will denote the Chinese data sets as Trec5C and Trec6C and the Spanish data set as Trec4S.
We used a Chinese-English lexicon from the Linguistic Data Consortium (LDC).
We pre-processed the dictionary as follows: [Footnote_1].
"1 Clearly, this is not correct; however, it simplified implementation."
Stem Chinese words via a simple algorithm to remove common suffixes and prefixes. 2. Use the Porter stemmer on English words. 3.
Split English phrases into words.
"If an English phrase is a translation for a Chinese word, each word in the phrase is taken as a separate translation for the Chinese word.~ 4. Estimate the translation probabilities. (We first report results assuming a uniform distribution on a word&apos;s translations."
"If a Chinese word c has n translations el, e2, ...en. each of them will be assigned equal probability, i.e., P(eilc)=l/n.[REF_CITE]supplements this with a corpus-based distribution.) 5. Invert the lexicon to make it an English-Chinese lexicon."
"That is, for each English word e, we associate it with a list of Chinese words cl, c2, ..."
Cm together with non-zero translation probabilities P( elc~).
"The resulting English-Chinese lexicon has 80,000 English words."
"On average, each English word has 2.3 Chinese translations."
"For Spanish, we downloaded a bilingual English-Spanish lexicon from the Internet[URL_CITE]containing around 22,000 English words (16,000 English stems) and processed it similarly."
Each English word has around 1.5 translations on average.
A co-occurrence based stemmer[REF_CITE]was used to stem Spanish words.
One difference from the treatment of Chinese is to include the English word as one of its own translations in addition to its Spanish translations in the lexicon.
"This is useful for translating proper nouns, which often have identical spellings in English and Spanish but are routinely excluded from a lexicon."
"One problem is the segmentation of Chinese text, since Chinese has no spaces between words."
"In these initial experiments, we relied on a simple sub-string matching algorithm to extract words from Chinese text."
"To extract words from a string of Chinese characters, the algorithm examines any sub-string of length 2 or greater and recognizes it as a Chinese word if it is in a predefined dictionary (the LDC lexicon in our case)."
"In addition, any single character which is not part of any recognized Chinese words in the first step is taken as a Chinese word."
Note that this algorithm can extract a compound Chinese word as well as its components.
"For example, the Chinese word for &quot;particle physics&quot; as well as the Chinese words for &quot;particle&quot; and &quot;physics&quot; will be extracted."
This seems desirable because it ensures the retrieval algorithm will match both the compound words as well as their components.
The above algorithm was used in processing Chinese documents and Chinese queries.
"English data from the 2 GB of TREC disks l&amp;2 was used to estimate P(WlG,..ngti~h), the general language probabilities for English words."
The evaluation metric used in this study is the average precision using the trec_eval program[REF_CITE].
"Mono-lingual retrieval results (using the Chinese and Spanish queries) provided our baseline, with the HMM retrieval system[REF_CITE]."
"Table 2 reports average precision for mono-lingual retrieval, average precision for cross-lingual, and the relative performance ratio of cross-lingual retrieval to mono-lingual. substitution&quot;, i.e., replacing a query term with all its translations and treating the translated query as a bag of words in mono-lingual retrieval."
"Suppose we have a simple query Q=(a, b), the translations for a are al, a2, a3, and the translations for b are bl, b2."
"Relative performance of cross-lingual IR varies query would be (at, a2, a3, b~, b2)."
"Since all terms between 67% and 84% of mono-lingual IR. are treated as equal in the translated query, this"
Trec6 Chinese queries have a somewhat higher gives terms with more translations (potentially relative performancethan Trec5 Chinese queries.
Longer queries have higher relative performancethan short queries in general.
"Overall, cross-lingual performance using our the more common terms) more credit in retrieval, even though such terms should potentially be given less credit if they are more common."
"Also, a document matching different"
HMM retrieval model is around 76% of mono- translations of one term in the original query lingual retrieval.
A comparison of our mono-lingual results with Trec5 Chinese and Trec6 Chinese results published in the TREC proceedings ([REF_CITE]1998) shows that our mono-lingual results are close to the top performers in the TREC conferences.
Our Spanish mono-lingual performance is also comparable to the top automatic runs of the TREC4 Spanish task[REF_CITE].
Since these mono-lingual results were obtained without using may be ranked higher than a document that matches translations of different terms in the original query.
"That is, a document that contains terms at, a2 and a3 may be ranked higher than a document which contains terms at and bl."
"However, the second document is more likely to be relevant since correct translations of the query terms are more likely to co-occur[REF_CITE]."
"A second method is to structure the translated query, separating the translations for one term sophisticated query processing techniques such from translations for other terms."
"This approach as query expansion, we believe the mono-lingual limits how much credit the retrieval algorithm results form a valid baseline."
Query sets Mono- Cross- % of lingual can give to a single term in the original query and prevents the translations of one or a few lingual Mono- terms from swamping the whole query.
There lingual are several variations of such a method Trec5C-short 0.2830 0.1889 ([REF_CITE]%
Trec5C-medium 0.3427 0.2449 72% Trec5C-long 0.3750 0.2735 73% Trec6C-short 0.3423 0.2617 77% Trec6C-medium 0.4606 0.3872 84% Trec6C-long 0.5104 0.4206 82% Trec4S 0.2252 0.1729 77% 1997).
One such method is to treat different translations of the same term as synonyms.
"Ballesteros, for example, used the INQUERY[REF_CITE]synonym operator to group translations of different query terms."
"However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other."
"By contrast, our"
HMM approach supports translation
Table 2: Comparing mono-lingual and cross- probabilities.
The synonym approach is lingual retrieval performance.
The scores on equivalent to changing all non-zero translation the monolingual and cross-lingual columns are probabilities P(W~[ Wy)&apos;sto 1 in our retrieyal average precision.
In this section we compare our approach with function.
Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations. two other approaches.
One approach is &quot;simple
These intuitions are supported empirically by the results in Table 3.
We can see that the HMM performs best for every query set.
Simple substitution performs worst.
"The synonym approach is significantly better than substitution, but is consistently worse than the HMM"
Substi- Synonym HMM tution
Trec5C-long 0.0391 0.2306 0.2735 Trec6C-long 0.0941 0.3842 0.4206 Trec4S 0.0935 0.1594 0.1729
"To get an upper bound on performance of any disambiguation technique, we manually disambiguated the Trec5C-medium, Trec6C-medium and Trec4S queries."
"That is, for each English query term, a native Chinese or Spanish speaker scanned the list of translations in the bilingual lexicon and kept one translation deemed to be the best for the English term and discarded the rest."
"If none of the translations was correct, the first one was chosen."
"The results in Table 4 show that manual disambiguation improves performance by 17% on Trec5C, 4% on Trec4S, but not at all on Trec6C."
"Furthermore, the improvement on Trec5C appears to be caused by big improvements for a small number of queries."
The one-sided t-test[REF_CITE]at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant.
It seems surprising that disambiguation does not help at all for Trec6C. We found that many terms have more than one valid translation.
"For example, the word &quot;flood&quot; (as in &quot;flood control&quot;) has 4 valid Chinese translations."
Using all of them achieves the desirable effect of query expansion.
"It appears that for Trec6C, the benefit of disambiguation is cancelled by choosing only one of several alternatives, discarding those other good translations."
"If multiple correct translations were kept in disambiguation, the improvement would be 4% for Trec6C-medium."
The results of this manual disambiguation suggest that there are limits to automatic disambiguation.
Query sets Degree of Disambiguation None Manual % of Mono-lingual
Trec5C-medium 0.2449 0.2873 84% (+17%)
Trec6C-medium 0.3872 0.3830 83% (-1%)
Trec4S 0.1729 0.1799 80% (+4%)
Results in the previous section showed that manual disambiguation can bring performance of cross-lingual IR to around 82% of mono-lingual IR.
"The remaining performance gap between mono-lingual and cross-lingual IR is likely to be caused by the incompleteness of the bilingual lexicon used for query translation, i.e., missing translations for some query terms."
This may be a more serious problem for cross-lingual IR than ambiguity.
"To test the conjecture, for each English query term, a native speaker in Chinese or Spanish manually checked whether the bilingual lexicon contains a correct translation for the term in the context of the query."
"If it does not, a correct translation for the term was added to the lexicon."
"For the query sets Trec5C-medium and Trec6C-medium, there are 100 query terms for which the lexicon does not have a correct translation."
"For the query set Trec4S, the percentage is 12%."
"The results in Table 5 show that with augmented lexicons, performance of cross-lingual[REF_CITE]%, 99% and 95% of mono-lingual IR on Trec5C-mediurn, Trec6C-medium and Trec4S."
The improvement over using the original lexicon lexicon than longer queries.
"Using a 7,000-word is 28%, 18% and 23% respectively."
"The results lexicon, the short queries only achieve 75% of demonstrate the importance cffa complete their performance with the full lexicon."
"Compared with the results in section 7, comparison, the medium-length queries achieve the results here suggest that missing translations 87% of their performance. have a much larger impact on cross-lingual IR than translation ambiguity does."
Figure 1 Impact of lexicon size on cross-lingual IR we keep only the n most frequent English words. performance The upper graph in Figure 1 shows the curve of cross-lingual IR performance as a function of the size of the lexicon based on the Chinese short and medium-lengthqueries.
Retrieval performance was averaged over Trec5C and
"We categorized the missing terms and found that most of them are proper nouns (especially locations and person names), highly technical"
"Trec6C. Initially retrieval performance increases terms, or numbers."
Such words understandably sharply with lexicon size.
"After the dictionary do not normally appear in traditional lexicons. exceeds 20,000, performance levels off."
An Translation of numbers can be solved using examination of the translated queries shows that simple rules.
"Transliteration, a technique that words not appearing in the 20,000-word lexicon guesses the likely translations of a word based usually do not appear in the larger lexicons either."
"Thus, increases in the general lexicon beyond 20,000 words did not result in a on pronunciation, can be readily used in translating proper nouns."
Another technique is automatic discovery of substantial increase in the coverage of the query translations from parallel or non-parallel corpora terms.
The lower graph in Figure 1 plots the retrieval[REF_CITE].
"Since traditional lexicons are more or less static repositories of performance as a function of the percent of the knowledge, techniques that discover translation full lexicon."
The figure shows that short queries from newly published materials can supplement are more susceptible to incompleteness of the them with corpus-specific vocabularies.
In this section we estimate translation probabilities from a parallel corpus rather than assuming uniform likelihood as in section 4.
"A Hong Kong News corpus obtained from the Linguistic Data Consortium has 9,769 news stories in Chinese with English translations."
It has 3.4 million English words.
"Since the documents are not exact translations of each other, occasionally having extra or missing sentences, we used document-level co-occurrence to estimate translation probabilities."
The Chinese documents were &quot;segmented&quot; using the technique discussed in section 4.
"Let co(e,c) be the number of parallel documents where an English word e and a Chinese word c co-occur, and df(c) be the document frequency of c. If a Chinese word c has n possible translations el to en in the bilingual lexicon, we estimate the corpus translation probability as: co(e i , c)"
"P_ corpus(ell c) = i=n MAX(df(c), ~ co(e i, c)) i=1"
"Since several translations for c may co-occur in a document, ~co(e~ c) can be greater than df(c)."
Using the maximum of the two ensures that
"Instead of relying solely on corpus-based estimates from a small parallel corpus, we employ a mixture model as follows:"
P(e Ic) = ~ P _ corpus(e Ic) +(1- #)P_ lexicon(e [c) The retrieval results in Table 6 show that combining the probability estimates from the lexicon and the parallel corpus does improve retrieval performance.
The best results are obtained when 13=0.7;this is better than using uniform probabilities by 9% on Trec5C-medium and 4% on Trec6C-medium.
"Using the corpus probability estimates alone results in a significant drop in performance, the parallel corpus is not large enough nor diverse enough for reliable estimation of the translation probabilities."
"In fact, many words do not appear in the corpus at all."
"With a larger and better parallel corpus, more weight should be given to the probability estimates from the corpus."
Trec5 - Trec6-medium medium
P_lexicon 0.2449 0.3872 13=0.3 0.39800.2557 13=0.5 0.2605 0.4021 13=0.7 0.2658 0.4035
P_corpus 0.2293 0.2971
Other studies which view IR as a query generation process include[REF_CITE].
Our work has focused on cross-lingual retrieval.
Many approaches to cross-lingual IR have been published.
One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents to the language of the queries[REF_CITE].
"For most languages, there are no MT systems at all."
"Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived."
"Another common approach is term translation, e.g., via a bilingual lexicon.[REF_CITE]."
"While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable."
Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de[REF_CITE].
The third approach to cross-lingual retrieval is to • Cross-lingual IR performance is typically document in another language.
Experiments cross-language information retrieval.&quot; Proceedings using the TREC5 and TREC6 Chinese test sets of the 20th ACM SIGIR International Conference and the TREC4 Spanish test set show the following: • Our retrieval model can reduce the on Research and Development[REF_CITE]pp. 84-91.
L. Ballesteros and W.B.[REF_CITE]. &quot;Resolving ambiguity for cross-language retrieval.&quot; performance degradation due to translation Proceedings of the 21st ACM SIGIR Conference on ambiguity This had been a major limiting Research and Development in Information factor for other query-translation approaches. • Some earlier studies suggested that query translation is not an effective approach to cross-lingual IR[REF_CITE]. resource available.
Conference[REF_CITE]. • Manual selection from the translations in the M. Davis and W.[REF_CITE]. &quot;QUILT: Implementing a Large Scale Cross-language Text bilingual dictionary improves performance little over the HMM.
"Retrieval System.&quot; Proceedings of ACM[REF_CITE]. • We believe an algorithm cannot rule out a A. Diekema, F. Oroumchain, P. Sheridan and E. possible translation with absolute confidence; it is more effective to rely on probability estimation/re-estimation to"
"Evaluation of Conceptual Interlingual Document Retrieval (CINDOR) in English and French.&quot; TREC7 Proceedings, NIST special publication. differentiate likely translations and unlikely translations. • Rather than translation ambiguity, a more"
P. Fung and K. Mckeown. &quot;Finding Terminology
This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora.
"Instead of looking up a •bilingual phrase dictionary, the compositional phrase (the translation of phrase can be derived from the translation of its components) in the query can be indirectly translated via a general-purpose Chinese-English dictionary look-up procedure."
A novel selection method for translations of query terms is also presented in detail.
Our query translation method ultimately constructs an English query in which each query term has a weight.
The evaluation results show that the retrieval performance achieved by our query translation method is about 73% of monolingual information retrieval and is about 28% higher than that of simple word-by-word translation way.
"With the rapid growth of electronic documents and the great development of network in China, there are more and more people touching the Intemet, on which, however, English is the most popular language being used."
"It is difficult for most people in China to use English fluently, so they would like to use Chinese to express their queries to retrieval the relevant English documents."
This situation motivates research in Cross Language Information Retrieval (CLIR).
"There are two approaches to CLIR, one is query translation; the other is translating original language documents to destination This research was supported by the National Science Fund of China for Distinguished Young Scholars under contact 69983009. language equivalents."
"Obviously, the latter is a very expensive task since there are so many documents in a collection and there is not yet a reliable machine translation system that can be used to process automatically."
Most researchers are inclined to choose the query translation approach [Oard. (1996)].
"Methods for query translation have focused on three areas: the employment of machine translation techniques, dictionary based translation [Hull &amp;[REF_CITE]; Ballesteros &amp;[REF_CITE]], parallel or comparable corpora for generating a translation model [Davis &amp;[REF_CITE]; Sheridan &amp;[REF_CITE]; Nie, Jian-Yun et a1.(1999)]."
"Machine translation (MT) method has many obstacles to prevent its employment into CLIR such as deep syntactic and semantic analysis, user queries consisting of only one or two words, and an arduous task to build a MT system."
Dictionary based query translation is the most popular method because of its easiness to perform.
"The main reasons leading to the great drops in CLIP,.effectiveness by this method are ambiguities caused by more than one translation of a query term and failures to translate phrases during query translation."
Previous studies [Hull &amp;[REF_CITE]; Ballesteros &amp;[REF_CITE]] have shown that automatic word-by-word (WBW) query translation via machine readable dictionary (MKD) results in a 40-60% loss in effectiveness below that of monolingual retrieval.
"With regard to the use of parallel corpora translation method, the critiques one often raises concern the availability of reliable parallel text corpora."
An alternative way is that making use of the comparable corpora because they are easier to be obtained and there are more and more bilingual even multilingual documents on the Internet.
"From analyzing a document collection, an associated word list can be yielded and it is often used to expansion the query in monolingual information retrieval [Qiu &amp;[REF_CITE]; Jing &amp;[REF_CITE]]."
"In this paper, a new query translation is the occurrence probabilities of term tI and t2 presented by combination dictionary based in a sentence."
"These probabilities can be method with the comparable corpora analyzing. calculated by the occurrence of term t~ and t2 Ambiguity problem and phrase information lost in the collection as equation (2), (3) and (4). are attacked in dictionary based Chinese-"
English Cross-Language information Retrieval (CECLIR).
The remainder of this paper is organized as follows: section 1 gives a method to calculate the mutual information matrices of Chinese-English comparable corpora.
Section 2 develops a scheme to select the translations of the Chinese query terms and introduces how the compositional phrase can be kept in our method.
"Replacing (1) with equation (2), (3) and (4), the after being removed the stop words be associated with each other and work together to mutual information of term tI and t2 can be express a query requirement."
"The association expressed by following formula. relationship between two words can be indicated by their mutual information, which can be further used to discover phrases [Church :&amp;[REF_CITE]]."
"If two words are independent n,,."
"N MI(q,t 2) = log 2 &apos;- (5)"
Hh nt 2
"Table 2 and table 3 show the occurrence with each other, their mutual information would frequency values and mutual information values be close to zero."
"On the other hand, if they are calculated by formula (5) for three Chinese strongly related, the mutual information would compositional phrases and their corresponding be much greater than zero and they would be English phrases respectively found in our much like to be a phrase; if they occur comparable corpora. complementarily, the mutual information would be negative."
"In conclusion¢ the bigger the mutual information of word pair, the more t1It2 n,, n,2 n,,,, : MI"
Graphic [interface 92 41 34 10.70 t~ and t~ in a Chinese sentence.
"The reason we Table 3: Mutual information of three English select a Chinese sentence to be a window other than a fixed length window is that a full Chinese sentence can keep more linguistic information phrases (N = 184,000)"
"Anal zing the Chinese-English comparable) and consequently, it is more reasonable that we corpora in this way, we can get two mutual can regard t~ and t2 to be a phrase when they information value matrices to indicate which co-occur in a sentence."
"P(t l) and P(t 2) are two terms (as to the Chinese collection, they are almost Chinese words after segmentation) would be most possible to be a phrase."
A word list associated to each Chinese query term can be obtained by looking up the mutual information value matrix of the Chinese corpus with a cutoff of M1 =1.50.
"As discussed above, the bigger the mutual information value between two terms, the more possible the two words would be a phrase."
We can infer that the associated word list of the query term contains the terms that are the most possible components of a compositional phrase.
"In other words, the phrase information can be kept by this way."
The Chinese query is translated into English via looking up the English senses of Chinese query term and words in its associated word list in a Chinese-English dictionary.
The procedures how to select appropriate tranlations and to construct the English query are discussed in section 2. 2 Translations selection and phrase keeping
It is a naive method to translate a Chinese query only by looking up each Chinese term to get its English senses in a Chinese-English dictionary.
"This method, however, results in too many ambiguities during the query translation and offers no path to select appropriate ones among the translations."
"In addition, phrases in the query can not be translated effectively."
Previous study has showed that failure to translate phrases greatly reduces the performance by up to 25% over automatic word-by-word (WBW) query translation [Ballesteros &amp;[REF_CITE]].
"In our method, those English translations most likely co-occur with each other can be obtained via looking up the mutual information value matrix of the English corpus with a cutoff"
M1 = 1.50.
"In this way, the English senses of terms in the associated word list can provide a good context for the translation of the Chinese query term and give a significant clue for its translations selection."
"In addition, the information of two terms (either Chinese or English) to be a phrase can also be stored in the associated word list."
"In the following, we firstly describe our method to select translations in detail, and then we give an example to demonstrate how to keep the phrase information in our method."
"Supposing the Chinese query is expressed by (e1,e~,.--, e, ). el, e2,...,e, are the segmented Chinese words of the query after removing the stop words."
"The translations of em (m = 1,...,r)by looking up the Chinese-English bilingual dictionary can be ordered in descending by following formula."
"W(fmt ) =lOglO(Ot&apos;i_Ml(f~)+ fl &quot;o_Ml(fmt )) (6) l~&apos;llgmkl&quot; t &quot; &quot; z z i_Mi(fml ) = k=l j=l / I~llrmkl (7) k=l r ]~1 l Z ZMI(f,~,J~ k) o_ MI(fml ) = i=l,i¢m k=l (8) r lYd •i=l,i~m Where f~ is one sense of the English translation set F m of the word e,~ (l = 1,...,IF.b g. is the association word set of em."
The size of E m is le.
"I and its element is e~ (k = 1.... ,le.I)"
"F~ is the English translation set of emk, its element is f,,~. ct is the coefficient to emphasize the inner mutual information between the English sense f t of the single Chinese query term em and the English sense f,~ of the em&apos;s association word emk."
"The first part of the formula (6) i_MI(f~) reflects the probability of English translation f,~ and f,~ to be a phrase. /3 is the coefficient to emphasize the outside mutual information between f,~ and the English sense ~* of the other Chinese terms included in the query."
"The second part of the formula (6) o_ Ml(f~) reflects the relevant value between the English sense f,~of e m and the whole query concept."
"Our method of translations selection can be described as follows: if the weight of any translation of the Chinese query term is greater than 1.00, the sense is selected to construct the English query."
"If there is no weight of any translation of the Chinese query term greater than 1.00, the sense with biggest one is selected to construct the English query."
"In this way, we can make an English query by the following Boolean expression. (manual)&quot; and the associated word list of term translations."
"The forward and backward &quot;&apos;~ ~E(management)&quot; is &quot;~ ~ (user), *J~(harrd maximum matching algorithm is used to disk), ~,Aq-(file)&quot;."
We process the associated segment the texts and find the combinatorial word &quot;&apos;~2~2(management)&quot; of the query term ambiguities.
"Of all the combinatorial &quot;~q~ (user)&quot; in a special way by adding an ambiguities, 91.2% are removed with the word appropriate value to their mutual information uni-gram prior probabilities."
"A stop word list of value to let theirs be the biggest in the 1210 elements is set up, which contains associated word list, because the associated frequently used functional words as well as symbols [Du &amp;[REF_CITE]]."
Our Chinese query word &quot;&apos;~(management)&quot; also occurs in the translation process contains following steps: original query.
Similar way is done with the (1) Segment the Chinese query according to the associated word &quot;~ ~ (user)&quot; of the query term method introduced above. &quot;~ ~ (management)&quot;.
"In this way, , the (2) Get the associated word list of each Chinese compositional phrase &quot; h~ ~ &apos;~ JX (user term included in the query from the Chinese management)&quot; can be kept in both associated mutual information matrix. word list of term &quot;It/&quot; (user)&quot; and term &quot;~X (3) Look up the English sense set of each (management)&quot;."
Chinese term and its associated word in the When term &quot;)~ ~&quot; is translated into English general-purpose Chinese-English bilingual by looking up the general-purpose Chinese- dictionary.
"English bilingual dictionary, we get its English (4) Select the English translation sense by the sense set &quot;user, consumer&quot; ordered by the method introduced in section 2 (in formula formula (6)."
"When term &quot;~&apos;JE&quot; is translated (6) the coefficents tx and fl are selected into English, we get its English sense set by &quot;management, administration, supervision, run&quot; 1.0 and 0.5 respectively in our experiment) and construct the English query ordered by the formula (6)."
We can fred the first on the basis of the formula (9). positions of the English translation set of the The query term &quot;~ P&quot; and term &apos;&quot;~&apos;JX&quot; are &quot;user&quot; document collection used in our experiments consists of several Chinese and and &quot;management&quot; respectively.
"From the point corresponding English computer manuals, of view of translation, the phrase &quot;user which include Linux-HOWTO, PostgreSQL management&quot; can be regarded as the English handbook, Mysql handbook, Linux kernel* and phrase translation of &quot;~ # &apos;~ ~&quot;."
"According[REF_CITE]volumes ([REF_CITE]to our translation selection and formula (9), we can Dec., 1999)&quot;."
"In order get a large number construct the English Boolean query as follows, document Chinese and English collections, we in which each query term has a weight. decomposed these manuals and let every Query = (user, 1.86)and ((management, 1.83) document no more than 15 sentences."
"As a or (administration, 1.63)) and (command, 1.92). *[URL_CITE]**[URL_CITE]result, Chinese-English bilingual comparable corpora are obtained in which contain about 8,200 Chinese documents and 12,500 English documents."
"All work in this study was performed on the[REF_CITE]information retrieval system [Du &amp;[REF_CITE]], which can process both Chinese and English Boolean queries."
Table 4 shows the precision and recall table for the three methods.
The first column in table 4 contains precision values averaged 13 queries and interpolated to eleven recall points from 0.0 to 1.0 in steps of 0.1.
The third column contains precision values achieved by our translation method (QT).
"The results in table 4 suggest that in this case, the WBW query translation leads to a great drop in effectiveness of 42.90% below that for monolingual retrieval (manual translation method)."
"The result of our query translation method greatly improves effectiveness by 28.22% over the WBW method, and its effectiveness is about 73.21% of that for monolingual retrieval."
"Although phrase translation is not executed directly in our method, the phrase information is kept effectively in the associated word list."
"Therefore, the phrase can be well ~anslated."
The associated word list also provides a good context for translation of the Chinese query terms (corresponding to the first part of formula (6) i_Ml(f~t)) and a good English translation is given a relatively high weight.
The results in table 4 show that our query translation method can construct a good English query and indeed improve the effectiveness.
"Automatic word-by-word query translation is an attractive method because it is easy to perform, resources are readily available, and performance is similar to that of other CLIP,. methods."
"However, there are a lot of ambiguities in translation of the query terms and failures to translate phrases correctly, which are mainly responsible for the large drops in effectiveness below monolingual retrieval performance."
"Aiming to tackle with these problems, we develop a new scheme for how to select translations in this paper."
"In addition, rather than using a bilingual phrase dictionary, we also put forward a new method to translate phrases indirectly by using the mutual information between two words in a full sentence and keep the phrase information in the associated word list effectively."
"As a result of our query translation method, an English query is constructed in which each query term has a weight."
"In this study, our method leads to improve the effectiveness by 28.22% over the word by word query translation method, but is still about 27% below the monolingual retrieval performance."
"If query expansion is employed in our method, we expect the performance should be further improved."
A shortcoming of our method is that the cost of calculation of the mutual information matrices is very large.
We are currently exploring an algorithm to generate the matrices more efficiently and the selection of coefficients in formula (6) also needs further research.
The authors wish to express their appreciation to those interpreters of computer manuals.
"Without theft selfless contribution, our experiment would be impossible."
Thanks to the anonymous reviewers for their helpful comments.
"In this paper, a method for the word alignment of English-Chinese corpus based on chunks is proposed."
The chunks of English sentences are identified firstly.
Then the chunk boundaries of Chinese sentences are predicted by the translations of English chunks and heuristic information.
The ambiguities of Chinese chunk boundaries are resolved by the coterminous words in English chunks.
"With the chunk aligned bilingual corpus, a translation relation probability is proposed to align words."
"Finally, we evaluate our system by real corpus and present the experiment results."
"Key Words: Word Alignment, Chunk Alignment, Bilingual Corpus, Lexicon Extraction"
"With the easier access to bilingual corpora, there is a tendency in NLP community to process and refine the bilingual corpora, which can serve as the knowledge base in support of many NLP applications, such as automatic or human-aid translation, multilingual terminology and lexicography, multilingual information retrieval system, etc."
"Different NLP applications need different bilingual corpora, which are aligned at different level."
"They can be divided by the nature of the segment to section level, paragraph level, sentence level, phrase level, word level, byte level, etc."
"As for our applications, we choose the chunk level to do alignment based on following considerations."
"Firstly, our applications, which include an example-based machine translation system, a computer aid translation system and a multilingual information retrieval system, need the alignment below the sentence level, on which we can acquire bilingual word and phrase dictionaries and. other useful translation information."
"Secondly, the word level alignment between English and Chinese language is difficult to deal with."
There are no cognate words.
The change in Chinese word order and word POS always produce many null and mistake correspondences.
"Next, we observe the phenomenon that when we translate the English sentence to Chinese sentence, all the words in one English chunk tend to be translated as one block of Chinese words which are coterminous."
"The word orders within these blocks tend to keep with the English chunk, also."
So there are stronger boundaries between chunks than between words when we translate texts.
"Finally, as we all known, chunk has been assigned syntactic structure[REF_CITE], which comprises a connected sub-graph of the sentence&apos;s parse tree."
So it&apos;s possible to align sentence structure and obtain translation grammars based on chunks by parsing.
Many researchers have studied the text alignment problem and a number of quite encouraging results have been reported to different level alignments.
"With sentence-alignedcorpus ready in hand, we focus our attention on the intra-sentence alignment between the sentence pairs."
"In this paper, a method for the word alignment of English-Chinese corpus based on chunks is proposed."
The chunks of English sentences are identified firstly.
Then the chunk boundaries of Chinese sentences are predicted by the bilingual lexicon and synonymy Chinese dictionary and heuristic information.
The ambiguities of Chinese chunk boundaries are resolved by the coterminous words in English chunks.
"With the chunk aligned bilingual corpus, a translation approaches seem simplify the word alignment relation probability is proposed to align words. problem and can&apos;t obtain much translation"
"Although this paper is related to English-Chinese word alignment, the idea can be used to any other language bilingual corpora."
"In the following sections, we first present a brief review of related work in word alignment."
Then discuss our alignment algorithm based on chunks in detail.
Following this is an analysis of our experimental results.
"Finally, we close our information above word level."
To combine these two approaches in a better way is the direction in near future.
In this paper we proposed a method to align the bilingual corpus base on chunks.
The linguistic knowledge such as POS tag and Chunk tag are used in a simply statistical model.
"There are basically two kinds of approaches on For our procedure in this paper, the bilingual word alignment: the statistical-based approaches corpus has been aligned at the sentence level, (Brown et. al., 1990; Gale &amp;[REF_CITE]; and the English language texts have been tagged Dagan et. al. 1993;[REF_CITE]), and the with POS tag, and the Chinese languagetexts lexicon-based approaches (Ker &amp;[REF_CITE]; have been segmented and tagged with POS tag."
"Wang et. al., 1999)."
We have available a bilingual lexicon which Several translation models based on word lists typical translation for many of the words in alignment are built[REF_CITE]in the corpus.
"We have available a synonymy order to implement the English-French Chinese dictionary, also."
We identify the chunks statistical machine translation.
"The probabilities, of English sentences and then predict the chunk such as translation probability, fertility boundaries of Chinese sentences from the probability, distortion probability, are estimated translation of every English chunks and by EM algorithm."
The Z2 measure is used by heuristic information by use of the bilingual Gale &amp;[REF_CITE]to align partial words. lexicon.
The ambiguities of Chinese chunk[REF_CITE]uses an improved Brown model to boundaries are resolved by the coterminous align the words for texts including OCR noise. words in English chunks.
"After produce the They first align word partially by character word candidate sets by statistical method, we string matching."
Then use the translation model calculate the translation relation probability to align words.
The detail algorithm for word Brown model to align the English-Chinese POS alignment is given in table 1. tagged corpus.
Ker &amp;[REF_CITE]propose an approach to align Chinese English corpus based Step 1: According to the definition of Chunk in on semantic class.
"There are two semantic English, separate the English sentence into classes are used in their model."
"One is the a few chunks and labeled with order semantic class of Longman lexicon of number from left to fight. contemporary English, the other is synonymy Step 2: Try to find the Chinese translation of Chinese dictionary."
The semantic class rules of every English chunk created in step 1 by translation between Chinese and English are bilingual dictionary and synonymyChinese extracted from large-scale training corpus.
"If the Chinese translation is fred, Chinese and English words are aligned by these then label the Chinese words with the same rules."
"His 1. model is based on bilingual lexicon, sense Step 3: Disambiguate the multi-label Chinese similarity and location distortion probability. words by the translation location of coterminous words within the same English The statistical-based approaches need complex chunk. training and are sensitive to training data."
It&apos;s a Step 4: Separate the Chinese sentence into a few pity that almost no linguistic knowledge is used chunks by heuristic information. in these approaches.
The lexicon-based Step 5: Save all the alignment at chunk level in whole corpus as a base for word alignment.
Step 6: Produce the word candidate sets by statistical method.
Step 7: Calculate the translation relation probability between every word and it&apos;s candidate translation words.
Step 8: Select the best translation by comparing the total TRP value in different alignment forms.
Outline ofAlignmentAlgorithm 3.2 Chunk Identifying of English Sentence
"Following[REF_CITE], there are two separate stages in chunking parser, which is the chunker and the attacher."
"The chunker converts a stream of words into a stream of chunks, and the attacher converts the stream of chunks into a stream of sentences."
So only the chunker is needed in this paper.
It&apos;s a non-deterministic version of a LR parser.
"For detail about chunker and the used grammars, please see[REF_CITE]."
Then the chunks in one sentence are labeled with order number from left to right.
"We observe the phenomenon that when we translate the English sentence to Chinese sentence, all the words in one English chunk tend to be translated as one block of Chinese words that are coterminous."
"The word orders within these blocks tend to keep with the English chunk, also."
There are three examples in figure 1.
The first sentence pair is chosen from an example sentence[REF_CITE].
The [The b~aldman] [was sitting] [on his suitcase]. [To a c c e ~ _ ~ _ ~ _ ~ c l second sentence pair is from a computer handbook.
In these sentence pair all English chunks can find the exactly Chinese Chunk.
In the third sentence pair only one English chunk can&apos;t find the exactly Chinese chunk for this sentence is chosen from a story and the translation is not literally.
"In order to find the Chinese translation of every English chunk, we use the bilingual dictionary and synonymy Chinese dictionary to implement the matching."
"If the Chinese translation of any words within the English chunk is found, then label the Chinese word with the same number used for labeling the English chunk."
"If there are Chinese words, which are labeled simultaneously by two or more number of English chunks, we use the number of nearby Chinese words to disambiguate."
"For example, in figure 2, the first Chinese word /~j may be correspondent to the English chunk 5 or 7."
"We have known that the words in one English chunk tend to be translated as one block of Chinese words that are coterminous,"
"So it&apos;s easy to decide the first Chinese word )x~ ffJ is correspondent to the English chunk 7, the second Chinese word )x~~ is correspondent to the English chunk 5."
"By the same way, we can find the correct translations of Chinese word ~ and ~ is English chunk 6 and chunk 8 respectively."
"In Step 4 of figure 2, the Chinese words with the same label number are bracketed with in one chunk."
"Finally, we separate the Chinese sentence into a few chunks by heuristic information based on POS tag (especially the preposition, conjunction, and auxiliary words) and the grammatical knowledge-base of contemporary Chinese (Yu shi wen, 1998). i c k ]"
"Ion &quot;Su2.p..9.~&apos;l. [I gathered] [from what they said] ,[that an elder sister] [of his] [ was coming ] [to stay with them],[ and that s h e ~ ] [ that e v [ ~ &apos;fl&apos; ] ~ qb ] [ ~ ] [ ~ ] [ - ~ ~ ] [ ~ ] [ ~ l&apos; ] ~ -- ~ ] , [ ~ ."
"R ~ ] [ ~ _ h ] [ ~ lJ ] . [This product 1] [is designed 2] for [low-cost 3], [turnkey solutions 4] and [mission-critical applications 5] that [require 6] [a central application host 7] and [ do not require 8] [networking 9]."
"Step 2 Label the translation of English chunk with it&apos;s order number i~(1) ~(I) ~ ~j ~(6/8) --~(7) ~,~,(7) ~(5/7) 5]~01,(7)~ ~(8) ~(6/8) I~ ~(9) {k~(3) ~2~:(3). ~ ~]&apos;~&apos;~ ~(4) ~~(4) (2)°"
"Step 3 Disambiguate the multi-label Chinese words i.~(1) ~:~(1) ~ ~ ~,~(6) --~(7) @,~,(7) ~(7) t~ (3) ~ . (3). ~ ~ 9 ~ : ( 4 ) ~ ~ ~.~&apos;~(5) &apos;f~-~(5/7) ~ ~gJ- 5]E;~R(7) ~ ~(8) ~(8) ~]~(9) (4) 7A Y~&apos;Pi(5) i~- ~ (5) ~ ~2@~9(2),"
"Separate the Chinese sentence into a few chunks ~(3)3, E~ ~ &apos; ~ 9 ~ : ~ ( 4 ) ] ~ [Y~&apos;I~ ~ ~ ( 5 ) ] ~ [~,i-~9(2)]0"
"Probability for Words candidate words co-occur with the Chinese words; N is the total number of chunks in which With the alignments at chunk level of whole the English word co-occur with the Chinese corpus, we propose a Translation Relation word; 13¢e is the penalty value to indicate the Probability (TRP) to implement the word"
POS change between the Engfish word and the alignment.
The translation Relation probability
Chinese word. of words are given by following equation:
By this equation we connect the chunk length P~ - and POS change with the co-occurrenceL:~ (1)
"The less the chunk length, the higher Where f¢ is the frequency of English word in the translation relation probability."
"For example, whole corpus; fc is the frequency of Chinese the chunk pak, which is composed by one Word in whole corpus; f~ is calculated by English word and two Chinese words, is more follow equation: reliable than the chunk pair, which is composed by four English words and four Chinese words."
N /ln( 2Lay ) + ln(Lav)
I L~i +
Lci An example is given in figure 3.
"There are 5 possible alignment forms in our consideration for this chunk, which includes three Engfish (2) words and three Chinese words."
Then calculate Where Lmv is the average words number of all the total TRP value for every possible alignment English chunks and all Chinese chunks which word pairs in each alignment form by equation are related to the English word in whole Corpus; (1).
"After we get the total TRP value for each L~i is the word number of the English chunk in alignment form, we choose the biggest one. which the English candidate words co-occur with the Chinese words; ~ is the word number floppy disk drive floppy disk drive floppy disk drive III"
A CB floppy disk drive floppy disk drive
X1 D E 4.2[REF_CITE].7%.
The accuracy for word alignment based on correctly aligned chunk pairs is 93.6%.
"The We tested our system with an English-Chinese errors mainly due to the following reasons: bilingual corpus, which is part of a computer Chinese segmentation error, stop words noise, handbook (Sco Unix handbook)."
There are POS tag error.
"In table 2, the total TRP values of noisy figures and tables."
Finally we extracted example in figure 3 are showed.
The form D is the best. accuracy for automatic chunk alignment is
Total TRP of C =0.6485 0.3529 X 1/2 0.8333 X 1/2
Total TRP of D =0.8640 0.8947 X 1/2 0.3429 X 1/2
Total TRP of E =0.2576 0.1722 X 1/2 5 Conclusions and Future Work domains without lost much accuracy.
To increase the correct rate of Chinese word
"With the more and more bilingual corpora, there segmentation is important for our word is a tendency in NLP community to process and alignment."
"To extract the corresponding syntax refine the bilingual corpora, which can information of English Chinese bilingual corpusserve as the knowledge base in support of many NLP by shallow parsing is a direction for future work, applications."
"In this paper, a method for the also. word alignment of English-Chinese corpus based on chunks is presented."
"After identified Acknowledgements the chunks of English sentences, we predict the chunk boundaries of Chinese sentences by the This research was. funded by Natural Science bilingual lexicon, synonymy Chinese dictionary Foundation of China (Grant No. 69983009). and heuristic information."
The ambiguities of Chinese chunk boundaries are resolved by the The authors would like to thank the anonymous coterminous words in English chunks.
"After reviewers for their helpful comments. produce the word candidate sets by statistical method, we calculate the translation relation References probability between every word pair and select the best alignment forms."
"We evaluate our Abney,[REF_CITE]."
Parsing by Chunks.
In: Robert system by real corpus and present the results.
"Berwick, Steven Abney and Carol Tenny (eds.), Pringciple-Based Parsing, Kluwer Academic"
"Although the results we got are quite promising Publishers to bilingual English Chinese text, there are still Brown, P. F., Della Pietra, S. A., Della Pietra, V., J., much to do in near future."
"The corpus we use in and Mercer, R. L., 1993."
The Mathematics of Statistical Machine Translation: Parameter our experinaent is a relative small corpus about Estimation.
"Chang, J. S., and Chert, M. H. C. 1994 Using Partial extend our method to the large corpus of other"
An empirical method for estimating term weights directly from relevance judgements is proposed.
The method is designed to make as few assump-tions as possible.
"It is similar to Berkeley&apos;s use of regressi[REF_CITE]where labeled relevance judgements are fit as a linear combination of (transforms of) tf, idf, etc., but avoids potentially troublesome assump-tions by introducing histogram methods."
Terms are grouped into bins.
Weights are computed based on the number of relevant and irrelevant documents associated with each bin.
"The result- • t: a term • d: a document • tf(t, d): term freq = # of instances of t in d • df(t): doc freq = # of docs d with tf(t, d) &gt; 1 • N: # of documents in collection • idf(t): inverse document freq: -log2 d~t) • df(t, tel, tf0): # of relevant documents d with tf(t, d) = tfo • df(t, rel, tfo): # of irrelevant documents d with tf(t, d) = tfo • el(t): expansion frequency = # docs d in query expansion with tf(t, d) &gt; 1 • TF(t): standard notion of frequency in corpus-based NLP: TF(t) = ~d tf(t, d) • B(t): burstiness: B(t) = 1 iff ~ df(t) is large."
"Table 1: Notation ing weights usually lie between 0 and idf, which is a surprise; standard formulas like tf. idf would assign values well outside this range."
The method extends naturally to include ad-ditional factors such as query expansion.
Terms mentioned explicitly in the query receive much larger weights than terms brought in via query expansion.
"In addition, whether or not a term t is mentioned explicitly in the query, if t ap-pears in documents brought in by query expan-sion (el(t) &gt; 1) then t will receive a much larger weight than it would have otherwise (ef(t) = 0)."
"The interactions among these factors, however, are complicated and collection dependent."
It is safer to use histogram methods than to impose unnec-essary and potentially troublesome assumptions such as normality and independence.
"Under the vector space model, the score for a document d and a query q is computed by sum-ming a contribution for each term t over an ap-propriate set of terms, T. T is often limited to terms shared by both the document and the query (minus stop words), though not always (e.g, query expansion). tf and idf."
Terms are assi._~ed to bins based on idf.
The column labeled idf is the mean idf for the terms in each bin.
"Ais estimated separately for each bin and each tf value, based on the labeled relevance judgements."
"Test 4 ~s~-. ~ 2 11 score~(d, q) = E t/(t, d) . idf(t) tET"
"Under the probabilistic retrieval model, docu-ments are scored by summing a similar contribu-tion for each term t. = ~ l P(tJrel)"
"In this work, we use A to refer to term weights. values in previous table."
Most points fall between the dashed lines (lower limit of A = 0 and upper limit of A = idf).
The plotting character denotes tf.
"Note that the line with tf = 4 is above the line with tf = 3, which is above the line with tf = 2, and so on."
The higher lines have larger intercepts and larger slopes than the lower lines.
"That is, when we fit A ,~, a(tf) + b(tf), idf, with"
"This paper will start by showing how to estimate A separate regression coefficients, a(tf) and b(tf), from relevance judgements."
"Three parameteriza-tions will be considered: (1) fit-G, (2) fit-B, which introduces burstiness, and (3) fit-E, which intro-duces expansion frequency."
The evaluation section shows that each model improves on the previous one.
"But in addition to performance, we are also interested in the interpretations of the parameters. 2 Supervised Training The statistical task is to compute A, our best esti-mate of A, based on a training set."
This paper will use supervised methods where the training mate-rials not only include a large number of documents but also a few queries labeled with relevancejudge-ments.
"To make the training task more manageable, it is common practice to map the space of all terms into a lower dimensional feature space."
"In other words, instead of estimating a different Afor each for each value of tf, we find that both a(tf) and b(tf) increase with t]. terms."
"In this way, all of the terms in a bin are assigned the weight, A. The common practice, for example, of assigning tf •idf weights can be interpreted as grouping all terms with the same idf into a bin and assigning them all the same weight, namely tf. idf."
"Cooper and his colleagues at Berkeley[REF_CITE]have been using regression methods to fit as a linear combination of idf, log(tf) and var-ious other features."
This method is also grouping terms into bins based on their features and assign-ing similar weights to terms with similar features.
"In general, term weighting methods that are fit to data are more flexible than weighting methods that are not fit to data."
"We believe this additional flexibility improves precision and recall (table 8). term in the vocabulary,we can model A as a func- Instead of multiple regression, though, we tion of tf and idf and various other featuresof choose a more empirical approach."
"Description (function of term t) 1 df(t, rel,O) _--# tel does d with tf(t,d) = 0 2 dr(t, tel, 1) _= # rel does d with tf(t, d) = 1 3 dr(t, rel, 2) _= # rel does d with tf(t, d) = 2 4 df(t, rel,3) ~ # rel does d with tf(t,d) = 3 5 df(t, rel,4+) ~ # tel does d with tf(t,d) _&gt; 6 dr(t, tel, O) ~ # tel does d with tf(t, d) = 0 7 dr(t, rel, 1) ~_# tel does d with tf(t, d) = 1 8 dr(t, tel, 2) ~ # rel does d with tf(t, d) = 2 9 df(t,~,3) -= #reml does d with t/(t,d) = 3 10 df(t, rel,4+) ~ # tel does d with tf(t,d) _&gt; 11 # tel does d 12 # tel does d 13 freq of term in corpus: TF(t) = ~a tf(t, d) 14 # does d in collection = N 15 dff = # does d with tf(t, d) _&gt;1 where dr(bin, rel, tf) is 1 dr(bin, tel, tf) ~ Ib/=l ~ df(t, rel,tf) tEbin"
"Similarly, the denominator can be approximated as:"
"P(bin, tfl~) ~ log2 dr(bin, tel, t]) where dr(bin, tel, tf) is 1 dff(bin, tel, t/) ~ Ib/nl ~ dff(t, ~, tf) t[REF_CITE]ef = # does d in query exp. with tf(t, d) &gt; 1 ~ret is an estimate of the total number of relevant 21 where: D (description), E (query expansion) 25 burstiness: B sumptions, when appropriate, can be very pow-erful (better estimates from less training data), but errors resulting from inappropriate assump-tions can outweigh the benefits."
In this empirical investigation of term weighting we decided to use conservative non-parametric histogram methods to hedge against the risk of inappropriate para-metric assumptions.
"Terms are assigned to bins based on features such as idf, as illustrated in table 2. (Later we will also use B and/or ef in the binning process.) is computed separately for each bin, based on the use of terms in relevant and irrelevant documents, according to the labeled training material."
"The estimation method starts with a training file which indicates, among other things, the num-ber of relevant and irrelevant documents for each term t in each training query, q. That is, for each t and q, we are are given dr(t, rel, tfo) and dr(t, tel, tfo), where dr(t, tel, tfo) is the number of relevant documents d with tf(t, d) = tfo, and df(t, rel, tfo) is the number of irrelevant docu-ments d with tf(t, d) = tfo."
The schema for the training file is described in table 3.
From these training observations we wish to obtain a mapping from bins to As that can be applied to unseen test material.
"We interpret )~as a log likelihood ratio: ~(bin, t/) = ~, og2-z-P(bin::--, tflrel) ~&apos;[bin, t/IN) where the numerator can be approximated as:"
"P(bin, triter) ,~.~togs_ dr(bin, rel, tf) Nrel documents."
"Since some queries have more rele-vant documents than others, N~t is computed by averaging: 1 tEbin"
"To ensure that Nr~l + ~&quot;~/= N, where N is the number of documents in the collection, we define"
This estimation procedure is implemented with the simple awk program in figure 2.
"The awk pro-gram reads each line of the training file, which con-tains a line for each term in each training query."
"As described in table 3, each training line contains 25 fields."
"The first five fields contain dr(t, tel, tf) for five values of tf, and the next five fields con-tain df(t, rel, tf) for the same five values of tf."
"The next two fields contain N,a and N;-~. As the awk program reads each of these lines from the training file, it assigns each term in each train-ing query to a bin (based on [log2(df)], except when df &lt; 100), and maintains running sums of the first dozen fields which are used for comput-ing dr(bin, rel, tf), df(bin, re&apos;---l,tf), l~rret and I~--~ for five values of tf."
"Finally, after reading all the training material, the program outputs the table of ks shown in table 2."
The table contains a col-umn for each of the five tf values and a row for each of the dozen idf bins.
"Later, we will consider more interesting binning rules that make use of additional statistics such as burstiness and query expansion."
Recall that the task is to apply the ks to new un-seen test data.
One could simply use the ks in table 2 as is.
"That is, when we see a new term in the test material, we find the closest bin in ta-ble 2 and report the corresponding ~ value."
"But since the idf of a term in the test set could easily fall between two bins, it seems preferable to find the two closest bins and interpolate between them."
"We use linear regression to interpolate along the idf dimension, as illustrated in table 4."
Table 4 is a smoothed version of table 2 where
A ~ a + b.idf.
"There are five pairs of coefficients, a and b, one for each value of tf."
Note that interpolation is generally not neces-sary on the tf dimension because tf is highly quantized.
"As long as tf &lt; 4, which it usually is, the closest bin is an exact match."
"Table 5: A comparison of the regression coeffi-cients for method fit-G with comparable coeffi- } cients from the multiple regression: A = a2 + b2 • idf + c2 •log(1 + tf) where a2 ---- -4.1, b2 = 0.66 and c2 = 3.9."
The differences in the two fits are particularly large when tf = 0; note that b(0) is negligible (0.05) and b2 is quite large (0.66).
"Re-ducing the number of parameters from 10 to 3 in this way increases the sum of square errors, which may or may not result in a large degradation in precision and recall."
Why take the chance?
"Table 6 is like tables 4 but the binning rule not only uses idf, but also burstiness (B)."
"Burstiness[REF_CITE](Church, tff &gt; 4, there is very little room for adjustments if we accept the upper limit of A &lt; idf."
"Although we interpolate along the idf dimen-sion, interpolation is not all that important along that dimension either."
"Figure 1 shows that the differences between the test data and the train-ing data dominate the issues that interpolation is 2000) is intended to account for the fact that some very good keywords such as &quot;Kennedy&quot; tend to be mentioned quite a few times in a document or not at all, whereas less good keywords such as &quot;except&quot; tend to be mentioned about the same number of times no matter what the document attempting to deal with."
The main advantage of regression is computational convenience; it is eas-ier to compute a + b. idf than to perform a binary search to find the closest bin.
Previous work[REF_CITE]used mul-tiple regression techniques.
Although our perfor-mance is similar (until we include query expan-sion) we believe that it is safer and easier to treat each value of tf as a separate regression for rea-sons discussed in table 5.
"In so doing, we are ba-sically restricting the regression analysis to such"
Note that the slopes and intercepts are larger when an extent that it is unlikely to do much harm (or B = 1 than when B = 0 (except when tf = 0). much good).
Imposing the limits of 0 &lt; A _&lt; idf also serves the purpose of preventing the regres-sion from wandering too far astray.
"Even though A usually lies between-0 and idf, we restrict A to 0 &lt; A &lt; idf, just to make sure. method fit-E. (The coefficients marked with an asterisk are worrisome because the bins are too small and/or the slopes fall well outside the nor-mal range of 0 to 1.)"
"The slopes rarely exceeded .8 is previous models (fit-G and fit-B), whereas fit-E has more slopes closer to 1."
"The larger slopes are associated with robust conditions, e.g., terms ap-pearing in the query (where = D), the document (tf &gt; 1) and the expansion (el &gt; 1)."
"If a term appears in several documents brought in by query •expansion (el &gt; 2), then the slope can be large even if the term is not explicitly mentioned in the query (where = E)."
"The interactions among tf , idf, ef and where are complicated and not easily captured with a straightforward multiple regres-sion. is about."
"Since &quot;Kennedy&quot; and &quot;except&quot; have similar idf values, they would normally receive similar term weights, which doesn&apos;t seem right."
Table 6 shows how Kwok&apos;s suggestion can be reformulated in our empirical framework.
"The table shows the slopes and intercepts for ten re-gressions, one for each combination of tf and B (B = 1 iff avtf is large."
"That is, B = 1 iff TF(t)/df(t) &gt; 1.83 - 0.048-idf)."
We applied query expansi[REF_CITE]to generate an expanded part of the query.
The original query is referred to as the description (D) and the new part is referred to as the expansion (E). (Queries also contain a narrative (N) part that is not used in the experiments below so that our results could be compared to previously published results.)
The expansion is formed by applying a base-line query engine (fit-B model) to the description part of the query.
"Terms that appear in the top k = 10 retrieved documents are assigned to the E portion of the query (where(t) = E), unless they were previously assigned to some other portion of the query (e.g., where(t) = D)."
"All terms, t, no matter where they appear in the query, also re-ceive an expansion frequency el, an integer from 0 to k = 10 indicating how many of the top k documents contain t. The fit-E model is: A = a(tf, where, ef) + b(tf ,where, el) •idf , where the regression coeffi-cients, a and b, not only depend on tf as in fit-G, but also depend on where the term appears in the query and expansion frequency el."
"We consider 5 values of tf, 2 values of where (D and E) and 6 values of ef (0, 1, 2, 3, 4 or more). 32 of these 60 pairs of coefficients are shown in table 7."
"As before, most of the slopes are between 0 and 1. is usually between 0 and idf, but we restrict A to 0 &lt; A &lt; idf, just to make sure."
"In tables 4-7, the slopes usually lie between 0 and 1."
"In the previous models, fit-B and fit-G, the largest slopes were about 0.8, whereas in fit- E, the slope can be much closer to 1."
"The larger slopes are associated with very robust conditions, e.g., terms mentioned explicitly in all three areas of interest: (1) the query (where = D), (2) the doc-ument (tf &gt; 1) and (3) the expansion (el &gt; 1)."
"Under such robust conditions, we would expect to find very little shrinking (downweighting to com-pensate for uncertainty)."
"On the other hand, when the term is not men-tioned in one of these areas, there can be quite a bit of shrinking."
Table 7 shows that the slopes are generally much smaller when the term is not in the query (where = E) or when the term is not in the expansion (el = 0).
"However, there are some exceptions."
The bottom right corner of ta-ble 7 contains some large slopes even though these terms are not mentioned explicitly in the query (where = E).
The mitigating factor in this case is the large el.
"If a term is mentioned in several documents in the expansion (el _&gt; 2), then it is not as essential that it be mentioned explicitly in the query."
"With this model, as with fit-G and fit-B, ~ tends to increase monotonically with tf and idf, though there are some interesting exceptions."
"When the term appears in the query (where = D) but not in the expansion (el = 0), the slopes are quite small (e.g., b(3,D,0) = 0.11), and the slopes actu-ally decrease as tf increases (b(2,D, 0) = 0.83 &gt; b(3,D,0) = 0.11)."
"We normally expect to see slopes of .7 or more when t.f &gt; 3, but in this case (b(3, D, 0) = 0.11), there is a considerable shrink-ing because we very much expected to see the term in the expansion and we didn&apos;t. ...."
"As we have seen, the interactions among t f, idf, ef and where are complicated and probably de- • B: restrict terms to bursty (B -- 1) terms"
"Table 8: Training helps: methods above the line • Ek: require terms to appear in more than k use training (with the possible exception of JCB1); docs brought in by query expansion (el(t) &gt; methods below the line do not. k). pend on many factors such as language, collection, Table 9: Filters: results vary somewhat depending typical query patterns and so on."
"To cope with on these choices, though not too much, which is such complications, we believe that it is safer to fortunate, since since we don&apos;t understand stop use histogram methods than to try to account for lists very well. all of these interactions at once in a single multiple regression."
"The next section will show that fit-E filter trained on sys. 11 R has very encouraging performance. 5 Experiments 2+, E1 tf,where,ef fit-E .354 .363 2+, E2 tf,where,ef fit-E .350 .359 2+, E4 tf,where,ef fit-E .333 .341"
"Two measures of performance are reported: (1) 11 2+ tf,where,ef fit-E .332 .366 point average precision and (2) R, precision after .360 .351NA"
"NA JCB1 retrieving Nrd documents, where Nrd is the num-ber of relevant documents."
"We used the &quot;short query&quot; condition of the NACSIS NTCIR-1[REF_CITE]: The best filters (Ek) improve the per- Collecti[REF_CITE]which consists of formance of the best method (fit-E) to nearly the about 300,000 documents in Japanese, plus about level of JCB1. 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements the K filter is slightly unhelpful. for testing."
"The result of &quot;short query&quot; is shown in A number of filters have been considered (ta-page 25[REF_CITE], which shows that ble 9)."
"Results vary somewhat depending on these &quot;short query&quot; is hard for statistical methods. choices, though not too much, which is fortunate,"
Two previously published systems are included since since we don&apos;t understand stop lists very in the tables below: JCB1 and BKJJBIDS.
"To the extent that there is a pattern, we sus- submitted by Just System, a company with a com-pect that words axe slightly better than bigrams, mercially successful product for Japanese word-and that the E filter is slightly better than the B processing, produced the best results using sophis-filter which is slightly better than the K filter."
Ta-ticated (and proprietary) natural language pro-ble 10 shows that the best filters (Ek) improve the cessing techniques.[REF_CITE]BKJJBIDS used performance of the best method (fit-E) to nearly Berkeley&apos;s logistic regression methods (with about the level of JCB1. half a dozen variables) to fit term weights to the labeled training material. filter sys.
UL LL II R Table 8 shows that training often helps.
The methods above the line (with the possible excep- 2 .283 .293fit-B + + tion of JCB1) use training; the methods below the 2 fit-B + .280 .296 -line do not.
"Fit-E has very respectable perfor- 2 +fit-B - .280 .296 mance, nearly up to the level of JCB1, not bad for 2 fit-B - - .275 .288 a purely statistical method."
The performance of fit-B is close to that 2 + .266 .279fit-G + of 2 ÷ .251 .268fit-G -
"For comparison sake, fit-B is shown 2 .248 .259fit-G + -both with and without the K filter."
The K filter 2 - .232 .249fit-G -restricts terms to sequences of Katakana and Kanji characters.
BKJJBIDS uses a similar heuristic[REF_CITE]: Limits do no harm: two limits are eliminate Japanese function words.
"Although the slightly better than one, and oneis slightly bet-"
"K filter does not change performance very much, ter than none. (UL = upper limit of ~ &lt; idf; LL the use of this filter changes the relative order of fit-B and BKJJBIDS."
These results suggest that = lower limit of 0 _&lt;~)
The final experiment (table 11) shows that re-stricting ~ to 0 &lt; ~ &lt; id] improves performance slightly.
The combination of both the upper limit and the lower limit is slightly better than just one limit which is better than none.
We view limits as a robustness device.
"Hopefully, they won&apos;t have to do much but every once in a while they prevent the system from wandering far astray."
"This paper introduced an empirical histogram-based supervised learning method for estimating term weights, ~. Terms are assigned to bins based on features such as inverse document frequency, burstiness and expansion frequency."
A different is estimated for each bin and each tf by counting the number of relevant and irrelevant documents associated with the bin and tff value.
"Regression techniques are used to interpolate between bins, but care is taken so that the regression cannot do too much harm (or too much good)."
"Three varia-tions were considered: fit-G, fit-B and fit-E. The performance of query expansion (fit-E) is particu-larly encouraging."
"Using simple purely statistical methods, fit-E is nearly comparable to JCB1, a sophisticated natural language processing system developed by Just System, a leader in the Japanese word processing industry. .-:"
"In addition to performance, we are also inter-ested in the interpretation of the weights."
Empiri-cal weights tend to lie between 0 and idf.
We find these limits to be a surprise given that standard term weighting formulas such as tf. idf generally do not conform to these limits.
"In addition, we find that ~ generally grows linearly with idf, and that the slope is between 0 and 1."
We interpret the slope as a statistical shrink.
"The larger slopes are associated with very robust conditions, e.g., terms mentioned explicitly in all three areas of interest: (1) the query (where = D), (2) the document (tf _&gt;1) and (3) the expansion (ef &gt; 1)."
"There is generally more shrinking for terms brought in by query expansion (where = E), but if a term is mentioned in several documents in the expan-sion (el &gt; 2), then it is not as essential that the term be mentioned explicitly in the query."
"The interactions among t f, id], where, B, el, etc., are complicated, and therefore, we have found it safer and easier to use histogram methods than to try to account for all of the interactions at once in a single multiple regression."
Authors thank Prof. Mitchell P. Marcus of Uni-versity of Pennsylvania for the valuable discussion about noise reduction in context of information retrieval.
This reseach is supported by Sumitomo Electric.
"In this paper, we report results on answering questions for the reading comprehension task, using a machine learning approach."
"We eval-uated our approach on the Remedia data set, a common data set used in several recent pa-pers on the reading comprehension task."
"Our learning approach achieves accuracy competi-tive to previous approaches that rely on hand-crafted, deterministic rules and algorithms."
"To the best of our knowledge, this is the first work that reports that the use of a machine learning approach achieves competitive results on answering questions for reading compre-hension tests."
The advent of the Internet has resulted in a massive information explosion.
We need to have an effective and efficient means of locat-ing just the desired information.
The field of information retrieval (IR) is the traditional discipline that addresses this problem.
"However, most of the prior work in IR deal more with document retrieval rather than &quot;in-formation&quot; retrieval."
This also applies to search engines on the Internet.
Current search engines take a list of input words and return a ranked list of web pages that contain (or not contain) the given words.
It is then left to the user to search through the returned list of web pages for the information that he needs.
"While finding the web pages that contain the desired information is an important first step, what an information seeker needs is often an answer to a question."
"That is, given a ques-tion, we want a system to return the exact answers to the question, and not just the doc-uments to allow us to further search for the"
The need for question answering (QA) sys-tems has prompted the initiation of the ques-tion answering track in TREC-8[REF_CITE]to address this problem.
"In the QA track, each participant is given a list of 200 questions, and the goal is to locate answers to these questions from a document database consisting of hundreds of thousands of documents (about two gigabytes of text)."
"Each participant is to return a ranked list of the five best answer strings for each question, where each answer string is a string of 50 bytes (or 250 bytes) that contains an answer to the question."
"What, when, where, and who ques-tions that have explicit answers given in some document in the database are emphasized, but not why questions."
"In a related but independent effort, a group at MITRE has investigated question answer-ing in the context of the reading comprehen-sion task[REF_CITE]."
"The docu-ments in this task axe 115 children stories at grade two to five from Remedia Publications, and the task involves answering five questions (who, what, when, where, and why question) per story, as a measure of how well a sys-tem has understood the story."
"Each story has an average of 20 sentences, and the question answering task as formulated for a computer program is to select a sentence in the story that answers to a question."
"Conversely, a question can have multiple correct answers, where each of sev-eral individual sentences is a correct answer."
An example story from the Remedia corpus and its five accompanying questions axe given in Figure 1.
"Each story has a title (such as &quot;Storybook Person Found Alive!&quot;) and date-line (such as &quot;ENGLAND,[REF_CITE]&quot;) in the Remedia corpus."
"Although both the TREC-8 QA task and the reading comprehension QA task are about question answering, there are a few differences in the two tasks."
"In TREC,-8, the answer to a question can be from any of the hundreds of thousands of documents in the database, whereas for the reading comprehension task, the answer only comes from the short story associated with the question."
"Thus, while the TREC-8 QA task requires efficient indexing and retrieval techniques to narrow down to the documents that contain the answers, this step is largely not needed for the reading com-prehension task."
"Also, an answer as defined in the TREC-8 QA task is a 50-byte or 250-byte answer string, whereas an answer is a complete sentence in the reading comprehen-sion task."
"Another perhaps more subtle differ-ence is that the questions formulated in both tasks have different motivation: for TI:tEC-8, it is for the purpose of information-seeking, whereas for reading comprehension, it is for testing whether a system has &quot;understood&quot; the story."
"Hence, it may well be that the questions in TREC-8 can be expected to be more &quot;cooperative&quot;, while those for reading comprehension can be uncooperative or even &quot;tricky&quot; in nature."
"In this paper, we only address the ques-tion answering task in the context of read-ing comprehension, although we expect that the techniques we developed in this paper will be equally applicable to answering questions in an information-seeking context like that of TREC-8."
"The early work on questiion answering[REF_CITE]focused more on knowledge representation and inference issues, and the work was not targeted at aal~wering questions from unrestricted text."
"However, no large scale evaluation was attempted, and the work was not based on a machine learning approach."
"Fueled by the question answering track ini-tiated in TREC-8[REF_CITE], there is a recent surge of research activities on the topic of question answering."
"Among the participants who returned the best scores at the TREC-8 QA track[REF_CITE], none of them uses a machine learning ap-proach."
"One exception is the work[REF_CITE]at the TREC-8 QA track, which uses logistic regression to rank potential an-swers using a training set with seven features."
"However, their features are meant for the task of selecting more specific answer spans, and are different from the features we use in this paper."
The TREC-8 QA test scores[REF_CITE]were also considerably lower than best QA test scores.
"Because of the huge number of documents used in the TRECC-8 QA track, the partici-pants have to perform efficient document in-dexing and retrieval in order to tackle the complete QA task."
"It has been found that both the shallow processing techniques of IR, as well as the more linguistic-oriented natural language processing techniques are needed to perform well on the TREC-8 QA track."
"In contrast, for our current QA work on reading comprehension, because the answer for each question comes from the associated story, no sophisticated IR indexing and retrieval tech-niques are used."
"Naturally, our current work on question an-swering for the reading comprehension task is most related to those[REF_CITE]."
"In fact, all of this body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the same development set of stories."
"The work[REF_CITE]initiated this series of work, and it reported an accuracy of 36.3% on answering the questions in the test stories."
"Subsequently, the work[REF_CITE]and[REF_CITE]improved the accuracy further to 39.7% and 41%, respectively."
"However, all of these three systems used handcrafted, deterministic rules and algorithms."
"In contrast, we adopt a machine learning approach in this paper."
"The one notable exception is the work[REF_CITE], which attempted a ma-chine learning approach to question answering for the same reading comprehension task."
"Un-fortunately, out of the several machine learn-ing algorithms they tried, the best approach (using neural network learning) only managed to obtain an accuracy of 14%."
This work casts doubt on whether a machine learning approach to question answering can achieve accuracy competitive to the handcrafted rule-based approach.
Our current work attempts to address exactly this issue.
"In this section, we present our machine learn-ing approach to question answering."
We have successfully implemented a question answer-ing system based on this approach.
Our sys-tem is named AQUAREAS (Automated QUes-tion Answering upon REAding Stories).
"The advantage of a machine learning approach is that it is more adaptable, robust, flexible, and maintainable."
There is no need for a human to manually engineer a set of handcrafted rules and continuously improve or maintain the set of rules.
"For every question, our QA task requires the computer program to pick a sentence in the associated story as the answer to that question."
"In our approach, we represent each question-sentence pair as a feature vector."
"Our goal is to design a feature vector represen-tation such that it provides useful information to a learning algorithm to automatically build five classifiers, one for each question type."
In prior work[REF_CITE]the num-ber and type of information sources used for computation is specific to and rlifFerent for each question type.
"In AQUAREAS, we use the same set of features for all five question types, leaving it to the learning algorithm to identify which are the useful features to test for in each question type."
The machine learning approach comprises two steps.
"First, we design a set of features to capture the information that helps to distin- guish answer sentences from non-answer sen-tences."
"Next, we use a learning algorithm to generate a classifier for each question type from the training examples."
Each training example or test example is a feature vector representing one question-sentence pair.
"Given a question q in a story, one positive example is generated from each sentence s that is marked (by the MITRE group) as an answer to q, and the ques-tion q itself."
"For negative training exam-ples, all other sentences that are not marked as answers to a question q can be used."
"In AQUAREAS, we use all other sentences that are marked as answers to the questions other than q in the same story to generate the neg-ative examples for question q."
This also helps to keep the ratio of negative examples to pos-itive examples from becoming too high.
Our feature representation was designed to capture the information sources that prior work[REF_CITE]used in their computations or rules.
"We hypothesize that given equivalent information sources, a ma-chine learning approach can do as well as a system built using handcrafted rules."
Our fea-ture vector consists of 20 features. • Diff-from-Max-Word- Match (DMWM)
"The possible values for this feature are 0, 1, 2, 3, ...."
"For a given question q and a sentence s, the value for this feature is computed by first counting the num-ber of matching words present in q and s, where two words match if they have the same morphological root."
"This gives the raw word match score m for the question-sentence pair q and s. Next, we find the highest raw word match score M over all sentences si in the story and q."
"The value of this feature DMWM for the question-sentence pair q and s is M - rn.1 1In an earlier version of AQUAREAS,we simply used the raw word match score m as the feature."
"However, the learned classifiersdid not performwell."
Wesuspect that the absolute raw word match score m may not matter as much as whether a sentence has the highest raw word match score M in a story (relative to other sentences in the same story).
We address this deft-ciency in our reformulated difference-from-maximum computation.
"Intuitively, a feature value of 0 is the best, indicating that for that question-sentence pair q and s, they have the most num-ber of matching words in the story, when comparing q with all sentences sz in the same story."
"In the case where there are zero match-ing words between a question q and all sentences sz in a story (i.e., M = 0), then this DMWM feature will be assigned 0 for all question-sentence pairs q and si in the story."
"To avoid such a situation where a value of 0 is also assigned for this feature even when there are no matching words, we instead assign a very large value (200) to this feature for such cases."
D [if-from-Max-Verb-Mat ch (DMVM)
"The possible values for this feature are 0, 1, 2, 3, ...."
"The value for this feature is computed in exactly the same way as DMWM, except that we only count main verb matches between a question and a sentence, excluding verbs whose morpho-logical roots are &quot;be&quot;, &quot;do&quot; or &quot;have&quot;. (Such verbs tend not to carry as much &quot;semantic&quot; information.)"
"DMWM-Prev, DMVM-Prev, DMWM-Next, DMVM-Next The possible values for each of these fea-tures are 0, 1, 2, 3, ...."
"Their compu-tation is similar to DMWM and DMVM, except that they are computed from the question q and the sentence s-1 (the sen-tence preceding the current sentence s in consideration), in the case of DMWM-Prey and DMVM-Prev, and the question q and the sentence s+l (the sentence fol-lowing s) in the case of DMWM-Next and DMVM-Next."
"For the title and dateline of a story, we take them as having no pre-vious or next sentences."
"For the first sen-tence in the body of a story, there is no previous sentence and likewise for the last sentence in the body of a story, there is no next sentence."
"For all such cases, we give a raw word/verb match score rn of 0 in the computation."
"We designed these 4 features to capture information that will be helpful to the why questions, since it has been observed in prior work[REF_CITE]that the answer sentence to a why question tends to follow (or precede) the sentence in the story that has the most number of word matches with the question. • Sentence-contains=Person, Sentence=contains- Organization) Sentence=contains-Location, Sentence=contains=Date, Sentence-contains-Time The possible values for each of these fea-tures are true or false."
"To compute these feature values for a sentence, we used the Remedia corpus provided by MITRE which has been hand-tagged with named entities."
"If a sentence contains at least one word tagged with the named en-tity person, then the feature Sentence-contalns-Person will be assigned the value true."
Its value is false otherwise.
"Sim-ilarly for the other four named entities organization, location, date, and time. • Coreference information Coreferenee information does not con-tribute any new features, but rather it may change the values assigned to the five features Sentence-contains-Person, Sentence-contains-Organization, ...."
"By using the Remedia corpus provided by MITRE which has been hand-tagged with coreference chains of noun phrases, we can propagate a named entity tag across all noun phrases in the same coref-erence chain."
We then utilize the revised propagated named entities to assign val-ues to the five named entity features for a sentence.
"The effect of using coreference information is that for some sentence, a named entity feature may have its value changed from false to true."
"This oc-curs when, for instance, a pronoun &quot;he&quot; in a sentence corefers to a noun phrase &quot;Mr."
Robin&quot; and inherits the named en-tity tag person from &quot;Mr.
"Robin&quot; in the same coreference chain. • Sentence=is-Title, Sentence=is- Dateline"
The possible values for each of these fea-tures are true or false.
"If a sentence is the title of a story, then the feature Sentence-is-Title will be assigned the value true."
Its value is false otherwise.
"Similarly, the feature Sentence-is-Dateline applies to a sentence which is the dateline of a story."
"It has been observed in prior work[REF_CITE]that such sentences may be more likely to be the answer sentences to some question type (for example, dateline can answer to when questions). keywords in sentences The idea behind the use of this group of features is that certain words in a sen-tence may provide strong clues to the sen-tence being the answer to some question type."
"For instance, the preposition &quot;in&quot; (such as &quot;... in the United States, ...&quot;) may be a strong clue that the sentence is an answer to a where question."
We devised an automated procedure to find such words.
"For each of the five ques-tion types, we collect all the sentences in the training stories that answer to the question type."
Any word (in its morpho-logical root form) that occurs at least 3 times in this set of sentences is a possi-ble candidate word.
"For each candidate word, we compute the following correla-tion metric C[REF_CITE]: (Nr+N~,- - N,_N,~+)v~"
C= x/(lv.+ +
"N~-)CN,++ N,_)(N.+ + N,+)0V,- + g,-) where Nr+ (Nn+) is the number of train-ing story sentences that answer (do not answer) to the question type and in which the word w occurs, and Nr_ (Nn_) is the number of training story sentences that answer (do not answer) to the question type and in which the word w does not occur."
N = Nr+ +
Nr_ + Nn+ + Nn-.
Note that the correlation metric C is the square root of the X2 metric.
A candidate word that has high positive C value is a good clue word.
"If such a word occurs in a sentence, then the sentence is likely to answer to the question type."
"For each question type, we find one word that has the highest positive C value for that question type."
"The following five words (&quot;name&quot;, &quot;call&quot;, &quot;year&quot;, &quot;in&quot;, and &quot;to&quot;) are found automatically in this way for the five question types who, what, when, where, and why, respectively."
"One feature is then formed for each word: whether a sentence contains the word &quot;name&quot;, whether a sentence contains the word &quot;call&quot;, etc."
The possible values for these features are true or false.
"Note that this list of keywords is deter-mined automatically from the training stories only, without looking at the test stories. • keywords in questions It has been observed in the work[REF_CITE]that certain words in a when or where question tend to indicate that the dateline is an ~n~wer sentence to the question."
"The words used[REF_CITE]are &quot;happen&quot;, &quot;take place&quot; &quot;this&quot;, &quot;story&quot;."
"In our work, we attempted to discover these words automatically, using the cor-relation metric."
"The method is the same as what we used to discover the keywords in sentences, except that Nr+ (Nn+) is the number of training story questions that have (do not have) dateline as an answer to the question, and in which the word w occurs, and Nr- (Nn-) is the number of training story questions that have (do not have) dateline as an answer to the question and in which the word w does not occur."
We again picked the word with the high-est positive C value for each question type.
"Only two words (&quot;story and &quot;this&quot;) are found, for the when and where ques-tion, respectively."
"For the other question types, either the dateline was never an answer sentence to the question type, or that no candidate words occur at least three times in the training story ques-tions."
"We then form one feature for each word: whether a question contains the word %tory&apos;, and whether a question contains the word &quot;this&quot;."
The possible values for these features are true or false.
"Again, these two keywords are determined from the training stories only, without looking at the test stories."
It is interesting to note that the words automatically determined by out procedure are also part of those words found manually in the prior work of (l:tiloffand[REF_CITE]).
"The next step is to use a machine learning al-gorithm to learn five classifiers from the train-ing examples, one classifier per question type."
"The learning algorithm we used in AQUAREAS is C5, a more recent version of C4.5[REF_CITE]."
"For each test example, the classifier will de-cide if it is positive (an answer) or negative (not an answer) with a confidence value."
"We pick as the answer to the question the sen-tence whose feature vector was classified posi-tive with the highest confidence, or in the ab-sence of such, the sentence classified negative with the lowest confidence."
AQUAREASbreaks ties in favor of the sentence appearing earlier in the story.
C5 accepts parameters that affect its learn-ing algorithm.
"The following three parame-ters were used in AQUAB.EASto achieve better performance, m avoids over-fitting the train-ing data by specifying that a minimum num-ber of rn examples must follow a decision tree branch, t specifies the maximum number of decision trees used in adaptive boosting to de-termine the final decision through voting, nip cost influences C5 to avoid false negatives (or false positives)."
"To evaluate our learning approach, we trained AQUAREA$on the same development set of stories and tested it on the same test set of stories as those used in all past work on the reading comprehension task ([REF_CITE];"
"Specifically, the set of stories used are published by Remedia Pub-licatious."
"We used the same softcopy version created by the MITRE group, and the ma-terial includes manual annotations of named entities and coreference cbalns as done by the MITRE group."
The training set consists of 28 stories from grade 2 and 27 stories from grade 5.
The test set consists of 30 stories from grade 3 and 30 stories from grade 4.
"The scoring metric that we used for evalu-ation is HumSent, which is the percentage of test questions for which AQUAREAShas cho-sen a correct sentence as the answer."
This metric is originally proposed[REF_CITE].
The correct answer sentences are chosen manually by the MITRE group.
"Although there were a few other scoring met- rics originally proposed[REF_CITE], all the metrics were found to correlate well with one another."
"As such, all subsequent work[REF_CITE]uses HumSent as the main scoring metric, and it is also the scoring metric that we adopted in this paper."
"Based on the complete set of 20 features de-scribed in the previous section, we trained one classifier per question type."
"For each question type, we uniformly use the same, identical set of features."
"The followinglearning parameters were found to give the best HuinSent accuracy and were uniformly used in generating all the decision tree classifiers for all question types reported in this paper: m = 37, t = 7, and nip cost = 1.2."
"Using a large rn results in simpler decision trees, t = 7 results in the use of boosting with multiple decision trees."
"Since there are a lot more negative training exam-ples compared to positive training examples (ratio of approximately 4:1), there is a ten-dency to generate a default tree classifyingall training examples as negative (since the ac-curacy of such a tree is already quite good -about 80% on our skewed distribution of train-ing examples)."
"Setting nip cost at 1.2 will make it more costly to misclassify a positive training example as negative, and thus more costly to generate the default tree, resulting in better accuracy."
We achieved an overall HumSent accuracy of 39.3% on the 300 test questions.
The breakdown into the number of questions an-swered correctly per question type is shown in the first row of Table 1.
"Our results indi-cate that our machine learning approach can achieve accuracy comparable with other ap-proaches that rely on handcrafted, determin-istic rules and algorithms."
"For comparison, the HumSent scores reported in the work of (Hirschm~.n et al., 1999),[REF_CITE],[REF_CITE], and[REF_CITE]are 36.3%, 41%, 39.7%, and 14%, respectively."
Figure 2 shows the sequence of decision trees generated via boosting for the when question type.
All the trees look reasonable and intuitive.
"The first tree states that if the Diff-from-Max-Word-Match is zero (i.e., the sentence has the highest number of word match to the question), then the sentence is an answer."
"Otherwise, the classifier tests for whether the sentence contains a date."
"If it does, then the sentence is an answer, else it is not an answer."
The second tree is a default tree that just classifies any sentence as not an answer.
The rest of the trees similarly test on features that we intuitively feel are indica-tive of whether a sentence answers to a when question.
"To investigate the relative importance of each type of features, we remove one type of features at a time and observe its impact on HuinSent accuracy."
The resulting drop in ac-curacy is tabulated in the remaining rows of Table 1.
The rows are ordered in decreasing overall HumSent accuracy.
"As expected, removing the word match fea-ture causes the largest drop in overall accu-racy, and the accuracy decline affects all ques-tion types."
"Removing the five named entity features also causes a large decline, affect-ing mainly the who, when, and where ques-tions."
"Named entities are useful for answer-ing these question types, since who typically asks for a person (or organization), when asks for date or time, and where asks for location."
"What is perhaps a little surprising is that the seven automatically discovered keywords are also found to be very important, and removing these seven features causes the second largest decline in overall HumSent accuracy."
"Coreference is found to affect the who, when, and where questions, as expected."
"The previous and next word/verb matches cause the largest decline for why questions, dropping the number of correctly answered why ques-tions to 3."
"Removing verb match also causes a 3% drop in overall accuracy, while dateline and title only affect the when questions."
"In our future work, we plan to investigate other potential knowledge sources that may further improve accuracy."
We also plan to in-vestigate the use of other supervised machine learning algorithms for this problem.
"We evaluated our approach on the Remedia data set, a common data set used in several recent papers on the reading compre-hension task."
"Our learning approach achieves accuracy competitive to previous approaches that rely on handcrafted, deterministic rules and algorithm~. To the best of our knowledge, this is the first work that reports that the use of a machine learning approach achieves competitive results on answering questions for reading comprehension tests."
Thanks to Marc Light and Eric Breck of MITRE Corporation for assistance in provid-ing the annotated Remedia corpus.
The development of natural language inter-faces (NLI&apos;s) for databases has been a chal-lenging problem in natural language process-ing (NLP) since the 1970&apos;s.
The need for NLI&apos;s has become more pronounced due to the widespread access to complex databases now available through the Internet.
A challenging problem for empirical NLP is the automated acquisition of NLI&apos;s from training examples.
We present a method for integrating statisti-cal and relational learning techniques for this task which exploits the strength of both ap-proaches.
Experimental results from three dif-ferent domains suggest that such an approach is more robust than a previous purely logic-based approach.
We use the term semantic parsing to refer to the process of mapping a natural language sentence to a structured meaning representa-tion.
One interesting application of semantic parsing is building natural language interfaces for online databases.
"The need for such appli-cations is growing since when information is delivered through the Internet, most users do not know the underlying database access lan-guage."
An example of such an interface that we have developed is shown in Figure 1.
Traditional (rationalist) approaches to con-structing database interfaces require an ex-pert to hand-craft an appropriate semantic parser ([REF_CITE];
"However, such hand-crafted parsers are time consllming to develop and suffer from prob-lems with robustness and incompleteness even for domain specific applications."
"Neverthe-less, very little research in empirical NLP has explored the task of automaticallyacquiring such interfaces from annotated training ex-amples."
"The only exceptions of which we are aware axe a statistical approach to map- ping airline-information queries into SQL pre-sented[REF_CITE], a probabilistic decision-tree method for the same task de-scribed[REF_CITE], and an approach using relational learning (a.k.a. inductive logicprogramming, ILP) to learn a logic-based semantic parser described[REF_CITE]."
The existing empirical systems for this task employ either a purely logical or purely sta-tistical approach.
"The former uses a deter-ministic parser, which can suffer from some of the same robustness problems as rational-ist methods."
"The latter constructs a prob-abilistic grammar, which requires supplying a sytactic parse tree as well as a semantic representation for each training sentence, and requires hand-crafting a small set of contex-tual features on which to condition the pa-rameters of the model."
Combining relational and statistical approaches can overcome the need to supply parse-trees and hand-crafted features while retaining the robustness of sta-tistical parsing.
"The current work is based on the CHILL logic-based parser-acquisition framework[REF_CITE], retain-ing access to the complete parse state for mak-ing decisions, but building a probabilistic re-lational model that allows for statistical pars-ing-"
"This section reviews our overall approach using an interface developed for a U.S. Geography database (Geoquery) as a sample applicati[REF_CITE]which is available on the Web (see hl:tp://gvg, cs. utezas, edu/users/n~./geo .html)."
First-order logic is used as a semantic repre-sentation language.
"CHILL has also been ap-plied to a restaurant database in which the logical form resembles SQL, and is translated automatically into SQL (see Figure 1)."
"We What statehas themost riversrunningthroughit? explain the features of the Geoquery repre- answer(S,most(S,R,(state(S),rlver(R),traverse(R,S)))). sentation language through a sample query:"
"Input: &quot;W&apos;hatisthe largestcityinTexas?&quot; Quc~&apos;y: answer(C,largest(C,(city(C),loc(C,S), const(S,stateid(texas))))). 2.2 Parsing Actions Our semantic parser employs a shift-reduce architecture that maintains a stack of pre- Objects are represented as logicalterms and viously built semantic constituents and a are typed with a semantic category using buffer of remaining words in the input."
"The logicalfunctions applied to possibly ambigu-parsing actions are automatically generated ous English constants (e.g. stateid(Mississippi ,) from templates given the training data."
"Relationships between ob-templates are INTRODUCE, COREF_VABS, jects are expressed using predicates; for in- DROP_CONJ, LIFT_CONJ, and SttIFT."
"IN-stance, Ioc(X,Y) states that X is located in Y."
TRODUCE pushes a predicate onto the stack We also need to handle quantifiers such based on a word appearing in the input and as &apos;largest&apos;.
We represent these using meta-information about its possible meanings in predicates for which at least one argument is a the lexicon.
COREF_VARS binds two argu-conjunctionofliterals.
"For example, largest(X, ments of two differentpredicates on the stack."
Goal) states that the object X satisfiesGoal
"DROP_CONJ (or LIFT_CON J) takes a pred- and is the largestobject that does so, using the appropriate measure of sizefor objects icate on the stack and puts it into one of the of arguments of a meta-predicate on the stack. its type (e.g. area for states,population for SHIFT simply pushes a word from the input cities).Finally,an nn.qpeci~ed object required bufferonto the stack."
The parsing actions are as an argument to a predicate can appear else- tried in exactly this order.
"The parser also where in the sentence, requiring the use of the requires a lexicon to map phrases in the in-predicate const(X,C) to bind the variable X to put into specific predicates, this lexicon can the constant C. Some other database queries also be learned automatically from the train- (or trainingexamples) forthe U.S. Geography domain are shown below:"
"What isthe capitalofTexas? answer(C,(ca pital(C,S),const(S,stateid (texas)))). ing data[REF_CITE]."
"Let&apos;s go through a simple trace of parsing the request &quot;What is the capital of Texas?&quot; A lexicon that maps &apos;capital&apos; to &apos;capital(_,_)&apos; and &apos;Texas&apos; to &apos;const(_,stateid(texas))&apos; su.~ces here."
Interrogatives like &quot;what&quot; may be mapped to predicates in the lexicon if neces-sary.
The parser begins with an initial stack and a buffer holding the input sentence.
Each predicate on the parse stack has an attached buffer to hold the context in which it was introduced; words from the input sentence are shifted onto this buffer during parsing.
The initial parse state is shown below:
"Parse Stack: [answer(_,_):O] Input Buffer: [what,is,the,ca pital,of,texas,?]"
"Since the first three words in the input buffer do not map to any predicates, three SHIFT actions are performed."
The next is an INTRODUCE as &apos;capital&apos; is at the head of the input buffer:
"Parse Stack: [capital(_,_):O, answer(_,_):[the,is,what]] Input Buffer: [capital,of,texas,?]"
"The next action is a COREF_VARS that binds the firstargument of capital(_,_) with the firstargument of answer(_,_)."
Parse Stack: negative example for an action if it is a posi-tive example for another action below the cur-rent one in the ordered list of actions.
Control conditions which decide the correct action for a given parse state axe learned for each action from these positive and negative examples.
"The initial CHILL system used ILP[REF_CITE]to learn Prolog control rules and employed deterministic parsing, us-ing the learned rules to decide the appropriate parse action for each state."
"The current ap-proach learns a model for estimating the prob-ability that each action should be applied to a given state, and employs statistical parsing[REF_CITE]to try to find the overall most probable parse, using beam search to control the complexity."
The advan-tage of ILP is that it can perform induction over the logical description of the complete parse state without the need to pre-engineer a fixed set of features (which vary greatly from one domain to another) that are relevant to making decisions.
"We maintain this advan-tage by using ILP to learn a committee of [capital(C,_)O: ,answer(C,_):[the,is,what]] hypotheses, and basing probability estimates Input Buffer: [capital,of,texas,?]"
"The next sequence of steps axe two SHIFT&apos;s, an INTRODUCE, and then a COR.EF_VARS:"
"Parse Stack: [const(S,stateid(texas)0&apos;): ca pital(C,S):[of, ca pital], answer(C,_):[the,is,what~ Input Buffer: [texas,?]"
The last four steps are two DROP_CONJ&apos;s followed by two SHIFT&apos;s:
"Parse Stack: [answer(C, (capital(C,S), const(S,stateld(texas)))): [?,texas,of, ca pital,the,is,what]] Input Buffer: I]"
This is the final state and the logical query is extracted from the stack.
"The initiallyconstructed parser has no con-straints on when to apply actions, and is thereforeoverlygeneraland generatesn11rner-ous spuriousparses."
Positiveand negativeex-amples are collectedforeach actionby parsing each tralnlng example and recordlng the parse states encountered.
Parse states to which an action should be applied (i.e. the action leads to building the correct semantic representa-tion) are labeled positive examples for that action.
"Otherwise, a parse state is labeled a on a weighted vote of them[REF_CITE]."
We believethat using such a proba-bilisticrelational model[REF_CITE]combines the advantages of frameworks based on first-orderlogicand thosebased on standard statisticaltechniques.
This section discusses the ILP method used to build a committee of logical control hypothe-ses for each action.
Most ILP methods use a set-covering method to learn one clause (rule) at a time and con-struct clauses using either a strictly top-down (general to specific) or bottom-up (specific to general) search through the space of possi-ble rules[REF_CITE].
"TAB-ULATE,1 on the other hand, employs both bottom-up and top-down methods to con-struct potential clauses and searches through the hypothesis space of complete logic pro-grams (i.e. sets of clauses called theories)."
It uses beam search to find a set of alternative hypotheses guided by a theory evaluation met-ric discussed below.
The search starts with aTABULATBstands for Top-doera And Bottom-Up cLAuse construction urith Theory Evaluation. retained.
"There are three possible outcomes in a refinement: 1) the current clause satisfies the noise-handling criterion and is simply re-turned (nextj is set to empty), 2) the current clause does not satisfy the noise-handling cri-teria and all possible refinements are returned (neztj is set to the refined clause), and 3) the current clause does not satisfy the noise- / handling criterion but there are no further re-"
"CQi := {(Coraplete(T(N,), Gj, ~+),neztj) [ as individual clauses."
"Hence, the theory is al-ways maintained complete (i.e. covering all positive examples)."
These final steps are per-formed in the Complete procedure.
The termination criterion checks for two conditions.
The first is satisfied if the next search queue does not improve the sum of being built for each theory in the search queue and the last finished clause of each theory sat-isfies the noise-handling criterion.
"Finally, a committee of hypotheses found by the algo-rithm is returned."
The goal of the search is to find accurate and yet simple hypotheses.
"We measure accu- racy using the m-estimate[REF_CITE], a smoothed measure of accuracy on the training the most specific hypothesis (the set of posi-data which in the case of a two-class problem tive examples each represented as a separate is defined as: clause)."
Each iteration of the loop attempts to refine each of the hypotheses in the current search queue.
"There are two cases in each it- accuracy(H) = s + m. p+ (1) n ,-I-rrt eration: 1) an existing clause in a theory is refined or 2) a new clause is begun."
"Clauses where s is the n-tuber of positive examples are learned using both top-down specialiT.~-covered by the hypothesis H, n is the total tion using a method similar to FOIL (Quin-number of examples covered, p+ is the prior lan, 1990) and bottom-up generalization using probability of the class (9, and m is a smooth- Least General Generalizations (LGG&apos;s)."
"Ad-ing parameter. vantages of combining both ILP approaches were explored in CHILLIN (ZeUe and Mooney, We measure theory complexity using a met- 1994), an ILP method which motivated the ric slmi]ar to that introduced in (Muggleton design of TABULATE."
An outline of the TAB- and[REF_CITE]).
The size of a Clause hav-
ULATEalgorithm is given in Figure 2. ing a Head and a Body is defined as follows
A noise-handling criterion is used to de- (ts=&quot;term size&quot; and ar=&quot;arity&apos;): cide when an individual clause in a hypoth-esis is sufficiently accurate to be permanently size(Clause) = 1+ ts(Head) + ts(Body) (2)
"I 1 T isa variable ts(T) = 2 r ~,, ¢o~t 2 + ts(argi(T)) (3)"
"The size of a clause is roughly the n,,mber of variables, constants, or predicate symbols it contains."
The size of a theory is the sum of the sizes of its clauses.
The metric M(H) used as the search heuristic is defined as:
M(H) = accuracy(H) +
C log2 size(H) (4) where C is a constant used to control the rel-ative weight of accuracy vs. complexity.
"We ass~,me that the most general hypothesis is as good as the most specific hypothesis; thus, C is determined to be:"
"C = EbSt -- EtSb (5) &amp; - &amp; where Et, Eb are the accuracy estimates of the most general and most specific hypotheses re-spectively, and St, Sb are their sizes."
A clause needs no further refinement when it meets the following criterion (as in RIPPER[REF_CITE]):
"P-.__.2_ &gt; (6) p + n where p is the number of positive examples covered by the clause, n is the number of neg-ative examples covered and -1 &lt;/~ _&lt; 1 is a parameter."
"The value of ~ is decreased when-ever the sum of the metric over the hypotheses in the queue does not improve although some of them still have ,nflni~hed or failed clauses."
A parser is a relation Parser C_Sentences x Queries where Sentences and Queries are the sets of natural language sentences and database queries respectively.
"Given a sen-tence I • Sentences, the set Q(1) = {q • Queries I (l,q) • Parser} is the set of queries that are correct interpretations of I."
A parse state consists of a stack of lexical-ized predicates and a list of words from the input sentence.
S is the set of states reach-able by the parser.
"Suppose our learned parser has n different parsing actions, the ith ac-tion a/is a function a/(s) :"
ISi -+ OSi where
ISi G S is the set of states to which the ac-tion is applicable and OSi C_S is the set of states constructed by the action.
The function ao(l) :
Sentences ~ IniS maps each sentence l to a corresponding unique initial parse state in In/S C_S.
A state is called afinalstate if there exists no parsing action applicable to it.
"The partial function a,+l(s) : FS ~ Queries is defined as the action that retrieves the query from the final state s 6 FS C S if one exists."
Some final states may not &quot;contain&quot; a query (e.g. when the parse stack contain.q predicates with unbound ~rariables) and therefore it is a partial function.
"When the parser meets such a final state, it reports a failure."
A path is a finite sequence of parsing ac-tions.
"Given a sentence 1, a good state s is one such that there exists a path from it to a query q 6 Q(1)."
"Otherwise, it is a bad state."
The set of parse states can be uniquely divided into the set of good states and the set of bad states given l and Parser.
S+ and S- are the sets of good and bad states respectively.
"Given a sentence l, the goal is to construct the query ~ such that = argmqaXP(q • Q(l) [l ~ q) (7) where I ~ q means a path exists from l to q."
"Now, we need to estimate P(q • Q(1)"
Il =-~ q).
"First, we notice that:"
P(q • Q(1) [l =~.q) ---- (8) P(s • FS+ II ~ s and an+l(S) ----q) where FS+ = FS N S+.
"For notational con-venience we drop the conditions and denote the above probabilities as P(q • Q(l)) and P(s • FS+) respectively, assuming these con-ditions in the following discussion."
The equa-tion states that the probability that a given query is a correct meaning for I is the same as the probability that the final state (reached by parsing l) is a good state.
We need to de-termine in general the probability of having a good resulting parse state.
"Given any parse state si at the jth step of parsing and an ac-tion ai such that si+1 = a/(sj), we have:"
PCsi+1 • (9) pCsj+l e o&amp;+ I • x&amp;+)pCs • x&amp;+) + P(Si+l • OSi+
"Isj ¢ ISi+)P(sj ¢~ ISi+) parse state from a bad one, the second term {al,..., an-l} applicable to s is less than or is zero."
"Now, we are ready to derive P(q • equal to a threshold a."
"If A(s) is empty, we"
"Suppose q = an+l(Sm), we have:"
P(q 6 Q(l)) ( = P(s~ •
"F~) . . . assume the maxlrn,,rn is zero."
"Cs) • os~ Is • xs~) = { ,c. ~ . ( ( , , ) ~ ~ IS os ~) ~) ifmaxCACs))&lt; 0 otherwise = P(s,n • FS + lsm-1•/St,_a)..."
"P(s~ • OS~_, I sj-1 • Is~_,)... where a is the threshold,3 c(an(s) • Ob~) is the count of the number of good states pro-"
"P(s2 • Ob~, Is1 • IS~,)P(&apos;I • IS~,) duced by the last action, and c(s • IS~) is the where ak denotes the index of which action count of the number of good states to which the last action is applicable. is applied at the kth step."
"We assume that = P(sl • I~aa) ~ 0 (which may not be true Now, let&apos;s discuss how P(ai(s) •"
OS~~ Ihk) in general).
"Now, we have m--"
I and Akare estimated.
"If hk ~ s (i.e. hk covers s), we have"
PCai(s)• o~ Ihk) = pc + O&quot;nc P(q 6 Q(l)) = 7 II P(sj+I • O~ lsj • IS~-3). (14) i=l Pc -t- nc (11)
Next we describe how we estimate the proba- where Pc and ne are the number of positive bili~ of the goodness of each action in a given and negative examples covered by hk respec-state (P(~(s) • o$ I s • I~)).
We n~ tively.
"Otherwise, if h~ ~=s (i.e. hk does not not estimate 7 since its value does not affect cover s), we have the outcome of equation (7). 4.2 Estimating Probabilities for Parsing Actions"
"PCai(s) • OS7&quot; Ihk) -- p&quot; + 8.n,, (15)"
"The committee of hypotheses learned by TAB- where Pu and nu are the n,,rnber of positive ULATE is used to estimate the probability that and negative examples rejected by hk respec-a particular action is a good one to apply to a tively. /9 is the probability that a negative given parse state."
Some hypotheses are more example is mislabelled and its value can be &quot;important&quot; than others in the sense that they estimated given # (in equation (6)) and the carry more weight in the decision.
A weight- total nnrnber of positive and negative exam-ing parameter is also included to lower the ples. probability estimate of actions that appear One could use a variety of linear combina-fm&apos;ther down the decision list.
For actions ai tion methods to estimate the weights Ak (e.g. where 1 &lt; i &lt; n - 1:
P(ai(s)• o~ Is • Is7-)=
"How-ever, we have taken a simple approach and (12) weighted hypotheses based on their relative .po,(i)-I ~ simplicity: AkP(~Cs) 60b~ ~ I h~) hk~H~ size(hk) -1 where s is a given parse state, pos(i) is the Ak = ~.lHd size(hi)_1&quot; (16) z-d=l position of the action ai in the list of ac-tions applicable to state s, Ak and 0 &lt; /~ &lt; 4.3 Searching for a Parse 1 are weighting parameters,z"
Hi is the set
"To find the most probably correct parse, the of hypotheses learned for the action ai, and parser employs a beam search."
"At each step, ~k A~ = 1. the parser finds all of the parsing actions ap-"
"To estimate the probability for the last ac- plicable to each parse state on the queue and tion an, we devise a simple test that checks calculates the probability of goodness of each if the maximum of the set A(s) of proba- ofthem using equations (12) and (13)."
It then bility estimates for the subset of the actions
"SThe threshold is set to 0.5 for all the experiments 2p is set to 0.95 for all the experiments performed. performed. computes the probability that the resulting state of each possible action is a good state using equation (11), sorts the queue of possi-ble next states accordingly, and keeps the best B options."
The parser stops when a complete parse is found on the top of the parse queue or a failure is reported.
Three different domains are used to demon-strate the performance of the new approach.
The first is the U.S. Geography domain.
"The database contains about 800 facts about U.S. states like population, area, capital city, neighboring states, major rivers, major cities, and so on."
"A hand-crafted parser, GEOBASE was previously constructed for this domain as a demo product for Turbo Prolog."
The second application is the restaurant query system il-lustrated in Figure 1.
"The database contains information about thousands of restaurants in Northern California, including the name of the restaurant, its location, its specialty, and a guide-book rating."
The third domain consists of a set of 300 computer-related jobs automat-ically extracted from postings to the USENET newsgroup austin.jobs.
"The database con-talus the following information: the job title, the company, the recruiter, the location, the salary, the languages and platforms used, and required or desired years of experience and de-grees."
The geography corpus contains 560 questions.
We also included results for the subset of 250 sentences originally used in the experiments reported[REF_CITE].
"The remaining questions were specif-icaUy collected to be more complex than the original 250, and generally require one or more meta-predicates."
The restaurant corpus con-taln~ 250 questions automatically generated from a hand-built grammar Constructed to re-flect typical queries in this domain.
The job corpus contains 400 questions automatically generated in a similar fashion.
The beam width for TABULATE was set~ to five for all the domains.
The deterministic parser used only the best hypothesis found.
The experiments were conducted using 10-fold cross validation.
"For each domain, the average recall (a.k.a. accuracy) and precision of the parser on dis-joint test data are reported where: of correct queries produced Recall = of test sentences Precision = # of correct queries produced # of complete parses produced&quot;"
A complete parse is one which contains an ex-ecutable query (which could be incorrect).
A query is considered correct if it produces the same answer set as the gold-standard query supplied with the corpus.
The results are presented in Table 1 and Fig-ure 3.
"By switching from deterministic to probabilistic parsing, the system increased the number of correct queries it produced."
Re-call increases almost monotonically with pars-ing beam width in most of the domains.
I_m-provement is most apparent in the Jobs do-maln where probabilistic parsing signiBcantly outperformed the deterministic system (80% vs 68%).
"However, using a beam width of one (and thus the probabilistic parser picks only the best action) results in worse perfor-mance than using the original purely logic-based determlni~tic parser."
This suggests that the probability esitmates could be improved since overall they are not indicating the sin-gle best action as well as a non-probabilistic approach.
"Precision of the system decreased with beam width, but not signi~cantly except for the larger Geography corpus."
"Since the system conducts a more extensive search for a complete parse, it risks increasing the num-ber of incorrect as well as correct parses."
The importance of recall vs. precision depends on the relative cost of providing an incorrect an-swer versus no answer at all.
Individual ap-plications may require emphasizing one or the other.
All of the experiments were run on a 167MHz UltraSparc work station under Sic-stus Prolog.
"Although results on the parsing time of the different systems are not formally reported here, it was noted that the difference between using a beam width of three and the original system was less than two seconds in all domains but increased to a~r0und twenty seconds when using a beam width of twelve."
"However, the current Prolog implementation is not highly optimized. parser using various beam widths in the different in the worst on the original geography queries, recall using the new approach, its performance since it is difficult to hand-crat~ a parser that varied signiGcantly from dom~;~ to domain. handles a sn~cient variety of questions."
"As a result, the recall did not always improve dramatically by using a larger beam width. 6 Conclusion Domain factors possibly affecting the perfor- A probabilistic framework for semantic shift-mance are the quality of the lexicon, the rel-reduce parsing was presented."
"A new ILP ative amount of data available for calculat- ing probability estimates, and the problem learning system was also introduced which of learns multiple hypotheses."
These two tech- &apos;~parser incompleteness&quot; with respect to the niques were integrated to learn semantic test data (i.e. there is not a path from a sen-parsers for building NLI&apos;s to on|ine databases. tence to a correct query which happens when Experimental results were presented that &apos;7 = 0).
"The performance of all systems were demonstrate that such an approach outper-basically equivalent in the restaurant domain, forms a purely logical approach in terms of where they were near perfect in both recall the accuracy of the learned parser. and precision."
This is because this corpus is relatively easier given the restricted range of possible questions due to the limited informa- 7 Ack~nowledgements tion available about each restaurant.
This research was supported by a grant from tems achieved &gt; 90% in recall and precision the Daimler-Chrysler Research and Technol-given only roughly 30% of the training data ogy Center and by the National Science Foun-in this domain.
"Finally, GEOBASE perfomed dation under grant n~I-9704943."
This paper presents the automatic construction of a Korean WordNet from pre-existing lexical resources.
A set of automatic WSD techniques is described for linking Korean words collected from a bilingual MRD to English WordNet synsets.
We will show how individual linking provided by each WSD method is then combined to produce a Korean WordNet for nouns.
There is no doubt on the increasing importance of using wide coverage ontologies for NLP tasks especially for information retrieval and cross-language information retrieval.
"While these ontologies exist in English, there are very few available wide range ontologies for other languages."
Manual construction of the ontology by experts is the most reliable technique but is costly and highly time-consuming.
This is the reason for many researchers having focused on massive acquisition of lexical knowledge and semantic information from pre-existing lexical resources as automatically as possible.
This paper presents a novel approach for automatic WordNet mapping using word sense disambiguafion.
The method has been applied to link Korean words from a bilingual dictionary to English WordNet synsets.
"To clarify the description, an example is given."
"To link the first sense of Korean word &quot;gwan-mog&quot; to WordNet synset, we employ a bilingual Korean-English dictionary."
The first sense of &apos;gwan-mog&apos; has &apos;bush&apos; as a translation in English and &apos;bush&apos; has five synsets in WordNet.
Therefore the first sense of &apos;gwan-mog&quot; has five candidate synsets.
"Somehow we decide a synset {shrub, bush} among five candidate synsets and link the sense of &apos;gwan-mog&apos; to this synset."
"As seen from this example, when we link the senses of Korean words to WordNet synsets, there are semantic ambiguities."
To remove the ambiguities we develop new word sense disambiguation heuristics and automatic mapping method to construct Korean WordNet based on the existing English WordNet.
This paper is organized as follows.
"In section 2, we describe multiple heuristics for word sense disambiguation for sense linking."
"In section 3, we explain the method of combination for these heuristics."
"Section 4 presents some experiment results, and section 5 will discuss some related researches."
Finally we draw some conclusions and future researches in section 6.
"The automatic mapping-based Korean WordNet plays a natural Korean-English bilingual thesaurus, so it will be directly applied to Korean-English cross-lingual information retrieval as well as Korean monolingual information retrieval. 2 Multiple heuristics for word sense disambiguation"
"As the mapping method described in this paper has been developed for combining multiple individual solutions, each single heuristic must be seen as a container for some part of the linguistic knowledge needed to disarnbiguate the *This research was supported by KOSEF special purpose basic research (1997.9- 2000.8 #970-1020-301-3)[REF_CITE]ambiguous WordNet synsets."
"Therefore, not a single heuristic is suitable to all Korean words collected from a bilingual dictionary."
We will describe each individual WSD (word sense disambiguation) heuristic for Korean word mapping into corresponding English senses.
Korean word WordNetsynsetEnglishword
Figure 1 shows the Korean word to WordNet synset association.
The j-th sense of Korean word kw has m translations in English and n WordNet synsets as candidate senses.
"Each heuristic is applied to the candidate senses (ws,, .... ws) and provides scores for them."
This heuristic comes from our previous Korean WSD research[REF_CITE]and assumes that all the translations in English for the same Korean word sense are semantically similar.
So this heuristic provides the maximum score to the sense that is most similar to the senses of the other translations.
This heuristic is applied when the number of translations for the same Korean word sense is more than 1.
The following formula explains the idea.
"Hi(s,) = max support( s,,ew~) - 1 ~&apos;~, (n-1)+a k,=l where EWi = (ewl s, ~ synset(ew)}"
"In this formula, Hi(s) is a heuristic score of synset s, s is a candidate synset, ew is a translation into English, n is the number of translations and synset(ew) is the set of synsets of the translation ew."
So Ew becomes the set of translations which have the synset sr.
"The parameter tx controls the relative contribution of candidate synsets in different number of translations: as the value of a increases, the candidate synsets in smaller number of translations get relatively less weight (a=0.5 was tuned experimentally), support(s,ew) calculates the maximum similarity with the synset s and the translation ew, which is defined as : support(si, ew) = max S(si, s) sEsynset( ew)"
"S2) = ~Sim( st, s2) if sire(s,, s2) _&gt;0"
"S(sl, l 0 otherwise"
Similarity measures lower than a threshold 0 are considered to be noise and are ignored.
"In our experiments, 0=0.3 was used. sim(s,s2) computes the conceptual similarity between concepts s~ and sz as in the following formula : sim(sl, s2)= 2 x level(MSCA(sl, s:)) level(sO + level(s2) where MSCA(sl,s2) represents the most specific common ancestor of concepts s~ and s2 and level(s) refers to the depth of concept s from the root node in the WordNetL"
This heuristic provides prior probability to each sense of a single translation as score.
"Therefore we will give maximum score to the synset of a monosemous translation, that is, the translation which has only one corresponding synset."
The following formula explains the idea.
"H2(s,) = max P(s, lew) ¢,n,EEW~ where EWi = {ew Isi ~ synset(ew) } P (si Iewj) -~ -- 1 nj where si ~ synset(ewj), nj = Isyr et( w,)l"
"In this formula, n is the number of synsets of the translation e~t~. 2.3 Heuristic 3: Sense Ordering[REF_CITE]reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense.[REF_CITE]found that automatic"
I We use English WordNet version 1.6 - L assignment of polysemous words in Brown Corpus to senses[REF_CITE]% correct with a heuristic of most frequently occurring sense.
We adopt these previous results to develop sense ordering heuristic.
The sense ordering heuristic provides the maximum score to the most frequently used sense of a Ixanslation.
The following formula explains the heuristic.
"H3(sO = max SO(s,,ew) ewEEW, where EW, = {ew Isi ~ synset(ew) } ot SO(s,,ew) x~ where si ~ synset(ew) ^ synset(ew) is sorted by frequency ^ s, is the x- th synset in synset(ew)"
"In this formula, x refers to the sense order of s in synset(ew): x is 1 when s, is the most frequently used sense of ew."
The information about the sense order of synsets of an English word was extracted from the WordNet. 0.8 prob 0.7 0.$ o.5 0.4 0.3
The value a=0.705 and fl=2.2 was acquired from a regression of Figure 2 semcor corpus[Footnote_2] data distribution.
2 semcor is a sense tagged corpus from part of Brown corpus.
This heuristic is based on the following facts:
Figure 3 explains IS-A relation heuristic.
"In figure 3, hkw is a hypemym of a Korean word kw and hew is a translation of hkw and ew is a translation of kw."
This heuristic assigns score 1 to the synsets which satisfy the above assumption according to the following formula:
"H,(s,) = max 1R(si, ew) ew~EW, where EWi = {ewl si ~ synset(ew) }"
"IR(s,,ew)={; otherwisefflsa(s&quot;si) where si ~ synset(ew) , sj ~ synset(hew)"
"In this formula, lsA(s,,s2) returns true if s, is a kind of s2."
This heuristic assumes that related concepts will be expressed using the same content words.
Given two definitions - that of the bilingual dictionary and that of the WordNet - this heuristic computes the total amount of shared content words.
"Hs(si)= max WM (si,ew) where EW~ = (ewl s~~ synset(ew) } WM (si, ew) = sim(X,Yi) sim(X,Y) ="
IX n YI Ix rl
"In this formula, X is the set of content words in English examples of bilingual dictionary and Y is the set of content words of definition and example of the synset s, in WordNet."
This heuristic uses cooccurrence measure acquired from the sense tagged Korean definition sentences of bilingual dictionary.
"To build sense tagged corpus, we use the definition sentences which have monosemous translation in bilingual dictionary."
"And we uses the 25 semantic tags of WordNet as sense tag : n6(s,) = max p(t, I x) xeDef with p =, - Z(l_a)/2× ~&apos;(1~ ~) p(ti lx) = Freq(ti, x) Freq(x)"
"In this formula, Defis the set of content words of a Korean definition sentence, t is a semantic tag corresponding to the synset s and n refers to Freq(x)."
"Given a candidate synset of a translation and 6 individual heuristic scores, our goal is to use all these 6 scores in combination to classify the synset as linking or discarding."
The combination of heuristics is performed by decision tree learning for non-linear relationship.
"Each internal node of a decision tree is a choice point, dividing an individual method into ranges of possible values."
Each leaf node is labeled with a classification (linking or disc~ding).
"The most popular method of decision tree induction, employed here, is C4.5[REF_CITE]."
"Figure 4 shows a training phase in decision ,tree based combination method."
"In the training phase, the candidate synset wsk of a Korean word is manually classified as linking or discarding and get assigned scores by each heuristic."
A training data set is constructed by these scores and manual classification.
The training data set is used to optimize a model for combining heuristics.
Figure 5 shows a mapping phase.
"In the mapping phase, the new candidate synset ws~ of a Korean word is rated using 6 heuristics, and then the decision tree, which is learned in the training phase, classifies w&amp; as linking or discarding."
The synset classified as linking is linked to the corresponding Korean word.
"In this section, we evaluate the performance of each six heuristics as well as the combination method."
"To evaluate the performance of WordNet mapping, the candidate synsets of 3260 senses of Korean words in bilingual dictionary was manually classified as linking or discarding."
We define &apos;precision&apos; as the proportion of correctly linked senses of Korean words to all the linked senses of Korean words in a test set.
We also define &apos;coverage&apos; as the proportion of linked senses of Korean words to all the senses of Korean words in a test set.
Table 1 contains the results for each heuristic evaluated individually against the manually classified data.
The test set here consists of the 3260 manually classified senses.
"In general, the results of each heuristic seem to be poor, but are always better than the random choice baseline."
The best heuristic according to the precision is the maximum similarity heuristic.
But it was applied to only 59.51% of 3260 senses of Korean words.
"The results of each heuristic are better than the random mapping, with a statistically significance at the 99% level."
This process is repeated nine times using each of the other nine parts as a validation set.
Table 2 shows the results of the other trials of the combination of all the heuristics.
Summing is a way to simply sum all the scores of each heuristic.
Then the candidate synset which has the highest summation of the scores is selected.
"Logistic regression, as described[REF_CITE], is a popular technique for binary classification."
This technique applies an inverse logit function and employs the iterative reweighted least squares algorithm.
This technique determines the weight of each heuristic.
"With the combination of the heuristics using summing, we obtained an improvement over maximum similarity heuristic (heuristic 1) of 9%, maintaining a coverage 100%."
The decision tree is able to correctly map 93.59% of the senses of
"Korean words in bilingual dictionary, maintaining a coverage 77.12%."
"Applying the decision tree to combine all the heuristics for all Korean words in bilingual dictionary, we obtain a preliminary version of the Korean[REF_CITE]4 senses of 17696 Korean nouns with an accuracy of 93.59% (-2-0.84%with 99% confidence)."
"Several attempts have been performed to automatically produce multilingual ontologies. (Knight &amp;[REF_CITE]) focuses on the construction of Sensus, a large knowledge base for supporting the Pangloss Machine Translation system, merging ontologies (ONTOS and UpperModel) and WordNet with monolingual and bilingual dictionaries. (Okumura &amp;[REF_CITE]) describes a semi-automatic method for associating a Japanese lexicon to an ontology using a Japanese/English bilingual dictionary as a &apos;bridge&apos;."
Several lexical resources and techniques are combined[REF_CITE]to map Spanish words from a bilingual dictionary to WordNet.
"In[REF_CITE], use of a taxonomic structure derived from a monolingual MRD is proposed as an aid to the mapping process."
This research is contrasted that it utilized bilingual dictionary to build monolingual thesaurus based on the existing popular lexical resources and used the combination of multiple unsupervided WSD heuristics.
This paper has explored the automatic construction of a Korean WordNet from pre-existing lexical resources - English wordNet and Korean/English bilingual dictionary.
We presented several techniques for word sense disambiguation and their application to disambiguate the translations in bilingual dictionary.
We obtained a preliminary version of the Korean[REF_CITE]4 senses of 17696 Korean nouns.
"In a series of experiments, we observed that the accuracy of mapping is over 90%."
This paper describes a first attempt at a sta-tistical model for simultaneous syntactic pars-ing and generalized word-sense disambigna-tion.
"On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agree-ment of 78.6%."
"In this paper we describe a generative, statis-tical model for simultaneously producing syn-tactic parses and word senses in sentences."
"We begin by motivating this new approach to these two, previously-separate problems, then, after reviewing previous work in these areas, we describe our model in detail."
"Finally, we will present the promising results of this, our first attempt, and the direction of future work."
Consider the following examples: 1.
IBM bought Lotus for $200 million. 2.
Sony widened its product line with per-sonal computers. 3.
"The bank issued a check for $100,000. 4."
"Apple is expecting [NP strong results]. 2 for [pp with personal computers] to attach to widened, because personal computers are products with which a product line could be widened."
"As pointed out[REF_CITE], word sense information can be a proxy for the semantic- and world-knowledge we as humans bring to bear on attachment decisions such as these."
This proxy effect is due to the &quot;lightweight semantics&quot; that word senses--in particular WordNet word senses--convey.
"Conversely, both the syntactic and semantic context in Example 3 let us know that bank is not a river bank and that check is not a restaurant bill."
"In Examples 4 and 5, knowing that the complement of expect is an NP or an SBAR provides information as to whether the sense is &quot;await&quot; or &quot;require&quot;."
"Thus, Examples 3-5 illustrate how the syntactic context of a word can help determine its meaning."
"In recent years, the success of statistical pars-ing techniques can be attributed to several fac-tors, such as the increasing size of comput-ing machinery to accommodate larger models, the availability of resources such as the Penn Treebank[REF_CITE]and the suc-cess of machine learning techniques for lower-level NLP problems, such as part-of-speech tagging[REF_CITE], and PP-attachment[REF_CITE]."
"However, perhaps even more significant has been the lexicalization 5."
"IBM expected [SBAa each employee to of the grammar formalisms being probabilis-wear a shirt and tie]. tically modeled: crucially, all the recent, suc-cessful statistical parsers have in some way With Example 1, the reading [IBM bought made use of bilexical dependencies."
"This in- [Lotus for $200 million]] is nearly impossi- cludes both the parsers that attach probabili-ble, for the simple reason that a monetary ties to parser moves ([REF_CITE]; Ratna-amount is a likely instrument for buying and parkhi, 1997), but also those of the lexicalized not for describing a company."
"Similarly, there PCFG variety[REF_CITE]."
"Even more crucially, the bilexical dependen- statistical approaches.[REF_CITE]uses cies involve head-modifier relations (hereafter wide &quot;bag-of-words&quot; contexts with a naive referred to simply as &quot;head relations&quot;)."
"The in- Bayes classifier.[REF_CITE]also uses tuition behind the lexicalization of a grammar wide context, but incorporates the one-sense-formalism is to capture lexical items&apos; idiosyn- per-discourse and one-sense-per-collocation cratic parsing preferences."
"The intuition be- constraints, using an unsupervised learn-hind using heads as the members of the bilex- ing technique."
The supervised technique in ical relations is twofold.
"First, many linguis-[REF_CITE]has a more specific notion tic theories tell us that the head of a phrase of context, employing not just words that can projects the skeleton of that phrase, to be filled appear within a window of Ik, but crucially in by specifiers, complements and adjuncts; words that abut and fall in the ~2 window such a notion is captured quite directly by of the target word."
"More recently,[REF_CITE]a formalism such as LTAG (Joshi and Sch- has shown how syntactic context, and depen-abes, 1997)."
"Second, the head of a phrase usu- dency structures in particular, can be suc-ally conveys some large component of the se- cessfully employed for word sense disambigua-mantics of that phraseJ"
"In this way, using tion.[REF_CITE]have shown head-relation statistics encodes a bit of the that by employing a fairly simple and some-predicate-argument structure in the syntac- what ad-hoc unsupervised method of WSD us-tic model."
"While there are cases such as John ing a WordNet-based similarity heuristic, they was believed to have been shot by Bill where could enhance PP-attachment performance to structural preference virtually eliminates one a significantly higher level than systems that of the two semantically plausible analyses, it made no use of lexical semantics (88.1% accu-is quite clear that semantics--and, in particu- racy)."
"Most recently,[REF_CITE], lar, lexical head semantics--play a very im- the authors made use of head-driven bilexi-portant role in reducing parsing ambiguity. cal dependencies with syntactic relations to (See[REF_CITE], pp. 207ff. , for an excel- attack the problem of generalized word-sense lent discussion of structural vs. semantic pars- disambiguation, precisely one of the two prob-ing preferences, including the above John was lems we are dealing with here. believed.., example.)"
Another motivation for incorporating word 3 The Model senses into a statistical parsing model has 3.1 Overview been to ameliorate the sparse data prob-lem.
"Inspired by the PP-attachment work of The parsing model we started with was ex-[REF_CITE], we use Word- tracted from BBN&apos;s SIFT system (Miller et Net vl.6[REF_CITE]as our seman- al., 1998), which we briefly present again here, tic dictionary, where the hypernym structure using examples from Figure 1 to illustrate the provides the basis for semantically-motivated model&apos;s parameters.3 soft clusters.2 We discuss this benefit of word The model generates the head of a con-senses and the details of our implementation stituent first, then each of the left- and right-further in Section 4. modifiers, generating from the head outward, using a bigram model of node labels."
"Here are 2.2.2 Word-sense disambiguation the first few elements generated by the model While there has been much work in this area, for the tree of Figure 1: let us examine the features used in recent 1. S and its head word and part of speech, 1Heads originated this way,but it has become nec-essary to distinguish &quot;semantic&quot; heads, such as nouns caught- VBD. and verbs, that correspond roughly to predicates and arguments, from &quot;functional&quot; heads, such as deter- 2."
"The head constituent of S, VP. miners, INFL&apos;s and complemeutizers, that correspond roughly to logical operators or are purely syntactic el- 3."
"The head word of the VP, caught-VBD. ements."
"In this paper, we almost always intend &quot;head&quot; to mean &quot;semantic head&quot;. 4."
"The premodifier constituent ADVP. 2Soft clusters are sets where the elements have weights indicating the strength of their membership 3Webegan with the BBN parser because its authors in the set, which in this case allows for a probability were kind enough to allow us to extent it, and because distribution to be defined over a word&apos;s membership its design allowedeasy integration with our existing in all the clusters."
WordNet code. 5.
"The head word of the premodifier ADVP, Figure 1."
"For brevity, we omit the smooth-also-RB. ing details of BBN&apos;s model (see[REF_CITE]for a complete description); we note that 6."
The premodifier constituent NP. all smoothing weights are computed via the 7.
"The head word of the premodifier NP, technique described[REF_CITE]. boy-NN."
The probability of generating p as the root label is predicted conditioning on only 8.
"The +END+ (null) postmodifier con- +TOP+, which is the hidden root of all parse stituent of the VP. trees: This process recurses on each of the modifier constituents (in this case, the subject NP and P(Pl +TOP+), e.g., P(S"
I + TOP+). (2) the VP) until all words have been generated. (Note that many words effectively get gener-
"The probability of generating a head node h ated high up in the tree; in this example sen- with a parent p is tence, the last words to get generated are the two the&apos;s ) P(hlp), e.g., P(VP IS). (3)"
"More formally, the lexicalized PCFG that sits behind the parsing model has rules of the The probability of generating a left-modifier l~ form is"
"P ~ LnLn-I&quot;&quot;L1HRI&quot;&quot;Rn-iRn (1) PL(Iz Ilz-l,p,h, wh),, e.g., (4) where P, H, L, and .P~ are all lexicalized non-"
"PL (NP IADVP, S, VP, caught) terminals, i.e., of the form X(w, t, fl, where X when generating the NP for NP(boy-NN), and is a traditional CFG nonterminal and (w, t, f/ the probability of generating a right modifier is the word-part-of.-speech-word-feature triple r; is that is the head of the phrase denoted by X. 4 The lexicalized nonterminal H is so named be-cause it is the head constztuent, where P inher-"
"PR(r~ Iri-l,p, h, Wh), e.g., (5) its its head triple from this head constituent."
"I + BEGIN+, VP, VBD, caught)"
"The constituents labeled L~ and .R~ are left-and right-modifier constituents, respectively. when generating the NP for NP(ball-NN). 5 The probabilities for generating lexical el- 3.2 Probability structure of the ements (part-of-speech tags, words and word original model features) are as follows."
"The part of speech We use p to denote the unlexicalized nontermi- tag of the head of the entire sentence, th, is nal corresponding to P, and similarly for li, ri and h."
"We now present the top-level genera- 5Thebidden nonterminal+BEGIN+ isusedto pro-tion probabilities, along with examples from vide a convenient mechanismfor determining the ini-tial probability of the underlying Markov process gen- 4The inclusion of the word feature in the BBN erating the modifying nonterminals; the hidden non-model was due to the work described in (Weischedel terminal +END+ is used to provide consistency to the et al., 1993), where word features helped reduce part underlying Markov process, i.e., so that the probabil-of speech ambiguity for unknown words. ities of all possible nonterminal sequences sum to 1. computed conditioning only on the top-most symbol p:6"
"P(th IP). (6) Part of speech tags of modifier constituents, tt, and tri, are predicted conditioning on the modifier constituent lz or ri, the tag of the head constituent, th, and the word of the head constituent, Wh:"
"P(tt, Ill, th, Wh) and P(tr, [ri, th, Wh). (7) The head word of the entire sentence, Wh, is predicted conditioning only on the top-most symbol p and th."
"P(Wh[th,p). (8) Head words of modifier constituents, wl, and wr,, are predicted conditioning on all the con-text used for predicting parts of speech in (7), as well as the parts of speech themsleves"
"P(wt, ]tt,, li, th, Wh) and P(wr, ]try, r~, th, Wh). (9)"
"The word feature of the head of the entire sen-tence, fh, is predicted conditioning on the top-most symbol p, its head word, wh, and its head tag, th:"
"P(fh [Wh,th,p). (10)"
"Finally, the word features for the head words of modifier constituents, fz, and fr,, are pre-dicted conditioning on all the context used to predict modifier head words in (9), as well as the modifier head words themselves:"
"P(ft, Iknown(wt, ), tt~,li, th, Wh) (11) and P(fr, Iknown(w~,), tr,, ri, th, Wh) where known(x) is a predicate returning true if the word x was observed more than 4 times in the training data."
The probability of an entire parse tree is the product of the probabifities of generating all of 4 Word-sense Extensions to the
"The desired output structure of our com-bined parser/word-sense disambiguator is a standard, Treebank-style parse tree, where the words not only have parts ef speech, but also WordNet synsets."
Incorporating synsets into the lexical part of the model is fairly straight-forward: a synset is yet another element to be generated.
The question is when to generate it.
"The lexical model has decomposed the genera-tion of the (w, t, f) triple into three steps, each conditioning on all the history of the previ-ous step."
"While it is probabilistically identical to predict synsets at any of the four possible points if we continue to condition on all the history at each step, we would like to pick the point that is most well-founded both in terms of the underlying linguistic structure and in terms of what can be well-estimated."
"In Sec-tion 2.2.1 we mentioned the soft-clustering as-pect of synsets; in fact, they have a duality."
"On the one hand, they serve to add specificity to what might otherwise be an ambiguous lexi-cal item; on the other, they are sets, clustering lexical items that have similar meanings."
"Even further, noun and verb synsets form a con-cept taxonomy, the hypernym relation forming a partial ordering on the lemmas contained in WordNet."
"The former aspect corresponds roughly to what we as human listeners or read-ers do: we hear or see a sequence of words in context, and determine incrementally the par-ticular meaning of each of those words."
"The latter aspect corresponds more closely to a mental model of generation: we have a desire or intention to convey, we choose the appropri-ate concepts with which to convey it, and we realize that desire or intention with the most felicitous syntactic structure and lexical real-izations of those concepts."
"As this is a genera-tive model, we generate a word&apos;s synset after generating the part of speech tag but before generating the word itself/ the elements of that parse tree, where an el-"
"The synset of the head of the entire sen-ement is either a constituent label, a part of speech tag, a word or a word feature."
"We ob- tence, Sh is predicted conditioning only on the tain maximum-likelihood estimates of the pa- top-most symbol p and the head tag, th: rameters of this model using frequencies gath-"
"P(Sh[th,p). (12) ered from the training data. 6This is the one place where we have altered the We accordingly changed the probability of original model, as the lexical components of the head of the entire sentence were all being estimated incor- 7We believe that synsets and parts of speech are rectly, causing an inconsistency in the model."
"We have largely orthogonal with respect to tlieir lexical infor-corrected the estimation of th, Wh and fh in our im- mation, and thus their relative order of prediction was plementation. not a concern. generating the head word of the entire sen- + ,Xn+3P(Wm, ISm,,tm,) tence to be ÷ ~n+4P(Wm, ]Sin,)"
"P(Wh ISh, th,p). (13) where once again, @i(Sh) is the zth hypernym"
The probability estimates for (12) and (13) are of Sh.
"For both the word and synset prediction not smoothed. models, by backing off up the hypernym chain, The probability model for generating there is an appropriate confiation of similar synsets of modifier constituents mi, complete head relations."
"For example, if in training the with smoothing components, is as follows: verb phrase [strike the target] had been seen, if the unseen verb phrase [attack the target] ap-peared during testing, then the training from P(Sm, Itin,, m~,Wh,Sh) = (14) the semantically-similar training phrase could be used, since this sense of attack is the hy- ~0P(Sm, Itm,, m,, w~, sh) pernym of this sense of stroke. + AlP(SIn, Itm,,m,,sh) Finally, we note that both of these synset- + A2P(sm, It~,,rn,,@l(Sh)) and word-prediction probability estimates . . . contain an enormous number of back-off lev-els for nouns and verbs, corresponding to the + )~n+llS(Sm, [tm,,mi,@n(Sh)) head word&apos;s depth in the synset hierarchy."
"A + ~+2P(Sm, Itin,, m,) valid concern would be that the model might be backing off using histories that are fax too + ~n+3P(Sm, ltm,) general, so we experimented with limiting the where @&apos;(Sh) is the ith hypernym of Sh."
"The hypernym back-off to only two, three and four WordNet hypernym relations, however, do not levels."
"This change produced a negligible dif-form a tree, but a DAG, so whenever there are ference in parsing performance,s multiple hypernyms, the uniformly-weighted mean is taken of the probabilities condition- 5 A New Approach, A New Data ing on each of the hypernyms."
"That is, Set"
"Ideally, the well-established gold standard for P(Sm, It~,,mi, @3(Sh)) = (15) n syntax, the Penn Treebank, would have a parallel word-sense-annotated corpus; unfor- 1 Z P(Sm,Itm,,m~,@~(sh)) n k=l tunately, no such word-sense corpus exists."
"However, we do have SemCor (Miller et al., when@~(~h) = (~(~h), ..., @~(sh)}. 1994) , where every noun, verb, adjective and Note that in the first level of back-off, we no adverb from a 455k word portion of the Brown longer condition on the head word, but strictly Corpus has been assigned a WordNet synset. on its synset, and thereafter on hypernyms of While all of the Brown Corpus was anno-that synset; these models, then, get at the tated in the style of Treebank I, a great deal heart of our approach, which is to abstract was also more recently annotated in Tree-away from lexical head relations, and move bank II format, and this corpus has recently to the more general lexico-semantic relations, been released by the Linguistic Data Con-here represented by synset relations. sortium.9 As it happens, the intersection be-"
"Now that we generate synsets for words us- tween the Treebank-II-annotated Brown and ing (14), we can also change the word genera-"
"SemCor comprises some 220k words, most of tion model to have synsets in its history: which is fiction, with some nonfiction and hu-mor writing as well."
"P(w~, Ism,, t~,, m,, Wh, Sh) = (16) We went through all 220k words of the cor-pora, synchronizing them."
"That is, we made ),0P(wm, I sin,, t~, mi, wh) sure that the corpora were identical up to + ~lP(wm, Isin,, t~, mi, Sh) the spelling of individual tokens, correcting all + A2P(wm, ISm,,tm,,mi,@l(sh)) 8We aim to investigate the precise effects of our ..o back-off strategy in the next version of our combined parsing/WSD model. +"
"A~,+lP(wm, Is~,,tm,,m,,@~(Sh)) 9We were given permission to use a pre-release ver- + ~,~+2.P(w~, Isin,, tin,, m,) sion of this Treebank II-style corpus. tokenization and sentence-breaking discrepan- all been assigned the same synset as was as-cies."
This correcton task ranged from the sim- signed the collocation as a whole.
"This is not ple, such as connecting two sentences in one as unreasonable as it may sound; for exam-corpus that were erroneously broken, to the ple, vice_president is a lemma in WordNet middling, such as joining two tokens in Sem- and appears in SemCor, so the merged corpus Cor that comprised a hyphenate in Brown, to has instances where the word president has the difficult, such as correcting egregious parse the synset vice president l, but only when annotation errors, or annotating entire sen- preceded by the word vice."
The cost of this tences that were omitted from SemCor.
"In par- decision is an increase in average polysemy. ticular, the case of hyphenates was quite fre-quent, as it was the default in SemCor to split 6 Training and Decoding up all such words and assign them their indi- Using this merged corpus, actual training of vidual word senses (synsets)."
"In general, we at- our model proceeds in an identical fashion tempted to make SemCor look as much as pos- to training the non-WordNet-extended model, sible like the Treebank II-annotated Brown, except that for each lexical relation, the hy-and we used the following guidelines for as- pernym chain of the parent head is followed signing word senses to hyphenates: 1. Assign the word sense of the head of the hyphenate."
"E.g., both twelve-foot and ten-foot get the word sense of foot_l (the unit of measure equal to 12 inches). 2."
"If there is no clear head, then attempt to annotate with the word sense of the hypernym of the senses of the hyphenate components."
"E.g., U.S.-Soviet gets the word sense of country_2 (a state or na-tion). 3."
"If options 1 and 2 are not possible, the hyphenate is split in the Treebank II file. 4."
"If the hyphenate has the prefix non- or anti-, annotate with the word sense of that which follows, with the understand-ing that a post-processing step could re-cover the antonymous word sense, if nec-essary."
"After three passes through the corpora, they were perfectly synchronized."
We are seeking permission to make this data set available to any who already have access to both SemCor and the Treebank II version of Brown.
"After this synchronization process, we merged the word-sense annotations of our cor- to derive counts for the various back-off levels described in Section 4."
We also developed a &quot;plug-&apos;n&apos;-play&quot; lexical model system to facili-tate experimentation with various word- and synset-prediction models and back-off strate-gies.
"Even though the model is a top-down, gen-erative one, parsing proceeds bottom-up."
"The model is searched via a modified version of CKY, where candidate parse trees that cover the same span of words are ranked against each other."
"In the unextended parsing model, the cells corresponding to spans of length one are seeded with (w,t,f) triples, with every possible tag t for a given word zv (the word-feature f is computed deterministically for w); this step introduces the first degree of ambi-guity in the decoding process."
"Our WordNet-extended model adds to this initial ambiguity, for each cell is seeded with (w, t, f, s) quadru-ples, with every possible synset s for a given word-tag pair."
"During decoding, two forms of pruning are employed: a beam is applied to each cell in the chart, pruning away all parses whose ranking score is not within a factor of e -k of the top-ranked parse, and only the top-ranked n sub-trees are maintained, and the rest are pruned away."
"The &quot;out-of-the-box&quot; BBN program uses rected SemCor with the tokens of our cor- values of-5 and 25 for k and n, respectively. rected version of the Treebank II Brown data."
"We changed these to default to -9 and 50, be-"
"Here we were forced to make two decisions. cause generating additional unseen items (in First, SemCor allows multiple synsets to be as- our case, synsets) will necessarily lower inter-signed to a particular word; in these cases, we mediate ranking scores. simply discard all but the first assigned synset."
"Second, WordNet has collocations, whereas Treebank does not."
"To deal with this dis- 7.1 Parsing parity, we re-analyze annotated collocations Initially, we created a small test set, blindly as a sequence of separate words that have choosing the last 117 sentences, or 1%, of"
"Norm, &quot;r&quot;* 68.6 3.83 25.9 44.871.2 pled every 100th sentence to create a new liT-sentence test set that spanned the entire range WN-ext, &quot;r&quot; 69.7 71.5 3.77 i25.0 45.7 of styles in the 220k words; we put all other Norm,bal 84.4 1.00 73.5 83.882.0 sentences in the training set.1°"
"For the sake of WN-ext, bal 80.5 82.2 1.43 !68.4 78.6 comparison, we present results for both test model performs at roughly the same level Figure 2: Head rules are tuned for syntax, not as the unextended version with respect to parsing--a shave better with the &quot;r&quot; test set, and slightly worse on the balanced test set."
"Recall, however, that this is in spite of adding more intermediate ambiguity during the de-coding process, and yet using the same beam width."
"Furthermore, our extensions have oc-curred strictly within the framework of the original model, but we believe that for the true advantages of synsets to become appar-ent, we must use trilexical or even tetralex- ~°Werealize these are very small test sets, but we presume they are large enough to at least give a good indicator of performance on the tasks evaluated."
"They were kept smallto allow for a rapid train-test-analyze cycle, z.e.,they wereactually used as developmenttest sets."
"With the completion of these initial experiments, we are going to designate a proper three-way divsion of training, devtest and test set of this new merged corpus."
"UThe scoresin the rows labeled Norm, &quot;r&quot;, indicat-ing the performance of the standard BBN model on the &quot;r&quot; test set, are actually scores based on 116 of the 117 sentences, as one sentence did not get parsed due to a timeout in the program. semantics. ical dependencies."
"Whereas such long-range dependencies might cripple a standard gen-erative model, the soft-clustering aspects of synsets should offset the sparse data problem."
"As an example of the lack of such dependen-cies, in the current model when predicting the attachment of [bought company [for million]], there is no current dependence between the verb bought and the object of the preposition million--a dependence shown to be useful in virtually all the PP attachment work, and par-ticularly[REF_CITE]."
"Re-lated to this issue, we note that the head rules, which were nearly identical to those used[REF_CITE], have not been tuned at all to this task."
"For example, in the sentence in Fig-ure 2, the subject Jane is predicted condition-ing on the head of the VP, which is the modal wdl, as opposed to the more semantically-content-rich kill."
"So, while the head relations provide a very useful structure for many syn- tactic decisions the parser needs to make, it is ~2This is partly an unfair comparison, then, since ours is a larger model, but wewanted to givethe stan- quite possible that the synset relations of this dard model everyconceivableadvantage. model would require additional or different de- balanced test set. gram delivers a synset for a WordNet part of pendencies that would help in the prediction of correct synsets, and in turn help further re-duce certain syntactic ambiguities, such as PP attachment."
This is because the &quot;lightweight semantics&quot; offered by synset relations can pro-vide selectional and world-knowledge restric-tions that simple lexicalized nonterminal rela-tions cannot.
The WSD results on the balanced test set are shown in Table 2.
A few important points must be made when evaluating these results.
"First, almost all other WSD approaches are aimed at distinguishing homonyms, as opposed to the type of fine-grained distinctions that can be made by WordNet."
"Second, almost all other WSD approaches attempt to disambiguate a small set of such homonymous terms, whereas here we are attacking the generahzed word-sense disambiguation problem."
"Third, we call attention to the fact that SemCor has a re-ported inter-annotator agreement of 78.6% overall, and as low as 70% for words with pol-ysemy of 8 or[REF_CITE], so it is with this upper bound in mind that one must consider the precision of any generalized WSD system."
"Finally, we note that the scores in Table 2 are for exact synset matches; that speech that is different from our gold file, we have called this a recall error, as this is con-sistent with all other WSD work, where part of speech ambiguity is not a component of an algorithm&apos;s precision."
This paper represents a first attempt at a combined parsing/word sense disambiguation model.
"Although it has been very useful to work with the BBN model, we are currently implementing and hope to augment a more state-of-the-art model, vzz.,"
Models 2 and 3[REF_CITE].
"We would also like to explore the use of a more radical model, where nonter-minals only have synsets as their heads, and words are generated strictly at the leaves."
"We would also like to incorporate long-distance context in the model as an aid to WSD, a demonstrably effective feature in virtually all the recent, statistical WSD work."
"Also, as mentioned earlier, we believe there are several features that would allow significant parsing improvement."
"Finally, we would like to inves-tigate the incorporation of unsupervised meth-ods for WSD, such as the heuristically-based methods[REF_CITE]and[REF_CITE], and the theoretically purer bootstrapping method[REF_CITE]."
"Bolstered by the success of (Stetina is, if our program delivers a synset that is, say, and[REF_CITE]),[REF_CITE]and especially the hypernym or sibling of the correct answer, no credit is given."
"While it is tempting to compare these re-sults to those[REF_CITE], who re-ported 79.4% overall accuracy on a different, larger test set using their non-discourse model, we note that that was more of an upper-bound study, examining how well a WSD al-gorithm could perform if it had access to gold-standard-perfect parse trees33"
"By way of fur-[REF_CITE], we believe there is great promise the incorporation of word-sense into a probabilistic parsing model."
"I would like to greatly acknowledge the re-searchers at BBN who allowed me to use and abuse their parser and who fostered the begin-ning of this research effort: Scott Miller, Lance Ramshaw, Heidi Fox, Sean Boisen and Ralph ther comparison, that algorithm has a feature Weischedeh Thanks to Michelle Engel, who space similar to the synset-prediction compo- helped enormously with the task of prepar-ing the new data set."
"Finally, I would like to 1nitis not clearhowor whythe results[REF_CITE]exceededthe reported inter-annotator agree- thank my advisor Mitch Marcus for his invalu-ment of the entire corpus. able technical advice and support."
"Message Understanding Conference (MUC-7), Kenneth Church. 1988."
A stochastic parts pro-
"Washington, D.C. gram and noun phrase parser for unrestricted"
Adwait Ratnaparkhi. 1997.
A linear observed text.
"In Second Conference on Applzed Natu- time statistical parser based on maximum en-ral Language Processing, pages 136-143, Austin, tropy models."
In Proceedzngs of the Second Texas.
Conference on Empzmcal Methods zn Natural M. Collins and J. Brooks. 1995.
"Prepositional Language Processing, Brown University, Prov-phrase attachment through a backed-offmodel. idence, Rhode Island."
Long sentence analysis has been a critical problem because of high complexity.
"This pa-per addresses the reduction of parsing com-plexity by intra-sentence segmentation, and presents maximum entropy model for deter-mining segmentation positions."
"The model featureslexicalcontexts of segmentation posi-tions, giving a probability to each potential position."
Segmentation coverage and accu-racy of the proposed method are 96% and 88% respectively.
The parsingefficiencyisim-proved by 77% in time and 71% in space. 1 Introduction Long sentence analysis has been a critical problem in machine translation because of high complexity.
"In EBMT (example-based machine translation),the longer a sentence is, the less possible it is that the sentence has an exact match in the translationarchive, and the lessflexiblean EBMT system willbe[REF_CITE]."
"In idiom-based ma-chine translati[REF_CITE], long sentence parsing isdifficultbecause more resourcesare spent during idiom recognitionphase as sen-tence length increases."
"A parser is often un-able to analyze long sentences owing to their complexity, though they have no grammatical errors[REF_CITE]."
This section describes the features.
"From a corpus, contextual evidences of segmentation positions are collected and combined, result-ing in features."
The features are used in iden-tifying potential segmentation positions and included in the model.
"A sentence is constructed by the combina-tion of words, phrases, and clauses under the well-defined grammar."
A sentence can be seg-mented into shorter segments that correspond to the constituents of the sentence.
"That is, segments correspond to the nonterminal sym-bols of the context-free grammar [Footnote_1]."
"1Nonterminal symbols include the ones for phrases, such as NP (noun phrase) and VP (verb phrase),"
The posi- tion of a word is called segmentable posi-tion that can be a starting position of a spe-cific segment.
"Though the analysis complexity can be re-duced by segmenting a sentence, there is a mis-segmentation risk that causes pars-ing failures."
A segmentation can be called safe segmentation that results in a coherent blocks of words.
"In English-Korean transla-tion, safe segmentation is defined as the one which generates safe segments."
"A segment is safe, when there is a syntactic category sym-bol N P dominating the segment and the seg-ment can be combined with adjacent segments under a given grammar."
"In Figure 1, (a) is an unsafe segmentation because the second seg-ment cannot be analyzed into one syntactic category, resulting in parsing failure."
"By the safe segmentation (b), the first segment cor-responds to a noun phrase and the second to a verb phrase, so that we can get a correct analysis result. (a) IThe students I who study hard will pass the exam] (b) IThe students who study hard"
A lexical context of a word includes seven-word window: three to the left of a word and three to the right of a word and a word itself.
"It also includes the part-of-speeches of these words, subcategorization information for two words to the left, and position value."
"The position value posi_v of the ith word wi is cal-culated as pos _v = r × R], where n is the number of words and R 2 repre-sents the number of regions in the sentence."
"Region is the sequentially ordered block of and the ones for clauses like RLCL (relative clause), SUBCL (subordinate clause). words in a sentence, and posi_v represents the constraint."
The operation join is defined as region in which a word lies.
"It is included to reflect the influence of the position of a word (al,..., an) • (bl,.-., bn) = (C1,..., Ca), on being a segmentation position."
An example of a training data and a re-sulting lexical context is shown in Figure 3.
A symbol &apos;#&apos; represents a segmentation posi-tion marked by human annotators.
"Therefore, the lexical context of word when includes the value 1 for attribute s_position? and follow-ings: three words to the left of when (became, terribly, and worried) and part-of-speeches of each word (VERB ADV ADJ), three words • Input: a set of active lexical contexts LCw = {lcl... lcn} for word w, where lcc/= (al,..., an). • Output: a set of lexical contextual constraints LCCw = {/ccl .../cck}, where lcc/= (C1,..., Cn). 1. Initialize LCCw = 0 to the right (they, saw, and what) and part-of-speeches (PRON VERB PRON), subcat- 2."
"Do the followings for each l~ E LCw egorization information for two words to the (a) For all lcj(j # i), Count(lcj) = # of left (0 1), and position value (2). matched attributes with Ic/ (b) max_cnt = arg maxlc¢eLC."
Count( Icj) Of course his parents became terribly worried #when they saw what was happening )
"For all lcj, where Count(lcj) = max..cnt, (c to Atzel. data is required."
"To alleviate this prob-lem, we generate lexical contextual con-stralnts by combining lexical contexts and We collect statistics for them."
To generate lex- 0 otherwise collect the statistics for each Icc.
"The fre- quency of each lcc is counted as the number ical contextual constraints and to identify of lexical contexts that satisfy the consistency segmentable positions, we define two oper- operation with the lcc. ations join (E9) and consistency (=)."
"Let n (al,...,an) and (bl,...,bn) be lexical con-texts and (C1,... ,On) be lexical contextual 167 i=1"
Identifying segmentable positions is per-formed with the consistency operation with the lexical context of word w and lcc E LCCw.
The word whose lexical context is consistent with lcc is identified as a segmentable posi-tion. 4 Determination Schemes of
Segmentation positions are determined through two steps: identifying segmentable positions and selecting the most appropriate position among them.
Segmentable positions are identified using the consistency operation.
Maximum entropy model in Section 2 gives a probability to each position.
Segmentation performance is measured in terms of coverage and accuracy.
"Coverage is the ratio of the number of actually segmented sentences to the number of segmentation tar-get sentences that are longer than ot words, where o~is a fixed constant distinguishing long sentences from short ones."
Accuracy is evalu-ated in terms of the safe segmentation ratio.
They are defined as follows: # of actually segmented Sent. coverage = ~ of Sent. to be segmented (3) # of Sent. with safe segmentation accuracy = ~ of actually segmented Sent. (a)
No contextual information is used in identify-ing segmentable positions.
They are empiri-cally identified.
A word that is tagged as a segmentation position more than 5 times is identified as a segmentable position.
"A set of segmentable positions, 9, is as follows. ~D= {wi [wi is tagged as segmentation position and #(tagged wi) &gt;_ 5}"
"In order to select the most appropriate po-sition, the segmentation appropriateness of each position is evaluated by the probability of word wi: # of tagged wi p(Wi) = # of wi in the corpus p(wi) represents the tendency that word wi will be used as a segmentation position."
A segmentation position w. is selected as the one that has highest p(wi) value: w. = arg max p(wi). wiE~
This scheme serves as a baseline for comparing the segmentation performance of the models.
Lexical contextual constraints are used in identifying segmentable positions.
"Compared with the baseline scheme, this scheme con-siders contextual information of a word."
All consistent words with the defined lexical con-textual constraints form a set of segmentable positions 79.
"The maximum likelihood principle gives a probability distribution for p(y Ilcc~), where y E {0, 1}."
"Segmentation appropriateness is evaluated by p(1 Ilcew,)."
A position with the highest p(1 I lcc~) becomes a segmentation position: w. = arg max p(1 I/CCwi). wi E~
"Contextual Constraints with Word Sets Due to insufficient training samples for con-structing lexical contextual constraints, some segmentable positions may not be identified."
To alleviate this problem we introduce word sets whose elements have linguistically similar features.
"We define four word sets: coordinate conjunction set, subordinate conjunction set, interogative set, auxiliary verb set."
The cate-gories of word sets and the examples of their members are shown in Table 1.
"Coordinate conjunctions haveonly 3 mem-bers, but they frequently apprear in long sen-tences."
"Subordinate conjunctions have 25 members, interogatives 5 members, and aux-iliary verbs have 12 members now."
The words belonging to each word set are treated equally.
"Lexical contextual constraints are constructed for words and word sets, so the statistics is collected for both of them."
"The set of seg-mentable positions T~is defined somewhat dif-ferently as: :D = {wi, wsj I (Icc,v, -= lcc~,) = 1 or (Icws~=--IcC.ws~)= 1}, 5.2 where wsj denotes a word set to which the jth"
In word in a sentence belongs.
Maximum Entropy Model
"We construct the corpus from two different A sentence longer than vt words is con- domains, where the sentences longer than 15 sidered as the segmentation target sentence, -words are extracted3."
The training portion is where c~is set to 12.
Table 3 compares seg-used to generate lexical contextual constraints mentation performance for each determina-and to collect statistics for maximum entropy tion scheme. model construction.
From high school[REF_CITE]sentences are tagged with segmen-
Table 3: Segmentation performance of the de-tation positions by human.
"Two people who termination schemes of segmentation position. have some knowledge about English syntactic structures read sentences, and marked words as segmentation positions where they paused."
"After generating lexical contextual con-straints, we constructed the maximum en-tropy model p(ylx), where x is a lexical con-textual constraint and y E {0,1}."
The model incorporates features that occur more than 5 times in the training data. 3626 candidate fea-tures were generated without word sets and 3878 features with word sets.
"In Table 2,"
Determination Coverage/ SC Schemes Accuracy (%)[REF_CITE]/77.6 0.776[REF_CITE].7/89 0.808
"By the comparison of the baseline scheme training time and the number of active fea- with others, the accuracy is observed to de-tures of the model are shown. pend on the context information."
Word sets Segmentation performance is evaluated us- are helpful for increasing coverage with less ing test portion that consists of 1800 sentences degradation of accuracy.
Each scheme has su-ffrom two domains: high school English texts periority in terms of the different measures. and the Byte Magazine.
"But in terms of applicability to practical sys-tems, the third scheme is best for our purpose. 3The sentences with commas are excluded because Table 4 shows the segmentation performance commais an explicit segmentation position."
Segments of the scheme using LCC with word sets. resulting from a segmentation at commas may be the manageable-sized ones.
"Our work is to segment long SU value for the sentences from the same sentences without explicit segmentation positions. domain as training data is about 0.88, and"
Coverage/ I 8C Length Accuracy(%)
"I 15~19 99.0/95.9 0.95[REF_CITE]~24 100/94.0 0.94 English Text 0.78 25~29 96.0/81.3 30~ 0.6g 100/67.5 15,-,19 0.8794.0/92.6"
"Though they slightly differ be-tween test domains, about 87% of long sen-tences can be parsed with less complexity and without causing parsing failures."
It suggests that the intra-sentence segmentation method can be utilized for efficient parsing of the long sentences.
Parsing efficiency is generally measured by the required time and memory for parsing.
"In most cases, parsing sentences longer than 30 words could not complete without intra-sentence segmentation."
"Therefore, the parsing is performed for the sentences longer than 15 and less than 30 words."
The efficiency improvement was measured by
"EItime = tunseg -- tseg ,,, x 100, tunseg"
"EImemory = rnunseg --mseg X 100, ~T~unse9 where $unseg and rrbanseg are time and memory during parsing without segmentation and tseg, rnseg are for the parsing with segmentation."
Table 5 summarizes the results.
"By segmenting long sentences into several manageable-sized segments, we can parse long sentences with much less time and space."
The intra-sentence segmentation method based on the maximum entropy model is corn- -pared with other approaches in terms of the segmentation coverage and the improvement of parsing efficiency.
"In[REF_CITE], a sentence is segmented into three segments."
"Though parsing efficiencycan be improved by segmenting a sentence, this method may be applied to only simple sen-tences[Footnote_4]."
4A simple sentence has one subject and one predi-cate.
Long sentences are generally coordi-nate sentences[Footnote_5] or complex sentences6.
5A coordinate sentence results ~om the combina-tion of several simple sentences by the coordinate con-junctions. - 6A complex sentence consists of a main clause and several subordinate clauses.
"They have more than two subjects, so applying this method to such sentences seems to be inap-propriate."
"In[REF_CITE], sentence patterns are used to segment long sentences."
This method improve parsing efficiency by 30% in time and 58% in space.
However collecting sentence patterns requires much hnman efforts and segmentation coverage is only about 36%.
Li&apos;s method[REF_CITE]for sentence segmentation also depends upon manual-intensive pattern rules.
Segmentation cover-age seems to be unsatisfactory for practical machine translation system.
The proposed method can be applied to co-ordinate and complex sentences as well as sim-ple sentences.
It shows segmentation coverage of about 96%.
"In addition, it needs no other human efforts except for constructing training data."
"Human ~.nnotators have only to read sentences and mark segmentation positions, which is more simple than collecting pattern rules or sentence patterns."
We can also get much improved parsing efficiency: about 77% in time and about 71% in space.
means for reducing parsing complexity.
"In International Workshop on paper presents a method for intra-sentence Parsing Technology, September. segmentation based on the maximum entropy Caroline Lyon and Ray Frank. 1995."
The method builds statistical models work Design for a Natural Language Parser. automatically from a text corpus to provide In International Conference on Artificial Neural the segmentation appropriateness for safe seg- Networks.
Tetsura Nasukawa. 1995.
Robust Parsing Based mentation. on Discourse Information.
"Meeting of the A CL, pages 33-46. about 87% of them were benefited from seg-"
David D. Palmer and Marti A. Hearst. 1997. mentation.
The statistical intra-sentence seg-
"Adaptive Multilingual Sentence Boundary mentation method can also relieve human of Computational Linguistics, Disambiguation. the burden of constructing information, such 23(2):241-265. as segmentation rules or sentence patterns."
A. Ratnaparkhi. 1994.
A Simple Introduction
Experiments suggest that the proposed maxi-mum entropy models can be incorporated into to Maximum Entropy Models for Natural Lano gnage Processing.
"Technical report, Institute for Research in Cognitive Science, University of the parser for practical machine translation systems."
Further works can be done in two direc- tions.
"First, studies on recovery mecha-"
J.C. Reynar and A. Ratnaparkhi. 1997.
A Maxi- nisms for unsafe segmentation before parsing seem necessary since ungafe segmentation may cause parsing failures.
"Second, parsing control mechanisms should be studied that exploit the mum Entropy Approach to IdentifyingSentence Boundaries."
"In Proceedings of the Fifth Confer-ence on Applied Natural Language Processing,"
"Snow stands for Sparse Network Of Winnows, and it is intended as a representative of on-line learning algorithms."
The basic component is the Winnow al-gorithm[REF_CITE].
It consists of a single rule called the combined hypothesis.
"More particularly, the Schapire and Singer&apos;s real AdaBoost."
"MH algorithm for multi-class multi-label classification (Schapire and Singer, to appear) has been used."
"As in that paper, very simple weak hypotheses are used."
They test the value of a boolean predicate and make a real-valued prediction based on that value.
"The predicates used, which are the bi-narization of the attributes described in sec-tion 3.2, are of the form &quot;f = v&quot;, where f is a feature and v is a value (e.g: &quot;p-rev&quot;mus_word ="
"Each weak rule uses a single hospital&quot;). feature, and, therefore, they can be seen as linear threshold algorithm with multiplicative weight updating for 2-class problems, which learns very fast in the presence of many bi-nary input features."
"In the Snow architecture there is a winnow node for each class, which learns to separate that class from all the rest."
"During training, each example is considered a positive example for winnow node associated to its class and a negative example for all the rest."
A key point that allows a fast learning is that the winnow nodes are not connected to all features but only to those that are &quot;relevant&quot; for their class.
"When classifying a new example, Snow is similar to a neural network which takes the input features and outputs the class with the highest activation."
"Snow is proven to perform very well in high dimensional domains, where both, the training examples and the target function re-side very sparsely in the feature space[REF_CITE], e.g: text categorization, context-sensitive spelling correction, WSD, etc."
"In this paper, our approach to WSD using Snow follows that[REF_CITE]."
"The main idea of boosting algorithms is to combine many simple and moderately accu-rate hypotheses (called weak classifiers) into a single, highly accurate classifier."
"The weak classifiers are trained sequentially and, con-ceptually, each of them is trained on the ex-amples which were most difficult to classify by the preceding weak classifiers."
These weak simple decision trees with one internal node (testing the value of a binary feature) and two leaves corresponding to the yes/no answers to that test.
"LazyBoosting[REF_CITE], is a simple modification of the AdaBoost."
"MH al-gorithm, which consists of reducing the fea-ture space that is explored when learning each weak classifier."
"More specifically, a small pro-portion p of attributes are randomly selected and the best weak rule is selected only among them."
"The idea behind this method is that if the proportion p is not too small, probably a sufficiently good rule can be found at each iteration."
"Besides, the chance for a good rule to appear in the whole learning process is very high."
"Another important characteristic is that no attribute needs to be discarded and, thus, the risk of eliminating relevant attributes is avoided."
The method seems to work quite well since no important degradation is observed in performance for values of p greater or equal to 5% (this may indicate that there are many irrelevant or highly dependant attributes in the WSD domain).
"Therefore, this modifica-tion significantly increases the efficiency of the learning process (empirically, up to 7 times faster) with no loss in accuracy."
"The DSO corpus is a semantically annotated corpus containing 192,800 occurrences of 121 nouns and 70 verbs, corresponding to the most frequent and ambiguous English words."
This corpus was collected by Ng and colleagues[REF_CITE]and it is available from the Linguistic Data Consortium (LDC)5.
"Therefore, local context attributes have two different corpora, namely Wall Street to be binarized in a preprocess, while the top- Journal (WSJ) and Brown Corpus (BC). ical context attributes remain as binary tests Therefore, it is easy to perform experiments about the presence/absence of a concrete word about the portability of alternative systems in the sentence."
"As a result the number of by training them on the WSJ part and testing attributes is expanded to several thousands them on the BE part, or vice-versa."
"Here- (from 1,764 to 9,900 depending on the partic-inafter, the WSJ part of DSO will be referred ular word). to as corpus A, and the BC part to as corpus B. The binary representation of attributes is At a word level, we force the number of exam- not appropriate for NB and EB algorithms. ples of corpus A and B be the same6 in order Therefore, the 15 local-context attributes are to have symmetry and allow the comparison taken straightforwardly."
"Regarding the binary in both directions. topical-context attributes, we have used the"
"From these corpora, a group of 21 words variants described[REF_CITE]. which frequently appear in the WSD litera-"
"For EB, the topical information is codified as ture has been selected to perform the com- a single set-valued attribute (containing all parative experiments (each word is treated words appearing in the sentence) and the cal-as a different classification problem)."
"These culation of closeness is modified so as to han-words are 13 nouns (age, art, body, car, child, dle this type of attribute."
"For NB, the top-cost, head, interest, line, point, state, thing, ical context is conserved as binary features, work) and 8 verbs (become, fall, grow, lose, but when classifying new examples only the set, speak, strike, tell)."
"Table 1 contains in- information of words appearing in the exam-formation about the number of examples, the ple (positive information) is taken into ac-number of senses, and the percentage of the count."
"In that paper, these variants are called most frequent sense (MF5) of these reference positive Exemplar-based (PEB) and positive words, grouped by nouns, verbs, and all 21 Naive Bayes (PNB), respectively."
PNB and words.
PEB algorithms are empirically proven to per-
"Let &quot;... w-3 w-2 w-1 w W+l w+2 w+3...&quot; The comparison of algorithms has been per-be the context of consecutive words around formed in series of controlled experiments us-the word w to be disambiguated, and p±, ing exactly the same training and test sets. (-3 &lt; i _&lt; 3)be the part-of-speech tag There are 7 combinations of training-test sets of word w±~. Attributes referring to local called: A+B-A+B, A+B-A, A+B-B, A-A, B-context are the following 15: P-3, P-2, B, A-B, and B-A, respectively."
"In this nota- P-l, P+i, P+2, P+3, w-l, W+l, (W-2,W-1), tion, the training set is placed at the left hand (w-i.w+i), side of symbol &quot;-&quot;, while the test set is at the (w+l,w+2), (w-2, W-l, w+l), right hand side."
"For instance, A-B means that(w-i, w+l, w+2), and (w+l,w+2,w+3), where the last seven cor- the training set is corpus A and the test set respond to collocations of two and three is corpus B. The symbol &quot;+&quot; stands for set consecutive words. union, therefore A+B-B means that the train- The topical context is formed by Cl,..., Cm, ing set is A union B and the test set is B. which stand for the unordered set of open class When comparing the performance of two al-words appearing in the sentence7. gorithms, two different statistical tests of sig-"
The four methods tested translate this nificance have been apphed depending on the information into features in different ways. case.
A-B and B-A combinations represent a Snow and LB algorithms require binary fea- single training-test experiment.
"In this cases, the McNemar&apos;s test of significance is used 6This is achievedby ramdomlyreducing the sizeof (with a confidence value of: X1,0.952= 3.842), the largest corpus to the size of the smallest. which is proven to be more robust than a sim- 7The already described set of attributes contains ple test for the difference of tw0_proportions. those attributes used[REF_CITE], with the exception of the morphology of the target word and In the other combinations, a 10-fold cross-the verb-object syntactic relation. validation was performed in order to prevent testing on the same material used for training. • Regarding the portability of the systems, In these cases, accuracy/error rate figures re- very disappointing results are obtained. ported in section 4 are averaged over the re- Restricting to [B results, we observe that sults ofthe 10 folds."
"The associated statistical the accuracy obtained in A-B is 47.1% tests of significance is a paired Student&apos;s t-test while the accuracy in B-B (which can with a confidence value of: t9,0.975 = 2.262. be considered an upper bound for LB in Information about both statistical tests can B corpus) is 59.0%, that is, a drop of be found[REF_CITE]. 12 points."
This experiment explores the effect of a sim-
The following conclusions can be drawn: ple tuning process consisting of adding to the original training set a relatively small sarn- • LB outperforms all other methods in ple of manually sense tagged examples of the all cases.
"Additionally, this superiority new domain."
"The size of this supervised por-is statistically significant, except when tion varies from 10% to 50% of the available comparing LB to the PEB approach in the corpus in steps of 10% (the remaining 50% is cases marked with an asterisk. kept for testing)."
"This set of experiments will • Surprisingly, LB in A+B-A (or A+B-B) be referred to as A+%B-B, or conversely, to does not achieve substantial improvement B+%A-A. to the results of A-A (or B-S) win fact, In order to determine to which extent the the first variation is not statistically sig- original training set contributes to accurately nificant and the second is only slightly disambiguate in the new domain, we also cal-significant."
"That is, the addition of extra culate the results for %A-A (and %B-B), that examples from another domain does not is, using only the tuning corpus for training. necessarily contribute to improve the re- Figure 1 graphically presents the results ob-sults on the original corpus."
This effect is tained by all methods.
"Each plot contains the also observed in the other methods, spe- X+%Y-Y and %Y-Y curves, and the straight cially in some cases (e.g. Snow in A+B-A lines corresponding to the lower bound MFC, vs. A-A) in which the joining of both and to the upper bounds Y-Y and X+Y-Y. training corpora is even counterproduc- As expected, the accuracy of all methods tive. grows (towards the upper bound) as more tun-ing corpus is added to the training set."
"How-SThe second and third column correspond to the train and test sets used by ([REF_CITE]; Ng, ever, the relation between X+%Y-Y and %Y-1997a) Y reveals some interesting facts."
"In plots 2a,"
Accuracy (%) A+B-A+B A+B-A B-B [ A-B A+B-B A-A B-A nouns 46.59±1.08 56.68±2.79 36.49±2.41 59.77±1.44 45.28±1.81 33.97 39.46[REF_CITE].49±1.37 48.74±1.98 44.23±2.67 48.85±2.09 45.96±2.6O 40.91 37.31 total 46.55±0.71 55.94±1.10 45.52±1.27 36.40 38.7153.90±2.01 39.21±1.90 nouns 62.29±1.25 68.89±0.93 55.69±1.94 66.93±1.44 56.17±1.60 36.62 45.99[REF_CITE].87±1.80 57.97±2.8660.18±1.64 64.21±2.26 56.14±2.79 50.20 50.75 total 61.55±1.04 67.25±1.07 55.85±1.81 56.80±1.12 47.6665.86±1.11 41.38 nouns 62.66±0.87 69.45±1 51 56.09±1.12 56.17±1.80 42.15 50.5369.38±1.24
"Furthermore, in plots la, 2b, and 3b a degradation on the accuracy performance is observed."
"Summarizing, these six plots show that for Naive Bayes, Exemplar Based, and Snow methods it is not worth keep-ing the original training examples."
"Instead, a better (but disappointing) strategy would be simply using the tuning corpus."
"However, this is not the situation of Lazy-Boosting (plots 4a and 4b), for which a mod-erate (but consistent) improvement of accu-racy is observed when retaining the original training set."
"Therefore, Lazy[3oosting shows again a better behaviour than their competi-tors when moving from one domain to an-other."
"The bad results about portability could be ex-plained by, at least, two reasons: 1) Corpus have been generated from the DSO corpus, by equilibrating the number of examples of each sense between A and B parts."
"In this way, the first difficulty is artificially overrided and the algorithms should be portable if examples of both parts are quite similar."
"Regarding portability, we observe a signifi-cant accuracy decrease of 7 and 5 points from A-A to B-A, and from B-B to A-B, respec-tively9."
"That is, even when the sazne distri-bution of senses is conserved between training and test examples, the portability of the su-pervised WSD systems is not guaranteed."
These results imply that examples have to be largely different from one corpus to an-other.
"By studying the weak rules generated by kazyBoosting in both cases, we could cor- A and [3 have a very different distribution of roborate this fact."
"On the one hand, the type senses, and, therefore, different a-priori bi- of features used in the rules were significantly ases; 2) Examples of corpus A and [3 con- different between corpora, and, additionally, tain different information, and, therefore, the there were very few rules that apply to both learning algorithms acquire different (and non sets; On the other hand, the sign of the pre-interchangeable) classification cues from both diction of many of these common rules was corpora,. somewhat contradictory between corpora."
"The first hypothesis is confirmed by observ-ing the bar plots of figure 2, which contain the distribution of the four most frequent senses 9This lossin accuracyis not as important as m the of some sample words in the corpora"
"A and first experiment,due to the simplificationprovidedby B. respectively."
In order to check the second the balancingofsensedistributions.
"According to our experiments, it seems that"
Accuracy (%) A+B-A+B A+B-A A+B-B A-A B-B A-B B-A nouns 48.75±0.91 48.90±1.69 48.61±0.96 48.87±1 68 48.61±0.96 48.99 48.99[REF_CITE].22±1 68 48.22±1.90 48.22±3 06 48.22±1.90 48.22±3.06 48.22 48.22 total 48.55±1 16 48.64±1.04 48.46±1.21 48.62±1.09 48.46±1.21 48.70 48.70 nouns 62.82±1.43 64.26±2.07 61.38±2.08 63.19±1.65 60.65±1.01 53.45 55.27[REF_CITE].82±1.53 69.33±2.92 64.32±3.27 68.51±2.45 63.49±2.27 60.44 62.55 total 64.35±1.16 66.20±2.12 62.50±1.47 65.22±1.50 61.74±1.18 56.12 58.05 Table 3: Accuracy results (5=standard deviation) of LazyBoostingon the sense-balanced corpora
"Furthermore, these results are in contradic- • Since most of the knowledge learned from tion with the idea of &quot;robust broad-coverage a domain is not useful when changing WSD&quot; introduced[REF_CITE], in which a to a new domain, further investigation is supervised system trained on a large enough needed on tuning strategies, specially on corpora (say a thousand examples per word) those using non-supervised algorithms. ~hould provide accurate disambiguation on • It is known that mislabelled examples re-any corpora (or, at least significantly better sulting from annotation errors tend to be than MFS). hard examples to classify correctly, and, Consequently, it is our belief that a number therefore, tend to have large weights in of issues regarding portability, tuning, knowl-the final distribution."
"This observation edge acquisition, etc., should be thoroughly allows both to identify the noisy exam-studied before stating that the supervised ML ples and use LazyBoosting as a way to paradigm is able to resolve a realistic WSD improve data quality."
"Preliminary exper-problem. iments have been already carried out in Regarding the ML algorithms tested, the this direction on the DSO corpus. contribution of this work consist of empiri-cally demonstrating that the LazyBoosting al- • Moreover, the inspection of the rules gorithm outperforms other three state-of-the- learned by kazyBoosting could provide art supervised ML methods for WSD."
Further- evidence about similar behaviours of a-more. this algorithm is proven to have better priori different senses.
This type of properties when is applied to new domains. knowledge could be useful to perform Further work is planned to be done in the clustering of too fine-grained or artificial following directions: senses. 180
Lexical acquisition from large corpora has long been considered as a means for enrich-ing vocabularies[REF_CITE].
"Depending on the studies, different is-sues are considered: the acquisition of terms[REF_CITE], the acquisition of subcatego-rization frames[REF_CITE], the acqui-sition of semantic links[REF_CITE], etc."
"While traditional electronic corpora such as journal articles or corpus resources (BNC, SUSANNE, Brown corpus) are satisfactory for classical lexical acquisition, Web corpora are 2 another source of knowledge (Crimmins et al., • The purpose of our task is to build lists of NEs, not to tag corpora."
"For this reason, we only collect non-ambiguous context-independent NEs; partial or incomplete occurrences such as anaphora are consid-ered as incorrect. • The types of NEs collected here are much more accurate than the four basic types defined in MUC."
"The proposed tech-nique could be extended to the collec-tion of any non-MUC names which can be grouped under a common hypernym: botanic names, mechanical parts, book titles, events... • We emphasize the role of document struc-ture in web-based collection."
Focusing[REF_CITE]) that can be used to acquire NEs because Two issues are addressed in this paper: of the constant updating of online data.
The purpose of our work is to propose a technique for the extraction of NEs from the Web through the combination of a harvester and shallow parsers.
Our study also belongs to corpus-based acquisition of semantic re-lationships through the analysis of specific lexico-syntactic contexts[REF_CITE]be-cause hypernym relationships are acquired to-gether with NEs.
The unique contribution of our technique is to offer an integrated ap-proach to the analysis of HTML documents that associates lexical cues with formatting instructions in a single and cohesive frame-work.
The combination of structural informa- 1.
"While traditional electronic corpora can be accessed directly and entirely through large-scale filters such as shallow parsers, access to Web pages is restricted to the narrow and specialized medium of a search engine."
"In order to spot and re-trieve relevant text chunks, we must fo-cus on linguistic cues that can be used to access pages containing typed NEs with high precision. 2. While Web pages are full of NEs, only a small proportion of them are relevant for the acquisition of public, fresh and well-known NEs (the name of someone&apos;s cat is not relevant to our purpose)."
"So that automatically acquired NEs can be used in a NE recognition task, they are asso-ciated with types such as actor (PER-SON), lake (LOCATION), or university (ORGANIZATION)."
"The need for selective linguistic cues (wrt to the current facilities offered by search engines) and for informative and typifying contexts has led us to focus on collections, a specific type of definitory contexts[REF_CITE]."
"Be-cause they contain specific linguistic triggers such as following or such as, definitory con-texts can be accessed through phrase queries to a search engine."
"In addition, these contexts use the classical scheme genus/differentia to define NEs, and thus provide, through the genus, a hypernym of the NEs they define."
Our study extends[REF_CITE]to Web-based and spatially formatted corpora.
"To acquire NEs from the Web, we have devel-oped a system that consists of three sequential modules (see Figure 1): 1."
"A harvester that downloads the pages re-trieved by a search engine from the four following query strings (1.a) following (NE) (1.c) (NE) such as (1.b) list of (NE) (1.d) such (NE) as in which (NE) stands for a typifying hypernym of NEs such as Universities, politicians, or car makers (see list in 4).."
"Three parallel shallow parsers Pc, P1 and Pa which extract candidate NEs respec-tively from enumerations, lists and ta-bles, and anchors.."
A post-filtering module that cleans up the candidate NEs from leading determiners or trailing unrelated words and splits co-ordinated NEs into unitary items.
The four strings ([Footnote_1].a-d) given above are used to query a search engine.
"1(NE) is international organzzations, here. The ty-pographical mark-up of the query string in the figure is ours. The hypernym is in bold italics and the dis-course marker is in bold. 2The harvester that retrieves Web pages through a search engine is a combination of wget available[URL_CITE]auc. dk/pub/infosystems/wget/ and Perl scripts."
They consist of an hypernym and a discourse marker.
They are expected to be followed by a collection of NEs.
Figure 2 shows five prototypical examples of collections encountered in HTML pages re-
I trieved through one of the strings ([Footnote_1].a-d)3 The first collection is an enumeration and consists of a coordination of three NEs.
"1(NE) is international organzzations, here. The ty-pographical mark-up of the query string in the figure is ours. The hypernym is in bold italics and the dis-course marker is in bold. 2The harvester that retrieves Web pages through a search engine is a combination of wget available[URL_CITE]auc. dk/pub/infosystems/wget/ and Perl scripts."
The second collection is a list organized into two sublists.
Each sublist is introduced by a hy-pernym.
The third structure is a list marked by bullets.
"Such lists can be constructed through an HTML table (this example), or by using enumeration marks (&lt;ul&gt; or &lt;ol&gt;)."
The fourth example is also a list built by us-ing a table structure but displays a more com-plex spatial organization and does not em-ploy graphical bullets.
"The fifth example is an anchor to a collection not provided to the reader within the document, but which can be reached by following an hyperlink instead."
The corpus of HTML pages is collected through two search engines with different ca-pabilities: AltaVista (AV) and Northern Light (NL).2 AV offers the facility of double-quoting the query strings in order to search for exact strings (also called phrases in IR).
NL does not support phrase search.
"However, in AV, the number of retrievable documents is limited to the 200 highest ranked documents while it is potentially unlimited in NL."
"For NL, the number of retrieved documents was however restricted to 2000 in order to limit process-ing times."
The choice of these two search en-gines is intended to evaluate whether a poorer query mode (bags of words in NL instead of strings in AV) can be palliated by accessing more documents (2000 max. for NL instead of 200 max. for AV).
"The corpus collected by the two search engines and the four f~.milies of queries is 2,958Mb large (details are given in Section 4)."
"So far, the type of the candidate NEs is pro-vided by the NE hypernym given in (1.a-d)."
"However, the initializer preceding the collec-tion of NEs to be extracted can contain more information on the type of the following NEs."
"In fact the initializer fulfills four distinct func-tions: 1. introduces the presence and the proxim-ity of the collection, e.g. Here is 2. describes the structure of the collection, e.g. a list of 3. gives the type of each item of the collec-tion, e.g. universities 4. specifies the particular characteristics of each item. e.g. universities in Vietnam"
The cues used by the harvester are elements which either introduce the collection (e.g. the .following) or describe the structure (e.g. a list of).
"In initializers in general, these first 2 functions need not be expressed explicitly by lexical means, as the layout itself indi-cates the presence and type of the collection."
Readers exploit the visual properties of writ-ten text to aid the construction of meaning[REF_CITE].
However it is necessary to be explicit when defining the items of the collection as this information is not available to the reader via structural properties.
Initializers gener-ally contain additional characteristics of the items which provide the differentia (under-lined here):
This is a list offAmerican companies with business interests in Latvia.
This example is the most explicit form an ini-tializer can take as it contains a lexical ele-ment which corresponds to each of the four functions outlined above.
"It is fairly simple to extract the details of the items from initializ-ers with this basic form, as the modification of the hypernym takes the form of a relative clause, a prepositional phrase or an adjectival phrase."
A detailed grammar of this form of initializer is as shown in Figure 3.5
The following NP is
We tag the collection by part of speech us-ing the TreeTagger[REF_CITE].
"The el-ements which express the differentia are ex-tracted by means of pattern matching: they are always the modifiers of the plural noun in the string, which is the hypernym of the items of the collection. 5pp = prepositional phrase, Ns = noun (singular), Npl = noun (plural), Vp = verb in present tense, rel.cl. = relative clause."
Initializers containing the search string such as behave somewhat differently.
"They are syntactically incomplete, and the missing con-stituent is provided by each item of the col-lecti[REF_CITE]."
These phrases vary considerably in structure and can require rela-tively complex syntactic rearrangement to ex-tract the properties of the hypernym.
We will not discuss these in more detail here.
One type of error in this system occurs when a paragraph containing the search string is followed by an unrelated list.
For example the harvester recognizes
Ask the long list of American com-panies who have unsuccessfully mar-keted products in Japan. as an initializer when in fact it is not related to any collection.
If it happened to be followed on the page by an collection of any kind the system would mistakenly collect the items as NEs of the type specified by the search string.
"The cue list of is commonly used in dis-cursive texts, so some filtering is required to identify collections which are not employed as initializers and to reduce the collection of er-roneous items."
Analyzing the syntactic forms h:has allowed us to construct a set of regular expressions which are used to eliminate non-initializers and disregard any items collected following them.
We have extracted 1813 potential initial-izers from the corpus of HTML pages col-lected via AV &amp; NL for the query string list of NE.
"Using lexico-syntactic patterns in or-der to identify correct initializers, we have de-signed a shallow parser for filtering and ana-lyzing the strings."
"This parser consists of 14 modules, 4 of which carry out pre-filtering to prepare and tag the corpus, and 10 of which carry out a fine-grained syntactic analysis, re-moving collections that do not function as ini-tializers."
"After filtering, the corpus contains 520 collections."
The process has a precision of 78% and a recall of 90%.
"This study is another application that demon-strates the usability of the WWW as a re-source for NLP (see, for instance,[REF_CITE]for an application of using WWW frequencies in selecting translations)."
"It also confirms the interest of non-textual lin-guistic features, such as formatting markups, inNLP for structured documents such as Web pages."
Further work on Web-based NE acqui-sition could take advantage of machine learn-ing techniques as used for wrapper inducti[REF_CITE].
"With the increasing availability of large amounts of electronic texts, linguists have ac-cess to more and more material for empiri-cally based linguistic research."
"Furthermore, electronic corpora are more and more richly annotated and thereby more and more de-tailed and structured information contained in the corpora becomes accessable."
Currently many corpora are tagged with morphosyntac-tic categories (part-of-speech) and there are already several syntactically annotated cor-
"However, in order to have access to these rich linguistic annotations, adequate query tools are needed."
"In the following, an example of a linguisti-cally relevant construction is considered that illustrates how useful access to structural in-formation in a corpus might be. fiber Chomsky habe ich ein Buch about Chomsky have I a book (1) gelesen read &apos;I have read a book about Chomsky&apos;"
Linguists are often concerned with con-structions that seem not very natural and where intuitions about grammaticality fail.
An example is (1) where we have an accusative object (ein Buch) which is positioned between the two verbal elements and whose modifier (the prepositional phrase iiber Chomsky) is topicailzed.
Some people claim (1) to be ungrammatical whereas other people are inclined to accept it.
In these cases it is very useful to search in an adequate corpus for more natural data show-ing the same construction (see also[REF_CITE]for other ex~.mples of the use of corpora for linguistic research).
"In order to find structures like (1) in a Ger-man corpus, one needs to search for (a) a prepositional phrase modifying the ac- pora."
"Examples are the Penn Treebank (Mar- cusative object and preceding the finite cus et al., 1994;"
"Obviously, two things need to be available in order to enable such a search."
"On the one *The work presented here was done as part of a project[REF_CITE]&quot;Linguistic Data Structures&quot; at hand, one needs a corpus with an annotation the UniversityofTiibingen. that is rich enough to encode the properties"
SIMPX (a) and (b).
"On the other hand, one needs a ± I query tool with a query language that allows to express the properties (a) and (b)."
I VF LK VC MF
Corpora encoding features such as (a) and ! (b) are for example the Verbmobil treebanks.
Query tools such as xkwic[REF_CITE]that allow to search on the tokens and their part-of-speech categories using regular expressions I I NX VXFIN do not allow a direct search on the syntac-
"I tic annotation structures of the corpus, i.e. a"
D search for specific relations between nodes in I
I the annotation such as dominance or linear
I I NX NX VXINF
I I [I
APPR NE VAFIN PPER ART NN VVPP precedence.
Therefore many queries a linguist fiber C. habe would like to ask using a syntactically anno- ich ein Buch gel~en tated corpus either cannnot be expressed in a Figure 1: Annotation of (1) in Verbmobil for- regular expression based language or at least cannot be expressed in an intuitive way.
"Even more recent query languages as SgmlQL[REF_CITE]and XML-QL[REF_CITE]that refer to the SGML or XML annotation of a corpus are in gen-eral not adequate for syntactically annotated corpora: if the annotations are trees and the nesting of SGML/XML elements encodes the tree structure, such query languages work nicely."
"With regular path expressions as sup-ported by XML-QL, it is possible to search not only for parent but also for dominance relations."
"However, in order to deal with discontinuous constituents, most syntactically annotated corpora do not contain trees but slightly different data structures."
"The Penn Treebank for example consists of trees with an additional coindexation relation, Negra al-lows crossing branches and in Verbmobil, an element (a tree-like structure) in the corpus might contain completely disconnected nodes."
"In order to express these annotations in XML, mat that contains approx. 38.000 trees (or rather tree-like annotation structures since, as al-ready mentioned, the structures are not al-ways trees)."
The corpus consists of spoken texts restricted to the domain of arrangement of business appointments.
The Verbmobil corpus is part-of-speech tagged using the Stuttgart Tiibingen tagset (STTS) described[REF_CITE].
"One of the design decisions in Verbmobil was that for the purpose of reusability of the treebank, the annotation scheme should not reflect a commitment to a particular syntac-tic theory."
Therefore a surface-oriented am notation scheme was adopted that is inspired by the notion of topological fields in the sense[REF_CITE].
The discontinuous position-ing of the verbal elements in verb-first and verb-second sentences (as in (1) for example) is the traditional reason to structure the Ger-one has to encode for example each node and man sentence by means of topological fields: each edge as a single element as in (Mengel The verbal elements have the categories[REF_CITE]).
"But then a query for a (linke Klammer) and VC (verbal complex), and dominance relation can no longer be formu- roughly everything preceding the LKforms the lated with a regular path expression. &apos;voffeld&apos; VF, everything between LK and vc"
"In this paper, I propose a query tool that forms the &apos;mittelfeld&apos; MFand the &apos;nachfeld&apos; NF allows to search for parent, dominance and follows the verbal complex. linear precedence relations even in corpora an-"
"The Verbmobil corpus is annotated with notated with structures slightly different from syntactic categories as node labels, grammat-trees. ical functions as edge labels and dependency relations."
The syntactic categories are based 2 The Verbmobil treebanks on traditional phrase structure and on the the-
The German Verbmobil corpus (Stegmann et ory of topological fields.
"In contrast to Negra al., 1998;[REF_CITE]) is a tree- or Penn Treebank, there are neither crossing bank annotated at the University of Tiibingen branches nor empty categories."
"Instead, de- pendency relations are expressed within the used as variables."
"Further, ~, I, ! are logi- • grammatical functions (e.g. OA-MOD for a con- cal connectives (conjunction, disjunction, and stituent modifying the accusative object). negation)."
"A sample annotation conformant to the Verbmobil annotation scheme is the annota- Definition 1 ((C, E, T)-queries) tion of (1) shown in Fig. 1. (The elements set (C, E, T)-queries are inductively defined: in boxes are edge labels.) (a) for all iE IN, tE T:"
"In order to search for structures as in Fig. 1, and token(i)=t token(i) !=t are one needs to search for trees containing a node queries, nl with label PX and grammatical function (b) for all iE IN, cE C: 0A-MOD, a node n2 with label VF that domi-cat (i)=c and cat (i) ! =c are queries, nates nl, a node n3 with label MFand a node (c) for all iE IN, eE E: n4 with label NXand gra.mmatical function 0A fct(i)=e and fct(i)!=e are queries, that is immediately dominated by n3. (d) for all i, j E IN: Evaluating a query for structures as in Fig. 1 on the Verbmobil corpus gives results i &gt; j andi !&gt; j are queries, such as (2) that sound much more natural i &gt;&gt; j andi !&gt;&gt; j are queries, than the constructed example (1). i .. j andi !.. j are queries, (e) for all queries ql, q2: •tja, fiber Flugverbindungen habe ich ql ~ q2 and (ql I q2) are queries. about flight connections have I (2) leider Of course, when adapting this language tokeine Information. unfortunately no information another corpus, depending on the specific an- &apos;unfortunately I have no information notation scheme, other unary or binary pred-about flight connections.&apos; icates might be added to the query language."
"This does not change the complexity of the This example illustrates the usefulness of query language in general. syntactic annotations for linguistic research However, it is also possible that at a later • and it shows the need of query languages and point negation needs to be allowed in a general query tools that allow access to these annota- way or that quantification needs to be added tions. to the query language for linguistic reasons."
Such modifications would affect the complex- 3
The query language ity of the language and the performance of 3.1 Syntax the tool.
"Therefore the decision was taken to As query language for the German Verbmo- keep the language as simple as possible in the bil corpus, a first order logic without quan- beginning. tification is chosen where variables are inter- 3.2 Intended models preted as existentially quantified."
"In the case of the German Verbmobil corpus, is only allowed for atomic formula."
"It seems the data structures are not trees, since struc-that even this very simple logic already gives tures as in Fig. 2, which shows the annotation a high degree of expressive power with respect of the long-distance wh-movement in (3), can to the queries linguists are interested in (see occur."
"The structure in Fig. 2 does not have for example[REF_CITE]for theoretical a unique root node, and the two nodes with investigations of query languages)."
"However, label SINPXhave neither a dominance nor a it might be that at a later stage the query linear precedence relation. language will be extended."
"Let C (the node labels, i.e. syntactic cate- (3) wen glaubst du liebt Maria gories and part-of-speech categories), E (the whom believe you loves Maria edge labels, i.e. grammatical functions) and T &apos;whom do you believe Maria loves&apos; (the terminals, i.e. tokens) be pairwise disjoint finite sets. &gt;, &gt;&gt;, .. are constants for the Therefore, the models of our queries are de-binary relations immediate dominance (par- fined as more general structures than finite ent relation), dominance (reflexive transitive trees. closure of immediate dominance) and linear A model is a tuple (/g,T~,T),£, p, ~/,a) precedence."
"The set 1N of natural numbers is where/g is the set of nodes, 7~,T~and £ are the binary relations immediate dominance (par-ent), dominance and linear precedence, # is a function assigning syntactic categories or Definition 3 (Satisfiability) part-of-speech tags to nodes, r/ is a function Let M = (Lt, P, 73,£, #, rl,a) be a query model mapping edges to grammatical functions, and and let g : iN ~ Lt be an injective .function. a assigns tokens to the leaves (i.e. the nodes For all i6 iN, t6 T, c6 C, e6 E: that do not dominate any other node). • M ~g token(i)=t iffa(g(i)) =t. ,.? , • M ~g token(i)!=t iffa(g(i)) #t. Definition 2 (Query model) • M ~g cat(i)=c iff#(g(i)) =c. Let C, E and T be disjoint alphabets. • M ~g cat (i) !=c iff ~(g(i)) ~c. ([REF_CITE]73,£, #, rl,a) is a query model with cat- • M ~g fct(i)=e iff there is a u 6 lg with egories C, edge labels E and terminals Tiff (u,g(i)) 6 P and r/((u,g(i))) =e. 1. l~ is a finite set with Lt n (C U E UT) = O, • M ~9 fct(±) ! =e iffthere is no u 6 Lt with the set of nodes. (~,g(i)) e p and ~((~,g(i)&gt;) =e 2. P, £, 73 • ILl× U, such that:"
"For all i, j 6 iN: (a) 79 is irreflexive, and for all x • ILl • i ~g i &gt; j iff (g(i),g(j)) 6 79 (i.e. g(i) there is at most one v • ILl with immediately dominates g(j)) (v,x) • 79. • M ~9 i !&gt; j iff (g(i),g(j)) ~ 79 (b) 73 is the reflexive transitive closure of • i ~g i &gt;&gt; j iff(g(i),g(j)) 6 73 (i.e. g(i) 79, and 73 is antisymmetric. dominates g(j)) (c) £ is transitive. • MEg i !&gt;&gt; j iff(g(i),g(j)) ~73 (d) .for all x, y • lg: if (x, y) • £, then • i ~g i .. j iff (g(i),g(j)) 6 £ &lt;~, y) ¢ 73 and (u,x) ¢ 73. (i.e. gCi) is le# oSg(j)) (e) for all z,y • U: (x,y) • £ i# for • M~g i !.. j iff(g(i),g(j)) ~£ all z, w • ld with (x, z), (y, w) • 73, For all queries ql, q2: (z, w) • £ holds. • M~gqx a q2 iffM~gql andM~gq2 (f) for all x, y, z • L(: if (x, y), (x, z) • 73, then either (x, z) • 73 or (z, x) • 73 or • M ~a (ql I q2) iffM ~a ql orM ~g q2. (x, z) • £ or (z, x) • £."
Note that the condition that g needs to 3. # :Lt ~ C is a total .function. be injective means that different variables are 4. rl : 7~ ~ E is a total .function. considered to refer to different-nodes.
"In this 5. a : {u • Ltl there is no u&apos; with (u,u&apos;) • respect, Def. 3 differs from traditional model- 79} ~ T is a total .function. theoretic semantics. columns pl, p2, dl, d2, 11 and 12stand for Figure 3: The relational database schema binary relations and have values 1 or 0 de-pending on whether the relation holds or not. pl signifies immediate dominance of As an example, consider the query for struc- the first node over the second, p2 immedi-tures as in (1) that is shown in (4)."
"The struc- ate dominance of the second over the first, ture in Fig. 1 is a query model satisfying (4). dl dominance of the first over the second, etc."
"Each node pair class has a unique iden- (4) cat(1)=PX &amp; fct(1)=0A-MOD tifier, namely its clad. -g~ cat(2)=VF ~ 2&gt;&gt;1 • tokens_/ contains all leaves from subcor-cat (3)=?IF ~ cat (4) =NX pus i with their tokens (word). &amp; fct(4)=0A ~ 3&gt;&gt;4 • node_pair_/contains all node pairs from subcorpus i with their pair class."
"Of 4 Storing the corpus in a database course, only pairs of nodes belonging to one As already mentioned, the general idea of the single annotation structure are stored. query tool is to store the information one 4.2 Initializing the database wants to search for in a relational database and then to translate an expression in the The storage of the corpus in the database is query language presented in the previous sec- done by an initializing component."
This com-tion into an SQL expression that is evaluated ponent extracts information from the struc-on the database.
"The first part is performed tures in export format (the format used for the by an initializing component and needs to be German Verbmobil corpus) and stores them done only once per corpus, usually by the cor- in the database."
The export format explicitly pus administrator.
"The second part, i.e. the encodes tokens, categories and edge labels, querying of the corpus, is performed by a linear precedence between leaves and the par-query component. ent (immediate dominance) relation."
Domi-The tool is implemented in Java with Java nance and linear precedence in general how-
Database Connectivity (JDBC) as interface ever need to be precompiled. and mysql as database management system.
First the dominance relation is computed simply as reflexive transitive closure of the 4.1 The relational database schema parent relation.
The German Verbmobil corpus consist of sev-
Linear precedence on the leaves can be im-eral subcorpora.
"In the relational database mediately extracted from the export format. there are two global tables, node_class and When computing linear precedence for inter-pair_class."
"Besides these, for each of the sub- nal nodes, the specific properties of the data corpora identified by i there are tables to- structures in Verbmobil (see Section 3) must kens_/ and node_pair_/. The database schema be taken into account."
"Unlike in finite trees, is shown in Fig. 3."
"The arrows represent for two nodes Ul,U2, the fact that ul domi-foreign keys."
"The colnmn cl_id in the ta- nates some x and u2 dominates some y and x ble node_pair_/, for example, is a foreign key is left of y is not s,fl~cient to decide that Ul referring to the colnmn clad in the table is left of u2."
Instead (see axiom (e) in Def. 2) pair_class.
"This means that each entry for the following holds for two nodes ul, u2: ul is clad in node_pair_/uniquely refers to One en- left of u2 iff for all x,y dominated by ul,u2 try for clad in pair_class. respectively: x is left of y."
"The content of the tables is as follows: In general, the database schema itself does #[REF_CITE]898511955 1 tokens_20 scheinbar tree_id node word ADV --[REF_CITE]nicht 24 0 scheinbar PTKNEG --[REF_CITE]beides 24 1 nicht PIS --[REF_CITE]zusammen ADV -- HI) not reflect the concrete properties of the query model, in particular the properties of the bi-nary relations are not part of the database schema, e.g. considering only the database, the dominance and linear precedence relations are not necessarily trA.n~itive."
"Therefore, the query tool can be easily adapted to other data structures, for example to feature structures with reentrancy as annotations."
"In this case, a modification of the part of the initializing component that computes the binary relations would be sufficient."
"As an example, consider how sentence 24 in the subcorpus cd20 (identifier 20) is stored in the database."
This sentence was chosen for the simple reason that it is not too long but contains enough nodes to provide a useful example.
"Besides this, its construction and its tokens are not of any interest here. database concerning sentence 24 are shown in Fig. 5."
Each line in the export format cor-responds to one node.
"The nodes are as-signed numbers 0, 1, ... in the order of the lines in the export format."
The nodes with tokens (i.e. that are leaves) are inserted into the table tokens_20.
"Furthermore, each pair of nodes occurring in sentence 24 is inserted into the table node_pair_20 together with its pair class."
Both orders of a pair are stored.1 The pair classes and node classes belonging to a pair can be found in the two global tables.
Consider for example the nodes 9 and 10 in sentence 24 (the node labelled NXthat domi-nates beides zusammen and the topmost node with label NX).The clad of this pair is 1327.
Fig. 4 shows the sentence in its export for- *
"In a previous version just one order was stored but it turned out that for some queries this causes an mat, i.e. the way it originally occurs in the exponential time complexity depending on the number corpus, together with a picture of the corre- of variables occurring in the query."
This problem is sponding structure.
Parts of the tables in the avoided storing both orders of a node pair.
"The corresponding entry in pair_class tells us tic structures involving category and edge la- that the second node is the :mother of the first, bels and binary relations can be searched."
"The that the second dominates the first, and that query component will be completed very soon there is no linear precedence relation between to process all queries defined in Section 3. the two nodes."
"Furthermore, the node classes The query component takes an expression identified by n_idl and had2 are such that the in the query language as input and trans-first node has label NXand grammatical func- lates this into a corresponding SQL expres-tion HDwhereas the second[ has label NXand sion, which is then passed to the database. no grammatical function."
"As an example, consider again the query (4) repeated here as (5): 4.3 The size of the database"
"So far, in order to test the tool, approximately (5) cat(1)=PX &amp; fct(1)=0A-MOD 8~ one quarter of the German Verbmobil corpus cat(2)=VF &amp; 2&gt;&gt;1 &amp; cat(3)=MF &amp; is stored in the database, namely the following cat(4)=NX ~ fct(4)=0A &amp; 3&gt;&gt;4 subcorpora: id sub- trees tokens pairs For query (5) as input performed on the corpus subcorpus cd20, the query component pro- 15 cdl5 1567 15474 1326416 duces the following SQL query: 20 cd20 2151 21069 1941056 21 cd21 2416 22360 1761082 SELECT DISTINCT npl.tree_id 22 cd22 1723 16587 1229324[REF_CITE]cd24 2255 22763 2129548 node_class AS ncl, node_class AS nc2,"
"The table pa~_class has 23024 entries and node_class AS nc3, node_class AS nc4, node_class has 213 entries."
"The following ta- node_pair_20 AS npl, pair_class AS pc1, ble shows the current size of the files: node_pair_20 AS np2, pair_class AS pc2 node_pair_15 9067 72556 AND npl.tree_id=np2, tree_id node_pair_20 10637713269 AND np2. cl_id=pc2, cl_id; node_pair_21 12039 96383 node_pair_22 8404 67153 As a second example consider the search for node_pair_24 14557 116694 long distance wh-movements as in (3)."
The annotation of (3) using the Verbmobil annota- 5 Searching the corpus tion scheme was shown in Fig. 2.
"In order to search the corpus, one needs of tures might be characterized by the following course to know the specific properties of the properties: there is an interrogative pronoun annotation scheme."
These are described in the (part-of-speech tag PWSfor substituting inter- STTS guidelines[REF_CITE]and the rogative pronoun) that is part of a simplex Verbmobil stylebook[REF_CITE]clause and there is another simplex clause con-that must be both available to any user of the tainlng a finite verb such that the two sim-query tool. plex clauses are not connected and the pro-
"Currently, the query component does not noun precedes the finite verb."
This leads to yet process all possible expressions in the the query (6): query language.
"In particular, it does not allow disjunctions and it does not allow to (6) cat(1)=PWS &amp; cat(2)=SIMPX &amp; 2&gt;&gt;1 query for tokens."
Other atomic queries com- &amp; cat(3)=SIMPX &amp; cat(4)=VVFIN bined with with negations and conjunctions &amp; 2!&gt;&gt;3 &amp; 3!&gt;&gt;2 &amp; 2!..3 ~ 3!..2 are possible.
"In particular, complex syntac- 8~ 1..4 ~ 3&gt;&gt;4"
"Performed on cd20, (6) as input leads to the result set of the first pair restricts the second"
NXwith function 0A in the lffFis restricted to those trees that were found when searching for the first pair.
"Obviously, the first pair is much more restrictive than the second."
"If the order is reversed, the query takes much more time to process."
"Currently the ordering of the pairs needs to be done by the user, i.e. depends on the incoming query."
"However, we plan to implement at least partly an ordering of the binary conjuncts in the query depending on the frequency of the syntactic categories and grammatical functions involved in the pairs."
The obvious advantage of using a relational database to store the corpus is that some parts of the work are taken over by the database
"AND pcI.n_idl=nc2, n_id management system such as the search of the AND pcl.n_id2=ncl.n_id AND pcl.dl=l corpus."
"Furthermore, and this is crucial, the AND pc2.n_idl=nc2 .n_id indexing functionalities of the database man- AND pc2.n_id2=nc3,n_id agement system can be used to increase the AND pc2.dl=O AND pc2.d2=O performance of the tool, e.g. indexes are put on clad in node_pair_/and on nAdl and had2 in pair_class."
"In this paper, I have presented a query tool for syntactically annotated corpora that is devel-oped for the German Verbmobil treebank an-notated at the University of Tiibingen."
The key idea is to extract in an initializing phase the information one wants to search for from the corpus and to store it in a relational database.
"The search itself is done by trans-lating an input query that is an expression in a simple quantifier free first order logic into an AND np3.tree_id=np4, tree_id SQL query that is then passed to the database"
"AND np4. cl_id=pc4, cl_id;"
Currently the database and the tool are running on a Pentium[REF_CITE]MB under Linux.
"On this machine, example (5) takes 1.46 sec to be answered by mysql, and example (6) takes 6.43 sec to be answered."
"This shows that although the queries, in par-ticular the last one, are quite complex and in-volve many intermediate results, the perfor-mauce of the system is quite efficient."
The performance of course depends cru-cially on the size of intermediate results.
In cases where more than one node pair is searched for (as in the two examples above) system.
An obvious advantage of this architecture is that a considerable amount of work is taken over by the database management system and therefore needs not to be implemented.
"Fur-thermore, the mysql indexing functionalities can be used to directly affect the performance of the search."
"The query tool is work in progress, and I briefly want to point out some of the things that still need to be done."
"First, the set of queries the tool can process needs to be ex-tended to all queries allowed in the query lan-guage."
This will be done very soon.
"An- other task for the near future is, as men-the order of the pairs is important since the tioned in the previous section, to add an or- dering mechanism on binary conjuncts in or- und neue."
Akten des 7.
"Internationalen Ger-der to ensure that the more restrictive node manistenkongresses G6ttingen, pages 329--340. pairs are searched for first."
"Further, the de- Laura Kallmeyer. 2000."
On the Complexity of sign of a graphical user-interface to enter the Queries for Structurally Annotated Linguistic Data.
"In Proceedings of ACIDCA&apos;2000, pages queries is planned, allowing to specify queries 105-110, March. by drawing partial trees instead of typing in Jacques Le Maitre, Elisabeth Murisasco, and the expressions in the query language."
"Finally, Monique Rolbert. 1998."
From Annotated Cor-we also want to implement a web-based user- pora to Databases: the SgmlQL Language.
In interface for the query tool.
"John Nerbonne, editor, Linguistic databases."
"Besides these tasks that all concern the cur- CSLI. rent query tool for the German Verbmobil cor- Mitchell Marcus, Grace Kim, Mary Arm pus, a more general issue to persue in the fu-"
"Marcinkiewicz, Robert MacIntyre, Ann Bies, ture is to adapt the tool to other corpora."
"In Mark Ferguson, Karen Katz, and Britta Schas-berger. 1994."
The Penn Treebank:
"Annotating some cases, this implies a modification of the Predicate Argument Structure."
"In ARPA &apos;94. way binary relations are precompiled, and in Andreas Mengel and Wolfgang Lezius. 2000."
An some other cases this would even lead to a
XML-based encoding format for syntactically modification of the query language and the annotated corpora.
"In Proceedings of LREC database schema, namely in those cases where 2000."
Briscoe and Carroll&apos;s (1997) verbal acquisition system distinguishes 163 SCFs and returns rel-ative frequencies for each SCF found for a given predicate.
"The SCFs are a superset of classes found in the Alvey NL Tools (ANLT) dictio-nary,[REF_CITE]and the COML~X Syntax dictionary,[REF_CITE]."
"They incorporate information about control of predicative arguments, as well as alterna-tions such as extraposition and particle move-ment."
The system employs a shallow parser to obtain the subcategorization information.
Po-tential SCF entries are filtered before the final SCF lexicon is produced.
The filter is the only component of this system which we experi-ment with here.
The three filtering methods which we compare are described below.
They applied BHT as follows.
"The sys-tem recorded the total number of sets of SCF cues (n) found for a given predicate, and the number of these sets for a given SCF (ra)."
"The system estimated the error probability (pe) that a cue for a SCF (scfi) occurred with a verb which did not take scfi. pe was esti-mated in two stages, as shown in equation 1."
"Firstly, the number of verbs which are mem-bers of the target SCF in the ANLTdictionary were extracted."
This number was converted to a probability of class membership by divid-ing by the total number of verbs in the dic-tionary.
The complement of this probability provided an estimate for the probability of a verb not taking scfi.
"Secondly, this proba-bility was multiplied by an estimate for the probability of observing the cue for scfi."
"This was estimated using the number of cues for i extracted from the Susanue corpus[REF_CITE], divided by the total number of cues."
The probability of an event with probability p happening exactly rn times out of n attempts is given by the following binomial distribution:
The statistic -21ogA is calculated as follows:-more times is: = (3) k=rn
"Finally, P(m+, n,pe) is the probability that m or more occurrences of cues for scfi will oc- log-likelihood = 2[logL(pl, kl, nl ) +logL(p2, k2, n2) -logL(p, kl, nl) -logL(p, k2, n2) ] (4) where cur with a verb which is not a member ofscfi, given n occurrences of that verb."
"A threshold logL(p, n, k) = k x logp + (n - k) x log(1 -p) on this probability, P(m+,n, pe), was set at less than or equal to 0.05."
"This yielded a 95% and or better confidence that a high enough pro- portion of cues for scfi have been observed for the verb to be legitimately assigned scfi. kl k2 kl + k2 Pl=--,nl P2------n2, P-- nl -4-n2"
Other approaches which use a binomial fil-
The LLR statistic provides a score that re-ter differ in respect of the calculation of the flects the difference in (i) the number of bits error probability.
Creased the number of available cues at the ex-
The LLR statistic detects differences be-pense of the reliability of these cues.
To main- tween pl and p2.
"The difference could tain high levels of accuracy, Manning applied potentially be in either direction, but we are higher bounds on the error probabilities for interested in LLRSwhere pl &gt; p2, i.e. where certain cues."
These bounds were determined there is a positive association between the SCF experimentally.
A similar approach was taken and the verb.
"For these cases, we compared by Briscoe,[REF_CITE]in a the value of -2logA to the threshold value modification to the Briscoe and Carroll sys- obtained from Pearson&apos;s Chi-Squared table, tem."
The overall performance was increased to see if it was significant at the 95% level2. by changing the estimates of pe according to the performance of the system for the target 2.2.3 Using a Threshold on the
"In the work described here, we use the original BHT proposed by Briscoe and Carroll."
Relative Frequencies as a Baseline 2.2.2
"In order to examine the baseline performance The Binomial Log Likelihood Ratio as a Statistical Filter of this system without employing any noti[REF_CITE]demonstrates the benefits of of the significance of the observations, we the LLR statistic, compared to Pearson&apos;s chi- used a threshold on relative frequencies."
"This squared, on the task of ranking bigram data. was done by extracting the SCFS, and rank- The binomial log-likelihood ratio test is ing them in the order of the probability of simple to calculate."
For each verb and SCF their occurrence with the verb.
The probabil-combination four counts are required.
These ities were estimated using a maximum likeli-are the number of times that: hood estimate (MLE) from the observed rela-tive frequencies.
"A threshold, determined em- 1. the target verb occurs with the target SCF pirically, was applied to these probability esti- (kl) mates to filter out the low probability entries for each verb. .... 2. the target verb occurs with any other SCF (nl kl) 2See[REF_CITE]for details of this&quot; method. -"
"To evaluate the different approaches, we took a sample of 10 million words of the BNC cor-pus[REF_CITE]."
We extracted all sentences containing an occurrence of one of fourteen verbs3.
"The verbs were chosen at random, subject to the constraint that they exhibited multiple complementation patterns."
"After the extraction process, we retained 3000 citations, on average, for each verb."
"The sentences con-taining these verbs were processed by the SCF acquisition system, and then we applied the three filtering methods described above."
We also obtained results for a baseline without any filtering.
The results were evaluated against a man-ual analysis of corpus data[Footnote_4].
"4The importance of the manual analysis is outlined[REF_CITE]. We use the same man-ual analysis as Briscoe and Carroll, Le. one from the Susanne, LOB, and SEC corpora. A manual analysisof the BNC data might produce better results. However, since the BNC isa heterogeneous corpus we feltitwas reasonable to testthe data on a differentcorpus, which is also heterogeneous."
"This was ob-tained by analysing up to a maximum of 300 occurrences for each of the 14 test verbs in LOB[REF_CITE], Susanne and SEC[REF_CITE]corpora."
"Follow-ing[REF_CITE], we calculated precision (percentage of SCFSacquired which were also exemplified in the manual analysis) and recall (percentage of the SCFs exemplified in the manual analysis which were acquired automatically)."
We also combined precision and recall into a single measure of overall per-formance using the F measure (MA.nniug and[REF_CITE]).
F = 2.precision. recall (5) precision + recall
Table 1 gives the raw results for the 14 verbs using each method.
"It shows the number of true positives (TP), .false positives (FP), and .false negatives (FN), as determined accord-ing to the manual analysis."
"The results for high frequency SCFs (above 0.01 relative fre-quency), medium frequency (between 0.001 and 0.01) and low frequency (below 0.001) SCFs are listed respectively in the second, 3These verbs were ask, begin, believe, cause, expect, find, give, help, like, move, produce, provide, seem, swing. third and fourth columns, and the final col-umn includes the total results for all frequency ranges."
"Table 2 shows precision and recall for the 14 verbs and the F measure, which combines pre-cision and recall."
"We also provide the baseline results, if all SCFs were accepted."
"From the results given in tables 1 and 2, the MLE approach outperformed both hypothesis tests."
"For both BHT and LLR there was an increase in FNs at high frequencies, and an increase in FPs at medium and low frequen-cies, when compared to MLE."
The number of errors was typically larger for LLR than BHT.
"The hypothesis tests reduced the number of FNS at medium and low frequencies, however, this was countered by the substantial increase in FPs that they gave."
"While BHT nearly al-ways acquired the three most frequent SCFs of verbs correctly, LLR tended to reject these."
"While the high number of FNS can be ex-plained by reports which have shown LLR to be over-conservative[REF_CITE], the high number of FPs is surprising."
"Although theoretically, the strength of LLR lies in its suitability for low frequency data, the results displayed in table 1 do not suggest that the method performs better than BHT on low frequency frames."
MLE thresholding produced better results than the two statistical tests used.
"Preci-sion improved considerably, showing that the classes occurring in the data with the high-est frequency are often correct."
"Although MLE thresholding clearly makes no attempt to solve the sparse data problem, it performs better than BHT or LLR overall."
"MLE is not adept at finding low frequency SCFS, however, the other methods are problematic in that they wrongly accept more than they correctly reject."
"The baseline, of accepting all SCFS, obtained a high recall at the expense of precision."
Our results indicate that MLE outperforms both hypothesis tests.
"There are two explana-tions for this, and these are jointly responsible for the results."
"Firstly, the SCF distribution is zipfian, as are many distributions concerned with nat-ural language[REF_CITE]."
Figure 1 shows the conditional distribution for the verb find.
This ~mf~ltered SCF prob-ability distribution was obtained from 20 M words of BNC data output from the SCF sys- tern.
The unconditional distribution obtained 1997) suggests the discrepancy between the from the observed distribution of SCFsin the unconditional and the conditional distribu- 20 M words of BNC is shown in figure 2.
"The tions. figures show SCF rank on the X-axis versus We examined the correlation between the SCF frequency on the Y-axis, using logarith- manual analysis for the 14 verbs, and the mic scales."
The line indicates the closest Zipf- unconditional distribution of verb types over like power law fit to the data. all SCFs estimated from the ANLTusing the
"Secondly, the hypothesis tests make the Spearman Rank Correlation Coefficient."
"The false assumption (H0) that the unconditional results included in table 3 show that only a and conditional distributions are correlated. moderate correlation was found averaged over The fact that a significant improvement in all verb types. performance is made by correcting the prior Both LLR and BHT work by comparing the probabilities according to the performance of observed value of p(scfi[verbj) to that ex-the system (Briscoe, Carroll and Korhonen, pected by chance."
They both use the observed [ Verb Rank Correlation ask 0.10 begin 0.83 believe 0.77 cause 0.19 expect 0.45 find 0.33 give 0.06 help 0.43 like 0.56 move 0.53 produce 0.95 provide 0.65 seem 0.16 swing 0.50
"Average 0.47 Table 3: Rank correlation between the condi-tional SCF distributions of the test verbs and the unconditional distribution value for p(sc.filverbj) from the system&apos;s out-put, and they both use an estimate for the un-conditional probability distribution (p(scfi)) for estimating the expected probability."
"They differ in the way that the estimate for the un-conditional probability is obtained, and the way that it is used in hypothesis testing."
"For BHT, the null hypothesis is that the ob-served value ofp(scfiIverbj) arose by chance, because of noise in the data."
"We estimate the probability that the value observed could have arisen by chance using p(m+, n,pe), pe is cal-culated using: • the SCF acquisition system&apos;s raw (until-tered) estimate for the unconditional dis-tribution, which is obtained from the Su-sanne corpus and • the ANLT estimate of the unconditional distribution of a verb not taking scf~, across all SCFs"
"For LLR, both the conditional (pl) and un-conditional distributions (p2) are estimated from the BNC data."
The unconditional proba-bility distribution uses the occurrence of scfi with any verb other than our target.
"The binomial tests look at one point in the SCF distribution at a time, for a given verb."
"The expected value is determined using the unconditional distribution, on the assumption that if the null hypothesis is true then this dis-tribution will correlate with the conditional distribution."
"However, this is rarely the case."
"Moreover, because of the zipfian nature of the distributions, the frequency differences at any point can be substantial."
"In these exper-iments, we used one-tailed tests because we were looking for cases where there was a pos-itive association between the SCF and verb, however, in a two-tailed test the null hypoth-esis would rarely be accepted, because of the substantial differences in the conditional and unconditional distributions."
A large number of false negatives occurred for high frequency SCFs because the probabil-ity we compared them to was too high.
"This probability was estimated from the combina-tion of many verbs genuinely occurring with the frame in question, rather than from an es-timate of background noise from verbs which did not occur with the frame."
"We did not use an estimate from verbs which do not take the SCF, since this would require a priori knowl-edge about the phenomena that we were en-deavouring to acquire automatically."
"For LLR the unconditional probability estimate (p2) was high, simply because this SCF was a com-mon one, rather than because the data was particularly noisy."
"For BHT, Re was likewise too high as the SCF was also common in the Susanne data."
"The ANLTestimate went some-way to compensating for this, thus we ob-tained fewer false negatives with BHT than LLR."
A large number of false positives occurred for low frequency SCFs because the estimate for p(scf) was low.
This estimate was more readily exceeded by the conditional estimate.
"For BHT false positives arose because of the low estimate of p(scf) (from Susanne) and because the estimate of p(-,SCF) from ANLT did not compensate enough for this."
"For LLR, there was no mean~ to compensate for the fact that p2 was lower than pl."
"In contrast, MLE did not compare two dis-tributions."
"Simply rejecting the low frequency data produced better results overall by avoid-ing the false positives with the low frequency data, and the false negatives with the high frequency data."
This paper explored three possibilities for fil-tering out the SCF entries produced by a SCF acquisition system.
"These were (i) a version of Brent&apos;s binomial filter, commonly used for this purpose, (ii) the binomial log-likelihood ratio test, recommended for use with low fre- 5 quency data and (iii) a simple method using"
We thank Ted Briscoe for many helpful dis-a threshold on the MLEs of the SCFSoutput cussions and suggestions concerning this work. from the system.
"Surprisingly, the simple MLE We also acknowledge Yuval Krymolowski for thresholding method worked best."
"The BHT useful comments on this paper. and LLR both produced an astounding mlm-ber of FPs, particularly at low frequencies."
"References Further work on handling low frequency Boguraev, B., Briscoe, E., Carroll, J., Carter, data in SCF acquisition is warranted."
"A non- D. and Grover, C. 1987."
"The derivation of a parametric statistical test, such as Fisher&apos;s ex-grammatically-indexed lexicon from the Long-act test, recommended[REF_CITE], man Dictionary of Contemporary English."
In might improve on the results obtained using Proceedings of the 25th Annual Meeting of parametric tests.
"However, it seems from our the Association for Computational Linguis-experiments that it would be better to avoid tics, Stanford, CA. 193-200. hypothesis tests that make use of the uncon-ditional distribution."
"Brent, M. 1991."
"Automatic acquisition of One possibility is to put more effort into the subcategorization frames from untagged text. estimation of pe, and to avoid use of the un-"
In Proceedings of the 29th Annual Meeting conditional distribution for this.
"In some re- of the Association for Computational Linguis-cent experiments, we tried optimising the es- tics, Berkeley, CA. 209-214. timates for pe depending on the performance Brent, M. 1993."
"From gra.mmar to lexicon: of the system for the target SCF, using the unsupervised learning of lexical syntax."
"Com-method proposed by Briscoe, Carroll and Ko- putational[REF_CITE].3: 243-262. rhonen (1997)."
"The estimates of pe were ob- Briscoe, E.J. and J.[REF_CITE]."
Automatic tained from a training set separate to the held-extraction of subcategorization from corpora. out BNC data used for testing.
Results using the new estimates for pe gave an improvement In Proceedings of the 5th ACL Conf. on Ap-plied Nat.
"Proc., Washington, DC. 356-of 10% precision and 6% recall, compared to the BHT results reported here."
"Automatic extraction of subcategoriza- 1997. provement in recall, making the overall per- tion frames from corpora - a framework and formance 3.9 worse than MLEaccording to the 3 experiments. &apos;97"
"Sparkle WP5 Deliverable, F measure."
Valence slightly better results than a Brent style BHT. induction with a head-lexicalized PCFG.
"In If MLE thresholding persistently achieves Proceedings of the 3rd Conference on Empir-better results, it would be worth investi-ical Methods in Natural Language Processing, gating ways of handling the low frequency Granada, Spain. data, such as smoothing, for integration with this method."
"However, more sophisticated Dunning, T. 1993."
"Accurate methods for the smoothing methods, which back-off to an un- Statistics of Surprise and Coincidence."
"Com-Conditional distribution, will also suffer from putational[REF_CITE].1: 61-74. the lack of correlation between conditional Gahl, S. 1998."
Automatic extraction of sub-and unconditional SCF distributions.
Any sta- corpora based on subcategorization frames tistical test would work better at low frequen- from a part-of-speech tagged corpus.
"In Pro-cies than the MLE,since this simply disregards ceedings of the COLING-A CL&apos;98, Montreal, all low frequency SCFs."
"In our experiments, ff Canada. we had used MLE only for the high frequency Garside, R., Leech, G. and Sampson, G. 1987. data, and BHT for medium and low, then over- The computational analysis of English: A all we would have had 54% precision and 67% corpus-based approach."
"Longman, London. recall."
"It certainly seems worth employing hy-pothesis tests which do not rely on the un- Gorrell, G. 1999."
Acquiring Subcategorisation conditional distribution for the low frequency from Textual Corpora.
"MPhil dissertation, SCFS."
"University of Cambridge, UK."
"Speak V 5 210 307 For the sake of this work we take a broad[REF_CITE]159 95 definition of collocations, which were classified Tell V 8 740 744 in three subsets: local content word collocations,"
Table 2: Data for selected words.
"Part of local part-of-speech and function-word speech, number of senses and number of collocations, and global content-word examples m BC and WSJ are shown. collocations."
"If a more strict linguistic perspective was taken, rather than collocations Local content word collocations we should speak about co-occurrence relations."
"Word-to-left Content Word In fact, only local content word collocations Word-to-right Content Word would adhere to this narrower view."
At least one
We only considered those collocations that
"Two-words-to-right Content Word could be easily exlracted form a part of speech Word-to-right-and-left tagged corpus, like word to left, word to right, Local PoS and function word collocations etc."
"Local content word collocations comprise Word-to-left PoS Function Word bigrams (word to left, word to right) and Word-to-right PoS Function Word trigrams (two words to left, two words to right Two-words-to-left PoS"
Both Function and both words to right and left).
At least one of Two-words-to-fight PoS
Words those words needs to be a content word.
"Local Word-to-fight-and-left PoS function-word collocations comprise also all Global content word collocations kinds of bigrams and trigrams, as before, but the Word in Window of 4"
Content Word words need to be function words.
Local PoS Word in sentence collocations take the Part of Speech of the words in the bigrams and trigrams.
Finally Table 3: Kinds of collocations considered global content word collocations comprise the
"We did not lemmatize content words, and we content words around the target word in two therefore do take into account the form of the different contexts: a window of 4 words around target word."
"For instance, governing body and the target word, and all the words in the governing bodies are different collocations for sentence."
Table 3 summarizes the collocations the sake of this paper. used.
These collocations have been used in other word sense disambiguation research and are also 4 Adaptation of decision lists to n-way referred to asfeatures ([REF_CITE]; Ng &amp; ambiguities[REF_CITE]).
"Compared[REF_CITE], who also Decision lists as defined in ([REF_CITE]; took into account grammatical relations, we 1994) are simple means to solve ambiguity only share the content-word-to-left and the problems."
"They have been successfully applied content-word-to-right collocations. to accent restoration, word&quot; sense disambiguation • The best kinds of collocations are local content word collocations, especially if two[REF_CITE]1.00 !.4921.00 ,5261.00 words from the context are taken into Word[REF_CITE].972 1.525.951 ,541.964 consideration, but the coverage is low."
"Function words to right and left also attain remarkable precision. • Collocations are stronger in the WSJ, surely Table 5: Train on BC, tag BC. due to the fact that the BC is balanced, and while our highest results do not reach 80%. therefore includes more genres and topics."
"It has to be noted that the test and training This is a first indicator than genre and topic examples come from the same corpus, which variations have to be taken into account. means that, for some test cases, there are • Collocations for fine-gained word-senses are training examples from the same document."
In sensibly weaker than those reported by somesense we can say that one sense per[REF_CITE]for two-way ambiguous discourse comes into play.
This point will be words.
"For WSJ Shared Contradict.BC instance,[REF_CITE]shows some collocations[REF_CITE]60 27 0 point which receive contradictory senses in the Art N BC and the WSJ."
"The collocation important Body N Car N point, for instance, is assigned the second senseI Child N in all 3 occurrences in the 13C, and the fourth Cost N sense[Footnote_2]in all 2 occurrences in the WSJ."
2 Defined as an isolated fact that is considered directories and 1 subset with 7. separately from the whole.
"Head N We can therefore conclude that the one sense Interest N per collocation holds across corpora, as the Line N contradictions found were due to tagging errors."
Point N The low amount of collocations in common State N would explain in itself the low figures on cross- Thing N corpora tagging.
"But yet, we wanted to further study the Become V Fall V common, which causes the low cross-corpora performance."
"We thought of several factors that could come into play: a) As noted earlier, the training and test examples from the in-corpus experiments are taken at random, and they could be drawn from the same document."
This could make the results appear better for in-corpora experiments.
"On the contrary, in the cross-corpora experiments training and testing example come from different documents. b) The genre and topic changes caused by the shift from one corpus to the other. c) Corpora have intrinsic features that carmot be captured by sole genre and topic variations. d) The size of the data, being small, would account for the low amount of collocations shared."
We explore a) in Section 7 mad b) in Section 8. c) and d) are commented in Section 8. 7 Drawing training and testing examples from the same documents affects performance
"In order to test whether drawing training and testing examples from the same document or not explains the different performance in in-corpora and cross-corpora tagging, low cross-corpora results, we performed the following experiment."
"Instead of organizing the 10 random subsets for cross-validation on the examples, we choose 10 subsets of the documents (also at random)."
This i The second sense of point is defined as the precise location of something; a spatially limited location.
"BC WSJ Collocation #2 #4 Other #2 #4 Other important point 3 0 0 0 2 0 pointofview 1 13 1 19 0 0 way, the testing examples and training examples are guaranteed to come from different documents."
"We also think that this experiment would show more realistic performance figures, as a real application can not expect to find examples from the documents used for training."
"Unfortunately, there are not any explicit document boundaries, neither in the BC nor in the WSJ."
"In the BC, we took files as documents, even if files might contain more than one excerpt from different documents."
This guarantees that document boundaries are not crossed.
"It has to be noted that following this organization, the target examples would share fewer examples from the same topic."
"For the WSJ, the only cue was the directory organization."
"In this case we were unsure about the meaning of this organization, but hand inspection showed that document boundaries were not crossing discourse boundaries."
Apr. pr. cov.
The results for WSJ indicate that drawing N .499 1.00 -.078 .573 .307 -.102 training and testing data from the same or V .543 1.00 -.021 .608 .379 -.027 different documents in itself does not affect so Overall .514 1.00 -.058 .587 .333 -.074 much the results.
"On the other hand, the results"
"Press: Editorial .504 .283 .593 .334 Trying to shed some light on this issue we Press:Reviews .438 .268 .488 .404 observed that the category press:reportage, is Religion .409 .306 .537 .326 related to the genre/topics of the WSJ."
"We SkillsandHobbies .569 .296 .571 .302 therefore designed the following experiment: we Popular Lore .488 .304 .563 .353 tagged each category in the BC with the Belles Lettres..... 516 .272 .524 .314 decision lists trained on the WSJ, and also with Miscellaneous .534 .321 .534 .304 Learned .518 .257 .563 .280 the decision lists trained on the rest of the"
General Fiction .525 .239 .605 .321 categories in the BC.
Mysteryand . . . . 523 .243 .618 .369
"Adventure and .... 551 .223 .702 .312 precision and coverage for press:reportage, Romance and .... 561 .271 .595 .340 both compared to the results for the other Humor .516 .321 .524 .337 categories, and to the results attained by the rest"
"Best precision results are shown in bold. • From all the categories, the collocations from press:reportage are the most similar to those of WSJ. 9 Reasons for cross-corpor a degradation • WSJ contains collocations which are closer The goal of sections 7 and 8 was to explore the to those of press:reportage, than those from possible causes for the low number of the rest of the BC. collocations in common between BC and WSJ."
"In other words, having related genre/topics help Section 7 concludes that drawing the examples having common collocations, and therefore, from different files is not the main reason for warrant better word sense disambiguation the degradation."
This is specially true when the performance. corpus has low genre/topic variation (e.g. WSJ).
Section 8 shows that sharing genre/topic is a key factor; as the WSJ corpus attains better results on the press:reportage category than the rest of
That only leaves the low amount of data available for this study (explanation d).
It is true that data-scarcity can affect the number of collocations shared across corpora.
"We think that larger amounts will make&apos;,this number grow, especially if the corpus draws texts from different genres and topics."
"Nevertheless, the figures[REF_CITE]indicate that even in those conditions genre/topic relatedness would help to find common collocations."
This paper shows that the one sense per collocation hypothesis is weaker for fine-grained word sense distinctions (e.g. those in WordNet): from the 99% precision mentioned for 2-way ambiguities[REF_CITE]we drop to 70% figures.
These figures could perhaps be improved using more available data.
"We also show that one sense per collocation does hold across corpora, but that collocations vary from one corpus to other, following genre and topic variations."
This explains the low results when performing word sense disambiguation across corpora.
"In fact, we demonstrated that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better. parameters into the word sense disambiguation models, and to apply them on a system to acquire training examples automatically."
"Manual development of large subcategorised lexicons has proved difficult because pred-icates change behaviour between sublan-guages, domains and over time."
"Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the rel-ative frequency of different subcategorization frames (SCFs) for a given predicate."
Over the past years acquiring subcatego-rization dictionaries from textual corpora has become increasingly popular (e.g.[REF_CITE]1993;[REF_CITE]).
The different approaches vary according to the methods used and the number of SCFs being extracted.
Regardless correlating: the conditional SCF distribution of an individual verb (p(scfilverbj)) and the unconditional SCF distribution of all verbs in general (p(scfl)).
"Contrary to this assump-tion, however, there is no significant correla-tion between the two distributions."
"Korhonen,[REF_CITE]have showed that a simple method of filtering SCFs on the basis of their relative frequency performs more accurately than statistical fil-tering."
This method sensitive to the sparse data problem is best integrated with smooth-ing.
Yet the performance of the sophisticated smoothing techniques which back-off to an un-conditional distribution also suffer from the lack of correlation between p(scfi[verbj) and p(scf0.
"In this paper, we propose a method for ob-taining more accurate back-off estimates for SCF acquisition."
"Taking Levin&apos;s verb classifi-cati[REF_CITE]as a starting point, we show that in terms of SCF distributions, in-dividual verbs correlate better with other se- of this, there is a ceiling on the performance mantically similar verbs than with all verbs of these systems at around 80% token recall*. in general."
"On the basis of this observation, we propose classifying verbs according to their *Tokenrecall is the percentage of SCFtokens in a sample of manually analysedtext that werecorrectly semantic class and using the conditional SCF acquiredby the system. distributions of a few other members in the same class as back-off estimates of the class verbs from each class and examined the de- (p(scfilsernantic class j)). gree with which the SCF distribution for these Adopting the SCF acquisition system of verbs correlates with the SCF distributions[REF_CITE]we report an ex- two other verbs from the same Levin class. periment which demonstrates how these esti-"
The latter verbs were chosen so that one of mates can be used in filtering.
"This is done the verbs is a synonym, and the other a hyper-by acquiring the conditional SCF distributions nym, of a test verb."
"We used WordNet (Miller for selected test verbs, smoothing these dis- et al., 1990) for defining and recognising these tributions with the unconditional distribution semantic relations."
"We defined a hypernym of the respective verb class, and applying a as a test verb&apos;s hypernym in WordNet, and a simple method for filtering the resulting set synonym as a verb which, in WordNet, shares of SCFs."
Our results show that the proposed this same hypernym with a test verb.
We also method improves the acquisition of SCFs sig- examined how well the SCF distribution for nificantly.
We discuss how this method can be the different test verbs correlates with the SCF used to benefit large-scale SCF acquisition. distribution of all English verbs in general and We begin by reporting our findings that the with that of a semantically different verb (i.e. SCF distributions of semantically similar verbs a verb belonging to a different Levin class). correlate well (section 2).
We then introduce We used two methods for obtaining the the method we adopted for constructing the scF distributions.
"The first was to acquire back-~offestimates for the data used in our ex- an unfiltered subcategorization lexicon for 20 periment (section 3.1), summarise the main million words of the British National Corpus features of the SCF acquisition approach (sec- (BN¢)[REF_CITE]data using Briscoe and tion 3.2), and describe the smoothing tech- Carroll&apos;s (1997) system (introduced in sec-niques adopted (section 3.3)."
"Finally, we re- tion 3.2)."
This gives us the observed distribu-view the empirical evaluation (section 4) and tion of SCFsfor individual verbs and that for discuss directions for future work (section 5). all verbs in the BNC data.
The second method was to manually analyse around 300 occur- 2
Examining SCF Correlation rences of each test verb in the BNC data.
This between Semantically Similar gives us an estimate of the correct SCF distri-
Verbs butions for the individual verbs.
"To examine the degree of SCF correlation for the correct distribution of SCFs over all between semantically similar verbs, we took English verbs was obtained by extracting the Levin&apos;s verb classification (1993) as a start- number of verbs which are members of each ing point."
"Levin verb classes are based on SCF class in the ANLTdictionary (Boguraev et the ability of a verb to occur in specific al., 1987). diathesis alternations, i.e. specific pairs of The degree of correlation was examined by syntactic frames which are assumed to be calculating the Kullback-Leibler distance (KL) meaning preserving."
"The classification pro-[REF_CITE]and the Spearman vides semantically-motivatedsets of syntactic rank correlation coefficient (Re) (Spearman, frames associated with individual classes. 1904) between the different distributions2."
While Levin&apos;s shows that there is corre-
"The results given in tables 1 and 2 were ob- lation between the SCFs related to the verb tained by correlating the observed SCF distri-sense, our aim is to examine whether there is butions from the BNC data."
Table 1 shows also correlation between the SCFs specific to an example of correlating the SCF distribu-the verb form.
"Unlike Levin, we are concerned tion of the motion verb .fly against that of (i) with polysemic scF distributions involving all its hypernym move, (ii) synonym sail, (iii) all senses of verbs."
"In addition, we are not only verbs in general, and (iv) agree, which is not interested in the degree of correlation between related semantically."
"The results show that sets of SCFs, but also in comparing the rank- the SCF distribution for .fly clearly correlates ing of SCFs between distributions."
"Neverthe- better with the SCF distribution for move and less, Levin classes provide us a useful starting sail than that for all verbs and agree."
"The av-point. 2Note that Io., &gt;_ 0, with IO.,near to 0 denoting Focusing on five broad Levin classes strong association, and -1 _&lt;RC &lt; 1, with RC near to change of possession, assessment, killing, mo- 0 denoting a-low degree of association and ttc near to tion, and destroy verbs - we chose four test -1 and 1 denoting strong association. 10."
"The verbs were chosen at random, sub- show that in terms of SCF distributions, verbs in all classes examined correlate better with their hypernym verbs than with all verbs in general."
"As one might expect, the polysemy of the individual verbs affects the degree of SCF cor-relation between semantically similar verbs."
The degree of SCF correlation is higher with those verbs whose predominant3 sense is in-volved with the Levin class examined.
"For example, the SCF distribution for the killing verb murder correlates better with that for the verb kill than that for the verb execute, whose predominant sense is not involved with killing verbs."
These results show that the verb sense spe-cific SCF correlation observed by Levin ex-tends to the verb form specific SCF correlation and applies to the ranking of SCFs as well.
This suggests that we can obtain more accu-rate back-off estimates for verbal acquisition by basing them on a semantic verb type.
"To ject to the constraint that they occurred fre-quently enough in corpus data4 and when ap-plicable, represented different sub-classes of each examined Levin class."
"To reduce the problem of polysemy, we required that the predominant sense of each verb corresponds to the Levin class in question."
This was ensured by manually verifying that the most frequent sense of a verb in WordNet corresponds to the sense involved in the particular Levin class.
"To obtain the back-off estimates, we chose 4-5 representative verbs from each verb class and obtained correct SCF distributions for these verbs by manually analysing around 300 occurrences of each verb in the ant data."
We merged the resulting set of SCF distributions to construct the unconditional SCF distribu-tion for the verb class.
This approach was taken to minimise the sparse data problem and cover SCF variations within verb classes and due to polysemy.
The bazk-off estimates find out whether such semantically motiwted 4[REF_CITE]occurrencesfor eachverb.
"This wasmerelyto guarantee accurate enoughtesting, SPredomlnant sense refershere to the most frequent as weevaluatedour results against manual analysisof sense of verbs in WordNet. corpus data (seesection 4). sending and carrying, send, ship, carry, bring, transport exerting force pull, push change of possession give, lend, contribute, donate, offer provide, supply, acquire, buy assessment, analyse searching fish, explore, investigate social interaction agree, communicate, struggle, marry, meet, visit killing kill, murder, slaughter, strangle destroy demolish, destroy, ruin, devastate appearance, disappearance arise, emerge, disappear, vanish and occurrence motion arrive, depart, march, move, slide, swing travel, walk, fly, sail, dance aspectuM begin, end, start, terminate, complete for motion verbs, for example, were obtained nomial hypothesis test, we adopted another by merging the SCF distributions of the verbs method, where the conditional SCF distribu-march, move, fly, slide and sail."
"Each verb tion from the system is smoothed before fil-used in obtaining the estimates was excluded tering the SCFS,using the different techniques when testing the verb itself."
"For example, introduced in section 3.3."
"After smoothing, when acquiring subcategorization for the verb filtering is performed by applying a threshold fly, estimates were obtained only using verbs to the resulting set of probability estimates. march, move, slide and sail."
We used training data to find an optimal av-erage threshold for each verb class examined.
This filtering method allows us to examine
"Add one smoothings has the effect of giving information about control of predicative ar-some of the probability space to the SCFs un-guments, as well as alternations such as ex-seen in the conditional distribution."
As it as-traposition and particle movement.
"The sys-sumes a uniform prior on events, it provides a tem employs a shallow parser to obtain the baseline smoothing method against which the subcategorization information."
Potential SCF entries are filtered before the final SCF lexi- 5See[REF_CITE]for detailedin-con is produced.
While Briscoe and Carroll formation about the smoothing techniquesdiscussed (1997) used a statistical filter based on bi- here.
"In Katz backing-[REF_CITE], some of the The results were evaluated against a man-probability space is given to the SCFs unseen ual analysis of the corpus data."
This was or of low frequency in the conditional distri- obtained by analysing up to a maximum of bution.
This is done by backing-offto an un- 300 occurrences for each test verb in BN¢ conditional distribution.
"Let p(xn) be a prob- or LOB[REF_CITE], Susanne and ability of a SCF in the conditional distribution, SEC[REF_CITE]corpora."
"We and p(xnv) its probability in the unconditional calculated type precision (percentage of SCFs distribution, obtained by maximum likelihood acquired which were also exemplified in the estimation."
"The estimated probability of the manual analysis) and recall (percentage of the scF is calculated as follows: SCFs exemplified in the manual analysis which were also acquired automatically), and com-bined them into a single measure of overall P(xn)= { (1-d) xp(xn) ifc(x=) &gt; cl performance using the F measure (Manning c~x p(xnp) otherwise (2) and[REF_CITE])."
"The cut off frequency ci is an empiri- F = 2. precision, recall (4) cally defined threshold determining whether precision -4-recall to back-off or not."
When counts are lower
"We estimated accuracy with which the sys-than cl they are held too low to give an accu-tem ranks true positive classes against the cor-rate estimate, and we back-offto an uncondi-rect ranking."
This was computed by calculat-tional distribution.
"In this case, we discount ing the percentage of pairs of SCFs at posi-p(x~) a certain amount to reserve some of the tions (n, m) such that n &lt; m in the system probablity space for unseen and very low fre-ranking that occur in the same order in the quency scFs."
The discount (d) is defined em-ranking from the manual analysis.
"This gives pirically, and a is a normalization constant us an estimate of the accuracy of the relative which ensures that the probabilities of the re-frequencies of SCFs output by the system."
"In sulting distribution sum to 1. addition to the system results, we also calcu- 3.3.3 Linear Interpolation lated KL and Rc between the acquired unfil-"
"While Katz backing-off consults different es- tered SCF distributions and the distributions timates depending on their specificity, linear obtained from the manual analysis. interpolation makes a linear combination of 4.2 Results them."
Linear interpolation is used here for the simple task of combining a conditional distri-
Table 5 gives average results for the 60 test bution with an unconditional one.
The esti- verbs using each method.
The results indi-mated probability of the SCF is given by cate that both add one smoothing and Katz backing-off improve the baseline performance only slightly.
"Linear interpolation outper- P(xn) = Al(p(z,~)) + )~2(p(xnp)) forms these methods, achieving better results(3) on all measures."
The improved KL indicates where the Ai denotes weights for differ- that the method improves the overall accu-ent context sizes (obtained by optimising the racy of SCF distributions.
The results with smoothing performance on the training data rtc and system accuracy show that it helps for all zn) and sum to 1. to correct the ranking of SCFs.
The fact that both precision and recall show clear im- timates for each verb class.
Out of ten verb provement over the baseline results demon-strates that linear interpolation can be suc-cessfully combined with the filtering method employed.
These results seem to suggest that a smoothing method which affects both the highly ranked SCFs and SCFs of low frequency is profitable for this task.
"In this experiment, the semantically moti-vated back-offestimates helped to reduce the sparse data problem significantly."
"While a to-tal of 151 correct SCFs were missing in the test data, only three were missing after smoothing with Katz backing-off or linear interpolation."
"For comparison, we re-run these experi-ments using the general SCF distribution of all verbs as back-off estimates for smooth-ing6."
"The average results for the 60 test verbs given in table 6 show that when using these estimates, we obtain worse results than with the baseline method."
"This demonstrates that while such estimates provide an easy solution to the sparse data problem, they can actually degrade the accuracy of verbal acquisition."
Table 7 displays individual results for the different verb classes.
"It lists the results ob-tained with KL and Rc using the baseline method and linear interpolation with semanti- classes, eight show improvement with linear interpolation, with both KL and Rc."
"However, two verb classes - aspectual verbs, and verbs of appearance, disappearance and occurrence - show worse results when linear interpolation is used."
"According[REF_CITE], these two verb classes need further classification before a full semantic account can be made."
The prob-lem with aspectual verbs is that the class contains verbs taking sentential complements.
"As Levin does not classify verbs on basis of their sentential complement-taking proper-ties, more classification work is required be-fore we can obtain accurate SCF estimates for this type of verb."
The problem with verbs of appearance is more specific to the verb class.
Levin remarks that the definition of appearance verbs may be too loose.
"In addition, there are signifi-cant syntactic differences between the verbs belonging to the different sub-classes."
This suggests that we should examine the degree of SCF correlation between verbs from different sub-classes before deciding on the fi-cally motivated estimates.
Examining the re-nal (sub-)class for which we obtain the es-sults obtained with linear interpolation allows timates.
"As the results with the combined us to consider the accuracy of the back-offes- Levin classes show, estimates can also be suc-cessfully built using verbs fromdifferent Levin 6These estimates were obtained by extracting the number of verbs which are members of each SCFclass classes, provided that the classes are similar in the ANLTdictionary."
See section 2 for details. enough. be used to improve SCF acquisition signifi-liminary experiment.
"However, our recent in-cantly, when combined with smoothing and vestigation shows that the total number of se-a simple filtering method. mantic classes across the whole lexicon is un-"
We have not yet explored the possibility likely to exceed 50.
This is because many of of using the semantically motivated estimates the Levin classes have proved similar enough with statistical filtering.
"In principle, this in terms of SCF distributions that they can be should help to improve the performance of the combined together."
Therefore the additional statistical methods which make use of back-off effort required to carry out the proposed work estimates.
"If filtering based on relative fre- seems justified, given the accuracy enhance-quencies still achieves better results, it would ment reported. be worth investigating ways of handling the low frequency data for integration with this 6 Acknowledgements method."
"As Korhonen, Gorrell and McCarthy I thank Ted Briscoe and Diana[REF_CITE]discuss, any statistical filtering method useful comments on this paper. would work better at low frequences than the one applied, since this simply disregards all References low frequency SCFS."
"Boguraev, B., Briscoe, E., Carroll, J., Carter,"
"In addition to refining the filtering method, D. and Grover, C. 1987."
The derivation of a our future work will focus on integrating grammatically-indexed lexicon from the Long-this approach with large-scale scF acquisition. man Dictionary of Contemporary English.
"In This will involve (i) defining the set of seman- Proceedings of the 25th Annual Meeting of tic verb classes across the lexicon, (ii) obtain-the Association .for"
"Computational Linguis-ing back-off estimates for each verb class, and tics, Stanford, CA. 193-200. (iii) implementing a method capable of auto-matically classifying verbs to semantic classes."
"Brent, M. 1991."
Automatic acquisition of The latter can be done by linking the Word- subcategorization frames from untagged text.
"In Proceedings of the 29th Annual Meeting Korhonen, A., Gorrell, G. and McCarthy, of the Association for Computational Linguis- D. 2000."
"Statistical filtering and subcatego-tics, Berkeley, CA. 209-214. rization frame acquisition."
"In Proceedings of the Joint SIGDAT Conference on Empirical Brent, M. 1993."
From grammar to lexicon:
Methods in Natural Language Processing and unsupervised learning of lexical syntax.
"Very Large Corpora, Hong Kong. putational[REF_CITE].3: 243-262."
"Leech, G. 1992. 100 million words of English: Briscoe, E. and Carroll, J. 1993."
Gener-the British NationM Corpus.
Language Re-alised probabilistic Lt~ parsing for unification- search 28(1): 1-13. based grammars.
"In Proceedings of the 5th ACL Conf. on Ap-a large subcategorization dictionary from cor-plied Natural Language Processing, Washing-pora."
"In Proceedings of the 31st Annual Meet-ton, DC. 356-363. ing of the Association for Computational Lin-Briscoe, T., Carroll, J. and Korhonen, A. guistics, Columbus, Ohio. 235-242. 1997."
"Automatic extraction of subcategoriza- Manning, C. and Schiitze, H. 1999."
Founda-tion frames from corpora - a framework and tions of Statistical Natural Language Process-3 experiments.
Sparkle WP5 Deliverable. ing.
"New York University, Ms. Available[URL_CITE]"
"Columbus, Ohio: 95-106."
"Katz, S. M. 1987."
Estimation of probabili-ties from sparse data for the language model component of speech recogniser.
"IEEE Trans-actions on Acoustics, Speech, and Signal[REF_CITE].3: 400-401."
In this paper we describe a preliminary implemen-
Particular emphasis is placed on the social commit-tation of a &apos;middle-level&apos; dialogue management sys- ments of the dialogue participants (obligations to tem.
"The key tasks of a dialogue manager are to act and commitments to propositions) without mak-update the representation of dialogue on the basis of ing explicit claims about the actual beliefs and in-processed input (generally, but not exclusively, lan- tentions of the participants."
"Also, heavy empha-guage utterances), and to decide what (if anything) sis is placed on how dialogue participants socially the system should do next."
"There is a wide range of GROUND[REF_CITE]the infor-opinions concerning how these tasks should be per- mation expressed in dialogue: the information state formed, and in particular, how the ongoing dialogue assumed in this theory specifies which information is state should be represented: e.g., as something very assumed to be already part of the common ground at specific to a particular domain, or according to some a given point, and which part has been introduced, more general theory of (human or human inspired) but not yet been established. dialogue processing."
"At one extreme, some systems The rest of this paper is structured as follows."
"The represent only the (typically very rigid) transitions theory of dialogue underlying the implementation is possible in a perceived dialogue for the given task, described in more detail in Section 2."
Section 3 de-often using finite states in a transition network to scribes the implementation itself.
Section 4 shows represent the dialogue: examples of this are sys- how the system updates its information state while tems built using Nuance&apos;s DialogueBuilder or the participating in a fairly simple dialogue.
CSLU&apos;s Rapid Application Prototyper.
"The other extreme is to build the dialogue processing theory on 2 Theoretical Background top of a full model of rational agency (e.g.,[REF_CITE])."
"The approach we take here lies One basic assumption underlying this work is that in between these two extremes: we use rich repre- it is useful to analyse dialogues by describing the sentations of information states, but simpler, more relevant &apos;information&apos; that is available to each par-dialogue-specific deliberation methods, rather than ticipant."
The notion of INFORMATION STATE (IS) is a deductive reasoner working on the basis of an ax- therefore employed in deciding what the next action iomatic theory of rational agency.
"We show in this should be, and the effects of utterances are described paper that the theory of information states we pro- in terms of the changes they bring about in ISs."
"A pose can, nevertheless, be used to give a character- particular instantiation of a dialogue manager, from isation of dialogue acts such as those proposed by this point of view, consists of a definition of the con-the Discourse Resource Initiative precise enough to tents of ISs plus a description of the update processes which map from IS to IS."
"Updates are typically trig- /.&apos;°&quot; .. -,. &apos;&quot;.., gered by &apos;full&apos; dialogue acts such as assertions or directives,1 of course, but the theory allows parts of utterances, including individual words and even sub-parts of words, to be the trigger."
"The update rules for dialogue acts that we assume here are a simpli-fied version of the formalisations proposed[REF_CITE](henceforth, PTT)."
The main aspects of PTT which have been im-plemented concern the way discourse obligations are handled and the manner in which dialogue partic-ipants interact to add information to the common &apos;E 1 ground.
"Obligations are essentially social in nature, and directly characterise spoken dialogue; a typical example of a discourse obligation concerns the rela-tionship between questions and answers."
"Poesio and Traum follow[REF_CITE]in suggesting that the utterance of a question imposes an obliga-tion on the hearer to address the question (e.g., by Figure 1: TrindiKit Architecture providing an answer), irrespective of intentions."
"As for the process by which common ground is es-in Section 3.4, we describe the rules used by the sys-tablished, or GROUNDING ([REF_CITE]; tem to adopt intentions and perform its own actions."
"An extended example of how these mechanisms are cal speech act theory is inherently too simplistic in used to track and participate in a dialogue is pre-that it ignores the fact that co-operative interaction sented in Section 4. is essential in discourse; thus, for instance, simply as-serting something does not make it become mutually 3.1 TrindiKit &apos;known&apos; (part of the common ground)."
"It is actually The basis for our implementation is the TrindiKit necessary for the hearer to provide some kind of ac- dialogue move engine toolkit implemented as part knowledgementthat the assertion has been received, of the TRINDI project[REF_CITE]."
"The understood or not understood, accepted or rejected, toolkit provides support for developing dialogue sys-and so on."
"Poesio and Traum view the public in- tems, focusing on the central dialogue management formation state as including both material that has components. already been grounded, indicated by GND here, and The system architecture assumed by the TrindiKit material that hasn&apos;t been grounded yet."
These com- is shown in Figure 1.
"A prominent feature of this ar-ponents of the information state are updated when chitecture is the information state, which serves as a GROUNDINGACTSsuch as acknowledgement are per- central &apos;blackboard&apos; that processing modules can ex-formed."
Each new contribution results in a new DIS- amine (by means of defined CONDITIONS)or change COURSE UNIT(DU) being added to the information (by means of defined OPERATIONS).
"The structure state[REF_CITE]and recorded in a list of &apos;un- of the IS for a particular dialogue system is defined&apos; grounded discourse units&apos; (UDUS); these DUs can by the developer who uses the TrindiKit to build then be subsequently grounded as the result, e.g., of that system, on the basis of his/her own theory of (implicit or explicit) acknowledgements. dialogue processing; no predefined notion of infor-mation state is provided.2."
"The toolkit provides a 3 Implementing PTT number of abstract data-types such as lists, stacks, In this section, we describe the details of the im- and records, along with associated conditions and plementation."
"First, in Section 3.1, we describe the operations, that can be used to implement the user&apos;s TrindiKit tool for building dialogue managers that theory of information states; other abstract types we used to build our system."
"In Section 3.2, we de- can also be defined."
"In addition to this customis-scribe the information states used in the implemen- able notion of information state, TrindiKit provides tation, an extension and simplification of the ideas a few system variables that can also used for inter-from PTT discussed in the previous section."
"Then, module communication."
"These include input for the in Section 3.3, we discuss how the information state raw observed (language) input, latest_moves which is updated when dialogue acts are observed."
"Finally, 2In TRINDIwe are experimentingwith multipleinstanti- 1We assume here the DRI classificationof dialogueacts ationsofthree differenttheoriesof informationstate (Traum[REF_CITE]. et al., 1999). contains the dialogue moves observed in the most r ] understandingAct( W,DU3)\ 1 ~1[OBL: ~lddre~(C,CA2) ] / // recent turn, latest_speaker, and next_moves, con- ~.. /.. /CAS:C2 ,~..,~g.(C.DU2) \/ II taining the dialogue moves to be performed by the ..... /.... \c^:: &apos;// II system in the next turn. / sc : &lt; &quot; H ! [COND: &lt; &gt; J H"
A complete system is assumed to consist of sev-
"H UDUS:&lt;DU3&gt; eral modules interacting via the IS. (See Figure 1 [ [OBL: &lt;,ddri~z(C.CA2)&gt; ]] H again.)"
The central component is called the DIA- DH: &lt;CA2:C2.into requi~t(W.?helpfore1):&gt;
LOGUE MOVE ENGINE (DME).
"The DME performs the processing needed to integrate the observed di- : alogue moves with the IS, and to select new moves for the system to perform."
These two functions are encapsulated in the UPDATE and SELECTION sub-modules of the DME.
"The update and select mod-ules are specified by means of typed rules, as well as sequencing procedures to determine when to apply the rules."
"We are here mainly concerned with UP-DATE RULES(urules), which consist of four parts: a name, a type, a list of conditions to check in the in- / J/"
If / LCOND: &lt; &gt; | LID:
J II DU2 / [ lilt / / / /CA5:
"C2.dil c,(C ivemule(W) )\ //H / / /DH: (CAS:C2.a,swer(C.CA2.CA4) ) //11 ko,, //11 LCOND: &lt;IICIpt(W.CA6)-&gt;obl(W~iveroutc(W))&gt; IIII JIII I / / [ / LID:"
"DU3 JII 1 /,,,,o_,..,,,.,( w.:,,.-, )\ II / ;li~oute(W ) t JI ~,,*~,~,d~(W.bU3)/ lINT: &lt;letrome(C)&gt;]"
"J formation state, and a list of operations to perform on the information state, urules are described in more detail below, in Section 3.3."
There are also
"Figure 2: Structure of Information States two modules outside the DME proper, but still cru-cial to a complete system: INTERPRETATION, which consumes the input and produces a list of dialogue model misunderstandings arising from the dialogue acts in the latest_moves variable (potentially mak- participants having differing views on what has been ing reference to the current information state), and grounded; as we are not concerned with this problem"
"GENERATION, which produces NL output from the dialogue acts in the next_moves variable."
"Finally, there is a CONTROLmodule, that governs the se-quencing (or parallel invocation) of the other mod-ules."
In this paper we focus on the IS and the DME; our current implementation only uses very simple interpretation and generation components.
In this section we discuss the information state used in the current implementation.
"The main difference between the implemented IS and the theoretical pro-posal[REF_CITE]is that in the im-plementation the information state is partitioned in fields, each containing information of different types, whereas in the theoretical version the information state is a single repository of facts (a DISCOURSE REPRESENTATION STRUCTURE)."
Other differences are discussed below.
"An example IS with some fields filled is shown in Figure 2; this is the IS which results from the second utterance in the example dialogue discussed in Section 4, A route please.3"
"The IS in Figure 2 is a record with two main parts, W and C. The first of these represents the system&apos;s (Wizard) view of his own mental state and of the (semi-)public information discussed in the di- here, we will ignore C in what follows. w contains information on the grounded mate-rial (GND), on the ungrounded information (UDUS, PDU and CDU), and on W&apos;s intentions (INT)."
GND contains the information that has already been grounded; the other fields contain information about the contributions still to be grounded.
"As noticed above, in PTT it is assumed that for each new ut-terance, a new DU is created and added to the IS."
The current implementation differs from the full the-ory in that only two DUs are retained at each point; the current DU (CDU) and the previous DU (PDU).
"The CDU contains the information in the latest con-tribution, while the PDU contains information from the penultimate contribution."
Information is moved from PDU to GND as a result of an ack (acknowl-edgement) dialogue act (see below.)
"The DUs and the GND field contain four fields, representing obligations (OBL), the dialogue history (DH), propositions to which agents are socially com-mitted (scP), and conditional updates (COND)."
The value of OBL is a list of action types: actions that agents are obliged to perform.
"An action type is specified by a PREDICATE, a DIALOGUE PARTICI-PANT, and a list of ARGUMENTS."
"The value of see is a list of a particular type of mental states, so-alogue; the second, his view of the user&apos;s (Caller) logue actions, which are instances of dialogue action then he or she has the obligation to perform the ac-types."
"A dialogue action is specified by an action tion type requested. (In this case, to give a route to type, a dialogue act id, and a confidence level CONF C.) (the confidence that an agent has that that dialogue act has been observed)."
"The situation in Figure 2 is the result of updates to the IS caused by utterance [2] in the dialogue in (6), We are now in a position to examine the update which is assumed to generate a direct act as well as mechanisms which are performed when new dia-an assert act and an answer act.5"
That utterance logue acts are recognised.
"When a dialogue par-is also assumed to contain an implicit acknowledge- ticipant takes a turn and produces an utterance, ment of the original question; this understanding act the interpretation module sets the system variable has resulted in the contents of DU2 being grounded latest_moves to contain a representation of the di- (and subsequently merged with GND), as discussed alogue acts performed with the utterance."
The up-below. dating procedure then uses update rules to modify
GND.OBL in Figure 2 includes two obligations. the IS on the basis of the contents of latest_moves The first is an obligation on W to perform an under- and of the previous IS.
"The basic procedure is de-standing act (the predicate is understandingAct, scribed in (1) below,s the participant is W, and there is just one argument, DU3, which identifies the DU in CDU by referring to its ID)."
The second obligation is an obligation on C (1) 1.
Create a new DU and push it on top of to address conversational act CA2; this ID points UDUs. to the appropriate info_request in the DH list by means of the ID number.
Obligations are specified 2.
"Perform updates on the basis of backwards in CDU and PDU, as well."
"Those in PDU are simply grounding acts. a subset of those in GND, since at point in the up-date process shown in Figure 2 this field contains ."
"If any other type of act is observed, record information that has already been grounded (note it in the dialogue history in CDU and apply that DU2 is not in UDUS anymore); but CDU con- the update rules for this kind of act tains obligations that have not been grounded yet -in particular, the obligation on W to address CA6. 4."
"Apply update rules to all parts of the IS GND.DH in this IS contains a list of dialogue ac- which contain newly added acts. tions whose occurrence has already been grounded: the info_request performed by utterance 1, with ar-gument a question,6 and the implicit acknowledge The first step involves moving the contents of CDU performed by utterance 2.7 The DHfield in CDU con- to PDU (losing direct access to the former PDU con-tains dialogue acts performed by utterance 2 that do tents) and putting in CDU a new empty DU with need to be grounded: a directive by C to W to per- a new identifier."
"The second and third steps deal form an action of type giveroute, and an assert explicitly with the contents of latest.moves, ap-by C of the proposition want(C, route), by which C plying one urule (of possibly a larger set) for each provides an answer to the previous info_request act in latest_moves."
The relevant effects for each
"CA2. act are summarised in (2), where the variables have The CONDfield in CDU contains a conditional up- the following types: date resulting from the directive performed by that IDx Dialogue Act Identification Number utterance."
"The idea is that directives do not imme- DU Identification NumberDUx diately lead to obligations to perform the mentioned DP Dialogue Participant (i.e., the speaker) action: instead (in addition to an obligation to ad- q A Question dress the action with some sort of response), their ef- A PropositionPROP fect is to add to the common ground the information An ActionAct that if the directive is accepted by the addressee, The other dialogue participanto(DP) P(ID)"
"The content of the ID, a proposition SThe fact that the utterance of a route please constitutes"
"The content of the ID, a question (2) act ID:2, accept(DP,ID2) addition, the direct rule introduces a conditional effect accomplished via rule resolution act: acceptance of the directive will impose an obli-act ID:2, ack(DP,DU1) gation on the hearer to act on its contents. effect peRec(w."
"In addition, all FORWARD ACTS9 in the DRI implemented at the moment."
"The main effect of an ack is to merge the information in the acknowledged As noted above, these rules have four parts; a DU (assumed to be PDU) into GND,also removing name, a type, a list of conditions, and a list of ef-this DU from UDUS."
Unlike the other acts described fects.
"The conditions in (4) state that there must be below, ack acts are recorded directly into GND.DH, a movein latest_moves whose predicate is inforeq. rather than into CDU.TOGND.DH."
The effectsl° state that the move should be recorded
"All of the other updates are performed in the third in the dialogue history in CDU,that an obligation to step of the procedure in (1)."
"The only effect of ac- address the request should be pushed into OBL in cept acts is to enable the conditional rules which CDU, and that the requirement for an understand-are part of the effect of assert and direct, leading ing act by W should be pushed directly into the list to social commitments and obligations, respectively. in W.GND. agree acts also trigger conditional rules introduced The fourth and final step of the algorithm cycles by check; in addition, they result in the agent be- through the updating process in case recently added ing socially committed to the proposition introduced facts have further implications."
"For instance, when by the act with which the agent agrees."
"Perform- an action has been performed that matches the an-ing an answer to question ID2 by asserting propo- tecedent of a rule in COND, the consequent is es-sition P(ID3) commits the dialogue participant to tablished."
"Likewise, when an action is performed the proposition that P(ID3) is indeed an answer to it releases any obligations to perform that action."
"Thus, accept, answer, and agree are all ways of"
"The two rules for assert are where the confidence releasing an obligation to address, since these are levels are actually used, to implement a simple ver- all appropriate backward looking actions."
"Similarly, ification strategy."
"The idea is that the system only an agent will drop intentions to perform actions it assumes that the user is committed to the asserted has already (successfully) performed. proposition when a confidence level of 2 is observed, 3.4 Deliberation while some asserts are assumed not to have been sufficientlywell understood, and are only assigned a We assume, in common with BDI-approaches to confidence level 1."
"This leads the system to perform agency (e.g.,[REF_CITE]) that intentions a check, as we will see shortly. 9Forward acts include assert, check, direct, and"
"The next three update rules, for check, direct, info_request. and info_req, all impose an obligation on the other l°The ID and HID values simply contain numbers identifying dialogue participant to address the dialogue act."
In the discourse units and conversational acts. are the primary mental attitude leading to an agen- the antecedent of this conditional is another t&apos;s actions.
"The main issues to explain then become action that is already intended. (This an-how such intentions are adopted given the rest of ticipatory planning allows the obligation to the information state, and how an agent gets from be discharged at the same time it is invoked, intentions to actual performance. e.g., without giving an intermediate accep- For the latter question, we take a fairly simplistic tance of an directive.) approach here: all the intentions to perform dia- 4. add an intention to perform a (dialogue) ac-logue acts are simply transferred to the next_moves tion motivated by the intention to perform system variable, with the assumption that the gen-the current task."
"In the case of the Au-eration module can realise all of them as a single ut-toroute domain, we have two cases: the sys-terance."
"A more sophisticated approach would be to tem may decide weight the importance of (immediate) realisation of sets of intentions and compare this to the likelihood (a) to check any dialogue acts in CDU at that particular utterances will achieve these effects confidence level 1, which contain infor-at minimal cost, and choose accordingly."
"We leave mation needed to discharge the intention this for future work (see (Traum and Dillenbourg, to give a route; or 1998) for some preliminary ideas along these lines), (b) to perform a question asking about a new concentrating here on the first issue - how the sys- piece of information that has not been es-tem adopts intentions to perform dialogue acts from tablished (this is decided by inspecting other aspects of the mental state."
GND.SCP and CDU.SCP).
"For example, The current system takes the followingfactors into it may decide to ask about the starting account: point, the time of departure, etc. • obligations (to perform understanding acts, to 4 Extended Example address previous dialogue acts, to perform other actions)"
"In this section, we discuss more examples of how the information state changes as a result of processing • potential obligations (that would result if an-and performing dialogue acts."
"It is useful to do this other act were performed, as represented in the by looking briefly at a typical Autoroute dialogue, COND field) shown in (6).11 Our implementation can process this • insufficiently understood dialogue acts (with a sort of dialogue using very simple interpretation and 1 confidence level in CDU.DH) generation routines that provide the dialogue acts • intentions to perform complex acts in latest_moves from the text strings, and produce W&apos;s output text from the dialogue acts which the The current deliberation process assumes maxi-system places in next_moves. mal cooperativity, in that the system always chooses to meet its obligations whenever possible, and also (6) W [1]: How can I help? chooses to provide a maximally helpful response C [2]: A route please when possible."
"Thus, when obliged to address a W [3]: Where would you like to start? previous dialogue act such as a question or direc- C [4]: Malvern tive, it will choose to actually return the answer or W [5]: Great Malvern? perform the action, if possible, rather than reject or C [6]: Yes negotiate such a performance, which would also be W [7]: Where do you want to go?"
"C [8]: Edwinstowe acting in accordance with the obligations (see (Kreu-W [9]: Edwinstowe in Nottingham? tel, 1998) on how acts might be rejected)."
"Yes In the current implementation, the following rules W [11]: When do you want to leave? are used to adopt new intentions (i.e., to update the C [12]: Six pm INT field): W [13]: Leaving at 6 p.m.?"
"Yes (5) 1. add an intention to acknowl- W [15]: Do you want the quickest or the edge(W,CDU), given an obligation to shortest route? perform a u-act, if everything in CDU is C [16]: Quickest sufficiently understood (i.e., to level 2); W [17]: Please wait while your route is cal- 2. add an intention to accept a directive or an- culated. swer a question as the result of an obligation to address a dialogue act; We assume that before the dialogue starts, W has the intention to ask C what kind of help is required, 3. add an intention to perform an action if COND contains a conditional that will estab- liThe interchanges have been cleaned up to some extent lish an obligation to perform the action, and here, mainly by removing pauses and hesitations. and that C has the intention to find a route."
"We also i : \scp(W,slirl( malvcrnD/ / // assume that W has the turn, and that the presence l.CO~D: &lt; &gt; J N of the how can I help intention triggers an utterance uous: &lt;oul&gt; //"
"I TOGND&quot; loB DH , : .... &lt;[REF_CITE]:C2.agree(C,[REF_CITE])&gt; 11 l/ directly."
"Figure 2, presented above, shows the in-"
W: formation state after utterance [2].
"The intentions iPDU: [ &quot;[SCP: &lt;sop(C,start(malvern)}&gt; H |/ in that figure lead directly to the system producing / / J/ l/ LCOND: &lt; &gt; utterance [3]. ]"
"J / [ [OBL: &lt;Iddress(C.CAI8)&gt; 11ll Looking a little further ahead in the dialogue, Fig- / /TOGND&quot; / DH: &lt;CAIS:"
"C2.info_nequesi(W.?dest)&gt;//// ure 3 shows the information state after utterance /c~U: / / sc~: &lt; &gt; /IN [4].12 Here we can see in CDU.TOGND.DH (along with / / L~oNo: &lt; &gt; ill/ the ack act[REF_CITE]in GND.DH) the dialogue moves / uo: ,~ .ill that this utterance has generated."
"Note that the as- J C: lINT:&lt;getroute(C)&gt;l sert, CAll, is only at confidence level 1, indicating lack of sufficient certainty in this interpretation as the town &apos;Great Malvern&apos;."
This lack of certainty and
"Figure 5: Information state following [7] the resulting lack of a relevant SCP in CDU.TOGND lead the deliberation routines to produce an inten-tion to check this proposition rather than to move After C&apos;s agreement in [6], the deliberation rou-directly on to another information request."
"This tine is able to move past discussion of the start-intention leads to utterance [5], which, after inter- ing point, and add an intention to ask about the pretation and updating on the new dialogue acts, next piece of information, the destination."
This leads to the information state in Figure 4.
"The in- leads to producing utterance [7], which also implic-teresting thing here is the condition which appears itly acknowledges [6], after which C&apos;s agreement is in CDU.TOGND.COND as a result of the check; the grounded, leading to the IS shown in Figure 5."
"Note interpretation of this is that, if C agrees with the that the list in W.GND.SCP in Figure 5 indicates that check, then W will be committed to the proposition both C and W are committed to the proposition that that the starting place is Malvern (C would also be the starting place is Malvern. committed to this by way of the direct effects of an agree act). 5 Conclusions"
Figure 2 and intermediate utterances.
Here we have deleted and to pick out one or two illustrative examples to these aspects from the figures for brevity and clarity. highlight the implementational approach which has been assumed.
Current and future work is directed J. Kreutel. 1998.
An obligation-driven computa-towards measuring the theory against more challeng- tional model for questions and assertions in dia-ing data to test its validity; cases where ground- logue.
"Master&apos;s thesis, Department of Linguistics, ing is less automatic are an obvious source of such University of Edinburgh, Edinburgh. tests, and we have identified a few relevant problem S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999. cases in the Autoroute dialogues."
"We do claim, how-"
"Technical Report Deliverable ever, that the implementation as it stands validates D2.2 - Manual, Trindi. a number of key aspects of the theory and provides M. Poesio, R. Cooper, S. Larsson, D. Traum, and a good basis for future work in dialogue modelling."
C. Matheson. 1999.
Annotating conversations for
"We empirically show that there are significant differ- [In its future population estimates&apos;] [made (3) ences between the discourse structure of Japanese public last year,2] [the Ministry of Health and texts and the discourse structure of their corre- Welfare predicted that the SAB would drop to sponding English translations."
"To improve trans- a new low of 1.499 in the future,s) [but would lation quality, we propose a computational model make a comeback after that,4] [increasing once for rewriting discourse structures."
"When we train again,s] [However, it looks as if that prediction our model on a parallel corpus of manually built will be quickly shattered.6] Japanese and English discourse structure trees, we"
Our aim in this paper is to identify genre- develop reasonable statistical pronominalization al-gorithms. independent factors that influence the decision to pronominalize.
"Results based on the annotation of twelve texts from four genres show that only a few factors have a strong influence on pronominaliza-tion across genres, i.e. distance from last mention, agreement, and form of the antecedent."
"Finally, we describe a probabilistic model of pronominalization derived from our data."
Generating adequate referring expressions is an ac- tive research topic in Natural Language Generation.
Adequate referring expressions are those that en-able the user to quickly and unambiguously identify the discourse entity that the expression co-specifies with.
"In this paper, we concentrate on an important aspect of that question, which has received less at-tention than the question of anaphora resolution in discourse interpretation, i.e., when is it feasible to pronominalize?"
Our aim is to identify the central factors that in-fluence pronominalization across genres.
"Section 2 motivates and presents the factors that were investi-gated in this study: distance from last mention, par-allelism, ambiguity, syntactic function, agreement, sortal class, syntactic function of the antecedent and form of the antecedent."
"Our analyses are based on a corpus of twelve texts from four different genres with a total of more than 24,000 words and 7126 referring expressions (Section 3)."
The results of the statistical analyses are summarized in Section 4.
There are strong statistical associations between each of the factors and pronominalization.
"Only when we combine them into a probabilistic model we can identify those factors whose contribution is really important, i.e. distance from last mention, agreement, and to a certain degree form of the an-tecedent."
Since these factors can be annotated tel-
"Lately, a number of researchers have done corpus-based work on NP generation and pronoun resolu-tion, and a number of studies have found differences in the frequency of both personal and demonstrative pronouns across genres."
"However, none of these studies compares the influence of different factors on pronoun generation across genres."
"Recently,[REF_CITE]have described a corpus-based approach to statistical NP generation."
"While they ask the same question as previous re-searchers (e.g.[REF_CITE]), their methods differ from traditional work on NP generation."
"On the basis of these annotations, they constructed de-cision trees for predicting surface forms of referring expressions based on these factors - with good re-sults: all 28 personal pronouns in their corpus were generated correctly."
"Unfortunately, they do not eval-uate the contribution of each of these factors, so we do not know which ones are important."
Work on corpus-based approaches to anaphora resolution is more numerous.
"The factors they use include dis-tance from last mention, syntactic function and con-text, agreement information, animacy of the refer-ent, a simplified notion of selectional restrictions,"
"Agree Agreement in person, gender, and number texts which are not available to us, and (2) we have Syn Syntactic function not yet found a labelling scheme for discourse struc- Class Sortal Class (cf."
"Tab. 2) SynAnte ture that has an inter-coder reliability comparable toSyntactic function of antecedent. & quot;F&quot; for first mention, &quot;N&quot; for deadend the MUC coreference annotation scheme."
"FormAnte Form of antecedent (pers. pron., poss."
"Based on our review of the literature and relevant pron., def."
"NP, proper name) work in linguistics (for sortal class, mainly Fraurud Dist Distance to last mention in units (1996) and[REF_CITE]), we have chosen the Dist4 Dist reduced to 4 values (deadend, nine factors listed in Table 1."
"Methodologically, we Dist=0, Dist= 1, Dist&gt;=2) distinguish two kinds of factors:"
Par Parallelism (Syn=SynAnte)
Ambig NP-level factors are independent from co-Number of competing discourse entities specification relations.
They depend on the semantics of the discourse entity or on discourse Table 1: Overview of factors information supplied for the NP generation algo-rithm by the NLG system.
Typical examples are and the length of the coreference chain.
"Cardie &amp; NP agreement by gender, number, person and case,[REF_CITE]describe an unsupervised algorithm the syntactic function of the NP (subject, object, for noun phrase coreference resolution."
"Their fac- PP adjunct, other), the sortal class of the discourse tors are taken[REF_CITE], with two excep-entity to which an NP refers, discourse structure, or tions."
"First, they replace complete syntactic infor-topicality of the discourse entities."
"In this paper, we mation with information about NP bracketing."
"Sec-focus on the first three factors, agreement (Agree), ond, they use the sortal class of the referent which syntactic function (Syn), and sortal class (Class). they determine on the basis of WordNet (Fellbaum, Since we are using syntactically annotated data 1998). in the Penn Treebank-II format, the syntactic func-"
"There has been no comparison between corpus-tion of an NP was derived from these annotations. based approaches for anaphora resolution and more Agreement for gender, number, and person was la-traditional algorithms based on focusing (Sidner, belled by hand."
"Since English has almost no nomi- 1983) or centering[REF_CITE]except for nal case morphemes, case was not annotated."
"However, their comparison Sortal classes provide information about the dis-is flawed by evaluating a syntax-based focus algo-course entity that a referring expression evokes or rithm on the basis of insufficient syntactic informa-accesses."
"The classes, summarized in Table 2, were tion."
"For pronoun generation, the original centering derived from EuroWordNet BaseTypes (Vossen, model[REF_CITE]provides a rule which is 1998) and are defined extensionally on the basis supposed to decide whether a referring expression of WordNet synsets."
Their selection was motivated has to be realized as a pronoun.
"However, this rule by two main considerations: all classes should oc-applies only to the referring expression which is the cur in all genres, and the number of classes should backward-looking center (Cb) of the current utter-be as small as possible in order to avoid problems ance."
With respect to all other referring expression with sparse data.
"Four classes, State, Event, Action, in this utterance centering is underspecified. and Property, cover different types of situations, Yeh &amp;[REF_CITE]propose a set of hand-two cover spatiotemporal characteristics of situa-crafted rules for the generation of anaphora (zero tions (Loc/Time)."
"The four remaining classes cover and personal pronouns, full NPs) in Chinese."
"How-the two dimensions &quot;concrete vs. abstract (Con-ever, the factors which appear to be important in cept)&quot; and &quot;human (Pers) vs. non-human (PhysObj) their evaluation are similar to factors described vs. institutionalised groups of humans (Group)&quot;. by authors mentioned above: distance, syntactic"
"Since we are only interested in the decision constraints on zero pronouns, discourse structure, whether to employ pronouns rather than full NPs salience and animacy of discourse entities. and less in the form of the NP itself, and since our 2.2 Our Factors methodology is based on corpus annotation, we did The factors we investigate in this paper only rely on not take into account more formal semantic cate-annotations of NPs and their co-specification rela- gories such as kinds vs. individuals. tions."
"We did not add any discourse structural anno- Co-specification-level factors depend on infor-tation, because (1) the texts are extracts from larger mation about sequences of referring expressions cess the same discourse entity."
"In this paper, we use CL."
"The high number of pronouns in CK and CL the following factors from the literature: distance is partly due to the fact that in one text from each to last mention (Dist and Dist4), ambiguity (Am- genre, we have a first person singular narrator."
"CK big), parallelism (Par), form of the antecedent (For- patterns with CF and CG in the average number mAnte), and syntactic function of the antecedent of MCUs; the sentences in the sample from mys- (SynAnte)."
"We also distinguish between discourse tery fiction are shorter and arguably less complex. entities that are only evoked once, deadend entities, CL also has disproportionally few deadend refer-and entities that are accessed repeatedly. ents."
The high percentage of deadend referents in Parallelism is defined on the basis of syntactic CK is due to the fact that two of the texts deal with function: a referring expression and its antecedent relationship between two people.
These four dis-are parallel if they have the same syntactic function. course referents account for the 4 longest corefer-
"For calculating distance and ambiguity, we seg- ence chains in CK (85, 96, 109, and 127 mentions). mented the texts into major clause units (MCUs)."
"Two annotators (the authors, both trained lin-"
"Each MCU consists of a major clause C plus guists), hand-labeled the texts with co-specification any subordinate clauses and any coordinated major information based on the specifications for the Mes-clauses whose subject is the same as that of C and sage Understanding Coreference task (Hirschman where that subject has been elided. &amp;[REF_CITE]; for theoretical reasons, we did Dist provides the number of MCUs between the not mark reflexive pronouns and appositives as co-current and the last previous mention of a discourse specifying)."
The MCUs were labelled by the sec-entity.
"When an entity is evoked for the first time, ond author."
All referring expressions were anno- Dist is set to &quot;D&quot;.
Dist4 is derived from Dist by as- tated with agreement and sortal class information. signing the fixed distance 2 to all referring expres-
Labels were placed using the GUI-based annotation sions whose antecedent is more than 1 MCU away. tool REFEREE[REF_CITE].
"Ambiguity is defined as the number of all discourse The annotators developed the Sortal Class anno-entities with the same agreement features that occur tation guidelines on the basis of two training texts. in the previous unit or in the same unit before the Then, both labellers annotated two texts from each current referring expression. genre independently (eight in total)."
These eight texts were used to determine the reliability of the 3 Data sortal class coding scheme.
"Since sortal class an-notation is intrinsically hard, the annotators looked Our data consisted of twelve (plus two) texts from up the senses of the head noun of each referring NP the Brown corpus and the corresponding part-of-that was not a pronoun or a proper name in Word-speech and syntactic annotations from the Penn Net."
Each sense was mapped directly to one or more Treebank[REF_CITE].
The texts were selected of the ten classes given in Table 2.
The annotators because they contained relatively little or no direct speech; segments of direct speech pose problems for then chose the adequate sense. both pronoun resolution and generation because of The reliability of the annotations were measured
Genre words ref. expr. entities sequ..
"MCUs % pron. %deadend med. len. number of sequences of co-specifying referring expressions. % deadend: percentage of discourse entities mentioned only once. % pronouns: percentage of all referring expressions realized as pronouns, in brackets: perc. of first person singular pronouns, perc. of second person singular pronouns, perc. of third person singular masculine and feminine pronouns, reed. len.: median length of sequences of co-specifying referring expressions with Cohen&apos;s n[REF_CITE]."
"Co- both for all referring expressions and for those that hen (1960) shows that a n between 0.68 and 0.80 al- occur in sequences of co-specifying referring ex-lows tentative conclusions, while e; &gt; 0.80 indicates pressions."
All of the tests were significant at the reliable annotations.
"For genres CF (n = 0.83), CK p &lt; 0.001-level, with the exception of Par: for ex- (n = 0.84) and CL (n = 0.83), the sortal class an- pressions that are part of co-specification sequences notations were indeed reliable, but not for genre CG the effect of that factor is not significant. (n = 0.63)."
"Nevertheless, overall, the sortal class In the next analysis step, we determine which of annotations were reliable (n ----0.8)."
"Problems are the feature values are associated disproportionally mainly due to the abstract classes Concept, Action, often with pronouns, and which values tend to be Event, State, and Property."
Abstract head nouns associated with full NPs.
"More specifically, we test sometimes have several senses that fit the context for each feature-value pair if the pronominalization almost equally well, but that lead to different sor- probability is significantlyhigher or lower than that tal classes."
"Another problem is metaphorical usage. computed over (a) the complete data set, (b) all re-"
"This explains the bad results for CG, which features ferring expressions in sequences of co-specifying many abstract discourse entities. referring expressions, (c) all third person referring expressions in sequences."
"Almost all feature values 4 Towards a Probabilistic show highly significant effects for (a) and (b), but"
Genre-Independent Model some of these effects vanish in condition (c).
"Be-In this section, we investigate to what extent the fac- low, we report on associations which are significant tors proposed in section 2.2 influence the decision to at p &lt; 0.001 under all three conditions. prominalize."
"For the purpose of the statistical analy- Unsurprisingly, there is a strong effect of agree-sis, pronominalization is modelled by a feature Pro. ment values: NPs referring to the first and second For a given referring expression, that feature has the person are always pronominalized, and third person value &quot;P&quot; if the referring expression is a personal masculine or feminine NPs, which can refer to per-or a possessive pronoun, else &quot;N&quot;."
"We model this sons, are pronominalized more frequently than third variable with a binomial distribution."
I person neuter and third person plural.
Pronouns are strongly preferred if the distance to the antecedent is 4.1 How do the Factors Affect also notice strong genre-independent effects of par- ear combination of the predictor variables.
"Although at first glance, Ambig appears to able weights indicate the importance of a variable have a significant effect as well, (median ambiguity for classification: the higher the absolute value of for nouns is 3, median ambiguity for pronouns 0), the weight, the more important it is. closer inspection reveals that this is mainly due to Logistic regression models are not only evaluated first and second person and third person masculine by their performance on training and test data."
We and feminine pronouns. could easily construct a perfect model of any train-
"The sortal classes show a number of interest- ing data set with n variables, where n is the size of ing patterns (cf."
Not only do the classes the data set.
"But we need models that are small, yet differ in the percentage of deadend entities, there predict the target values well."
"A suitable criterion are also marked differences in pronominalizabil- is the Akaike Information Criterion (AIC, Akaike ity."
"There appear to be three groups of sortal (1974)), which punishes both models that do not fit classes: Person/Group, with the lowest rate of dead-end entities and the highest percentage of pro-nouns - not only due to the first and second per-son personal pronouns-, Location/PhysObj, with roughly two thirds of all entities not in sequences and a significantly lower pronominalization rate, and Concept/Action/Event/Property/State/Concept, with over 80% deadend entities."
"Within this group, Action, Event, and Concept are pronominalized more frequently than State and Property."
Time is the least frequently pronominalized class.
"An impor-tant reason for the difference between Loc and Time might be that Times are almost always referred back to by temporal adverbs, while locations, especially towns and countries, can also be accessed via third person neuter personal pronouns."
Interactions between the factors and genre were examined by an analysis of deviance run on a fit-ted logistic regression model; significance was cal-culated using the F-test.
All factors except for Par show strong (p &lt; 0.001) interactions with Genre.
"In other words, the influence of all factors but paral-lelism on pronominalization is mediated by Genre."
"There are two main reasons for this effect: first, some genres contain far more first and second per-son personal pronouns, which adds to the weight of Agree, and second, texts which are about persons and the actions of persons, such as the texts in CK and CL, tend to use more pronouns than texts which are mainly argumentative or expository."
"To separate the important from the unimportant fac-tors, many researchers use decision and regression trees, mostly the binary CART variant[REF_CITE]."
"We use a different kind of model here, logistic regression, which is especially well suited for categorical data analysis (cf. eg."
The quality of a factor is judged by the amount of variation in the target variable that it ex-plains.
Note that increased prediction accuracy does not necessarily mean an increase in the amount of variation explained.
"As the model itself is a contin-uous approximation of the categorical distinctions to be modelled, it may occur that the numerical vari-ation in the predictions decreases, but that this de-crease is lost when re-translating numerical predic-tions into categorical ones."
The factors for our model were selected based on the following procedure: We start with a model that always predicts the most frequent class.
"We then de-termine which factor provides the greatest reduction in the AIC, add that factor to the model and retrain."
This step is repeated until all factors have been used or adding another factor does not yield any signifi-cant improvements anymore.3
"This procedure invariably yields the sequence Dist4, Agree, Class, FormAnte, Syn, SynAnte, Am-big, Par, both when training models on the complete data set and when training on a single genre."
"Inspec-tion of the AIC values suggests that parallelism is the least important factor, and does not improve the AIC significantly."
"Therefore, we will discard it from the outset."
All other factors are maintained in the initial full model.
This model is purely additive; it does not include interactions between factors.
"This approach allows us to filter out factors which only mediate the influence of other factors, but do not ex-ert any significant influence of their own."
Note that this probabilistic model only provides a numerical description of how its factors affect pronominaliza-tion in our corpus.
"As such, it is not equivalent to a theoretical model, but rather provides data for fur- 3WeexcludedDist from this stepwiseprocedure,sincethe or[REF_CITE])."
"In this model, the value relevant information is coveredalready by Dist4, which fur-of the binary target variable is predicted by a lin- thermorehasmuchfewervalues."
Class Act Concept Event Group Loc Pers PhysObj Prop State Time
CF CG CK CL all influence is the form of the antecedent.
"The syn- % correct 97.1 93.5 93.6 91.5 93.1[REF_CITE].7 654.8 786.1 904.0 2685.8 % variation 83.0 65.4 70.1 65.4 68.7 Table 5: Quality of models fitted to each of the genre-specific corpora (CF, CG, CK, CL) and the complete data set (all). % correct: correctly pre-dicted pronominalization decition, AIC:"
"Akaike In-formation Criterion, % variation: percentage of original variation in the data (as measured by de-viance) accounted for by the model ther theoretical interpretation."
Results of a first evaluation of the full model are summarized in Table 5.
The model can ex-plain more than two thirds of the variation in the complete data set and can predict pronominalization quite well on the data it was fitted on.
The mat-ter becomes more interesting when we examine the genre-specific results.
"Although overall prediction performance remains stable, the model is obviously suited better to some genres than to others."
"The best results are obtained on CF, the worst on CL (mys-tery fiction)."
"In the CL texts, MCUs are short, a third of all referring expressions are pronouns, there is no first person singular narrator, and most para-graphs which mention persons are about the inter- tactic function of the referring expression and of its antecedent are less important, as is ambiguity."
"In order to examine the importance of the fac-tors in more detail, we refitted the models on the complete data set while omitting one or more of the three central features Dist4, Agree, and Class."
The results are summarized in Table 6.
"The most inter-esting finding is that even if we exclude all three factors, prediction accuracy only drops by 3.2%."
"This means that the remaining 4 factors also con-tain most of the relevant information, but that this information is coded more &quot;efficiently&quot;, so to speak, in the first three."
"Speaking of these factors, ques-tions concerning the effect of sortal class remains."
"Remarkably enough, when sortal class is omitted, accuracy increases by 0.7%."
The increase in A1C can be explained by a decrease in the amount of explained variation.
"A third result is that informa-tion about the form of the antecedent can substitute for distance information, if that information is miss-ing."
Both variables code the crucial distinctions be-tween expressions that evoke entities and those that access evoked entities.
"Furthermore, a pronominal antecedent tends to occur at a distance of less than 2 MCUs."
"The contribution of syntactic function re-mains stable and significant, albeit comparatively unimportant."
"Predictive Power: To evaluate the predictive action between two persons. power of the models computed so far, we determine The Relative Importance of Factors."
All val- the percentage of correctly predicted pronouns and ues of Dist4 have very strong weights in all mod- NPs.
The performance of the trained models was els; this is clearly the most important factor.
"The compared to two very simple algorithms: same goes for Agree, where the first and second per-son are strong signs of pronominalization, and, to a Algorithm A: Always choose the most frequent lesser degree, masculine and feminine third person option (i.e. noun). singular."
The most important distinction provided
"Algorithm B: If the antecedent is in the same by Class appears to be that between Persons, non-"
"MCU, or if it is in the previous MCU and there Persons, and Times."
"This holds as well when the is no ambiguity, choose a pronoun; else choose model is only trained on third person referring ex- a noun. pressions."
"For singular referring expressions, Per-sonhood information is reflected in gender, but not Table 7 summarises the results of the compari-for plural referring expressions."
Another important son.
To determine the overall predictive power of excluded ~ explained variationfit
We have described a probabilistic model of pronom-is to be pronominalised or not.
"Setup for genres: inalization that is able to correctly predict 93% of model is trained on three genres, tested on the re-all pronouns in a corpus that consists of twelve texts maining one the model, we used 10-fold cross-validation."
"Al-gorithm A always fares worst, while algorithm B, which is based mainly on distance, the strongest fac-tor in the model, performs quite well."
"Its overall performance is 3.2% below that of the full model, and 3.6% below that of the full model without sor-tal class information."
"It even outperforms the mod-els on CG, which has the lowest percentage of Per-sons (12.9% vs. 35%[REF_CITE].4% and 43.5% for CL and CK)."
"For all other genres, the statistical models outperform the simple heuristics."
Excluding sortal class information can boost prediction perfor-mance on unseen data by as much as 0.4% for the complete corpus.
"The apparent contradiction be-tween this finding and the results reported in the previous section can be explained if we consider that not only were some sortal classes comparatively rare in the data (Property, Event), but that our sortal class definition may still be too fine-grained."
We evaluated the genre-independence of the model by training on three genres and testing on the fourth.
"The results show that the model fares quite well for genre CF, which is also the genre where the overall fit was best (see Table 5)."
We therefore hy- from four different genres.
"Since the model was de-rived from a limited corpus and a limited number of genres, we cannot guarantee that our results are ap-plicable to all texts without modifications."
"But since its performance on our sample is consistently above 90% correct, we are reasonably confident that our main findings will hold for a wide variety of texts and text types."
"In particular, we isolated several fac-tors which are robust predictors of pronominaliza-tion across genres: distance from last mention and agreement, and to a certain extent the form of the antecedent, which appears to be a good substitute if the other two factors are not available."
"All three fea-tures can be computed on the basis of a chunk parse, a rough morphosyntactic analysis of the resulting NPs, and co-specification sequences."
"In computa-tional terms, they are comparatively cheap."
"Large corpora can be annotated relatively quickly with this information, which can then be used for statistical pronoun generation."
"The comparatively expensive sortai class anno-tation, on the other hand, was not very important in the final model; in fact, prediction accuracy de-creased when sortal class was included."
"There are two main reasons for this: first, the proposed sortal class annotation scheme needs further work, second, the relationship between sortal class and erence in discourse."
"In ACL &apos;99 Workshop on the pronominalization may well be too intricate to be Relationship between Discourse~Dialogue Structure modelled by the factor Class alone. and Reference, University of Maryland,[REF_CITE]"
We set out to find a genre-independent model
"Fellbaum, Christiane (Ed.) (1998)."
An Elec-of pronominalization.
The model we found per-tronic Lexical Database.
"Cambridge, Mass.: MIT forms quite well, but genre still considerably affects its performance."
"Where does the remaining, unex-"
Cognitive ontology and NP form. plained variation come from?
The variation might be just that - stylistic variation.
It might stem from one of the traditional factors that we did not take
"In T. Fretheim &amp; J. Gundel (Eds.), Reference and Referent Accessibility, pp. 65-87."
"Amsterdam, The Netherlands: Benjamins. into account here, such as thematic role."
"However, Ge Niyu, John Hale &amp;[REF_CITE]."
"A sta-, we suspect that the crucial factor at play here is dis-course structure (McCoy &amp;[REF_CITE]). tistical approach to anaphora resolution."
"In Proceed-ings of the Sixth Workshop on Very Large Corpora, Montr6al, Canada, pp. 161-170."
"Acknowledgements Work on this paper was be- Grosz, Barbara J., Aravind K. Joshi &amp; Scott Weinstein gun while Michael Strube was a postdoctoral fellow at the Institute for Research in Cognitive Science, University of Pennsylvania, and Maria Wolters vis- (1995)."
Centering: A framework for modeling the lo-cal coherence of discourse.
"Computational Linguis- would like to thank Kathleen McCoy, Jonathan De-"
"Cristofaro, and the three anonymous reviewers for muc. sais. com/proceedings/."
"Ihaka, Ross &amp;[REF_CITE]."
R: A language their comments on earlier stages of this work. for data analysis and graphics.
"Journal of Computa- tional and Graphical Statistics, 5:299-314."
This paper describes a method for linear text seg-mentation which is twice as accurate and over seven times as fast as the state-of-the-art[REF_CITE].
Inter-sentence similarity is replaced by rank in the local context.
Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address sew~ral topics or different aspects of the same topic.
The aim of linear text segmentation is to discover the topic boundaries.
"The uses of this procedure include information retrieval[REF_CITE], summarizati[REF_CITE], text understanding, anaphora resoluti[REF_CITE], language mod-elling ([REF_CITE]17) and improving document navigation for the visually disabled[REF_CITE]."
This paper focuses on domain independent meth-ods for segmenting written text.
We present a new algorithm that builds on previous work by Reynar[REF_CITE].
The primary distinc-tion of our method is the use of a ranking scheme and the cosine similarity measure (van[REF_CITE]) in formulating the similarity matrix.
We pro-pose that the similarity values of short text segments is statistically insignificant.
"Thus, one can only rely on their order, or rank, for clustering."
"Existing work falls into one of two categories, lexical cohesion methods and multi-source methods[REF_CITE]."
The former stem from the work of Halliday and Hasan[REF_CITE].
"They pro-posed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repe-titi[REF_CITE], context vectors[REF_CITE], entity repetiti[REF_CITE], semantic simi-larity[REF_CITE], word distance model[REF_CITE]and word frequency model[REF_CITE]to detect cohesion."
"Methods for finding the topic boundaries include s-liding window[REF_CITE], lexical chains[REF_CITE], dynamic programming[REF_CITE], agglomer-ative clustering[REF_CITE]and divisive clustering[REF_CITE]."
Lexical cohesion methods are typi-cally used for segmenting written text in a collection to improve information retrieval[REF_CITE].
"Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phras-es, prosodic features, reference, syntax and lexical attracti[REF_CITE]using decision trees[REF_CITE]and probabilis-tic models[REF_CITE]."
Work in this area is largely mo-tivated by the topic detection and tracking (TDT) initiative[REF_CITE].
The focus is on the segmentation of transcribed spoken text and broad-cast news stories where the presentation format and regular cues can be exploited to improve accuracy.
Our segmentation algorithm takes a list of tokenized sentences as input.
A tokenizer[REF_CITE]and a sentence boundary disam-biguation algorithm[REF_CITE]or EAGLE[REF_CITE]may be used to convert a plain text docu-ment into the acceptable input format.
Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern mateher and a stopword list.
A stemming algorithm[REF_CITE]is then applied to the re-maining tokens to obtain the word stems.
A dic-tionary of word stem frequencies is constructed for each sentence.
This is represented as a vector of frequency counts.
"Let fi,j denote the frequency of word j in sentence i."
"The similarity between a pair of sentences x,y is computed using the cosine measure as shown in Each value in the similarity matrix is replaced by equation 1."
This is applied to all sentence pairs to its rank in the local region.
The rank is the num-generate a similarity matrix. ber of neighbouring elements with a lower similarity value.
"Figure 2 shows an example of image ranking E:, f.~., x :~., using a 3 x 3 rank mask with output range {0, 8}. sim(x,y) = ~_~., .~., ,,, f2.xEjf2. (1)"
"For segmentation, we used a 11 x 11 rank mask."
The output is expressed as a ratio r (equation 2) to cir-cumvent normalisation problems (consider the cases Figure 1 shows an example of a similarity matrix ~. when the rank mask is not contained in the image).
High similarity values are represented by bright pix-els.
"The bottom-left and top-right pixel show the Similarity matrix Rank matrix Similarity matrix Rank matrix self-similarity for the first and last sentence, respec- tively."
Notice the matrix is symmetric and contains I 5 bright square regions along the diagonal.
"These re- i7 gions represent cohesive text segments. # of elements with a lower value r = ([Footnote_2]) # of elements examined demonstrate the effect of image ranking, the process was applied to the matrix shown in figure 1 to produce figure 32."
"2The process was applied to the original matrix, prior to 1The contrast of the image has been adjusted to highlight contra.st enhancement. The output image has not been en-the image features. hanced."
Notice the contrast has been 3.2 Ranking improved significantly.
"Figure 4 illustrates the more For short text segments, the absolute value of subtle effects of our ranking scheme, r(x) is the rank sire(x, y) is unreliable."
"An additional occurrence of (1 x 11 mask) of f(x) which is a sine wave with a common word (reflected in the numerator) causes decaying mean, amplitude and frequency (equation a disproportionate increase in sim(x,y) unless the 3). denominator (related to segment length) is large."
"Thus, in the context of text segmentation where a segment has typically &lt; 100 informative tokens, one can only use the metric to estimate the order of sim-ilarity between sentences, e.g. a is more similar to b than c."
"Furthermore, language usage varies throughout a document."
"For instance, the introduction section of a document is less cohesive than a section which is about a particular topic."
"Consequently, it is inap-propriate to directly compare the similarity values from different regions of the similarity matrix."
"In non-parametric statistical analysis, one com-pares the rank of data sets when the qualitative be-haviour is similar but the absolute quantities are un-reliable."
We present a ranking scheme which is an
Figure 3: The matrix in figure 1 after ranking. adaptation of that described in (O&apos;[REF_CITE]).
The final process determines the location of the topic boundaries.
The method is based on Reynar&apos;s max-imisation algorithm[REF_CITE].
"A text segment is defined by two sentences i,j (inclusive)."
This is represented as a square region along the di-agonal of the rank matrix.
"Let si,j denote the sum of the rank values in a segment and aij = (j - i + 1)2 be the inside area."
"B = {bl,...,bm} is a list of m (:oherent text segments, sk and ak refers to the sum of rank and area of segment k in B. D is the inside density of B (see equation 4)."
D (4) - )-~k~l ak
"To initialise the process, the entire document is placed in B as one coherent text segment."
Each step of the process splits one of the segments in B.
The split point is a potential boundary which maximises D. Figure 5 shows a working example.
"The number of segments to generate, m, is deter-mined automatically."
D (n) is the inside density of n segments and 5D(n) --
D (n) -D (n-l) is the gradient.
"For a document with b potential boundaries, b step-s of divisive clustering generates (D (1), ..., D (b+l)} and {SD(2),...,SD (b+l)} (see figure 6 and 7)."
An unusually large reduction in 5D suggests the opti-inal clustering has been obtained[Footnote_3] (see n = 10 in figure 7).
"3In practice, convolution (mask {1,2,4,8,4,2,1}) is first aI)plied to 5D to smooth out sharp local changes"
"Let # and v be the mean and variance of 5D(n),n E {2, ..., b + 1}. m is obtained by applying the threshold, p + c × v/~, to 5D (c = 1.2 works well in practice)."
The running time of each step is dominated by the computation of sk.
"Given si,j is constant, our algo-rithm pre-computes all the values to improve speed performance."
"The procedure computes the values a-long diagonals, starting from the main diagonal and 4 ....kl t&apos;1"
The definition of a topic segment ranges from com-plete stories[REF_CITE]to summaries ([REF_CITE]~ and[REF_CITE]).
"Given the quality of an algorithm is task dependent, the following experiments focus 003 on the relative performance."
Our evaluation strat-egy is a variant of that described in ([REF_CITE]
O.02 71-73) and the TDT segmentation task[REF_CITE].
We assume a good algorithm is one that finds the most prominent topic boundaries. the gradient.
4 3 i = 411i
A sample is characterised by the range of n. The corpus was generated by an auto-matic procedure5.
Table [Footnote_1] presents the corpus s-works towards the corner.
"1 i i | , i 4Only the news articles ca**.pos and informative text cj**.pos were used in the experiment. 5All experiment data, algorithms, scripts att(I detailed re- Figure 8: Improving speed performance by pre- suits are available from the author. computing si,j. 6All experiments were conducted on a[REF_CITE]Mllz PC wi h 128Mb RAM running RedHat Linux 6.0 and tilet Blackd wn Linux port of .IDK1.1.7 v3.o"
"The method has a com-tatistics. plexity of order 12Ln2• Let ri,j refer to the rank value in the rank matrix R and S to the sum of rank ma-"
I Range of n I 3- 11 - - 11 trix.
"Given R of size n × n, S is computed in three samples [ 400 I 310: I 610: I 9100 I # steps (see equation 5)."
Figure 8 shows the result of applying this procedure to the rank matrix in figure 5.
Table 1: Test corpus statistics. boundaries as real boundaries.
"B(r,b) randomly se-lects b boundaries as real boundaries."
The accuracy of the last two algorithms are com-puWA analytically.
We consider the status of m po-tential bomldaries as a bit string (1 --+ topic bound-ary).
The terms p(miss) and p(fa) in equation 6 cor-responds to p(samelk ) and p(difflk) = 1-p(same[k).
"Equatioll 7, 8 and 9 gives the general form of p(samelk ), B(.,.?) and B(r,b), respectively[Footnote_7]."
7The full derivation of our method is available from the author.
We compare three versions of the TextTiling algo-rithm[REF_CITE].
"H94(c,d) is Hearst&apos;s C im-"
Five versions of Reynar&apos;s optimisation algorithm[REF_CITE]were evaluated.
R98 and R98(min) are exact implementations of his maximisation and minimisation algorithm.
"R98(~,~o~) is my version of the maximisation algorithm which uses the cosine coefficient instead of dot density for measuring sim-ilarity."
It incorporates the optimisations described in section 3.4.
"R98(m,dot) is the modularised version of R98 for experimenting with different similarity measures."
"R98(m,s,) uses a variant of Kozima&apos;s semantic sim-ilarity measure[REF_CITE]to compute block similarity."
Word similarity is a function of word co-occurrence statistics in the given document.
Word-s that belong to the same sentence are considered to be related.
"Given the co-occurrence frequen-cies f(wi, wj), the transition probability matrix t is computed by equation 10."
Iw ) =
Ej s=norm(~t&apos;)i=l (11)
Experimental result (table 4) shows the cosine co-efficient and our spread activation method improved segmentation accuracy.
The speed optimisations sig-nificantly reduced the execution time.
We compare three versions of Segmenter[REF_CITE].
K98(B) is the original Perl implementation of the algoritlun (version 1.6).
"K98(j) is my imple- use mentation of the algorithm, K98(j,a) is a version of of extrema for clustering has a greater impact on accuracy than linearising the similarity scores (figure K98(j) which uses a document specific chain break- 4). ing strategy."
The distribution of link distances are used to identify unusually long links.
The threshold is a function # + c x vf5 of the mean # and variance u. We found c = 1 works well in practice.
Table 5 summarises the experimental results.
"K98(p) performed significantly better than K98g,,)."
This is due to the use of a different part-of-speech tagger and shallow parser.
The difference in speed is largely due to the programming languages and term clustering strategies.
"Our chain breaking strategy improved accuracy (compare K98(j) with K98(j,~)). and C99(b)."
The former is an exact implementation Experimental result (table 8) shows our algorith-of the algorithm described in this paper.
The latter m C99 is more accurate than existing algorithms. is given the expected number of topic segments for A two-fold increase in accuracy and seven-fold in-fair comparison with R98.
Both algorithms used a crease in speed was achieved (compare C99(b) with 11 x 11 ranking mask.
"If one disregards segmentation accuracy, H94 The first experiment focuses on the impact of our has the best algorithmic performance (linear)."
"C99, automatic termination strategy on C99(~) (table 6)."
K98 and R98 are all polynomial time algorithms.
C99(b) is marginally more accurate than C99.
This The significance of our results has been confirmed indicates our automatic termination strategy is effec- by both t-test and KS-test. tive but not optimal.
The minor reduction in speed performance is acceptable.
The second experiment investigates the effect of different ranking mask size on the performance of size.
A 1x 1 ranking mask reduces all the elements in the rank matrix to zero.
"Interestingly, the increase 5 in ranking mask size beyond 3 x 3 has insignificant"
Conclusions and future work
"A segmentation algorithm has two key elements, a effect on segmentation accuracy."
This suggests the clustering strategy and a similarity me~sure.
Our results show divisive clustering (R98) is more precise than sliding window (H94) and lexical chains (K98) for locating topic boundaries.
Four similarity measures were examined.
"The co-sine coefficient (R98(s,co,)) and dot density measure (R98(m,(lot)) yield similar results."
"Our spread activa-tion based semantic measure (R98(.....,)) improved a.ccura(:y."
"This confirms that although Kozima&apos;s ap-l)roaeh[REF_CITE]is computationally expen-sive, it does produce more precise segmentation."
"Tile most significant improvement was due to our ranking scheme which linearises the cosine coefficien-t. Our exl)eriments demonstrate that given insuffi- (:lent data, tile qualitative behaviour of the cosine m(,asul&apos;e is indeed more reliable than the actual val-"
"Although our evaluation scheme is sufficient for this (:omparative study, further research requires a large scale, task independent benchmark."
It would be interesting to corot)are C99 with the multi-source method described[REF_CITE]using the TDT corpus.
We would also like to develop a linear time and multi-source version of the algorith- IIl.
This paper has benefitted from the comments of Mary McGee Wood and the anonymous reviewer-s. Thanks are due to my parents and department tbr making this work possible; Jeffrey Reynar for discussions and guidance on the segmentation prob-lem; Hideki Kozima for help on the spread activation nmasure; Min-Yen Kan and Marti Hearst for their segmentation algorithms; Daniel Orain for references to image processing techniques; Magnus Rattray and Stephen Marsland for help on statistics and mathe-matics.
The training set for these experiments was sections 01-21 of the Penn Treebank[REF_CITE].
The test set was section 23.
The parser induction algorithm used in all of the experiments in this pa-per was a distribution of Collins&apos;s model 2 parser[REF_CITE].
All comparisons made below refer to results we obtained using Collins&apos;s parser.
The results for bagging are shown in Figure 2 and Table 1.
"The row of figures are (from left-to-right) training set F-measure~, test set F-measure, percent perfectly parsed sentences in training set, and per-cent perfectly parsed sentences in test set."
An en-semble of bags was produced one bag at a time.
"In the table, the Initial row shows the performance achieved when the ensemble contained only one bag, Final(X) shows the performance when the ensem-ble contained X bags, BestF gives the performance of the ensemble size that gave the best F-measure score."
"TrainBestF and TestBestF give the test set performance for the ensemble size that performed the best on the training and test sets, respectively."
"On the training set all of the accuracy measures are improved over the original parser, and on the test set there is clear improvement in precision and recall."
"The improvement on exact sentence accuracy for the test set is significant, but only marginally so."
"The overall gain achieved on the test set by bag-ging was 0.8 units of F-measure, but because the entire corpus is not used in each bag the initial per-formance is approximately 0.2 units below the best previously reported result."
The net gain using this technique is 0.6 units of F-measure.
The AdaBoost algorithm was presented[REF_CITE]and has become a widely-known successful method in machine learn-ing.
"The AdaBoost algorithm imposes one con-straint on its underlying learner: it may abstain from making predictions about labels of some samples, but it must consistently be able to get more than 50°-/oaccuracy on the samples for which it commits to a decision."
That accuracy is measured accord-ing to the distribution describing the importance of samples that it is given.
The learner must be able to get more correct samples than incorrect samples by mass of importance on those that it labels.
This statement of the restriction comes from Schapire and Singer&apos;s study (1998).
It is called the weak learning criterion.
They also provided a better characterization of its theo-retical performance.
"The version of AdaBoost used in their work is shown in Algorithm 3, as it is the version that most amenable to parsing."
"Algorithm: AdaBoost ([REF_CITE]&quot;) (3) Given: Training set /: as in bagging, except yi E {-1, [Footnote_1]} is the label for example xi."
"1This is the balanced version ofF-measure, where precision and recall are weighted equally."
"Initial uniform distribution D1(i) = 1/m. Number of iterations, T. Counter t = 1. tI,, ¢~, and ¢ are as in Bagging. 1. Create Lt by randomly choosing with replace-ment m samples from L: using distribution Dt. 2."
Classifier induction: Ct ~- ~(Lt) 3. Choose at E IR. 4. Adjust and normalize the distribution.
"Zt is a normalization coefficient. [Footnote_1] D, +, (i) = -~-"
"1This is the balanced version ofF-measure, where precision and recall are weighted equally."
Dt (i) exp(-c~tYiCt( xi )) 5.
Increment t. Quit if t &gt; T. 6. Repeat from step 1. 7.
"The final hypothesis is ~)boost(:g) ~- sign Z ~t¢,(x) t"
The value of at should generally be chosen to min-imize
"Z Dt (i) exp(-a~ YiCt (x,)) i in order to minimize the expected per-sample train-ing error of the ensemble, which Schapire and Singer show can be concisely expressed by I-] Zt."
"They also give several examples for how to pick an appropriate a, and selection generally depends on the possible outputs of the underlying learner."
Boosting has been used in a few NLP systems.
Each parser Ct nent.
They found they could achieve accuracies on in the ensemble gets a vote with weight at for both tasks that were competitive with the state of the constituents they predict.
Precisely those the art.
"As a side effect, they found that inspecting constituents with weight strictly larger than the samples that were consistently given the most 1 ~--~tat are put into the final hypothesis. weight during boosting revealed some faulty anno-"
A potential constituent can be considered correct tations in the corpus.
"In all of these systems, Ad-if it is predicted in the hypothesis and it exists in aBoost has been used as a traditional classification system. 3.2 Boosting for Parsing the reference, or it is not predicted and it is not in the reference."
Potential constituents that do not ap-pear in the hypothesis or the reference should not
Our goal is to recast boosting for parsing while con- make a big contribution to the accuracy computa-sidering a parsing system as the embedded learner. tion.
"There are many such potential constituents, The formulation is given in Algorithm 4."
"The in- and if we were maximizing a function that treated tuition behind the additive form is that the weight getting them incorrect the same as getting a con-placed on a sentence should be the sum of the weight stituent that appears in the reference correct, we we would like to place on its constituents."
The would most likely decide not to predict any con-weight on constituents that are predicted incorrectly stituents. are adjusted by a factor of 1 in contrast to a factor Our model of constituent accuracy is thus sim- of ~ for those that are predicted incorrectly.
Algorithm: Boosting A Parser ple.
Each prediction correctly made over T(s) will be given equal weight.
"That is, correctly hypothesizing (4) a constituent in the reference will give us one point,"
"Given corpus C with size m = ICI = ~s.~C(s,t) but a precision or recall error will cause us to miss and parser induction algorithm g."
Initial uniform one point.
"Constituent accuracy is then a/(a+b+c), distribution Dl(i) = 1/m. Number of iterations, Counter t = 1."
"T. where a is the number of constituents correctly hy-pothesized, b is the number of precision errors and c is the number of recall errors. 1. Create Ct by randomly choosing with replace- In Equation 1, a computation of aca as described ment m samples from C using distribution Dr. is shown. 2. Create parser ft ~ g(Ct). 3. Choose at E R (described below)."
D(i) 4. Adjust and normalize the distribution.
Zt is c6T(si) i a normalization coefficient.
"For all i, let parse (1: Otca = D(i) tree ~-~&apos;~-- ft(s,)."
"Let ~(T,c) be a function indi- i cCT(si) cating that c is in parse tree r, and ITI is the number of constituents in tree T. T(s) is the set of constituents that are found in the reference Boosting algorithms were developed that at- or hypothesized annotation for s."
Dt+l (i) :
The experimental results for boosting are shown in Figure 3 and Table 2.
There is a large plateau in performance from iterations 5 through 12.
"Because of their low accuracy and high degree of specializa-tion, the parsers produced in these iterations had little weight during voting and had little effect on the cumulative decision making."
"As in the bagging experiment, it appears that there would be more precision and recall gain to be had by creating a larger ensemble."
In both the bagging and boosting experiments time and resource constraints dictated our ensemble size.
In the table we see that the boosting algorithm equaled bagging&apos;s test set gains in precision and re-call.
"The Initial performance for boosting was lower, though."
"We cannot explain this, and expect it is due to unfortunate resampling of the data dur-ing the first iteration of boosting."
"Exact sentence accuracy, though, was not significantly improved on the test set."
"Overall, we prefer bagging to boosting for this problem when raw performance is the goal."
"There are side effects of boosting that are useful in other respects, though, which we explore in Section 4.2."
It was hypothesized in the course of investigating the failures of the boosting algorithm that the parser in-duction system did not satisfy the weak learning cri-terion.
It was noted that the distribution of boosting weights were more skewed in later iterations.
Inspec-tion of the sentences that were getting much mass placed upon them revealed that their weight was be-ing boosted in every iteration.
"The hypothesis was that the parser was simply unable to learn them. 3.3.2 Corpus Trimming In order to evaluate how well boosting worked with a learner that better satisfied the weak learning cri-terion, the boosting experiment was run again on the Treebank minus the troublesome sentences de-scribed above."
The results are in Table 3.
This dataset produces a larger gain in comparison to the results using the entire Treebank.
"The initial ac-curacy, however, is lower."
"We hypothesize that the boosting algorithm did perform better here, but the parser induction system was learning useful informa-tion in those sentences that it could not memorize (e.g. lexical information) that was successfully ap-plied to the test set."
In this manner we managed to clean our dataset to the point that the parser could learn each sentence in isolation.
The corpus-makers cannot necessarily be blamed for the sentences that could not be mem-orized.
"All that can be said about those sentences is that for better or worse, the parser&apos;s model would not accommodate them. 4 Corpus Analysis 4.1 Noisy Corpus: Empirical Investigation"
"To acquire experimental evidence of noisy data, dis-tributions that were used during boosting the sta-ble corpus were inspected."
"The distribution was ex-pected to be skewed if there was noise in the data, or be uniform with slight fluctuations if it fit the data well."
We see how the boosting weight distribution changes in Figure 1.
The individual curves are in-dexed by boosting iteration in the key of the figure.
This training run used a corpus of 5000 sentences.
"The sentences are ranked by the weight they are given in the distribution, and sorted in decreasing or-der by weight along the x-axis."
"The distribution was smoothed by putting samples into equal weight bins, and reporting the average mass of samples in the bin as the y-coordinate."
Each curve on this graph cor-responds to a boosting iteration.
"Since there were 5000 samples, all samples initially had a y-value of 0.0002. within the top 100 most heavily weighted trees at the end of 15 iterations of boosting the stable cor-pus."
"Collins&apos;s parser induction system is able to learn to produce any one of these structures in isolation, but the presence of conflicting information in differ-ent sentences prevents it from achieving 100% accu-racy on the set."
We suspect our best parser diversification techniques gives performance gain approximately equal to dou-bling the size of the training set.
"While this cannot be directly tested without hiring more annotators, an expected performance bound for a larger train-ing set can be produced by extrapolating from how well the parser performs using smaller training sets."
There are two characteristics of training curves for
Notice first that the left endpoints of the lines large corpora that can provide such a bound: train- move from bottom to top in order of boosting ing curves generally increase monotonically in theit- eration.
"The distribution becomes monotonically absence of over-training, and their first derivatives more skewed as boosting progresses."
Secondly we generally decrease monotonically. 39832 34.988.73 88.54 88.63 the corpus quality.
"A particular class of errors, in-consistencies, can then be investigated."
"Inconsistent Table 4: Effects of Varying Training Corpus Size annotations are those that appear plausible in iso-lation, but which conflict with annotation decisions made elsewhere in the corpus."
The training curves we present in Figure 4 and Ta-
In Figure 5 we show a set of trees selected from ble 4 suggest that roughly doubling the corpus size in the range of interest (between 10000 and 40000 sentences) gives a test set F-measure gain of approx-imately 0.70.
Bagging achieved significant gains of approxi-mately 0.60 over the best reported previous F-measure without adding any new data.
"In this re-spect, these techniques show promise for making performance gains on large corpora without adding more data or new parsers."
"We have shown two methods, bagging and boosting, for automatically creating ensembles of parsers that produce better parses than any individual in the en-semble."
"Neither of the algorithms exploit any spe-cialized knowledge of the underlying parser induc-tion algorithm, and the data used in creating the ensembles has been restricted to a single common training set to avoid issues of training data quantity affecting the outcome."
"Our best bagging system performed consistently well on all metrics, including exact sentence accu-racy."
It resulted in a statistically significant F-measure gain of 0.6 over the performance of the base-line parser.
That baseline system is the best known Treebank parser.
This gain compares favorably with a bound on potential gain from increasing the corpus size.
"Even though it is computationally expensive to create and evaluate a small (15-30) ensemble of parsers, the cost is far outweighed by the opportu-nity cost of hiring humans to annotate 40000 more sentences."
The economic basis for using ensemble methods will continue to improve with the increasing value (performance per price) of modern hardware.
"Our boosting system, although dominated by the bagging system, also performed significantly better than the best previously known individual parsing result."
We have shown how to exploit the distri-bution created as a side-effect of the boosting al-gorithm to uncover inconsistencies in the training corpus.
A semi-automated technique for doing this as well as examples from the Treebank that are in-consistently annotated were presented.
Perhaps the biggest advantage of this technique is that it requires no a priori notion of how the inconsistencies can be characterized.
We would like to thank Michael Collins for enabling all of this research by providing us with his parser and helpful comments.
This work was funded by NSF grant[REF_CITE].
The views expressed in this paper are those of the authors and do not necessarily reflect the views of the MITRE Corporation.
This work was done while both authors were at Johns Hopkins University.
The goal of this paper is to describe how the EuroWordNet framework for representing lexical meaning is being modified within an Italian National Project in order to include information on adjectives.
The focus is on the &apos;new&apos; semantic relations being encoded and on the revisions we have made to the EuroWordNet
Top Ontology structure.
We also briefly discuss the utility of the information which is being encoded for computational applications.
The Princeton WordNet (henceforth WN) is a lexical semantic network in which the meanings of words are represented in terms of their conceptual and lexical relations to other words.
"The basic notion around which it is developed is that of a synset (synonyms set), i.e. a set of words with the same Part-of-Speech (PoS) that can be interchanged in a certain context."
"Various conceptual and lexical relations are then encoded between synsets of the same PoS: e.g., hyponymy, antonymy, meronymy, etc. ([REF_CITE];"
"Within the EuroWordNet (henceforth EWN) projectI a similar (multilingual) lexical resource was developed, retaining the basic underlying design of WN, but enriching the set of lexical-semantic relations to be encoded for nouns and verbs in various ways2, in order to obtain a maximally re-usable resource for computational applications."
"Thus, a) cross-PoS (xPos) relations were added so that different surface realizations of similar concepts within and across languages could be matched (e.g., the noun research and the verb to research could be linked as 1 EWN was a project in the EC Language Engineering[REF_CITE]programme."
"In a first phase, the partners involved were the University of Amsterdam (coordinator); the Istituto di Linguistica Computazionale, CNR, Pisa; the Fundacion Universidad Empresa (a cooperation of UNED, Madrid, Politecnica de Catalunya, Barcelona, and the University of Barcelona); the University of Sheffield; and Novell Linguistic Development (Antwerp), changed to Lemout &amp; Hauspie during the project."
"In a further phase, the database was extended with German, French, Estonian and Czech."
"Complete information on EWN can be found at its web site[URL_CITE]2Adjectives and adverbs were encoded in EWN only as targets of relations from nouns and verbs. & apos;XPOSNEAR__SYNONYMS&apos; in EWN); b) some relations were identified which provide detailed information on semantic components lexicalized within word roots (e.g., to hammer could be linked to the noun hammer by means of an &apos;INVOLVED_INSTRUMENT&apos; relation); c) some labels were distinguished which could be added to relations to make their semantic entailments more explicit and precise (cf."
"The links among the wordnets of different languages were realized by means of an Interlingual-Index (ILI), constituted by an unstructured list of the Princeton WN (version 1.5) synsets."
"In addition, a hierarchy of language-independent concepts, reflecting fundamental semantic distinctions (e.g., Object and Substance, Dynamic and Static, Cause, Manner, etc.), was built: the Top Ontology (TO)."
"The TO consists of language-independent features which may (or may not) be lexicalized in various ways, or according to different patterns, in different languages[REF_CITE]."
"Via the ILI, all the concepts in the monolingual wordnets are directly or indirectly linked to the TO."
The following picture shows the EWN data structure (see also[REF_CITE]):
"While in WN all PoSs are represented, in EWN detailed information was encoded only for nouns and verbs and no analysis was carried out with respect to lexical-semantic relations which could be used to describe the semantics of adjectives and adverbs."
"In an Italian National Project we are building a large wordnet, ltalWordNet (henceforth IWN)3, by extending the network built for Italian in EWN."
"Thus, we are both increasing the coverage for nouns and verbs and adding adjectives and some adverbs4."
"To be able to encode information on adjectives we have enriched the set of the EWN lexical-semantic relations, aiming at encoding data which can be useful for computational applications."
"Moreover, we have revised the TO in order to account for the semantics of the new lexical categories being encoded."
In this paper we describe the main changes made to the EWN framework in order to encode information on adjectives in IWN.
"Firstly, we provide a brief overview of the WN treatment of adjectives."
"Then, we discuss the set of relations being encoded for this category in IWN."
"Finally, we show the integration made to the EWN TO."
We then conclude the paper by adding some remarks on the utility of the data being encoded for computational applications. 1 Adjectives in WN
In WN adjectives are divided into two major classes: descriptive adjectives and relational adjectives.
A descriptive adjective is &quot;one that ascribes a value of an attribute to a noun&quot; ([REF_CITE]:27).
"Descriptive adjectives combine with nouns to express some qualities of the thing, person or concept they designate."
"Typically, in this group we find adjectives that designate the physical dimension of an object, its weight, abstract values etc."
"Besides these referent-modifying adjectives, we also find reference-modifying adjectives (cf."
"Typical examples of the latter areformer, future, present. 3 ItalWordNet will be the reference lexical resource among various integrated language resources and software tools for the automatic treatment of the Italian written and spoken language which are being developed within the SI-TAL (&apos;Integrated System for the Automatic Treatment of Language&apos;) National Project. 4 Actually, we shall only encode adverbs derived from adjectives by adding the suffix -mente, for which a derivation relation with an adjective will be encoded."
"Relational adjectives, on the other hand, mean something like &quot;relating/pertaining to, associated with&quot;, and usually have a morphologically strong link with a noun."
"Typical examples are musical, atomic, chemical 5."
Synonymy is the basic relation encoded for all the PoSs (since it is used to build synsets).
"While in WN the noun and verb networks are then mainly developed around the superordinate (hyper/hyponymy) relationship, the organization of descriptive adjectives can &quot;be visualized in terms of barbell-like structures, with a direct antonym in the centre of each disk surrounded by its semantically similar adjectives (which constitute the indirect antonyms of the adjectives in the opposed disk)&quot; ([REF_CITE]: 212)."
"The main relation encoded for these adjective synsets is antonymy, claimed to be the most prominent relation, both from a psycholinguistic point of view and from a more strictly lexical-semantic one, in the definition of the semantics of descriptive adjectives."
Hyponymy is substituted by a &apos;similarity&apos; relation.
"Relational adjectives, on the contrary, are not organized in this way, because their semantics cannot be described by using these relations."
"Indeed, they only point to the noun to which they pertain (e.g. atomic is linked to atom)."
"Finally, information on the selectional preferences of both descriptive and relational adjectives is sometimes encoded (e.g., between high and degree), by using an &apos;is_attribute_of&apos; relation. 2 The IWN relations for adjectives"
"As for the other lexical categories, also in IWN the basic relation encoded for adjectives is synonymy, on the basis of which synsets are built."
"Following EWN, we also encode a NEAR._SYNONYMY relation when two synsets are very close in meaning but their members cannot be put in the same synset (and no other relation results appropriate to link them; see Alonge et s Note that some adjectives have both a descriptive sense and a relational one."
"For example, musicale (musical) can modify the noun voce (voice) when we want to say that a voice is sweet-sounding and melodious, but can also be combined with the noun strumento (instrument) when we want to indicate that an instrument can be used to produce music. al. 1998 for a discussion of this relation, and Alonge et al., in prep., for a complete and detailed discussion of the linguistic design of IWN)."
"Then, we encode a number of additional relations, which have been identified by taking into consideration i) theoretical works; ii) the EAGLES recommendations on semantic encoding (cf."
"Together with synonymy, the hyponymy relation constitutes the &apos;bone structure&apos; of both WN and EWN."
"However, as we have seen, in WN the possibility of encoding hyponymy for adjectives is denied and the basic relation encoded for adjectives is antonymy, while EWN did not really deal with adjectives and a complete network for them was not built."
Within IWN we have reconsidered the possibility of encoding hyponymy for adjectives.
"By analysing data coming from machine-readable dictionaries we find subsets of adjectives which have a genus + differentia definition, like nouns or verbs."
"That is, these adjectives seem to be organised into classes sharing a superordinate."
"This is the case, e.g., of adjectives indicating a &apos;containing&apos; property (acquoso - watery; alcalino - alkaline), or a &apos;suitable-for&apos; property (difensivo - defensive; educativo - educational), etc."
"In IWN we have decided, therefore, to encode hyponymy also for these sets of adjectives."
"The taxonomies which can be built on the basis of this relation are different from those built for nouns or verbs, since they are generally very flat, consisting almost always of two levels only (an exception is the color adjectives taxonomy)."
"However, by encoding a hyponymy relation for these adjectives, we obtain classes for which it will be possible to make various inferences."
"For instance, it will be possible to infer semantic preferences of certain classes: e.g., all the adjectives occurring in the taxonomy of contenente (containing) will occur as attributes of concrete nouns; adjectives found in the taxonomy of affetto (affected by an illness) will never be predicated of nouns referring to objects, etc."
"Furthermore, it will also be possible to infer information on syntactic characteristics of adjectives found in the same taxonomy: e.g., the hyponyrns of atto (suitable for) are always found in predicative position (and do not accept any complements); the hyponyms of privo (lacking) may occur both in attributive and in predicative position (and may take certain prepositional complements), etc."
"As it was done for all the relations identified in EWN, we have built substition tests or diagnostic frames based on normality judgements (cf."
Inserting two words in the test sentences built evokes a &apos;normality&apos;/ &apos;abnormality&apos; judgement on the basis of which each relation can be determined.
"These tests are used by encoders both to verify the existence of relations between synsets and to encode them in a consistent way (for a complete lists of the tests built see Alonge et al., in prep.)"
"As in WN, also in IWN the antonymy relation remains an important relation to describe the semantics of various adjectives."
"Following theoretical work[REF_CITE], we have further distinguished between COMPLEMENTARY ANTONYMY and GRADABLE ANTONYMY6."
The former relation links adjectives referring to opposing properties/ concepts: when one holds the other is excluded (alive~dead).
The latter relation is used for those antonym pairs which refer to gradable properties (long~short).
"In case it is not clear if two opposing adjectives refer to complementary or gradable properties, we can still use an underspecified ANTONYMYrelation."
Also this information can be useful for computational applications since word pairs presenting one of the two kinds of opposition may occur in different contexts (cf.
"In WN a relation between adjectives and nouns is encoded for relational adjectives which point to a noun to which they &apos;pertain&apos;: atomic~atom, industrial~industry, etc."
"This relation will be encoded also in IWN, by using the label PERTAINS TO."
Another relation &apos;inherited&apos; from WN can be useful to distinguish both adjective senses and their semantic preferences7: altol (tall) IS A VALUEOF statura (stature) alto2 (high)
IS_A_VALUEOF altezza (height).
"Other relations are then being encoded which are not in WN, but are encoded for nouns and verbs in EWN."
"In WN each PoS forms a separate system of language-internal relations and conceptually close concepts are totally separated only because they differ in PoS. In EWN, instead of using as main classificatory criterion the traditional distinction among PoSs, drawn upon heterogeneous criteria, a purely semantic distinction was adopted (following[REF_CITE])."
"Thus, a distinction was drawn among Ist order entities (loes - referred to by concrete nouns), 2&quot;d order entities (2oes - referred to by verbs, adjectives or nouns indicating properties, states, processes or events), and 3rdorder entities (3oes referred to by abstract nouns indicating propositions existing independently of time and space)."
"By drawing this distinction it was possible to relate lexical items that, either within a language or across different languages, refer to close concepts, although they belong to different PoSs."
"Thus, as said above, the possibility to encode &apos;near-synonymy&apos; between synsets of the same order (but different PoSs) was provided."
"Furthermore, other cross-PoS relations were identified which allow to obtain a better description of word meanings."
In IWN we maintain the same distinction among semantic orders and encode for adjectives some relations which can be encoded for the other 2oes.
"In particular, we encode the &apos;INVOLVED&apos; and &apos;CAUSE&apos; relations."
"The INVOLVEDrelation links a 2oes with aloe or 3oe referring to a concept incorporated within 6 A similar distinction is also made within the SIMPLE EC project[REF_CITE], whose goal is adding semantic information to the set of harmonized lexicons built within the PAROLE project for twelve European languages."
"Of course, the sub-classification ofantonymy can also be used for nouns and verbs. 7Furthermore, these relations being encoded between an adjectival synset and a nominal or verbal one are also useful to distinguish adjective classes as described[REF_CITE], and reported[REF_CITE]."
"Indeed, such classes are often indicated by the nouns linked to adjectives. the meaning of the 2oe8."
"Examples for adjectives are given in the following: filoso (= pieno di fili) (thready, filamentous) HAS_HYPERONYM pieno (full) INVOLVED filo (thread) imberbe (= privo di barba) (beardless)"
HAS_HYPERONYM privo (lacking)
INVOLVED barba (beard).
Another relation which can be encoded is the CAUSE relation:
"A new relation, not present either in WN or in EWN, will be encoded for a class of adjectives indicating the possibility of some events occurring: giudicabile (= che pub essere giudicato) (triable)"
"LIABLE_TO giudicare (tojudge) inaccostabile, inavvicinabile (= che non pub essere avvicinato) (which cannot be approached) LIABLETO awicinare, accostare (to approach) negative. 8 E.g., to lapidate has as INVOLVED_INSTRUMENT stone; to work has as INVOLVED_AGENT worker (see[REF_CITE]). 9 As said above, in EWN various features were encoded to make implications of relations explicit: conjunction and disjunction (for multiple relations of the same kind encoded for a synset); non-factivity (to indicate that a causal relation does not necessarily apply); intention (added to a cause relation to indicate intention to cause a certain result); negation (to explicitly encode the impossibility of a relation occurring)."
These features are also used in IWN.
"The table below gives an overview of the main relations being encoded for adjectives in IWN (for the other relations being encoded for adjectives see Alonge et al., in prep.):"
GRAD ANTONYMY adj/adj beautiful/u~ly
COMP ANTONYMY adj/adj alive/ dead
HYPONYMY adj/adj watery/containing
PERTAINS TO adj/noun chemical/chemistry
IS A VALUE_OF adj/noun tail/stature
INVOLVED adj/noun dental/tooth
CAUSE adj/verb; depurative/ adj/noun to depurate LIABLE_TO adj/verb; triable/to judge adj/noun
"In the EWN TO all the entities belonging to the 2ndorder have been organized into two different classification schemes, which represent the first division below 2ndOrder Entity: • Situation Type: the event-structure or Aktionsart (or lexical aspect) of a situation; • Situation Components: the most salient semantic components that characterize situations."
The Situation Types provide a classification of 2oes in terms of the event-structure (or Aktionsart) of the situation they refer to: a basic distinction was drawn between Static and Dynamic.
The Situation Components represent a more conceptual and intuitive classification of word meanings because they can be viewed like the most salient semantic components of a concept.
"Examples of Situation Components are: Manner, Existence, Communication, Cause."
"Situation Type represents disjoint features that cannot be combined, whereas it is possible to assign any combination of Situation Components to a word meaning."
Here below the Top Concepts identified for 2oes are shown:
"In order to be able to draw generalizations on adjective meanings by using the TO, we partially modified this scheme."
"First of all, we moved the PROPERTY and RELATION nodes under the SITUATIONCOMPONENTnode."
"This was done for two interconnected reasons: first of all, because this distinction is not directly linked to Aktionsart (lexical aspect), while the distinctions under SITUATION TYPE are Aktionsart distinctions, i.e. they are connected with the &quot;the procedural characteristics (i.e. the &apos;phasal structure&apos;, &apos;time extension&apos; and &apos;manner of development&apos;) ascribed to any given situation referred to by a verb phrase&quot; ([REF_CITE]: 70)~°."
"Secondly, adjectives may refer to PROPERTIESor RELATIONS,but they may be either stative or not (cfr. e.g.[REF_CITE])."
"Thus, in our system it has to be possible to specify that an adjective expresses a PROPERTY while being DYNAMIC."
"In any case, since many adjectives may have both a DYNAMICsense and a STATICone, we have also the possibility to under-specify this information Io Of course, in EWN all 2oes (and therefore also nouns or adjectives) can be classified according to their Aktionsart. by linking adjectives directly to the SITUATION TYPEnode."
"Adjectives may indicate many different types of properties: temporal (passeggiata mattutina -morning walk), psychological (canzone triste -sad song), social (uomo ricco - rich man), physical (superficie legnosa - wooden surface), physiological (bambino magro - thin child), perceptive (minestra calda hot soup), quantitative (magra ricompensa - poor reward) and intensity properties (vino forte - strong wine)."
"In the EWN TO there are already nodes which may be used to represent these distinctions (TIME, MENTAL,SOCIAL,PHYSICAL, QUANTITY)but we needed to better specify or also add some features."
"For example, we have added, under the already present node PHYSICAL, the node MATERIAL,to represent, among others, some Italian adjectives ending in -oso (for example legnoso - wooden, acquoso -watery) which indicate the property of containing a certain material."
"Moreover, we added the node PHYSIOLOGICAL (to classify adjectives corresponding to tired, hungry, sick, etc.) under PHYSICAL.For adjectives denoting an intensity, we then added the node INTENSITY directly under the SITUATIONCOMPONENTnode."
One of the main problem we had was that no Top Concept in the EWN TO could be used to classify the reference-modifying adjectives (cf. above).
"These are a very particular kind of adjectives, because they do not indicate a property of the referent of the noun they modify."
"So, aiming at showing the distinction between referent-modifiers and reference-modifiers, we created two new Top Concepts under the node PROPERTY: ATTRIBUTEand FUNCTIONAL,where the latter can be used for reference-modifying adjectives (according to the definition provided by Chierchia &amp;[REF_CITE]for the category referred to by these adjectives: &quot;a function from properties to properties&quot;)."
"Like all descriptive adjectives, also the reference-modifiers classified under the node FUNCTIONALcan be linked to other SITUATION COMPONENTS."
"Functional adjectives for which the temporal aspect prevails (ex former, presente - present) can be classified under the node TIME; adjectives referring to some &apos;epistemological&apos; property (potenziale potential, necessario - necessary) can be linked tO[REF_CITE]; etc."
A particular case of functional useful for computational applications which adjectives are the &apos;argumental&apos; ones.
"They exploit our resource, we believe that the &apos;new&apos; introduce a comparison between different relations being encoded may provide equally entities (e.g., simile - similar, diverso - different, relevant information, especially because many etc.)."
A comparison presupposes a relation adjectives cannot be defined by means of the between different entities so these adjectives can WN relations.
Let&apos;s take into consideration just be linked to both PROPERTY and RELATION. a few examples.
"Since in the EWN TO these two Top Concepts The adjective depresso is ambiguous in that it were two different kinds of SITUATIONTYPE has (at least) three readings:, they were mutually exclusive; now, in the IWN 1) which has been lowered, flattened (said of a revised TO they can be conjoined. land);"
Here below the IWN Top Concepts for 2&quot;aOrder 2) being in bad physical or moral conditions (said
Entities are shown: 2N° ORDERENTITY 3) affected by depression (said of a person).
"For these three senses of the adjective we would encode different relations, extractable from our sources: depressol IS_CAUSEDBY deprimere (to lower) IS A VALUE_OF terreno (land) depresso2"
"HAS_HYPERONYM colpito (affected) IS_CAUSEDBY depressionel (depression - economic sense) depress% HAS_HYPERONYM affetto (affected, suffering from sthg.)"
IS_CAUSED_BY depressione2 (depression - medical sense)
"The relations encoded could, e.g., help disambiguate the occurrences of the adjective in contexts such as: Gianni era depresso (Gianni was depressed) or Quella regione ~ depressa (That area is depressed)."
"Indeed, by checking the semantic information encoded for the two senses of depressione linked to depresso, it&apos;s possible to provide the right interpretation for the sentences under analysis; on the other hand, the first sense of the adjective would be excluded because of the IS A VALUE OF relation encoded."
"In this case, also the TO links could be helpful: actually, depresso2 would be linked to SOCIALand CONDITION,while depresso3 would be linked to MENTALand EXPERIENCE."
"Although the ANTONYMY, PERTAINS_TO, and Many adjectives found in our sources do not ISATTRIBUTEOF relations, already encoded in seem to have (lexicalized) antonyms, nor cannot the Princeton WN, are fundamental relations to be defined by using the PERTAINS_TO or describe the adjective semantics, and they can be"
IS A VALUE OF relations.
"These are often linked to nouns or verbs, by various relations: 11Since this node is used for situations involving the possibility or likelihood of other situations. piumato = coperto di piume (plumed = covered with"
"For these and many other adjectives the &apos;new&apos; relations identified in IWN are necessary, given that we often cannot encode other relations for them."
"The relations encoded provide fundamental semantic information on them, which can, for instance, be used to infer semantic preferences (e.g., only certain loes can be modified by piumato, etc.)."
"The inclusion of this information in a large database which is mainly intended for computational applications can be very useful, mainly because it may help in resolving ambiguities and may be used to draw inferences of different nature."
The performance of machine learning algorithms can be improved by combining the output of different systems.
In this paper we apply this idea to the recognition of noun phrases.
We generate different classifiers by using different representations of the data.
By combining the results with voting tech-niques described[REF_CITE]we manage to improve the best reported performances on standard data sets for base noun phrases and ar-bitrary noun phrases.
Their results have been obtained by combining the output of different taggers with system combination techniques such as majority voting.
This approach cancels errors that are made by the minority of the taggers.
"With the best voting technique, the com-bined results decrease the lowest error rate of the component taggers by as much as 19% (Van[REF_CITE])."
The fact that combination of classifiers leads to improved performance has been reported in a large body of machine learning work.
We would like to know what improvement combi-nation techniques would cause in noun phrase recog-nition.
"For this purpose, we apply a single memory-based learning technique to data that has been rep-resented in different ways."
We compare various com-bination techniques on a part of the Penn Treebank and use the best method on standard data sets for base noun phrase recognition and arbitrary noun phrase recognition.
In this section we start with a description of our task: recognizing noun phrases.
After this we introduce the different data representations we use and our machine learning algorithms.
We conclude with an outline of techniques for combining classifier results.
Noun phrase recognition can be divided in two tasks: recognizing base noun phrases and recognizing arbi-trary noun phrases.
Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase.
"For example, the sentence"
"In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] . contains six baseNPs (marked as phrases between square brackets)."
The phrase $ 366.50 an ounce is a noun phrase as well.
"However, it is not a baseNP since it contains two other noun phrases."
Two baseNP data sets have been put forward[REF_CITE].
The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank[REF_CITE]as training material and one section (20) as test material[Footnote_1].
1This[REF_CITE]baseNP data set is available via[URL_CITE]
"The baseNPs in this data are slightly different from the ones that can be derived from the Treebank, most notably in the attachment of genitive markers."
The recognition task involving arbitrary noun phrases attempts to find both baseNPs and noun phrases that contain other noun phrases.
A stan-dard data set for this task was put forward at the CoNLL-99 workshop.
It consist on the same parts of the Penn Treebank as the main baseNP data set:[REF_CITE]-18 as training data and section 20 as test data2.
The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the[REF_CITE]data sets.
"In both tasks, performance is measured with three scores."
"First, with the percentage of detected noun phrases that are correct (precision)."
"Second, with the percentage of noun phrases in the data that were found by the classifier (recall)."
"And third, with the FZ=I rate which is equal to ([Footnote_2]*preci- the distance between a pair of data items (Daele-sion*recall)/(precision+recall)."
2Software for generating the data is available[URL_CITE]
"The latter rate has mans et al., 1999b). ml-IG has been used success-been used as the target for optimization. fully on a large variety of natural language process-ing tasks."
"In our example sentence in section 2.1, noun phrases"
"Beside IBI-IG, we have used IGTREE in the combi- nation experiments."
IGTREE is a decision tree vari-are represented by bracket structures.
Both (Mufioz ant[REF_CITE]-IG[REF_CITE].
"It uses the et al., 1999) and (Tjong Kim Sang and Veenstra, same feature weight method as IBI-IG."
One classifier can be trained to recog- close to the root node.
A new item is classified by nize open brackets (O) while another will process traveling down from the root node until a leaf node close brackets (C).
Their results can be converted to is reached or no branch is available for the current baseNPs by making pairs of open and close brackets feature value.
The most frequent classification of the with large probability scores[REF_CITE]or current node will be chosen. by regarding only the shortest phrases between open 2.4 and close brackets as baseNPs (Tjong Kim Sang and
We have used the bracket repre-
Our experiments will result in different classifica-sentation (O+C) in combination with the second tions of the data and we need to find out how to baseNP construction method. combine these.
"For this purpose we have evaluated An alternative representation for baseNPs has different voting mechanisms, effectively the voting been put forward[REF_CITE]. methods as described[REF_CITE]."
They have defined baseNP recognition as a tagging All combination methods assign some weight to the task: words can be inside a baseNP (1) or outside of results of the individual classifier.
For each input to-baseNPs (O).
"In the case that one baseNP immedi- ken they pick the classification score with the high-, ately follows another baseNP, the firstword in the est total score."
"For example, if five classifiers have second baseNP receives tag B. Example: weights 0.9, 0.4, 0.8, 0.6 and 0.6 respectively and they classify some token as npstart, null, npstart, Ino earlyi tradingr ino"
"Hongl Kongz null and null, then the combination method will pick MondayB ,o goldz waso quotedo ato $r npstart since it has a higher total score (1.7) than 366.50z anB ounce/ -o null (1.6)."
"The values of the weights are usually es-timated by processing a part of the training data, This set of three tags is sufficient for encoding the tuning data, which has been kept separate as baseNP structures since these structures are non- training data for the combination process. recursive and nonoverlapping. (Tjong[REF_CITE]) have pre-"
"In the first voting method, each of the five classi- tiers receives the same weight (majority)."
"The sec-sented three variants of this tagging representation. ond method regards as the weight of each individual First, the B tag can be used for the first word of classification algorithm its accuracy on the tuning every noun phrase (IOB2 representation)."
"Second, data (TotPrecision)."
The third voting method com-instead of the B tag an E tag can be used to mark the putes the precision of each assigned tag per classifier last word of a baseNP immediately before another and uses this value as a weight for the classifier in baseNP (IOE1).
"And third, the E tag can be used those cases that it chooses the tag (TagPrecision). for every noun phrase final word (IOE2)."
They have The fourth method uses the tag precision weights used the[REF_CITE]representa- as well but it subtracts from them the recall val-tion as well (IOB1).
We will use these four tagging ues of the competing classifier results.
"Finally, the representations as well as the O+C representation. fifth method uses not only a weight for the current classification but it also computes weights for other 2.3 Machine learning algorithms possible classifications."
"The other classifications are We have used the memory-based learning algorithm determined by examining the tuning data and reg- IBI-IG which is part of TiMBL package (Daelemans istering the correct values for every pair of classifier et al., 1999b)."
In memory-based learning the train- results (pair-wise voting). ing data is stored and a new item is classified by the most frequent classification among training items
Apart from these five voting methods we have also processed the output streams with two classifiers: which are closest to this new item.
Data items are IBI-IG (memory-based) and IGTREE (decision tree). represented as sets of feature-value pairs.
In IBI-IG This approach is called classifier stacking.
"Like (Van each feature receives a weight which is based on the[REF_CITE]), we have used different input amount of information which it provides for com- versions: one containing only the classifier output puting the classification of the items in the training and another containing both classifier output and data."
These feature weights are used for computing a compressed representation of the classifier input.
For the latter purpose we have used the part-of-speech tag of the current word.
"Our first goal was to find out whether system combi-nation could improve performance of baseNP recog-nition and, if this was the fact, to select the best combination technique."
"For this purpose we per-formed a 10-fold cross validation experiment on the baseNP training data, sections 15-18 of the WSJ part of the Penn Treebank (211727 tokens)."
"Like the data used[REF_CITE], this data was retagged by the Brill tagger in or-der to obtain realistic part-of-speech (POS) tags3."
The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used[REF_CITE].
"The data was converted to the five data represen-tations (IOB1, IOB2, IOE1, IOE2 and O+C) and IBI-IG was used to classify it by using 10-fold cross validation."
This means that the data was divided in ten consecutive parts of about the same size af-ter which each part was used as test data with the other nine parts as training data.
"The standard pa-rameters of IBI-IG have been used except for k, the number of examined nearest neighbors, which was set to three."
Each word in the data was represented by itself and its POS tag and additionally a left and right context of four word-POS tag pairs.
"For the first four representations, we have used a second pro-cessing stage as well."
"In this stage, a word was repre-sented by itself, its POS tag, a left and right context of three word-POS tag pairs and a left and right context of two classification results of the first pro-cessing stage (see figure 1)."
The second processing stage improved the FZ=I scores with almost 0.7 on average.
"The classifications of the IOB1, IOB2, IOE1 and IOE2 representations were converted to the open bracket (O) and close bracket (C) representations. baseNP training data (211727 tokens)."
Each com-bination performs significantly better than any of the five individual classifiers listed under Represen-tation.
The performance differences between the combination methods are not significant.
After this conversion step we had five O results and five C results.
"In the bracket representations, to-kens can be classified as either being the first token of an NP (or the last in the C representation) or not."
The results obtained with these representations have been measured with accuracy rates: the percentage of tokens that were classified correctly.
Only about one in four tokens are at a baseNP boundary so guessing that a text does not contains baseNPs will already give us an accuracy of 75%.
Therefore the accuracy rates obtained with these representations are high and the room for improvement is small (see table 1).
"However, because of the different treatment of neighboring chunks, the five classifiers disagree in about 2.5% of the classifications."
It seems useful to use combination methods for finding the best classi-fication for those ambiguous cases.
The five O results and the five C results were pro-cessed by the combination techniques described in section 2.4.
The accuracies per input token for the combinations can be found in table 2.
"For both data representations, all combinations perform sig-nificantly better than the best individual classifier (p&lt;0.001 according to a X2 test)[Footnote_4]."
4We have performed significance computations on the bracket accuracy rates because we have been unable to find a satisfactory method for computing significance scores for
Unlike in (Van
Majority voting O:98.10%[REF_CITE]
C:98.29% 93.63% 92.89% 93.26 C:98.2% 92.893.1% 92.4% (Tjong Kim Sang and Veenstra~ 1999) 97.58% 92.50% 92.25% 92.37[REF_CITE]97.37% 91.80% 92.27% 92.03
Table 3: The results of majority voting of different data representations applied to the two standard data sets put forward[REF_CITE]compared with earlier work.
"The accuracy scores indicate how often a word was classified correctly with the representation used (O, C or IOB1)."
The training data[REF_CITE]contained 211727 tokens while section 00 was processed with 950028 tokens of training data.
Majority voting outperforms all earlier reported results for the two data sets.
Further- 3). more the performance differences between the com- We have also applied majority voting to the NP bination methods are not significant (p&gt;0.05).
To data set put forward on the CoNLL-99 workshop. our surprise the five voting techniques performed the In this task the goal is to recognize all NPs.
We assume that this has happened because have approached this as repeated baseNP recogni-the accuracies of the individual classifiers do not dif- tion.
A first stage detects the baseNPs.
"The recog-fer much and because the classification involves a nized NPs are replaced by their presumed head word binary choice. with a special POS tag and the result is send to a Since there is no significant difference between the second stage which recognizes NPs with one level of combination methods, we can use any of them in the embedding."
The output of this stage is sent to a remaining experiments.
We have chosen to use ma- third stage and this stage finds NPs with two levels jority voting because it does not require tuning data. of embedding and so on.
We have applied it to the two data sets mentioned In the first processing stage we have used the five[REF_CITE].
The first data set data representations with majority voting.
This ap-uses[REF_CITE]-18 as training data (211727 proach did not work as well for other stages.
The tokens) and section 20 as test data (47377 tokens).
O+C representation outperformed the other four The second one uses sections 02-21 of the same cor- representations by a large margin for the valida-pus as training data (950028 tokens) and section 00 tion data[Footnote_5].
5The validation data is the test set we have used for esti-
This caused the combined output of as test data (46451 tokens).
All data sets were pro- all five representations being worse than the O+C cessed in the same way as described earlier.
Therefore we have only used the O+C repre-results of these experiments can be found in table 3. sentation for recognizing nombaseNPs.
"The overall[REF_CITE]as test set, we managed to reduce system reached an F~=I score of 83.79 and this is the error ofthe best result known to us with 6% with slightly better than the best rate reported at the the error rate dropping from 7.2% to 6.74%, and for section 00 this difference was almost 18% with the"
"CoNLL-99 workshop (82.98[REF_CITE], an error reduction of 5%)."
"The chunks can be combined to trees by a sec-ond processing stage, the attacher.[REF_CITE]have build a chunker by apply-ing transformation-based learning to sections of the Penn Treebank."
"Rather than working with bracket structures, they have represented the chunking task as a tagging problem."
POS-like tags were used to account for the fact that words were inside or out-side chunks.
They have applied their method to two segments of the Penn Treebank and these are still being used as benchmark data sets.
Several groups have continued working with the Ramshaw and Marcus data sets for base noun phrases.[REF_CITE]use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks.
This method records POS tag se-quences which contain chunk boundaries and uses these sequences to classify the test data.
Its per-formance is somewhat worse than that of Ramshaw and Marcus (F~=1=91.[Footnote_6] vs. 92.0) but it is the best result obtained without using lexical information6.[REF_CITE]store POS tag sequences that make up complete chunks and use these se-quences as rules for classifying unseen data.
"6We have applied majority voting of five data represen-tations to the Ramshaw and Marcus data set without using lexical information and the results were: accuracy O: 97.60%, accuracy C: 98.10%, precision: 92.19%, recall: 91.53% and F~=I: 91.86."
This approach performs worse than the method of Arga-mon et al. (F~=1=90.9).
"Three papers mention having used the memory-based learning method IBI-IG.[REF_CITE]in-troduced cascaded chunking, a two-stage process in which the first stage classifications are used to im-prove the performance in a second processing stage."
This approach reaches the same performance level as Argamon et al. but it requires lexical informa-tion.[REF_CITE]report a good per-formance for baseNP recognition but they use a dif-ferent data set and do not mention precision and recall rates. (Tjong[REF_CITE]) compare different data representations for this task.
Their baseNP results are slightly better than those of Ramshaw and Marcus (F~=1=92.37).[REF_CITE]describes a baseNP chunker built from training data by a technique called supertag-ging.
"The performance of the chunker was an improvement of the Ramshaw and Marcus results (Fz=I =92.4).[REF_CITE]use SNOW, a net-work of linear units, for recognizing baseNP phrases and SV phrases."
They compare two data representa-tions and report that a representation with bracket structures outperforms the IOB tagging representa-tion introduced[REF_CITE].
SNoW reaches the best performance on this task (Fz=I =92.8).
There has been less work on identifying general noun phrases than on recognizing baseNPs.[REF_CITE]extended a definite clause grammar with rules induced by a learner that was based upon the maximum description length principle.
He pro-cessed other parts of the Penn Treebank than we with an F~=I rate of about 60.
Our earlier effort to process the CoNLL data set was performed in the same way as described in this paper but with-out using the combination method for baseNPs.
We obtained an F~=I rate of 82.98[REF_CITE].
We have put forward a method for recognizing noun phrases by combining the results of a memory-based classifier applied to different representations of the data.
We have examined different combination tech-niques and each of them performed significantly bet-ter than the best individual classifier.
We have cho-sen to work with majority voting because it does not require tuning data and thus enables the indi-vidual classifiers to use all the training data.
This approach was applied to three standard data sets for base noun phrase recognition and arbitrary noun phrase recognition.
For all data sets majority voting improved the best result for that data set known to US.
Varying data representations is not the only way for generating different classifiers for combination purposes.
We have also tried dividing the training data in partitions (bagging) and working with artifi-cial training data generated by a crossover-likeoper-ator borrowed from genetic algorithm theory.
"With our memory-based classifier applied to this data, we have been unable to generate a combination which improved the performance of its best member."
An-other approach would be to use different classifica-tion algorithms and combine the results.
We are working on this but we are still to overcomethe prac-tical problems which prevent us from obtaining ac-ceptable results with the other learning algorithms.
"Acknowledgements We would like to thank the members of the CNTS group in Antwerp, Belgium, the members of the ILK group in Tilburg, The Netherlands and three anony-mous reviewers for valuable discussions and com-ments."
This research was funded by the European TMR network Learning Computational Grammars~.
This paper presents the syntactic and semantic tags used to annotate predicate-argument structure in the Berkeley FrameNet Project.
"It briefly explains the theory of frame semantics on which semantic annotation is based, discusses possible applications of FrameNet annotation, and compares FrameNet to other prominent iexical resources."
"This paper presents the tagset used to annotate the predicate-argument structures of English verbs, adjectives, and nouns in the Berkeley FrameNet Project (NSF IR]-9618838, &quot;Tools for Lexicon Building&quot;), a corpus-based computational lexicography project based on the theory of frame semantics (see[REF_CITE])."
It briefly explains the theoretical background and shows how frame-semantic annotation creates lexicographic generalizations that are not possible with more traditional linguistic approaches to argument structure based on thematic roles.
Frame semantics characterizes the semantic and syntactic properties of predicating words by relating them to semantic frames.
"These are schematic representations of situations involving various participants, props, and other conceptual roles, each of which is a frame element (FE)."
The semantic arguments of a predicating word correspond to the FEs of the frame or frames associated with that word.
Frames are organized in a structure that can be modeled by an inheritance lattice.
"They range from being very general, like case frames[REF_CITE]or other simple event schemas underlying thematic roles, to being lexically specific."
The most interesting frames are those at an intermediate level of specificity which encapsulate generalizations about the semantic and syntactic properties of word classes that are overlooked by thematic role analyses.
"One example is the commercial transaction frame, which includes the following FEs: Buyer, Seller, Goods, and Money."
The following sentence schemas show how these FEs are expressed differently by different Commercial Transaction words:
Buyer bought Goods from Seller for Money
Buyer paid Seller Money for Goods
Buyer paid Money to Seller for Goods
Seller sold goods to Buyer for Money
Seller sold Buyer Goods for Money
Buyer spent Money on Goods (Seller not expressed)
Goods cost Buyer Money (Seller not expressed)
Different words assign the Commercial Transaction FEs to different Phrase Types (PTs) and Grammatical Functions (GFs).
"For example, buy treats the Buyer as an NP subject (i.e."
"External Argument) and the Seller as a PP complement headed byfrom, while sell treats the Buyer as a direct object or a PP complement headed by to and the Seller as a subject."
The purpose of FrameNet annotation is to gather information like this about the grammatical realization of FEs for various frames.
"The goals of the project are to create a database of information about English words and the frames they inherit, provide annotated corpus examples that illustrate how information about FEs is expressed by complements and modifiers of these words in attested sentences, and contribute to a suite of software tools to support annotation, database building, and database interface."
An important part of FrameNet work is the annotation of corpus sentences with frame-semantic information.
"We use the British National Corpus (BNC), because no equally comprehensive corpus exists for American English (though efforts are underway to create a comparable American National Corpus--see[REF_CITE])."
"Each annotated example sentence shows argument-structure properties of one target verb, adjective or noun."
The main task of annotation is to tag the arguments (and occasionally modifiers) of the target with the names of the FEs that they express.
"A secondary task is to mark other lexicographically relevant elements, such as support verbs of target nouns, and certain non-meaningful elements that indicate lexicographically relevant grammatical constructions (e.g. extraposition and the existential construction). (See Fillmore &amp;[REF_CITE]on lexicographic relevance.)"
"Here is an example sentence from the BNC showing the annotation properties of the complements of the target verb tell: (1) [Maltravers (Speaker, NP, Ext)] decided not to tell [Stephen (Addressee, NP, Obj)] [about the inscription in the Le Carr6 book (Topic, PPabout, Comp)]."
The annotated constituents appear in brackets.
"Following each constituent is a set of parentheses containing the FE, PT and GF associated with that constituent (actual annotations consist of XML markup created using the Alembic Workbench software from the Mitre Corporation)."
PT and GF information is added by an automatic phrase labeler developed by the technical team.
Below are the FrameNet Phrase Types.
"This is intended to be a comprehensive list of the types of syntactic constituent that can express FEs of major predicating words of English (nouns, verbs and adjectives)."
These constituents occur either as arguments or as lexicographically relevant modifiers.
"In constructing this list, we made extensive reference to the Comlex syntax[REF_CITE]. 2.1.1 Noun phrase types"
NP Noun phrase (the witness)
N Non-maximal nominal (personal chat)
Poss Possessive NP (the child&apos;s decision)
There Expletive there (there was a fight)
It Expletive it (it&apos;s nice that you came) 2.1.2 Prepositionalphrase types
PP Prepositional phrase (look at me)
Ping PP with gerundive object (keepfrom laughing)
Part Particle(look it up) 2.1.3 Verbphrase types
VPfin Finite verb phrase (we atefish)
VPbrst Bare stem VP (let us eatfish)
VPto To-marked infinitive VP (we want to eat sh)
VPwh WH-VP (we know how to win)
VPing Gerundive VP (we like winning) 2.1.4 Complement clause types
Sfin Finite clause (it&apos;s nice thatyou came)
Swh WH-clause (ask who won)
Sif lflwhether clause (ask if we won) Sing Gerundive clause (we saw them running)
Sto To-marked clause (we want them to win)
Sforto For-to-marked clause (we would likefor them to win)
Sbrst Bare stem clause (we insist that they win)
In certain cases FrameNet marks as two constituents what are treated as &quot;small clauses&quot; in some analyses.
"For example, in the sentence I consider Pat a genius, Pat and a genius would be tagged separately. 2.1.50therphrase types AjP Adjective phrase (an interesting idea)"
Adverb phrase (you put that nicely)
"Quo Quote (&quot;Indeed, &quot;she said)"
Below is a list of the FrameNet GFs.
"This list is intended to characterize all the grammatical contexts relative to English verbs, adjectives and nouns that are regularly occupied by FE-expressing constituents."
"In this list, we do not make the traditional distinction between obliques and arguments/complements (the former are simply PP complements)."
"Ext External Argument (Argument outside phrase headed by target verb, adjective or noun)"
"Comp Complement (Argument inside phrase headed by target verb, adjective or noun)"
"Mod Modifier (Non-argument expressing FE of target verb, adjective or noun)"
Xtrap Extraposed (Verbal or clausal complement extraposed to end of VP)
Obj Object (Post-verbal argument; passivizable or does not alternate with PP)
Pred Predicate (Secondary predicate complement of target verb or adjective)
Head Head (Head nominal in attributive use of target adjective)
Gen Genitive Determiner (Genitive Determiner of nominal headed by target)
"Besides being expressed by the PTs and GFs listed above, FEs may remain unexpressed under the different conditions discussed below (see[REF_CITE])."
"In sentence (1), the FEs Message, Medium and Code are not expressed."
"It can however be inferred that there must be a Message, Medium and Code in a communicative event of the type described by this sentence."
"These FEs, while conceptually present in this sentence, are optionally expressed, and there are no particular restrictions on their nonexpression."
"This is called Indefinite Null Instantiation (INI). 2.3.2 Definite Null Instantiation With some words, an FE may be unexpressed in a sentence only if it assumed that the person to whom the sentence is addressed has specific information about the frame element in question."
This kind of non-expression is called Definite Null Instantiation (DNI).
"For example, the words tell, inform and notify allow a Message to be omitted when it is clear what the Message is, e.g. How did I know you won?"
Because Pat already tom me. 2.3.3 Constructionally licensed null instantiation
"Certain grammatical constructions, such as Passive and Imperative, allow an External Argument FE to be unexpressed, e.g. Harsh things were said, Tell me about yourself."
This is Constructionally Licensed Null Instantiation (CNI).
FrameNet annotation marks prominent unexpressed FEs as well as FEs that are expressed overtly.
"In order to achieve this, we place the symbols INI, DNI and CNI immediately after the target word in every annotated sentence, and place the appropriate FE tag on the appropriate symbol."
"For example, the case of DNI discussed above would be annotated as follows: (2) [Pat (Speaker)] already told INI [DNI (Message)]"
CNI [me (Addressee)].
"Frames are organized into the following domains: Body, Chance, Cognition, Communication, Emotion, Health, Life Stages, Motion, Perception, Society, Space, Time, Transaction, and a General domain."
Each domain contains several frames that characterize different word classes.
"Because there are many frames, it is not possible to give the complete list of FEs."
The next section discusses FEs from the Communication domain.
It is typically the case that different frames in the same domain share FEs.
"For that reason, each domain can be characterized by a basic frame that defines its FEs in general terms, and more specific frames, corresponding to word classes, that are based on this basic frame through inheritance or some other principled relation."
Let us consider the basic frame of verbal communication.
The following FEs consistently appear in frames relating to verbal communication:
Speaker (A person who performs an act of verbal communication)
Addressee (An actual or intended recipient of a verbal message)
Message (A communicated proposition)
Topic (The subject matter of a message)
Medium (A physical channel of communication)
Code (The language or other code used to communicate)
These FEs all derive their meaning from the concept of a basic communicative event.
"Clearly a true frame representation cannot just consist of a list of role names, but must characterize such events."
"Currently FrameNet frame descriptions exist only in text form, but the ultimate aim is to express them in a machine-readable format."
An important component of such a representation will be feature structures that express relations between frames symbolically.
"These might be combined with computational event models that are able to generate inferences, such as the x-schemas developed[REF_CITE]and[REF_CITE]."
"The FEs of the Basic Verbal Communication frame are relevant to sentence (1) above, in which the Speaker is expressed as an NP Ext, the Addressee is expressed as an NP Obj, and the Topic is expressed as a PP Comp headed by about."
The other FEs are not expressed.
"We will examine how the FEs above are realized in different Communication frames, and in the process, will see some of the kinds of generalizations that can be expressed through frame-semantic lexical analysis."
The Basic Verbal Communication frame characterizes events of verbal communication in the most general terms.
Different Communication words represent different types of communicative event and different ways of construing such events.
Generalizations over these words are captured by different frames in the domain.
"Some focus on, or profile, in Langacker&apos;s (1987) terminology, the relation between a Speaker and a propositional Message."
"These are grouped into frames characterizing different speech acts, e.g. asking (Questioning frame), requesting (Request frame), asserting (Statement frame), and promising (Commitment frame)."
"A few words profile the relation between a Speaker and an act of speaking, but not the propositional Message communicated (talk, speak)."
"Some denote events of reciprocal communication discuss, argue, (e.g. conversation, etc.)."
And so on.
"The following sections summarize properties of specific frames, and discuss some assumptions about the ontology of the Communication domain that might account for these properties."
Verbs and nouns in the Statement frame profile a relation between a Speaker and a propositional
Message that has the speech act status of an assertion.
"Because of the importance of the Message FE, these words frequently occur with Sfin Comp (finite clausal complements): (3) [Others (Speaker, NP, Ext)] assert [that anthropology is the tree and sociology the branch (Message, Sfin, Comp)]. (4) [Managers (Speaker, NP, Ext)] claim [there was no radiological hazzard to staff or the public (Message, Sfin, Comp)]. (5) [His (Speaker, Poss, Gen)] claims [to have more energy (Message, VPto, Comp)] are simply laughable."
Message can be expressed with different PPs as well.
"For example, with the target noun claim, it can be expressed as a to-marked infinitive VP, as in example (5)."
"Verbs and nouns in the Speaking frame profile a relation between a Speaker and an act of speaking, but do no allow a Message to be expressed: (6) [She (Speaker, NP, Ext)] never spoke [about her feelings (Topic, PPabout, Comp)]."
This fact can be explained if we analyze the basic meaning of Speaking words as being something like &apos;X say something&apos;.
"The Message role can be thought of as being incorporated into this meaning, with incorporation in this case being equivalent to obligatory INI."
"Like words in the Statement frame, these words frequently occur with clausal Complements."
"However, because these Complements express requests rather that assertions, they often occur as bare stem clauses: (7) In all cases [the respondent (Speaker, NP, Ext)] may request [in writing (Medium, PPin, Comp)] [that the disciplinary findings be published (Message, Sbrst, Comp)]."
One class of words in the Communication domain does not use the FEs Speaker and Addressee as they occur in other frames.
"These are nouns and verbs or reciprocal communication, which are treated in the Conversation frame."
"In this frame, the human interlocutors can be expressed in separate constituents, assigned the roles Protagonist-I (Prot-1) and Protagonist-2 (Prot-2), or they can be expressed in a single conjoined or plural constituent, assigned the role Protagonists (Prots)."
This class of words demonstrates the complex interaction of frames.
"The FEs Prot-1, Prot-2 and Prots are not equivalent to any of the Basic Verbal Communication FEs, but do relate to them in a regular way."
"Each of these roles must be thought of as relating to two or more Communicative subevents, and as corresponding to a Speaker in some and an Addressee in others."
"This basic structure is taken not from any frame in the domain of Communication, but from a Reciprocity frame, which is in the General domain."
"The Reciprocity frame may be thought of as an Aktionsart frame that structures events and relations from other frames in a particular way, such that there are multiple subeventualities of the same type as that of the input frame, and the bindings or fillers of the roles are reversed from one subeventuality to the another."
The complexity introduced by frames such as these points to the need to distinguish between conceptual roles and FEs in any given frame.
"While the Communication roles Speaker and Addressee are not FEs in the Conversation frame, they are conceptual roles, because the conceptual representation of Conversation makes reference to them. (In the Speaking frame discussed above, we can also think of the incorporated Message role as being a conceptual role rather than an FE.)"
"FEs in any given frame should therefore be defined as those roles for which the frame specifies conventional means of syntactic expression, even if these means are not employed in all sentences."
"Here is a BNC example of argue with a disjoint expression of interlocutors: (8) &quot;[You (Prot-1, NP, Ext)] can&apos;t argue [politics (Topic, NP, Comp)] [with foreigners (Prot-2, PPwith, Comp)]&quot;, sighed the policeman."
"Prot-2, the less prominent interlocutor, is regularly expressed in this frame by a PP Comp headed by with."
This is consistent with the general behavior of the Reciprocity frame as it occurs in combination with other frames and domains (e.g. Pat had a collision/relationship/agreement with Kim).
"Here are BNC examples of the joint expression of interlocuters with argue: (9) [They (Prots, NP, Ext)] argued [angrily (Manner, AdvP, Mod)] [over who was the real &quot;Prince of Sleaze&quot; (Topic, PPover, Comp)]. (10) [Mr. and Mrs. Popple (Prots, NP, Ext)] always argued [INI (Topic)] at least once a week."
FrameNet annotations provide more detail than existing lexical resources about the way in which particular semantic roles (i.e. FEs) are linked with particular means of syntactic expression.
"Since different senses of ambiguous words are defined relative to different frames, this linking information could potentially be used for lexical disambiguation."
"For example, consider the verb argue as it is treated in the WordNet database[REF_CITE]."
"Below are the three WordNet senses of argue, and the sentence frames that are associated with each sense:"
"Sense 1: argue, reason -- (present reasons and arguments)"
EX: Sam and Sue argue
Sam wants to argue with Sue
"Sense 2: argue, contend, debate, fence -- (have an argument about something)"
EX: Sam and Sue argue
EX; Sam wants to argue with Sue
"Sense 3: argue, indicate -- (give evidence of; &quot;The evidence argues for your claim&quot;; &quot;The results indicate the need for more work&quot;)"
"The three senses listed above correspond to three different frames: Sense 1 corresponds to the Statement frame, Sense 2 to the Conversation frame, and Sense 3 to the Evidence frame in the"
"However, the information about sentence frames provided in WordNet does not correspond to the generalizations that are apparent in FrameNet."
"For example, WordNet gives the same sentence frames for Senses 1 and 2, while in the FrameNet database, the senses of argue defined relative to the Statement and Conversation frames are characterized by different argument structures: only Statement argue allows finite clausal complements expressing Message; Conversation argue has the properties of other reciprocal communication words, which Statement argue lacks, and does not allow clausal Complements (or any other expression of Message)."
"COMLEX[REF_CITE]recognizes the syntactic frames in which argue occurs, but does not provide information about the linking of syntactic constituents with semantic roles, or about the different complementation properties of different senses of ambiguous words."
FrameNet semantic annotation captures human knowledge about the ways in which semantic roles (FEs) are conventionally expressed by different words in various word classes and domains.
The kind of information in the FrameNet database is not expressed in the same level of depth in any existing print dictionary or computational lexical resource.
"While WordNet describes semantic relations between words, it does not recognize the conceptual schemas, i.e. frames, that mediate in these relations, and therefore does not have the means to link arguments of predicating words with the semantic roles they express."
"COMLEX and NOMLEX provide detailed information about the syntactic frames in which verbs and nouns occur, but also lack a means to link syntactic arguments with semantic roles."
FrameNet therefore provides information that complements major existing lexical resources.
"The authors would like to thank the National Science Foundation for supporting this project, and the International Computer Science Institute in Berkeley for giving it a home."
Thanks also to Oxford University Press and Sue Atkins for
"This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context."
"Despite the simplicity of this approach, empirical results disam-biguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rival-ing the best previously published results."
"Word sense disambiguation is often cast as a prob-lem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text using methods from statistics or machine learning."
These approaches typically represent the context in which each sense-tagged instance of a word occurs with a set of linguistically motivated features.
A learning algorithm induces a representative model from these features which is employed as a classifier to perform disambiguation.
This paper presents a corpus-based approach that results in high accuracy by combining a number of very simple classifiers into an ensemble that per-forms disambiguation via a majority vote.
This is motivated by the observation that enhancing the fea-ture set or learning algorithm used in a corpus-based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning al-gorithm.
"For example, a Naive Bayesian classifier[REF_CITE]is based on a blanket assumption about the interactions among features in a sense-tagged corpus and does not learn a representative model."
"Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g.,[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE])."
These studies represent the context in which an ambiguous word occurs with a wide variety of features.
"However, when the con- tribution of each type of feature to overall accuracy is analyzed (eg.[REF_CITE]), shallow lexi-cal features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships."
"It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often signifi-cantly greater than that of any of the individual clas-sifiers that make up the ensemble (e.g.,[REF_CITE])."
"In natural language processing, ensemble techniques have been successfully applied to part-of-speech tagging (e.g.,[REF_CITE]) and parsing (e.g.,[REF_CITE])."
"When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense dis-ambiguation might best be improved by combining the output of a number of such classifiers into an ensemble."
This paper begins with an introduction to the Naive Bayesian classifier.
"The features used to rep-resent the context in which ambiguous words occur are presented, followed by the method for selecting the classifiers to include in the ensemble."
"Then, the line and interest data is described."
Experimental re-sults disambiguating these words with an ensemble of Naive Bayesian classifiers are shown to rival pre-viously published results.
This paper closes with a discussion of the choices made in formulating this methodology and plans for future work.
A Naive Bayesian classifier assumes that all the fea-ture variables representing a problem are condition-ally independent given the value of a classification variable.
"In word sense disambiguation, the context in which an ambiguous word occurs is represented by the feature variables (F1, F2,..., F~) and the sense of the ambiguous word is represented by the classi-fication variable (S)."
"In this paper, all feature vari-ables Fi are binary and represent whether or not a particular word occurs within some number of words to the left or right of an ambiguous word, i.e., a win- dow of context."
"For a Naive Bayesian classifier, the quency counts of shallow lexical features from two joint probability of observing a certain combination windows of context; one including I words to the left of contextual features with a particular sense is ex- of the ambiguous word and the other including r pressed as: n p(F~,F~,..., Fn, S) = p(S) H p(FilS) i=1"
The parameters of this model are p(S) words to the right.
"Note that Naive_Bayes (0,0) in-cludes no words to the left or right; this classifier acts as a majority classifier that assigns every instance of an ambiguous word to the most frequent sense in the training data."
Once the individual classifiers are and trained they are evaluated using previously held-out p(Fi]S).
"The sufficient statistics, i.e., the summaries test data. of the data needed for parameter estimation, are the The crucial step in building an ensemble is select-frequency counts of the events described by the in- ing the classifiers to include as members."
"The ap-terdependent variables (Fi, S)."
"In this paper, these proach here is to group the 81 Naive Bayesian clas-counts are the number of sentences in the sense- sifiers into general categories representing the sizes tagged text where the word represented by Fi oc- of the windows of context."
"There are three such curs within some specified window of context of the ranges; narrow corresponds to windows 0, 1 and 2 ambiguous word when it is used in sense S. words wide, medium to windows 3, 4, and 5 words"
"Any parameter that has a value of zero indicates wide, and wide to windows 10, 25, and 50 words that the associated word never occurs with the spec- wide."
There are nine possible range categories since ified sense value.
These zero values are smoothed there are separate left and right windows.
"For exam-by assigning them a very small default probability. ple, Naive_Bayes(1,3) belongs to the range category Once all the parameters have been estimated, the (narrow, medium) since it is based on a one word model has been trained and can be used as a clas- window to the left and a three word window to the sifier to perform disambiguation by determining the right."
"The most accurate classifier in each of the most probable sense for an ambiguous word, given nine range categories is selected for inclusion in the the context in which it occurs."
Each of the nine member classifiers votes for the most probable sense given the particular con-text represented by that classifier; the ensemble dis-
The contextual features used in this paper are bi- ambiguates by assigning the sense that receives a nary and indicate if a given word occurs within some majority of the votes. number of words to the left or right of the ambigu-ous word.
No additional positional information is 3 Experimental Data contained in these features; they simply indicate if the word occurs within some number of surrounding The line data was created[REF_CITE]words. by tagging every occurrence of line in the ACL/DCI
Wall Street Journal corpus and the American Print-Punctuation and capitalization are removed from ing House for the Blind corpus with one of six pos-the windows of context.
All other lexical items are sible WordNet senses.
These senses and their fre-included in their original form; no stemming is per- formed and non-content words remain.
This representation of context is a variation quency distribution are shown in Table 1.
"This data has since been used in studies[REF_CITE],[REF_CITE], and (Leacock et al., the bag-of-words feature set, where a single window 1998)."
"In that work, as well as in this paper, a subset of context includes words that occur to both the left of the corpus is utilized such that each sense is uni-and right of the ambiguous word."
"An early use of formly distributed; this reduces the accuracy of the this representation is described[REF_CITE], majority classifier to 17%."
The uniform distribution where word sense disambiguation is performed with is created by randomly sampling 349 sense-tagged a Naive Bayesian classifier.
"The work in this pa-examples from each sense, resulting in a training cor-per differs in that there are two windows of context, pus of 2094 sense-tagged sentences. one representing words that occur to the left of the ambiguous word and another for those to the right."
The interest data was created by (Bruce and
"This data set was subsequently used The first step in the ensemble approach is to train a for word sense disambiguation experiments by (Ng separate Naive Bayesian classifier for each of the 81 and[REF_CITE]),[REF_CITE], and (Peder-possible combination of left and right window sizes. sen and[REF_CITE])."
The previous studies and this
"Naive_Bayes (1,r) represents a classifier where the paper use the entire 2,368 sense-tagged sentence cor-model parameters have been estimated based on Ire- pus in their experiments."
"The senses and their ire- sense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 sense count money paid for the use of money 1252 a share in a company or business 500 readiness to give attention 361 advantage, advancement or favor 178 activity that one gives attention to 66 causing attention to be given to 11 total 2368 quency distribution are shown in Table 2."
"Unlike line, the sense distribution is skewed; the majority sense occurs in 53% of the sentences, while the small-est minority sense occurs in less than 1%."
Eighty-one Naive Bayesian classifiers were trained and tested with the line and interest data.
Five-fold cross validation was employed; all of the sense-tagged examples for a word were randomly shuffled and divided into five equal folds.
Four folds were used to train the Naive Bayesian classifier while the remaining fold was randomly divided into two equal sized test sets.
"The first, devtest, was used to eval-uate the individual classifiers for inclusion in the en-semble."
"The second, test, was used to evaluate the accuracy of the ensemble."
"Thus the training data for each word consists of 80% of the available sense-tagged text, while each of the test sets contains 10%."
This process is repeated five times so that each fold serves as the source of the test data once.
The average accuracy of the individual Naive Bayesian classifiers across the five folds is reported in Tables
Each classifier is based upon a distinct representa-tion of context since each employs a different com-bination of right and left window sizes.
The size and range of the left window of context is indicated along the horizontal margin in Tables 3 and 4 while the right window size and range is shown along the vertical margin.
"Thus, the boxes that subdivide each table correspond to a particular range category."
The classifier that achieves the highest accuracy in each range category is included as a member of the ensem-ble.
"In case of a tie, the classifier with the smallest total window of context is included in the ensemble."
"The most accurate single classifier for line is Naive_Bayes (4,25), which attains accuracy of 84% The accuracy of the ensemble created from the most accurate classifier in each of the range categories is 88%."
"The single most accurate classifier for interest is Naive._Bayes(4,1), which attains accuracy of 86% while the ensemble approach reaches 89%."
"The in-crease in accuracy achieved by both ensembles over the best individual classifier is statistically signifi-cant, as judged by McNemar&apos;s test with p = .01."
These experiments use the same sense-tagged cor-pora for interest and line as previous studies.
Sum-maries of previous results in Tables 5 and 6 show that the accuracy of the Naive Bayesian ensemble is comparable to that of any other approach.
"How-ever, due to variations in experimental methodolo-gies, it can not be concluded that the differences among the most accurate methods are statistically significant."
"For example, in this work five-fold cross validation is employed to assess accuracy while[REF_CITE]train and test using 100 randomly sampled sets of data."
Similar differences in train-ing and testing methodology exist among the other studies.
"Still, the results in this paper are encourag-ing due to the simplicity of the approach."
The interest data was first studied[REF_CITE].
"They employ a representation of context that includes the part-of-speech of the two words surrounding interest, a morphological feature indicating whether or not interest is singular or plu-ral, and the three most statistically significant co-occurring words in the sentence with interest, as de-termined by a test of independence."
"These features are abbreviated as p-o-s, morph, and co-occur in"
"The interest data was included in a study by ([REF_CITE].63 .73 wide 25 .63 .74 10 .62 .75 5 .61 .75 medium 4 .60 .73 [Footnote_3] .58 .73 2 .53 .71 narrow 1 .42 .68 0 .14 .58 0 1 narrow .80 .83 .83 .83.82 .83 .83 .80 .82 .s4 .83 .83 .83 .83 .81 .83 .83 .84 .82 .83 .83 .80 .82 .82 .83 .81 .82 .82 .80 .82 .82 .82 .82 .82 .82 .79 .82 .83 .83 .82 .81 .82 .79 .81 .82 .82 .81 .81 .81 .78 .79 .80 .79 .80 .81 .81 .73 .77 .79 .79 .79 .79 .80 2 3 4 5 10 25 50 medium wide 50 .74 .80 wide 25 .73 .80 10 .75 .82 5 .73 .83 medium 4 .72 .83 [Footnote_3] .70 .84 2 .66 .83 narrow 1 .63 .82 0 .53 .72 0 1 narrow .82 .83 .83 .83 .82 .80 .81 .82 .83 .83 .83 .81 .80 .80 .84 .82 .81 .81 •84 .84 .84 .85 .86 .85 .85 .83 .81 .81 .85 .85 .84 .84 .83 .81 .8O .86 .86 .86 .85 .83 .81 .80 .85 .86 .86 .84 .83 .80 .80 .85 .85 .86 .85 .82 .81 .80 .77 .78 .79 .77 .77 .76 .75 2 4 10 255 503 medium wide accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 89% when evaluated with the test data. and[REF_CITE]), who represent the context of an 4.1.2 Line ambiguous word with the part-of-speech of three The line data was first studied by (Leacock et al., words to the left and right of interest, a morpho- 1993)."
3 and 4. The standard deviations were between .01 and .025 and are not shown given their relative con-sistency.
3 and 4. The standard deviations were between .01 and .025 and are not shown given their relative con-sistency.
"They evaluate the disambiguation accuracy logical feature indicating if interest is singular or of a Naive Bayesian classifier, a content vector, and plural, an unordered set of frequently occurring a neural network."
"The context of an ambiguous keywords that surround interest, local collocations word is represented by a bag-of-words where the that include interest, and verb-object syntactic re-window of context is two sentences wide."
"These features are abbreviated p-o-s, ture set is abbreviated as 2 sentence b-o-w in Table morph, co-occur, collocates, and verb-obj in Table 6."
When the Naive Bayesian classifier is evaluated 5.
"A nearest-neighbor classifier was employed and words are not stemmed and capitalization remains. achieved an average accuracy of 87% over repeated However, with the content vector and the neural net-trials using randomly drawn training and test sets. work words are stemmed and words from a stop-list[REF_CITE]and (Pedersen and Bruce, are removed."
They report no significant differences 1997) present studies that utilize the original Bruce in accuracy among the three approaches; the Naive and Wiebe feature set and include the interest data.
"Bayesian classifier achieved 71% accuracy, the con-"
"The first compares a range of probabilistic model tent vector 72%, and the neural network 76%. selection methodologies and finds that none outper-"
"The line data was studied again by (Mooney, form the Naive Bayesian classifier, which attains ac- 1996), where seven different machine learning curacy of 74%."
The second compares a range of ma- methodologies are compared.
All learning algo-chine learning algorithms and finds that a decision rithms represent the context of an ambiguous word tree learner (78%) and a Naive Bayesian classifier using the bag-of-words with a two sentence window (74%) are most accurate. of context.
"In these experiments words from a stop- method feature set ensemble oi~9 varying left &amp; right bSo-w neural net local ~z topical b-o-w, p-o-s naive bayes local &amp; topical b-o-w, p.-o-s neural net 2 sentence b-o-w content vector naive bayes naive bayes 2 sentence b-o-w perceptron previous results for line list are removed, capitalization is ignored, and words are stemmed."
The two most accurate methods in this study proved to be a Naive Bayesian classifier (72%) and a perceptron (71%).
The line data was recently revisited by both[REF_CITE]and[REF_CITE].
The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local con-text while the other represents topical context.
The latter utilize a Naive Bayesian classifier.
In both cases context is represented by a set of topical and local features.
The topical features correspond to the open-class words that occur in a two sentence window of context.
The local features occur within a window of context three words to the left and right of the ambiguous word and include co-occurrence features as well as the part-of-speech of words in this window.
These features are represented as lo-cal &amp; topical b-o-w and p-o-s in Table 6.[REF_CITE]report accuracy of 87% while[REF_CITE]report accuracy of 84%.
"The word sense disambiguation ensembles in this pa-per have the following characteristics: • The members of the ensemble are Naive Bayesian classifiers, • the context in which an ambiguous word oc-curs is represented by co-occurrence features extracted from varying sized windows of sur-rounding words, • member classifiers are selected for the ensembles based on their performance relative to others with comparable window sizes, and • a majority vote of the member classifiers deter-mines the outcome of the ensemble."
Each point is discussed below.
The Naive Bayesian classifier has emerged as a con-sistently strong performer in a wide range of com-parative studies of machine learning methodologies.
"A recent survey of such results, as well as pos-sible explanations for its success, is presented[REF_CITE]."
"A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accu-racy than the Naive Bayesian classifier (e.g.,[REF_CITE],[REF_CITE],[REF_CITE],[REF_CITE])."
In many ensemble approaches the member classi-tiers are learned with different algorithms that are trained with the same data.
"For example, an en-semble could consist of a decision tree, a neural net-work, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data."
"This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different."
"This is motivated mentarity of errors between two taggers that could by the belief that there is more to be gained by vary- be adapted for use with larger ensembles such as the ing the representation of context than there is from one discussed here, which has nine members. using many different learning algorithms on the same 5.4 Disambiguation by majority vote data."
This is especially true in this domain since the Naive Bayesian classifier has a history of success and In this paper ensemble disambiguation is based on a since there is no generally agreed upon set of features simple majority vote of the nine member classifiers. that have been shown to be optimal for word sense An alternative strategy is to weight each vote by disambiguation. 5.2 Co-occurrence features the estimated joint probability found by the Naive Bayesian classifier.
"However, a preliminary study found that the accuracy of a Naive Bayesian ensem-"
Shallow lexical features such as co-occurrences and ble using a weighted vote was poor.
"For interest, collocations are recognized as potent sources of dis- it resulted in accuracy of 83% while for line it was ambiguation information."
While many other con- 82%.
"The simple majority vote resulted in accuracy textual features are often employed, it isn&apos;t clear of 89% for interest and 88% for line. that they offer substantial advantages."
"For exam-ple,[REF_CITE]report that local collocations 6 Future Work alone achieve 80% accuracy disambiguating interest,"
A number of issues have arisen in the course of this while their full set of features result in 87%.
"Prelim-work that merit further investigation. inary experiments for this paper used feature sets The simplicity of the contextual representation that included collocates, co-occurrences, part-of-can lead to large numbers of parameters in the Naive speech and grammatical information for surrounding Bayesian model when using wide windows of con-words."
"However, it was clear that no combination of text."
Some combination of stop-lists and stemming features resulted in disambiguation accuracy signifi-could reduce the numbers of parameters and thus cantly higher than that achieved with co-occurrence improve the overall quality of the parameter esti- features. 5.3 Selecting ensemble members mates made from the training data.
"In addition to simple co-occurrence features, the use of collocation features seems promising."
These The most accurate classifier from each of nine pos-are distinct from co-occurrences in that they are sible category ranges is selected as a member of words that occur in close proximity to the ambiguous the ensemble.
This is based on preliminary experi-word and do so to a degree that is judged statisti-ments that showed that member classifiers with sim-cally significant. ilar sized windows of context often result in little or One limitation of the majority vote in this paper no overall improvement in disambiguation accuracy. is that there is no mechanism for dealing with out-
This was expected since slight differences in window comes where no sense gets a majority of the votes. sizes lead to roughly equivalent representations of This did not arise in this study but will certainly context and classifiers that have little opportunity occur as Naive Bayesian ensembles are applied to for collective improvement.
"For example, an ensem-larger sets of data. ble was created for interest using the nine classifiers Finally, further experimentation with the size of in the range category (medium, medium)."
The ac-the windows of context seems warranted.
"The cur-curacy of this ensemble was 84%, slightly less than rent formulation is based on a combination of intu-the most accurate individual classifiers in that range which achieved accuracy of 86%. ition and empirical study."
An algorithm to deter- mine optimal windows sizes is currently under de- Early experiments also revealed that an ensemble velopment. based on a majority vote of all 81 classifiers per-formed rather poorly.
The accuracy for interest was 7[REF_CITE]% and line was disambiguated with slightly less than 80% accuracy.
The lesson taken This paper shows that word sense disambiguation from these results was that an ensemble should con- accuracy can be improved by combining a number sist of classifiers that represent as differently sized of simple classifiers into an ensemble.
"A methodol-windows of context as possible; this reduces the im- ogy for formulating an ensemble of Naive Bayesian pact of redundant errors made by classifiers that classifiers is presented, where each member classifier represent very similarly sized windows of context. is based on co-occurrence features extracted from The ultimate success of an ensemble depends on the a different sized window of context."
This approach ability to select classifiers that make complementary was evaluated using the widely studied nouns line errors.
"This is discussed in the context of combin- and interest, which are disambiguated with accuracy ing part-of-speech taggers[REF_CITE]. of 88% and 89%, which rivals the best previously They provide a measure for assessing the comple- published results."
This work extends ideas that began in collabora-tion with Rebecca Bruce and Janyce Wiebe.
Clau-dia Leacock and Raymond Mooney provided valu-able assistance with the line data.
I am indebted to an anonymous reviewer who pointed out the impor-tance of separate test and devtest data sets.
A preliminary version of this paper appears[REF_CITE].
"Thc lincar logic[REF_CITE]provides a power-ful framcwork to cxprcss categorial gt&apos;ammars[REF_CITE]and Lambek calculus[REF_CITE], and a lot of work has presented proof nets uses for lin-guistic purposcs, with a special look at proof nets for Lambek calculus[REF_CITE]."
"But they have mainly explored the syntactic ca-pabilities of proof nets, describing parsing processes."
This paper wants to focus on the generation capa-bilities of proof nets thanks to their semantic readings as expressed in (de[REF_CITE]).
"The main features of our proposal consist in the use of proof nets lot Lambek calculus, of the Curry-Howard iso-morplaisna[REF_CITE], of se-mantic proof nets with semantic expressions ?t la Mon-tagu¢[REF_CITE], and in an algorithm for proof search with a target proof net."
"They represent proofs in linear logic with more accuracy than sequential proofs: on one hand they are more compact, on the other hand they identify unessentially different sequential proofs (for instance in the order of the rules introduction)."
"From a one-sided sequent and a sequential proof of it, we obtain a proof net by unfolding every formula as a tree (whose nodes are the binary connectives and the leaves are formulas, e.g. atomic ones) and linking to-gether the formulas occurring in the same axiom rule of tile sequent calculus."
But proof nets have a more intrinsic definition that pre-vents us to come back every time to sequential proofs.
They can be defined as graphs with a certain property (i.e. verifying a correctness criterion) such that every proof net with this property corresponds to a sequential proof and such that every proof net built from a sequen-tial proof has this property.
So that we do not present the sequent calculus but only the proof net calculus.
"In this paper, we do not consider all the proof nets, but a part of the multiplicative ones: those of the intuition-istic implicative linear logic."
"In this case, sequents are made of several antecedent [brmulas, but only one succe-dent formula."
"To deal with tile intuitionistic notion with proof nets (since we consider one-sided sequents), we use the notion of polarities with the input (,: negative) and the output (o: positive)[REF_CITE]to decorate formulas."
Positive ones correspond to succedent formulas and negative ones to antecedent for-mulas.
"Given the links of table 1, we define proof structures as graphs made of these links such that: 1. any premise of any link is connected to exactly one conclusion of some other link; 2. any conclusion of any link is connected to at most one premise of some other link; 3. input (resp. output) premises are connected to input (resp. output) conclusions of the same type."
"Note that the two links for tile negative and positive implications correspond to the two connectives of the lin-ear logic: Tensor and Par, so that we name these links after these latter connectives."
"But in the following, only the graphical forms of the links play a role."
Proof nets are proof structures that respect the correct-ness criterion.
We mentioned the intrinsic definition of proof nets that enables the complete representation of sequential proofs.
"Tile cut elimination property of sequent calculus also ap-pears intrinsically in the proof net formalism with a sire- &quot;/1131 70 pie rewriting process described in table 2 (in case of com-plex formulas as in the third rewriting rule, those rules can apply again on the result and propagate until reach-ing atoms)."
Definitions of proof nets tbr Lambek calculus first ap-peared in ([REF_CITE]).
"They naturally raised as Lam-bek calculus is an intuitionistic fragment of non commu-tative linar logic (with two linear implications: &quot;&apos;\&quot; on the left and &quot;/&quot; on tile right), and the consequences on the proof net calculus we presented in section 2.1 are:. we get two tensor links: one for the formulas (B/A)- (the one in table 1) and one lbr the for-mula (B\A)- (just inverse the polarities of the premises)."
"And two par links : one for the lbrmula (A\B) + and one for (A/B) + (idem); • formulas in Lambek&apos;s sequents are ordered, so that conclusions of the proof nets are cyclically ordered and axiom links may not cross."
"If Tv is the set of basic types (e.g. S, NP... ), the set T of syntactic types ~bllows T ::= ~[T\T[T/T."
"Note that from a syntactic category, we can untbld the formula to obtain a graph which only lacks axiom links to become a proof structure."
"So that the parsing process in this framework is, given the syntactic categories of the items and their order, to put non crossing axiom links such that the proof structure is a proof net."
It means there is a proof of .b&apos; given types in a certain order.
"L J nical reasons, the order of the conclusions (i.e. the types used) in the proof net to prove S is the reverse order of the words associated to these types."
"As an example, with the lexicon of table 3, proving that John lives in Paris is a correct sentence leads to find axiom links between the atoms in the figure l(a)."
Fig-ure I(b) shows it actually happens and proves the syn-tactic correctness of the sentence.
"Table 3: Lexicon lexical entry syntaxiccategory John NP Paris N P lives N P\,b&apos; in (,S&apos;\,S&apos;)/ N P"
"In this section, we present how (de[REF_CITE]) propose to use proof nets as semantic recipes."
"As a slight difference with this work, we only deal in this paper with semantic recipes that correspond to linear A-terms in the Montague&apos;s semantics framework."
The idea of expressing the semantics with proof nets refers to the fact that both the A-terms (with the Curry-Howard isomorphism) and the proof nets repre-sent prooS; of intuitionistic implicative linear logic.
"And indeed, the linear A-terms may be encoded as proof nets."
"On the other hand, given an intuitionistic implicative proof net, a simple algorithm (given in (de Groote and"
"Then, instead of associating a A-term to a [exical entry"
Let us now consider the problem of generation.
"We have , a given semantic proof net (like the one in figure 4(b)) wc can associate a proof net."
"For instance, on the seman- and we want to gather syntactic entries with axiom links tic side, we can use the Montagovian types e and t and such that: typed constants."
"Of course, we want to keep the compo-"
I. this yields a correct (syntactic) proof net; sitionalily principle of Montague&apos;s semantics that maps 2. the meaning of the resulting proof net matches the any syntactic association rule with a semantic associa-given semantic expression. tion rule.
"We express it in a straightforward way with the ft~llowing homomorphism (for as many basic categories As we already said it, we assume that we have some lex- as required): & quot;H(.:VP) = e 7¢(A\B) = &quot;K(A) ~ &quot;H(B) &quot;H(,~&apos;) = t &quot;H(A/B) = &quot;H(B) --o &quot;H(A) ical entries, and we try to make the generation with these entries, each one used once and only once."
"Thus, if we define: * I/0 the semantic proof net of the expression we want to generate;"
"And for a lexical item, given its syntactic type, we as-"
SUlne its semantic proof net to verify: * IIi the semantic proof nets associated to the given lexical entries i we use; • the type of its unique output conclusion is the ho- * Ti the unfolding in proof structure of the syntactic momorplaic image of the syntactic type: formula of the lexical item i; • its input conclusions (if any) are decorated with • F the forest made of the syntactic trees of all the typed constants.
An example of such a lexicon is given in table 4.
Figure 2: Syntactic proof net for John lives in Paris considered lexical entries plus the output (the type we want to derive).
"The generation problem (see figure 5) is to find a match-ing M of atomic formulas of F such that: 1. F endowed with M (let us call this proof structure F&apos;) is a correct proof net; 2. when cut-linking H(F&apos;) with the Hi, and eliminat-ing these cuts, we obtain H0."
This problem is not an original one: making proof search with proof nets always leads to look for match-ing between atomic formulas of opposite polarities.
Let us illustrate the process on a short example.
We that an answer to this problem would consist in taking f&apos; use the lexicon of table 4 to parse the sentence John lives and try every possible matching.
This brute-force tech-in Paris.
"The first thing is to define with the syntactic cat- nique would of course appear essentially inefficient, and egories of the different lexical items the syntactic proof our purpose is to use everything we know to prune tile net of figure 2."
It provides the way we should compose search domain. the semantic recipes of each lexical item: we take its ho-
"Nevertheless, note that even with such an algorithm, momorphic image as in figure 4(a), and we substitute to we already reach the decidability (because the finitness every input its semantic definition with cut-links. of the number of the matchings) without making any as-"
"Then the cut-elimination on the resulting proof net sumption on the form of the semantic entries (neither on gives a new proof net (on figure 4(b)) we can use as the the order of the associated A-terms, nor the presence of a semantic analysis of Jolm lives in Paris."
"If necessary, we free variable)."
And we want to keep these good proper- can come back to the A-term expression:(in p)(live j). ties in our algorithm.
This section first establishes some equivalent relations between cut-elimination on proof nets and matrix equa-lions.
We then show how to use these equations in the generation process and how we can solve them.
It en-ables us to characterize the properties required by the se-mantic proof nets to have a polynomial resolution of the generation process.
"First, as expressed[REF_CITE]and refornmlated in ([REF_CITE]:[REF_CITE]), we state the algebraic representation of cuFeliminalion on proof nets."
"Due to lack of space, we can not develop it, but tile principle is to express cut-elimination between axioms with incidence matrices and paths in graphs."
Let us consider a proof net U. We denote by (e i )1&lt;i&lt;.~ all the vertices taking place for atoms in ft.
"We can define U the incidence matrix of axiom links, cr the in-cidence matrix of cut links (we assume without loss of generality that they happen only between axiom links), and II the incidence matrix of axiom links of-ff where ~-is the proof net resulting from all the cut eliminations on"
"As mentioned in section 4.2, we can consider the (ei) such that in ~&quot; : • Vi E [1,p], ei is not cut-linked (then, because • Res(cr, U) = (1 - o:-&apos;)U(1 - o.U)-l(1 - o&quot;2) • ( tUlII1 - o.1tLil)U1 = o.=,~,h(1 - o&apos;4t&apos;3)-1 to.._, of • A = tcr,_Xcr&apos;.,arrd U3 = X -1 -~ 0&quot;4. the hypothesis made in section 4.2, B(ei) is cut- linked); Of course, all tire terms are defined. • &apos;7&apos;i E [p+ 1. p+ m], ei is cut-linked but B(ei) is not cut-linked; We base the proof search algorithm corresponding to the • Vi E [p + m + 1, p + 1~ + n], both e.i and B(ei ) are generation process we are dealing with on this third rela- cut-linked. tion, as explained in the next sections."
Note: Remember we assume that there is no axiom link 4.4 Solving the Equations such that both its conclusions are not cut-linked.
So p =
"In this section (proof search oriented), we consider&quot; that"
III. the axiom links we are looking for are those whose two conclusions are involved in cut links.
That is we want to IB(e ) is the atom in [7 such that there is an axiom linkbetweene and BIe). complete U3.
"As in the previous section we proceeded by equivalence, solving the equation (1) correponds to solving the equation .4 = ¢r,.Y tcr._, (2) in .Y with X inversible."
"Then, we have to solve (r3 : X-1 +0&quot;4 such that tU3 = (ra and Ua = 1."
"Let 0&quot;._, ¢ .~,,,,,,(1R), X = (xi,j) E A4,+(IR)and .1 E .XA,,(LR)."
"Let the two sequences 1 &lt; it &lt; ... &lt; i/ &lt; m and 1 &lt; Ji &lt; ... &lt; j/ &lt; m be such that with ((,, 6) ¢[i, ,1l] × [[, ll], Eab = (5iaSjb)i,jEtl,m]x[1,,1]2 &lt;, : E,,j, : ),,+ I l"
"In other words, 0.&apos;-uj = 1 ca&gt;Ell E [1,1]"
A i = ih
A j = ill •
"Then l / cr.,X tcr.-_, = ( E EittJq ) * X * t(E Ei,.+j,.., ) /t=l /-,=1 l / = E E (aiill+t&gt;JqJ+2OJila)l&lt;-i&apos;JSrn ll =1 lz=l"
"It follows that ifA = (aij)i,j = o&apos;2-&apos;~-&quot;to2 then"
"E [1,1]&apos;+&apos;,xj,,j,~ = aiqi,2. (3)"
"A consequence of this result is that if o4 = 0, then / = n and we determine X completely with relation (3), and then the same for Ua."
"This configuration correspond to the fact that in the (given) semantic proof nets, no out-put contains the two conclusions of a same axiom link."
"In this latter case, the computation is not so simple and should be mixed with word o,&apos;der constraints."
Let us process on an example the previous results.
"We still use the lexicon of table 4, and we want to generate (if possible) a sentence whose meaning is given by the proof net of ligure 7: (try(find j))m."
"We first need to associate every atom with all index (in the figures, we only indicate a number i beside the atom to express it is el)."
"Of course, we have to know how to recognize the ei that are the same in U (figure 6) and in 11 (figure 7)."
This can be done by looking at the typed constants decorating the input conclusions (for the mo-ment. we don&apos;t have a general procedure in the complex cases).
We also assume in this numbering that we know which of the atoms in H(F) are linked to t+ (the unique out-put).
"In our case where 0.4 = 0, it is not a problem to make such a statement."
"In other cases, tile complexity would increase polynomially."
"Then, the given matrices are:"
"According to the definition of the (it) and tile (jr) fami-lies such that 0.2 = El El+j+, we have: ; 4 + 2 ,o I 2 3 4 5 6 7 8"
"Then .1;5,2 -- 1 -- al, 4 X4, 3 -="
"I = a2,5 ./?2,5 -= 1 --~ a4,1 .1~3,4 -- 1 -- a5, 2 ./.&apos;i,7 ~ 1 - a3, 9 397,1 = 1 = ag,3 ah:;,8 ~ 1 ~ a7,10 ;l~8,6 ~-- 1 --~ alo,7 and in this case 0&quot;4 = 0, so according to tile preceeding oooooi il notes .\ is completely determined and 000100~ 001000 010000"
X = Ua = oooooo 000000 C 000010 3
"We can add this matching to the syntactic forest of fig-ure 8(a) (do not forget that//3 represents the edges be-tween ei with i E [17,22])and obtain on F the matching of figure 8(b)."
"We still have to ensure the correctness of this proof net (because we add all the tensor and par links), but it has a quadratic complexity (less than the matrix computation)."
"In this case, it is correct."
"Note: • Actually, this only gives us the axiom links."
It still necessitates to compute the word order to have no crossing axiom link.
This can be done from the ax-iom links easier than quadratic time: it is a well-bracketing check.
"Here, it is easy to see that putting the John item on the left would achieve the result of Mary seeks John, • The choice of seeks and its high order type (for in-tcnsionnality) shows there is no limitation on the or-der of the A-term."
We took advantage of proof nets on the semantic point of view and we expressed the generation process as a guided proof search.
"On top of keeping the decidability property of this framework, we characterized the seman-tic proof nets that enable a polynomial time process."
Such properties are crucial because it is the central part of the generation process (considering Lambek calculus).
But there are other things left to look at.
"As the very next steps, we should work on the atoms numbering and the choice of the lexical items."
Appropriate interactions be-tween word order contraints and matrix resolution in the hard case should also be considered.
"Moreover, another point is to benefit from the power of linear logic and deal with non linear A-terms."
"Finally. since different extensions of Lambek calcu-lus based on proof nets[REF_CITE]have been considered, we hope our pro-posal and its good properties to apply to other linguistic approaches."
Many corpus-based machine translation systems require parallel corpora.
"In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus."
"To gloss a word, we first identify its similar words that occurred in the same context in a large corpus."
We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus.
"Word-for-word glossing is the process directly translating each word or term in the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty include responsibility, obligation, role, ..."
This list is then used to select a translation for duty.
"In the next section, we describe the resources required by our algorithm."
"In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context."
Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm.
"In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work. of 2. Resources a document without considering the word order."
The input to our algorithm includes a collocation Automating this process would benefit many database[REF_CITE]and a corpus-based NLP applications.
"For example, in cross- thesaurus[REF_CITE], which are both available language information retrieval, glossing a on the Interne0."
"In addition, we require a document often provides a sufficient translation bilingual thesaurus."
"Below, we briefly describe for humans to comprehend the key concepts. these resources."
"Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine 2.1."
Collocation database translation (MT) system.
Given a word w in a dependency relationship
"Many corpus-based MT systems require (such as subject or object), the collocation parallel corpora ([REF_CITE]; Brown et database can be used to retrieve the words that al., 1991;[REF_CITE]; Resnik, occurred in that relationship with w, in a large 1999)."
"Figure 1 disambiguation algorithm and a non-paralM shows excerpts of the entries in the collocation bilingual corpus to resolve translation database for the words corporate, duty, and ambiguity. fiduciary."
The database contains a total of 11
"In this paper, we present a word-for-word million unique dependencyrelationships. glossing algorithm that requires only a source language corpus."
The intuitive idea behind our algorithm is the following.
Suppose w is a word I Available[URL_CITE]and[URL_CITE]to be translated.
We first identify a set of words similar to w that occurred in the same context as 2 We use the term collocation to refer to a pair of words that occur in a dependencyrelationship (rather w in a large corpus.
We then use this set (called than the linear proximity of a pair of words).
"Using the collocation database, Lin used an unsupervised method to construct a corpus-based thesaurus[REF_CITE]consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs."
"Given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w. For example, the clustered similar words of duty are shown in Table 1."
"Using the corpus-based thesaurus and a bilingual dictionary, we manually constructed a bilingual thesaurus."
The entry for a source language word w is constructed by manually associating one or more clusters of similar words of w to each candidate translation of w. We refer to the assigned clusters as Words Associated with a Translation (WAT).
"For example, Figure 2 shows an excerpt of our English~French bilingual thesaurus for the words account and duty."
"Although the WAT assignment is a manual process, it is a considerably easier task than providing lexicographic definitions."
"Also, we only require entries for source language words that have multiple translations."
"In Section 7, we corporate: modifier-of: client 196, debt 236, development 179, fee 6, function 16, headquarter 316,[REF_CITE]levy 3, liability 14, manager 203, market 195, obligation 1, personnel 7, profit 595, responsibility 27, rule 7, staff 113, tax 201, training 2, vice president 231.... duty: objeet-of: assume 177, breach 111, carry out 71, do 114, have 257, impose 114, perform 151.... subject-of: affect 4, apply 6, include 42, involve 8, keep 5, officer 22, protect 8, require 13, ... adj-modifier: active 202, additional 46, administrative 44, fiduciary 317, official 66, other 83, ... fiduciary: modifier-of: act 2, behavior I, breach 2, claim I, company 2, duty 317, irresponsibility 2, obligation 32, requirement 1, responsibility 89, role 2, ... account: WAT for 1. compte: investment, transaction, payment, saving,i ii~::::.=....................... money, contract, Budget, reserve, security,! contribution, debt, property holding 2.rapport: report, statement, testimony, card, story, record, document, data, information, view, cheek, figure, article, description, estimate, assessment, number, statistic, comment, letter, picture, note, ... duty: 1. devoir: responsibility, obligation, task, function, role, post, position, job, chore, mission, assignment, liability.... 2. taxe: tariff, restriction, tax, regulation, requirement, procedure, penalty, quota, rule, levy, ... discuss a method for automatically assigning the WATs."
The contextually similar words of a word w are words similar to the intended meaning of w in its context.
"Figure 3 gives the data flow diagram for our algorithm for identifying the contextually similar words of w. Data are represented by ovals, external resources by double ovals and processes by rectangles."
"By parsing a sentence with Minipar3, we extract the dependency relationships involving w. For each dependency relationship, we retrieve 3Available[URL_CITE]"
Input from the collocation database the words occurred in the same dependency relationship w.
"We refer to this set of words as the cohort the responsibility and tax senses of duty, reflecting the fact that the meaning of duty is indeed ambiguous if corporate duty is its sole context."
"In contrast, the second row in Table 2 clearly indicates the responsibility sense of duty."
"While previous word sense disambiguation algorithms rely on a lexicon to provide sense inventories of words, the contextually similar words provide a way of distinguishing between different senses of words without committing to any particular sense inventory."
Figure 4 illustrates the data flow of the word- that for-word glossing algorithm and Figure 5 as describes it. of
"For example, suppose we wish to translate w for that dependency relationship."
Consider the into French the word duty in the context word duty in the contexts corporate duty and corporatefiduciary duty.
Step [Footnote_1] retrieves the fiduciary duty.
The cohort of duty in corporate candidate translations for duty and its WATs duty consists of nouns modified by corporate in from Figure 2.
"In Step 2, we construct two lists Figure 1 (e.g. client, debt, development.... ) and of contextually similar words, one for the the cohort of duty in fiduciary duty consists of dependency context corporate duty and one for nouns modified by fiduciary in Figure 1 (e.g. the dependency contextfiduciary duty, shown in act, behaviour, breach.... )."
The proposed translation for the context
"Intersecting the set of similar words and the is obtained by maximizing the group similarities cohort then forms the set of contextually similar between the lists of contextually similar words words of w. For example, Table 2 shows the and the WATs. contextually similar words of duty in the Using the group similarity measure from contexts corporate duty and fiduciary duty."
"Section 5, Table 3 lists the group similarity words in the first row are retrieved by scores between each list of contextually similar intersecting the words in Table 1 with the nouns words and each WAT as well as the final modified by corporate in Figure 1."
"Similarly, combined score for each candidate translation. the second row represents the intersection of the The combined score for a candidate is the sum words in Table I and the nouns modified fiduciary in Figure 1. by of the logs of all group similarity scores involving its WAT."
The correct proposed
The first set of contextually similar words in translation for duty in this context is devoir since
The corpus-based thesaurus contains only the similarities between individual pairs of words.
"In our algorithm, we require the similarity between groups of words."
The group similarity measure we use is proposed[REF_CITE].
"It takes as input two groups of elements, Gl and G2, and a similarity matrix, sim, which specifies the similarity between individual elements."
"GI and G2 are describable by graphs where the vertices are the words and each weighted edge between vertices wl and w2 represents the similarity, sim(wl, w2), between the words wl and Wz."
Karypis et al. consider both the interconnectivity and the closeness of the groups.
"The absolute interconnectivity between Gt and G2,AI(G t, G2), is defined as the aggregate similarity between the two groups: x~GiYEG2"
"The absolute closeness between G~ and G2, AC(G~, G2), is defined as the average similarity between a pair of elements, one from each group:"
The difference between the absolute 1970).
We used the same approximation interconnectivity and the absolute closeness is methods as[REF_CITE]. that the latter takes zero similarity pairs into The similarity between G1 and G2 is then account.
"In Figure 6, the interconnectivity in (a defined as follows:) and (b) remains constant."
"However, the closeness in (a) is higher than in (b) since there groupSim(G,, G2)= R/(G,, G2)× RC(G,, G2) are more zero similarity pairs in (b)."
"Karypis et al. normalized the absolute where interconnectivity and closeness by the internal 2AI(G,,G2)"
"Minimal edge bisection is performed for all applicable to any source/destination language WATs and all sets of contextually similar words. pair as long as a source language parser is However, the minimal edge bisection problem is available."
We considered English-to-French NP-complete[REF_CITE]. translations in our experiments.
"Fortunately, state of the art graph partitioning We experimented with six English nouns that algorithms can approximate these bisections in have multiple French translations: account, duty, polynomial time ([REF_CITE]race, suit, check, and record."
"ROM, we extracted a testing corpus[Footnote_4] consisting of the first 100 to 300 sentences containing the non-idiomatic usage of the six nounss."
4 Available at fip.cs.umanitoba.ca/pub/ppantei/ download/wfwgtest.zip
"Then, we manually tagged each sentence with one of the candidate translations shown in Table 4."
Each noun in Table 4 translates more frequently to one candidate translation than the other.
"In fact, always choosing the candidate procbs as the translation for suit yields 94% accuracy."
A better measure for evaluating the system&apos;s classifications considers both the algorithm&apos;s precision and recall on each candidate translation.
Table 5 illustrates the precision and recall of our glossing algorithm for each candidate translation.
"Albeit precision and recall are used to evaluate the quality of the classifications, overall accuracy is sufficient for comparing different approaches with our system."
"In Section 3, we presented an algorithm for identifying the contextually similar words of a word in a context using a corpus-based thesaurus and a collocation database."
Each of the six nouns has similar words in the corpus-based thesaurus.
"However, in order to find contextually similar words, at least one similar word for each noun must occur in the collocation database in a given context."
"Thus, the algorithm for constructing contextually similar words is dependent on the coverage of the collocation database."
"We estimated this coverage by counting the number of times each of the six nouns, in several different contexts, has at least one contextually similar word."
The result is shown in Table 6.
"In Section 5, we described a group similarity metric, groupSim, which we use for comparing a WAT with a set of contextually similar words."
"In Figure 7, we compare the translation accuracy of our algorithm using other group similarity metrics."
Suppose G~ and (/2 are two groups of words and w is the word that we wish to translate.
The metrics used are:
I. closest&amp; sum of similarity of the three closest pairs of words from each group.
WORD CANDIDATE PRECISION RECALL account compte 0.982 0.902 rapport 0.680 0.927 duty devoir 0.951 0.963 taxe 0.897 0.867 race course 0.945 0.989 race 0.947 0.783 suit proc6s 0.996 0.993 costume 0.889 0.941 check ch6que 0.951 0.924 contr61e 0.714 0.800 record record 0.968 0.918 enregistrement 0.529 0.750
"WORD NUMBEROF COVERAGE[REF_CITE]95.7% duty 343 93.3% race 294 92.5% suit 332 91.9% check 87.[Footnote_5]%2519 record 1655 92.8% 2. gs: Z sim(x,w)x max sire(x,y)+ Z sire(y, w)xmax sire(y,x) Z.,&apos;im{x,w)+Z ~im@~, ) 3. dC: as defined in Section 5. 4."
"5 Omitted idiomatic phrases include take into account, keep in check, check out, ..."
AI: as defined in Section 5. 5. RC: as defined in Section 5. 6.
RI: as defined in Section 5.
"In mostFrequent, we include the results obtained if we always choose the translation that occurs most frequently in the testing corpus."
We also compared the accuracy of our glossing algorithm with Systran&apos;s translation system by feeding the testing sentences into Systran&apos;s web interface[Footnote_6] and manually examining the results.
6 Available at babelfish.altavista.com/cgi-bin/translate
Figure 8 summarizes the overall accuracy obtained by each system and the baseline on the testing corpus.
Systran tended to prefer one candidate translation over the other and committed the majority of its errors on the non-preferred senses.
"Consequently, Systran is very accurate if its preferred sense is the frequent sense (as in account and duty) but is very inaccurate if its preferred sense is the infrequent one (as in race, suit, and check). 7."
Conclusion and Future Work
This paper presents a word-for-word glossing algorithm.
The gloss of a word is determined by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus.
The algorithm presented in this paper can be improved and extended in many ways.
"At present, our glossing algorithm does not take the prior probabilities of translations into account."
"For example, in WSJ, the bank account sense of account is much more common than the report sense."
We should thus tend to prefer this sense of account.
This is achievable by weighting the translation scores by the prior probabilities of the translations.
We are investigating an Expectation-Maximization (EM)[REF_CITE]algorithm to learn these prior probabilities.
"Initially, we assume that the candidate translations for a word are uniformly distributed."
"After glossing each word in a large corpus, we refine the prior probabilities using the frequency counts obtained."
This process is repeated several times until the empirical prior probabilities closely approximate the true prior probabilities.
"Finally, as discussed in Section 2.3, automatically constructing the bilingual thesaurus is necessary to gloss whole documents."
This is attainable by adding a corpus-based destination language thesaurus to our system.
The process of assigning a cluster of similar words as a WAT to a candidate translation c is as follows.
"First, we automatically obtain the candidate translations for a word using a bilingual dictionary."
"With the destination language thesaurus, we obtain a list S of all words similar to c. With the bilingual dictionary, replace each word in S by its source language translations."
"Using the group similarity metric from Section 5, assign as the WAT the cluster of similar words (obtained from the source language thesaurus) most similar to S."
"Arabic inflectional morphology requires infixation, prefixation and suffixation, giving rise to a large space of morphological variation."
In this paper we describe an approach to reducing the complexity of Arabic morphology generation using discrimination trees and transformational rules.
"By decoupling the problem of stem changes from that of prefixes and suffixes, we gain a significant reduction in the number of rules required, as much as a factor of three for certain verb types."
We focus on hollow verbs but discuss the wider applicability of the approach.
"Morphologically, Arabic is a non-concatenative language."
The basic problem with generating Arabic verbal morphology is the large number of variants that must be generated.
Verbal stems are based on triliteral or quadriliteral roots (3- or 4-radicals).
Stems are formed by a derivational combination of a root morpheme and a vowel melody; the two are arranged according to canonical patterns.
Roots are said to interdigitate with patterns to form stems.
"For example, the Arabic stem katab (he wrote) is composed of the morpheme ktb (notion of writing) and the vowel melody morpheme &apos;a-a&apos;."
"The two are coordinated according to the pattern CVCVC (C=consonant, V=vowel)."
"All these patterns undergo some stem changes with respect to voweling in the 2 tenses (perfect and imperfect), the 2 voices (active and passive), and the 5 moods (indicative, subjunctive, jussive, imperative and energetic).~"
"The stem used in the conjugation of the verb may differ depending on the person, number, gender, tense, mood, and the presence of certain root consonants."
"Stem changes combine with suffixes in the perfect indicative (e.g., katab-naa &apos;we wrote&apos;, kutib-a &apos;it was written&apos;) and the imperative (e.g. uktub-uu &apos;write&apos;, plural), and with both prefixes and suffixes for the imperfect tense in the indicative, subjunctive, and jussive moods (e.g. ya-ktub-na &apos;they write, feminine plural&apos;) and in the energetic mood (e.g. ya-ktub-unna or ya-ktub-un &apos;he certainly writes&apos;)."
There are a total of 13 person-number-gender combinations.
"Distinct prefixes are used in the active and passive voices in the imperfect, although in most cases this results in a change in the written form only if diacritic marks are used.2 Most previous computational treatments of Arabic morphology are based on linguistic models that describe Arabic in a non-concatenative way and focus primarily on analysis."
"In two-level systems, the lexical level includes short vowels that are typically not realized on the the surface level."
"He introduces a multi-tape two-level model and a formalism where three tapes are used for the lexical level (root, pattern, and vocalization) and one tape for the surface level."
"In this paper, we propose a computational approach that applies a concatenative treatment to Arabic morphology generation by separating the issue of infixation from other inflectional variations."
"We are developing an Arabic morphological generator using MORPHE[REF_CITE], a tool for modeling morphology based on discrimination trees and regular expressions."
"MORPHE is part of a suite of tools developed at the Language Technologies Institute, Carnegie Mellon University, for knowledge-based machine translation."
"Large systems for MT from English to Spanish, French, German, Portuguese and a prototype for Italian have already been developed."
"Within this framework, we are exploring English to Arabic translation and Arabic generation for pedagogical purposes."
"We generate Arabic words including short vowels and diacritic marks, since they are pedagogically useful and can always be stripped before display."
Our approach seeks to reduce the number of rules for generating morphological variants of Arabic verbs by breaking the problem into two parts.
"We observe that, with the exception of a few verb types, there is very little interaction between stem changes and the processes of prefixation and suffixation."
"It is therefore possible to decouple, in large part, the problem of stem changes from that of prefixes and suffixes."
"The gain is a significant reduction in the size number of transformational rules, as much as a factor of three for certain verb classes."
"This improves the space efficiency of the system and its maintainability by reducing duplication of rules, and simplifies the rules by isolating different types of changes."
"To illustrate our approach, we focus on a particular type of verbs, termed hollow verbs, and show how we integrate their treatment with that of more regular verbs."
We also discuss how the approach can be extended to other classes of verbs and other parts of speech.
Verb roots in Arabic can be classified as shown in Figure 1.3 A primary distinction is made between weak and strong verbs.
Weak verbs have a weak consonant (&apos;w&apos; or &apos;y&apos;) as one or more of their radicals; strong verbs do not have any weak radicals.
Strong verbs undergo systematic changes in stem voweling from the perfect to the imperfect.
The first radical vowel disappears in the imperfect.
"Verbs whose middle radical vowel in the perfect is &apos;a&apos; can change it to &apos;a&apos; (e.g., qaTa&apos;a &apos;he cut&apos; -&gt; yaqTa&apos;u &apos;he cuts&apos;),4 &apos;i&apos; (e.g., Daraba &apos;he hit&apos; -&gt; yaDribu &apos;he hits&apos;), or &apos;u&apos; (e.g., kataba &apos;he wrote&apos; -&gt; yaktubu &apos;he writes&apos;) in the imperfect."
"Verbs whose middle radical vowel in the perfect is &apos;i&apos; can only change it to &apos;a&apos; (e.g., shariba &apos;he drank&apos; -&gt; yashrabu &apos;he drinks&apos;) or &apos;i&apos; (e.g., Hasiba &apos;he supposed&apos; -&gt; yaHsibu &apos;he supposes&apos;)."
"Verbs with middle radical vowel &apos;u&apos; in the perfect do not change it in the imperfect (e.g., Hasuna &apos;he was beautiful&apos; -&gt; yaHsunu &apos;he is beautiful&apos;)."
"For strong verbs, neither perfect nor imperfect stems change with person, gender, or number."
Hollow verbs are those with a weak middle radical.
"In both perfect and imperfect tenses, the underlying stem is realized by two characteristic allomorphs, one short and one long, whose use depends on the person, number and gender. 3 Grammars of Arabic are not uniform in their classification of &quot;hamzated&quot; verbs, verbs containing the glottal stop as one of the radicals (e.g. [sa?a[] &apos;to ask&apos;)."
"Hamzated verbs change the written &apos;seat&apos; of the hamza from &apos;alif&apos; to &apos;waaw&apos; or &apos;yaa?&apos;, depending on the phonetic context. 4 In the Arabic transcription capital letters indicate emphatic consonants; &apos;H&apos; is the voiceless pharyngeal fricative ; &quot;&apos; the voiced pharyngeal fricative ; &apos;?&apos; is the glottal stop &apos;hamza&apos;."
Hollow verbs fall into four classes:.
"Verbs of the pattern CawaC or CawuC (e.g. [Tawut] &apos;to be long&apos;), where the middle radical is &apos;w&apos;."
Their characteristic is a long &apos;uu&apos; between the first and last radical in the imperfect.
From the underlying root [zawar]: zaara &apos;he visited&apos; and yazuuru &apos;he visits&apos;
Perfect: -zur- and -zaar- Imperfect:-zur- and-zuur-.
"Verbs of the pattern CawiC, where the middle radical is &apos;w&apos;."
Their characteristic is a long &apos;aa&apos; between the first and last radical in the imperfect.
From the underlying root [nawim]: naama &apos;he slept and yanaamu &apos;he sleeps&apos;
Stem aUomorphs :
Perfect: -nirn- and -naam-
"Verbs of the pattern CayaC, where the middle radical is &apos;y&apos;."
Their characteristic is a long &apos;ii&apos; before the first and last radical in the imperfect.
From the underlying root [baya&quot; ]: baa&quot; a &apos;he sold&apos; and yabii&quot; u &apos;he sells&apos;
Stem allomorphs :
Perfect: -bi&apos;- and -baa&apos;-
Imperfect: and -bi&apos;- and -bii&apos;-.
"Verbs of the pattern CayiC, where middle radical is &apos;y&apos;."
From the underlying root [hayib]: haaba &apos;he feared&apos; and yahaabu &apos;he fears&apos;
Stem allomorphs :
Perfect: -bib- and-haab- Imperfect: -hab- and-haab-
"In the relevant literature (e.g.,[REF_CITE]), verbs belonging to the above classes are all assumed to have the pattern CVCVC."
The pattern does not show the verb conjugation class and makes it difficult to predict the type of stem allomorph to use.
"To avoid these problems, we keep information on the middle radical and vowel in the base form of the verb."
"In generation, classes 2 and 4 of the verb can be handled as one because they have the same perfect and imperfect stemsP"
"We describe our approach to modeling strong and hollow verbs below, following a description of the implementation framework."
MORPHE[REF_CITE]is a tool that compiles morphological transformation rules into either a word parsing program or a word generation program.[Footnote_6] In this paper we will focus on the use of MORPHE in generation.
6 MORPHE is written in Common Lisp and the compiled MFH and transformation rules are themselves a set of Common Lisp functions.
Input and Output.
MORPHE&apos;s output is simply a string.
Input is a feature structure (FS) which describes the item that MORPHE must transform.
A FS is implemented as a recursive Lisp list.
"Each element of the FS is a feature-value pair (FVP), where the value can be atomic or complex."
A complex value is itself a FS.
"For example, the FS for generating the Arabic zurtu &apos;I visited&apos; would be: ((ROOT &quot;zawar&quot;) (CAT V) (PAT CVCVC) (VOW HOL) (TENSE PERF) (MOOD IND) (VOICE ACT) (NI/MBER SG) (PERSON i))"
"The choice of feature names and values, other than ROOT, which identifies the lexical item to be transformed, is entirely up to the user."
The FVPs in a FS come from one of two sources.
"Static features, such as CAT (part of speech) and ROOT, come from the syntactic lexicon, which, in addition to the base form of words, can contain morphological and syntactic features."
"Dynamic features, such as TENSEand NUMBER,are set by MORPHE&apos;s caller."
The Morphological Form Hierarchy.
MORPHE is based on the notion of a morphological form hierarchy (MFH) or tree.
Each internal node of the tree specifies a piece of the FS that is common to that entire subtree.
The root of the tree is a special node that simply binds all subtrees together.
The leaf nodes of the tree correspond to distinct morphological forms in the language.
Each node in the tree below the root is built by specifying the parent of the node and the conjunction or disjunction of FVPs that define the node.
Portions of the Arabic MFH are shown in Figures 2-4.
A rule attached to each leaf node of the MFH effects the desired morphological transformations for that node.
A rule consists of one or more mutually exclusive clauses.
"The &apos;if&apos; part of a clause is a regular expression pattern, which is matched against the value of the feature ROOT (a string)."
"The &apos;then&apos; part includes one or more operators, applied in the given order."
"Operators include addition, deletion, and replacement of prefixes, infixes, and suffixes."
The output of the transformation is the transformed ROOTstring.
An example of a rule attached to a node in the MFH is given in Section 3.1 below.
"In generation, the MFH acts as a discrimination network."
The specified FS is matched against the features defining each subtree until a leaf is reached.
"At that point, MORPHE first checks in the irregular forms lexicon for an entry indexed by the name of the leaf node (i.e., the MF) and the value of the ROOTfeature in the FS."
"If an irregular form is not found, the transformation rule attached to the leaf node is tried."
"If no rule is found or none of the clauses of the applicable rule match, MORPHE returns the value of ROOT unchanged. 3 Handling Arabic Verbal Morphology in MORPHE"
Figure 2 sketches the basic MFH and the division of the verb subtree into stem changes and prefix/suffix additions. [Footnote_7] The inflected verb is generated in two steps.
"7 The use of two parts of the same tree for the two problems is a constraint of MORPHE&apos;s implementation, which does not permit multiple trees with separate roots."
MORPHE is first called with the feature CHG set to STEM.
The required stem is returned and temporarily substituted for the value of the ROOTfeature.
"The second call to MORPHE, with feature CHG set to PSFIX, adds the necessary prefix and/or suffix and returns the fully inflected verb."
Figure 2 also shows some of the features used to traverse the discrimination tree.
The feature PAT is used in conjunction with the ROOT feature to select the appropriate affixes.
"Knowing the underlying root and its voweling is crucial for the determination of hollow verb stems, as described in Section 1."
Knowing the pattern is also important in cases where it is unclear.
"For example, verbs of pattern CtVCVC insert a &apos;t&apos; after the first radical (e.g. ntaqat &apos;to move, change location&apos;, intransitive)."
"With some consonants as first radicals, in order to facilitate pronunciation, the &apos;t&apos; undergoes a process of assimilation whose effects differ depending on the preceding consonant."
"For example, the pattern CtVCVC verb from zaHam &apos;to shove&apos; instead of *ztaHarn is zdaHam &apos;to team&apos;."
"It is also difficult to determine from just the string ntaqat whether this is pattern nCVCVC of the verb *taqat (if it existed) or pattern CtVCVC of naqat &apos;to transport, move&apos;, transitive)."
"As a demonstration of our approach, we discuss the case of hollow verbs, whose characteristics were described in Section 1."
"Figure 3 shows the MFH for strong and hollow verbs of pattern CVCVC in the perfect tense, active voice."
"We use the feature vow to carry information about the voweling of the verb in the imperfect (discussed below) and overload it to distinguish hollow and other kinds of verbs. (TENSEPERF) / &apos; , , ~ (VOW(*or*a i u))"
A (PERS(*or*12)) (PERS3) shorts t e m ~ (NUM(*or*sl~dl)) (NUMPL) longstem (GENDERM) (GENDERF) longstem shortstem
Figure 3: The Perfect Stem Change Subtree for
Strong and Hollow Verbs of Pattern CVCVC
"In the perfect active voice, regular strong verbs do not undergo any stem changes, but doubled radical verbs do."
Rules effecting these changes are attached to the node labeled with the FVP (vow (*or* a i u)).[Footnote_8]
8 Hamzated verbs changes are due to interactions with specific suffixes and are best dealt with in the prefixation and suffixation subtree.
"The hollow verbs, on the other hand, use a long stem with a middle &apos;alif&apos; (e.g. [daam] &apos;to last&apos;) for third person singular and dual (masculine and feminine) and for third person plural masculine."
"The remaining person-number-gender combinations take a short stem whose voweling depends on the underlying root of the verb, as specified earlier."
Transformation rules attached to the leaf nodes perform the conversion of the ROOT feature value to the short and long stem.
"Inside the stem change rules, the four different classes of hollow verbs are treated as three separate conditions (classes 2 and 4 can be merged, as described in Section 1) by matching on the middle radical and the adjacent vowels and replacing them with the appropriate vowel."
"An example of such a rule, which changes the perfect stem to a short one for persons 1 and 2 both singular and plural, follows."
The syntax %{var} is used to indicate variables with a given set of values.
"Enclosing a string in parenthesis associates it with a numbered register, so the replace infix (ri) operator can access it for substitution."
Figure 4 shows the imperfect subtree for strong and hollow verbs.
"Strong verbs are treated efficiently by three rules branching on the middle radical vowel, given as the value of vow."
"The consonant-vowel pattern of the computed stem is shown (e.g. for kataba &apos;he wrote&apos;, the imperfect stem would be -ktub- in the pattern CCuC)."
"As described in Section 1, the possible vowel in the imperfect is restricted but not always determined by the perfect vowel and so must be stored in the syntactic lexicon.[Footnote_9] Separating stem changes from the addition of prefixes and suffixes significantly reduces the number of transformation rules that must be written by eliminating much repetition of prefix and suffix addition for different stem changes."
"9 In the presence of certain second and third radicals, the middle radical vowel is more precisely determined. This information can be incorporated into the syntactic lexicon as it is being built."
"For strong verbs of pattern CVCVC, there is at least a three-fold reduction in the number of rules for active voice (recall the different kinds of vowel changes for these verbs from perfect to imperfect described in Section 1)."
Other patterns and the passive of pattern CVCVC verbs show less variation in stem voweling but more variation in prefix and suffix voweling.
"Since some of the patterns share the same prefix and suffix voweling, once the stem has been determined, the prefixation and suffixation rules can be shared by pattern groups."
"The hollow verb subtree is not as small for the imperfect as it is for the perfect, since the stem depends not only on the mood but also on the person, gender, and number."
It is still advantageous to decouple stem changes from prefixation and suffixation.
"Suffixes differ in the indicative and subjunctive moods; if the two types of changes were merged, the stem transformations would have to be repeated in each of the two moods and for each person-number-gender combination."
The same observation applies to stem changes in the passive voice as well.
"Significant replication of transformational rules that include stem changes makes the system bigger and harder to maintain in case of changes, particularly because each transformational rule needs to take into consideration the four different classes of hollow verbs."
Consider again the example verb form zurtu &apos;I visited&apos; and the feature structure (FS) given in Section 2.
"During generation, the feature-value pair (CHGSTEM) is added to the FS before the first call to MORPHE."
"Traversing the MFH shown in Figure 2, MORPHE finds the rule v-stem-fl-act-perf-12 given in Section 3.1 above."
"The first clause fires, replacing the &apos;awa&apos; with &apos;u&apos; and MORPHE returns the stem -zur-."
This stem is substituted as the value of the ROOTfeature in the FS and the feature-value pair (CHGSTEM) is changed to (CHG PSFIX) before the second call to MORPHE.
This time MORPHE traverses a different subtree and reaches the rule: (morph-rule v-psfix-perf-l-sg It II (+s &quot;otu&quot;) ))
"This rule, currently simply appends &quot;otu&quot; to the string, and MORPHE returns the string &quot;zurotu&quot;, where the &apos;o&apos; denotes the diacritic &quot;sukuun&quot; or absence of vowel."
This is the desired form for zurtu &apos;I visited&apos;.
In this paper so far we have focused on regular and hollow verbs of the pattern CVCVC.
Here we examine how our approach applies to other verb types and other parts of speech.
"The two-step treatment of verbal inflection described in this paper is easily extended to the passive, to doubled radical and hamzated verbs, and to different patterns of strong and hollow verbs."
"In fact, since not all higher patterns are affected by the presence of a middle or weak radical (e.g. patterns CVCCV, CaaCVC, taCVCCVC and others), the subtrees for these patterns will be significantly less bushy than for pattern CVCVC."
"The two-step treatment also covers verbs with a weak first radical, especially the radical &apos;w&apos;, which is normally dropped in the active imperfect (e.g. perfect stem warad &apos;to come&apos;, imperfect stem -rid-).~°"
"Alternatively, it can be placed in the 10Exceptions to this rule exist (e.g. the verb waji[ &apos;to be afraid&apos;), with imperfect stem - wjat-) but are rare and can be handled in MORPHE by placing the irregular stem in the syntactic lexicon and checking for it prior to calling MORPHE for stem changes. irregular lexicon, which MORPHE consults when it reaches a leaf node, prior to applying any of the transformational rules."
"Verbs with a weak third radical, including doubly or trebly weak verbs, are the most problematic since the stem changes interact heavily with the inflectional suffixes, and less is gained by trying to modify the stem separately."
We are currently investigating this issue and the best way to treat it in MORPHE. 4.2 Extending the Approach to Other Parts of Speech
The two-step approach to generating verbal morphology also presents advantages for the inflectional morphology of nouns and adjectives.
"In Arabic, the plural of many nouns, especially masculine nouns, is not formed regularly by suffixation."
"Instead, the stem itself undergoes changes according to a complex set of patterns (e.g. rajut &apos;man&apos; pluralizes as rijaat &apos;men&apos;), giving rise to so-called &quot;broken plurals&quot;."
"The inflection of broken plurals according to case (nominative, genitive, accusative) and definiteness, however, is basically the same as the inflection The radical &apos;y&apos; is largely not dropped or changed. of most masculine or feminine singular nouns."
The same holds true for adjectives.
Finally we note that our two-step approach can also be used to combine derivational and inflectional morphology for nouns and adjectives.
Deverbal nouns and present and past participles can be derived regularly from each verb pattern (with the exception of deverbal nouns from pattern CVCVC).
"Relational or &quot;nisba&quot; adjectives are derived, with small variations, from nouns."
"Since these parts of speech are inflected as normal nouns and adjectives, we can perform derivational and inflectional morphology in two calls to MORPHE, much as we do stem change and prefix/suffix addition."
We have presented a computational model that handles Arabic morphology generation concatenatively by separating the infixation changes undergone by an Arabic stem from the processes of prefixation and suffixation.
Our approach was motivated by practical concerns.
We sought to make efficient use of a morphological generation tool that is part of our standard environment for developing machine translation systems.
"The two-step approach significantly reduces the number of morphological transformation rules that must be written, allowing the Arabic generator to be smaller, simpler, and easier to maintain."
The current implementation has been tested on a subset of verbal morphology including hollow verbs and various types of strong verbs.
We are currently working on the other kinds of weak verbs: defective and assimilated verbs.
"Other categories of words can be handled in a similar manner, and we will turn our attention to them next."
"The main tagger used for the comparison experiment is the probabilistic exponential-model-based, error-driven learner we described in detail in (Haji~ and Hladk~, 1998)."
"Modifications had to be made, how-ever, to make it more universal across languages."
"The model described in (Haji~ and Hladk~, 1998) is a general exponential (specifically, a log-linear) model (such as the one used for Maximum Entropy-based models): pAc(ylx ) = exp(~]in_1AJi(y, x)) z(x) (1) where fi(y,x) is a binary-valued feature of the event value being predicted and its context, A~ is a weight of the feature fi, and Z(x) is the natural normalization factor."
This model isthen essentially reduced to Naive Bayes by the approximation of the forms as a resultof morphological analysis.
"For un-ambiguous word forms (unambiguous from the point ofview ofa certaincategory),the ambiguity classset contains only a singlevalue; for ambiguous forms, there are 2 or more values in the AC."
"For example, let&apos;ssuppose we use part-of-speech (POS), number and tense as morphological categories for English; then the word form &quot;borrowed&quot; is2-way ambiguous in POS ({V,J} for verb and adjective,respectively), unambiguous innumber (linguisticarguments apart, number istypicallyregarded &quot;not applicable&quot;to ad-jectives as well as to almost all forms of verbs in English), and 3-way ambiguous in tense ({P,N,-) for past tense, past participle, and &quot;not applicable&quot; in the adjective form)."
"The predictions of the models are always condi-tioned on the ambiguity class of the category (POS, NUMBER, ...) in question."
"In other words, there is a separate model for each category and an ambigu-ity class from that category."
"Naturally, there is no model for unambiguous ACs classes."
"However, even though the ambiguity classes bring very valuable in-formation about the word form being tagged and a reliable information about the context (since they are fixed during tagging), using ACs causes also an unwelcome effect of partitioning the already scarce data and also effectively ignores statistics of the un-ambiguous cases."
"The context of features uses the neighboring words (original word forms) and ambiguity classes on sub-tags, where their relative position in text might be either fixed (0, -1, +1) or &quot;variable&quot; using a value of the POS subtag as the &quot;stop here&quot; criterion, up to 4 text positions (words) apart."
"The original model uses the ambiguity classes not only for conditioning on context in features, but also use laterversionsofthe annotated data (than those found on for the individual models based on category and an the Multext-East CD) which we obtained directlyfrom the AC. authors of the annotations afterthe Multext-CD had been More general features have been introduced, published,sincethe new data contain rathersubstantialim- which do not depend on the ambiguity class of the"
The &quot;stop&quot; criterion for finding the appropriate rel- been used throughout as the only evaluation crite-ative position was originally based on hard coded rion.
"However, since some results reported previ-choices suitable for the Czech language only, and of ously were apparently obtained using only the &quot;real&quot; course it depended on the tagset as well."
"This depen- words as the total for accuracy evaluation, whereas dency has been removed by selecting the appropriate in other experiments every token counts (including conditions automatically when building the pool of punctuation7, for example), we have computed both possible features at the initialization phase5 (using and report them separatelys. the relative frequency of the POS ambiguity classes, 4.2 Availability of Dictionary Information and a threshold to cut off less frequent categories to"
We use two methods to obtain the set of possible limit the size of the feature pool).
"Even though the full computation of the appropriate feature weight is still prohibitive (the more so when the general features are added), the learner is now allowed to vary the weights (in several discrete steps) during feature selection, as a (somewhat crude) at-tempt to depart from the Naive Bayes simplification to the approximation of the &quot;correct&quot; Maximum En-tropy estimation."
"In order to compare the effects of (not) using an in-dependent dictionary, we have added an unknown word handling module to the code.6 It extracts the prefix and suffix frequency information (and the combination thereof) from the training data."
"Then, for each of the combinations, it selects the most fre-quent set of tags seen in the training data and stores it for later use."
"When tagging, the data is first piped through a &quot;guesser&quot; which assigns to the unknown words such a set of possible tags which is stored with the longest matching prefix/suffix combination. phologically)."
Both methods include handling un-known words.
"First, we use only information which may be obtained automatically from the manually annotated corpus (we call this method automatic)."
This is the way the Maximum Entropy tagger[REF_CITE]runs if one uses the binary version from the website (see the comparison in Section 5).
"However, it is not unreasonable to assume that a larger independent dictionary exists which can help to obtain a list of possible tags for each word form in test data."
"This is what we have at our disposal for the languages in question, since the development of such a dictionary was part of the Multext-East project."
"We can thus assume a dictionary info is available for unknown words in the test data, i.e., even though there is no statistics available for them (since they did not appear in the training data), all possible tags for (almost9) every test token are avail-able."
This method is referred to as independent in the following text.
"We have also used a third method of obtain-ing a dictionary information (called mized), namely, by using only the words from the training data, rAnd sometimes a separate token for sentence boundary STable 1 has been computed using all tokens."
"In fact, the 5Also,the use ofvariable-distance context may be switched languages differsignificantly in the proportion ofpunctuation: offentirely. from about 18% (English) to 30% (Estonian). 6Originally, the code relied exclusively on the use of such 9Depending on the quality of the independent dictionary. an independent dictionary."
"Sincethe coverage of the Czech Of course, the tagsets must match, which could be a problem dictionary we have used is extensive, we have been simply per se."
"Here it is simple, since the dictionaries have been ignoring the unknown wordproblem altogether in the past. developed using the same tagsets as the tagged data. but complementing the information about them ob-tained from the training data by including all other Table 9: Exponential w/feature selection vs. Max-possible tags for such words."
"Therefore the net result imum Entropy tagger (Words-only Error Rate, no is that during testing, we have only training words dictionary) at our disposal, but with a complete dictionary in-"
The baseline error rate is computed as follows.
"Hungarian 8.55%8.16% First of all, we use the independent dictionary for Romanian 7.76% 7.66% obtaining the possible tags for each word."
Then we[REF_CITE].26% 17.44% extract only the lexical information from the current position11 and counts used for smoothing (which is 4.3 Tagger Comparison based on the ambiguity classes only and it does not use lexical information).
"The system is then trained The work[REF_CITE]consistently com-normally, which means it uses the lexical information pares several taggers (HMM, Brill&apos;s Transformation-only if the AC-based smoothing[Footnote_12] alone does not based Tagger, Ratnaparkhi&apos;s Maximum Entropy work."
"12UsingACs linearly interpolated with global unigram sub- speed. The runtime speed ofthe MaxEnt tagger is lower,only tag distribution and finally the uniform distribution. about 10 words per second vs. almost 500 words per second; 13Byreasonable we mean less than a day of CPU for train- it should be noted however that we are comparing MaxEnt&apos;s ing. java bytecode and C."
"This baseline method is thus very close to the tagger, and the Daelemans et al.&apos;s Memory-based usual baseline method of using simple conditional Tagger) on Slovene."
We have chosen the Maximum distribution of tags given words.
Entropy tagger[REF_CITE]for a compari-
"The message of Table 2 seems to be obvious; but before we jump to conclusions, let&apos;s present another set of experiments."
"In view of the recent interest in dealing with &quot;small languages&quot;, and with regard to the questions of cost-effectiveness of using &quot;human&quot; resources (i.e. annotation vs. rule-writing vs. tools development etc.), we have also performed experiments with re-duced training data size (but with an enriched fea-ture pool - by lowering thresholds, adding more of the &quot;general features&quot; as described above, etc. - as son with our universal tagger, since it achieved (by a small margin) the best overall result on Slovene as reported there (86.360% on all tokens) of tag-gers available to us (MBT, the best overall, was not freely available to us at the time of writing)."
"We have trained and tested the Maximum Entropy Tag-ger on exactly the same data, using the off-the-shelf (java binary only) version."
The results are compared in Table 9.
"Since we want to show how a tagger accuracy is influenced by the amount of training data available, we have run a series of experiments comparing the allowed by reasonable time/space constraints). 13"
"These results are summarized in Table 3 (using only the dictionary derived from the training data), Table 4 (using words from training data with mor-phological information complemented from a dictio-nary) and Table 5 (using the &quot;independent&quot; dictio-nary)."
"In all cases, we again count only true words (no punctuation)."
"Accordingly, the major POS er-ror rate is reported, too (12 POS tags to be dis-tinguished only: Noun, Verb, Adjective.... ; see Ta-bles 6, 7, and 8). 1°This arrangement removes the &quot;closed vocabulary&quot; phe-nomenon from the test data, since for the Multext-East data, we did not have a truly independent vocabulary available. 11Words from the training data which are not singletons (freq &gt; 1) are used."
"Surprisingly enough, it would not hurt results of the exponential tagger to the maximum entropy tagger when there is only a limited amount of data available."
The results are summarized[REF_CITE].
"Since the public version of the MaxEnt tagger cannot be modified to take advantage of nei-ther the mixed nor the independent dictionary, we have compared it only to the automatic dictionary version of the exponential tagger."
"To save space, the results are tabulated only for the training data sizes of 2000, 5000 and 20000 words."
"Again, only the &quot;true&quot; word error rate is reported."
"As the tables show, for the languages we tested, the exponential, feature-based tagger we adapted from (Haji~ and Hladk~, 1998) achieves similar re-sults as the Maximum[REF_CITE]. (using exactly the same (full) training data; the &quot;score&quot; to use them too."
"Webelieveit is due to the smoothing method is 3:3, with the MaxEnt tagger being substantially used."
"Even though this is validonly for the baseline experi- better on English; probably the development lan-ment, we haveobserved in general that this form of exponen-tial model (with error-driven training, that is) is remarkably 14Otherwisethe acknowledged leader in English tagging resistant to overtrainlng. 15Theonly substantial difference we noticed was in tagging guage bias shows herein)."
"However, when the train- The resulting accuracy (of both taggers) is still ing data size goes down, the advantage of predicting unsatisfactory not only from the point of view of the single morphological categories separately favors results obtained on English, but also from the prac-the exponential tagger (with the notable and sub- tical point of view: approx. 85% accuracy (Czech, stantial exception of English)."
"The less data, the Slovene) typically means that about five out of six larger the difference[REF_CITE]. 10-word sentences contain at least one error in it."
"That is bad news e.g. for parsing projects involving 16Onthe other hand, the Exponential tagger has been de- tagging as a preliminary step. veloped on Czech originally and it lost on this language."
"It should be noted that the original version of the exponen-tial tagger did contain moreCzech-specificfeatures, and thus might in fact do better."
5.1 The Differences Among Languages in a relativelysmall collectionof about 100k tokens is high - from 401 (Hungarian) to 1033 (Slovene); com-
The following discussion abstracts from the tagset pare that to English with only 139 tags.
"However, it design, relying on the fact that the Multext-East is interesting to see that the average per-token ambi-project has been driven by common tagset guidelines guity is much more narrowly distributed, and in fact to an unprecedented extent, given the very different English ranks 3rd (after Hungarian and Slovene), languages involved."
"At the same time, we acknowl- Czech being the last with almost every other token edge that even so, their design for the individual ambiguous on average."
This ambiguity does not cor-languages might have influenced the results.
"Also, respond with the results obtained: Slovene, being the quality of the annotation is an important factor; the second least ambiguous, is the second most dif-we believe though that the later data we obtained ficult to tag."
"Only Czech behaves consistently by for the experiments described here are within the tailing the pack in both cases. range of usual human error and do not suffer from negligence1~. 5.2 Comparison to Previous Results First of all, it is clear that these languages differ Any comparison is necessarily difficult due to differ-substantially just by looking at the simple training ent evaluation methodologies, even within the &quot;best- 17Specifically, we are sure that the post-release Czech, only&quot;, accuracy-based reporting."
"Nevertheless, we Sloveneand Hungariandata we are usingare without anno- will try. tation defectsbeyondthe usualoccasionalannotationerror, For Romanian, Tufts in his recent work (Tufts, as they have been doublechecked,and we also believethat 1999) reports 98.5% accuracy (i.e. 1.5% error rate) the other two languagesare reasonablyclean."
"Bulgarian,al-thoughpresenton the CD, is unfortunatelyunusablesinceit on Romanian, using the classifier combination ap-has not beenmanuallyannotated; forEnglish,see above. proach advocated by e.g.[REF_CITE]."
"His results are well above the 3.29% error rate achieved already Table 2 clearly suggests, even the baseline here (with even a larger tagset of 1391 vs. 486 here), tagging results obtained with the help of an indepen-but the paper does not say how this number has been dent dictionary are comparable (if not better) than computed (training data size, the all-token/words- the fully-trained tagger on 100k words, but without only question) thus making any conclusions difficult the dictionary information."
The situation is even to make.
"He also argues that his method is language clearer when comparing the POS-only results: here independent but no results are mentioned for other the &quot;independent&quot; dictionary results are better by languages. far, with almost no training data needed."
"For Czech, previous work achieved similar results Looking at the characteristics of the languages, it (6.20% on newspaper text using the all-tokens-based is apparent that the inflections cause the problem: error rate computation, on 160,000 training tokens; the coverage of a previously unseen text is inferior to vs. 7.04% here on approx, halfthat amount of train- the usual coverage of English or another analytical ing data; same handling of unknown words)."
This is language.
"Therefore, unless we can come up with in line with the expectations, since the same method- a really clever way of learning rules for dealing with ology (tagging as well as evaluation) has been used, previously unseen words, it is clearly strongly prefer-except the features used in that work were specifi- able to work on a morphologicaldictionary19, rather cally tuned to Czech. than to try to annotate more data."
"The most detailed account of Slovene[REF_CITE]reports various results, which might not 6 Future Work be directly comparable because it is unclear whether they use the all-tokens-based or words-only compu-"
We would like to compare more taggers using still tation of the error rate.
"They report 6.421% error other methodologies, especially the MBT tagger, rate on the full tagset on known words, and 13.583% which achieved the best results on Slovene but which on all words (tokens?) including unknown words was not available to us at the time of writing this (the exponential tagger we used achieved 13.82% on paper."
"Obviously, we would also like to use the clas-all tokens, 16.26% on words only)."
"They use almost sifter combination method on them, to confirm the the same data (Orwell&apos;s 1984, but leaving out the really surprisingly good results on Romanian and Appendices)ls."
They also report that the original test it on the other languages as well.
"Czech-specific exponential tagger used as a basis for We would also like to enrich the best taggers avail-the work reported here achieved 7.28% error rate on able today (such as the Maximum Entropy tagger) Slovene on full tags on the same data, which means by using the dictionary information available and that by the changes to the exponential tagger aimed compare the results with the exponential feature-at its language independence we introduced in Sec-based tagger we have been using in the experiments tion 3, we have not achieved any improvement (on here."
"Slovene) of the exp. tagger (the error rate stayed at For Czech and Slovene, the results are still far be- 7.26% - using all-tokens-based evaluation numbers, low what one would like to see (in absolute terms)."
"It dictionary available; but the data was not exactly seems that the key lies in the initial feature set defi-the same, presumably). nition - including statistical tagset clustering, which 5.3 Dictionary vs. Training Data might potentially lead to more reliable estimates of This is, according to our opinion, the most interest- certain parameters while using still the same size of ing result of the experiments described so far."
"As training data. 18Their tag count is lower (1021) than here (1033), but that&apos;s not really relevant."
"Theydo not report the average 19Not necessarilymanually- apparently, even a partially ambiguity or a similar measure. supervised methodwouldbe oftremendoushelp. 7 Helmut Schmid. 1994."
Probabilistic part-of-speechAcknowledgements The author wishes to thank many Multext-East par- tagging using decision trees.
"In Proceedings of In-ticipants for their efforts to improve the original ternational Con]erence on New Methods in Lan-data, especially to Niki Petkevi~, Tomaz Erjavec, guage Processing, pages 44-49, Manchester, Eng-Heiki-Jaan Ka~lep and G£bor Pr6sz6ky, and for pro- land. viding the final versions of the annotated data for Dan Tufts. 1999."
Tiered tagging and combined lan-the experiments.
Any errors and mistakes are solely guage models classifiers.
"In Proceedings of Text, to be blamed on the author, not the annotators, of Speech and Dialogue&apos;99, Mari~nskd LLzn~, Czech course."
"Republic, Sept. 15-18."
Jean Vdronis. 1996a.
"In this section, we introduce CDG and then describe how CDG constraints can be learned from sentences annotated with grammatical information."
"Constraint Dependency Grammar (CDG), first introduced by Maruyama[REF_CITE], uses constraints to determine the grammatical dependencies for a sentence."
The parsing algorithm is framed as a constraint satis-faction problem: the rules are the constraints and the solutions are the parses.
"A CDG is defined as a five-tuple, (2E,R, L, C, T), where ~ = {al,..., c%} is a finite set of lexical categories (e.g., determiner), R = {rl,...,rp} is a finite set of uniquely named roles or role ids (e.g., governor, needl, need2), L = {ll,...,lq} is a finite set of labels (e.g., subject), C is a constraint formula, and T is a table that specifies allowable category-role-label combinations."
A sentence s - WlW2W3...wn has length n and is an element of ~*.
"For each word wi E ~ of a sentence s, there are up to p different roles (with most words needing only one or two[REF_CITE]), yielding a maximum of n * p roles for the entire sentence."
"A role is a variable that is assigned a role value, an element of the set L × (1, 2,..., n}."
"Role values are denoted as l-m, where l E L and m E (1, 2,..., n} is called the modifiee."
"Maruyama originally used a modifiee of NIL to indicate that a role value does not require a modifiee, but it is more parsimonious to indicate that there is no dependent by setting the modifiee to the position of its word. mar approaches, thus providing a finer degree of Role values are assigned to roles to record the syn-control over the syntactic analysis of a sentence. tactic dependencies between words in the sentence."
"The governor role is assigned role values such that the sentence Clear the screen from the Resource the modifiee of the word indicates the position of the Management corpus[REF_CITE](the ARV word&apos;s governor or head (e.g., DET-3, when assigned and ARVP in the gray box will be discussed later), to the governor role of a determiner, indicates its which is a corpus we will use to evaluate our speech function and the position of its head)."
Every word processing system.
We have constructed a conven-in a sentence has a governor role.
"Need roles are tional CDG with around 1,500 unary and binary used to ensure the requirements of a word are met. constraints (i.e., its arity is 2) that were designed For example, an object is required by a verb that to parse the sentences in the corpus."
"This CDG subcategorizes for one, unless it has passive voice. covers a wide variety of grammar constructs (includ- The required object is accounted for by requiring ing conjunctions and wh-movement) and has a fairly the verb&apos;s need role to be assigned a role value with rich semantics."
"Words can (so its degree is 4), 24 labels, and 13 lexical fea-have more than one need role, depending on the lex- ture types (subcat, agr, case, vtype (e.g., progres-ical category of the word."
"The table T indicates the sive), mood, gap, inverted, voice, behavior (e.g., roles that a word with a particular lexical category must support."
A sentence s is said to be generated by the gram-mar G if there exists an assignment A that maps a role value to each of the roles for s such that C is satisfied.
"There may be more than one assignment of role values to the roles of a sentence that satisfies C, in which case there is ambiguity."
C is a first-order predicate calculus formula over all roles that requires that an assignment of role values to roles be consistent with the formula; those role values incon-sistent with C can be eliminated.
"A subformula P~ of C is a predicate involving =, &lt;, or &gt;, or predi-cates joined by the logical connectives and, or, if, or not."
"A subformula is a unary constraint if it con-tains only a single variable (by convention, we use zl) and a binary constraint if it contains two vari-ables (by convention zl and z2)."
An example of a unary and binary constraint appears in Figure 2.
"A CDG has an arity parameter a, which indicates the maximum number of variables in the subformu-las of C, and a degree parameter d, which is the number of roles in the grammar."
An arity of two suffices to represent a grammar at least as power-ful as a context-free grammar[REF_CITE].
"In[REF_CITE], we developed a way to write constraints concerning the category and feature values of a modifiee of a role value (or role value pair)."
These constraints loosely capture binary constraint information in unary con-straints (or beyond binary for binary constraints) and results in more efficient parsing.
"Au,liwy¢~nst]llnt requiring that • role vmluo Abinaryoonatrllnt requiringthat• rolevllue IlmlgnlKI to the vernot role of I determiner withthe libel SIlllgned to• ne~dl role of one have the label D~ lind • modlflee pointing to word pOkltIt"
"Imother word whole governor • lub4NIqtNl~twocd* role I=mml~gnened• role veltmwiththe libel 8UBJ mass), type (e.g., interrogative, relative), semtype, takesdet, and conjtype)."
The parse in Figure 3 is an assignment of role values to roles that is consis-tent with the unary and binary constraints.
"A role value, when assigned to a role, has access to not only the label and modifiee of its role value, but also the role name of the role to which it is assigned, informa-tion specific to the word (i.e., the word&apos;s position in the sentence, its lexical category, and feature values for each feature), and information about the lexical class and feature values of its modifiee."
Our unary and binary constraints use this information to elim-inate ungrammatical assignments. sented by the assignment of role values to roles as-sociated with a word with a specific lexical category and • rnodlflee that point• beck attheflratword. (if (and (= (category x1) determiner) (if (and (= (label xI ) S) and one feature value per feature.
ARVs and ARVPs (= (rid x1) G)) (see gray box) represent grammatical relations that (= (rid Xl) N1) (and (= (label x1) DET) (= (rnod Xl) (pos x2)) can be extracted from a sentence&apos;s parse. (&gt; (rood x1) (pos Xl)))) (= (rid x2) G)) (and (= (label x2) SUBJ) (= (rood x2) (pos xl)))) 3.2 Learning CDG Constraints
Figure 2: A Unary and binary constraint for CDG.
The grammaticality of a sentence in a language de-fined by a CDG was originally determined by apply-
The white box in Figure 3 depicts a parse for ing the constraints of the grammar to the possible role value assignments.
"If the set of all possible role space of all possible ARVs for the grammar[Footnote_1]; hence, values assigned to the roles of a sentence of length n the set provides an alternative characterization of is denotedS1 =Y;.x RxPOSxLxMODx"
"1,fit 1 can also include information about the possible lexical categories and feature values of the modifiee of Xl."
"Ftx the unary constraints for the grammar, which can ... x Fk, where k is the number of feature types, be partitioned into positive (grammatical) and neg- Fi represents the set of feature values for that type, ative (ungrammatical) ARVs."
"During parsing, if a POS = {1, 2,..., n} is the set of possible positions, role value does not match one of the elements in the MOD = {1, 2,..., n} is the set of possible modi- positive ARV space, then it should be disallowed. flees, and n is sentence length (which can be any Positive ARVs can be obtained directly from the arbitrary natural number), then unary constraints partition $1 into grammatical and ungrammatical role values."
"Similarly, binary constraints partition the set $2 = $1 x $1 = S~ into compatible and in-compatible pairs."
"Building upon this concept of role value partitioning, it is possible to construct another way of representing unary and binary constraints because CDG constraints do not need to reference the exact position of a word or a modifiee in the sentence to parse sentences[REF_CITE]."
"To represent the relative, rather than the abso-lute, position information for the role values in a grammatical sentence, it is only necessary to repre-sent the positional relations between the modifiees and the positions of the role values."
"To support an arity of 2, these relations involve either equality or less-than relations over the modifiees and positions of role values assigned to the roles zl and x2."
"Since unary constraints operate over role values assigned to a single role, the only relative position relations that can be tested are between the role value&apos;s posi-tion (denoted as Pzl) and its modifiee (denoted as Mzl); one and only one of the following three re-lations must be true: (P~:I &lt; Mzl), (Mzl &lt; Pzl), or (Pzl = Mzl)."
"Since binary constraints operate over role values assigned to pairs of roles, zl and z2, the only possible relative position relations that can be tested are between Pzl and Mxt, P:e2 and Mx2, Pzl and Mz~, Pz[Footnote_2] and Mxt, Pzt and Px2, Mxl and Mz2."
2.A2 can also include information about the possible lexical tence. Note that .At is a finite set representing the categories and feature values of the modifiees of Xl and x2.
Note that each of the six has three positional relations (as in the case of unary relations on Pzl and Mzt) such that one and only one of them is simultaneously true.
The unary and binary positional relations provide the necessary mechanism to develop an alternative view of the unary and binary constraints.
"First, we develop the concept of an abstract role value (ARV), which is a finite characterization of all possible role values using relative, rather than absolute, position relations."
"Formally, an ARV for a particular gram-mar G = (~,, R, L, C, T, Ft,..., Fk) is an element of the set: .dl ="
"ExR× L xFt ×...xFkxUC, where UC encodes the three possible positional relations be-tween Pxl and Mxl."
"The gray box of Figure 3 shows an example of an ARV obtained from the parsed sen- parses of sentences: for each role value in a parse for a sentence, simply extract its category, feature, role, and label information, and then determine the po-sitional relation that holds between the role value&apos;s position and modifiee."
"Similarly the set of legal abstract role value pairs (ARVPs), A2 = ]ExRxLxFtx...xFkx~xRxLx F[Footnote_1] x... x"
"1,fit 1 can also include information about the possible lexical categories and feature values of the modifiee of Xl."
"Fk x BC, where BC encodes the positional relations among Pxl, Mxt, Px2, and Mx2, provides an alternative definition for the binary constraints2."
The gray box of Figure 3 shows an example of an ARVP obtained from the parsed sentence.
Positive ARVPs can be obtained directly from the parses of sentences.
"For each pair of role values assigned to different roles, simply extract their category, feature, role, and label information, and then determine the positional relations that hold between the positions and modifiees."
"An enumeration of the positive ARV/ARVPs can be used to represent the CDG constraints, C, and ARV/ARVPs are PAC-learnable from positive ex-amples, as can be shown using the techniques[REF_CITE]."
"ARV/ARVP con-straints can be enforced by using a fast table lookup method to see if a role value (or role value pair) is allowed (rather than propagating thousands of con-straints), thus speeding up the parser."
Resource Management Domain An experiment was conducted to determine the plausibility and the benefits of extracting CDG con-straints from a domain-specific corpus of sentences.
"For our speech application, the ideal CDG should be general enough to cover sentences similar to those that appear in the corpus while being restrictive enough to eliminate sentences that are implausible given the observed sentences."
"Hence, we investigate whether a grammar extracted from annotated sen-tences in a corpus achieves this precision of cover-age."
We also examine whether a learned grammar has the ability to filter out incorrect sentence hy-potheses produced by the HMM component of our system in Figure 1.
"To investigate these issues, we have performed an experiment using the standard"
"Resource Management (RM)[REF_CITE]and Initially, we bootstrapped the grammar by annotat-"
"Extended Resource Management (RM2) ((DARPA), ing a 200 sentence subset of the RM corpus and ex- 1990) corpora."
These mid-size speech corpora have tracting a fairly general grammar from the annota-a vocabulary of 991 words and contain utterances of tions.
"Then using increasingly restrictive grammars sentences derived from sentence templates based on at each iteration, we used the current grammar to interviews with naval personnel familiar with naval identify sentences that required annotation and ver-resource management tasks."
They were chosen for ified the parse information for sentences that suc- several reasons: they are two existing speech corpora from the same domain; their manageable sizes make them a good platform for the development of tech-niques that require extensive experimentation; and the sentences have both syntactic variety and rea-sonably rich semantics.
"RM contains 5,190 separate utterances (3,990 testing, 1,200 training) of 2,845 distinct sentences (2,245 training, 600 testing)."
"We have extracted several types of CDGs from annota-tions of the RM sentences and tested their generality using the 7,396 sentences in RM2 (out of the 8,173) that are in the resource management domain but are distinct from the RM sentences."
We compare these CDGs to each other and to the conventional CDG described previously.
"The corpus-based CDGs were created by extract-ing the allowable grammar relationships from the RM sentences that were annotated by language ex-perts using the SENATOR annotation tool, a CGI (Common Gateway Interace) HTML script written in GNU C++ version 2.8.1[REF_CITE]."
We tested two major CDG variations: those derived di-rectly from the RM sentences (Sentence CDGs) and those derived from simple template-expanded RM sentences (Template CDGs).
"For example, &quot;List ceeded."
This iterative technique reduced the time required to build a CDG from about one year for the conventional CDG to around two months[REF_CITE].
Several methods of extracting an ARV/ARVP grammar from sentences or template-extended sen-tences were investigated.
"The ARVPs are extracted differently for each method; whereas, the ARVs are extracted in the same manner regardless of the method."
Recall that ARVs represent the set of ob-served role value assignments.
"In our implementa-tion, each ARV includes: the label of the role value, the role to which the role value was assigned, the lexical category and feature values of the word con-taining the role, the relative position of the word and the role value&apos;s modifiee, and the modifiee&apos;s lexical category and feature values (modifiee constraints)."
We use modifiee constraints for ARVs regardless of extraction method because their use does not change the coverage of the extracted grammar and not using the information would significantly slow the parser[REF_CITE].
"Because the ARVP space is larger than the ARV space, we investigate six varia-tions for extracting the pairs: 1."
"Full Mod: contains all grammar and feature MIDPAC&apos;s deployments during (date)&quot; is a sentence value information for all pairs of role values from containing a date template which allows any date annotated sentences, as well as modifiee con-representations."
"For these experiments, we focused straints."
"For a role value pair in a sentence to be on templates for dates, years, times, numbers, and considered valid during parsing with this gram-latitude and longitude coordinates."
"Each template mar, it must match an ARVP extracted from the name identifies a sub-grammar which was produced annotated sentences. by annotating the appropriate strings."
We then an- 2.
Full: like Full Mod except it does not impose notated sentences containing the template names as modifiee constraints on a pair of role values during if they were regular sentences.
"Although annotating a corpus of sentences can be ers feature and modifiee constraints only for pairs a labor intensive task, we used an iterative approach that are directly related by a modifiee link."
"Dur-that is based on parsing using grammars with vary-ing parsing, if a role value pair is related by a ing degrees of restrictiveness."
"A grammar can be modifiee link, then a corresponding ARVP with made less restrictive by ignoring: full feature and modifiee information must appear * lexical information associated with a role value&apos;s in the grammar for it to be allowed."
"If the pair modifiee in the ARVPs, is not directly related, then an ARVP must be o feature information of two role values in an ARVP stored for the grammar relations, ignoring feature not directly related based on their modifiee rela- and modifiee constraint information. tions, 4."
"Feature: like Feature Mod except it does not . syntactic information provided by two role values impose modifiee constraints on a pair of role val-that are not directly related, ues during parsing. • specific feature information (e.g., semantics or 5."
"Direct Mod: stores only the grammar, feature, subcategorization). and modifiee information for those pairs of role each RM grammar."
RM2 using the conventional CDG and CDGs derived ARVP from sentences only or template-expanded sentences.
"Template Percent Variation CDG [ CDG Increase ~: Parsed with ARVP ~ Parsed with Full Mod Variation Sentence CDG[REF_CITE]408,912 51.43%[REF_CITE](60.32%) 165,480 200,792 3,735 (50.50%)[REF_CITE].34%[REF_CITE]758[REF_CITE](71.88%) 4,509 (60.97%)49,468 14.74%[REF_CITE](80.14%) Feature[REF_CITE](72.54%)36,558 40,308 10.26% Direct[REF_CITE](83.94%) 41,124 14.30%47,004[REF_CITE](78.04%)"
"Direct Mod8.29% 5,464 (73.88%) ARVs[REF_CITE](84.82%) 5,931 (80.19%)4,424 4,648 5.06%[REF_CITE](96.59%) not applicable values that are directly related by a modifiee link."
"During parsing, if a role value pair is related by of 1200 utterances was 5.0%, the 1-best sentence ac-such a link, then a corresponding ARVP must ap- curacy was 72.1%, and the word graph coverage ac-pear in the grammar for it to be allowed."
Any curacy was 95.1%.
"Also, the average uncompressed pair of role values not related by a modifiee link word graph size was 75.15 nodes, and our compres-is allowed (an open-world assumption). sion algorithm resulted in a average word graph size 6."
Direct: like Direct Mod except it does not im- of 28.62 word nodes.
"When parsing the word graph, pose modifiee constraints on a pair of role values the probability associated with a word node can ei-during parsing. ther represent its acoustic score or a combination"
"Grammar sizes for these six grammars, extracted of its acoustic and stochastic grammar score."
"We either directly from the 2,845 sentences or from the use the acoustic score because (Johnson and Harper, 2,845 sentences expanded with our sub-grammar 1999) showed that by using a word node&apos;s acoustic templates, appear in Table 1."
"The largest gram- score alone when extracting the top sentence candi- mars were derived using the Full Mod extrac-tion method, with a fairly dramatic growth result-ing from processing template-expanded sentences."
"The Feature and Direct variations are more man-ageable in size, even those derived from template-expanded sentences."
Size is not the only important consideration for a grammar.
Other important issues are grammar generality and the impact of the grammar on the accuracy of selecting the correct sentence from the recognition lattice of a spoken utterance.
"After extracting the CDG grammars from the RM sen-tences and template-expanded sentences, we tested the generality of the extracted grammars by using each grammar to parse the 7,396 RM2 sentences."
See the results in Table 2.
"The grammar with the greatest generality was the conventional CDG for the RM corpus; however, this grammar also has the unfortunate attribute of being quite ambigu-ous."
The most generalizable of extracted grammars uses the Direct method on template-expanded sen-tences.
"In all cases, the template-expanded sen-tence grammars gave better coverage than their cor-responding sentence-only grammars. date after parsing gave a 4% higher sentence accu-racy."
"For the parsing experiments, we processed the 1,080 word graphs produced for the RM test set that contained 50 or fewer word nodes after com-pression (out of 1,200 total) in order to efficiently compare the 12 ARV/ARVP CDG grammars and the conventional CDG (the larger word graphs re-quire significant time and space to parse using the conventional CDG)."
"The num-ber of role values prior to binary constraint propa-gation differ across the grammars with an average (and SD) for the conventional grammar of 504.99 (442.00), for the sentence-only grammars of 133.37 (119.48), and for the template-expanded grammars of 157.87 (145.16)."
"Table 3 shows the word graph parsing speed and the path, node, and role value (RV) ambiguity after parsing; Table 4 shows the sentence accuracy and the accuracy and percent cor-rect for words."
Note that percent correct words is calculated using N-D N -S and word accuracy using
"We have also used the extracted grammars to where N is the number of words, D is N-D-S-I N post-process word graphs created by the word graph the number of deletions, S is the number of substi-compression algorithm of (Johnson and Harper, tutions, and I is the number of insertions. 1999) for the test utterances in the RM corpus."
"As The most selective RM sentence grammar, Full was reported[REF_CITE], the Mod, achieves the highest sentence accuracy, but word-error rate of our HMM recognizer with an em- at a cost of a greater average parsing time than bedded word pair language model on the RM test set the other RM sentence grammars."
Higher accu- racy appears to be correlated with the ability of the ity to improve sentence accuracy of our speech sys-constraints to eliminate word nodes from the word tem.
To achieve balance between precision and cov-graph during parsing.
"The least restrictive sentence erage of our corpus-induced grammars, we have ex-grammar, Direct, is less accurate than the other panded the RM sentences with templates for expres-sentence grammars and offers an intermediate speed sions like dates and times."
"The grammars extracted of parsing, most likely due to the increased ambigu- from these expanded sentences gave increased RM2 ity in the parsing space."
"The fastest grammar was coverage without sacrificing even 1% of the sentence the Feature-Mod grammar, which also offers an accuracy."
We are currently expanding the number of intermediate level of accuracy.
"Its size (even with templates in our grammar in an attempt to obtain templates), restrictiveness, and speed make it very full coverage of the RM2 corpus using only template-attractive."
The template versions of each grammar expanded RM sentences.
We have recently added showed a slight increase in average parse times (from ten semantic templates to the grammar and have processing a larger number of role values) and a improved the coverage by 9.19% without losing any slight decrease in parsing accuracy.
The conven- sentence accuracy.
"We are also developing a stochas-tional grammar was the least competitive of the tic version of CDG that uses a statistical ARV, which grammars both in speed and in accuracy. is similar to a supertag[REF_CITE]."
The ability to extract ARV/ARVP grammars with H. U. Block. 1997.
Language components in VERB-varying degrees of specificity provides us with the MOBIL.
"In Proc. of the Int. Conf. of Acoustics, ability to rapidly develop a grammar with the abil- Speech, and Signal Proc., pages 79-82."
Japanese Subordinate Clauses based on Scope Embedding Preference
"Japanese Sentence First, we overview dependency analysis of a Japanese sentence."
Since words in a Japanese the following two constraints: 1.
Every chunk (bunsetsu) except the last one modifies only one posterior chunk (bun-setsu). 2.
No modification crosses to other modifica-tions in a sentence.
"Table 1 gives an example of word segmenta-tion, part-of-speech tagging, and bunsetsu seg-mentation (chunking) of a Japanese sentence, where the verb and the adjective are tagged with their parts-of-speech as well as conjuga-tion forms."
"Figure i shows the phrase structure, the bracketing,5 and the dependency (modifica-tion) relation of the chunks (bunsetsus) within the sentence."
A Japanese subordinate clause is a clause whose head chunk satisfies the following properties. 1.
"The content words part of the chunk (bunsetsu) is one of the following types: (a) A predicate (i.e., a verb or an adjective). (b) nouns and a copula like &quot;Noun1 dearu&quot; (in English, &quot;be Noun1&quot;). 2."
"The function words part of the chunk (bunsetsu) is one of the following types: (a) Null. (b) Adverb type such as &quot;Verbl ippou-de&quot; (in English, &quot;(subject) Verb1 ..., on the other hand,&quot;). (c) Adverbial noun type such as &quot;Verb1 tame&quot; (in English, &quot;in order to Verb1&quot;). (d) FormM noun type such as &quot;Verb1 koto&quot; (in English, gerund &quot;Verbl-ing&quot;). (e) Temporal noun type such as &quot;Verb1 mae&quot; (in English, &quot;before (subject) Verb1 ...&quot;). (f) A predicate conjunctive particle such as &quot;Verbl ga&quot; (in English, &quot;although (subject) Verbl ...,&quot;). (g) A quoting particle such as &quot;Verbl to (iu)&quot; (in English, &quot;(say) that (subject) Verbl ...&quot;). (h) (a),,~(g) followed by topic marking particles and/or sentence-final particles."
Figure 2: Definition of Japanese Subordinate Clause 2.2 Japanese Subordinate Clause to-tomoni (Clause2)&quot; and &quot;Verbl nagara (Clause2)&quot;.
The following gives the definition of what we call a &quot;Japanese subordinate clause&quot; throughout this Category B: 46 expressions representing paper.
A clause in a sentence is represented as cause and discontinuity such as &quot;Verb1 a sequence of chunks.
"Since the Japanese lan- te (Clause2)&quot; (in English &quot;Verbl and guage is a head-final language, the clause head (Clause2)&quot;) and &quot;Verb1 node&quot; (in English is the final chunk in the sequence."
"A grammati- &quot;because (subject) Verb1 ...,&quot;). cal definition of a Japanese subordinate clause is Category C: One expression representing in-given in Figure 2.6 For example, the Japanese dependence, &quot;Verb1 ga&quot; (in English, &quot;al-sentence in Table 1 has one subordinate clause, though (subject) Verb1 ...,&quot;). whose scope is indicated as the shaded rectangle in Figure 1."
"The category A has the narrowest scope, while the category C has the broadest scope, i.e., 2.3 Scope Embedding Preference of Subordinate Clauses Category A -4 Category B -4 Category C We introduce the concept[REF_CITE]&apos;s classification of Japanese subordinate clauses where the relation &apos;-&lt;~denotes the embedding by describing the more specific classification by relation of scopes of subordinate clauses."
"Then, Shirai et al. scope embedding preference of Japanese subor-(1995)."
A subordinate clause can be embedded within ing to the embedding relation of their scopes. the scope of another subordinate clause which inherently has a scope of the same or a broader Category A: Seven expressions representing breadth. simultaneous occurrences such as &quot;Verb1 2.
"A subordinate clause can not be embedded within the scope of another subordinate clause SThis definition includes adnominal or noun phrase which inherently has a narrower scope. modifying clauses &quot;Clause1 (NP1)&quot; (in English, rela-tive clauses &quot;(NP1) that Clause1&quot;)."
"Since an adnom- For example, a subordinate clause of &apos;Category inal clause does not modify any posterior subordinate"
"B&apos; can be embedded within the scope of another clauses, but modifies a posterior noun phrase, we regard adnominal clauses only as modifeeswhen considering de- subordinate clause of &apos;Category B&apos; or &apos;Category pendencies between subordinate clauses."
"C&apos;, but not within that of &apos;Category A&apos;."
"Figure 3 (a) gives an example of an anterior Japanese Dependency Preference of Japanese subordinate clause ( &quot;kakimaze-nagara&quot;, Cate- Subordinate Clauses gory A), which is embedded within the scope 1."
"The head vp chunk of Clause1 can modifythat of a posterior one with a broader scope (&quot;ni- of Clause2 if Clause2 inherently has a scope of mashita-ga-,&quot;, Category C)."
"Since the poste- the same or a broader breadth compared with rior subordinate clause inherently has a broader that of Clause1. scope than the anterior, the anterior is embed- 2."
The head vp chunk of Clausez can not mod-ded within the scope of the posterior.
"On the ify that of Clause2 if Clause2 inherently has a other hand, Figure 3 (b) gives an example of narrowerscope comparedwith that of Clause1. an anterior Japanese subordinate clause (&quot;ari- 3 Learning Dependency Preference masu-ga-,&apos;, Category C), which is not embed-of Japanese Subordinate Clauses ded within the scope of a posterior one with a narrower scope ( &quot;kakimaze-nagara&quot;, Category As we mentioned in section 1, the rule-based A)."
"Since the posterior subordinate clause in- approach[REF_CITE]to analyz-herently has a narrower scope than the anterior, ing dependencies of subordinate clauses using the anterior is not embedded within the scope scope embedding preference has serious limi-of the posterior. tation in its coverage against corpora of large size for practical use."
"In order to overcome 2.4 Preference of Dependencies the limitation of the rule-based approach, in between Subordinate Clauses based this section, we propose a method of learning on Scope E m b e d d i n g"
P r e f e r e n c e dependency preference of Japanese subordinate
Following the scope embedding preference of clauses from a bracketed corpus.
"We formalize Japanese subordinate clauses proposed by Mi- the problem of deciding scope embedding pref-nami (1974),[REF_CITE]applied it erence as a classification problem, in which var-to rule-based Japanese dependency analysis, ious types of linguistic information of each sub-and proposed the following preference of decid- ordinate clause are encoded as features and used ing dependencies between subordinate clauses. for deciding which one of given two subordinate Suppose that a sentence has two subordinate clauses has a broader scope than the other."
"As clauses Clausez and Clause2, where the head a statistical learning method, we employ the de-vp chunk of Clauses precedes that of Clause2. cision list learning method[REF_CITE]. that of Clause2, but modifies that of another calculated and the decision list is constructed subordinate clause or the matrix clause which by the following procedure. follows Clause2. 1."
"For each piece of evidence, calculate the likeli- Roughly speaking, the first corresponds to the hood ratio of the conditional probability of a de-case where Clause2 inherently has a scope of the cision D = xl (given the presence of that piece same or a broader breadth compared with that of evidence) to the conditional probability of of Clause1, while the second corresponds to the the rest of the decisions"
"D =-,xl: case where Clause2 inherently has a narrower"
Each decision rule in a decision list is sorted 2.
"The final line of a decision list is defined as &apos;a default&apos;, where the likelihood ratio is calculated TOurmodeling is slightly different from those of other standard approaches to statistical dependency analy- as the ratio of the largest marginal probability sis ([REF_CITE]; Haruno of the decision D = xl to the marginal proba-et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given[REF_CITE]discusses several techniques for two vp chunks or clauses, and the casewhere dependency avoiding the problems which arise when an observed relation does not hold."
In contrast to those standard ap- count is 0.
"Among those techniques, we employ the sim-proaches, we ignore the case where the head vp chunk plest one, i.e., adding a small constant c~ (0.1 &lt; c~ &lt; of Clause1 modifiesthat of another subordinate clause 0.25) to the numerator and denominator."
With this which precedesClause2.
"This is because we assume that modification, more frequent evidence is preferred when this case is more loosely related to the scope embedding there exist several evidences for each of which the con-preference of subordinate clauses. ditional probability P(D =x [E=I) equals to 1. final word is conjugative, iv) Lexical: lexicalized P(D=xl) forms of &apos;Grammatical&apos; features which appear l°g2 P(D=&quot;xl) more than 9 times in EDR corpus."
"The &apos;default&apos; decision of this final line is D-= xz ture of these four types is binary and its value with the largest marginal probability. is &apos;1&apos; or &apos;0&apos; (&apos;1&apos; denotes the presence of the cor-responding feature, &apos;0&apos; its absence)."
The whole 3.3 Feature of Subordinate Clauses feature set shown in Table 2 is designed so as to
"Japanese subordinate clauses defined in sec- cover the 210,000 sentences of EDR corpus. tion 2.2 are encoded using the following four types of features: i)"
"Punctuation: represents 3.4 Decision List Learning of whether the head vp chunk of the subordinate Dependency Preference of clause is marked with a comma or not, ii) Gram- Subordinate Clauses matical: represents parts-of-speech of function First, in the modeling of the evidence, we con-words of the head vp chunk of the subordi- sider every possible correlation (i.e., depen-nate clause,9 iii) Conjugation form of chunk- dency) of the features of the subordinate clauses 9Terms ofparts-of-speechtags and conjugationforms listed in section 3.3."
"Furthermore, since it is are borrowed from those of the Japanese morphological necessary to consider the features for both of the ana/ysis system Chasen[REF_CITE]. given two subordinate clauses, we consider all the sentence-final vp chunk."
As shown in Fig-
"We divided the 210,000 sentences of the whole ure 4 (b), the head vp chunks Segl and Seg2 EDR bracketed Japanese corpus into 95% train-have feature sets ~&apos;1 and ~&apos;2, respectively."
"Then, ing sentences and 5~0 test sentences."
"Then, every possible subsets F1 and F2 of ~1 and we extracted 162,443 pairs of subordinate ~2 are considered,n respectively, and training clauses from the 199,500 training sentences, and pairs of an evidence and a decision are collected learned a decision list for dependency prefer-as in Figure 4 (c)."
"In this case, the value of the ence of subordinate clauses from those pairs. decision D is &quot;beyond&quot;, because Segl modifies The default decision in the decision list is the sentence-final vp chunk, which follows Seg2."
"D =&quot;beyond&quot;, where the marginal probability P(D = &quot;beyond&quot;) = 0.5378, i.e., the baseline 1°Our formalization of the evidence of decision list precision of deciding dependency between two learning has an advantage over the decision tree learn- subordinate clauses is 53.78 %."
We limit the fre-ing[REF_CITE]approach to feature selection of de-quency of each evidence-decision pair to be more pendency analysis[REF_CITE].
"In the feature selection procedure of the decision tree learning method, than 9."
"The total number of obtained evidence-the utility of each feature is evaluated independently, decision pairs is 7,812."
We evaluate the learned and thus the utility of the combination of more than one decision list through several experiments.12 features is not evaluated directly.
"On the other hand, in First, we apply the learned decision list to our formalization of the evidence of decision list learn-deciding dependency between two subordinate ing, we consider every possible pair of the subsets F1 and"
"Fz, and thus the utility of the combination of more than clauses of the 5% test sentences."
"We change one features is evaluated directly. the threshold of the probability P(D I E)13 in lXSince the feature &apos;predicate-conjunctive-particle(chunk-final)&apos; subsumes &apos;predicate-conjunctive- 12Details of the experimental evaluation will be pre-particle(chunk-final)-&quot;ga&quot;, they are not considered sented[REF_CITE]. together as one evidence."
"I~P(D I E) can be used equivalently to the likelihood the decision list and plot the trade-off between subordinate clauses, in which scope embed-coverage and precision. 14 As shown in the plot ding preference of subordinate clauses is ex-of &quot;Our Model&quot; in Figure 5, the precision varies ploited."
We evaluated the estimated dependen-from 78% to 100% according to the changes of cies of subordinate clauses through several ex-the threshold of the probability P(D IE). periments and showed that our model outper-
"Next, we compare our model with the other formed other related models. two models: (a) the model learned by apply-ing the decision tree learning method[REF_CITE]to our task of deciding depen- M. Collins. 1996."
"A new statistical parser based on dency between two subordinate clauses, and (b) bigram lexical dependencies."
"In Proceedings of the a decision list whose decisions are the following 34th Annual Meeting of ACL, pages 184-191."
"EDR (Japan Electronic Dictionary Research Insti-two cases, i.e., the case where dependency rela- tute, Ltd.). 1995."
EDR Electronic Dictionary tion holds between the given two vp chunks or
"Technical Guide. clauses, and the case where dependency relation"
J. Eisner. 1996.
Three new probabilistic models for does not hold.
The model (b) corresponds to a dependency parsing: An exploration.
"In Proceed-model in which standard approaches to statis- ings of the 16th COLING, pages 340-345. tical dependency analysis ([REF_CITE]; Fujio M. Fujio and Y. Matsumoto. 1998."
Japanese de-and[REF_CITE]) are pendency structure analysis based on lexicalized applied to our task of deciding dependency be- statistics.
In Proceedings of the 3rd Conference on tween two subordinate clauses.
Their results Empirical Methods in Natural Language Process- are also in Figures 5 and 6.
Figure 5 shows that &quot;Our Model&quot; outperforms the other two mod-els in coverage.
Figure 6 shows that our model outperforms both of the models (a) and (b) in coverage and precision.
"Finally, we examine whether the estimated dependencies of subordinate clauses improve the precision[REF_CITE]&apos;s statistical dependency analyzer. 15"
"Depending on the threshold of P(D [ E), we achieve 0.8,,~1.8% improvement in chunk level precision, and 1.6~-4.7% improvement in sentence level,is"
"This paper proposed a statistical method for learning dependency preference of Japanese ratio. 14Coverage: the rate ofthe pairs ofsubordinate clauses whose dependencies are decided by the decision list, against the total pairs of subordinate clauses, Precision: the rate of the pairs of subordinate clauses whose depen-dencies are correctly decided by the decision list, against those covered pairs of subordinate clauses. 15[REF_CITE]&apos;s lexicalized depen-dency analyzer is similar to that[REF_CITE], where various features were evaluated through performance test and an optimal feature set was manually selected. 16The upper bounds of the improvement in chunk level and sentence level precisions, which are estimated by providing[REF_CITE]&apos;s statistical de-pendency analyzer with correct dependencies of subor-"
This paper demonstrates that machine learning is a suitable approach for rapid parser development.
"The quality of the treebank, particularly crucial given its small size, is supported by a consistency checker. 1 Introduction Given the enormous complexity of natural language, parsing is hard enough as it is, but often unforeseen events like the crises in Bosnia or East-Timor create a sudden demand for parsers and machine transla-tion systems for languages that have not benefited from major attention of the computational linguis-tics community up to that point."
"Like Japanese, Korean is a head-final agglutinative language."
"It is written in a phonetic alphabet called hangul, in which each two-byte character represents one syllable."
"While our parser operates on the orig-inal Korean hangul, this paper presents examples in a romanized transcription."
"In sentence (1) for example, the verb is preceded by a number of so-called eojeols (equivalent to bunsetsus in Japanese) like &quot;chaeg-eul&quot;, which are typically composed of a content part (&quot;chaeg&quot; = book) and a postposition, which often corresponds to a preposition in English, but is also used as a marker of topic, subject or ob-ject (&quot;eul&quot;). ,_"
"I- , ~ _ °];gl 7 _"
Na-neun eo-je geu chaeg-eul sass-da. ITOPIC yesterday this bookoBJ bought. (1) I bought this book yesterday.
"Our parser produces a tree describing the structure of a given sentence, including syntactic and semantic roles, as well as additional information such as tense."
"For example, the parse tree for sentence (1) is shown below:"
"For preprocessing, we use a segmenter and mor-phological analyzer, KMA, and a tagger, KTAG, both provided by the research group of Prof. Rim of"
"KMA, which comes with a built-in Korean lexicon, segments Korean text into eojeols and provides a set of possible sub-segmentations and &lt; morphological analyses."
KTAG then tries to select -3 the most likely such interpretation.
"Our parser is initialized with the result of KMA, preserving all ~ interpretations, but marking KTAG&apos;s choice as the top alternative."
"The additional resources used to train and test a parser for Korean, which we will describe in more detail in the next section, were (1) a 1187 sentence treebank, (2) a set of 133 context features, and (3) background knowledge in form of an &apos;is-a&apos; ontology with about 1000 entries."
"These resources were built by a team consisting of the principal researcher and two graduate students, each contributing about 3 months."
"The treebank sentences are taken from the Korean newspaper Chosun, two-thirds from 1994 and the re-mainder from 1999."
Sentences represent continuous articles with no sentences skipped for length or any other reason.
The average sentence length is 21.0 words.
"The feature set describes the context of a partially parsed state, including syntactic features like the part of speech of the constituent at the front/top of the input list (as sketched in figure 2) or whether the second constituent on the parse stack ends in a"
Boxes represent frames.
"The asterisk (*) represents the comma, as well as semantic features like whether or current parse position."
"Optionally, parse actions can not a constituent is a time expression or contains have additional arguments, like target syntactic or se-a location particle."
The feature set can accommo-mantic classes to overwrite any default.
"Elements on the date any type of feature as long as it is computable, input list are identified by positive integers, elements on and can thus easily integrate different types of back-the parse stack by negative integers."
The feature &apos;Synt of ground knowledge. -1&apos; for example refers to the (main) syntactic category of 3.3 Background Knowledge the top stack element.
"Before the reduce operation, the feature &apos;Synt of-1&apos; would evaluate to np (for &quot;a book&quot;), The features are supported by background knowl-after the operation to vp (for &quot;bought a book&quot;)."
"The in-edge in the form of an ontology, which for example put list is initialized with the morphologically analyzed has a time-particle concept with nine sub-concepts words, possibly still ambiguous."
After a sequence of shift (accounting for 9 of the 1000 entries mentioned
"The first graduate student, a native Korean and a native Korean and computer science major, in-linguistics major, hired for 11 weeks, spent about stalled Korean tools including a terminal for hangul 2 weeks getting trained, 6 weeks on building two- and the above mentioned KMA and KTAG, wrote a thirds of the treebank, 2 weeks providing most back- number of scripts tying all tools together, made some ground knowledge entries and 1 week helping to tool improvements, built one-third of the treebank and also contributed to the feature set."
"The prin-cipal researcher, who does not speak Korean, con-tributed about 3 person months, coordinating the project, training the graduate students, writing tree-bank consistency checking rules (see section 6), mak-ing extensions to the tree-to-parse-action-sequence module (see section 4.1) and contributing to the background knowledge and feature set."
"We base our training on the machine learning based approach of (Hermjakob k:[REF_CITE]), allow-ing however unrestricted text and deriving the parse action sequences required for training from a tree-bank."
The basic mechanism for parsing text into a shallow semantic representation is a shift-reduce type parser[REF_CITE]that breaks parsing into an ordered sequence of small and manageable parse actions.
Figure 2 shows a typical reduce action.
The key task of machine learning then is to learn to pre-dict which parse action to perform next.
"Two key advantages of this type of deterministic parsing are that its linear run-time complexity with respect to sentence length makes the parser very fast, and that the parser is very robust in that it produces a parse tree for every input sentence."
Figure 3 shows the overall architecture of parser training.
"From the treebank, we first automatically generate a parse action sequence."
"Then, for every step in the parse action sequence, typically several dozens per sentence, we automatically compute the value for every feature in the feature set, add on the parse action as the proper classification of the parse action example, and then feed these examples into a machine learning program, for which we use an ex-tension of decision trees ([REF_CITE]; Hermjakob &amp;[REF_CITE])."
We built our parser incrementally.
"Starting with a small set of syntactic features that are useful across all languages, early training and testing runs reveal machine learning conflict sets and parsing errors that point to additionally required features and possibly also additional background knowledge."
"A conflict set is a set of training examples that have identical values for all features, yet differ in their classification (= parse action)."
Machine learning can therefore not possibly learn how to handle all examples correctly.
This is typically resolved by adding an additional feature that differentiates between the examples in a linguistically relevant way.
Even treebanking benefits from an incremental ap-proach.
"Trained on more and more sentences, and at the same time with also more and more features, parser quality improves, so that the parser as a tree-banking tool has to be corrected less and less fre-quently, thereby accelerating the treebanking pro-cess. ¢ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
Knowledge Base (&quot;ontology&quot;) Treebank . temporal-concept ~-the-year: iday-of-the-week &apos; computer science Monday ...
Sunday ~ syntactic-element parse action sequence generator (automatic) verb noun adverb . . . . . . . . . . . . . . . . . . . . . . . . . . . . count-noun mass-noun Parse action sequence: Shift noun
I Shift noun i
"Feature set: &apos;, l"
Reduce 2 as mod head Svnt Svnt of-2 of- 1 S~n~ Done ~ parse example generator (automatic)
Parse action examples: [Unavail Unavail Noun [ Shift noun [ [Unavaii Noun Noun [ Shift noun I [Noun Noun Unavail
I Reduce 2 as mod head I [Unavaii Noun Unavail [Done I decision structure builder (automatic)
Parse decision structure:
Synt of 1 N ~ ail
Shi~tnoun //~nt of-2
Done Reduce 2 as rood head
Figure 3: Derivation of the parser from a treebank and a feature set.
"The resulting parser has the form of a decision structure, an extension of decision trees."
"Given a seen or unseen sentence in form of a list of words, the decision structure keeps selecting the next parse action until a single parse tree covering the entire sentence has been built."
The segmenter and morphological analyzer KMA re-turns a list of alternatives for each eojeol.
"However, the alternatives are not atomic but rather two-level stituent in the logged parse."
"Labeled preci- sion/recall measures not only structural correctness, but also the correctness of the syntactic label."
"Cor-rect operations measures the number of correct operations during a parse that is continuously cor- rected based on the logged sequence; it measures as well as for intermediate inspection, and therefore might have unduly influenced the evaluation."
"A particularly striking number is the tagging accu-racy, 94.2%, which is dramatically below the equiv-alent 98% to 99% range for a good English or Japanese parser."
"In a Korean sentence, only larger constituents that typically span several words are separated by spaces, and even then not consistently, so that segmentation errors are a major source for tagging problems (as it is to some degree however also for Japanese[Footnote_2])."
"2While Japanese does not use spaces at all, script changes between kanji, hiragana, and katakana provide a lot of seg-mentation guidance. Modern Korean, however, almost exclu-sively uses only a single phonetic script."
"We found that the segmen-tation part of KMA sometimes still struggles with relatively simple issues like punctuation, proposing for example words that contain a parenthesis in the middle of standard alphabetic characters."
"We have corrected some of these problems by pre- and post-processing the results of KMA, but believe that there is still a significant potential for further improve-ment."
"In order to assess the impact of the relatively low tagging accuracy, we conducted experiments that simulated a perfect tagger by initializing the parser with the correctly segmented, morphologically ana-lyzed and tagged sentence according to the treebank."
"By construction, the tagging accuracy in table 2 rises to 100%."
"Since the segmenter/tagger returns not just atomic but rather two-level constituents, the precision and recall values benefit particularly strongly, possibly inflating the improvements for these metrics, but other metrics like crossing brack-ets per sentence show substantial gains as well."
"Thus we believe that refined pre-parsing tools, as they are in the process of becoming available for Korean, will greatly improve parsing accuracy."
"However, for true low density languages, such high quality preprocessors are probably not available so that our experimental scenario might be more re-alistic for those conditions."
"On the other hand, some low density languages like for example Tetun, the principal indigenous language of East Timor, are based on the Latin alphabet, separate words by spaces and have relatively little inflection, and there-fore make morphological analysis and segmentation relatively simple."
It is difficult to maintain a high treebank quality.
"When training on a small treebank, this is particu-larly important, because there is not enough data to allow generous pruning."
Treebanking is done by humans and humans err.
Even with annotation guidelines there are often ad-ditional inconsistencies when there are several an-notators.
"In the Penn Treebank[REF_CITE]for example, the word ago as in &apos;two years ago&apos;, is tagged 414 times as an adverb and 150 times as a preposi- Acknowledgments tion."
"I would like to thank Kyoosung Lee for installing,"
"In many treebanking efforts, basic taggers and improving and conncecting Korean pre-processing parsers suggest parts of speech and tree structures tools like segmenter and tagger as well as starting that can be accepted or corrected, typically speed- the treebanking, and Mina Lee, who did most of the ing up the treebanking effort considerably."
"How- treebanking. ever, incorrect defaults can easily slip through, leav-ing blatant inconsistencies like the one where the References constituent &apos;that&apos; as in &apos;the dog that bit her&apos; is tree-"
M. J. Collins. 1997.
"Three Generative, Lexicalised banked as a noun phrase containing a conjunction"
Models for Statistical Parsing.
"From the very beginning of treebanking, we have"
U. Hermjakob and R. J. Mooney. 1997.
Learning therefore passed all trees to be added to the tree-
Parse and Translation Decisions From Examples bank through a consistency checker that looks for
With Rich Context.
"ACL, pages 482-489. type of phrase, the consistency checker draws on a"
URL: file://[URL_CITE]list of acceptable patterns in a BNF style notation. /contex-acl-97.ps.
Z While this consistency checking certainly does not U. Hermjakob. 1997.
"Learning Parse and Transla-guarantee to find all errors, and can produce false tion Decisions From Examples With Rich Context. alarms when encountering rare but legitimate con- Ph.D. thesis, University of Texas at Austin, Dept. structions, we have found it a very useful tool to of Computer[REF_CITE]-12. maintain treebank quality from the very beginning, URL: file://[URL_CITE]easily offsetting the about three man days that it /hermjakob-dissertation-97.ps.Z took to adapt the consistency checker to Korean."
"Geunbae Lee, Jong-Hyeok Lee, and Hyuncheol Rho."
"For a number of typical errors, we extended the 1997."
Natural Language Processing for Session-checker to automatically correct errors for which this
"Based Information Retrieval Interface on the Web. could be done safely, or, alternatively, suggest a"
"In Proceedings of IJCAI-97 workshop on AI in dig-likely correction for errors and prompt for confir- ital libraries, pages 43-48. mation/correction by the treebanker."
M. P. Marcus. 1980.
A Theory of Syntactic Recog-nition for Natural Language.
"M. Marcus, B. Santorini, and M. A. Marcinkiewicz."
Comparisons with related work are unfortunately 1993.
"Building a Large Annotated Corpus of En-very problematic, because the corpora are differ- glish: The Penn Treebank."
"Computational Lin-ent and are sometimes not even described in other guistics 19(2), pages 313-330. work."
In most cases Korean research groups also use J. R. Quinlan. 1993.
"C4.5 Programs for Machine other evaluation metrics, particularly dependency Learning."
"Morgan Kaufmann Publishers, San Ma-accuracy, which is often used in dependency struc- teo, California. ture approaches."
"Training on about 40,000 sentences K. J. Seo, K. C. Nam, and K. S. Choi. 1998."
"A Prob-[REF_CITE]achieves a crossing brackets rate of abilistic Model for Dependency Parsing Consider-1.07, a better value than our 1.63 value for regular ing Ascending Dependencies."
"Journal of Literary parsing or the 1.13 value assuming perfect segmen- and Linguistic Computing,[REF_CITE](2). tation/tagging, but even for similar text types, com- Juntae Yoon, Seonho Kim, and Mansuk Song. 1997. parisons across languages are of course problematic."
New Parsing Method Using Global Association
"It is clear to us that with more training sentences, Table."
"In Proc. of the International Workshop on and with more features and background knowledge Parsing Technology. to better leverage the increased number of train-ing sentences, accuracy rates can still be improved significantly."
"But we believe that the reduction of parser development time from two years or more down to three months is in many cases already very valuable, even if the accuracy has not &apos;maxed out&apos; yet."
"And given the experience we have gained from this project, we hope this research to be only a first step to an even steeper development time reduction."
A particularly promising research direction for this is to harness knowledge and training resources across languages.
Our goal is to learn a representation for each word in terms of features which characterize the syntactic and semantic context in which the word tends to appear.
Our features are defined as simple relations over a collection of predicates that capture (some of) the information available in a sentence.
"Definition 1 Let s =&lt; wl,w2,...,wn &gt; be a sen-tence in which wi is the i-th word."
"Let :£ be a col-lection of predicates over a sentence s. IS(s))1, the Information source(s) available for the sentence s is a representation ors as a list of predicates I E :r,"
"XS(S) = {II(Wll , ...Wl,),-.., /~g(W~l , ..-Wk,)}."
Ji is the arity of the predicate Ij.
"Example 2 Let s be the sentence &lt; John, X,at, the, clock, to, see, what, time, it, is &gt; Let ~={word, pos, subj-verb}, with the interpreta-tion that word is a unary predicate that returns the value of the word in its domain; pos is a unary predicate that returns the value of the pos of the word in its domain, in the context of the sentence; subj- verb is a binary predicate that returns the value of the two words in its domain if the second is a verb in the sentence and the first is its subject; it returns ¢ otherwise."
"IS(s) = {word(wl) = John, ..., word(w3) = at,..., word(wn) = is, pos(w4) = DET,..., s bj - verb(w , w2) = {John, x}...}."
The IS representation of s consists only of the pred-icates with non-empty values.
"E.g., pos(w6) = modal is not part of the IS for the sentence above. subj - verb might not exist at all in the IS even if the predicate is available, e.g., in The ball was given to Mary."
"Clearly the IS representation of s does not contain all the information available to a human reading s; it captures, however, all the input that is available to the computational process discussed in the rest of this paper."
"The predicates could be generated by any external mechanism, even a learned one."
This issue is orthogonal to the current discussion.
Our goal is to learn a representation for each word of interest.
"Most efficient learning methods known today and, in particular, those used in NLP, make use of a linear decision surface over their feature space[REF_CITE]."
"Therefore, in or-der to learn expressive representations one needs to compose complex features as a function of the in-formation sources available."
A linear function ex-pressed directly in terms of those will not be expres-sive enough.
"We now define a language that allows 1We denote IS(s) as IS wherever it is obvious what the referred sentence we is, or whenever we want to indicate In-formation Source in general. features are defined using the SIS."
"When defining a feature, the naming of nodes in s is done relative to a distinguished node, denoted wp, which we call the focus word of the feature."
Regardless of the arity of the features we sometimes denote the feature f defined with respect to wp as f(wp).
"Definition 7 (Proximity) Let SIS(s) = (s, E) be the linear structure and let I E Z be a k-ary predicate with range R. Let Wp be a focus word and C ="
"In order to learn with features generated using these C(wp, [l,r]) the chain around it."
"Then, the proximity definitions as input, it is important that features features for I with respect to the chain C are defined generated when applying the definitions on different as: ISs are given the same identification."
"In this pre-sentation we assume that the composition operator 1 ifI(w) = a,a E R,w E C along with the appropriate IS element (e.g., Ex. 2, fc(l(w), a) = { 0 otherwise Ex. 9) are written explicitly as the identification of (3) the features."
Some of the subtleties in defining the output representation are addressed in (Cumby and The second type of feature composition defined
"Definition 8 (Collocation) Let fl,...fk be fea-"
"So far we have presented features as relations over ture definitions, collocc (f l ,f 2, . . . f k) is a restricted IS(s) and allowed for Boolean composition opera- conjunctive operator that is evaluated on a chain tors."
In most cases more information than just a list C of length k in a graph.
"Specifically, let C = of active predicates is available."
"We abstract this {wj,, wj=, .. . ,wjk } be a chain of length k in SIS(s). using the notion of a structural information source Then, the collocation feature for fl,.., fk with re- (SIS(s)) defined below."
This allows richer class of spect to the chain C is defined as feature types to be defined. 2We note that we do not define the features will be used in the learning process.
"These are going to be defined in a data 1 ifVi = 1,...k, fi(wj,) = 1 driven way given the definitions discussed here and the input collocc(fl, . . . , fk) = { 0 otherwise"
Example 9 Let s be the sentence in Example 2.
We define some of the features with respect to the linear structure of the sentence.
"The word X is used as the focus word and a chain [-10, 10] is defined with respect to it."
The proximity features are defined with respect to the predicate word.
"We get, for example: fc(word) ----John; fc(word) = at; fc(word) = clock."
"Collocation features are defined with respect to a chain [-2, 2] centered at the focus word X. They are defined with respect to two basic features fl, f2 each of which can be either f(word, a) or f(pos, a)."
"The resulting features include, for example: collocc(word, word)= {John- X}; collocc(word, word) = {X - at}; collocc(word, pos) = {at- DET}."
"So far we have described feature definitions which make use of the linear structure of the sentence and yield features which are not too different from stan-dard features used in the literature e.g., n-grams with respect to pos or word can be defined as colloc for the appropriate chain."
"Consider now that we are given a general directed acyclic graph G = (s, E) on the the sentence s as its nodes."
Given a distin-guished focus word wp 6 s we can define a chain in the graph as we did above for the linear structure of the sentence.
"Since the definitions given above, Def. 7 and Def. 8, were given for chains they would apply for any chain in any graph."
This generaliza-tion becomes interesting if we are given a graph that represents a more involved structure of the sentence.
"Consider, for example the graph DG(s) in Fig-ure 1. DG(s) described the dependency graph of the sentence s."
"An edge (wi,wj) in DG(s) repre-sent a dependency between the two words."
In our feature generation language we separate the infor-mation provided by the dependency grammar3 to two parts.
"The structural information, provided in the left side of Figure 1, is used to generate SIS(s)."
The labels on the edges are used as predicates and are part of IS(s).
"Notice that some authors[REF_CITE]have used the struc-tural information, but have not used the information given by the labels on the edges as we do."
The following example defines features that are used in the experiments described in Sec. 4.
"A subj-verb 3This information can be produced by a functional de-pendency grammar (FDG), which assigns each word a spe-cific function, and then structures the sentence hierarchically feature, fsubj-verb, can be defined as a collocation over chains constructed with respect to the focus word join."
"Moreover, we can define fsubj-verb to be active also when there is an aux_vrb between the subj and verb, by defining it as a disjunction of two collocation features, the subj-verb and the subj-aux_vrb-verb."
"Other features that we use are conjunctions of words that occur before the focus verb (here: join) along all the chains it occurs in (here: will, board, as) and collocations of obj and verb."
"As a final comment on feature generation, we note that the language presented is used to define &quot;types&quot; of features."
These are instantiated in a data driven way given input sentences.
"A large number of fea-tures is created in this way, most of which might not be relevant to the decision at hand; thus, this pro-cess needs to be followed by a learning process that can learn in the presence of these many features."
Our experimental investigation is done using the SNo W learning system[REF_CITE].
Earlier ver-sions of SNoW[REF_CITE]have been applied successfully to several natural lan-guage related tasks.
"Here we use SNo W for the task of word prediction; a representation is learned for each word of interest, and these compete at evalua-tion time to determine the prediction."
The SNo W architecture is a sparse network of linear units over a common pre-defined or incrementally learned feature space.
It is specifically tailored for learning in domains in which the potential number of features might be very large but only a small subset of them is actually relevant to the decision made.
Nodes in the input layer of the network represent simple relations on the input sentence and are being used as the input features.
"Target nodes represent words that are of interest; in the case studied here, each of the word candidates for prediction is repre-sented as a target node."
"An input sentence, along with a designated word of interest in it, is mapped into a set of features which are active in it; this rep-resentation is presented to the input layer of SNoW and propagates to the target nodes."
Target nodes are linked via weighted edges to (some of) the input features.
"Let At = {Q,... ,i,~} be the set of features that are active in an example and are linked to the target node t."
"Then the linear unit corresponding to t is active iff t E wi &gt; Ot, iEAt where w~ is the weight on the edge connecting the ith feature to the target node t, and Ot is the threshold"
A given example is treated autonomously by each target subnetwork; an example labeled t may be 1.
"To compare mistake driven algorithms with treated as a positive example by the subnetwork naive Bayes, trigram with backoff and a simple for t and as a negative example by the rest of the maximum likelihood estimation (MLE) base-target nodes."
The learning policy is on-line and line. mistake-driven; several update rules can be used 2.
To create a set of experiments which is compa-within SNOW.
"The most successful update rule is rable with similar experiments that were previ-a variant of Littlestone&apos;s Winnow update rule (Lit-ously conducted by other researchers. tlestone, 1988), a multiplicative update rule that is tailored to the situation in which the set of input 3."
"To build a baseline for two types ofextensions of features is not known a priori, as in the infinite the simple use oflinear features: (i) Non-Linear attribute model[REF_CITE]."
"This mechanism is features (ii) Automatic focus of attention. implemented via the sparse architecture of SNOW. 4. To evaluate word prediction as a simple lan- That is, (1) input features are allocated in a data guage model. driven way - an input node for the feature i is al-located only if the feature i was active in any input We chose the verb prediction task which is sim-sentence and (2) a link (i.e., a non-zero weight) ex- ilar to other word prediction tasks (e.g.,(Golding ists between a target node t and a feature i if and and[REF_CITE])) and, in particular, follows the only if i was active in an example labeled t. paradigm[REF_CITE]."
"There, a list of the confusion sets is"
"One of the important properties of the sparse ar- constructed first, each consists of two different verbs. chitecture is that the complexity of processing an The verb vl is coupled with v2 provided that they example depends only on the number of features ac- occur equally likely in the corpus."
"In the test set, tive in it, na, and is independent of the total num- every occurrence of vl or v2 was replaced by a set ber of features, nt, observed over the life time of the {vl, v2} and the classification task was to predict the system."
This is important in domains in which the correct verb.
"For example, if a confusion set is cre-total number of features is very large, but only a ated for the verbs &quot;make&quot; and &quot;sell&quot;, then the data small number of them is active in each example. is altered as follows: make the paper --+ {make,sell} the paper Once target subnetworks have been learned and the network is being evaluated, a decision sup- sell sensitive data --~ {make,sell} sensitive data port mechanism is employed, which selects the dominant active target node in the SNoW"
The evaluated predictor chooses which of the twounit via a winner-take-all mechanism to produce a fi- verbs is more likely to occur in the current sentence. nal prediction.
"In choosing the prediction task in this way, weSNoW is available publicly at http ://L2R. cs.uiuc. edu/- cogcomp, html. make sure the task in difficult by choosing between competing words that have the same prior proba-bilities and have the same part of speech."
"A fur-ther advantage of this paradigm is that in future experiments we may choose the candidate verbs so that they have the same sub-categorization, pho-netic transcription, etc. in order to imitate the first phase of language modeling used in creating can-didates for the prediction task."
"Moreover, the pre-transformed data provides the correct answer so that (i) it is easy to generate training data; no supervi-sion is required, and (ii) it is easy to evaluate the results assuming that the most appropriate word is provided in the original text."
Results are evaluated using word-error rate (WER).
"Namely, every time we predict the wrong word it is counted as a mistake."
We used the Wall Street Journal (WSJ) of the years 88-89.
"The size of our corpus is about 1,000,000 words."
The corpus was divided into 80% training and 20% test.
The training and the test data were processed by the FDG parser[REF_CITE].
Only verbs that occur at least 50 times in the corpus were chosen.
This resulted in 278 verbs that we split into 139 confusion sets as above.
"Af-ter filtering the examples of verbs which were not in any of the sets we use 73, 184 training examples and 19,852 test examples."
In order to test the advantages of different feature sets we conducted experiments using the following features sets: 1.
"Linear features: proximity of window size 4-10 words, conjunction of size 2 using window size 4-2."
The conjunction combines words and parts of speech. 2.
"Linear + Non linear features: using the lin-ear features defined in (1) along with non linear features that use the predicates subj, obj, word, pos, the collocations subj-verb, verb-obj linked to the focus verb via the graph structure and conjunction of 2 linked words."
"The over all number of features we have generated for all 278 target verbs was around 400,000."
In all tables below the NB columns represent results of the naive Bayes algorithm as implemented within SNoW and the SNoW column represents the results of the sparse Winnow algorithm within SNOW.
"Table 1 summarizes the results of the experiments with the features sets (1), (2) above."
"The baseline experiment uses MLE, the majority predictor."
"In addition, we conducted the same experiment using trigram with backoff and the[REF_CITE].3%."
From these results we conclude that using more expressive features helps significantly in reducing the WER.
"However, one can use those types of features only if the learning method handles large number of pos-sible features."
This emphasizes the importance of the new learning method.
Table 2: Comparison of the improvement achieved using similarity methods[REF_CITE]and using the methods presented in this paper.
Results are shown in percentage of improvement in accuracy over the baseline.
Table 2 compares our method to methods that use similarity measures[REF_CITE].
"Since we could not use the same corpus as in those experiments, we compare the ratio of improvement and not the WER."
"The baseline in this studies is different, but other than that the experiments are identical."
We show an improvement over the best similarity method.
"Furthermore, we train using only 73,184 examples while[REF_CITE]train using 587, 833 examples."
Given our experience with our approach on other data sets we conjecture that we could have improved the results further had we used that many training examples.
"SNoW is used in our experiments as a multi-class predictor - a representation is learned for each word in a given set and, at evaluation time, one of these is selected as the prediction."
The set of candidate words is called the confusion set[REF_CITE].
Let C be the set of all target words.
In previ-ous experiments we generated artificially subsets of size 2 of C in order to evaluate the performance of our methods.
"In general, however, the question of determining a good set of candidates is interesting in it own right."
"In the absence, of a good method, one might end up choosing a verb from among a larger set of candidates."
We would like to study the effects this issue has on the performance of our method.
"In principle, instead of working with a single large confusion set C, it might be possible to,split C into subsets of smaller size."
"This process, which we call the focus of attention (FOA) would be beneficial only if we can guarantee that, with high probability, given a prediction task, we know which confusion experiments were conducted that use the phonetic set to use, so that the true target belongs to it."
"In transcription ofthe words to generate confusion sets. fact, the FOA problem can be discussed separately for the training and test stages."
Bline NB SNoW
Train All[REF_CITE].84 11.6 12.3 1.
Given our training policy (Sec. 3) ev-
Train PC[REF_CITE].84 11.6 11.3 ery positive example serves as a negative exam-ple to all other targets in its confusion set.
For Table 4: Simulating Speech Recognizer: Word a large set C training might become computa- Error Rate for Training and testing with tionally infeasible. confusion sets determined based on phonetic 2.
Testing: considering only a small set of words classes (PC) from a simulated speech recog-as candidates at evaluation time increases the nizer. baseline and might be significant from the point of view of accuracy and efficiency.
"In the first experiment (Table 4), the transcription of each word is given by the broad phonetic groups"
"To evaluate the advantage of reducing the size of to which the phonemes belong i.e., nasals, fricative, the confusion set in the training and test phases, we etc.4."
"For example, the word &quot;b_u_y&quot; is transcribed conducted the following experiments using the same using phonemes as &quot;b_Y&quot; and here we transcribe it features set (linear features as in Table 1)."
Bline NB SNoW Train All[REF_CITE].44 65.22 65.05 Train All[REF_CITE].6 13.54 13.15 Train 2[REF_CITE].6 13.54 11.55 as &quot;P_VI&quot; which stands for &quot;Plosive_Vowell&quot;.
This partition results in a partition of the set of verbs into several confusions sets.
"A few of these confusion sets consist of a single word and therefore have 100% baseline, which explains the high baseline."
Bline NB SNoW
Table 3: Evaluating Focus of Attention: Word Train All[REF_CITE].63 26.36 27.54
Error Rate for Training and testing using Train PC[REF_CITE].63 26.36 25.55 all the words together against using pairs of words.
Table 5: Simulating Speech Recognizer: Word
Error Rate for Training and testing with &quot;Train All&quot; means training on all 278 targets to-confusion sets determined based on phonetic gether. &quot;Test all&quot; means that the confusion set is classes (PC) from a simulated speech recog-of size 278 and includes all the targets.
The results nizer.
"In this case only confusion sets that shown in Table 3 suggest that, in terms of accuracy, have less than 98% baseline are used, which the significant factor is the confusion set size in the explains the overall lower baseline. test stage."
The effect of the confusion set size on training is minimal (although it does affect training Table 5 presents the results of a similar exper-time).
"We note that for the naive Bayes algorithm iment in which only confusion sets with multiple the notion of negative examples does not exist, and words were used, resulting in a lower baseline. therefore regardless of the size of confusion set in As before, Train All means that training is done training, it learns exactly the same representations. with all 278 targets together while Train PC means Thus, in the NB column, the confusion set size in that the PC confusion sets were used also in train-training makes no difference. ing."
"We note that for the case of SNOW, used here The application in which a word predictor is used with the sparse Winnow algorithm, that size of the might give a partial solution to the FOA problem. confusion set in training has some, although small, For example, given a prediction task in the context effect."
"The reason is that when the training is done of speech recognition the phonemes that constitute with all the target words, each target word repre-the word might be known and thus suggest a way sentation with all the examples in which it does not to generate a small confusion set to be used when occur are used as negative examples."
"When a smaller evaluating the predictors. confusion set is used the negative examples are more Tables 4,5 present the results of using artificially likely to be &quot;true&quot; negative. simulated speech recognizer using a method of gen-eral phonetic classes."
"That is, instead of transcrib- 5 Conclusion ing a word by the phoneme, the word is transcribed by the phoneme classes([REF_CITE]This paper presents a new approach to word predic-)."
"Specifically,these experiments deviate from the task tion tasks."
"For each word of interest, a word repre-definition given above."
"The confusion sets used are sentation is learned as a function of a common, but of different sizes and they consist of verbs with dif- 4In this experiment, the vowels phonemes were divided ferent prior probabilities in the corpus."
Two sets of into two different groups to account for different sounds. potentially very large set of expressive (relational) features.
Given a prediction task (a sentence with a missing word) the word representations are evalu-ated on it and compete for the most likely word to complete the sentence.
We have described a language that allows one to define expressive feature types and have exhibited experimentally the advantage of using those on word prediction task.
We have argued that the success of this approach hinges on the combination of using a large set of expressive features along with a learning approach that can tolerate it and converges quickly despite the large dimensionality of the data.
We believe that this approach would be useful for other disambiguation tasks in NLP.
We have also presented a preliminary study of a reduction in the confusion set size and its effects on the prediction performance.
In future work we intend to study ways that determine the appropriate confusion set in a way to makes use of the current task properties.
"The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model, and how to smooth the empirically ob-tained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning."
"For exam-ple, in a second-order Markov grammar we con-ditioned the L2 label according to the distribu-tion p(L2 I Lt,M,I,t,h,H)."
"Also, remember that H is a placeholder for any other informa-tion beyond the constituent e that may be useful in assigning c a probability."
"In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17]."
A complete review of log-linear models is beyond the scope of this paper.
"Rather, we concentrate on the aspects of these models that most di-rectly influenced the model presented here."
"To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input."
"In our work we as-sume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does."
In the parser we further assume that fea-tures are chosen from certain feature schemata and that every feature is a boolean conjunc-tion of sub-features.
"For example, in computing the probability of the head&apos;s pre-terminal t we might want a feature schema f(t, l) that returns 1 if the observed pre-terminal of c = t and the label of c = l, and zero otherwise."
"This feature compute/~(a I b), and perhaps iS(a I c) as well, is obviously composed of two sub-features, one and take some combination of these values as recognizing t, the other 1."
"If both return 1, then one&apos;s best estimate for p(a I b, c)."
This method the feature returns 1. is known as &quot;deleted interpolation&quot; smoothing.
Now consider computing a conditional prob-
"In max-entropy models one can simply include ability p(a I H) with a set of features fl... features for all three events fl(a, b,c), f2(a, b), fj that connect a to the history H."
"In a log-linear and f3(a, c) and combine them in the model ac-model the probability function takes the follow- cording to Equation 3, or equivalently, Equation ing form: 4."
"The fact that the features are very far from independent is not a concern. 1 eXl(a,H)fl(a,H)+...+,km(a,H).fm(a,H) Now let us note that we can get an equation p(a I H) - Z(H) of exactly the same form as Equation 4 in the ( )3 following fashion:"
"Here the Ai are weights between negative and positive infinity that indicate the relative impor- p(alb, c)p(alb, c,d) p(alb, c,d)=p(alb)-~alb) p(alb, c) tance of a feature: the more relevant the feature to the value of the probability, the higher the ab- (5) solute value of the associated X. The function Note that the first term of the equation gives a Z(H), called the partition function, is a normal- probability based upon little conditioning infor-izing constant (for fixed H), so the probabilities mation and that each subsequent term is a num- over all a sum to one. ber from zero to positive infinity that is greater"
"Now for our purposes it is useful to rewrite or smaller than one if the new information be-this as a sequence of multiplicative functions ing considered makes the probability greater or gi(a,H) for 0 &lt; i &lt; j: p(a I H)= go(a,H)gl(a,H) ...gj(a,H)."
"Here go(a,H) = 1/Z(H) and gi(a,H) smaller than the previous estimate."
"As it stands, this last equation is pretty much ( ) content-free."
But let us look at how it works for4 a particular case in our parsing scheme.
Con- = sider the probability distribution for choosing e&apos;~(a&apos;n)f~(a&apos;H).
The intuitive idea is that each the pre-terminal for the head of a constituent. factor gi is larger than one if the feature in ques-
"In Equation I we wrote this as p(t I l, H)."
"As tion makes the probability more likely, one if the we discuss in more detail in Section 5, several feature has no effect, and smaller than one if it different features in the context surrounding c makes the probability less likely. are useful to include in H: the label, head"
Maximum-entropy models have two benefits pre-terminal and head of the parent of c (de-for a parser builder.
"First, as already implicit in noted as lv, tv, hp), the label of c&apos;s left sibling our discussion, factoring the probability compu- (lb for &quot;before&quot;), and the label of the grand-tation into a sequence of values corresponding parent of c (la)."
"That is, we wish to compute to various &apos;tfeatures&quot; suggests that the proba- p(t I l, lv, tv, lb,lg, by)."
We can now rewrite this bility model should be easily changeable -- just in the form of Equation 5 as follows: change the set of features used.
"This point is p(t I 1, Iv, tv, lb, IQ,hv) = emphasized by Ratnaparkhi in discussing his parser [17]."
"Second, and this is a point we have p(t l t)P(t l t, tv)"
"P(t l t, tv, tv) p(t l t, tp, tv, tb) not yet mentioned, the features used in these p(t l l) p(t l l, lp) p(t l t, tp,tp)"
P(t l t&apos;Iv&apos;tv&apos;Ib&apos;Ig)p(t l t&apos;Ip&apos;tv&apos;Ib&apos;Ig&apos;hP). (6) models need have no particular independence of one another.
"This is useful if one is using a log-p(t Iz, t,, t,, lb) p(t"
"It, l,, t,, lb, t,) linear model for smoothing."
"That is, suppose we want to compute a conditional probability Here we have sequentially conditioned on p(a ] b,c), but we are not sure that we have steadily increasing portions of c&apos;s history."
"In enough examples of the conditioning event b,c many cases this is clearly warranted."
"For ex-in the training corpus to ensure that the empiri- ample, it does not seem to make much sense cally obtained probability/~(a [ b, c) is accurate. to condition on, say, hv without first condition-"
The traditional way to handle this is also to ing on tp.
"In other cases, however, we seem to be conditioning on apples and oranges, so to speak."
"For example, one can well imagine that one might want to condition on the par-ent&apos;s lexical head without conditioning on the left sibling, or the grandparent label."
"One way to do this is to modify the simple version shown in Equation 6 to allow this: p(t Il, l., b, h,) = p(t t l)P(t l l, lv) P(t l l, lp, tv) P(t l l, lv, tp, lb) p(t i l) p(t l l,lp) p(t l l,lv,tv) p(t I l, lp, tp, p(t I l, t,,, p(t I l, lp, tp) p(t I l, tp, (7)"
Note the changes to the last three terms in Equation 7.
"Rather than conditioning each term on the previous ones, they are now condi-tioned only on those aspects of the history that seem most relevant."
"The hope is that by doing this we will have less difficulty with the splitting of conditioning events, and thus somewhat less difficulty with sparse data."
We make one more point on the connec-tion of Equation 7 to a maximum entropy for-mulation.
"Suppose we were, in fact, going to compute a true maximum entropy model based upon the features used in Equation 7, fl(t,l),f2(t,l, lp),f3(t,l, lv) ...."
"This requires finding the appropriate his for Equation 3, which is accomplished using an algorithm such as iterative scaling [11] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values."
"With no prior knowledge of values for the )q one traditionally starts with )~i = 0, this being a neutral assump-tion that the feature has neither a positive nor negative impact on the probability in question."
"With some prior knowledge, non-zero values can greatly speed up this process because fewer it-erations are required for convergence."
"We com-ment on this because in our example we can sub-stantially speed up the process by choosing val-ues picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7. (Our experience is that rather than requiring 50 or so iterations, three suffice.)"
"Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7."
The major advantage of using Equation 7 is that one can generally get away without com-puting the partition function Z(H).
"In the sim-ple (content-free) form (Equation 6), it is clear that Z(H) = 1."
"In the more interesting version, Equation 7, this is not true in general, but one would not expect it to differ much from one, and we assume that as long as we are not pub-lishing the raw probabilities (as we would be doing, for example, in publishing perplexity re-sults) the difference from one should be unim-portant."
"As partition-function calculation is typically the major on-line computational prob-lem for maximum-entropy models, this simpli-fies the model significantly."
"Naturally, the distributions required by Equation 7 cannot be used without smooth-ing."
"In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi&apos;s maximum-entropy parser [17]."
"While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].)"
"We created a parser based upon the maximum-entropy-inspired model of the last section, smoothed using standard deleted interpolation."
"As the generative model is top-down and we use a standard bottom-up best-first probabilis-tic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our prob-abilistic model."
"For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but con-ditioned only on standard PCFG information."
This allows the second pass to see expansions not present in the training corpus.
"We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words."
"We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probabil-ity that a given pre-terminal is realized using a previously unobserved word."
"As noted above, the probability model uses"
"In the previous sections we have concentrated on the relation of the parser to a maximum-entropy approach, the aspect of the parser that ous work five smoothed probability distributions, aspect is the sole or even the most important reason for its comparative success."
"Here we list what we believe to be the most significant con-one tributions and give some experimental results each for L~, M, Ri, t, and h."
The equation for on how well the program behaves without them. the (unsmoothed) conditional probability distri-
We take as our starting point the parser bution for t is given in Equation 7.
"The other labled[REF_CITE]in Figure 1 [5], as that is the four equations can be found in a longer version program from which our current parser derives. of this paper available on the author&apos;s website That parser, as stated in Figure 1, achieves an[URL_CITE]"
L and R are condi-average precision/recall of 87.5.
"As noted in [5], tioned on three previous labels so we are using that system is based upon a &quot;tree-bank gram-a third-order Markov grammar."
"Also, the label mar&quot; -- a grammar read directly off the train-of the parent constituent Ip is conditioned upon ing corpus."
This is as opposed to the &quot;Markov-even when it is not obviously related to the fur-grammar&quot; approach used in the current parser. ther conditioning events.
This is due to the im-
"Also, the earlier parser uses two techniques not portance of this factor in parsing, as noted in, employed in the current parser."
"First, it uses e.g., [14]."
"In keeping with the standard methodology a clustering scheme on words to give the sys- [5, tem a &quot;soft&quot; clustering of heads and sub-heads. 9,10,15,17], we used the Penn Wall Street Jour- (It is &quot;soft&quot; clustering in that a word can be-nal tree-bank [16] with sections 2-21 for train- long to more than one cluster with different ing, section 23 for testing, and section 24 for weights -- the weights express the probability development (debugging and tuning). of producing the word given that one is going"
Performance on the test corpus is measured to produce a word from that cluster.)
"Second, using the standard measures from [5,9,10,17]."
Without these enhance-centage of sentences with &lt; 2 cross brackets ments[REF_CITE]performs at the 86.6% level for (2CB).
"Again as standard, we take separate sentences of length &lt; 40. measurements for all sentences of length &lt;_ 40"
In this section we evaluate the effects of the and all sentences of length &lt; 100.
Note that various changes we have made by running var-the definitions of labeled precision and recall are ious versions of our current program.
To avoid those given in [9] and used in all of the previous repeated evaluations based upon the testing cor-work.
"As noted in [5], these definitions typically pus, here our evaluation is based upon sen-give results about 0.4% higher than the more tences oflength &lt; 40 from the development cor-obvious ones."
The results for the new parser pus.
"We note here that this corpus is somewhat as well as for the previous top-three individual more difficult than the &quot;official&quot; test corpus. parsers on this corpus are given in Figure For example, the final version of our system1. achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus."
"This is indicated in Figure 2, where the model la-beled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus."
This is in accord with our experience that development-corpus results are from 0.3% to 0.5% lower than those obtained on the test corpus.
The model labeled &quot;Old&quot; attempts to recreate the[REF_CITE]system using the current program.
"It makes no use of special maximum-entropy-inspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guess-ing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar."
This parser achieves an average precision/recall of 86.2%.
"This is consistent with the average pre-cision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the for-mer on the development corpus."
"Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser."
One of the first and without doubt the most signifi-cant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.
"As already noted,[REF_CITE]first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question."
"In contrast, the current parser first guesses the head&apos;s pre~terminal, then the head, and then the expansion."
"It turns out that usefulness of this process had a/ready been discovered by Collins [10], who in turn notes (personal communica-tion) that it was previously used by Eisner [12]."
"However, Collins in [10]does not stress the de-cision to guess the head&apos;s pre-terminal first, and it might be lost on the casual reader."
"Indeed, it was lost on the present author until he went back after the fact and found it there."
In Figure 2 we show that this one factor improves perfor-mance by nearly 2%.
"It may not be obvious why this should make so great a difference, since most words are ef-fectively unambiguous. (For example, part-of-speech tagging using the most probable pre-terminal for each word is 90% accurate [8].)"
We believe that two factors contribute to this per-formance gain.
"The first is simply that if we first guess the pre~terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t)."
"This quantity is a relatively intuitive one (as, for ex-ample, it is the quantity used in a PCFG to re-late words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probabil-ity upon which all smoothing of p(h) is based."
This one &apos;~fix&quot;makes slightly over a percent dif-ference in the results.
The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.
"For example, when we first guess the lexical head we can move from computing p(r I 1,lp, h) to p(r I l,t, lp, h)."
"So, e.g., even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not)~ the &quot;ng&quot; end-ing allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expan-sions can be considerable sharpened."
"For exam-ple, the tree-bank PCFG probability of the rule &quot;vp --+ vbg np&quot; is 0.0145, whereas once we con-dition on the fact that the lexical head is a vbg we get a probability of 0.214."
The second modification is the explicit mark-ing of noun and verb-phrase coordination.
We have already noted the importance of condition-ing on the parent label lv.
"So, for example, information about an np is conditioned on the parent -- e.g., an s, vp, pp, etc."
Note that when an np is part of an np coordinate structure the our best model uses a Markov-grammar ap-proach.
"As one can see in Figure 2, a first-order Markov grammar (with all the aforemen-tioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser."
"However, a second-order grammar does slightly"
"Figure 3: Verb phrase with both main and aux- better and a third-order grammar does signifi- iliary verbs parent will itself be an np, and similarly for cantly better than the tree-bank parser. 6 Conclusion a vp."
"But nps and vps can occur with np and We have presented a lexicalized Markov gram-vp parents in non-coordinate structures as well. mar parsing model that achieves (using the now For example, in the Penn Treebank a vp with standard training/testing/development sections both main and auxiliary verbs has the structure of the Penn treebank) an average preci-shown in Figure 3."
Note that the subordinate sion/recall of 91.1% on sentences of length &lt; vp has a vp parent. 40 and 89.5% on sentences of length &lt; 100.
Thus np and vp parents of constituents are This corresponds to an error reduction of 13% marked to indicate if the parents are a coor- over the best previously published single parser dinate structure.
"A vp coordinate structure results on this test set, those of Collins [9]. is defined here as a constituent with two or That the previous three best parsers on this more vp children, one or more of the con- test [5,9,17] all perform within a percentage stituents comma, cc, conjp (conjunctive phrase), point of each other, despite quite different ba-and nothing else; coordinate np phrases are de- sic mechanisms, led some researchers to won-fined similarly."
Something very much like this is der if there might be some maximum level of done in [15].
"As shown in Figure 2, condition- parsing performance that could be obtained us-ing on this information gives a 0.6% improve- ing the treebank for training, and to conjec-ment."
We believe that this is mostly due to ture that perhaps we were at it.
The results improvements in guessing the sub-constituent&apos;s reported here disprove this conjecture.
The re-pre-terminai and head.
"Given we are already sults of [13] achieved by combining the afore-at the 88% level of accuracy, we judge a 0.6% mentioned three-best parsers also suggest that improvement to be very much worth while. the limit on tree-bank trained parsers is much"
Next we add the less obvious conditioning higher than previously thought.
"Indeed, it may events noted in our previous discussion of the be that adding this new parser to the mix may final model -- grandparent label Ia and left yield still higher results. sibling label lb."
"From our perspective, perhaps the two mostWhen we do so using our maximum-entropy-inspired conditioning, we get important numbers to come out of this re-another 0.45% improvement in average preci- search are the overall error reduction of 13% sion/recall, as indicated in Figure 2 on the line over the results in [9] and the intermediate-labeled &quot;MaocEnt-Inspired&apos;."
Note that we also result improvement of nearly 2% on labeled pre-tried including this information using a stan- cision/recall due to the simple idea of guess-dard deleted-interpolation model.
The results ing the bead&apos;s pre-terminal before guessing the here are shown in the line &quot;Standard Interpola- head.
Neither of these results were anticipated tion&quot;.
"Including this information within a stan- at the start of this research. dard deleted-interpolation model causes a 0.6% As noted above, the main methodological decrease from the results using the less conven- innovation presented here is our &quot;maximum-tional model."
"Indeed, the resulting performance entropy-inspired&quot; model for conditioning and is worse than not using this information at all. smoothing."
Two aspects of this model deserve Up to this point all the models considered some comment.
"The first is the slight, but im-in this section are tree-bank grammar models. portant, improvement achieved by using this That is, the PCFG grammar rules are read di- model over conventional deleted interpolation, rectly offthe training corpus."
"As already noted, as indicated in Figure 2."
"We expect that as we experiment with other, more semantic con-ditioning information, the importance of this as-pect of the model will increase."
"More important in our eyes, though, is the flexibility of the maximum-entropy-inspired model."
"Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing."
"Ultimately it is this flexibility that let us try the various condi-tioning events, to move on to a Markov gram-mar approach, and to try several Markov gram-mars of different orders, without significant pro-gramming."
"Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for pars-ing down to more semantic levels of detail."
It is to this project that our future parsing work will be devoted.
We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.
The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL).
"The error-recognition system, ALEK, performs with about 80% precision and 20% recall."
Introduction A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence[REF_CITE].
Much information about usage can be obtained from quite a limited context:[REF_CITE]found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it.
Statistically-based computer programs have been able to do the same with a high level of accuracy[REF_CITE].
The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word.
"We have developed a statistical system, ALEK (Assessing Le____xicalKnowledge), that uses statistical analysis for this purpose."
What kinds of anomalous elements does ALEK identify?
"Writers sometimes produce errors that violate basic principles of English syntax (e.g., a desks), while other mistakes show a lack of information about a specific vocabulary item (e.g., a knowledge)."
"In order to detect these two types of problems, ALEK uses a 30-million word general corpus of English from the San Jose Mercury News (hereafter referred to as the general corpus) and, for each target word, a set of 10,000 example sentences from North American newspaper textI (hereafter referred to as the word-specific corpus). i The corpora are extracted from the ACL-DCI corpora."
In selecting the sentences for the word
ALEK infers negative evidence from the contextual cues that do not co-occur with the target word - either in the word specific corpus or in the general English one.
It uses two kinds of contextual cues in a +2 word window around the target word: function words (closed-class items) and part-of-speech tags[REF_CITE].
"The Brill tagger output is post-processed to &quot;enrich&quot; some closed class categories of its tag set, such as subject versus object pronoun and definite versus indefinite determiner."
The enriched tags were adapted from Francis and Ku~era (I 982).
"After the sentences have been preprocessed, ALEK counts sequences of adjacent part-of-speech tags and function words (such as determiners, prepositions, and conjunctions)."
"For example, the sequence a/ATfull-time/JJjob/NN contributes one occurrence each to the bigrams"
"AT+JJ, JJ+NN,a+JJ, and to the part-of-speech tag trigram AT+JJ+NN."
Each individual tag and function word also contributes to its own unigram count.
These frequencies form the basis for the error detection measures.
"From the general corpus, ALEK computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are, therefore, likely to be ungrammatical in English (e.g., singular determiner preceding plural noun, as in *a desks)."
Mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent.
Here we use this measure for the opposite purpose - to find combinations that occur less often than expected.
"ALEK also looks for sequences that are common in general but unusual in the word specific corpus (e.g., the knowledge)."
These divergences between the two corpora reflect syntactic properties that are peculiar to the target word.
"The system computes mutual information comparing the proportion of observed occurrences of bigrams in the general corpus to the proportion expected based on the assumption of independence, as shown below:"
"Here, P(AB) is the probability of the occurrence of the AB bigram, estimated from its frequency in the general corpus, and P(A) and P(B) are the probabilities of the first and second elements of the bigram, also estimated from the general corpus."
Ungrammatical sequences should produce bigram probabilities that are much smaller than the product of the unigram probabilities (the value of MI will be negative).
"Trigram sequences are also used, but in this case the mutual information computation compares the co-occurrence of ABC to a model in which A and C are assumed to be conditionally independent given B (see[REF_CITE])."
M/= log2 P( B) x P(A IB) x P(C IB)
"Once again, a negative value is often indicative of a sequence that violates a rule of English. 2.2 Comparing the word-specific corpus to the general corpus:"
ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus.
The measures for bigrams and trigrams are similar to those given above except that the probability in the numerator is estimated from the word-specific corpus and the probabilities in the denominator come from the general corpus.
"To return to a previous example, the phrase a knowledge contains the tag bigram for singular determiner followed by singular noun (ATNil)."
This sequence is much less common in the word-specific corpus for knowledge than would be expected from the general corpus unigram probabilities of AT and NN.
"In addition to bigram and trigram measures, ALEK compares the target word&apos;s part-of-speech tag in the word-specific corpus and in the general corpus."
"Specifically, it looks at the conditional probability of the part-of-speech tag given the major syntactic category (e.g., plural noun given noun) in both distributions, by computing the following value. ( P=p~c~c_corm(taglcategory)"
"I o g 2 / ~ ~ t. p=o,e,o, _ co, =(tag I )"
"For example, in the general corpus, about half of all noun tokens are plural, but in the training set for the noun knowledge, the plural knowledges occurs rarely, if at all."
"The mutual information measures provide candidate errors, but this approach overgenerates - it finds rare, but still quite grammatical, sequences."
"To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times."
This increases ALEK&apos;s precision at the price of reduced recall.
"For example, a knowledge will not be treated as an error because it appears in the training corpus as part of the longer a knowledge of sequence (as in a knowledge of mathematics)."
ALEK also uses another statistical technique for finding rare and possibly ungrammatical tag and function word bigrams by computing the %2 (chi square) statistic for the difference between the bigram proportions found in the word-specific and in the general corpus: ~Pgeneral_corpu~i -egerneral_corpus) /
The %2measure faces the same problem of overgenerating errors.
"Due to the large sample sizes, extreme values can be obtained even though effect size may be minuscule."
"To reduce false positives, ALEK requires that effect sizes be at least in the moderate-to-small range[REF_CITE]."
Direct evidence from the word specific corpus can also be used to control the overgeneration of errors.
"For each candidate error, ALEK compares the larger context in which the bigram appears to the contexts that have been analyzed in the word-specific corpus."
"From the word-specific corpus, ALEK forms templates, sequences of words and tags that represent the local context of the target."
"If a test sentence contains a low probability bigram (as measured by the X2test), the local context of the target is compared to all the templates of which it is a part."
"Exceptions to the error, that is longer grammatical sequences that contain rare sub-sequences, are found by examining conditional probabilities."
"To illustrate this, consider the example of a knowledge and a knowledge of."
"The conditional probability of of given a knowledge is high, as it accounts for almost all of the occurrences of a knowledge in the word-specific corpus."
"Based on this high conditional probability, the system will use the template for a knowledge of to keep it from being marked as an error."
"Other function words and tags in the +1 position have much lower conditional probability, so for example, a knowledge is will not be treated as an exception to the error."
"TOEFL essays are graded on a 6 point scale, where 6 demonstrates &quot;clear competence&quot; in writing on rhetorical and syntactic levels and 1 demonstrates &quot;incompetence in writing&quot;."
"If low probability n-grams signal grammatical errors, then we would expect TOEFL essays that received lower scores to have more of these n-grams."
"To test this prediction, we randomly selected from the[REF_CITE]essays for each of the 6 score values from 1.0 to 6.0."
"Score % of bigrams % Of trigrams each score value, all 50 essays were concatenated to form a super-essay."
"In every super-essay, for each adjacent pair and triple of tags containing a noun, verb, or adjective, the bigram and trigram mutual information values were computed based on the general corpus."
Table 1 shows the proportions ofbigrams and trigrams with mutual information less than -3.60.
"As predicted, there is a significant negative correlation between the score and the proportion of low probability bigrams (rs= -.94, n=6, p&lt;.01, two-tailed) and trigrams (r~= -.84, n=6, p&lt;.05, two-tailed)."
"ALEK was developed using three target words that were extracted from TOEFL essays: concentrate, interest, and knowledge."
These words were chosen because they represent different parts of speech and varying degrees of polysemy.
Each also occurred in at least 150 sentences in what was then a small pool of TOEFL essays.
"Before development began, each occurrence of these words was manually labeled as an appropriate or inappropriate usage -without taking into account grammatical errors that might have been present elsewhere in the sentence but which were not within the target word&apos;s scope."
Critical values for the statistical measures were set during this development phase.
The settings were based empirically on ALEK&apos;s performance so as to optimize precision and recall on the three development words.
Candidate errors were those local context sequences that produced a mutual information value of less than -3.60 based on the general corpus; mutual information of less than -5.00 for the specific/general comparisons; or a X2value greater than 12.82 with an effect size greater than 0.30.
Precision and recall for the three words are shown below.
Target word n Precisi[REF_CITE].875 .280[REF_CITE].840 .330[REF_CITE].918 .570
Test Word Precision Recall Total Recall Test Word Precision Recall Total Recall
ALEK was tested on 20 words.
"These words were randomly selected from those which met two criteria: (1) They appear in a university word list (&apos;[REF_CITE]) as words that a student in a US university will be expected to encounter and (2) there were at least 1,000 sentences containing the word in the TOEFL essay pool."
"To build the usage model for each target word, 10,000 sentences containing it were extracted from the North American News Corpus."
Preprocessing included detecting sentence boundaries and part-of-speech tagging.
"As in the development system, the model of general English was based on bigram and trigram frequencies of function words and part-of-speech tags from 30-million words of the San Jose Mercury News."
"For each test word, all of the test sentences were marked by ALEK as either containing an error development, marked each usage of the target word as incorrect or correct and in the case of incorrect usage indicated how far from the target one would have to look in order to recognise that there was an error."
"For example, in the case of &quot;an period&quot; the error occurs at a distance of one word fromperiod."
"When the error is an omission, as in &quot;lived in Victorian period&quot;, the distance is where the missing word should have appeared."
"In this case, the missing determiner is 2 positions away from the target."
"When more than one error occurred, the distance of the one closest to the target was marked."
Table 3 lists the precision and recall for the 20 test words.
The column labelled &quot;Recall&quot; is the proportion of human-judged errors in the 250-sentence sample that were detected by ALEK. &quot;Total Recall&quot; is an estimate that extrapolates from the human judgements of the sample to the entire test set.
We illustrate this with the results for pollution.
The human judge marked as
"To evaluate the system, for each test word we randomly extracted 125 sentences that ALEK classified as containing no error (C-set) and 125 sentences which it labeled as containing an error (E-set)."
"The linguist, who had no part in ALEK&apos;s"
"To estimate overall incorrect usage, we computed a weighted mean of these two rates, where the weights reflected the proportion of sentences that were in the E-set and C-set."
The E-set contained 8.3% of the pollution sentences and the C-set had the remaining 91.7%.
"With the human judgements as the gold standard, the estimated overall rate of incorrect usage is (.083 x .912 + .917 x .184) = .245."
"ALEK&apos;s estimated recall is the proportion of sentences in the E-set times its precision, divided by the overall estimated error rate (.083 × .912) / .245 = .310."
The precision results vary from word to word.
Conclusion and pollution have precision in the low to middle 90&apos;s while individual&apos;s precision is 57%.
"Overall, ALEK&apos;s predictions are about 78% accurate."
"The recall is limited in part by the fact that the system only looks at syntactic information, while many of the errors are semantic."
ALEK recognizes all of these types of errors.
"For closed class words, ALEK identified whether a word was missing, the wrong word was used (choice), and when an extra word was used."
"Open class words have a fourth error category, form, including inappropriate compounding and verb agreement."
"During the development stage, we found it useful to add additional error categories."
"Since TEOFL graders are not supposed to take punctuation into account, punctuation errors were only marked when they caused the judge to &quot;garden path&quot; or initially misinterpret the sentence."
"Spelling was marked either when a function word was misspelled, causing part-of-speech tagging errors, or when the writer&apos;s intent was unclear."
"The distributions of categories for hits and misses, shown in Table 4, are not strikingly different."
"However, the hits are primarily syntactic in nature while the misses are both semantic (as in open-class:choice) and syntactic (as in closed-class:missing)."
"ALEK is sensitive to open-class word confusions (affect vs effect) where the part of speech differs or where the target word is confused with another word (*ln this aspect,... instead ofln this respect, ...)."
"In both cases, the system recognizes that the target is in the wrong syntactic environment."
Misses can also be syntactic - when the target word is confused with another word but the syntactic environment fails to trigger an error.
"In addition, ALEK does not recognize semantic errors when the error involves the misuse of an open-class word in"
"Category % Hits % Misses combination with the target (for example, make in &quot;*they make benefits&quot;)."
Closed class words typically are either selected by or agree with a head word.
"So why are there so many misses, especially with prepositions?"
The problem is caused in part by polysemy -when one sense of the word selects a preposition that another sense does not.
"When concentrate is used spatially, it selects the preposition in, as &quot;the stores were concentrated in the downtown area&quot;."
"When it denotes mental activity, it selects the preposition on, as in &quot;Susan concentrated on her studies&quot;."
"Since ALEK trains on all senses of concentrate, it does not detect the error in &quot;*Susan concentrated in her studies&quot;."
"Another cause is that adjuncts, especially temporal and locative adverbials, distribute freely in the word-specific corpora, as in &quot;Susan concentrated in her room.&quot; This second problem is more tractable than the polysemy problem - and would involve training the system to recognize certain types of adjuncts."
"False positives, when ALEK &quot;identifies&quot; an error where none exists, fall into six major categories."
The percentage of each false positive type in a random sample of 200 false positives is shown in Table 5.
Domain mismatch: Mismatch of the newspaper-domain word-specific corpora and essay-domain test corpus.
One notable difference is that some TOEFL essay prompts call for the writer&apos;s opinion.
"TOEFL essays often contain first person references, whereas newspaper articles are written in the third person."
We need to supplement the word-specific corpora with material that more closely resembles the test corpus.
Tagger: Incorrect analysis by the part-of-speech tagger.
"When the part-of-speech tag is wrong,"
ALEK often recognizes the resulting n-gram as anomalous.
Many of these errors are caused by training on the Brown corpus instead of a corpus of essays.
"Syntactic analysis: Errors resulting from using part-of-speech tags instead of supertags or a full parse, which would give syntactic relations between constituents."
"For example, ALEK false alarms on arguments of ditransitive verbs such as offer and flags as an error &quot;you benefits&quot; in &quot;offers you benefits&quot;."
"Free distribution: Elements that distribute freely, such as adverbs and conjunctions, as well as temporal and locative adverbial phrases, tend to be identified as errors when they occur in some positions."
Punctuation: Most notably omission of periods and commas.
"Since these errors are not indicative of one&apos;s ability to use the target word, they were not considered as errors unless they caused the judge to misanalyze the sentence."
"An undesirable result of our &quot;enriched&quot; tag set is that some tags, e.g., the post-determiner last, occur too infrequently in the corpora to provide reliable statistics."
Solutions to some of these problems will clearly be more tractable than to others.
Comparison of these results to those of other systems is difficult because there is no generally accepted test set or performance baseline.
"Given this limitation, we compared ALEK&apos;s performance to a widely used grammar checker, the one incorporated in Microsoft&apos;s[REF_CITE]."
"We created files of sentences used for the three developmentwords concentrate, interest, and knowledge, and manually corrected any errors outside the local context around the target before checking them[REF_CITE]."
The performance for concentrate showed overall precision of 0.89 and recall of 0.07.
"For interest, precision was 0.85 with recall of 0.11."
"In sentences containing knowledge, precision was 0.99 and recall was 0.30."
"In summary,[REF_CITE]&apos;s precision in error detection is impressive, but the lower recall values indicate that it is responding to fewer error types than does ALEK."
"In particular,[REF_CITE]is not sensitive to inappropriate selection of prepositions for these three words (e.g., *have knowledge on history, *to concentrate at science)."
"Of course,[REF_CITE]detects many kinds of errors that ALEK does not."
Research has been reported on grammar checkers specifically designed for an ESL population.
"These have been developed by hand, based on small training and test sets."
"This system was tested on 79 sentences containing determiner and agreement errors, and 101 grammatical sentences."
We calculate that their precision was 78% with 54% recall.
"Park,[REF_CITE]adapted a categorial grammar to recognize &quot;classes of errors [that] dominate&quot; in the nine essays they inspected."
"This system was tested on eight essays, but precision and recall figures are not reported."
The unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text.
Preliminary results indicate that ALEK&apos;s error detection is predictive of TOEFL scores.
"If ALEK accurately detects usage errors, then it should report more errors in essays with lower scores than in those with higher scores."
"We have already seen in Table 1 that there is a negative correlation between essay score and two of ALEK&apos;s component measures, the general corpus n-grams."
"However, the data in Table 1 were not based on specific vocabulary items and do not reflect overall system performance, which includes the other measures as well."
Table 6 shows the proportion of test word occurrences that were classified by ALEK as containing errors within two positions of the target at each of 6 TOEFL score points.
"As predicted, the correlation is negative (rs = -1.00, n = 6, p &lt; .001, two-tailed)."
"These data support the validity of the system as a detector of inappropriate usage, even when only a limited number of words are targeted and only the immediate context of each target is examined."
"For comparison, Table 6 also gives the estimated proportions of inappropriate usage by score point based on the human judge&apos;s classification."
"Here, too, there is a negative correlation: rs = -.90, n = 5, p &lt; .05, two-tailed."
"Although the system recognizes a wide range of error types, as Table 6 shows, it detects only about one-fifth as many errors as a human judge does."
"To improve recall, research needs to focus on the areas identified in section 3.2 and, to improve precision, efforts should be directed at reducing the false positives described in 3.3."
ALEK is being developed as a diagnostic tool for students who are learning English as a foreign language.
"However, its techniques could be incorporated into a grammar checker for native speakers."
We present a method for automatically detect-ing errors in a manually marked corpus us-
We evaluate this method over the part of ing anomaly detection.
Anomaly detection is speech tagged portion of the Penn Treebank cor-a method for determining which elements of a pus[REF_CITE].
"In one experiment, large data set do not conform to the whole. our method detected 1000 anomalies within a This method fits a probability distribution over data set of 1.25 million tagged elements."
Human the data and applies a statistical test to detect judges evaluated the results of the application anomalous elements.
"In the corpus error detec- of this method and verified that 69% of iden-tion problem, anomalous elements are typically tified anomalies are in fact tagging errors."
In marking errors.
"We present the results of ap- another experiment, our method detected 4000 plying this method to the tagged portion of the anomalies of which 44% are tagging errors."
Penn Treebank corpus.
The tagged portion of the Penn Treebank
Manually marking corpora is a time consuming has been extensively utilized for construction and expensive process.
The process is subject to and evaluation of taggers.
"This includes human error by the experts doing the marking. transformation-based tagging ([REF_CITE]; Brill Unfortunately, many natural language process- and[REF_CITE])."
Weischedelet al. (1993) applied ing methods are sensitive to these errors.
Markov Models to tagging.
Ad-several experts pass over the corpus to ensure wait[REF_CITE]estimates a probabil-consistency.
For large corpora this can be a ity distribution for tagging using a maximum tremendous expense. entropy approach.
"In this paper, we propose a method for au-"
"Regarding error detection in corpora, Rat-tomatically detecting errors in a marked cor- naparkhi (1996) discusses inconsistencies in pus using an anomaly detection technique."
This the Penn Treebank and relates them to inter-technique detects anomalies or elements which annotator differences in tagging style.
"Abney, do not fit in with the rest of the corpus."
Much related work to the anomaly detection
"To detect the anomalies, we first compute a problem stems from the field of statistics in probability distribution over the entire corpus. the study of outliers."
This work examines de-
Then we apply a statistical test which identi- tecting and dealing with outliers in univariate fies which elements are anomalies.
"In this case data, multivariate data, and structured data the anomalies are the elements with very low where the probability distribution over the data likelihood."
These elements are marked as errors is given a priori.
Statistics gives a set of discor-and are thrown out of the corpus.
The model is dancy tests which can be applied to any given recomputed on the remaining elements.
"At con- element in the dataset to determine whether it clusion, we are left with two data sets: one the is an outlier."
A survey of outliers in statistics is given[REF_CITE].
Anomaly detection is extensively used within the field of computer security specifically in in-trusion detecti[REF_CITE].
Typically anomaly detection methods are applied to de-tect attacks by comparing the activity during an attack to the activity under normal use[REF_CITE].
The method used in this paper is based on a method for anomaly detection which detects anomalies in noisy data[REF_CITE].
The sparse Markov transducer probability modeling method is an extension of adaptive mixtures of probabilistic transducers[REF_CITE].
"Naive Bayes learning, which is used to estimate probabilities in this paper, is described[REF_CITE]."
"More formally, anomaly detection is the process of determining when an element of data is an outlier."
"Given a set of training data without a probability distribution, we want to construct an automatic method for detecting anomalies."
We are interested in detecting anomalies for two main reasons.
"One, we are interested in model-ing the data and the anomalies can contaminate the model."
"And two, the anomalies themselves can be of interest as they may show rarely oc-curring events."
"For the purposes of this work, we axe most interested in identifying mistagged elements, i.e. the second case."
"In order to motivate a method for detect-ing anomalies, we must first make assumptions about how the anomalies occur in the data."
"We use a &quot;mixture model&quot; for explaining the pres-ence of anomalies, one of several popular models in statistics for explaining outliers[REF_CITE]."
"In the mixture model, there are two probability distributions which generate the data."
An element xi is either generated from the majority distribution or with (small) probabil-ity A from an alternate (anomalous) distribu-tion.
"Our distribution for the data, D, is then:"
D -- (1 - A)M +
"AA (I) where M is the majority distribution, and A is the anomalous distribution."
The mixture framework for explaining anomalies is indepen-dent of the properties of the distributions M and A.
"In other words, no assumptions about the nature of the probability distributions are necessary."
"The specific probability distribu-tions, M and A, are chosen based on prior knowledge of the problem."
"Typically M is a structured distribution which is estimated over the data using a machine learning technique, while A is a uniform (random) distribution rep-resenting elements which do not fit into M."
"In the corpus error detection problem, we are assuming that for each tag in the corpus with probability (1 - A) the human annotator markes the corpus with the correct tag and with prob-ability A the human annotator makes an error."
"In the case of an error, we assume that the tag is chosen at random."
"Detecting anomalies, in this framework, is equivalent to determining which elements were generated by the distribution A and which ele-ments were generated by distribution M. Ele-ments generated by A are anomalies, while ele-ments generated by M are not."
"In our case, we have probability distributions associated with the distributions M and A, PM and PA respec-tively."
"The algorithm partitions the data into two sets, the normal elements M and the anomalies A. For each element, we make a determination of whether it is an anomaly and should be in-cluded in A or a majority element in which it should be included in M. We measure the like-lihood of the distribution under both cases to make this determination."
"The likelihood, L, of distribution D with probability function P over elements Xl,...,XN is defined as follows:"
N L(D) = l&apos;I PD(Xi) = (2) i-----1
"Since the product of small numbers is difficult to compute, we instead compute the log likeli-hood, LL."
The log likelihood for our case is:
IMI log(1 - A) + ~[] log(PM(xi)) xiEM +lAIlogA + ~ log(PA(xj)) (3) xj EA
In order to determine which elements are conditional on a finite set of inputs.
"A Markov anomalies, we use a general principal for deter- transducer of order L is the conditional proba-mining outliers in multivariate data (Barnett, bility distribution of the form: 1."
The tag of the current word Ti. 2.
The current word Wi. 3.
The previous tag ~-I. 4.
The next tag 7~+1.
"Over records containing these 4 elements, compute our probability distributions."
ti = t- ~=lnJ- (i- 1).
The goal of the sparse Markov transducer estimation algorithm is to estimate a conditional probability of this form based upon a set of inputs and their cor-responding outputs.
"However, the task is com-plicated due to the lack of knowledge a priori of which inputs the probability distribution is conditional on. we Intuitively, a fixed order Markov Chain of or-der L is equivalent to a n-gram with n = L."
"In a variable order Markov Chain, the value of"
The anomaly detection framework is indepen- n changes depending on the context.
"For ex- dent of specific probability distributions. ample, some elements in the data may use aDif- ferent probability distributions have different bigram, while others may use a trigram."
"Since the anomaly detection frame- sparse Markov transducer uses a weighted sum work does not depend on a specific probability of n-grams for different values of n and these distribution, we can choose the probability dis- weights depend on the context."
"In addition the tribution to best model the data based on our weighted sum is over not only n-grams, but also intuitions about the problem. n-grams with wild cards such as a trigram where"
"To illustrate this, we perform two sets of ex- only the first and last element is conditioned on. periments, each using a different probability dis- In this case we are &apos;looking at the input se-tribution modeling method."
"The first set of quence of the current word, Wt, the previous experiments uses sparse Markov transducers as tag, Tt-1, and the next tag, Tt+l."
"The out-the probability modeling method, while the sec- put is the set of all possible tags."
The models ond uses a simple naive Bayes method.
"that are in the weighted sum are the trigram, WtTt-lTt+l; the bigrams WtTt-1, WtTt+l and Tt-lTt+l; and the unigrams Wt, Tt-1 and Tt+l."
Sparse Markov transducers compute probabilis-
The specific weights of each model depends on tic mappings over sparse data.
"A Markov trans- the context or the actual values of Wt, Tt-1, and ducer is defined to be a probability distribution Tt+l."
Sparse Markov transducers depend on a set of prior probabilities that incorporate prior knowl-edge about the importance of various elements in the input sequence.
These prior probabilities are set based on the problem.
"For this problem, we use the priors to encode the information that the current word, Wt, is very important in de-termining the part of speech."
Each model in the weighted sum uses a pseudo-count predictor.
This predictor com-putes the probability of an output (tag) by the number of times that a specific output was seen in a given context.
"In order to avoid probabil-ities of 0, we assume that we have seen each output at least once in every context."
"In fact, these predictors can be any probability distri-bution which can also depend on what works best for the task."
The probability distribution for the tags was also estimated using a straight forward naive Bayes approach.
"We are interested in the probability of a tag, given the current word, the previous tag, and the next tag, or the probability distribution P(TiIWi, Ti-t, ~+1) which using Bayes Rule is equivalent to:"
"P(Ti}Wi, Ti-l, Ti+l ) = P(Wi, Ti-I, Ti+zlTi) * P(Ti) P(Wi, Ti-,,Ti+I) (6)"
If we make the Naive Bayes independence as-sumption and we assume that the denominator is constant for all values this reduces to:
"P(~IW~, ~-1, Ti+,) = P(WiIT~) * P(T~-IIT~) * P(Ti+zlTi) * P(Ti) (7) C where C is a normalization constant in order to have the probabilities sum to 1."
Each of the val-ues on the right side of the equation can easily be computed over the data estimating a proba-bility distribution.
Each probability distribution was trained over each record giving a model over the entire data.
The probability model is then used to deter-mine whether or not an element is an anomaly by applying the test in equation (3).
Typi-cally this can be done in an efficient manner because the approach does not require reesti-mating the model over the entire data set.
"If an element is designated as an anomaly, we remove it from the set of normal elements andefficiently reestimate the probability distribution to obtain more anomalous elements."
The method was applied to the Penn Tree-bank corpus and a set of anomalies were gen-erated.
These anomalies were evaluated by hu-man judges to determine if they are in fact tag-ging errors in the corpus.
The human judges were natural language processing researchers (not the author) familiar with the Penn Tree-bank markings.
"In the experiments involving the sparse Markov transducers, after applying the method, 7055 anomalies were detected."
"In the ex-periments involving the naive Bayes learning method, 6213 anomalies were detected."
Sample output from the system is shown in figure 1.
The error is shown in the context marked with !!!.
The likelihood of the tag is also given which is extremely low for the errors.
The system also outputs a suggested tag and its likelihood which is the tag with the highest likelihood for that context.
"As we can see, these errors are clearly annotation errors."
"Since the anomalies detected from the two probability modeling methods differed only slightly, we performed human judge verification of the errors over only the results of the sparse Markov transducer experiments."
The anomalies were ordered based on their likelihood.
"Using this ranking, the set of anoma-lies were broken up into sets of 1000 records."
We examined the first 4000 elements by randomly selecting 100 elements out of each 1000.
Human judges were presented with the sys-tem output for four sets of 100 anomalies.
The judges were asked to choose among three op-tions for each example: 1.
The tag in the corpus sen-tence is incorrect. 2.
Unsure - The judge is unsure whether or not the corpus tag is correct.
Its/PRP$ fast-food NN restaurants/NNS -/:/ including/VBG
"Denny/NNP &apos;s/eOS ,/, Hardee/Nie &apos;s/POS ,/, Quincy/NNP &apos;s/POS and/CC"
"E1/NNP Pollo/NNP Loco/NNP (/( &quot;/&quot; ! ! the/NN!!! only/JJ significant/JJ fast-food/NN! chain/NN to/TO specialize/VB in/IN char-broiled/JJ chicken/NN &quot;/&quot; )/) -/: are/VBP stable/JJ ,/, recession-resistant/JJ and/CC growing/VBG ./."
Suggested Tag: DT (0.998262)
Error 0.019231: Not/RB even/RB Jack/NNP Lemmon/NNP &apos;s/POS expert/JJ doddering/JJ !!! makes/NNS!!! this/DT trip/NN worth/NN taking/VBG ./.
Suggested Tag: VBZ (0.724359)
Error 0.014286: It/PRP also/RB underscores/VBZ the/DT difficult/JJ task/NN ahead/RB as/IN !!!
Coors/NNS!!! attempts/VBZ to/TO purchase/VB Stroh/NNP Brew-ery/NNP Co./NNP and/CC fight/VB off/RP increasingly/RB tough/JJ competition/NN from/IN Anheuser-Busch/NNP Cos/NNP ./.
Suggested Tag: NNP (0.414286)
Figure 1: Sample output of anomalies in Penn Treebank corpus.
"The errors are marked with !!!. 3. System Error - The tag in the corpus sen- of this analysis, if we attempted to automati-tence is correct and the system incorrectly cally correct the first 1000 examples where the marked it as an error. error rate was 69%, this method would have led to a reduction of the total number of errors in"
The &quot;unsure&quot; choice was allowed because of the the corpus by 245. inherent subtleties in differentiating between types of tags such as &quot;VB vs. VBP&quot; or &quot;VBD 5 Conclusion vs. VBN&quot;.
The cor- detection techniques.
"As shown, the anomalies pus error rate was computed by throwing out detected in the Penn Treebank corpus tend to the unsure cases and computing:"
Corpus error rate = Corpus Errors System Errors + Corpus Errors be tagging errors.
This method has some inherent limitations (8) because not all errors in the corpus would mani-fest themselves as anomalies.
"In infrequent con-texts or ambiguous situations, the method may not have enough information to detect an error."
The total corpus error rate over the 400 manu-
"In addition, if there are inconsistencies between ally checked examples was was 44%."
"As can be annotators, the method would not detect the seen, many of the anomalies are in fact errors errors because the errors would be manifested in the corpus. over a significant portion of the corpus."
"For each error, we asked the human judge to Although this paper presents a fully au-determine if the correct tag is the systems sug- tomatic method for error detection in cor-gested tag."
"Out of the total 158 corpus errors, pora, this method can also be used as a semi-the systems correct tag would have corrected automatic method for correcting errors."
The the error in 145 cases. method can guide an annotator to the elements
"Since the verified examples were random, we which are most likely errors."
The method can can assume that 91% of corpus errors would be greatly reduce the number of elements that an automatically corrected if the system would re- annotator needs to examine.
The method is very sensitive to the effective-ness of the probability model in modeling the normal elements.
Extensions to the probabil-ity distributions presented here such as adding information about endings of words or using more features could increase the accuracy of the probability distribution and the overall perfor-mance of the anomaly detection system.
Other future work involves applying this method to other marked corpora.
"In contrast, there is essentially no CF compo-"
"The ambiguity inherent in natural language means nent in systems which directly interpret HPSGgram-that during parsing, some segments of the input mars."
"Although HPSG feature structures are typed, string may end up being analysed as the same type an initial CF category equality test cannot be im-of linguistic object in several different ways."
"Each plemented straightforwardly in terms of the top-of these different ways must be recorded, but subse- level types of feature structures since two compat-quent parsing steps must treat the set of analyses as ible types need not be equal, but could stand in a single entity, otherwise the computation becomes a subtype-supertype relationship."
"In addition, the theoretically intractable."
"Earley&apos;s algorithm (Ear- feature structure subsumption test is potentially ex-ley, 1970), for example, avoids duplication of parse pensive since feature structures are large, typically items by maintaining pointers to alternative deriva- containing hundreds of nodes."
It is therefore an open tions in association with the item.
"This process question whether parsing systems using grammars of has been termed &apos;local ambiguity packing&apos; (Tomita, this type can gain any advantage from local ambi- 1985), and the structure built up by the parser, a guity packing. &apos;parse forest&apos; (Billot &amp;:[REF_CITE])."
"Context free The question is becoming increasingly impor- (CF) grammars represent linguistic objects in terms tant, though, as wide-coverage HPSGgrammars are of atomic category symbols."
"The test for duplicate starting to be deployed in practical applications--parse items--and thus being able to pack the sub- for example for &apos;deep&apos; analysis in the VerbMo-analyses associated with them--is equality of cate- bil speech-to-speech translation system (Wahlster, gory symbols."
"In the final parse forest every differ- 1997; Kiefer, Krieger, Carroll, &amp;[REF_CITE]).1"
"In ent combination of packed nodes induces a distinct, this paper we answer the question by demonstrating valid parse tree. that (a) subsumption- and equivalence-based feature Most existing unification-based parsing systems structure packing is applicable to large HPSGgram-either implicitly or explicitly contain a context-free mars, and (b) average complexity and time taken core."
"For example, in the CLE[REF_CITE]for the parsing task can be greatly reduced."
"In the (manually-assigned)functors ofthe Prolog terms Section 2 we present a new, linear-time, bidirec-forming the categories constitute a CF &apos;backbone&apos;."
"In the Alvey Tools system[REF_CITE]each dis- 1A significantbody of work on efficientprocessingwith such grammarshas been buildingup recently,with investi-tinct set of features is automatically given a unique gations into efficientfeature structure operations, abstract-identifier and this is associated with every category machine-basedcompilation,CF backbonecomputation,and containing those features."
"The packing technique finite-stateapproximationofHPSGderivations,amongstoth-has been shown to work well in practice in these ers (Flickinger,Oepen,Uszkoreit,&amp;[REF_CITE]). tional subsumption test for typed feature structures, dags in step."
"First, it checks whether the current which we use in a bottom-up, chart-based parsing node in either dag is involved in a reentrancy that algorithm incorporating novel, efficient accounting is not present in the other: for each node visited mechanisms to guarantee minimal chart size (Sec- in one dag it adds a temporary pointer (held in the tion 3)."
"We present a full-scale evaluation of the &apos;copy&apos; slot) to the corresponding node in the other techniques on a large corpus (Section 4), and com- dag."
"If a node is reached that already has a pointer plete the picture with an empirically-based discus- then this is a point of reentrancy in the dag, and sion of grammar restrictors and parsing strategies if the pointer is not identical to the other dag node (Section 5). then this reentrancy is not present in the other dag."
In this case the possibility that the former dag sub- 2 sumes the latter is ruled out.
After the reentrancyEfficient Subsumption and Equivalence check the type-supertype relationship between theAlgorithms types at the current nodes in the two dags is deter-
"Our feature structure subsumption algorithm2 as-mined, and if one type is not equal to or a supertype sumes totally well-typed structures (Carpenter, of the other then subsumption cannot hold in that 1992) and employs similar machinery to the direction."
"Finally, after successfully checking the quasi-destructive unification algorithm described by type-supertype relationships, the function recurses[REF_CITE]."
"In particular, it uses temporary into the arcs outgoing from each node that have the pointers in dag nodes, each pointer tagged with a same label."
"Since we are assuming totally well-typed generation counter, to keep track of intermediate feature structures, it must be the case that either the results in processing; incrementing the generation sets of arc labels in the two dags are the same, or counter invalidates all temporary pointers in a sin-one is a strict superset of the other."
Only arcs with gle operation.
"But whereas quasi-destructive unifi- the same labels need be processed; extra arcs need cation makes two passes (determining whether the not since the type-supertype check at the two nodes unification will be successful and then copying out will already have determined that the feature struc-the intermediate representation) the subsumption ture containing the extra arcs must be subsumed by algorithm makes only one pass, checking reentran-the other, and they merely serve to further specify cies and type-supertype relationships at the same it and cannot affect the final result. time.3 The algorithm, shown in Figure 1, also si-"
"Our implementation of the algorithm contains ex-multaneously tests if both feature structures sub-tra redundant but cheap optimizations which for rea-sume each other (i.e. they are equivalent), if either sons of clarity are not shown in figure 1; these in-subsumes the other, or if there is no subsumption clude tests that forwardp is true immediately before relation between them in either direction. the first supertype check and that backwardp is true The top-level entry point dag-subsumes-pO and before the second.4 subsidiary function dag-subsumes-pO0 each return"
"The use of temporary pointers means that the two values, held in variables ]orwardpand back- space complexity of the algorithm is linear in the wardp, both initially true, recording whether it is sum of the sizes of the feature structures."
"However, possible that the first dag subsumes the second in our implementation the &apos;copy&apos; slot that the point-and/or vice-versa, respectively."
"When one of these ers occupy is already present in each dag node (it is possibilities has been ruled out the appropriate vari-required for the final phase of unification to store able is set to false; in the statement of the algorithm new nodes representing equivalence classes), so in the two returned values are notated as a pair, i.e. practice the subsumption test does not allocate any (/orwardp,backwardp)."
If at any stage both vari-new storage.
All pointer references take constant ables have become set to false the possibility of sub-time since there are no chains of &apos;forwarded&apos; point-sumption in both directions has been ruled out so ers (forwarding takes place only during the course of the algorithm exits. unification and no forwarded pointers are left after-
The (recursive) subsidiary function dag-subsumes-wards).
"Assuming the supertype tests can be carried pO0 does most of the work, traversing the two input 1 procedure dag-subsumes-p(dagl,dag2) --_ 2 (forwardp, backwardp) &lt;-- { establish context for non-local exit} 3 catch with tag &apos;fail&apos; dag-subsumes-pO(dagl, dag2, true, true); 4 invalidate-temporary-pointers(); {reset temporary &apos;copy&apos; pointers} 5 return (forwardp, backwardp); 6 end 7 procedure dag-subsumes-pO(dagl,dag2,forwardp, backwardp) - 8 {check reentraneies} if (dagl.copy is empty) then dagl.copy &lt;---dag2; 9 else if~dagl.copy ~ dag2) then forwardp &lt;-- false; fi 10 if (dag2.copy is empty) then dag[Footnote_2].copy ~- dagl; 11 else if (dag2.copy p dagl) then backwardp ~- false; fi 12 if (forwardp = false and backwardp = false) then 13 throw (false, false) with tag &apos;fail&apos;; {reentrancy check failed} 14 fi 15 if (not supertype-or-equal-p(dagl.type,dag[Footnote_2].type)) then forwardp +- false; fi {check types} 16 if (not supertype-or-equal-p(dag[Footnote_2].type,dagl.type)) then backwardp &lt;-- false; fl 17 if (forwardp = false and backwardp = false) then 18 throw (false, false) with tag &apos;fail&apos;; {no subtype relations} 19 fi 20 {check shared arcs recursively} for each arc in intersect(dagl.arcs, dag[Footnote_2].arcs) do 21 (forwardp, backwardp) &lt;- 22 dag-subsumes-pO(destination of arc for dagl, destination of arc for dag2, forwardp, backwardp); 23 od 24 return (forwardp, backwardp); {signal result to caller} 25 end"
"2Although independently-developed implementations of 4There is scope for further optimisation of the algorithm in essentially the same algorithm can be found in the source code the case where dagl and dag2 are identical: full processing in-of The Attribute Logic Engine (ALE) version 3.2 (Carpenter side the structure is not required (since all nodes inside it will &amp;[REF_CITE]) and the SICStus Prolog term utilities library be identical between the two dags and any strictly internal (Penn, personal communication), we believe that there is no reentrancies will necessarily be the same), but we would still previous published description of the algorithm. need to assign temporary pointers inside it so that any exter- 3Feature structure F subsumes feature structure G iff: nal reentrancies into the structure would be treated correctly. (1) if path p is defined in F then p is also defined in G and In our tests we have found that as far as constituents that are the type of the value of p in F is a supertype or equal to the candidates for local ambiguity packing are concerned there is value in G, and (2) all paths that are reentrant in F are also in fact little equality of structures between them, so special reentrant in G. equality processing does not justify the extra complication."
"2Although independently-developed implementations of 4There is scope for further optimisation of the algorithm in essentially the same algorithm can be found in the source code the case where dagl and dag2 are identical: full processing in-of The Attribute Logic Engine (ALE) version 3.2 (Carpenter side the structure is not required (since all nodes inside it will &amp;[REF_CITE]) and the SICStus Prolog term utilities library be identical between the two dags and any strictly internal (Penn, personal communication), we believe that there is no reentrancies will necessarily be the same), but we would still previous published description of the algorithm. need to assign temporary pointers inside it so that any exter- 3Feature structure F subsumes feature structure G iff: nal reentrancies into the structure would be treated correctly. (1) if path p is defined in F then p is also defined in G and In our tests we have found that as far as constituents that are the type of the value of p in F is a supertype or equal to the candidates for local ambiguity packing are concerned there is value in G, and (2) all paths that are reentrant in F are also in fact little equality of structures between them, so special reentrant in G. equality processing does not justify the extra complication."
"2Although independently-developed implementations of 4There is scope for further optimisation of the algorithm in essentially the same algorithm can be found in the source code the case where dagl and dag2 are identical: full processing in-of The Attribute Logic Engine (ALE) version 3.2 (Carpenter side the structure is not required (since all nodes inside it will &amp;[REF_CITE]) and the SICStus Prolog term utilities library be identical between the two dags and any strictly internal (Penn, personal communication), we believe that there is no reentrancies will necessarily be the same), but we would still previous published description of the algorithm. need to assign temporary pointers inside it so that any exter- 3Feature structure F subsumes feature structure G iff: nal reentrancies into the structure would be treated correctly. (1) if path p is defined in F then p is also defined in G and In our tests we have found that as far as constituents that are the type of the value of p in F is a supertype or equal to the candidates for local ambiguity packing are concerned there is value in G, and (2) all paths that are reentrant in F are also in fact little equality of structures between them, so special reentrant in G. equality processing does not justify the extra complication."
"2Although independently-developed implementations of 4There is scope for further optimisation of the algorithm in essentially the same algorithm can be found in the source code the case where dagl and dag2 are identical: full processing in-of The Attribute Logic Engine (ALE) version 3.2 (Carpenter side the structure is not required (since all nodes inside it will &amp;[REF_CITE]) and the SICStus Prolog term utilities library be identical between the two dags and any strictly internal (Penn, personal communication), we believe that there is no reentrancies will necessarily be the same), but we would still previous published description of the algorithm. need to assign temporary pointers inside it so that any exter- 3Feature structure F subsumes feature structure G iff: nal reentrancies into the structure would be treated correctly. (1) if path p is defined in F then p is also defined in G and In our tests we have found that as far as constituents that are the type of the value of p in F is a supertype or equal to the candidates for local ambiguity packing are concerned there is value in G, and (2) all paths that are reentrant in F are also in fact little equality of structures between them, so special reentrant in G. equality processing does not justify the extra complication."
"Figure 1: Bidirectional, linear-time feature structure subsumption (and equivalence) algorithm. out in constant time (e.g. by table lookup), and that clear whether they achieve maximal compactness in the grammar allows us to put a small constant upper practice: see Table 1)."
"In the former case the parse bound on the intersection of outgoing arcs from each forest produced will not be optimally compact; in node, the processing in the body of dag-subsumes- the latter it will be, but maintaining chart consis-pO0 takes unit time."
The body may be executed up tency and parser correctness becomes a non-trivial to N times where N is the number of nodes in the problem.
Packing of a new edge into an existing one smaller of the two feature structures.
So overall the we call proactive (or forward) packing; for the more algorithm has linear time complexity.
"In practice, complex situation involving a new edge subsuming our implementation (in the environment described in an existing one we introduce the term retroactive (or Section 4) performs of the order of 34,000 top-level backward) packing. feature structure subsumption tests per second."
"Several issues arise when packing an old edge (old) into one that was newly derived (new) retroactively: 3 Ambiguity Packing in the Parser (i) everything derived from old (called derivatives[REF_CITE]and[REF_CITE]have old in the following) must be invalidated and ex-investigated local ambiguity packing for unification cluded from further processing (as new is known grammars with CF backbones, using CF category to generate more general derivatives); and (ii) all equality and feature structure subsumption to test pending computation involving old and its deriva-if a newly derived constituent can be packed."
If a tives has to be blocked efficiently.
"Derivatives of new constituent is equivalent to or subsumed by an old that are invalidated because of retroactive pack-existing constituent, then it can be packed into the ing may already contain packed analyses, however, existing one and will take no further part in pro- which still represent valid ambiguity."
These need to cessing.
"However, if the new constituent subsumes be repacked into corresponding derivatives of new an existing one, the situation is not so straightfor- when those become available."
"In turn, derivatives of ward: either (a) no packing takes place and the new old may have been packed already, such that they constituent forms a separate edge[REF_CITE], or need not be available in the chart for subsequent sub- (b) previous processing involving the old constituent sumption tests."
"Therefore, the parser cannot simply is undone or invalidated, and it is packed into the delete everything derived from old when it is packed; new one (Moore &amp;[REF_CITE]; however, it is un- instead, derivatives must be preserved (but blocked) 3 for each parent in edge.parents do block(parent, freeze); od {recursively freeze derivatives} 4 end 5 procedure packed-edge-p(new) - 6 for each old in chart[new.start][new.end] do {passive edges with same span} 7 {test category subsumption} (forwardp, backwardp) ~- dag-subsumes-p(old.dag, new.dag); 8 {equivalent or proactive packing} if (forwardp = true and old.frozen = fa/se) then 9 old.packed ~-- (new I old.packed); {pack &apos;new&apos; into &apos;old&apos;} 10 return true; {return to caller; signal success} 11 fi 12 if (backwardp) then {retroactive packing} 13 {raise all packings into new host} new.packed ~-- (new.packed @ old.packed); 14 old.packed +-- 0; 15 if (old.frozen = false) then new.packed e- (old I new.packed); fi {pack &apos;old&apos; into &apos;new&apos;} 16 block(old, frost); {frost &apos;old&apos; and freeze derivatives} 17 {remove &apos;old&apos; from the chart} delete(old, chart); 18 fl 19 od 20 return false; {signal failure to pack &apos;new&apos; to caller} 21 end"
Figure 2: Algorithm called on each newly derived edge to achieve maximal packing. until the derivations have been recomputed on the tion of one or more existing edges from the chart basis of new.[Footnote_5] As new is equivalent to or more gen- and blocking of derivatives.
"5The situation is simpler in the CLE parser (Moore &amp;[REF_CITE]) because constituents and dominance relations are separated in the chart. The CLEencoding, in fact, does not record the actual daughters used in building a phrase (e.g. as unique references or pointers, as we do), but instead preserves the category information (i.e. a description) of those daugh-ters. Hence, in extracting complete parses from the chart, the CLE has to perform (a limited) search with re-unification of categories; in this respect, the CLE parse forest still is an"
"Whenever the parser eral than old it is guaranteed to derive at least the accesses the chart (i.e. in trying to combine edges) same set of edges; furthermore, the derivatives of or retrieves a task from the agenda, it is expected new will again be equivalent to or more general than to ignore all edges and parser tasks involving such the corresponding edges derived from old. edges that have a non-null &apos;frozen&apos; value."
"The procedure packed-edge-p(), sketched in Fig- existing edge old is packed retroactively, it is frosted ure 2, achieves pro- and retroactive packing with- and ignored by the parser; as old now represents lo-out significant overhead in the parser; the algorithm cal ambiguity, it still has to be taken into account can be integrated with arbitrary bottom-up (chart- when the parse forest is unpacked."
Derivatives of based) parsing strategies.
"The interface assumes old, on the other hand, need to be invalidated in that the parser calls packed-edge-pO on each new both further parsing and later unpacking, since they edge new as it is derived; a return value of true indi- would otherwise give rise to spurious analyses; ac- cates that new was packed proactively and requires no further processing."
"Conversely, a false return value from packed-edge-p 0 signals that new should subsequently undergo regular processing."
"The sec-ond part of the interface builds on notions we call frosting and freezing, meaning temporary and per-mament invalidation of edges, respectively."
"As a side-effect of calls to packed-edge-p(), a new edge can cause retroactive packing, resulting in the dele-"
"Frosting and freezing is done in the subsidiary pro-cedure block() that walks up the parent link recur-sively, storing a mark into the &apos;frozen&apos; slot of edges that distinguishes between temporary frosting (in the top-level call) and permanent freezing (in recur-sire calls)."
"For a newly derived edge new, packed-edge-pO tests mutual subsumption against all passive edges that span the same portion of the input string."
"When forward subsumption (or equivalence) is de-tected and the existing edge old is not blocked, reg-ular proactive packing is performed (adding new to the packing list for old) and the procedure returns immediately.[Footnote_6] In the case of backward subsump- tion, analyses packed into old are raised into new Figure 3 compares total chart size (in all-paths (using the append operator &apos;~&apos; because new can at- mode) for the regular LKB parser and our variant tract multiple existing edges in the loop); old itselfis with pro- and retroactive packing enabled."
6packing an edge el into another edge e2 logically means that e2 will henceforth serve as a representative for el and
"Factor-only packed into new when it is not blocked already. ing ambiguity reduces the number of passive edges Finally, old is frosted, its derivatives are recursively by a factor of more than three on average, while for frozen, and old is deleted from the chart."
"In contrast a number of cases the reduction is by a factor of 30 to proactive packing, the top-level loop in the pro- and more."
"Compared to regular parsing, the rate of cedure continues so that new can pick up additional increase of passive chart items with respect to sen-edges retroactively."
"However, once a backward sub- tence length is greatly diminished. sumption is detected, it follows that no proactive To quantify the degree of packing we achieve packing can be achieved for new, as the chart can- in practice, we re-ran the experiment reported by not contain an edge that is more general than old."
"The results reported We have carried out an evaluation of the algo-[REF_CITE](using the CLEgram-rithms presented above using the LinGO grammar mar of English) and those obtained using pro- and (Flickinger &amp;[REF_CITE]), a publicly-available,multi- retroactive packing with the LinGO grammar are purpose, broad-coverage HPSGof English developed presented in Table 1.8"
Although the comparison at CSLI Stanford.
"With roughly 8,000 types, an av- involves different grammars we believe it to be in-erage feature structure size of around 300 nodes, and structive, since (i) both grammars have comprehen- 64 lexical and grammar rules (fleshing out the inter- sive coverage, (ii) derive the same numbers of read-action of HPSG ID schemata, wellformedness prin- ings for all test sentences in this experiment, (iii) ciples, and LP constraints), LinGO is among the require (almost) the same number of nodes for the largest HPSGgrammars available."
"We used the LKB basic cases (zero and one PP), (iv) exhibit a similar system ([REF_CITE]1999) as an experimen- size in nodes for one core PP (measured by the in-tation platform since it provides a parameterisable crement from n = 0 to n = 1), and (v) the syntactic bottom-up chart parser and precise, fine-grained simplicityofthe test material hardly allows crosstalk profiling facilities (Oepen &amp;[REF_CITE]).7 All"
"It is unclear whether the counting of &apos;packed lengths from 1 to 20 words are represented with 100 nodes&apos;[REF_CITE]includes con records or not, since only maa records are required in parse tree recovery. test items each; although sentences in the corpus"
"In any case, both types of chart record need to be checked by range up to 36 words in length there are relatively subsumption as new entries are added to the chart."
"Con-few longer than 20 words. versely, in our setup each edge represents not only the node"
Kim saw a cat (in the hotel) n with other grammatical phenomena.
"Comparing rel- verts to the unrestricted constraint set, so we can al-ative packing efficiency with increasing ambiguity low overgeneration in the first phase and filter glob- (the columns labeled &apos;-&apos; in Table 1), our method ap- ally inconsistent analyses during unpacking."
"Thus, pears to produce a more compact representation of the right choice of grammar restrictor can be viewed ambiguity than the CLE,and at the same time builds as an empirical rather than analytical problem. a more specific representation of the parse forest that Table 2 summarizes packing efficiency and parser can be unpacked without search."
"To give an impres- performance for three different restrictors (labeled sion of parser throughput, Table 1 includes timings no, partial, and full semantics, respectively); to for our parsing and unpacking (validation) phases, gauge effects of input complexity, the table is fur- contrasted with the plain, non-packing LKB parser: as would be expected, parse time increases linearly in the number of edges, while unpacking costs re-flect the exponential increase in total numbers of analyses; the figures show that our packing scheme achieves a very significant speedup, even when un-packing time is included in the comparison."
"In order for the subsumption relation to apply mean-ingfully to HPSGsigns, two conditions must be met."
"Firstly, parse tree construction must not be dupli-cated in the feature structures (by means of the HPSG DTRSfeature) but be left to the parser (i.e. recorded in the chart); this is achieved in a stan-dard way by feature structure restricti[REF_CITE]applied to all passive edges."
"Secondly, the pro-cessing of constraints that do not restrict the search space but build up new (often semantic) structure should be postponed, since they are likely to inter-fere with subsumption."
"For example, analyses that differ only with respect to PP attachment would have the same syntax, but differences in semantics may prevent them being packed."
"This problem can be overcome by using restriction to (temporarily) re-move such (semantic) attributes from lexical entries and also from the rule set, before they are input ther subdivided by sentence length into two groups (of around 1,000 sentences each)."
"Compared to reg-ular parsing, packing with the full semantics in place is not effective: the chart size is reduced slightly, but the extra cost for testing subsumption increases total parse times by a factor of more than four."
"Eliminat-ing all semantics (i.e. the entire HPSGC0NTvalue), on the other hand, results in overgeneralisation: with less information in the feature structures we achieve the highest number of packings, but at the same time rules apply much more freely, resulting in a larger chart compared to parsing with a partial se-mantics; moreover, unpacking takes longer because the parse forest now contains inconsistent analyses."
Restricting compositional semantics but preserving attributes that participate in selection and agree-ment results in minimal chart size and parsing time (shown in the partial semantics figures) for both di-visions of the test corpus.
"The majority of packings involve equivalent fea-ture structures which suggests that unpacking could be greatly simplified if the grammar restrictor was guaranteed to preserve the generative capacity of the grammar (in the first parsing phase); then, only packings involving actual subsumption would have to be validated in the unpacking phase.[Footnote_9] Finally, to the parser in the initial parse forest construction grammar is working towards a stricter separation of restrictive phase."
9There is room for further investigation here: partly for
"The second, unpacking phase ofthe parser re- (selectional) and constructive (compositional) constraints in numbers are averaged over 1,000 items per class; packings are, from left to right: equivalence (&apos;-&apos;), pro- (&apos;-~&apos;) and retroactive (&apos;r&apos;) packings, and the number of edges that were frozen (&apos;±&apos;). we note that the number of retroactive packings is 6 Conclusions relatively small, and on average each such packing leads to only one previously derived edge being in-"
We have presented novel algorithms for efficient sub-validated.
"This, of course, is a function of the order sumption checking and pro- and retroactive local in which edges are derived, i.e. the parsing strategy. ambiguity packing with large feature structures, and have provided strong empirical evidence that our All the results in Table 2 were obtained with a approach can be applied beneficially to chart pars- &apos;right corner&apos; strategy which aims to exhaust compu-ing with a large, broad-coverage HPSG of English. tation for any suffix of the input string before mov-"
"By comparison to previous work in unification-based ing the input pointer to the left; this is achieved by parsing we have demonstrated that pro- and retroac-means of a scoring function end - start-W- (where start tive packing are well-suited to achieve optimal pack-and end are the vertices of the derivation that would ing; furthermore, experimental results obtained with result from the computation, and n is the total input a publicly-available HPSG processing platform con-length) that orders parser tasks in the agenda."
"How-firm that ambiguity packing can greatly reduce av-ever, we have observed (Oepen &amp;[REF_CITE]) erage parse complexity for this type of grammars. that HPSG-type, highly lexicalized grammars bene-fit greatly from a bidirectional, &apos;key&apos;-driven, active In related work,[REF_CITE]describes an ap-parsing regime, since they often employ rules with proach to packing in which alternative feature struc-underspecified arguments that are only instantiated tures are represented as packed, distributed disjunc-by coreference with other daughters (where the &apos;key&apos; tions of feature structure fragments."
"Although the daughter is the linguistic head in many but not all approach may have potential, the shifting of com-constructions)."
"This requirement and the general plex accounting into the unification algorithm is at non-predictability of categories derived for any to- variance with the findings[REF_CITE], ken substring (in particular with respect to unary who report large speed-ups from the elimination of rule applications), means that a particular parsing disjunction processing during unification."
"Unfortu-strategy may reduce retroactive packing but cannot nately, the reported evaluation measures and lack of avoid it in general."
"With pro- and retroactive pack- discussion of parser control issues are insufficient to ing and the minimal accounting overhead, we find allow a precise comparison. overall parser throughput to be very robust against We intend to develop the approach presented in variation in the parsing strategy."
Lavie and Rosd this paper in several directions.
"Firstly, we will en- (2000) present heuristics for ordering parser actions hance the unpacking phase to take advantage of the to achieve maximally compact parse forests--though large number of equivalence packings we observe. only with respect to a CF category backbone---in the This will significantly reduce the amount of work it absence of retroactive packing; however, the tech- needs to do."
"Secondly, many application contexts niques we have presented here allow local ambigu- and subsequent layers of semantic processing will ity packing and parser tuning--possibly including not require unfolding the entire parse forest; here, priority-driven best-first search--to be carried out we need to define a selective, incremental unpack-mostly independently of each other. ing procedure."
"Finally, applications like VerbMo-bil favour prioritized best-first rather than all-paths the grammar and underlying semantic theory."
We expect that parsing.
"Using slightly more sophisticated account-our approach to packing will benefit from these developments. ing in the agenda, we plan to investigate priority propagation in a best-first variant of our parser."
"Acknowledgements Kiefer, B., Krieger, H.-U., Carroll, J., &amp; Malouf, R. We are grateful to Ulrich Callmeier,[REF_CITE]."
"A bag of useful techniques for efficient and Dan Flickinger, and three anonymous reviewers for robust parsing."
"In Proceedings of the 37th Meeting comments on a draft of the paper, to Bob Moore for of the Association for Computational Linguistics a detailed explanation of the workings of the CLE parser, and to Gerald Penn for information about (pp. 473-480)."
"College Park, MD."
"Lavie, A., &amp; Ros~, C. (2000)."
Optimal ambiguity related implementations of the subsumption algo-rithm.
"The research was supported by the Deutsche Forschungsgemeinschaft as part of the Collaborative Research Division Resource-Adaptive Cognitive Pro- cesses, project B4 (PERFORM); and by a UK EPSRC packing in context-free parsers with interleaved unification."
In Proceedings of the 6th Interna-tional Workshop on Parsing Technologies (pp. 147-158).
"Maxwell III, J. T., &amp; Kaplan, R. M. (1995)."
Advanced Fellowship to the second author.
This paper presents a new approach to
The algorithm proposed here for ranking sen-tences in a forest is a bottom-up dynamic pro-gramming algorithm.
"It is analogous to a chart parser, but performs an inverse compari-son."
"Rather than comparing alternate syntactic structures indexed to the same positions of an \ / \ 7 input sentence, it compares alternate phrases corresponding to the same semantic input."
"As in a probabilistic chart parser, the key insight of this algorithm is that the score for each of the phrases represented by a particu-lar node in the forest can be decomposed into a context-independent (internal) score, and a context-dependent (external) score."
"The inter-nal score, once computed, is stored with the phrase, while the external score is computed in combination with other sibling nodes."
"In general, the internal score for a phrase as-sociated with a node p can be defined recur-sively as:"
"I(p) = 1-Ij=lJ I(cj) • E(cjJcontext(cl..Cj_l) ) where I stands for the internal score, E the ex-ternal score, and cj for a child node of p."
"The specific formulation of I and E, and the pre-cise definition of the context depends on the lan-guage model being used."
"As an example, in a bigram model,I I=1 for leaf nodes, and E can be expressed as:"
E = P(EirstWord(ej)lLastWord(cj_l) )
"Depending on the language model being used, a phrase will have a set of externally-relevant features."
These features are the aspects of the phrase that contribute to the context-dependent scores of sibling phrases.
"In the case of the bi-gram model, the features are the first and last words of the phrase."
In a trigram model it is the first and last two words.
"In more elaborate lan-guage models, features might include elements such as head word, part-of-speech tag, constitu-tent category, etc."
A crucial advantage of the forest-based method is that at each node only the best in-ternally scoring phrase for each unique combi-nation of externally relevant features needs to be maintained.
The rest can be pruned with-out sacrificing the guarantee of obtaining the overall optimal solution.
This pruning reduces exponentially the total number of phrases that need to be considered.
"In effect, the ranking"
VP.344 ~ VP.225
TO.341 VB.342 VBN.330 225: 341: 342: 330: might have to be eaten may have could have might be required may be required could be required might be having may be having could be having might be obliged may be obliged could be obliged 344: might ... eaten may ... eaten could ... eaten
"Figure 4: Pruning phrases from a forest node, assuming a bigram model algorithm exploits the independence that exists between most disjunctions in the forest."
"To illustrate this, Figure 4 shows an exam-ple of how phrases in a node are pruned, as-suming a bigram model."
"The rule for node VP.344 in the forest of Figure 3 is shown, to-gether with the phrases corresponding to each of the child nodes."
"If every possible com-bination of phrases is considered for the se-quence of nodes on the right-hand side, there are three unique first words, namely &quot;might&quot;, &quot;may&quot; and &quot;could&quot;, and only one unique final word, &quot;eaten&quot;."
"Given that only the first and last words of a phrase are externally relevant features in a bigram model, only the three best scoring phrases (out of the 12 total) need to be maintained for node VP.344--one for each unique first-word and last-word pair."
"The other nine phrases can never be ranked higher, no matter what constituents VP.344 later combines with."
"Pseudocode for the ranking algorithm is shown below. &quot;Node&quot; is assumed to be a record composed at least of an array of child nodes, &quot;Node-&gt;c[1..N],&quot; and best-ranked phrases, &quot;Node-&gt;p[1..M].&quot; The function Con-catAndScore concatenates two strings together, and computes a new score for it based on the formula given above."
"The function Prune guar- antees that only the best phrase for each unique tive search algorithm on a lattice is O((vN)~), set of features values is maintained."
The core where l is approximately the length of the loop in the algorithm considers the children of longest sentence in the lattice.
"The forest-based the node one-by-one, concatenating and scoring algorithm thus offers an exponential reduction the phrases of the first two children and prun- in complexity while still guaranteeing an opti-ing the results, before considering the phrases mal solution."
"A capped N-best heuristic search of the third child, and concatenating them with algorithm on the other hand has complexity the intermediate results, and so on."
From the O(vN1).
"However, as mentioned earlier, it typ-pseudocode, it can be seen that the complex- ically fails to find the optimal solution with ity of the algorithm is dominated by the num- longer sentences. ber of phrases associated with a node (not the In conclusion, the tables in Figure 5 and Fig-number of rules used to represent the forest, ure 6 show experimental results comparing a nor the number of children in a an AND node). forest representation to a lattice in terms of the More specifically, because of the pruning, it de- time and space used to rank sentences."
"These pends on the number of features associated with results were generated from 15 test set inputs, the language model, and the average number of whose average sentence length ranged from 14 unique combinations of feature values that are to 36 words."
They were ranked using a bigram seen.
"If f is the number of features, v the av- model."
The experiments were run on a Sparc erage number of unique values seen in a node Ultra 2 machine.
"Note that the time results for for each feature, and N the number of N best the lattice are not quite directly comparable to being maintained for each unique set of fea- those for a forest because they include overhead ture values (but not a cap on the number of costs for loading portions of a hash table."
"It was phrases), then the algorithm has the complex- not possible to obtain timing measurements for ity O((vN)2/) (assuming that children of AND the search algorithm alone."
We estimate that nodes are concatenated in pairs).
"Note that f=2 roughly 80% of the time used in processing the for the bigram model, and f=4 for the trigram lattice was used for search alone."
"Instead, the model. results in Figure 5 should be interpreted as a"
"In comparison, the complexity of an exhaus- comparison between different kinds of systems."
RankForest(Node) { if ( Leafp(Node))
LeafScore( Node); forj=ltoJ { if ( not(ranked?(Node-&gt;e[j])))
RankForest(Node- &gt; c[j]); }
"In that respect, it can be observed from Ta-ble 5 that the forest ranking program performs at least 3 or 4 seconds faster, and that the time needed does not grow linearly with the num-ber of paths being considered as it does with the lattice program."
Instead it remains fairly constant.
"This is consistent with the theoreti-cal result that the forest-based algorithm does for m=l to NumberOfPhrasesIn(Node-&gt;c[1]) not depend on sentence length, but only on the"
Node-&gt;p[m] = (Node-&gt;c[1])-&gt;p[m]; k=ffpr j=2 to J { number of different alternatives being consid-ered at each position in the sentence. for m=l to NumberOfPhrasesIn(Node)
"From Table 6 it can be observed that when for n=l to NumberOfPhrasesIn( Node-&gt;c[j]) there are a relatively moderate number of sen-tences being ranked, the forest and the lattice temp[k++] = ConcatAndScore( are fairly comparable in their space consump-"
"Node-&gt;p[m], (Node- &gt;c[j])- &gt;Pin]); Prune( temp); tion."
The forest has a little extra overhead in representing hierarchical structure.
"However, the space requirements of a forest do not grow for m=l to NumberOfPhrasesIn(temp) linearly with the number of paths, as do those"
Node-&gt;p[m] = (temp[m]); } of the lattice.
"Thus, with very large numbers of paths, the forest offers significant savings in space."
The spike in the graphs deserves particular comment.
Our current system for producing 2so~ forests from semantic inputs generally produces
OR-nodes with about two branches.
The par-ticular input that triggered the spike produced a forest where some high-level OR-nodes had a much larger number of branches.
"In a lattice, any increase in the number of branches expo-nentially increases the processing time and stor-age space requirements."
"However, in the forest representation, the increase is only polynomial with the number of branches, and thus did not produce a spike. cess using a lattice versus a forest representa-tion."
"The X-axis is the number of paths (loglo scale), and the Y-axis is the time in seconds."
The forest representation and ranking algorithm have been implemented as part of the Nitro-gen generator system.
The results shown in the previous section illustrate the time and space advantages of the forest representation which make calculating the mathematically op-timal sentence in the forest feasible (particularly for longer sentences).
"However, obtaining the mathematically optimal sentence is only valu-able if the mathematical model itself provides a good fit."
"Since a forest representation makes it possible to add syntactic information to the mathematical model, the next question to ask is whether such a model can provide a better fit for natural English than the ngram models we have used previously."
"In future work, we plan to modify the forests our system produces so they conform to the Penn Treebank corpus[REF_CITE]annotation style, and then do experiments using models built with Tree-bank data."
"Special thanks go to Kevin Knight, Daniel Marcu, and the anonymous reviewers for their comments."
This research was supported in part[REF_CITE]291.
"We present a cut and paste based text summa-rizer, which uses operations derived from an anal-ysis of human written abstracts."
"The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge re-suiting phrases together as coherent sentences."
"Our work includes a statistically based sentence decom-position program that identifies where the phrases of a summary originate in the original document, pro-ducing an aligned corpus of summaries and articles which we used to develop the summarizer."
There is a big gap between the summaries produced by current automatic summarizers and the abstracts written by human professionals.
Certainly one fac-tor contributing to this gap is that automatic sys-tems can not always correctly identify the important topics of an article.
"Another factor, however, which has received little attention, is that automatic sum-marizers have poor text generation techniques."
Most automatic summarizers rely on extracting key sen-tences or paragraphs from an article to produce a summary.
"Since the extracted sentences are discon-nected in the original article, when they are strung together, the resulting summary can be inconcise, incoherent, and sometimes even misleading."
"We present a cut and paste based text sum-marization technique, aimed at reducing the gap between automatically generated summaries and can be used alone or together to transform extracted sentences into sentences in human-written abstracts."
The operations were identified based on manual and automatic comparison of human-written abstracts and the original articles.
"Examples include sentence reduction, sentence combination, syntactic transfor-mation, and lexical paraphrasing. (2) Development of an automatic system to perform cut and paste operations."
Two opera-tions - sentence reduction and sentence combination - are most effective in transforming extracted sen-tences into summary sentences that are as concise and coherent as in human-written abstracts.
"We implemented a sentence reduction module that re-moves extraneous phrases from extracted sentences, and a sentence combination module that merges the extracted sentences or the reduced forms resulting from sentence reduction."
"Our sentence reduction model determines what to cut based on multiple sources of information, including syntactic knowl-edge, context, and statistics learned from corpus analysis."
"It improves the conciseness of extracted sentences, making them concise and on target."
Our sentence combination module implements combina-tion rules that were identified by observing examples written by human professionals.
It improves the co-herence of extracted sentences. (3) Decomposing human-wrltten summary sentences.
The cut and paste technique we propose here is a new computational model which we based human-written abstracts. on analysis of human-written abstracts.
"To do thisRather than focusing on how to identify key sentences, as do other re- analysis, we developed an automatic system that can searchers, we study how to generate the text of a match a phrase in a human-written abstract to the summary once key sentences have been extracted. corresponding phrase in the article, identifying its The main idea of cut and paste summarization most likely location."
This decomposition program is to reuse the text in an article to generate the allows us to analyze the construction of sentences summary.
"However, instead of simply extracting in a human-written abstract."
"Its results have been sentences as current summarizers do, the cut and used to train and test the sentence reduction and paste system will &quot;smooth&quot; the extracted sentences sentence combination module. by editing them."
"Such edits mainly involve cutting In Section 2, we discuss the cut and paste tech-phrases and pasting them together in novel ways. nique in general, from both a professional and com-"
The key features of this work are: putational perspective.
We also describe the six cut (1) The identification of cutting and past- and paste operations.
"In Section 3, we describe the system architecture."
"The major components of the Document sentence: When it arrives some-system, including sentence reduction, sentence com- time next year in new TV sets, the V-chip will bination, decomposition, and sentence selection, are give parents a new and potentially revolution-described in Section 4."
The evaluation results are ary device to block out programs they don&apos;t shown in Section 5.
Related work is discussed in want their children to see.
"Finally, we conclude and discuss future Summary sentence: The V-chip will give par-work. ents a device to block out programs they don&apos;t want their children to see. 2 Cut and paste in summarization The deleted material can be at any granularity: a word, a phrase, or a clause."
Multiple components 2.1 Related work in professional can be removed. summarizing (2) sentence combination Merge material from several sentences.
"It can be Professionals take two opposite positions on whether used together with sentence reduction, as illustrated a summary should be produced by cutting and past-in the following example, which also uses paraphras-ing the original text."
"One school of scholars is ing: opposed; &quot;(use) your own words... Do not keep too close to the words before you&quot;, states an early Text Sentence 1: But it also raises serious book on abstracting for American high school stu-questions about the privacy of such highly dents[REF_CITE]."
"Another study, however, personal information wafting about the digital shows that professional abstractors actually rely on world. cutting and pasting to produce summaries: &quot;Their Text Sentence 2: The issue thus fits squarely professional role tells abstractors to avoid inventing into the broader debate about privacy and se-anything."
"They follow the author as closely as pos- curity on the internet, whether it involves pro-sible and reintegrate the most important points of tecting credit card number or keeping children a document in a shorter text&quot; (Endres-Niggemeyer from offensive information. et al., 1998)."
Some studies are somewhere in be-
Summary sentence: But it also raises the is-tween: &quot;summary language may or may not follow sue of privacy of such personal information that of author&apos;s&quot;[REF_CITE].
Other guidelines or and this issue hits the head on the nail in the books on abstracting[REF_CITE]broader debate about privacy and security on do not discuss the issue. the internet.
"Our cut and paste based summarization is a com- (3) syntactic transformation putational model; we make no claim that humans In both sentence reduction and combination, syn-use the same cut and paste operations. tactic transformations may be involved."
"For exam-ple,the positionof the subject in a sentence may be"
We manually analyzed 30 articles and their corre- (4) lexicalparaphrasing sponding human-written summaries; the articles and Replace phrases with their paraphrases.
"For in-their summaries come from different domains ( 15 stance, the summaries substituted point out with general news reports, 5 from the medical domain, note, and fits squarely into with a more picturesque 10 from the legal domain) and the summaries were description hits the head on the nail in the previous written by professionals from different organizations. examples."
We found that reusing article text for summarization (5) generalization or specification is almost universal in the corpus we studied.
"Replace phrases or clauses with more general or fined six operations that can be used alone, sequen-specific descriptions."
"Examples of generalization tially, or simultaneously to transform selected sen-and specification include: tences from an article into the corresponding sum-mary sentences in its human-written abstract:"
"Generalization: &quot;a proposed new law that (1) sentence reduction would require Web publishers to obtain Remove extraneous phrases from a selected sen- parental consent before collecting personal in-tence, as in the following example 1: formation from children&quot; --+ &quot;legislation to protect children&apos;s privacy on-line&quot; Specification: &quot;the White House&apos;s top drug 1All the examples in this section wereproduced by human official&quot; ~ &quot;Gen."
"Barry R. McCaffrey, the professionals White House&apos;s top drug official&quot; (6) reordering written abstracts, the automatic decomposition pro-"
Change the order of extracted sentences.
"For in- gram, a syntactic parser, a co-reference resolution stance, place an ending sentence in an article at the system, the WordNet lexical database, and a large-beginning of an abstract. scale lexicon we combined from multiple resources."
"In human-written abstracts, there are, of course, The components in dotted lines are existing tools or sentences that are not based on cut and paste, but resources; all the others were developed by ourselves. completely written from scratch."
"We used our de-composition program to automatically analyze 300 4 Major components human-written abstracts, and found that 19% ofsen-"
"The main focus of our work is on decomposition of tences in the abstracts were written from scratch. summaries, sentence reduction, and sentence com-"
There are also other cut and paste operations not bination.
"We also describe the sentence extraction listed here due to their infrequent occurrence. module, although it is not the main focus of our work. 3 System architecture 4.1 Decomposition of human-written The architecture of our cut and paste based text summary sentences summarization system is shown in Figure 1."
"Input The decomposition program, see (Jing and McKe-to the system is a single document from any domain. own, 1999) for details, is used to analyze the con-"
"In the first stage, extraction, key sentences in the ar- struction of sentences in human-written abstracts. ticle are identified, as in most current summarizers."
"The results from decomposition are used to build In the second stage, cut and paste based generation, a the training and testing corpora for sentence reduc-sentence reduction module and a sentence combina- tion and sentence combination. tion module implement the operations we observed The decomposition program answers three ques-in human-written abstracts. tions about a sentence in a human-written abstract: The cut and paste based component receives as (1) Is the sentence constructed by cutting and past-input not only the extracted key sentences, but also ing phrases from the input article? (2) If so, what the original article."
"This component can be ported phrases in the sentence come from the original arti-to other single-document summarizers to serve as cle? (3) Where in the article do these phrases come the generation component, since most current sum- from? marizers extract key sentences - exactly what the We used a Hidden Markov Model[REF_CITE]extraction module in our system does. solution to the decomposition problem."
"Other resources and tools in the summarization mathematically formulated the problem, reducing it system include a corpus of articles and their human- to a problem of finding, for each word in a summary sentence, a document position that it most likely reduction is to &quot;reduce without major loss&quot;; that is, comes from."
"The position of a word in a document we want to remove as many extraneous phrases as is uniquely identified by the position of the sentence where the word appears, and the position of the word within the sentence."
"Based on the observation of cut and paste practice by humans, we produced a set of general heuristic rules."
Sample heuristic rules in-clude: two adjacent words in a summary sentence are most likely to come from two adjacent words in the original document; adjacent words in a summary sentence are not very likely to come from sentences that are far apart in the original document.
We use these heuristic rules to create a Hidden Markov Model.
The Viterbi algorithm[REF_CITE]is used to efficiently find the most likely document position for each word in the summary sentence.
Figure 2 shows sample output of the program.
"For the given summary sentence, the program cor-rectly identified that the sentence was combined from four sentences in the input article."
It also di-vided the summary sentence into phrases and pin-pointed the exact document origin of each phrase.
"A phrase in the summary sentence is annotated as (FNUM:SNUM actual-text), where FNUM is the se-quential number of the phrase and SNUM is the number of the document sentence where the phrase comes from."
SNUM = -1 means that the compo-nent does not come from the original document.
The phrases in the document sentences are annotated as (FNUM actual-text).
"The task of the sentence reduction module, de- possible from an extracted sentence so that it can be concise, but without detracting from the main idea that the sentence conveys."
"Ideally, we want to re-move a phrase from an extracted sentence only if it is irrelavant to the main topic."
Our reduction module makes decisions based on multiple sources of knowledge: (1) Grammar checking.
"In this step, we mark which components of a sentence or a phrase are obligatory to keep it grammatically correct."
"To do this, we traverse the sentence parse tree, produced by the English Slot Grammar(ESG) parser devel-oped at IBM[REF_CITE], in top-down order and mark for each node in the parse tree, which of its children are obligatory."
"The main source of knowledge the system relies on in this step is a large-scale, reusable lexicon we combined from mul-tiple resources[REF_CITE]."
"The lexi-con contains subcategorizations for over 5,000 verbs."
This information is used to mark the obligatory ar-guments of verb phrases. by adding up the scores of its children nodes in the 4.3 Sentence combination parse tree.
"This score indicates how important the To build the combination module, we first manu-phrase is to the main topic in discussion. ally analyzed a corpus of combination examples pro- (3) Corpus evidence."
"The program uses a cor- duced by human professionals, automatically cre-pus of input articles and their corresponding reduced ated by the decomposition program, and identified forms in human-written abstracts to learn which a list of combination operations."
"Table 1 shows the components of a sentence or a phrase can be re- combination operations. moved and how likely they are to be removed by To implement a combination operation, we need professionals."
This corpus was created using the de- to do two things: decide when to use which com-composition program.
"We compute three types of bination operation, and implement the combining probabilities from this corpus: the probability that actions."
"To decide when to use which operation, we a phrase is removed; the probability that a phrase is analyzed examples by humans and manually wrote reduced (i.e., the phrase is not removed as a whole, a set of rules."
Two simple rules are shown in Fig-but some components in the phrase are removed); ure 4.
Sample outputs using these two simple rules and the probability that a phrase is unchanged at are shown in Figure 5.
"We are currently exploring all (i.e., neither removed nor reduced)."
These cor- using machine learning techniques to learn the com-pus probabilities help us capture human practice. bination rules from our corpus. (4) Final decision.
The final reduction decision The implementation of the combining actions in-is based on the results from all the earlier steps.
"A volvesjoining two parse trees, substituting a subtree phrase is removed only if it is not grammatically with another, or adding additional nodes."
"We im-obligatory, not the focus of the local context (indi- plemented these actions using a formalism based on cated by a low context importance score), and has a Tree Adjoining Grammar[REF_CITE]. reasonable probability of being removed by humans."
"The phrases we remove from an extracted sentence 4.4 Extraction Module include clauses, prepositional phrases, gerunds, and The extraction module is the front end of the sum-to-infinitives. marization system and its role is to extract key sen-"
The result of sentence reduction is a shortened tences.
Our method is primarily based on lexical re-version of an extracted sentence 2.
This shortened lations.
"First, we link words in a sentence with other text can be used directly as a summary, or it can words in the article through repetitions, morpholog-be fed to the sentence combination module to be ical relations, or one of the lexical relations encoded merged with other sentences. in WordNet, similar to step 2 in sentence reduction."
Figure 3 shows two examples produced by the re-
An importance score is computed for each word in a duction program.
"The corresponding sentences in sentence based on the number of lexical links it has human-written abstracts are also provided for com- with other words, the type of links, and the direc-parison. tions of the links."
"After assigning a score to each word in a sentence, 2It is actually also possible that the reduction program decides no phrase in a sentence should be removed, thus the we then compute a score for a sentence by adding up result of reduction is the same as the input. the scores for each word."
This score is then normal- ized over the number of words a sentence contains. in the abstracts.
"We compared the set of sentences The sentences with high scores are considered im- identified by the program with the set of sentences portant. selected by the majority of human subjects, which is"
"The extraction system selects sentences based on used as the gold standard in the computation of pre-the importance computed as above, as well as other cision and recall."
"The program achieved an average indicators, including sentence positions, cue phrases, 81.5% precision, 78.5% recall, and 79.1% f-measure and tf*idf scores. for 10 documents."
The average performance of 14
The humans reduced the length of the 500 sentences decomposed.
"In the second experiment, we tested the system in a summary alignment task."
"We ran by 44.2% on average, and the system reduced the the decomposition program to identify the source length of the 100 test sentences by 32.7%. document sentences that were used to construct the The evaluation of sentence combination module sentences in human-written abstracts."
Human sub- is not as straightforward as that of decomposition jects were also asked to select the document sen- or reduction since combination happens later in the tences that are semantlc-equivalent to the sentences pipeline and it depends on the output from prior modules.
To evaluate just the combination compo-
"They were also asked to score the coherence of the nent, we assume that the system makes the same summaries based on a scale from 0 to 10."
"On aver-reduction decision as humans and the co-reference age, the extraction-based summaries have a score of system has a perfect performance."
"This involves 4.2 for conciseness, while the revised summaries have manual tagging of some examples to prepare for the a score of 7.9 (an improvement of 88%)."
The average evaluation; this preparation is in progress.
"The eval- improvement for the three systems are 78%, 105%, uation of sentence combination will focus on the ac- and 88% respectively."
The revised summaries are cessment of combination rules. on average 41% shorter than the original extraction-
The overM1 system evMuation includes both in- based summaries.
"For summary coherence, the aver-trinsic and extrinsic evaluation."
"In the intrinsic evM- age score for the extraction-based summaries is 3.9, uation, we asked human subjects to compare the while the average score for the revised summaries is quality of extraction-based summaries and their re- 6.1 (an improvement of 56%)."
"The average improve-vised versions produced by our sentence reduction ment for the three systems are 69%, 57%, and 53% and combination modules."
These summaries tion EvMuation Conference[REF_CITE]and are all extraction-based.
"We then ran our sentence compare how our revised summaries can influence reduction and sentence combination system to re- humans&apos; performance in tasks like text categoriza-vise the summaries, producing a revised version for tion and ad-hoc retrieval. each summary."
"We presented human subjects with the full documents, the extraction-based summaries, 6 Related work and their revised versions, and asked them to com-pare the extraction-based summaries and their re-[REF_CITE]addressed the problem ofrevising vised versions."
The human subjects were asked to summaries to improve their quality.
"They suggested score the conciseness of the summaries (extraction- three types of operations: elimination, aggregation, based or revised) based on a scale from 0 to 10 - and smoothing."
"The goal of the elimination opera-the higher the score, the more concise a summary is. tion is similar to that of the sentence reduction op- eration in our system."
The difference is that while probabilistic functions of a markov process.
"In-eliminationalways removes parentheticals, sentence- equalities, (3):1-8. initial PPs and certain adverbial phrases for every Edward T. Cremmins. 1982."
"The Art ofAbstracting. extracted sentence, our sentence reduction module ISI Press, Philadelphia. aims to make reduction decisions according to each Brigitte Endres-Niggemeyer, Kai Haseloh, Jens case and removes a sentence component only if it Mfiller, Simone Peist, Irene Santini de Sigel, considers it appropriate to do so."
"The goal of the Alexander Sigel, Elisabeth Wansorra, Jan aggregation operation and the smoothing operation Wheeler, and Brfinja Wollny. 1998."
Summarizing is similar to that of the sentence combination op-
"Springer, Berlin. eration in our system."
"However, the combination Raya Fidel. 1986."
Writing abstracts for free-text operations and combination rules that we derived searching.
"Journal[REF_CITE](1):11- from corpus analysis are significantly different from those used in the above system, which mostly came 21, March."
Hongyan Jing and Kathleen R. McKeown. 1998. from operations in traditional natural language gen-eration.
This paper presents a novel architecture for text summarization using cut and paste techniques ob-served in human-written abstracts.
In order to auto-
"Combining multiple, large-scale resources in a reusable lexicon for natural language generation."
"In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, volume 1, pages 607-613, Universit6 de Montreal, Quebec, Canada, August."
"Hongyan Jing and Kathleen R. McKeown. 1999. matically analyze a large quantity of human-written abstracts, we developed a decomposition program."
The automatic decomposition allows us to build large corpora for studying sentence reduction and
The decomposition of human-written summary sentences.
In Proceedings of the P2nd
"In-ternational ACM SIGIR Conference on Re-sentence combination, which are two effective op- search and Development in Information Re- erations in cut and paste."
"We developed a sentence trieval(SIGIR&apos;99), pages 129-136, University of"
"Berkeley, CA, August. reduction module that makes reduction decisions us-ing multiple sources of knowledge."
We also investi-
Hongyan Jing. 2000.
Sentence reduction for au-gated possible sentence combination operations and tomatic text summarization.
In Proceedings of implemented the combination module.
A sentence[REF_CITE]. extraction module was developed and used as the Aravind.K. Joshi. 1987.
Introduction to tree-front end of the summarization system. adjoining grammars.
"In A. Manaster-Ramis, ed-"
We are preparing the task-based evaluation of the overall system.
"We also plan to evaluate the porta- itor, Mathematics of Language."
"John Benjamins, Amsterdam."
We will also extend the system to query-based sum-marization and investigate whether the system can be modified for multiple document summarization.
Automatic generation of text summaries for spoken language faces the problem of containing incorrect words and passages due to speech recognition er-rors.
This paper describes comparative experiments where passages with higher speech recognizer confi-dence scores are favored in the ranking process.
Re-sults show that a relative word error rate reduction of over 10% can be achieved while at the same time the accuracy of the summary improves markedly.
"The amount of audio data on-line has been grow-ing rapidly in recent years, and so methods for ef-ficiently indexing and retrieving non-textual infor-mation have become increasingly important (see, e.g., the TREC-7 branch for &quot;Spoken Document Re-trieval&quot;[REF_CITE])."
One way of compressing audio information is the automatic creation of textual summaries which can be skimmed much faster and stored much more effi-ciently than the audio itself.
There has been plenty of research in the area of summarizing written lan-guage (see[REF_CITE]for a compre-hensive overview).
"So far, however, very little atten-tion has been given to the question how to create and evaluate a summary of spoken audio based on automatically generated transcripts from a speech recognizer."
"One fundamental problem with those summaries is that they contain incorrectly recog-nized words, i.e., the original text is to some extent &quot;distorted&quot;."
"Several research groups have developed interac-tive &quot;browsing&quot; tools, where audio (and possibly video) can be accessed together with various types of textual information (transcripts, summaries) via a graphical user interface[REF_CITE]."
"With these tools, the problem of misrecognitions is alleviated in the sense that the user can always easily listen to the audio recording corresponding to a passage in a textual summary."
"In some instances, however, this approach may not be feasible or too expensive to pursue, and a short, stand-alone textual repre- sentation of the spoken audio may be preferred or even required."
"This paper addresses in particular this latter case and (a) explores means of making textual summaries less distorted (i.e., reducing their word error rate (WElt)), and (b) assesses how the accuracy of the summaries changes when methods for word error rate reduction are applied."
Summary accuracy will be a function of how much relevant information is present in the sun&apos;mary.
Our results from experiments on four television shows with multiple speakers show that it is possi-ble to reduce word error rate while at the same time also improving the accuracy of the summary.
"Fur-thermore, this paper presents a novel method for evaluation of textual summaries from spoken lan-guage data. . . . ."
"The paper is organized as follows: In the next : section, we review related work on spoken language summarization."
In section 3 we describe our sum: marizer.
"Next, we present and discuss our proposal for an audio summarization evaluation metric (sec-tion 4)."
In section 5 we describe the Corpus that we use for our experiments and how it was annotated.
Sections 6 and 7 describe experixnents on both hu.... man and machine generated transcripts of the audio data.
"Finally, we discuss and summarize the results in sections 8 and 9."
"In a question-answer test with summaries of five dialogues, subjects could identify most of the key concepts using a summary size of only five turns."
"However, the results vary widely across five different dialogues tested in this experiment (between 20% and 90% accuracy).[REF_CITE]went one step further and report that they were able to reduce the word error rate in summaries (as opposed to full texts) by using speech recognizer confidence scores."
They combined inverse frequency weights with confidence scores for each recognized word.
Using summaries composed of one 30-gram per minute (approximately 15% length ranked (tr).
"As &quot;query&quot; we use a frequency vector of the fulltext),the WER dropped from 25% for for all content words within a topical segment."
The the fulltext to 10% forthese summaries.
"They also A-parameter (0.0 &lt; A &lt; 1.0) is used to trade off the conducted a qualitativestudy where human subjects influenceof Ca) vs. (b). were given summaries of n-grams of different length Both similarity metrics (sire1, sire2) are inner and also summaries with speaker utterances as min- vector products of (_stemmed) term frequencies (see imal units, either giving a high weight to the inverse equations 2 to 4); tft is a vector of stem frequencies frequency scores or to the confidence scores."
"The in a turn; f, are in-segment frequencies of a stem; utterance summaries were considered best, followed f, rna= are maximal segment frequencies of any stem closely by 30-gram summaries, both using high con- in the topical segment, sirnl can be normalized or fidence score weights."
This suggests that not only not.
"The formulae for tfa (equation 4) are inspired does the WER drop by extracting passages that are from Cornell&apos;s SMART system[REF_CITE]; we more likelyto be correctlyrecognized but also do will call these parameters &quot;smax&apos;, &quot;log&quot;, and ¢&apos;freq&quot;, summaries seem to be &quot;better&quot;which are generated respectively. that way."
"While the results[REF_CITE]are in- dicative for theirapproach, we want to investigate the benefits of using speech recognizer confidence scoresin more detailand particularlyfindout about the trade-off between WER and summarization ac-curacy when we vary the influenceof the confidence scores."
"To our knowledge, this paper addresses this trade-off for the first time in a clear, numerically de-scribable way."
"To be able to obtain numerical values for summary accuracy, we had our corpus annotated for relevance (section 5) and devised an evaluation scheme that allows the calculation of summary ac-curacy for both human and machine generated tran-scripts (section4)."
"Prior to summarizing, the input text is cleaned up for disfluencies,such as hesitations,filledpauses, neztturn = argmax(Asima(tn,j,query) tnr,~ -([Footnote_1] - A) maxsim2 (tnrd, tr,t~)) (1) tr, k siml : tf~tft or [tf=tit~ (2) I /I -I tftztft~"
1More details about this component and other parts of the 3The human reference is considered to be an &quot;optimal&quot; or
"I II I tfi,, = 0.5 + 0.5 /~&quot; or 1 + logfi., or ~,, (4)"
"Using the MMR algorithm, we obtain a listof ranked turns for each topicalsegment."
We com-and repetitions.
"I In the context of multi-topical pute this both for human and machine generated recordings we use for our experiments, summaries transcriptsof the audio files(&quot;referencetext&quot; vs. are generated for each topicalsegment separately. &quot;hypothesistext&quot;).3"
The segment boundaries were determined to be at those placeswhere the majority Cat leasthalf)of the Evaluation metrics4 human annotators agreed (see section 5).
The challenge of devising a meaningful evaluation agreement for topical boundaries is fairly good (and metric for the task of audio summarization is that higher than the agreement on relevant words or pas- it has to be applicable to both the reference (hu-sages).2 man transcript)and the hypothesis transcripts(au-
"To determine the content of the summaries, we tomatic speech recognizer (ASR) transcripts)."
"We use a &quot;maximal marginal relevance&quot; (MMR) based want to be able to assess the qualityof the sum-summarizer with speaker turns as minimal units (cf. mary with respect to the relevancemarkings of the[REF_CITE]). human annotators (seesection5), as well as to re-"
The MMR formula is given in equation 1.
It gen- latethis&quot;summary accuracy&quot; to the word errorrate erates a list of turns ranked by their relevance and presentin the ASR transcripts. states that the next turn to be put in this ranked list will be taken from the turns which were not yet
The approach we take isto align the words in the summary with the words in the referencetranscript ranked (tar) and has the following properties: it is (wa).
"For ASR transcripts,word substitutionsare (a) maximally similar to a &quot;query&quot; and (b) max- aligned with their &quot;true original&quot; and word inser-imally dissimilar to the turns which were already tions are aligned with a NIL dummy."
"That way, we can determine for each individual word wa in the summary (a) whether it occurs in a &quot;relevant phrase&quot; and (b) whether it is correctly recognized or a recognition error (for ASR transcripts)."
We define word error rate as WER = (S + I + D)/(S +
"I + C) (I=insertion, D=deletion, S=substitution, C=correct)."
Each word&apos;s relevance score r is the average num-ber it occurs in the human annotators&apos; relevant phrases (0.0 &lt; r &lt;_. 1.0).
Relevance scores for in-sertions and substitutions are always 0.0.
"We choose to define the summary accuracy sa (&quot;relevance&quot;) as the sum of relevance scores of all n aligned words ~--~° r~, divided by the maximum achievable relevance score with the same number of n words somewhere in the text (i.e., 0.0 &lt; sa &lt;_ 1.0)."
"Word deletions obviously do not show up in the sum-mary, but are accounted for, as well, to make the WER computation sound."
"To better illustrate how these metrics work, we demonstrate them on a simplified example of only two speaker turns (Figure 1)."
The first line repre-sents the relevance score r for each word (the number this word was within a &quot;relevant phrase&quot; divided by the number of annotators for that text).
"In turn 1, &quot;this is to illustrate&quot; was only marked relevant by two annotators, whereas &quot;the idea&quot; by 3 out of 4 annotators."
"The second line provides the reference transcript, the third line the ASB. transcript."
"Line 4 gives the type of word error, and line 5 the con-fidence score of the speech recognizer (between 0.0 and 1.0, 1.0 meaning maximal confidence)."
Now let us assume that turn 2 shows up in the summary.
"The scores are computed as follows: • When summarizing the reference: Here, the word error rate is trivially 0.0; the summary accuracy sa is the sum of all relevance scores (-6.0) divided by the maximal achievable score with the same number of words (n = 7). l&quot;hrn 2 has 6 words which were marked relevant by all coders (r -- 1.0), turn l&apos;s highest score is r = 0.75."
Therefore: sa2 = 6.0/(6.0 + 0.75) = 0.89.
This is higher than the summary accuracy for turn 1: sal = 3.5/6.0 = 0.58(n = 6). • When summarizing the ASR transcript (&quot;hy-pothesis&quot;): Selecting turn 2 will give sa2 = 0.0•2.25 = 0.0 (n = 5).
"For turn 1, sal = 2.25/(0.75 + 0.5 + 0.5 + 0.5 + 0.0 + 0.0) = 1.0 (n = 6; the sum in the denominator can only use relevance scores based on the aligned words wa which were correctly recognized, therefore the 1.0-scores in turn 2 cannot be used)."
"Turn 2 has WER=6[5=l.2, turn 1 has WER=3/6=0.5."
"Obviously, when summarizing the ASB. output, we would rather have turn 1 showing up in the summary than turn 2, because turn 2 is completely off from the truth and turn 1 only partially."
"The fact that turn 2 was considered to be more relevant by human coders cannot, in our opinion, be used to favor its inclusion in the summary."
An exception would be a situation where the user has immediate access to the audio as well and is able to listen to selected passages from the summary (see section 1).
"In our case, where we focus on text-only summaries to be used stand-alone, we have to minimize their word error rate."
"Given that, turn 1 has to be favored over turn 2, both because of its lower WEB, and because of its higher accuracy with respect to the relevance anno-tations."
"In order to increase the likelihood that turns with lower WEB, are selected over turns with higher WEB., we make use of the speech recognizer&apos;s con-fidence scores which are attached to every word hy-pothesis and can be viewed as probabilities: they are in [0.0,1.0], high values reflecting a high confidence in the correctness of the respective word.[Footnote_4] Follow-ing[REF_CITE]we conjecture that we can use these confidence scores to increase the probabil-ity of passages with lower WEB, to show up in the summary."
"4The speech recognizer computes these scores based on the acoustic stability of words during lattice rescoring. 5For EXP, we define 0° ----O."
"To test how far this assumption is justi-fied, we correlated the WEB. with various metrics of confidence scores: (i) sum of scores, (ii) average of scores, (iii) number of scores above a threshold, (iv) the latter normalized by the number of all scores, and (v) the geometric mean of scores."
Table 1 shows the correlation coefficients (Pearson r) for the four ASK transcripts we used in our experiments (see sec-tion 5).
"To prevent the influence of large differences in turn length, those computations were done for subsequent &quot;buckets&quot; of 50 words each."
"Since in most cases we achieve the highest corre-lation coefficient (absolute value) for method (iv = avgth) (average number of words whose confidence score is greater than a threshold of 0.95), we apply this metric to the computation of turn-query similar-ities (sire1 in equation 1)."
"We use the two following formulae to adjust the similarity,scores. (We shall call these adjustments MULTand EXP in the follow-ins.) [mutt] sirn~ ="
Siml (1 + aavgth) (5) [ezp] sim&apos;l&apos; = simlavgth ~ (6)
"For both equations it holds that if a = 0.0, the scores don&apos;t change, whereas if c~ &gt; 0.0, we en-hance the weights of turns with many high confi-dence scores (&quot;boosting&quot;) and hence increase their likelihood of showing up earlier in the summary. 5"
"Even though our evaluation method looks like it would &quot;guarantee&quot; an increase in summary accu- racy when the word error rate is reduced, this is in not necessarily the case."
"For example, it could turn a summary for each topical region."
"Those phrases usually do not coincide exactly with speaker turns out that while we can reduce WER by &quot;boosting&quot; and the annotators were encouraged to mark sec-passages with higher confidence scores, those pas- tions of text freely such that they would form mean-sages might have (much) fewer words marked rele- ingful, concise, and informative phrases."
Three an- • vant than those being present in the summary with- ~ notators could listen to the audio while annotat-out boosting.
"This way, it would be conceivable to ing the corpus, the other three only had the hu-create low word error summaries that contain also man generated transcripts available. 2 of the 6 an-very few relevant pieces of information."
"However, as notators only finished the NewsHour data, so we we will see later, WER reduction goes hand in hand have the opinion of 4 annotators for the recordings with an increase of summary accuracy."
BUCHANANand GRAYand of 6 annotators[REF_CITE]CENT. 5 Data characteristics and annotation 6 Table 2 describes the main features of the corpus we used for our experiments: we selected four audio ex-
Experiments on human generated transcripts
"We created summaries of the reference transcripts cerpts from four television shows, together with hu- using different parameters for the MMR computa-man generated textual transcripts."
All these shows tion:
"For tf we used &quot;freq&quot;, &quot;log&quot;, and &quot;smax&quot;; fur-are conversations between multiple speakers."
"The ther, we did or did not normalize these weights; fi-audio was sampled at 16kHz and then also automat- nally, we varied the MMR-A from 0.85 to 1.0."
"Sum-ically transcribed using a gender independent, vo- marization accuracy was determined at 5%, 10%, cal tract length normalized, large vocabulary speech 15%, 20%, and 25% of the text length of each sum-recognizer which was trained on about 80 hours of marized topical segment and then averaged over all Broadcast News data[REF_CITE]."
The average sample points in all segments.
"Since these were word error rates for our 4 recordings ranged from word-based lengths, words were added incrementally 25% to 50%. to the summary in the order of the turns ranked via"
The reference transcripts of the four recordings MMR; turns were cut off when the length limit was were given to six human annotators who had to seg- reached.
"As explained in the example in section 4, ment them into topically coherent regions and to de- the accuracy score is defined as the fraction of the cide on the &quot;most relevant phrases&quot; to be included sum of all individual word relevance scores (as de-"
Table 3: Reference summarization accuracy of MMR summaries termined by human annotators) over the maximum possible score given the current number of words in the summary.
"Table 3 shows the summary accuracy results for the best parameter setting (if=log, no normaliza-tion) ~."
"Using the same summarizer as before, we now cre-ated summaries from ASR transcripts."
"Addition-ally to the summary accuracy, we evaluate now also the WER for each evaluation point."
"Again, we ran a series of experiments for different parameters of the MMR formula (if=log, smax, freq; with/without normalization)."
"As before, we achieved the best re-sults for non normalized scores and tf=log."
We var-ied a from 0.0 to 10.0 to see how much of an effect we would get from the &quot;boosting&quot; of turns with many high confidence scores (see equations 5 and 6).
"The ExP formula yielded better results than MULl? (Table 4), the optimum for ExP was reached for = 3.0 with a[REF_CITE].6%, an absolute improve-ment of over 8% over the average of WER=35.1% for the complete ASR transcripts (non-summarized)."
"The summarization accuracy peaks at 0.47, a 9% absolute improvement over the a = 0.0-baseline and only about 5% absolute lower than for reference sum-maries (Table 4 and Figure 2)."
"When we compare the baseline of ~ = 0.0 (i.e., no &quot;boosting&quot; of high confidence turns) to the best re-sult (a = 3.0), we see that the WER drops markedly by about 12% relative from 30.1 to 26.6%."
"At the same time, the summarization accuracy increases by about 18% relative form 0.401 to 0.472."
"Results for the MULT formula confirm this trend, but it is considerably weaker: approximately 6% WER reduction and 14% accuracy improvement for c~= 10.0 over the c~= 0.0 baseline."
An appendix (section 11) provides an example of actual summaries generated by our system for the first topical segment of the BACK conversation.
It illustrates how WER reduction and summary ac-curacy improvement can be achieved by using our confidence boosting method.
"The most significant result of our experiments is, in our opinion, the fact that the trade-off between word and summary accuracy indeed leads to an op-timal parameter setting for the creation of textual summaries for spoken language (Figure 2)."
"Using a formula which emphasizes turns containing many high confidence scores leads to an average WER re-duction of over 10% and to an average improvement in summary accuracy of over 15%, compared to the baseline of a standard MMR-based summary."
"Comparing our results to those reported[REF_CITE], we find that their relative"
I IBm°K19C[ NT BUOHAANIOAYI avgth -0.79 -0.11 -0.43 -0.03
"This work was funded in part by ATR - Inter- preting Telecommunications Research Laboratories of Japan, and the US Department of Defense."
Jaime Carbonell and Jade Goldstein. 1998.
WER reduction for summaries over full texts was considerably larger than ours (60% vs. 24%).
"We conjecture that reasons for this may be due to the different nature and quality of the confidence scores, and (not unrelated), to the different absolute WER of MMR, diversity-based reranking for reordering documents and producing summaries."
"In Proceed-ings of the ~Ist ACM-SIGIR International Con-ference on Research and Development in Informa-tion Retrieval, Melbourne, Australia."
"John S. Garofolo, Ellen M. Voorhees, Cedric G. P. of the two corpora (25% vs. 35%): in transcripts with higher WER, the confidence scores are usually less reliable (eft Table 1)."
"Looking at the four audio recordings individually, we see that the improvements vary strongly across different recordings."
We conjecture that one reason
"Auzanue, and Vincent M. Stanford. 1999."
Spoken document retrieval: 1998 evaluation and investi-gation of new metrics.
"In Proceedings of the ESCA workshop: Accessing information in spoken audio, pages 1-7."
"Cambridge, OK, April."
"J. J. Godfrey, E. C. Holliman, and J. McDaniel. for this fact may be due to the high variation in the correlation between WER and confidence scores on a turn basis (Table 5)."
"This would explain why, e.g., BACK&apos;S improvements are much stronger than 1992."
SWITCHBOARD: telephone speech corpus for research and development.
"In Proceedings of the ICASSP-9~, volume 1, pages 517-520."
"Julia Hirschberg, Steve Whittaker, Don Hindle, Fer- those of the BUCHANANrecording or why there are no improvements for the 19CENT recording."
"How-ever, GRAY does improve despite its very low abso-lute correlation. 9 Summary nando Pereira, and Amit Singhal. 1999."
Finding information in audio: A new paradigm for audio browsing/retrieval.
"In Proceedings of the ESCA workshop: Accessing information in spoken audio, pages 117-122."
"Cambridge, OK, April."
"Inderjeet Mani and Mark T. Maybury, editors. 1999."
"In this paper, we presented experiments on sum-maries of both human and machine generated tran- scripts from four recordings of spoken language."
Advances in automatic text summarization.
"MIT Press, Cambridge, MA."
"Gerard Salton, editor. 1971."
The SMART Retrieval explored the trade-offof word accuracy vs. summary accuracy (relevance) using speech recognizer confi-dence scores to rank passages with lower word error rate higher in the summarization process.
System -- Experiments in Automatic Text Pro-cessing.
"Prentice Hall, Englewood Cliffs, New Jer-sey."
"Robin Valenza, Tony Robinson, Marianne Hickey,"
"Results comparing our approach to a simple MMR ranking show that while the WER can be reduced by over 10%, summarization accuracy improves by over 15% as measured against transcripts with relevance annotations. and"
Roger Tucker. 1999.
Summarisation of spo-ken audio through informationextraction.
"In Pro-ceedings of the ESCA workshop: Accessing in-formation in spoken audio, pages 111-116."
"Cam-bridge, OK, April."
"Alex Waibel, Michael Bett, and Michael Finke. 10 Acknowledgements We thank the six human annotators for their tedious work of annotating the corpus with topical segment boundaries and relevance information."
We also want 1998.
Tracking and summa-rizing meetings.
In Proceedings of the DARPA Broadcast News Workshop.
"Hua Yu, Michael Finke, and Alex Waibel. 1999. to thank Alon Lavie and the three anonymous re-viewers for useful feedback and comments on earlier drafts of this paper."
Progress in automatic meeting transcription.
"In Proceedings of EUROSPEECH-99, Budapest, Hungary, September. life."
Figure 3 shows the human transcript of this seg-ment which is about two minutes long and con-sists of 5 speaker turns.
"Figure 4 contrasts the machine generated summaries for this segment (a) without confidence boosting (a -- 0.0) and (b) using the optimal confidence boosting (c~ = 3.0, method ExP)."
Insertions and substitutions are capitalized and marked with I- or S- prefixes.
Table 6 compares the relative summary accuracies (sa) and word error rates (WER in %) for these two summaries (aver-age over the 5 sample points from 5% to 25% sum-mary length).
"Additionally, the turns that show up in the summaries are listed in their ranking order."
"Table 7 provides the average relevance scores, word error rates, and confidence scores (&quot;avgth&quot;) for each turn of this topical segment."
"We observe that the most relevant turn is turn 1 which has, incidentally, also the lowest WER."
"Whereas in case (a) (o = 0.0), turn 2 is ranked first and therefore dominates the lower relevance and"
Templates are the easiest way to implement surface
We present three systems for surface natural lan-
A template for describing a flight noun guage generation that are trainable from annotated phrase in the air travel domain might be flight corpora.
"The first two systems, called NLG[Footnote_1] and departing from $city-fr at $time-dep and NLG2, require a corpus marked only with domain- arriving in $city-to at $time-arr where the specific semantic attributes, while the last system, words startingwith &quot;$&quot; are actually variables--called NLG3, requires a corpus marked with both representingthe departure city,and departure time, semantic attributes and syntactic dependency infor- the arrivalcity,and the arrivaltime, respectively--mation."
All systems attempt to produce a grammat- whose valueswillbe extracted from the environment ical natural language phrase from a domain-specific in which the template is used.
The approach of semantic representation.
"NLG[Footnote_1] serves a baseline writing individualtemplates isconvenient,but may system and uses phrase frequencies to generate a not scale to complex domains in which hundreds whole phrase in one step, while NLG2 and NLG3 or thousands of templates would be necessary, and use maximum entropy probability models to indi- may have shortcomings in maintainability and text vidually generate each word in the phrase."
"The sys- quality (e.g., see[REF_CITE]for a discussion). tems NLG2 and NLG3 learn to determine both the There are more sophisticated surface genera-word choice and the word order of the phrase."
"We tion packages, such as FUF/SURGE (Elhadad and present experiments in which we generate phrases[REF_CITE]), KPML[REF_CITE], MUMBLE describe flights in the air travel domain."
The surface generation model NLG1 simply chooses the most frequent template in the training data that corresponds to a given set of attributes.
Its perfor-mance is intended to serve as a baseline result to the more sophisticated models discussed later.
"Specifi-cally, nlgl(A) returns the phrase that corresponds to the attribute set A: nlgl(A) = { argInaXphraseeTA[e string mpyt ] C(phrase, A) TATA= where TA are the phrases that have occurred with A in the training data, and where C(phrase, A) is the training data frequency of the natural language phrase phrase and the set of attributes A. NLG1 will fail to generate anything if A is a novel combi-nation of attributes."
The surface generation system NLG2 assumes that the best choice to express any given attribute-value set is the word sequence with the highest probabil-ity that mentions all of the input attributes exactly once.
"When generating a word, it uses local infor-mation, captured by word n-grams, together with certain non-local information, namely, the subset of the original attributes that remain to be generated."
"The local and non-local information is integrated with use of features in a maximum entropy prob-ability model, and a highly pruned search procedure attempts to find the best scoring word sequence ac-cording to the model."
"The probability model in NLG2 is a conditional dis-tribution over V U * stop,, where V is the genera-tion vocabulary and where .stop. is a special &quot;stop&quot; symbol."
The generation vocabulary V consists of all the words seen in the training data.
"The form of the maximum entropy probability model is identical to the one used[REF_CITE]: k f$(wi ,wi-1 ,wi-2,at~ri) YIj=I Otj p(wilwi-l, wi-2,attri) = Z(Wi-l, wi-2, attri) k tot j=l where wi ranges over V t3 .stop. and {wi-l,wi-2,attri} is the history, where wi de-notes the ith word in the phrase, and attri denotes the attributes that remain to be generated at posi-tion i in the phrase."
"The fj, where fj(a, b) E {0, 1}, are called features and capture any information in the history that might be useful for estimating p(wi[wi-1, wi-2, attri)."
"The features used in NLG2 are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm[REF_CITE], are set to maximize the likelihood of the training data."
"The probability of the sequence W = wl ... wn, given the attribute set A, (and also given that its length is n) is:"
"Pr(W = wa...wnllen(W) = n,A) = n H p(wilwi_1, wi_2, attri ) i=l"
"The feature patterns, used in NLG2 are shown in Table 3."
"The actual features are created by match-ing the patterns over the training data, e.g., an ac-tual feature derived from the word bi-gram template might be: 1 if wi = from f(wi, Wi--1,Wi--2,attr~) = and wi-t = flight and $city -- fz E attri 0 otherwise also discarded."
"When the search terminates, there will be at most"
"The search procedure attempts to find a word se- N completed sequences, ofpossibly differinglengths. quence wl ... wn of any length n ~ M for the input Currently, there is no normalization for different attribute set A such that 1. wn is the stop symbol ,stop, lengths, i.e., all sequences of length n &lt; M are equiprobable: 2."
All of the attributes in A are mentioned at least Pr(len(W) = n) = -~ n &lt; M once =0 n&gt;M 3.
All of the attributes in A are mentioned at most once
NLG2 chooses the best answer to express the at- and where M is an heuristically set maximum phrase tribute set A as follows: length.
"The search is similar to a left-to-right breadth- nlg2(A) = argmaXwew,,g2 Pr(len(W) = n) . first-search, except that only a fraction of the word"
"Pr(Wllen(W ) = n, A) sequences are considered."
"More specifically, the search procedure implements the recurrence:"
"WN,1 = top(N, (wlw e V}) Wg,i+l = top(N, next(WN,i)) where Wnt~2 are the completed word sequences that satisfy the conditions of the NLG2 search described above."
"The set WN# is the top N scoring sequences of NLG3 addresses a shortcoming of NLG2, namely length i, and the expression next(WN,i) returns that the previous two words are not necessarily the all sequences wl...Wi+l such that wl...wi E best informants when predicting the next word."
"WN,i, and wi+l E V U .stop.."
"The expression stead, NLG3 assumes that conditioning on syntacti-top(N, next(WN#)) finds the top N sequences in cally related words in the history will result on more next(Wg,i)."
"During the search, any sequence that accurate surface generation."
"The search procedure ends with ,stop. is removed and placed in the set in NLG3 generates a syntactic dependency tree from"
"Description Feature f(wi, Wi-1, Wi-2, attri) .... top-to-bottom instead of a word sequence from left-to-right, where each word is predicted in the context of its syntactically related parent, grandparent, and siblings."
NLG3 requires a corpus that has been an-notated with tree structure like the sample depen-dency tree shown in Figure 1.
"The probability model for NLG3, shown in Figure 2, conditions on the parent, the two closest siblings, the direction of the child relative to the parent, and the attributes that remain to be generated."
"Just as in NLG2, p is a distribution over V t2 .stop,, and the Improved Iterative Scaling algo-rithm is used to find the feature weights aj."
"The expression chi(w) denotes the ith closest child to the headword w, par(w) denotes the parent of the headword w, dir E {left, right} denotes the direc-tion of the child relative to the parent, and attrw,i denotes the attributes that remain to be generated in the tree when headword w is predicting its ith child."
"For example, in Figure 1, if w =&quot;flights&quot;, then Chl(W) =&quot;evening&quot; when generating the left children, and chl(w) =&quot;from&quot; when generating the right children."
"As shown in Figure 3, the proba-bility of a dependency tree that expresses an at-tribute set A can be found by computing, for each word in the tree, the probability of generating its left children and then its right children.1 In this formulation, the left children are generated inde-pendently from the right children."
"As in NLG2, NLG3 assumes the uniform distribution for the length probabilities Pr(# of left children = n) and Pr(# of right children = n) up to a certain maxi-mum length M&apos; = 10."
The feature patterns for NLG3 are shown in Ta-ble 4.
"As before, the actual features are created by matching the patterns over the training data."
The features in NLG3 have access to syntactic informa-tion whereas the features in NLG2 do not.
Low fre-quency features involving word n-grams tend to be unreliable; the NLG3 system therefore only uses fea-tures which occur K times or more in the training data.
"Furthermore, if a feature derived from Table 4 looks at a particular word chi(w) and attribute a, we only allow it if a has occurred as a descendent of chi(w) in some dependency tree in the training set."
"As an example, this condition allows features that look at chi(w) =&quot;to&quot; and $city-toE attrw,i but dis-allows features that look at ch~(w) =&quot;to&quot; and $city-frE attrw,i."
"The idea behind the search procedure for NLG3 is similar to the search procedure for NLG2, namely, to explore only a fraction of the possible trees by con-tinually sorting and advancing only the top N trees at any given point."
"However, the dependency trees are not built left-to-right like the word sequences in NLG2; instead they are built from the current head (which is initially the root node) in the following order: 1. Predict the next left child (call it xt) 2."
"If it is *stop,, jump to (4) 3. Recursively predict children of xt."
Resume from (1) 4. Predict the next right child (call it Xr) 5.
"If it is *stop*, we are done predicting children for the current head 6. Recursively predict children ofxr."
Resume from (4)
"As before, any incomplete trees that have generated a particular attribute twice, as well as completed trees that have not generated a necessary attribute are discarded by the search."
The search terminates when either N complete trees or N trees of the max-imum length M are discovered.
NLG3 chooses the best answer to express the attribute set A as follows: nlga(A) = argmax Pr(TIA )
TET.Iga where Tntga are the completed dependency trees that satisfy the conditions of the NLG3 search described above.
"The training and test sets used to evaluate NLG1, NLG2 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain."
The annotation scheme used a total of 26 attributes to represent flights.
System Parameters % Correct % OK
The training set consisted of 6000 templates describ-ing flights while the test set consisted of 1946 tem-plates describing flights.
"All systems used the same training set, and were tested on the attribute sets extracted from the phrases in the test set."
"For ex-ample, if the test set contains the template &quot;flights to $city-to leaving at Stime-dep&quot;, the surface gener-ation systems will be told to generate a phrase for the attribute set { $city-to, Stime-dep }."
"The out-put of NLG3 on the attribute set { $city-to, $city-fr, $time-dep } is shown in Table 9."
There does not appear to be an objective auto-matic evaluation method[Footnote_2] for generated text that correlates with how an actual person might judge the output.
2Measuring word overlap or edit distance between the sys-tem&apos;s output and a &quot;reference&quot; set would be an automatic scoring method. We believe that such a method does not accurately measure the correctness or grammaticality of the text.
"Therefore, two judges -- the author and a colleague -- manually evaluated the output of all three systems."
Each judge assigned each phrase from each of the three systems one of the following rankings:
Correct: Perfectly acceptable
"OK: Tense or agreement is wrong, but word choice is correct. (These errors could be corrected by post-processing with a morphological analyzer.)"
Bad: Words are missing or extraneous words are present
No Output: The system failed to produce any out-put
"While there were a total 1946 attribute sets from the test examples, the judges only needed to evalu-ate the 190 unique attribute sets, e.g., the attribute set { $city-fr $city-to } occurs 741 times in the test data."
"Subjective evaluation of generation output is not ideal, but is arguably superior than an auto-matic evaluation that fails to correlate with human linguistic judgement."
"The results of the manual evaluation, as well as the values of the search and feature selection param-eters for all systems, are shown in Tables 5, 6, 7, and 8. (The values for N, M, and K were determined by manually evaluating the output of the 4 or 5 most common attribute sets in the training data)."
"The weighted results in Tables 5 and 6 account for mul-tiple occurrences of attribute sets, whereas the un-weighted results in Tables 7 and 8 count each unique attribute set once, i.e., { $city-fr $city-to } is counted 741 times in the weighted results but once in the un-weighted results."
"Using the weighted results, which represent testing conditions more realistically than the unweighted results, both judges found an im-provement from NLG1 to NLG2, and from NLG2 to NLG3."
NLG3 cuts the error rate from NLG1 by at least 33% (counting anything without a rank of Correct as wrong).
"NLG2 cuts the error rate by at least 22% and underperforms NLG3, but requires far less annotation in its training data."
NLG1 has no chance of generating anything for 3% of the data --it fails completely on novel attribute sets.
"Using the unweighted results, both judges found an improve-ment from NLG1 to NLG2, but, surprisingly, judge A found a slight decrease while judge B found an increase in accuracy from NLG2 to NLG3."
"The un-weighted results show that the baseline NLG1 does well on the common attribute sets, since it correctly generates only less than 50% of the unweighted cases but over 80% of the weighted cases."
"The NLG2 and NLG3 systems automatically at-tempt to generalize from the knowledge inherent in the training corpus of templates, so that they can generate templates for novel attribute sets."
There is some additional cost associated with producing based translation systems. the syntactic dependency annotation necessary for We suspect that our statistical generation ap-
"NLG3, but virtually no additional cost is associated proach should perform accurately in domains of sim-with NLG2, beyond collecting the data itself and ilar complexity to air travel."
"In the air travel do- identifyingthe attributes. main, the length of a phrase fragment to describe"
The trainable surface NLG systems in this pa- an attribute is usually only a few words.
"Domains per differ from grammar-based systems in how they which require complex and lengthy phrase fragments determine the attribute ordering and lexical choice. to describe a single attribute will be more challeng- NLG2 and NLG3 automaticallydetermine attribute ing to model with features that only look at word ordering by simultaneously searching multiple or- n-grams for n E {2,3)."
Domains in which there derings.
"In grammar-based approaches, such pref- is greater ambiguity in word choice will require a erences need to be manually encoded."
"NLG2 and more thorough search, i.e., a larger value of N, at NLG3 solve the lexical choice problem by learning the expense of CPU time and memory."
"Most im-the words (via features in the maximum entropy portantly, the semantic annotation scheme for air probability model) that correlate with a given at- travel has the property that it is both rich enough tribute and local context, whereas (Elhadad et al., to accurately represent meaning in the domain, but 1997) uses a rule-based approach to decide the word simple enough to yield useful corpus statistics."
"Our choice. approach may not scale to domains, such as freely occurring newspaper text, in which the semantic an-"
"While trainable approaches avoid the expense of notation schemes do not have this property. crafting a grammar to determine attribute order- Our current approach has the limitation that it ing and lexicai choice, they are less accurate than ignores the values of attributes, even though they grammar-based approaches."
"For short phrases, ac-might strongly influence the word order and word curacy is typically 100% with grammar-based ap-choice."
"This limitation can be overcome by using proaches since the grammar writer can either cor-features on values, so that NLG2 and NLG3 might rect or add a rule to generate the phrase of interest discover -- to use a hypothetical example -- that once an error is detected."
"Whereas with NLG2 and &quot;flights leaving $city-fr&quot; is preferred over &quot;flights NLG3, one can tune the feature patterns, search pa-from $city-fr&quot; when $city-fr is a particular value, rameters, and training data itself, but there is no guarantee that the tuning will result in 100% gener- such as &quot;Miami&quot;. ation accuracy. 6 Conclusions Our approach differs from the corpus-based surface generation approaches of (Langkilde and This paper presents the firstsystems (known to the[REF_CITE]) and[REF_CITE]. (Langkilde author) that use a statisticallearning approach to and[REF_CITE]) maps from semantics to words produce natural language text directlyfrom a se-with a concept ontology, grammar, and lexicon, and mantic representation."
"Information to solve the ranks the resulting word lattice with corpus-based attribute ordering and lexical choice problems--statistics, whereas NLG2 and NLG3 automatically which would normally be specifiedin a large hand-learn the mapping from semantics to words from a written graxnmar-- is automatically collected from corpus. (Berger et ai., 1996) describes a statistical data with a few feature patterns, and is combined machine translation approach that generates text in via the maximum entropy framework."
NLG2 shows the target language directly from the source text. that using just local n-gram information can out-
"NLG2 and NLG3 are also statistical learning perform the baseline, and NLG3 shows that usingap- proaches but generate from an actual semantic rep- syntactic information can further improve genera-resentation."
This comparison suggests that statis- tion accuracy.
"We conjecture that NLG2 and NLG3 tical MT systems could also generate text from an should work in other domains which have a com- &quot;interlingua&quot;, in a way similar to that of knowledge- plexity similar to air travel,as well as available an- notated data."
"The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work."
This work was supported in part by DARPA Contract #[REF_CITE]-97-C-0012.
"MIMIC employs an initiative module which is decou-pied from its goal selection process, while allowing the outcome of both processes to guide its response strat-egy selection."
"A major advantage of this design is that changes in the system&apos;s strategy selection behavior, and thus its adaptation behavior, can be achieved by modi-fying parameters in the initiative module, while leaving the goal and strategy selection processes intact."
"The ini-tiative module determines initiative distribution based on participant roles, cues observed in the current user ut-terance, and dialogue history."
"Although all three factors may play a role in modifying MIMIC&apos;s adaptation be-havior, we discuss how the effects of participant roles and dialogue history are modified to create systems used as bases of comparison in our experiments."
Participant roles affect the default initiative distribu-
Utterance Task Dialogue[REF_CITE]S:
"Hello, this is MIMIC, the movie information system."
Choices for task are when and where.
"When. [when is] (20) S: Choices for theater are Middlebrook Galeria, Loews East Hanover, Montgomery Cinemas, System System Atlantic Triplex Cinema, and others. (21)"
What theater would you like? (22) U: Montclair. [montclair] (23) S:
Choices for theater are Wellmont Theater and Clearviews Screening Zone.
"This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm,[REF_CITE]:10pm. (27) Please say the name of the movie or theater or town you would like information about. (28) U: Analyze This in Montclair. [analyze this in montclair] (29) S:"
"Choices for task are location, what, where, and when."
Choices for theater are Wellmont Theater and Clearviews Screening[REF_CITE]
"What theater would you like? (35) U: Clearviews Screening Zone. [screening zone] (36) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, Sys~mSystem and 6:30pm. (37) Please say the name of the movie or theater or town you would like information about. tion for MIMIC&apos;s initial utterance."
"For naive users, it logue initiative but never has task initiative."
MIMIC-MI may be more appropriate for MIMIC to employ system- behaves similarly to many existing non-adaptive mixed initiative strategies.
"This system, MIMIC-SI, can be initiative dialogue systems (e.g., ([REF_CITE]; achieved by setting the default initiative distribution to Papineni et&apos; al., 1999))."
Figure 3 shows a dialogue in-provide definite evidence for the system having both task volving MIMIC-MI in which the user again attempts to and dialogue initiatives)
Figure 2 shows a dialogue acquire the same information as in the previous two dia-in which a user interacts with MIMIC-SI to obtain the logues.
MIMIC-SI prompts the to solicit a theater name from the user.
"Instead of provid-user for one piece of information at a time and provides ing helpful information as in (6) in Figure 1, MIMIC-MI (sometimes unnecessary) assistance, in the form of valid relied on the user to change her problem-solving strategy dialogue moves, during each turn. in (50) to obtain the information needed to answer the Taking into account dialogue history allows MIMIC system&apos;s question. to determine the initiative distribution based on the cu-mulative effect of previously observed cues."
This ef- 3 Experimental Design fect can be disabled so that only local dialogue con-text affects the initiative distribution for the next turn.
"Our main goal in evaluating MIMIC is to determine Based on MIMIC&apos;s parameter settings, this adjustment whether users find the mixed initiative and automatic results in MIMIC-MI, a system which always has dia- adaptation aspects of its dialogue strategies useful."
"We compared MIMIC to two control systems: MIMIC-SI 3Based on the Dempster-Shafertheory,if the bpas of the default and MIMIC-MI, since they employ dialogue manage-initiative distributionor those of a detected cue providedefiniteevi-dencefordrawinga certainconclusion,thenno subsequentcueshave ment strategies similar to those in many existing sys-anyeffectonchangingthatconclusion. tems."
The comparison between MIMIC and MIMIC-SI
Hoboken Antz strategy adaptation.
"The following three factors were controlled in our experiments: (a) EasyTask 1. System version: For each experiment, two systems Town Theater Movie Two Times were used: MIMIC and a control system."
"In the first (if playing) (if playing) experiment MIMIC was compared with MIMIC-SI, Millbum Analyze This and in the second experiment, with MIMIC-MI."
Hgts Analyze This Mountainside Analyze
"This Madison True Crime 2. Order: For each experiment, all subjects were ran-"
Hoboken True Crime domly divided into two groups.
"One group per-formed tasks using MIMIC first, and the other group (b) DifficultTask used the control system first. 3."
Task difficulty: 3-4 tasks which highlighted differ- Figure 4: Sample Tasks for Evaluation Experiments ences between systems were used for each experi-ment.
Based on the amount of information to be ac-
Eight subjects4 participated in each experiment.
"Each quired, we divided the tasks into two groups: easy of the subjects interacted with both systems to perform and difficult; an example of each is shown in Fig-ure 4. 4The subjects were Bell Labs researchers, summer students, and their friends."
"Mostof them are computerscientists, electrical engi-"
"Elapsed time (see.) 277.5 0.0162229.5 using one system, they filled out a questionnaire to as-sess user satisfaction by rating 8-9 statements, similar to those[REF_CITE], on a scale of 1-5, where 5 indicated highest satisfaction."
"Approximately two days later, they attempted the same tasks using the other sys-tem.5 These experiments resulted in 112 dialogues with approximately 2,800 dialogue turns."
"In addition to user satisfaction ratings, we automat-ically logged, derived, and manually annotated a num-ber of features (shown in boldface below)."
"For each task/subject/system triplet, we computed the task suc-cess rate based on the percentage of slots correctly filled in on the task worksheet, and counted the # of calls needed to complete each task.6"
"For each call, the user-side of the dialogue was recorded, and the elapsed time of the call was automatically computed."
All user ut-terances were logged as recognized by our automatic speech recognizer (ASR) and manually transcribed from the recordings.
"We computed the ASR word error rate, ASR rejection rate, and ASR timeout rate, as well as # of user turns and average sentence length for each task/subject/system triplet."
"Additionally, we recorded the cues that the system automatically detected from each user utterance."
"All system utterances were also logged, along with the initiative distribution for each system turn and the dialogue acts selected to generate each system response. (b) MIMICvs."
"Table 1: Comparison of Performance Features and task difficulty.7 If no interaction effects emerged, we compared system versions using paired sample t-tests.8 4 Results and Discussion Following the PARADISE evaluation scheme (Walker Based on the features described above, we com- et al., 1997), we divided performance features into four pared MIMIC and the control systems, MIMIC-SI and groups:"
"MIMIC-MI, along three dimensions: performance fea- • Task success: task success rate, # of calls. tures, in which comparisons were made using previously proposed features relevant to system performance (e.g., • Dialogue quality: ASR rejection rate, ASR timeout ([REF_CITE]; Danieli rate, ASR word error rate. and[REF_CITE])); discourse fea- • Dialogue efficiency: # of user turns, elapsed time. tures, in which comparisons were made using character- • System usability: user satisfaction. istics of the resulting dialogues; and initiative distribu-tion, where initiative characteristics of all dialogues in-"
"For both experiments, the ANOVAs showed no inter-volving MIMIC from both experiments were examined. action effects among the controlled factors."
"Tables l(a) 4.1 Performance Features and l(b) summarize the results of the paired sample t-tests based on performance features, where features that For our performance evaluation, we first applied a three-differed significantly between systems are shown in ital-way analysis of variance (ANOVA)[REF_CITE]to ics. 9"
"These results show that, when compared with either each feature using three factors: system version, order, 7User satisfactionwas a per subject as opposed to a per task per-neers, or linguists,and none had prior knowledgeof MIMIC. formance feature; thus, we performed a two-way ANOVAusing the SWeused the exact same set of tasks rather than designingtasks of factorssystemversionandorder. similar difficultylevels because we intended to compare all available 8Thispaperfocuseson evaluatingthe effectofMIMIC&apos;s mixedini-features between the two system versions, including ASR word error tiative and automaticadaptation capabilities."
"We assess theseeffects rate, whichwould havebeen affected by the choice of movie/theater basedon comparisonsbetween systemversionwhenno interactionef-names in the tasks. fects emergedfrom the ANOVAtests usingthe factorssystemversion, 6Althoughthe vast majority of tasks were completed in one call, order,and task difficulty."
"Effectsbased on systemorder and task diffi-somesubjects,whenunabletomakeprogress,didnotchangestrategies culty alonearebeyondthe scopeof this paper. as in (41)-(49)in Figure 3; instead, they hungup and started the task 9Typicallyp&lt;0.05 is considered statistically significant (Cohen, over. 1995). control system, users were more satisfied with MIMIC t° and that MIMIC helped users complete tasks more effi- Discourse Feature MIMIC"
Sl P ciently.
Users were able to complete tasks in fewer turns Cue: TakeOverTask 0 1.84 5 and in a more timely manner using MIMIC.
Cue: AmbiguousActResolved 1.69 4.59 0
"When comparing MIMIC and MIMIC-MI, dialogues &apos;Cue: AmbiguousAction 3 6.59 0.0008 involving MIMIC had a lower timeout rate."
When Avg sentence length (words) 5.45 0.00166.82
Cue: InvalidAction 0.94 0.17381.16
MIMIC detected cues signaling anomalies in the dia-
"Cue: NoNewInfo 1.28 1.38 0.766 logue, it adapted strategies to provide assistance, which in addition to leading to fewer timeouts, saved users time (a) MIMICvs."
MIMIC-SI(n=32) and effort when they did not know what to say.
"In con-trast, users interacting with MIMIC-MI had to iteratively reformulate questions until they obtained the desired in- formation from the system, leading to more timeouts Discourse Feature MIMIC (see (41)-(49) in Figure 3)."
"However, when comparing 2.33Cue: TakeOverTask MIMIC and MIMIC-SI, even though users accomplished Cue: InvalidAction 2.04 tasks more efficiently with MIMIC, the resulting dia- Cue: NoNewlnfo 2.25 logues contained more timeouts."
"As opposed to MIMIC- Cue: AmbiguousActResolved 2.08 SI, which always prompted users for one piece of infor- Avg sentence length (words) 5.26 Cue: AmbiguousAction 4.13"
"MI P 0 0 3.75 0.0011 4.79 0.0161 1.13 0.0297 5.63 0.1771 mation at a time, MIMIC typically provided more open- 4.38 0.8767 ended prompts when the user had task initiative."
"Even though this required more effort on the user&apos;s part in for-mulating utterances and led to more timeouts, MIMIC quickly adapted strategies to assist users when recog-nized cues indicated that they were having trouble."
"To sum up, our experiments show that both MIMIC&apos;s mixed initiative and automatic adaptation aspects re-sulted in better performance along the dialogue efficiency and system usability dimensions."
"Moreover, its adap- • tation capabilities contributed to better performance in terms of dialogue quality."
"MIMIC, however, did not con-tribute to higher performance in the task success dimen-sion."
"In our movie information domain, the tasks were sufficiently simple; thus, all but one user in each experi- • ment achieved a 100% task success rate."
Our second evaluation dimension concerns characteris- • tics of resulting dialogues.
We analyzed features of user utterances in terms of utterance length and cues observed and features of system utterances in terms of dialogue acts.
"For each feature, we again applied a three-way"
"NoNewlnfo: triggered when the user is unable to make progress toward task completion, either when the user does not know what to say or the ASR en-gine fails to recognize the user&apos;s utterance. lnvalidAction/InvalidActionResolved: triggered when the user utterance makes an invalid as-sumption about the domain and when the invalid assumption is corrected, respectively."
"AmbiguousAction/AmbiguousActionResolved: trig-gered when the user query is ambiguous and when the ambiguity is resolved, respectively."
"Tables 2(a) and (b) summarize the results of the paired ANOVA test, and if no interaction effects emerged, we sample t-tests based on user utterance features where fea-performed a paired sample t-test to compare system ver- tures whose numbers of occurrences were significantly sions. different according to system version used are shown in The cues detected in user utterances provide insight italics. 12Table 2(a) shows that users expected the system into both user intentions and system capabilities."
The to adapt its strategies when they attempted to take control cues that MIMIC automatically detects are a subset of of the dialogue.
"Even though MIMIC-SI did not behave those discussed[REF_CITE]: il as expected, the users continued their attempts, resulting in significantly more occurrences of TakeOverTask in di- • TakeOverTask: triggered when the user provides alogues with MIMIC-SI than with MIMIC."
"Furthermore, more information than expected; an implicit indi-the average sentence length in dialogues with MIMIC cation that the user wants to take control of the was only 1.5 words per turn longer than in dialogues l°Therangeofusersatisfactionscoreswas8-40forexperimentone with MIMIC-SI, providing further evidence that users and 9-45forexperimenttwo. ltA subsetofthesecuescorrespondslooselytopreviouslyproposed ~2Sincesystemdialogueacts are often selected based on cues de-evaluation metrics(e.g.,[REF_CITE])."
"However, our tectedin userutterances, we onlydiscussresultsofour user utterance systemautomaticallydetectsthesefeaturesinsteadofrequiringmanual feature analysis,usingdialogueact analysisresultsas additionalsup-annotationbyexperts. portforourconclusions. capabilities."
Comparisons with MIMIC-SI provide ev- (b)DiscourseFeatures idence that MIMIC&apos;s ability to give up initiative better matched user expectations.
"Moreover, comparisons with MIMIC-MI show that MIMIC&apos;s ability to opportunisti-cally take over initiative resulted in dialogues in which Table 3: Correlation Between Task Initiative Distribution anomalies were more efficiently resolved and progress and Features (n=56) toward task completion was more consistently made. 4.3 Initiative Analysis because ASR performance worsens when MIMIC takes over task initiative."
"However, in that case, we would have Our final analysis concerns the task initiative distri-expected the results in Section 4.1 to show that the ASR bution in our adaptive system in relation to the fea-rejection and word error rates for MIMIC-SI are signif-tures previously discussed."
"For each dialogue involving icantly greater than those for MIMIC, which are in turn MIMIC, we computed the percentage of turns in which significantly greater than those for MIMIC-MI, since in MIMIC had task initiative and the correlation coefficient"
MIMIC-SI the system always had task initiative and in (r) between the initiative percentage and each perfor- MIMIC-MI the system never took over task initiative. mance/discourse feature.
"To determine if this correlation To the contrary, Tables l(a) and l(b) showed that the was significant, we performed Fisher&apos;s r to z transform, differences in ASR rejection rate and ASR word error upon which a conventional Z test was performed (Cohen, rate were not significant between system versions, and 1995)."
Table l(b) showed that ASR word error rate for MIMIC-Tables 3(a) and (b) summarize the correlation between MI was in fact quite substantially higher than that for the performance and discourse features and the percent- MIMIC.
"This suggests that the causal relationship is the age of turns in which MIMIC has task initiative, respec-other way around, i.e., MIMIC&apos;s adaptation capabilities tively. 13"
"Again, those correlations which are statistically allowed it to opportunistically take over task initiative significant are shown in italics."
Table 3(a) shows a strong when ASR performance was poor. positive correlation between task initiative distribution Table 3(b) shows that all cues are positively correlated and the number of user turns as well as the elapsed time with task initiative distribution.
"For AmbiguousAction, of the dialogues."
"Although earlier results (Table l(a)) lnvalidAction, and NoNewlnfo, this correlation exists be-show that dialogues in which the system always had task cause observation of these cues contributed to MIMIC initiative tended to be longer, we believe that this corre-having task initiative."
"However, note that AmbiguousAc-lation also suggests that MIMIC took over task initiative tionResolved has a stronger positive correlation with task more often in longer dialogues, those in which the user initiative distribution than does AmbiguousAction, again was more likely to be having difficulty."
"Table 3(a) fur-indicating that MIMIC&apos;s adaptive strategies contributed ther shows moderate correlation between task initiative to more efficient resolution of ambiguous actions. distribution and ASR rejection rate as well as ASR word In brief, our initiative analysis lends additional sup-error rate."
"It is possible that such a correlation exists port to the conclusions drawn in our performance and 13Thistestwasnotperformedforusersatisfaction,sinceusersaris- discourse feature analyses and provides new evidence factionwasapersubjectandnotaperdialoguefeature. for the advantages of MIMIC&apos;s adaptation capabilities."
In addition to taking over task initiative when previously system.
"In Proceedings of the AAAI Spring Sympo-identified dialogue anomalies were encountered (e.g., de- sium on Empirical Methods in Discourse Interpreta-tection of ambiguous or invalid actions), our analysis tion and Generation, pages 34-39. shows that MIMIC took over task initiative when ASR Jean Gordon and Edward H. Shortliffe. 1984."
"The performance was poor, allowing the system to better con-"
Dempster-Shafer theory of evidence.
"In Bruce strain user utterances,t4 Buchanan and Edward Shortliffe, editors, Rule-Based Expert Systems: The MYCIN Experiments of the 5 Conclusions Stanford Heuristic Programming Project, chapter 13, This paper described an empirical evaluation of MIMIC, pages 272-292."
Addison-Wesley. an adaptive mixed initiative spoken dialogue system.
We Diane J. Litman and Shimei Pan. 1999.
Empirically conducted two experiments that focused on evaluating evaluating an adaptable spoken dialogue system.
"In the mixed initiative and automatic adaptation aspects of Proceedings of the 7th International Conference on MIMIC and analyzed the results along three dimensions: User Modeling, pages 55-64. performance features, discourse features, and initiative H. Meng, S. Busayaponchai, J. Glass, D. Goddeau, distribution."
"Our results showed that both the mixed L. Hetherington, E. Hurley, C. Pao, J. Polifroni, initiative and automatic adaptation aspects of the sys- S. Seneff, and V. Zue. 1996."
A conversa-tem led to better performance in terms of user satisfac- tional system in the automobile classifieds domain.
In tion and dialogue efficiency.
"In addition, we found that Proceedings of the International Conference on Spo-MIMIC&apos;s adaptation behavior better matched user expec- ken Language Processing, pages 542-545. tations, more efficientlyresolved anomalies in dialogues, Stefan Ortmanns, Wolfgang Reichl, and Wu Chou. 1999. and led to higher overall dialogue quality."
An efficient decoding method for real time speech recognition.
In Proceedings of the 5th European Con-Acknowledgments ference on Speech Communication and Technology.
"K.A. Papineni, S. Roukos, and R.T. Ward. 1999."
Free-We would like to thank Bob Carpenter and Christine flow dialog management using forms.
"In Proceedings Nakatani for their help on experimental design, Jan van of the 6th European Conference on Speech Communi-Santen for discussion on statistical analysis, and Bob cation and Technology, pages 1411-1414."
Carpenter for his comments on an earlier draft of this pa-
"Patti Price, Lynette Hirschman, Elizabeth Shriberg, and per."
Support for the second author is provided by an NSF
Elizabeth Wade. 1992.
Subject-based evaluation mea-graduate fellowship and a Lucent Technologies GRPW sures for interactive spoken language systems.
"In Pro- grant. ceedings of the DARPA Speech and Natural Language Workshop, pages 34-39."
HMIHY is a spoken dialogue system based on the no-tion of call routing[REF_CITE].
"In the HMIHY call rout-ing system, services that the user can access are classified into 14 categories, plus a category called other for calls that cannot be automated and must be transferred to a human operator[REF_CITE].~ Each category describes a different task, such as person-to-person dialing, or receiving credit for a misdialed number."
"The system determines which task the caller is requesting on the basis of its understanding of the cMler&apos;s response to the open-ended system greeting A T~ T, How May I Help You?."
"Once the task has been determined, the infor-mation needed for completing the caller&apos;s request is obtained using dialogue submodules that are specific for each task[REF_CITE]."
"In addition to the TASKSUCCESS dialogues in which HMIHY successfully automates the cus-tomer&apos;s call, illustrated in Figure 1, and the calls that are transferred to a human operator, there are three other possible outcomes for a call, all of which are problematic."
"The first category, which we call HANGUP, results from a customer&apos;s decision to hang up on the system."
A sample HANGUP dialogue is in Figure 2.
A caller may hang up because s/he is frustrated with the system; our goal is to learn from the corpus which system behaviors led to the caller&apos;s frustration.
"The second problematic category (WIZARD), re-suits from a human customer care agent&apos;s decision to system."
There were a number of agents who partici-pated as wizards during the trial of HMIHY and each wizard was simply told to take over the call if s/he perceived problems with the system&apos;s performance.
"The wizard&apos;s decision was logged by the experimen-tal setup, resulting in labelling the call as one that the wizard took over.s"
A dialogue where the wizard decided that the dialogue was problematic and took over the call is shown in Figure 3.
Sh AT&amp;T How may I help you?
Uh (silence) $2:
Please briefly tell me how I may help you?
U2: I&apos;m trying to call 1 8 hundred call A T T. $3: Do you want to place a call?
"Through my calling card. $4: May I have your card number, please?"
It&apos;s 8 7 6 5 4 3 2 1 0 and then my pin number is 8 7 6 5. (misrecognized) $5:
Please enter or say your card number again.
WIZARD STEPS IN
Figure 3: Sample WIZARD Dialogue
"The third problematic category, the TASKFAILURE dialogues, are cases where the system completed the call, but carried out a task that was not the task that the customer was actually requesting."
An ex-ample TASKFAILUREdialogue is given in Figure 4: HMIHY interpreted utterance U2 as a request to make a third-party call e.g. to bill it to my home phone.
"HMIHY then asked the caller for the infor-mation it needed to carry out this task, the caller complied, and the system completed the call."
"The corpus of 4774 dialogues used in our exper-iments was collected in severM experimental trials of HMIHY on live customer traffic (Pdccardi and Gorin, to appear; E.[REF_CITE]), and is referred to as HM2 in (Riccardi and Gorin, to appear))."
"During the trial, all of the system behav-iors were automatically recorded in a log file, and later the dialogues were transcribed by humans and take over the call from the system."
"Because HMIHY labelled with a semantic category representing the task that the caller was asking HMIHY to perform, on a per utterance basis."
The logfiles also included labels indicating whether the wizard had taken over the call or the user had hung up. 3 Training an Automatic
Problematic Dialogue Predictor
Our experiments apply the machine learning pro-gram RIPPER[REF_CITE]to automatically induce a &quot;problematic dialogue&quot; classification model.
"RIP-PER takes as input the names of a set of classes to be learned, the names and ranges of values of a fixed set of features, and training data specifying the class and feature values for each example in a training set."
Its output is a classification model for predicting the class of future examples.
"In RIPPER, the classifica-tion model is learned using greedy search guided by an information gain metric, and is expressed as an ordered set of if-then rules."
"To apply RIPPER, the dialogues in the corpus must be encoded in terms of a set of classes (the output classification) and a set of input features that are used as predictors for the classes."
"We start with the dialogue categories described above, but since our goal is to develop algorithms that predict/identify problematic dialogues, we treat HANGUP, WIZARD and TASKFAILURE as equivalently problematic."
Thus we train the classifier to distinguish between two classes: TASKSUCCESSand
Note that our categorization is inherently noisy because we do not know the real reasons why a caller hangs up or a wizard takes over the call.
"The caller may hang up because she is frustrated with the system, or she may • Acoustic/ASR Features - recog, recog-numwords, ASR-duration, dtmf-flag, rg-modality, rg-grammar • NLU Features - a confidence measure for all of the possible tasks that the user could be trying to do - salience-coverage, inconsistency, context-shift, top-task, nexttop-task, top-confidence, dill- confidence • Dialogue Manager Features - sys-label, utt-id, prompt, reprompt, confirma-tion, subdial - running tallies: num-reprompts, num-confirms, num-subdials, reprompt%, confir-mation%, subdialogue% • Hand-Labelled Features - tscript, human-label, age, gender, user-modality, clean-tscript, cltscript-numwords, rsuccess • Whole-Dialogue Features num-utts, num-reprompts, percent-reprompts, num-confirms, percent-confirms, num-subdials, percent-subdials, dial-duration."
"There are 8 features that describe the whole dia-logue, and 47 features for each of the first four ex-changes."
We encode features for the first four ex-changes because we want to predict failures before they happen.
"We utilized features logged by the system because they are produced automatically, and thus could be used during runtime to alter the course of the dia-logue."
"The system modules that we collected infor-mation from were the acoustic processer/automatic speech recognizer (ASR) (Riccardi and Gorin, to ap-pear), the natural language understanding (NLU) module[REF_CITE], and the dialogue man-ager (DM)[REF_CITE]."
Below we de-scribe each module and the features obtained from it.
ASR takes as input the acoustic signal and outputs a potentially errorful transcription of what it believes the caller said.
"The ASR features for each of the first four exchanges were the output of the speech recognizer (recog), the number of words in the recognizer output (recog-numwords), the duration in seconds of the input to the recognizer (asr-duration), a flag for touchtone input (dtmf-flag), the input modality expected by the recognizer (rg-modality) (one of: none, speech, touchtone, speech+touchtone, touchtone-card, speech+touchtone-card, touchtone-date, speech+touchtone-date, or none-final-prompt), and the grammar used by the recognizer (rg-grammar)."
The motivation for the ASR features is that any one of them may have impacted performance.
"For example, it is well known that longer utterances are less likely to be recognized correctly, thus asr-duration could be a clue to incorrect recognition re-suits."
"In addition, the larger the grammar is, the more likely an ASR error is, so the name of the grammar vg-grammar could be a predictor of incor-rect recognition."
The natural language understanding (NLU) mod-ule takes as input a transcription of the user&apos;s utter-ance from ASR and produces 15 confidence scores representing the likelihood that the caller&apos;s task is one of the 15 task types.
"It also extracts other relevant information, such as phone or credit card numbers."
There are also features that the NLU module calculates based on processing the utterance.
"These include an intra-utterance measure of the inconsistency be-tween tasks that the user appears to be requesting (inconsistency), a measure of the coverage of the utterance by salient grammar fragments (salience-coverage), a measure of the shift in context between utterances (context-shift), the task with the highest confidence score (top-task), the task with the second to-top confidence scores (diff-confidence)."
The motivation for these NLU features is to make use of information that the NLU module has based on processing the output of ASR and the current dis-course context.
"For example, for utterances that fol-low the first utterance, the NLU module knows what task it believes the caller is trying to complete."
"If it appears that the caller has changed her mind, then the NLU module may have misunderstood a previ-ous utterance."
The context-shift feature indicates the NLU module&apos;s belief that it may have made an error (or be making one now).
The dialogue manager (DM) takes the output of NLU and the dialogue history and decides what it should say to the caller next.
"It decides whether it believes there is a single unambiguous task that the user is trying to accomplish, and how to resolve any ambiguity."
"The DM features for each of the first four exchanges are the task-type label which includes a label that indicates task ambiguity (sys-label), utter-ance id within the dialogue (implicit in the encod-ing), the name of the prompt played before the user utterance (prompt), and whether that prompt was a reprompt (reprompt), a confirmation (confirm), or a subdialogue prompt (subdiaO, a superset of the re-prompts and confirmation prompts."
The DM features are primarily motivated by pre-vious work.
The task-type label feature is to cap-ture the fact that some tasks may be harder than others.
"The utterance id feature is motivated by the idea that the length of the dialogue may be impor-tant, possibly in combination with other features like task-type."
"The different prompt features for initial prompts, reprompts, confirmation prompts and sub-dialogueprompts are motivated by results indicating that reprompts and confirmation prompts are frus-trating for callers and that callers are likely to hy-perarticulate when they have to repeat themselves, which results in ASR errors[REF_CITE]."
"The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num.confirms), and num-ber of subdialogue prompts (num-subdials), that had been played up to each point in the diMogue, as well as running percentages (percent-reprompts, percent-confirms, percent-subdials)."
The use of running tal-lies and percentages is based on the assumption that these features are likely to produce generalized pre-dictors[REF_CITE].
"The features obtained via hand-labelling were hu-man transcripts of each user utterance (tscript), a set of semantic labels that are closely related to the system task-type labels (human-label), age (age) and highest confidence score (nexttop-task), the value of gender (gender) of the user, the actual modality of the highest confidence score (top-confidence), and the user utterance (user-modality) (one of: nothing, the difference in values between the top and next- speech, touchtone, speech+touchtone, non-speech), and a cleaned transcript with non-word noise infor- • Acoustic/ASR Features mation removed (clean-tscript)."
From these features we calculated two derived features.
"The first was the - recog, recog-numwords, ASR-duration, dtmf-number of words in the cleaned transcript (cltscript flag, rg-modality numwords), again on the assumption that utterance • NLU Features length is strongly correlated with ASR and NLU er-rors."
"The second derived feature was based on cal- - salience-coverage, inconsistency, context-shift, top-confidence, dig-confidence culating whether the human-label matches the sys-label from the dialogue manager (rsuccess)."
"There • Dialogue Manager Features were four values for rsuccess: rcorrect, rmismatch, - utt-id, reprompt, confirmation, subdial rpartial-match and rvacuous-match, indicating re-spectively correct understanding, incorrect under- - running tallies: num-reprompts, num-standing, partial understanding, and the fact that confirms, num-subdials, reprompt%, confir-mation%, subdialogue% there had been no input for ASR and NLU to oper-ate on, either because the user didn&apos;t say anything or because she used touch-tone."
"Figure 6: Automatic task independent (AUTO, The whole-dialogue features derived from the per- TASK-INDEP) features available at runtime. utterance features were: num-utts, num-reprompts, percent-reprampts, hum.confirms, percent-confirms, num-subdials, and per-cent-subdials for the whole di- reliable than the error rates from cross-validation. alogue, and the duration of the entire dialogue in seconds (dial-duration)."
"In the experiments, the features in Figure 5 except"
We present results for both predicting and identi-the Hand-Labelled features are referred to as the AU- fying problematic dialogues.
Because we are inter- TOMATIC feature set.
"We examine how well we can ested in predicting that a dialogue will be problem-identify or predict problematic dialogues using these atic at a point in the dialogue where the system can features, compared to the full feature set including do something about it, we compare prediction ac-the Hand-Labelled features."
"As mentioned earlier, curacy after having only seen the first exchange of we wish to generalize our problematic dialogue pre- the diMogue with prediction accuracy after having dictor to other systems."
"Thus we also discuss how seen the first two exchanges, with identification ac-well we can predict problematic dialogues using only curacy after having seen the whole dialogue."
"For features that are both automatically acquirable dur- each of these situations we also compare results for ing runtime and independent of the HMIHY task. the AUTOMATICand AUTO, TASK-INDEP feature sets The subset of features from Figure 5 that fit this (as described earlier), with results for the whole fea-qualification are in Figure 6."
We refer to them as ture set including hand-labelled features.
"Table 1 the AUTO, TASK-INDEP feature set. summarizes the results."
The output of each RIPPER. experiment is a clas-
The baseline on the first line of Table 1 repre-sification model learned from the training data.
"To sents the prediction accuracy from always guess-evaluate these results, the error rates of the learned ing the majority class."
"In 5-fold cross- curacy from simply guessing TASKSUCCESSwithout validation, the total set of examples is randomly di- having seen any of the dialogue yet. vided into 5 disjoint test sets, and 5 runs of the learn-"
The first EXCHANGE 1 row shows the results of ing program are performed.
"Thus, each run uses the using the AUTOMATICfeatures from only the first examples not in the test set for training and the re- exchange to predict whether the dialogue outcome maining examples for testing."
An estimated error will be TASKSUCCESSor PROBLEMATIC.
The results rate is obtained by averaging the error rate on the show that the machine-learned classifier can predict testing portion of the data from each of the 5 runs. problematic dialogues 8% better than the baseline Since we intend to integrate the rules learned after having seen only the first user utterance.
"Using by RIPPER into the HMIHY system, we examine only task-independent automatic features (Figure 6) the precision and recall performance of specific hy- the EXCHANGE 1 classifier can still do nearly as well. potheses."
Because hypotheses from different cross-
"The ALL row for EXCHANGE 1 indicates that even validation experiments cannot readily be combined if we had access to human perceptual ability (the together, we apply the hypothesis learned on one hand-labelled features) we would still only be able randomly selected training set (80% of the data) to to distinguish between TASKSUCCESSand"
PROBLEM-that set&apos;s respective test data.
Thus the precisi[REF_CITE]% accuracy after having seen and recall results reported below are somewhat less the first exchange.
"The EXCHANGE l&amp;2 rows of Table 1 show the re- task (given by the NLU module) is moderate or low suits using features from the first two exchanges in and there was touchtone input in the user utterance. the dialogue to predict the outcome of the dialogue.4 The second rule says that if the difference between The additional exchange gives roughly an additional the top confidence score and the second-ranked con- 7% boost in predictive accuracy using either of the fidence score is moderate or low, and the duration AUTOMATIC feature sets."
"This is only 8% less than of the user utterance is more than 7 seconds, predict the accuracy we can achieve using these features af- PROBLEMATIC. ter having seen the whole dialogue (see below)."
The The performance of these rules is summarized in ALL row for EXCHANGE l&amp;2 shows that we could Table 2.
"These results show that given the first ex-achieve over 86% accuracy if we had the ability to change, this ruleset predicts that 22% of the dia-utilize the hand-labelled features. logues will be problematic, while 36% of them ac-"
The FULL DIALOGUErow in Table 1 for AUTO- tually will be.
"Of the dialogues that actually will MATIC and AUTO, TASK-INDEPfeatures shows the be problematic, it can predict 41% of them."
"Once ability of the classifier to identify problematic dia- it predicts that a dialogue will be problematic, it is logues, rather than predict them, using features for correct 69% of the time."
"As mentioned earlier, this the whole dialogue."
The ALL row for the FULLDI- reflects an overMl improvement in accuracy of 8% ALOGUEshows that we could correctly identify over over the baseline. 92% of the outcomes accurately if we had the ability The rules learned by training on the automatic to utilize the hand-labelled features. task-independent features for exchanges 1 and 2 are
Note that the task-independent automatic fea- given below.
"As in the first rule set, the features that tures always perform within 2% error of the auto- the classifier appears to be exploiting are primarily matic features, and the hand-labelled features con- those from the ASR and NLU modules. sistently perform with accuracies ranging from 6-8% Exchanges l&amp;2, Automatic Task-greater."
The rules that RIPPER learned on the basis of the if (e2-recog-numwords &lt; 0) A (el-cliff-confidence &lt; .95) Exchange 1 automatic features are below. then problematic.
"Exchange 1, Automatic Features: if (el-salience-coverage &lt; .889)"
A (e2-recog contains if (el-top-confidence _&lt; .924)
A (el-dtmf-flag = &apos;1&apos;) &quot;I&apos;)
"A (e2-asr-duration &gt; 7.48) then problematic. then problematic, if (el-top-confidence &lt; .924)"
A (e2-asr-duration &gt;_5.36) if (el-cliff-confidence _&lt;.916)
A (el-asr-duration &gt; 6.92)
"A (el-asr-duration &gt; 8.6) then problematic. then problematic, if (e2-recog is blank)"
A (e2-asr-duration &gt; 2.8) then default is tasksuccess. problematic. if (el-salience-coverage &lt; .737)
A (el-recog contains
"According to these rules, a dialogue will be prob- &quot;help&quot;)"
A (el-asr-duration &lt; 7.04) then problematic. lematic if the confidence score for the top-ranked if (el-cliff-confidence &lt; .924)
A (el-dtmf-flag = &apos;1&apos;)
"A (el-asr-duration &lt; 6.68) then problematic. 4[REF_CITE]% of the dialogues consisted of only two ex- default is tasksuccess. changes, we exclude the second exchange features for those dialogues where the second exchange consists only of the sys-tem playing a closing prompt."
We also excluded any features The performance of this ruleset is summarized in that indicated to the classifier that the second exchange was Table 3.
"These results show that, given the first the last exchange in the dialogue. two exchanges, this ruleset predicts that 26% of the dialogues will be problematic, while 36% of them tures for Exchanges l&amp;2 as shown in Table 1."
The actually will be.
"Of the problematic dialogues, it rules that RIPPER learns for Exchanges 1&amp;52when can predict 57% of them."
"Once it predicts that a the AUTOMATIC, TASK-INDEPfeature set is aug-dialogue will be problematic, it is correct 79% of mented with the single hand-labelled rsuccess fea-the time."
"Compared with the classifier for the first ture is shown below. utterance alone, this classifier has an improvement Exchanges 1~2, Rsuccess -b[REF_CITE]% in recall and 10% in precision, for an overall Task-Independent Features: improvement in accuracy of 7% over using the first ife2-salience-coverage ~ 0.651 A e2-asr-duration &gt;_0.04 exchange alone."
"A e2-rsuccess=Rvacuous-match then problematic,"
"One observation from these hypotheses is the clas- if e2-rsuccess=Rmismatch A el-top-confidence &lt; 0.909 sifier&apos;s preference for the asr-duration feature over then problematic, the feature for the number of words recognized if e2-rsuccess=Rmismatch A e2-context-shift &lt; 0.014"
One would expect longer utter- e2-salience-coverage ~ 0.2
"A e2-recog-numwords &lt; 12 ( ances to be more difficult, but the learned rulesets then problematic, indicate that duration is a better measure of utter- if e2-rsuccess=Rmismatch ^ el-rsuccess=Rmismatch ance length than the number of words."
"Another ob- then problematic, servation is the usefulness of the NLU confidence if e2-rsuccess=Rmismatch A e2-top-confidence &lt; 0.803 scores and the NLU salience-coverage in predicting ^ e2-asr-duration &gt;__2.68 ^ e2-asr-duration &lt; 6.32 then problematic dialogues."
"These features seem to pro- problematic, vide good general indicators of the system&apos;s success if el-rsuccess=Rmismatch A el-diff-confidence &gt; 0.83 in recognition and understanding."
"The fact that the then problematic, main focus of the rules is detecting ASR and NLU if e2-rsuccess=Rmismatch A e2-context-shift &gt;_ 0.54 errors and that none of the DM behaviors are used then problematic, as predictors also indicates that, in all likelihood, the ife2-asr-duration &gt; 5.24 A e2-salience-coverage &lt; 0.833 DM is performing as well as it can, given the noisy A e2-top-confidence &lt; 0.801 A e2-recog-numwords &lt; 7 input that it is getting from ASR and NLU."
"A e2-asr-duration &lt; 16.08 then problematic, To identify potential improvements in the prob- if el-diff-confidence &lt; 0.794"
"A el-asr-duration &gt; 7.2 lematic dialogue predictor, we analyzed which hand-"
"A el-inconsistency &gt; 0.024 A el-inconsistency &gt; 0.755 labelled features made large performance improve- then problematic, ments, under the assumption that future work can default is tasksuccess focus on developing automatic features that ap-"
Note that the rsuccess feature is frequently used in proximate the information provided by these hand- the rules and that RIPPER learns rules that combine labelled features.
"The analysis indicated that the the rsuccess feature with other features, such as the vsuceess feature alone improves the performance of confidence, asr-duration, and salience-coverage fea-the[REF_CITE].5%, as reported in (Langkilde tures. et al., 1999), to 92.3%."
"Using rsuccess as the only feature results in 73.75% accuracy for exchange 1, 5[REF_CITE].9% accuracy for exchanges 18z2 and 85.3% accu-"
"In summary, our results show that: (1) All feature racy for the full dialogue."
"In addition, for Exchanges sets significantly improve over the baseline; (2) Us-l&amp;2, the accuracy of the AUTOMATIC, TASK-INDEP ing automatic features from the whole dialogue, we feature set plus the rsuccess feature is 86.5%, which can identify problematic dialogues 23% better than is only 0.2% less than the accuracy of ALL the lea- the baseline; (3) Just the first exchange provides sig- nificantly better prediction (8%) than the baseline; J. Chu-Carroll and R. Carpenter. 1999."
The second exchange provides an additional sig- based natural language call routing.
"Computa-nificant (7%) improvement, (5) A classifier based on tional[REF_CITE]-3:361-387. task-independent automatic features performs with less than 1% degradation in error rate relative to the automatic features."
"Even with current accuracy rates, the improved ability to predict problematic dialogues means that it may be possible to field the system without human agent oversight, and we ex-pect to be able to improve these results."
The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations.
Our work builds on earlier research on learning to identify di-alogues in which the user experienced poor speech recognizer performance[REF_CITE].
"How-ever, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans."
"In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime."
We are exploring several ways to improve the per-formance of and test the problematic dialogue pre-dictor.
"First, we noted above the extent to which the hand-labelled feature rsuccess improves classifier performance."
"In other work we report results from training an rsuccess classifier on a per-utterance level[REF_CITE], where we show that we can achieve 85% accuracy using only fully automatic fea-tures."
In future work we intend to use the (noisy) output from this classifier as input to our problem-atic dialogue classifier with the hope of improving the performance of the fully automatic feature sets.
"In addition, since it is more important to minimize errors in predicting PROBLEMATICdialogues than er-rors in predicting TASKSUCCESSdialogues, we intend to experiment with RIPPER&apos;S loss ratio parameter, which instructs RIPPER to achieve high accuracy for the PROBLEMATIC class, while potentially reducing"
"S. Seneff, V. Zue, J. Polifroni, C. Pao, L. Hethering-ton, D. Goddeau, and J. Glass. 1995."
The pre-liminary development of a displayless PEGASUS system.
In ARPA SLT Workshop.
"E. Shriberg, E. Wade, and P. Price. 1992."
Human- overall accuracy.
"Finally, we plan to integrate the learned rulesets into the HMIHY dialogue system to improve the system&apos;s overall performance. machine problem solving using spoken language systems (SLS):"
Factors affecting performance and user satisfaction.
"In Proc. of the DARPA Speech and NL Workshop, pages 49-54."
"M. A. Walker, J. C. Fromer, and S. Narayanan."
"Our corpus consists of a set of dialogues between humans and TOOT, an SDS for accessing train schedules from the web via telephone, which was collected to study both variations in SDS strat-egy and user-adapted interacti[REF_CITE]."
"TOOT is implemented on a platform com-bining ASR, text-to-speech, a phone interface, a finite-state dialogue manager, and application func-tions[REF_CITE]."
The speech recognizer is a speaker-independent hidden Markov model system with context-dependent phone models for telephone speech and constrained grammars for each dialogue state.
"Confidence scores for recognition were avail-able only at the turn, not the word, level[REF_CITE]."
An example TOOT dialogue is shown in Fig-ure 1.
"Subjects performed four tasks with one of sev-eral versions of TOOT, that differed in terms of locus of initiative (system, user, or mixed), confirmation strategy (explicit, implicit, or none), and whether these conditions could be changed by the user during the task."
Dia-logues were recorded and system and user behavior logged automatically.
The concept accuracy (CA) of each turn was manually labeled by one of the exper-imenters.
"If the ASR output correctly captured all the task-related information in the turn (e.g. time, departure and arrival cities), the turn was given a CA score of 1 (a semantically correct recognition)."
"Otherwise, the CA score reflected the percentage of"
"We first looked for distinguishing prosodic charac-teristics of misrecognitions, defining misrecognitions in two ways: a) as turns with WER&gt;0; and b) as turns with CA&lt;I. As noted in Section 1, previous studies have speculated that hyperarticulated speech (slower and louder speech which contains wider pitch excursions) may be associated with recognition fail-ure."
"So, we examined the following features for each user turn: 2 • maximum and mean fundamental frequency values (F0 Max, F0 Mean) • maximum and mean energy values (RMS Max, RMS Mean) • total duration • length of pause preceding the turn (Prior Pause) * speaking rate (Tempo) • amount of silence within the turn (% Silence)"
"F0 and I:LMSvalues, representing measures of pitch excursion and loudness, were calculated from the output of Entropic Research Laboratory&apos;s pitch tracker, get_fO, with no post-correction."
Timing vari-ation was represented by four features.
Duration within and length of pause between turns was com-puted from the temporal labels associated with each turn&apos;s beginning and end.
"Speaking rate was ap-proximated in terms of syllables in the recognized string per second, while % Silence was defined as the percentage of zero frames in the turn, i.e., roughly the percentage of time within the turn that the speaker was silent."
These features were chosen based upon previous findings (see Section 1) and observa-correctly recognized task information in the turn. tions from our data.
The dialogues were also transcribed by hand and
"To ensure that our results were speaker indepen-these transcriptions automatically compared to the dent, we calculated mean values for each speaker&apos;s ASR recognized string to produce a word error rate recognized turns and their misrecognized turns for (WEPt) for each turn."
Note that a concept can be every feature.
"Then, for each feature, we created correctly recognized even though all words are not, vectors of speaker means for recognized and misrec-so the CA metric does not penalize for errors that ognized turns and performed paired t-tests on the are unimportant to overall utterance interpretation. vectors."
"For example, for the feature &quot;F0 max&quot;, For the study described below, we examined 1994 we calculated mean maxima for misrecognized turns user turns from 152 dialogues in this corpus."
The and for correctly recognized turns for each of our speech recognizer was able to generate a recognized thirty-nine speakers.
We then performed a paired string and an associated acoustic confidence score t-test on these thirty-nine pairs of means to de-per turn for 1975 of these turns. 1 1410 of these 1975 rive speaker-independent results for differences in F0 turns had a CA score of 1 (for an overall conceptual maxima between correct and incorrect recognitions. accuracy score of 71%) and 961 had a WER of 0 (for
"Tables 1 and 2 show results of these compar-an overall transcription accuracy score of 49%, with isons when we calculate misrecognition in terms of a mean WER per turn of 47%). 2While the features were automatically computed, turn 1For the remaining turns, ASR output &quot;no speech&quot; (and beginnings and endings werehand segmented in dialogue-level TOOTplayed a timeout message) or &quot;garbage&quot; (TOOT played speech files, as the turn-level files created by TOOTwere not a rejection message). available."
"WER&gt;0 and CA&lt;l, respectively."
"These results in- higher in pitch or loudness, or that are longer, or dicate that misrecognized turns do differ from cor- that follow longer pauses, are less likely to be recog-rectly recognized ones in terms of prosodic features, nized correctly than that same speaker&apos;s turns that although the features on which they differ vary are lower in pitch or loudness, shorter, and follow slightly, depending upon the way &quot;misrecognition&quot; shorter pauses -- however correct recognition is de-is defined."
"Whether defined by WER or CA, mis- fined. recognized turns exhibit significantly higher F0 and It is interesting to note that the features we found RMS maxima, longer durations, and longer preced- to be significant indicators of failed recognitions (F0 ing pauses than correctly recognized speaker turns. excursion, loudness, long prior pause, and longer du-"
"For a traditional WER definition of misrecognition, ration) are all features previously associated with misrecognitions are slightly higher in mean F0 and hyperarticulated speech."
Since prior research has contain a lower percentage of internal silence.
"For a suggested that speakers may respond to failed recog- CA definition, on the other hand, tempo is a signif- nition attempts by hyperarticulating, which itself icant factor, with misrecognitions spoken at a faster may lead to more recognition failures, had we in fact rate than correct recognitions -- contrary to our hy- simply identified a means of characterizing and iden-pothesis about the role of hyperarticulation in recog- tifying hyperarticulated speech prosodically? nition error."
"Since we had independently labeled all speaker While the comparisons in Tables 1 and 2 were turns for evidence of hyperarticulation (two of the made on the means of raw values for all prosodic fea- authors labeled each turn as &quot;not hyperarticulated&quot;, tures, little difference is found when values are nor- &quot;some hyperarticulation in the turn&quot;, and &quot;hyperar-malized by value of first or preceding turn, or by con- ticulated&quot;, following[REF_CITE]), we were verting to zscores.3 From this similarity between the able to test this possibility."
"We excluded any turn performance of raw and normalized values, it would either labeler had labeled as partially or fully hy-seem to be relative differences in speakers&apos; prosodic perarticulated, and again performed paired t-tests values, not deviation from some &apos;acceptable&apos; range, on mean values of misrecognized versus recognized that distinguishes recognition failures from success- turns for each speaker."
Results show that for both ful recognitions.
"A given speaker&apos;s turns that are WER-defined and CA-defined misrecognitions, not only are the same features significant differentiators The only differencesoccur for CA defined misrecognition, when hyperarticulated turns are excluded from the where normalizing by first utterance results in significant dif-ferences in mean RMS, and normalizing by preceding turn analysis, but in addition, tempo also is significant results in no significant differencesin tempo. for WER-defined misrecognition."
"So, our findings for the prosodic characteristics of recognized and of first two features to calculate rejections and ask the misrecognized turns hold even when perceptibly hy- user to repeat the utterance, whenever the confi-perarticulated turns are excluded from the corpus. dence score fell below a pre-defined grammar-specific threshold."
The other features represent the exper- 4 Predicting Misrecognitions Using Machine Learning
"Given the prosodic differences between misrecog-nized and correctly recognized utterances in our corpus, is it possible to predict accurately when a particular utterance will be misrecognized or not?"
"This section describes experiments using the ma-chine learning program RIPPER[REF_CITE]to au-tomatically induce prediction models, using prosodic as well as additional features."
"Like many learning programs, RIPPER takes as input the classes to be learned, a set of feature names and possible values, and training data specifying the class and feature imental conditions under which the data was col-lected (whether users could adapt TOOT&apos;s dialogue strategies, TOOT&apos;s initial initiative and confirmation strategies, experimental task, speaker&apos;s name and characteristics)."
"We included these features to de-termine the extent to which particulars of task, sub-ject, or interaction influenced ASR success rates or our ability to predict them; previous work showed that these factors impact TOOT&apos;s performance[REF_CITE]."
"Except for the task, subject, gender, native language, and hyperarticulation scores, all of our features are au-tomatically available. values for each training example."
Table 3 shows the relative performance of a num-a classification model for predicting the class of fu- ber of the feature sets we examined; results here ture examples.
"The model is learned using greedy search guided by an information gain metric, and is expressed as an ordered set of if-then rules."
Our predicted classes correspond to correct recog-nition (T) or not (F).
"As in Section 3, we examine both WER-defined and CA-defined notions of cor-rect recognition, and represent each user turn as a set of features."
"The features used in our learning experiments include the raw prosodic features in Ta-bles 1 and 2 (which we will refer to as the feature set &quot;Prosody&quot;), the hyperarticulation score discussed in Section 3, and the following additional potential pre-dictors of misrecognition (described in Section 2): • ASR grammar • ASR confidence • ASR string • system adaptability • dialogue strategy • task number • subject • gender • native speaker"
"The first three features are derived from the ASR process (the context-dependent grammar used to recognize the turn, the turn-level acoustic confidence score output by the recognizer, and the recognized string)."
"We included these features as a baseline against which to test new methods of predicting are for misrecognition defined in terms of WER. 5 A baseline classifier for misrecognition, predicting that ASR is always wrong (the majority class of F), has an error of 48.66%."
The best performing feature set includes only the raw prosodic and ASR features and reduces this error to an impressive 6.53% +/- .63%.
Note that this performance is not improved by adding manually labeled features or experimen-tal conditions: the feature set corresponding to ALL features yielded the statistically equivalent 6.68% +/- 0.63%.
"With respect to the performance of prosodic fea-tures, Table 3 shows that using them in conjunction with ASR features (error of 6.53%) significantly out-performs prosodic features alone (error of 12.76%), which, in turn, significantly outperforms any single prosodic feature; duration, with an error of 17.42%, is the best such feature."
"Although not shown in the table, the unnormalized prosodic features sig-nificantly outperform the normalized versions by 7- 13%."
"Recall that prosodic features normalized by first task utterance, by previous utterance, or by z scores showed little performance difference in the analyses performed in Section 3."
"This difference may indicate that there are indeed limits on the ranges in features such as F0 and RMS maxima, duration and preceding pause within which recognition per-formance is optimal."
"It seems reasonable that ex-treme deviation from characteristics of the acoustic training material should in fact impact ASR perfor-mance, and our experiments may have uncovered, if not the critical variants, at least important acoustic misrecognitions, although, currently, we know of no correlates of them."
"However, it is difficult to com- ASR system that includes recognized string in its rejection calculations.4 TOOT itself used only the SThe errors and standard errors (SE) result from 25-fold cross-validation on the 1975turns where ASR yielded a string 4Note that, while the entire recognized string is provided and confidence."
"When twoerrorsplus or minus twice the stan-to the learning algorithm, RIPPERrules test for the presence dard error do not overlap, they are statistically significantly of particular words in the string. different. pare our machine learning results with the statisti- nized string."
"This feature is in fact the best perform-cal analyses, since a) the statistical analyses looked ing single feature in predicting our data (15.24%). at only a single prosodic variable at a time, and b) And, at a 95% confidence level, the error using data points for that analysis were means calculated ASR confidence scores, the recognized string, and per speaker, while the learning algorithm operated grammar (9.01%) matches the performance of our on all utterances, allowingfor unequal contributions best performing feature set (6.53%)."
"It seems that, by speaker. at least in our task and for our ASR system, the appearance of particular words in the recognized We now address the issue of what prosodic fea-strings is an extremely useful cue to recognition ac-tures are contributing to misrecognition identifica-curacy."
"So, even by making use of information cur-tion, relative to the more traditional ASR tech-rently available from the traditional ASR process, niques."
Do our prosodic features simply correlate ASR systems could improve their performance on with information already in use by ASR systems identifying rejections by a considerable margin.
"A (e.g., confidence score, grammar), or at least avail-caveat here is that this feature, like grammar state, able to them (e.g., recognized string)?"
"First, the is unlikely to generalize from task to task or recog-error using ASR confidence score alone (22.23%) nizer to recognizer, but these findings suggest that is significantly worse than the error when prosodic both should be considered as a means of improving features are combined with ASR confidence scores rejection performance in stable systems. (10.99%) -- and is also significantly worse than The classification model learned from the best per-the use of prosodic features alone (12.76%)."
"Simi-forming feature set in Table 3 is shown in Figure 2.6 larly, the error using ASR confidence scores and the The first rule RIPPER finds with this feature set is ASR grammar (17.77%) is significantly worse than that if the user turn is less than .9 seconds and the prosodic features alone (12.76%)."
"Thus, prosodic recognized string contains the word &quot;yes&quot; (and possi-features, either alone or in conjunction with tradi-bly other words as well), with an acoustic confidence tional ASR features, significantly outperform these score &gt; -2.6, then predict that the turn will be cor-traditional features alone for predicting WER-based rectly recognized.7 Note that all of the prosodic fea-misrecognitions. 6Rules are presented in order of importance in classifying Another interesting finding from our experiments data."
"When multiple rules are applicable, RIPPER uses the is the predictive power of information available to first rule. current ASR systems but not made use of in calcu- 7The confidence scores observed in our data ranged from lating rejection likelihoods, the identity of the recog- a high of -0.087662 to a low of-9.884418. if (duration &lt; 0.897073)"
A (confidence &gt; -2.62744 )
A (string contains &apos;yes&apos;) then T if (duration &lt; 1.03872 )
A (confidence &gt; -2.69775)
A (string contains &apos;no&apos;) then T if (duration &lt; 0.982051)
A (confidence &gt; -1.99705)
A (tempo &gt; 3.1147) then T if (duration &lt; 0.813633)
A (duration &gt; 0.642652)
A (confidence &gt; -3.33945)
A (F0 Mean &gt; 176.794) then T if (duration &lt; 1.30312)
A (confidence &gt; -3.37301)
A (% silences ~_0.647059) then T if (duration 0.610734)
A (confidence &gt; -3.37301)
A (% silences &gt; 0.521739) then T if (duration &lt; 1.09537)
A (string contains &apos;Baltimore&apos;) then T if (duration &lt; 0.982051)
A (string contains &apos;no&apos;) then T if (duration &lt; 1.1803)
A (confidence &gt; -2.93085)
A (grammar ----date) then T if (duration &lt; 1.09537)
A (confidence &gt; -2.30717)
A (% silences &gt; 0.356436)
A (F0 Max &gt; 249.225) then T if (duration &lt; 0.868743)
A (confidence &gt; -4.14926 )
A (% silences &gt; 0.51923)
A (F0 Max &gt; 205.296) then T if (duration &lt; 1.18036)
A (string contains &apos;Philadelphia&apos;) then T else F
"A statistical comparison of recognized versus mis-shown to be significant in our statistical analyses recognized utterances indicates that F0 excursion, (Section 3) are not the same features as in the rules."
"But, as noted above, our data points in these two experiments differ."
"It is useful to note though, that while this ruleset contains all three ASR features, none of the experimental parameters was found to be a useful predictor, suggesting that our results are not specific to the particular conditions of and par-ticipants in the corpus collection, although they are specific to the lexicon and grammars."
Results of our learning experiments with mis-recognition defined in terms of CA rather than WER show the overall role of the features which predict WER-defined misrecognition to be less successful in predicting CA-defined error.
"Table 4 shows the relative performance of the same feature sets dis-cussed above, with misrecognition now defined in terms of CA&lt;I. As with the WER experiments, the best performing feature set makes use of prosodic and ASR-derived features."
"However, the predictive power of prosodic over ASR features decreases when misrecognition is defined in terms of CA -- which is particularly interesting since ASR confidence scores are intended to predict WER rather than CA; the er-ror rate using ASR confidence scores alone (13.52%) is now significantly lower than the error obtained using prosody (18.18%)."
"However, prosodic features loudness, longer prior pause, and longer duration are significant prosodic characteristics of both WER and CA-defined failed recognition attempts."
Results from a set of machine learning experiments show that prosodic differences can in fact be used to im-prove the prediction of misrecognitions with a high degree of accuracy (12.76% error) for WER-based misrecognitions- and an even higher degree (6.53% error) when combined with information currently available from ASR systems.
"The use of ASR confi-dence scores alone had a predicted[REF_CITE].23%, so the improvement over traditional methods is quite considerable."
"For CA-defined misrecognitions, the improvement provided by prosodic features is con-siderably less."
One of our future research directions will be to understand this difference.
Another future direction will be to address the is-sue ofjust why prosodic features provide such use-ful indicators of recognition failure.
"Do the features themselves make recognition difficult, or are they instead indirect correlates of other phenomena not captured in our study?"
"While the negative influence of speaking rate variation on ASR has been reported before (e.g.[REF_CITE], it is tradition-ally assumed that ASR is impervious to differences in F0 and RMS; yet, it is known that F0 and RMS variations co-vary to some extent with spectral char-acteristics (e.g. ([REF_CITE]; Fant et still improve the predictive power of ASR confidence al., 1995)), so that it is not unlikely that utterances scores, to 11.34%, although this difference is not sig- with extreme values for these may differ critically nificant at a 95% confidence level."
And the error from the training data.
Other prosodic features may rate of the three ASR features combined (11.70%) is be more indirect indicators of errors.
"Longer ut-reduced to the lowest error rate in our table when terances may simply provide more chance for error prosodic features are added (10.43%); this error rate than shorter ones, while speakers who pause longer is (just) significantly different from the use of ASR before utterances and take more time making them confidence scores alone."
"Thus, for CA-defined mis- may also produce more disfluencies than others. recognitions, our experiments have uncovered only We are currently replicating our experiment on a minor improvements over traditional ASR rejection new domain with a new speech recognizer."
"We are calculation procedures. examining the W99 corpus, which was collected in a fact hold up across recognizers."
We also are extend-
"A. G. Bouwman, J. Sturm, and L. Boves. 1999. ing our TOOT corpus analysis to include prosodic Incorporating confidence measures in the dutch analyses of turns in which users become aware of train timetable information system developed in misrecognitions and correct them."
"In addition, we the ARISE project."
"International Con-are exploring whether prosodic differences can help ference on Acoustics, Speech and Signal Process-explain the &quot;goat&quot; phenomenon -- the fact that ing, volume 1, pages 493-496, Phoenix."
"In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations."
"Yet, relatively few have embedded one of these algorithms in a task."
"In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction."
The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.
Several technical challenges confronted us and were solved: • How could the limited semantic interpretation required in information extraction be integrated into the statistical learning algorithm?
"We were able to integrate both syntactic and semantic information into the parsing process, thus avoiding potential errors of syntax first followed by semantics. • Would TREEBANKing of the variety of news sources in MUC-7 be required?"
"Or could the University of Pennsylvania&apos;s TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire, which includes dozens of newspapers?"
Manually creating source-specific training data for syntax was not required.
"Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation. * Would semantic annotation require computational linguists?"
We were able to specify relatively simple guidelines that students with no training in computational linguistics could annotate.
We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported[REF_CITE].
"The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts)."
"For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it."
"For each person, one must find all of the person&apos;s names within the document, his/her type (civilian or military), and any significant descriptions (e.g., titles)."
"For each location, one must also give its type (city, province, county, body of water, etc.)."
"For the following example, the template element in Figure I was to be 3 generated: &quot;...according to the report by"
"Edwin Dorn, under secretary of defense for"
Integrated Sentential Processing
Almost all approaches to information personnel and readiness....
Dorn&apos;s conclusion extraction - even at the sentence level - are that Washington...&quot; based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler &lt;[REF_CITE]-13&gt; := ones.
"Currently, the prevailing architecture for ENT_NAME: &quot;Edwin Dorn&quot; dividing sentential processing is a four-stage &quot;Dorn&quot; pipeline consisting of:"
ENT_DESCRIPTOR: &quot;under secretary of 1. part-of-speech tagging defense for personnel and readiness&quot; 2. name finding ENT_CATEGORY: PER_CIV 3. Figure 1: An example of the information to be extracted for TE. 4.
"The Template Relations (TR) task involves identifying instances of three relations in the text: syntactic analysis, often limited to noun and verb group chunking semantic interpretation, usually based on pattern matching"
"Since we were interested in exploiting recent advances in parsing, replacing the syntactic • the products made by each company analysis stage of the standard pipeline with a modem statistical parser was an obvious • the employees of each organization, possibility."
"However, pipelined architectures suffer from a serious disadvantage: errors • the (headquarters) location of each accumulate as they propagate through the organization. pipeline."
"For example, an error made during part-of-speech-tagging may cause a future TR builds on TE in that TR reports binary error in syntactic analysis, which may in turn relations between elements of TE."
For the cause a semantic interpretation failure.
"There following example, the template relation in is no opportunity for a later stage, such as Figure 2 was to be generated: &quot;Donald M. parsing, to influence or correct an earlier stage Goldstein, a historian at the University of such as part-of-speechtagging."
Pittsburgh who helped write...&quot; An integrated model can limit the propagation of errors by making all decisions jointly.
"For &lt;[REF_CITE]-5&gt; := this reason, we focused on designing an PERSON: &lt;[REF_CITE]-18&gt; integrated model in which tagging, name- ORGANIZATION: &lt;ENTITY- finding, parsing, and semantic interpretation 9601020516-9&gt; decisions all have the opportunity to mutually &lt;[REF_CITE]-9&gt; := influence each other."
ENT_NAME: &quot;University of Pittsburgh&quot;
A ENT_TYPE: ORGANIZATION second consideration influenced our decision toward an integrated model.
We were ENT_CATEGORY: ORG_CO already using a generative statistical model for &lt;[REF_CITE]-18&gt; := part-of-speech tagging (Weischedel et al.
"ENT_NAME: &quot;Donald M. Goldstein&quot; 1993), and more recently, had begun using a ENT_TYPE: PERSON generative statistical model for name finding ENT_DESCRIPTOR: &quot;a historian at the[REF_CITE]."
"Finally, our newly University of Pittsburgh&quot; constructed parser, like that[REF_CITE], Figure 2: An example of informationto be was based on a generative statistical model. extracted for TR Thus, each component of what would be the first three stages of our pipeline was based on 227 the same general class of statistical model."
"Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model."
"If the single generalized model could then be extended to semantic anal);sis, all necessary sentence level processing would be contained in that model."
"Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties - especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs - would also benefit semantic analysis."
Our integrated model represents syntax and semantics jointly using augmented parse trees.
"In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations."
An example of an augmented parse tree is shown in Figure 3.
"The five key facts in this example are: • &quot;Nance&quot; is the name of a person. • &quot;A paid consultant to ABC News&quot; describes a person. t &quot;ABC News&quot; is the name of an organization. • The person described as &quot;a paid consultant to ABC News&quot; is employed by ABC News. • The person named &quot;Nance&quot; and the person described as &quot;a paid consultant to ABC News&quot; are the same person. reference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case, because the argument does not connect directly to the relation, the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection."
Further details are discussed in the section Tree Augmentation.
"To train our integrated model, we required a large corpus of augmented parse trees."
"Since it was known that the MUC-7 evaluation data would be drawn from a variety of newswire sources, and that the articles would focus on rocket launches, it was important that our training corpus be drawn from similar sources and that it cover similar events."
"Thus, we did not consider simply adding semantic labels to the existing Penn TREEBANK, which is drawn from a single source - the Wall Street Journal - and is impoverished in articles about rocket launches."
"Instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events."
The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.
"Initially, we tried to annotate the training corpus by hand marking, for each sentence, the entire augmented tree."
It soon became painfully obvious that this task could not be performed in the available time.
Our annotation staff found syntactic analysis particularly complex and slow going.
"By necessity, we adopted the strategy of hand marking only the semantics."
"Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed."
"To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK 1."
"The model (see Section 7) was first trained on purely syntactic parse trees from the TREEBANK, producing a model capable of broad-coverage syntactic parsing. 2."
"Next, for each sentence in the semantically annotated corpus: a."
The model was applied to parse the parses that were consistent with the semantic annotation.
A parse was considered consistent if no syntactic constituents crossed an annotated entity or description boundary. b.
The resulting parse tree was then augmented to reflect semantic structure in addition to syntactic structure.
Nodes are inserted into the parse tree to distinguish names and descriptors that are not bracketed in the parse.
"For example, the parser produces a single noun phrase with no internal structure for &quot;Lt."
David Edwin Lewis&quot;.
"Additional nodes must be inserted to distinguish the description, &quot;Lt."
"Cmdr.,&quot; and the name, &quot;David Edwin Lewis.&quot;."
Semantic labels are attached to all nodes that correspond to names or descriptors.
"These labels reflect the entity type, such as person, organization, or location, as well as whether the node is a proper name or a descriptor.."
"For relations between entities, where one entity is not a syntactic modifier of the other, the lowermost parse node that spans both entities is identified."
A semantic tag is then added to that node denoting the relationship.
"For example, in the sentence &quot;Mary Fackler Schiavo is the inspector general of the U.S. Department of Transportation,&quot; a co-reference semantic label is added to the S node spanning the name, &quot;Mary Fackler Schiavo,&quot; and the descriptor, &quot;the inspector general of the U.S. Department of Transportation.&quot;."
Nodes are inserted into the parse tree to distinguish the arguments to each relation.
"In cases where there is a relation between two entities, and one of the entities is a syntactic modifier of the other, the inserted node serves to indicate the relation as well as the argument."
"For example, in the phrase &quot;Lt."
"David Edwin Lewis,&quot; a node is inserted to indicate that &quot;Lt."
Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot;.
"Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes."
These labels serve to form a continuous chain between the relation and its argument. 7 Model Structure
"In our statistical model, trees are generated according to a process similar to that described[REF_CITE]."
"The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process."
"For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward."
"Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created."
"Word features are introduced primarily to help with unknown words, as[REF_CITE]."
We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.
"At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements."
"We pick up the derivation just after the topmost S and its head word, said, have been produced."
The next steps are to generate in order: 1.
"A head constituent for the S, in this case a VP. 2. Pre-modifier constituents for the S. In this case, there is only one: a PER/NP. 3."
"A head part-of-speech tag for the PER/NP, in this case PER/NNP. 4."
"A head word for the PER/NP, in this case nance. 5."
"Word features for the head word of the PER/NP, in this case capitalized. 6."
"A head constituent for the PER/NP, in this case a PER-R/NP. 7. Pre-modifier constituents for the PER/NP."
"In this case, there are none.."
Post-modifier constituents for the PER/NP.
"First a comma, then an SBAR structure, and then a second comma are each generated in turn."
This generation process is continued until the entire tree has been produced.
We now briefly summarize the probability structure of the model.
"The categories for head constituents, ch, are predicted based solely on the category of the parent node, cp: e(c h Icp), e.g. P(vpls )"
"Modifier constituent categories, Cm, are predicted based on their parent node, cp, the head constituent of their parent node, Chp,the previously generated modifier, Cm-1, and the head word of their parent, wp."
Separate probabilities are maintained for left (pre) and right (post) modifiers:
"PL(Cm ICp,Chp,Cm_l,Wp), e.g. PL(per I np Is,vp, null, said) PR(c~ ICe,Ch~,Cm-l,Wp), e.g. PR(null [s, vp,null, said)"
"Part-of-speech tags, tin, for modifiers are predicted based on the modifier, Cm, the part-of-speech tag of the head word, th, and the head word itself, wh:"
"P(tmICm,th,wh), e.g. P(per / nnp [per /np, vbd, said)"
"Head words, win, for modifiers are predicted based on the modifier, cm, the part-of-speech tag of the modifier word , t,,, the part-of-speech tag of the head word, th, and the head word itself, Wh:"
"P(Wm ICm,tmth,Wh), e.g. P(nance Iper / np, per / nnp, vbd, said)"
"Finally, word features, fro, for modifiers are predicted based on the modifier, cm, the part-of-speech tag of the modifier word , tin, the part-of-speech tag of the head word , th, the head word itself, Wh, and whether or not the modifier head word, w,,, is known or unknown."
"P(fm [Cm,tm,th,Wh,known(Wm)), e.g. P(cap Iper I np, per / nnp, vbd, said, true)"
The probability of a complete tree is the product of the probabilities of generating each element in the tree.
"If we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write:"
H e(e I h) e~tree
Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.
"However, because these estimates are too sparse to be relied upon, we use interpolated estimates consisting of mixtures of successively lower-order estimates (as[REF_CITE])."
"For modifier constituents, the mixture components are:"
"Icp, chp, Cm_l , w p) = 21 P(c,, ICp,Chp,C,,_I,W,) +22 P(cm I%,chp,Cm_,)"
"For part-of-speech tags, the mixture components are:"
"P&apos;(tm ICm,th,Wh)=21 P(tm Icm,wh) +&apos;~2 e(tm Icm,th) +~3 P(t,, IC~,)"
"For head words, the mixture components are: P&apos;(wm ICm,tm,th, wh) ="
"JL1 P(wm ICm,tm,Wh) +22 P(wm Icm,tm,th) +23 P(wm ICm,t,,) +~4"
"Finally, for word features, the mixture components are:"
"Given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation."
"More precisely, it must find the most likely augmented parse tree."
"Although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chart-based search."
The search is kept tractable through a combination of CKY-style dynamic programming and pruning of low probability elements.
"Whenever two or more constituents are equivalent relative to all possible later parsing decisions, we apply dynamic programming, keeping only the most likely constituent in the chart."
Two constituents are considered equivalent if: 1.
They have identical category labels. 2.
Their head constituents have identical labels. 3.
They have the same head word. 4.
Their leftmost modifiers have identical labels..
Their rightmost modifiers have identical labels. 9.2 Pruning
"Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a Task Recall Entities (TE) 83% Relations (TR) 64% threshold of the highest scoring constituent are maintained; all others are pruned."
"For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent[REF_CITE]."
"We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node."
"Thus, the scores used in pruning can be considered as the product of:."
"The probability of generating a constituent of the specified category, starting at the topmost node.."
"The probability of generating the structure beneath that constituent, having already generated a constituent of that category."
"Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence."
"The semantics - that is, the entities and relations - can then be directly extracted from these sentential trees. 10 Experimental Results"
"Our system for MUC-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging."
The evaluation results are summarized in Table 1.
"In both Template Entity (TE) and Template Relation (TR), our system finished in second place among all entrants."
Nearly all of the work was done by the sentential model; disabling the cross-sentence model entirely reduced our overall F-Score by only 2 points.
"While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding."
"We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see[REF_CITE]), and evaluated name finding accuracy on the MUC-7 named entity test."
The results are summarized in Table 2.
"While performance did not quite match the best previously reported results for any of these three tasks, we were pleased to observe that the scores were at or near state-of-the-art levels for all cases."
"We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) can be used effectively for information extraction."
"A single model proved capable of performing all necessary sentential processing, both syntactic and semantic."
We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.
The semantic training corpus was produced by students according to a simple set of guidelines.
This simple semantic annotation was the only source of task knowledge used to configure the model.
"It is generally recognized that the common non-terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and se-mantic information one would like about parts of a syntactic tree."
"For example, the Penn Tree-bank gives each constituent zero or more &apos;func-tion tags&apos; indicating semantic roles and other related information not easily encapsulated in the simple constituent labels."
"We present a sta-tistical algorithm for assigning these function tags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, which rises to 99% when considering &apos;no tag&apos; as a valid choice."
"Parsing sentences using statistical information gathered from a treebank was first examined a decade ago[REF_CITE]and is by now a fairly well-studied problem ([REF_CITE],[REF_CITE],[REF_CITE])."
"But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR.The Penn treebank contains a great deal of additional syntactic and seman-tic information from which to gather statistics; reproducing more of this information automat-ically is a goal which has so far been mostly ignored."
This paper details a process by which some of this information--the function tags--may be recovered automatically.
"In the Penn treebank, there are 20 tags (fig-ure 1) that can be appended to constituent la-bels in order to indicate additional information about the syntactic or semantic role of the con- *"
This researchwas funded in part by NSF grants LIS-[REF_CITE]. stituent.
We have divided them into four cate-gories (given in figure 2) based on those in the bracketing guidelines[REF_CITE].
"A con-stituent can be tagged with multiple tags, but never with two tags from the same category.1"
"In actuality, the case where a constituent has tags from all four categories never happens, but constituents with three tags do occur (rarely)."
"At a high level, we can simply say that hav-ing the function tag information for a given text is useful just because any further information would help."
"But specifically, there are distinct advantages for each of the various categories."
"Grammatical tags are useful for any application trying to follow the thread of the text--they find the &apos;who does what&apos; of each clause, which can be useful to gain information about the situa-tion or to learn more about the behaviour of the words in the sentence."
"The form/function tags help to find those constituents behaving in ways not conforming to their labelled type, as well as further clarifying the behaviour of ad-verbial phrases."
"Information retrieval applica-tions specialising in describing events, as with a number of the MUC applications, could greatly benefit from some of these in determining the where-when-whyof things."
"Noting a topicalised constituent could also prove useful to these ap-plications, and it might also help in discourse analysis, or pronoun resolution."
"Finally, the &apos;miscellaneous&apos; tags are convenient at various times; particularly the CLI~&apos;closely related&apos; tag, which among other things marks phrasal verbs and prepositional ditransitives."
"To our knowledge, there has been no attempt so far to recover the function tags in pars-ing treebank text."
"In fact, we know of only 1There is a single exception in the corpus: one con-stituent is tagged with -LOC-I~R.This appears to be an error. one project that used them at all: (Collins, ture value would be estimated as 1997) defines certain constituents as comple- ments based on a combination of label and func-tion tag information."
This boolean condition is then used to train an improved parser.
"P(flYl, • •, Y,) P(flfl, f2,...,fj), j &lt; n, (1) where/3 refers to an empirically observed prob- ability 2 Features."
"Of course, if features 1 through i only co-occur a few times in the training, this value may not be reliable, so the empirical probability We have found it useful to define our statisti- is usually smoothed: cal model in terms of features."
"A &apos;feature&apos;, in this context, is a boolean-valued function, gen-erally over parse tree nodes and either node la-bels or lexical items."
"Features can be fairly sim-ple and easily read off the tree (e.g. &apos;this node&apos;s label is X&apos;, &apos;this node&apos;s parent&apos;s label is Y&apos;), or"
"AiP(flfl, fa,..., fi) + (2)"
The values for )~i can then be determined ac-slightly more complex (&apos;this node&apos;s head&apos;s part- cording to the number of occurrences of features of-speech is Z&apos;).
"This is concordant with the us- 1 through i together in the training. age in the maximum entropy literature (Berger One way to think about equation 1 (and et al., 1996). specifically, the notion that j will depend on"
"When using a number of known features to the values of fl... fn) is as follows: We begin guess an unknown one, the usual procedure is with the prior probability of f. If we have data to calculate the value of each feature, and then indicating P(flfl), we multiply in that likeli-essentially look up the empirically most proba- hood, while dividing out the original prior."
"If ble value for the feature to be guessed based on we have data for/3(flfl, f2), we multiply that those known values."
"Due to sparse data, some in while dividing out the P(flfl) term."
"This is of the features later in the list may need to be repeated for each piece of feature data we have; ignored; thus the probability of an unknown fea- at each point, we are adjusting the probability p(/)"
"P(SlSl, S:) P(flft, $2,..., f~) P(flfl,f2,... ,fn) j &lt; n P(f) P(flA, A,...,f¢-x) P(flfl)"
"P(flfl,..., Yi-1,A) (3) -,_-o&quot; p- ff, we already have estimated."
"If knowledge about feature fi makes S more likely than with just fl... fi-1, the term where fi is added will be greater than one and the running probability will be adjusted upward."
"This gives us the new probability shown in equation 3, which is ex-actly equivalent to equation 1 since everything except the last numerator cancels out of the equation."
"The value of j is chosen such that features f l . . - f j are sufficiently represented in the training data; sometimes all n features are used, but often that would cause sparse data problems."
"Smoothing is performed on this equa-tion exactly as before: each term is interpolated between the empirical value and the prior esti-mated probability, according to a value of Ai that estimates confidence."
"But aside from per-haps providing a new way to think about the problem, equation 3 is not particularly useful as it is--it is exactly the same as what we had before."
"Its real usefulness comes, as shown[REF_CITE], when we move from the no-tion of a feature chain to a feature tree."
These feature chains don&apos;t capture everything we&apos;d like them to.
"If there are two independent features that are each relatively sparse but occa-sionally carry a lot of information, then putting one before the other in a chain will effectively block the second from having any effect, since its information is (uselessly) conditioned on the first one, whose sparseness will completely di-lute any gain."
"What we&apos;d really like is to be able to have a feature tree, whereby we can condition those two sparse features independently on one common predecessor feature."
"As we said be-fore, equation 3 represents, for each feature fi, the probability of f based on fi and all its pre-decessors, divided by the probability of f based only on the predecessors."
"In the chain case, this means that the denominator is conditioned on every feature from 1 to i - 1, but if we use a feature tree, it is conditioned only on those fea-tures along the path to the root of the tree."
A notable issue with feature trees as opposed to feature chains is that the terms do not all cancel out.
Every leaf on the tree will be repre-target ~ feature
"Figure 3: A small example feature tree sented in the numerator, and every fork in the tree (from which multiple nodes depend) will be represented at least once in the denomina-tor."
For example: in figure 3 we have a small feature tree that has one target feature and four conditioning features.
"Features b and d are in-dependent of each other, but each depends on a; c depends directly only on b. The unsmoothed version of the corresponding equation would be"
"P(fla, b,c, d) ,~ p,~ P(fla) ~)(f]a, b) P(f[a, b, c) P(fla, d) which, after cancelling of terms and smoothing, results in"
"P(fla, b,c,d) P(fla, b,c)P(fla, d) P(fla) (4)"
Note that strictly speaking the result is not a probability distribution.
It could be made into one with an appropriate normalisation--the so-called partition function in the maximum-entropy literature.
"However, if the indepen-dence assumptions made in the derivation of equation 4 are good ones, the partition func-tion will be close to 1.0."
We assume this to be the case for our feature trees.
Now we return the discussion to function tag-ging.
"There are a number of features that seem is to call a constituent correct if there exists in head is only defined for prepositional phrases, the correct parse a constituent with the same and is the head of the object of the preposi-start and end points, label, and function tag tional phrase."
This data is very important in (or lack thereof).
"Since we treated each of the distinguishing, for example, &apos;by John&apos; (where four function tag categories as a separate fea- John might be a logical subject) from &apos;by next ture for the purpose of tagging, evaluation was year&apos; (a temporal modifier) and &apos;by selling it&apos; (an adverbial indicating manner). also done on a per-category basis."
The denominator of the accuracy measure 3 Experiment should be the maximum possible number we
"In the training phase of our experiment, we could get correct."
"In this case, that means gathered statistics on the occurrence of func- excluding those constituents that were already tion tags in sections 2-21 of the Penn treebank. wrong in the parser output; the parser we used Specifically, for every constituent in the tree- attains 89% labelled precision-recall, so roughly bank, we recorded the presence of its function 11% of the constituents are excluded from the tags (or lack thereof) along with its condition- function tag accuracy evaluation. (For refer-ing information."
"From this we calculated the ence, we have also included the performance of empirical probabilities of each function tag ref- our function tagger directly on treebank parses; erenced in section 2 of this paper."
Values of )~ the slight gain that resulted is discussed below.) were determined using EM on the development corpus (treebank section 24).
Another consideration is whether to count
"To test, then, we simply took the output of non-tagged constituents in our evaluation."
"On our parser on the test corpus (treebank section the one hand, we could count as correct any 23), and applied a postprocessing step to add constituent with the correct tag as well as any function tags."
"For each constituent in the tree, correctly non-tagged constituent, and use as we calculated the likelihood of each function tag our denominator the number of all correctly-according to the feature tree in figure 4, and labelled constituents. (We will henceforth refer for each category (see figure 2) we assigned the to this as the &apos;with-null&apos; measure.)"
"On the other most likely function tag (which might be the hand, we could just count constituents with the null tag). correct tag, and use as our denominators the"
"There are, it seems, two reasonable baselines for this and future work."
"First of all, most con-stituents in the corpus have no tags at all, so obviously one baseline is to simply guess no tag for any constituent."
"Even for the most com-mon type of function tag (grammatical), this method performs with 87% accuracy."
Thus the with-null accuracy of a function tagger needs to be very high to be significant here.
The second baseline might be useful in ex-amining the no-null accuracy values (particu-larly the recall): always guess the most common tag in a category.
This means that every con-stituent gets labelled with &apos;-SBJ-THP-TPC-CLR&apos; (meaning that it is a topicalised temporal sub-ject that is &apos;closely related&apos; to its verb).
"This combination of tags is in fact entirely illegal by the treebank guidelines, but performs ad-equately for a baseline."
"The precision is, of course, abysmal, for the same reasons the first baseline did so well; but the recall is (as one might expect) substantial."
The performances of the two baseline measures are given in Table 1.
"In table 2, we give the results for each category."
"The first column is the with-null accuracy, and the precision and recall values given are the no-null accuracy, as noted in section 4."
Grammatical tagging performs the best of the four categories.
"Even using the more difficult no-null accuracy measure, it has a 96% accu-racy."
"This seems to reflect the fact that gram-matical relations can often be guessed based on constituent labels, parts of speech, and high-frequency lexical items, largely avoiding sparse-data problems."
"Topicalisation can similarly be guessed largely on high-frequency information, and performed almost as well (93%)."
"On the other hand, we have the form/function tags and the &apos;miscellaneous&apos; tags."
"These are characterised by much more semantic information, and the relationships between lexical items are very important, making sparse data a real problem."
"All the same, it should be noted that the performance is still far better than the baselines."
The feature tree given in figure 4 is by no means the only feature tree we could have used.
"In- 88.493% 88.472% deed, we tried a number of different trees on the development corpus; this tree gave among the best overall results, with no category perform-ing too badly."
"However, there is no reason to use only one feature tree for all four categories; the best results can be got by using a separate tree for each one."
One can thus achieve slight (one to three point) gains in each category.
"The overall performance, given in table 3, ap-pears promising."
"With a tagging accuracy of about 87%, various information retrieval and knowledge base applications can reasonably ex-pect to extract useful information."
The performance given in the first row is (like all previously given performance values) the function-tagger&apos;s performance on the correctly-labelled constituents output by our parser.
"For comparison, we also give its performance when run directly on the original treebank parse; since the parser&apos;s accuracy is about 89%, working di-rectly with the treebank means our statistics are over roughly 12% more constituents."
This second version does slightly better.
"Figure 6: SBARand conditioning info, as parsed the parsed version is that although the con-stituent itself may be correctly bracketed and la-belled, its exterior conditioning information can However, this was not the conditioning infor-still be incorrect."
An example of this that ac- mation that the tagger received.
The parser tually occurred in the development corpus (sec- had instead decided on the (incorrect) parse in tion 24 of the treebank) is the &apos;that&apos; clause in figure 6.
"As such, the tagger&apos;s decision makes the phrase &apos;can swallow the premise that the re- much more sense, since an SBARunder two VPs wards for such ineptitude are six-figure salaries&apos;, whose heads are VB and MDis rather likely to be correctly diagrammed in figure 5."
"The function an ADV.(For instance, the &apos;although&apos; clause of tagger gave this SBARan ADV tag, indicating an the sentence &apos;he can help, although he doesn&apos;t unspecified adverbial function."
"This seems ex- want to.&apos; has exactly the conditioning environ-tremely odd, given that its conditioning infor- ment given in figure 6, except that its prede-mation (nodes circled in the figure) clearly show cessor is a comma; and this SBARwould be cor-that it is part of an NP, and hence probably mod- rectly tagged ADV.)The SBARitself is correctly ifies the preceding NN.Indeed, the statistics give bracketed and labelled, so it still gets counted the probability of an ADVtag in this condition- in the statistics."
"Happily, this sort of case seems ing environment as vanishingly small. to be relatively rare."
Another thing that lowers the overall perfor-mance somewhat is the existence of error and in-consistency in the treebank tagging.
"Some tags seem to have been relatively easy for the human treebank taggers, and have few errors."
"Other tags have explicit caveats that, however well-justified, proved difficult to remember for the taggers--for instance, there are 37 instances of a PP being tagged with LGS (logical subject) in spite of the guidelines specifically saying, &apos;[LGS] attaches to the NP object of by and not to the PP node itself.&apos;[REF_CITE]"
"Each mistag-ging in the test corpus can cause up to two spu-rious errors, one in precision and one in recall."
Still another source of difficulty comes when the guidelines are vague or silent on a specific issue.
"To return to logical subjects, it is clear that &apos;the loss&apos; is a logical subject in &apos;The company was hurt by the loss&apos;, but what about in &apos;The com-pany was unperturbed by the loss&apos; ?"
"In addition, a number of the function tags are authorised for &apos;metaphorical use&apos;, but what exactly constitutes such a use is somewhat inconsistently marked."
It is as yet unclear just to what degree these tagging errors in the corpus are affecting our results.
This work presents a method for assigning func-tion tags to text that has been parsed to the simple label level.
"Because of the lack of prior research on this task, we are unable to com-pare our results to those of other researchers; but the results do seem promising."
"However, a great deal of future work immediately suggests itself: • Although we tested twenty or so feature trees besides the one given in figure 4, the space of possible trees is still rather un-explored."
"A more systematic investiga-tion into the advantages of different feature trees would be useful. • We could add to the feature tree the val-ues of other categories of function tag, or the function tags of various tree-relatives (parent, sibling). • One of the weaknesses of the lexical fea-tures is sparse data; whereas the part of speech is too coarse to distinguish &apos;by John&apos; (LGS) from &apos;by Monday&apos; (TMP), the lexi-cal information may be too sparse."
"This could be assisted by clustering the lexical items into useful categories (names, dates, etc.), and adding those categories as an ad-ditional feature type. • There is no reason to think that this work could not be integrated directly into the parsing process, particularly if one&apos;s parser is already geared partially or entirely to-wards feature-based statistics; the func-tion tag information could prove quite use-ful within the parse itself, to rank several parses to find the most plausible."
"Given the lack of word delimiters in written Japanese, word segmentation is generally consid-ered a crucial first step in processing Japanese texts."
Typical Japanese segmentation algorithms rely ei-ther on a lexicon and grammar or on pre-segmented data.
"In contrast, we introduce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyz-ers over a variety of error metrics."
Because Japanese is written without delimiters be-tween words) accurate word segmentation to re-cover the lexical items is a key step in Japanese text processing.
"Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) er-rors[REF_CITE]."
"Typically, Japanese word segmentation is per-formed by morphological analysis based on lexical and grammatical knowledge."
"This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, al-though using this heuristic alone achieves less than 60% accuracy[REF_CITE]."
Character sequences consisting solely of kanji pose a challenge to morphologically-based seg-reenters for several reasons.
"First and most importantly, kanji sequences often contain domain terms and proper nouns:[REF_CITE]notes that 50-85% of the terms in various technical dictio-"
"Sequence length # of characters % of corpus 1 - 3 kanji 20,405,486 25.6 4 - 6 kanji 12,743,177 16.1 more than 6 kanji 3,966,408 5.1[REF_CITE]071 46.8 naries are composed at least partly of kanji."
"Such words tend to be missing from general-purpose lexicons, causing an unknown word problem for morphological analyzers; yet, these terms are quite important for information retrieval, information extraction, and text summarization, making correct segmentation of these terms critical."
"Second, kanji sequences often consist of compound nouns, so grammatical constraints are not applicable."
"For instance, the sequence sha-chohlkenlgyoh-mulbu-choh (presidentlandlbusinesslgeneral manager = &quot;a president as well as a general manager of business&quot;) could be incorrectly segmented as: sha-chohlken-gyohlmulbu-choh (presidentlsubsidiary business[Tsutomu [a name][general manager); since both alternatives are four-noun sequences, they cannot be distinguished by part-of-speech information alone."
"Finally, heuristics based on changes in character type obviously do not apply to kanji-only sequences."
"Although kanji sequences are difficult to seg-ment, they can comprise a significant portion of Japanese text, as shown in Figure 1."
"Since se-quences of more than 3 kanji generally consist of more than one word, at least 21.2% of 1993 Nikkei newswire consists of kanji sequences requiring seg-mentation."
"Thus, accuracy on kanji sequences is an important aspect of the total segmentation process."
"As an alternative to lexico-grammatical and su-pervised approaches, we propose a simple, effi- cient segmentation method which learns mostly s, ? from very large amounts of unsegmented training I data, thus avoiding the costs of building a lexicon ABCb{ i WXYZ t, or grammar or hand-segmenting large amounts of training data."
"Some key advantages of this method % • are: /, • No Japanese-specific rules are employed, en-hancing portability to other languages. • A very small number of pre-segmented train-ing examples (as few as 5 in our experiments) Figure 2: Collecting evidence for a word boundary are needed for good performance, as long as - are the non-straddling n-grams 81 and 82 more large amounts of unsegmented data are avail- frequent than the straddling n-grams tl, t2, and t3? able. • For long kanji strings, the method produces re-"
"Let I&gt; (y, z) be an indicator function that is 1 when sults rivalling those produced by Juman 3.61 y &gt; z, and 0 otherwise,2"
"In order to compensate[REF_CITE]and Chasen 1.0 the fact that there are more n-gram questions than[REF_CITE], two morphological (n - 1)-gram questions, we calculate the fraction of analyzers in widespread use."
"For instance, we affirmative answers separately for each n in N: achieve 5% higher word precision and 6% bet- ter morpheme recall. 2 Algorithm 2 n--1 1 vn(k) = 2(n- 1) x&gt;(#(87), ~=1 j=l"
Our algorithm employs counts of character n-grams
"Then, we average the contributions of each n-gram in an unsegmented corpus to make segmentation de-order: cisions."
We illustrate its use with an example (see 1
INI ~ vn(k)
Let &quot;A B C D W X Y Z&quot; represent an eight-kanji nEN sequence.
"To decide whether there should be a word After vN(k) is computed for every location, bound-boundary between D and W, we check whether n-aries are placed at all locations ~ such that either: grams that are adjacent to the proposed boundary, such as the 4-grams sl =&quot;A B C D&quot; and 82 =&quot;W • VN(g) &gt; VN(e -- 1) and VN(g) &gt; VN(e + 1) X Y Z&quot;, tend to be more frequent than n-grams that (that is, e is a local maximum), or straddle it, such as the 4-gram tl ----&quot;B C D W&quot;."
"If so, we have evidence of a word boundary between • VN (2) &gt; t, a threshold parameter."
"D and W, since there seems to be relatively little The second condition is necessary to allow for cohesion between the characters on opposite sides of this gap. single-character words (see Figure 3)."
"Note that it also controls the granularity of the segmentation: The n-gram orders used as evidence in the seg-low thresholds encourage shorter segments. mentation decision are specified by the set N. For Both the count acquisition and the testing phase instance, if N = {4} in our example, then we pose are efficient."
"Computing n-gram statistics for all the six questions of the form, &quot;Is #(s~) &gt; #(tj)?&quot;, possible values of n simultaneously can be done in where #(x) denotes the number of occurrences of x in the (unsegmented) training corpus."
"If N {2,4}, then two more questions (Is &quot;#(C D)"
"O(m log m) time using suffix arrays, where m is = the training corpus size ([REF_CITE]; &gt;"
"However, if the set N of #(D W)?&quot; and &quot;Is #(W X) &gt; #(O W)?&quot;) are added. n-gram orders is known in advance, conceptually simpler algorithms suffice."
"Memory allocation for More formally, let s~ and 8~ be the non-straddling n-grams just to the left and right of lo- :Note that we do not take into account the magnitude of cation k, respectively, and let t~ be the straddling the difference between the two frequencies; see section 5 for n-gram with j characters to the right of location k. discussion. count tables can be significantly reduced by omit-ting n-grams occurring only once and assuming the count of unseen n-grams to be one."
"In the applica-tion phase, the algorithm is clearly linear in the test corpus size if [NI is treated as a constant."
"Finally, we note that some pre-segmented data is necessary in order to set the parameters N and t. However, as described below, very little such data was required to get good performance; we therefore deem our algorithm to be &quot;mostly unsupervised&quot;."
Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire (see Figure 1).
Each held-out subset was hand-segmented and then split into a 50-sequence parameter-training set and a 450-sequence test set.
"Finally, any sequences occurring in both a test set and its corresponding parameter-training set were discarded from the parameter-training set, so that these sets were disjoint. (Typically no more than five sequences were removed.)"
"Each held-out set contained 500 randomly-extracted kanji sequences at least ten characters long (about twelve on average), lengthy sequences being the most difficult to segment[REF_CITE]."
"To obtain the gold-standard annotations, we segmented the sequences by hand, using an observa-tion[REF_CITE]that many kanji compound words consist of two-character stem words together with one-character prefixes and suf-fixes."
"Using this terminology, our two-level bracket-ing annotation may be summarized as follows.[Footnote_3] At the word level, a stem and its affixes are bracketed together as a single unit."
"3A complete description of the annotation policy, including the treatment of numeric expressions, may be found in a tech-nical report[REF_CITE]."
"At the morpheme level, stems are divided from their affixes."
"For example, although both naga-no (Nagano) and shi (city) can appear as individual words, naga-no-shi (Nagano city) is bracketed as [[naga-no][shi]], since here shi serves as a suffix."
"Loosely speaking, word-level bracketing demarcates discourse entities, whereas morpheme-level brackets enclose strings that cannot be further segmented without loss of meaning. [Footnote_4]"
4This level of segmentation is consistent with Wu&apos;s (1998) MonotonicityPrinciple for segmentation.
"For instance, if one segments naga-no in naga-no-shi into naga (long) and no (field), the intended mean-ing disappears."
Here is an example sequence from our datasets:
"Three native Japanese speakers participated in the annotation: one segmented all the held-out data based on the above rules, and the other two reviewed 350 sequences in total."
The percentage of agree-ment with the first person&apos;s bracketing was 98.42%: only 62 out of 3927 locations were contested by a verifier.
"Interestingly, all disagreement was at the morpheme level."
"We evaluated our segmentation method by com-paring its performance against Chasen 1.[URL_CITE][REF_CITE]and Juman 3.61,6[REF_CITE], two state-of-the-art, publically-available, user-extensible morphological analyzers."
"In both cases, the grammars were used as distributed without modification."
"The sizes of Chasen&apos;s and Ju-man&apos;s default lexicons are approximately 115,000 and 231,000 words, respectively."
"Comparison issues An important question that arose in designing our experiments was how to en-able morphological analyzers to make use of the parameter-training data, since they do not have pa-rameters to tune."
"The only significant way that they can be updated is by changing their grammars or lexicons, which is quite tedious (for instance, we had to add part-of-speech information &apos;to new en-tries by hand)."
"We took what we felt to be a rea-sonable, but not too time-consuming, course of cre-ating new lexical entries for all the bracketed words in the parameter-training data."
Evidence that this
Word and morpheme accuracy
The standard metrics in word segmentation are word precision and recall.
"Treating a proposed segmentation as a non-nested bracketing (e.g., &quot;lAB ICI&quot; corresponds to the bracketing &quot;[AB][C]&quot;), wordprecision (P) is defined as the percentage of proposed brackets that exactly match word-level brackets in the annotation; word recall (R) is the percentage of word-level an-notation brackets that are proposed by the algorithm in question; and word F combines precision and re-call: F = 2PR/(P + R)."
One problem with using word metrics is that morphological analyzers are designed to produce morpheme-level segments.
"To compensate, we al-groups represent our algorithm with parameters tuned for different optimization criteria. tered the segmentations produced by Juman and Chasen by concatenating stems and affixes, as iden-tified by the part-of-speech information the analyz- was appropriate comes from the fact that these ad- ers provided. (We also measured morpheme accu-ditions never degraded test set performance, and in- racy, as described below.) deed improved it by one percent in some cases (only Figures 4 and 8 show word accuracy for Chasen, small improvements are to be expected because the Juman, and our algorithm for parameter settings parameter-training sets were fairly small). optimizing word precision, recall, and F-measure"
"It is important to note that in the end, we are com- rates."
Our algorithm achieves 5.27% higher preci-paring algorithms with access to different sources sion and 0.25% better F-measure accuracy than Ju-of knowledge.
"Juman and Chasen use lexicons and man, and does even better (8.8% and 4.22%, respec-grammars developed by human experts."
Our al- tively) with respect to Chasen.
"The recall perfor-gorithm, not having access to such pre-compiled mance falls (barely) between that of Juman and that knowledge bases, must of necessity draw on other of Chasen. information sources (in this case, a very large un-"
"As noted above, Juman and Chasen were de-segmented corpus and a few pre-segmented exam- signed to produce morpheme-level segmentations. ples) to compensate for this lack."
Since we are in-
"We therefore also measured morpheme precision, terested in whether using simple statistics can match recall, and F measure, all defined analogously to the performance of labor-intensive methods, we do their word counterparts. not view these information sources as conveying Figure 5 shows our morpheme accuracy results. an unfair advantage, especially since the annotated We see that our algorithm can achieve better recall training sets were small, available to the morpho- (by 6.51%) and F-measure (by 1.38%) than Juman, logical analyzers, and disjoint from the test sets. and does better than Chasen by an even wider mar-gin (11.18% and 5.39%, respectively)."
Precision was generally worse than the morphological analyz-
"We report the average results over the five test sets ers. using the optimal parameter settings for the corre- Compatible Brackets Although word-level accu-sponding training sets (we tried all nonempty sub- racy is a standard performance metric, it is clearly sets of {2, 3, 4, 5, 6} for the set of n-gram orders N very sensitive to the test annotation."
"Morpheme ac-and all values in {.05, .1, .15,..., 1} for the thresh- curacy suffers the same problem."
"Indeed, the au-old t)7."
"In all performance graphs, the &quot;error bars&quot; thors of Juman and Chasen may well have con-represent one standard deviation."
The results for structed their standard dictionaries using different Chasen and Juman reflect the lexicon additions de- notions of word and morpheme than the definitions we used in annotating the data.
"We therefore devel- 7For simplicity, ties were deterministically broken by pre-ferring smaller sizes of N, shorter n-grams in N, and larger oped two new, more robust metrics to measure the threshold values, in that order. number of proposed brackets that would be incor- rect with respect to any reasonable annotation."
Our novel metrics account for two types of er-rors.
"The first, a crossing bracket, is a proposed bracket that overlaps but is not contained within an annotation bracket[REF_CITE]."
"Cross-ing brackets cannot coexist with annotation brack-ets, and it is unlikely that another human would create such brackets."
"The second type of er-ror, a morpheme-dividing bracket, subdivides a morpheme-level annotation bracket; by definition, such a bracket results in a loss of meaning."
See Fig-ure 6 for some examples.
We define a compatible bracket as a proposed bracket that is neither crossing nor morpheme-dividing.
The compatible brackets rate is simply the compatible brackets precision.
"Note that this met-ric accounts for different levels of segmentation si-multaneously, which is beneficial because the gran-ularity of Chasen and Juman&apos;s segmentation varies from morpheme level to compound word level (by our definition)."
"For instance, well-known university names are treated as single segments by virtue of be-ing in the default lexicon, whereas other university names are divided into the name and the word &quot;uni-versity&quot;."
"Using the compatible brackets rate, both segmentations can be counted as correct."
"We also use the all-compatible brackets rate, which is the fraction of sequences for which all the proposed brackets are compatible."
"Intuitively, this function measures the ease with which a human could correct the output of the segmentation algo-rithm: if the all-compatible brackets rate is high, then the errors are concentrated in relatively few sequences; if it is low, then a human doing post-processing would have to correct many sequences."
Figure 7 depicts the compatible brackets and all-compatible brackets rates.
"Our algorithm does bet-ter on both metrics (for instance, when F-measure is optimized, by 2.16% and 1.9%, respectively, in comparison to Chasen, and by 3.15% and 4.96%, respectively, in comparison to Juman), regardless of training optimization function (word precision, re-call, or F -- we cannot directly optimize the com-patible brackets rate because &quot;perfect&quot; performance is possible simply by making the entire sequence a single segment). 4.1 Discussion to producing good segmentations."
"Indeed, in some cases, both are needed to achieve the best perfor- Minimal human effort is needed."
"In contrast mance; also, each condition when used in isolation to our mostly-unsupervised method, morphological yields suboptimal performance with respect to some analyzers need a lexicon and grammar rules built performance metrics. using human expertise."
The workload in creating dictionaries on the order of hundreds of thousands accuracy optimize optimize optimize of words (the size of Chasen&apos;s and Juman&apos;s de-precision recall F-measure fault lexicons) is clearly much larger than annotat-word M M &amp; T M ing the small parameter-training sets for our algo-morpheme M &amp; T T T rithm.
"We also avoid the need to segment a large amount of parameter-training data because our al- Figure 9: Entries indicate whether best performance gorithm draws almost all its information from an is achieved using the local maximum condition (M), unsegmented corpus."
"Indeed, the only human effort the threshold condition (T), or both. involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes."
"In contrast, previously proposed 5 Related Work supervised approaches have used segmented train-ing sets ranging from 1000-5000 sentences (Kash- Japanese Many previously proposed segmenta-ioka et al., 1998) to 190,000 sentences (Nagata, tion methods for Japanese text make use of either 1996a). a pre-existing lexicon ([REF_CITE]; Mat- sumoto and[REF_CITE]; Takeuchi and Matsumoto, To test how much annotated training data is actu- 1995;[REF_CITE]) or ally necessary , we experimented with using minis-pre-segmented training data ([REF_CITE]; Papa: cule parameter-training sets: five sets of only five georgiou, 1994;[REF_CITE]; Kashioka et al., strings each (from which any sequences repeated in 1998;[REF_CITE])."
Other approaches the test data were discarded).
It took only 4 minutes bootstrap from an initial segmentation provided by to perform the hand segmentation in this case.
"As a baseline algorithm such as Juman (Matsukawa et shown in Figure 8, relative word performance was al., 1993;[REF_CITE]). not degraded and sometimes even slightly better."
"In Unsupervised, non-lexicon-based methods for fact, from the last column of Figure 8 we see that Japanese segmentation do exist, but they often have even if our algorithm has access to only five anno-limited applicability."
"Both Tomokiyo and Ries tated sequences when Juman has access to ten times (1997) and[REF_CITE]explicitly as many, we still achieve better precision and better"
F measure. avoid working with kanji charactes.
Both the local maximum and threshold condi- type of Hidden Markov Model with linguistically- tions contribute.
"In our algorithm, a location is deemed a word boundary if VN(k) is either (1) k determined topology, to segment kanji compound a words."
"However, their method does not handle local maximum or (2) at least as big as the thresh- three-character stem words or single-character stem old t."
"It is natural to ask whether we really need two words with affixes, both of which often occur in conditions, or whetherjust one would suffice. proper nouns."
"In our five test datasets, we found"
We therefore studied whether optimal perfor- that 13.56% of the kanji sequences contain words mance could be achieved using only one of the con- that cannot be handled by the short unit model. ditions.
"Figure 9 shows that in fact both contribute[REF_CITE]propose using the heuris- tic that high-frequency character n-grams may rep-resent (portions of) new collocations and terms, but the results are not experimentally evaluated, nor is a general segmentation algorithm proposed."
"The work[REF_CITE]similarly relies on high-frequency character n-grams, but again, is more concerned with using these frequent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon."
"Our algorithm, on the hand, is fundamen-tally different in that it incorporates no explicit no-tion of word, but only &quot;sees&quot; locations between characters."
"Chinese According[REF_CITE], most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub-lished instance (the mutual-information method[REF_CITE]) of a purely statistical ap-proach."
"In a later paper,[REF_CITE]presents a transformation-based algorithm, which requires pre-segmented training data."
"To our knowledge, the Chinese segmenter most similar to ours is that[REF_CITE]."
"They also avoid using a lexicon, determining whether a given location constitutes a word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions."
"However, the statis-tics they use (mutual information and t-score) are more complex than the simple n-gram counts that we employ."
"Our preliminary reimplementation of their method shows that it does not perform as well as the morphological analyzers on our datasets, al-though we do not want to draw definite conclusions because some aspects of Sun et al&apos;s method seem incomparable to ours."
"We do note, however, that their method incorporates numerical differences between statistics, whereas we only use indicator functions; for example, once we know that one trigram is more common than another, we do not take into account the difference between the two frequencies."
We conjecture that using absolute differences may have an adverse effect on rare sequences.
"In this paper, we have presented a simple, mostly-unsupervised algorithm that segments Japanese se- quences into words based on statistics drawn from a large unsegmented corpus."
"We evaluated per-formance on kanji with respect to several metrics, including the novel compatible brackets and all-compatible brackets rates, and found that our al-gorithm could yield performances rivaling that of lexicon-based morphological analyzers."
"In future work, we plan to experiment on Japanese sentences with mixtures of character types, possibly in combination with morphologi-cal analyzers in order to balance the strengths and weaknesses of the two types of methods."
"Since our method does not use any Japanese-dependent heuristics, we also hope to test it on Chinese or other languages as well."
"We have shown that, in its textbook form, the standard algorithm for eliminating left recur-sion from CFGs is impractical for three diverse, independently-motivated, natural-language gram-mars."
We apply a number of optimizations to the algorithm--most notably a novel strategy for order-ing the nonterminals of the grammar--but one of the three grammars remains essentially intractable.
We then explore an alternative approach based on the LC grammar transform.
"With several optimiza-tions of this approach, we are able to obtain quite compact non-left-recursive forms of all three gram-mars."
"Given the diverse nature of these grammars, we conclude that our techniques based on the LC transform are likely to be applicable to a wide range of CFGs used for natural-language processing."
We propose a method for identifying diathesis alter-nations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms.
The method uses selectional pref-erences acquired as probability distributions over WordNet.
Preferences for the target slots are com-pared using a measure of distributional similarity.
"The method is evaluated on the causative and cona-tive alternations, but is generally applicable and does not require a priori knowledge specific to the alternation."
Diathesis alternations are alternate ways in which the arguments of a verb are expressed syntactically.
The syntactic changes are sometimes accompanied by slight changes in the meaning of the verb.
An ex- ample of the causative alternation is given in (1) be-low.
"In this alternation, the object of the transitive variant can also appear as the subject of the intransi-tive variant."
"In the conative alternation, the transi-tive form alternates with a prepositional phrase con-struction involving either at or on."
An example of the conative alternation is given in (2). 1.
The boy broke the window ~-* The window broke. 2.
The boy pulled at the rope *-* The boy pulled the rope.
We refer to alternations where a particular seman-tic role appears in different grammatical roles in al-ternate realisations as &quot;role switching alternations&quot; (RSAS).
It is these alternations that our method ap-plies to.
"Recently, there has been interest in corpus-based methods to identify alternations[REF_CITE], and associated verb classifications[REF_CITE]."
"These have either relied on a priori knowledge specified for the alternations in advance, or are not suitable for a wide range of alternations."
"The fully automatic method outlined here is applied to the causative and conative alternations, but is applicable to other RSAS."
Diathesis alternations have been proposed for a number of NLP tasks.
Several researchers have sug-gested using them for improving lexical acquisition.
"They have also been suggested for the recovery of predicate argument structure, necessary for SCF ac-quisiti[REF_CITE]."
He used alternations to indicate where the argument head data from different slots can be com-bined since it occupies the same semantic relation-ship with the predicate.
Different diathesis alternations give different em-phasis and nuances of meaning to the same basic content.
These subtle changes of meaning are impor-tant in natural language generati[REF_CITE].
Alternations provide a means of reducing redun-dancy in the lexicon since the alternating scFs need not be enumerated for each individual verb if a marker is used to specify which verbs the alterna-tion applies to.
"Alternations also provide a means of generalizing patterns of behaviour over groups of verbs, typically the group members are semantically related."
Levin&apos;s classification is not intended to be exhaustive.
Au-tomatic identification of alternations would be a use-ful tool for extending the classification with new participants.
"Levin&apos;s taxonomy might also be used alongside observed behaviour, to predict unseen be-haviour."
"Levin&apos;s classification has been extended by other NLP researchers ([REF_CITE];Dang et al.,"
"I bs,r t,on i uoao activ,t ~ ....... ~ e..t i/ construction time war l measure ] /&quot;( time ] car ( relation ] migrationmeal ceremonial ,, migration drum A"
"T ---0a- t ..... / time [time_period] fo .v,=t,4 week speech month yelling afternoon 1998)."
Each SCF entry includes a frequency extend the classification by using grammatical in- count and lists the argument heads at all slots. formation in LDOCE alongside semantic information Selectional preferences are automatically acquired in WordNet.
What is missing is a way of classifying for the slots involved in the role switching.
We refer verbs when the relevant information is not available to these as the target slots.
For the causative al-in a manmade resource.
"Using corpora by-passes ternation, the slots are the direct object slot of the reliance on the availability and adequacy of MRDs. transitive SCF and the subject slot of the intransi-"
"Additionally, the frequency information in corpora is tive."
"For the conative, the slots are the direct object helpful for estimating alternation productivity (La- of the transitive and the PP of the np v pp SCF. pata, 1999)."
Estimations of productivity have been Selectional preferences are acquired using the suggested for controlling the application of alterna-method devised[REF_CITE].
We propose a erences for a slot are represented as a tree cut model method to acquire knowledge of alternation partic- (TCM).
"This is a set of disjoint classes that partition ipation directly from corpora, with frequency infor-the leaves of the WordNet noun hypernym hierar-mation available as a by-product. chy."
A conditional probability is attached to each of the classes in the set.
To ensure the TCM covers all
"the word senses in WordNet, we modify Li and Abe&apos;s"
We use both syntactic and semantic information for original scheme by creating hyponym leaf classes be-identifying participants in RSAs.
"Firstly, syntactic low all WordNet&apos;s hypernym (internal) classes."
Each processing is used to find candidates taking the alter- leaf holds the word senses previously held at the in-nating SeEs.
"Secondly, selectional preference models ternal class."
The nominal argument heads from a are acquired for the argument heads associated with target slot are collected and used to populate the a specific slot in a specific SCF of a verb.
WordNet hierarchy with frequency information.
"We use the SCF acquisition system of Briscoe and head lemmas are matched to the classes which con-[REF_CITE], with a probabilistic LR parser (Inui et tain them as synonyms."
"Where a lemma appears as a al., 1997) for syntactic processing."
"The corpus data synonym in more than one class, its frequency count is POS tagged and lemmatised before the LR parser is divided between all classes for which it has direct is applied."
Subcategorization patterns are extracted membership.
"The frequency counts from hyponym from the parses, these include both the syntactic cat- classes are added to the count for each hypernym egories and the argument heads of the constituents. class."
"A root node, created above all the WordNet These subcategorization patterns are then classified roots, contains the total frequency count for all the according to a set of 161 SeE classes."
The SeE en- argument head lemmas found within WordNet.
"The tries for each verb are then subjected to a statistical minimum description length principle (MDL) (Rissa-filter which removes SCFs that have occurred with nen, 1978) is used to find the best TCM by consid- ering the cost (in bits) of describing both the model and the argument head data encoded in the model."
The cost (or description length) for a TCM is cal-culated according to equation 1.
"The number of parameters of the model is given by k, this is the number of classes in the TCM minus one."
S is the sample size of the argument head data.
The cost of describing each argument head (n) is calculated us-ing the log of the probability estimate for the classes on the TCM that n belongs to (Cn). k description length = ~ x log ISI- E logp(cn) (1) nES
A small portion of the TCM for the object slot of start in the transitive frame is displayed in figure 1.
WordNet classes are displayed in boxes with a label which best reflects the sense of the class.
The prob-ability estimates are shown for the classes along the TCM.
Examples of the argument head data are dis-played below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes.
We assume that verbs which participate will show a higher degree of similarity between the preferences at the target slots compared with non-participating verbs.
To compare the preferences we compare the probability distributions across WordNet using a measure of distributional similarity.
"Since the prob-ability distributions may be at different levels of WordNet, we map the TCMs at the target slots to a common tree cut, a &quot;base cut&quot;."
We experiment with two different types of base cut.
The first is simply a base cut at the eleven root classes of WordNet.
We refer to this as the &quot;root base cut&quot; (I~BC).
The sec-ond is termed the &quot;union base cut&quot; (tJBC).
This is obtained by taking all classes from the union of the tWO TCMs which are not subsumed by another class in this union.
Duplicates are removed.
Probabilities are assigned to the classes of a base cut using the estimates on the original TCM.
The probability esti-mate for a hypernym class is obtained by combining the probability estimates for all its hyponyms on the original cut.
Figure 2 exemplifies this process for two TOMs (TCM[Footnote_1] and TCM2) in an imaginary hierarchy.
"1We also experimented with euclidian distance, the L1 norm, and cosine measures. The differences in performance of these measures were not statistically significant."
"The UBC is at the classes B, c and D."
To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence (aSD) proposed[REF_CITE]. 1
"This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence, pl(x) and p2(x) are the two probability distributions which are being compared."
The ~ constant is a value between 0 and 1 which smooths pl(x) with p2(z) so that ~SD is always defined.
We use the same value (0.99) for as Lee.
"If a is set to 1 then this measure is equivalent to the Kulback-Liebler divergence. asd(pl(x),p2(x)) = x pl(z)) + (([Footnote_1] - .) × (2)"
"1We also experimented with euclidian distance, the L1 norm, and cosine measures. The differences in performance of these measures were not statistically significant."
We experiment with a SCF lexicon produced from 19.3 million words of parsed text from the BNC[REF_CITE].
"We used the causative and conative alternations, since these have enough candidates in our lexicon for experimentation."
Evaluation is per-formed on verbs already filtered by the syntactic processing.
The SCF acquisition system has been evaluated elsewhere[REF_CITE].
We selected candidate verbs which occurred with 10 or more nominal argument heads at the target slots.
The argument heads were restricted to those which can be classified in the WordNet hypernym hi-erarchy.
Candidates were selected by hand so as to obtain an even split between candidates which did participate in the alternation (positive candidates) and those which did not (negative candidates).
Four human judges were used to determine the &quot;gold stan-dard&quot;.
The judges were asked to specify a yes or no decision on participation for each verb.
They were Mso permitted a don&apos;t know verdict.
The kappa statistic[REF_CITE]was calculated to ensure that there was significant agreement be-tween judges for the initial set of candidates.
"From these, verbs were selected which had 75% or more agreement, i.e. three or more judges giving the same yes or no decision for the verb."
For the causative alternation we were left with 46 positives and 53 negatives.
For the conative alter-nation we had 6 of each.
"In both cases, we used the Mann Whitney U test to see if there was a signifi-cant relationship between the similarity measure and participation."
We then used a threshold on the sim-ilarity scores as the decision point for participation to determine a level of accuracy.
We experimented with both the mean and median of the scores as a threshold.
Seven of the negative causative candi-dates were randomly chosen and removed to ensure an even split between positive and negative candi-dates for determining accuracy using the mean and median as thresholds.
The following subsection describes the results of the experiments using the method described in sec-tion 3 above.
Subsection 4.2 describes an experiment on the same data to determine participation using a similarity measure based on the intersection of the lemmas at the target slots.
The results for the causative alternation are dis- knit miss outline pack paint plan prescribe pull remain steal suck warn wash played in table 1 for both the rt~c and the uBc.
"The The results for the uBc experiment are very similar. relationship between participation and ~SD is highly If the median is used, the number of FPs and FNs significant in both cases, with values of p well below are evenly balanced."
This is because the median 0.01.
"Accuracy for the mean and median thresholds threshold is, by definition, taken midway between are displayed in the fourth and fifth columns."
Both the test items arranged in order of their similarity thresholds outperform the random baseline of 50%. scores.
"There are an even number of items on either The results for the vl3c are slightly improved, com- side of the decision point, and an even number of pared to those for the rtBc, however the improve- positive and negative candidates in our test sample. ment is not significant."
"Thus, the errors on either side of the decision point The numbers of false negative (FN) and false posi- are equal in number. tive (FP) errors for the mean and median thresholds For both base cuts, there are a larger number of are displayed in table 2, along with the threshold and false positives than false negatives when the mean accuracy."
The outcomes for each individual verb for is used.
"The mean produces a higher accuracy than the experiment using the RBC and the mean thresh- the median, but gives an increase in false positives. old are as follows: Many false positives arise where the preferences at both target slots are near neighbours in WordNet. • True negatives: For example, this occurred for eat and drink."
"There add admit answer believe borrow cost declare de- verbs have a high probability mass (around 0.7) un-mand expect feel imagine know notice pay per- der the entity class in both target slots, since both form practise proclaim readremember sing sur- people and types of food occur under this class."
"In vive understand win write cases like these, the probability distributions at the • True positives: asc, and frequently the UBC, are not sufficiently dis-accelerate bang bend boil break burn change tinctive. close cook cool crack decrease drop dry end ex-"
"The polysemy of the verbs may provide another pand fly improve increase match melt open ring explanation for the large quantity of false positives. rip rock roll shatter shut slam smash snap spill The SCFS and data of different senses should not split spread start stop stretch swing lilt turn ideally be combined, at least not for coarse grained wake sense distinctions."
We tested the false positive and true negative candidates to see if there was a re- • False negatives: lationship between the polysemy of a verb and its flood land march repeatterminate misclassification.
The number of senses (according • False positives: to WordNet) was used to indicate the polysemy of a ask attack catch choose climb drink eat help kick verb.
The Mann Whitney U test was performed on the verbs found to be true negative and false positive using the Rat.
A significant relationship was not found between participation and misclassification.
Both groups had an average of 5 senses per verb.
"This is not to say that distinguishing verb senses would not improve performance, provided that there was sufficient data."
"However, verb polysemy does not appear to be a major source of error, from our preliminary analysis."
"In many eases, such as read which was classified both by the judges, and the sys-tem as a negative candidate, the predominant sense of the verb provides the majority of the data."
"Alter-nate senses, for example, the book reads well, often do not contribute enough data so as to give rise to a large proportion of errors."
"Finding an appropriate inventory of senses would be difficult, since we would not wish to separate related senses which occur as alternate variants of one another."
The inventory would therefore require knowledge of the phenomena that we are endeavouring to acquire automatically.
"To show that our method will work for other RSAS, we use the conative."
Our sample size is rather small since we are limited by the number of positive can-didates in the corpus having sufficient frequency for both sets.
The sparse data problem is acute when we look at alternations with specific prepositions.
A sample of 12 verbs (6 positive and 6 negative) re-mained after the selection process outlined above.
For this small sample we obtained a significant re-sult (p = 0.02) with a mean accuracy of 67% and a median accuracy of 83%.
"On this occasion, the median performed better than the mean."
More data is required to see if this difference is significant.
This experiment was conducted using the same data as that used in the previous subsection.
"In this ex-periment, we used a similarity score on the argument heads directly, instead of generalizing the argument heads to WordNet classes."
The venn diagram in fig-ure 3 shows a subset of the lemmas at the transitive and intransitive SCFs for the verb break.
"The lemma based similarity measure is termed lemmaoverlap (LO) and is given in equation 3, where A and B represent the target slots."
"LO is the size of the intersection of the multisets of argument heads at the target slots, divided by the size of the smaller of the two multisets."
The intersection of two mul-tisets includes duplicate items only as many times as the item is in both sets.
"For example, if one slot contained the argument heads {person, person, person, child, man, spokeswoman}, and the other slot contained {person, person, child, chair, collec-tion}, then the intersection would be {person, per-son, child}, and LO would be 3 g."
This measure ranges between zero (no overlap) and I (where one set is a proper subset of that at the other slot).
"Lo(A, B) ="
"Imuttiset inlerseetion(A B)I (3) Ismallest set(A, B)I"
"Using the Mann Whitney U test on the LO scores, we obtained a z score of 2.00."
"This is significant to the 95% level, a lower level than that for the class-based experiments."
The results using the mean and median of the LO scores are shown in table 3.
Perfor-mance is lower than that for the class-based experi-ments.
"The outcome for the individual verbs using the mean as a threshold was:- • True negatives: add admit answer borrow choose climb cost de-clare demand drink eat feel imagine notice out-line pack paint perform plan practise prescribe proclaim read remain sing steal suck survive un-derstand wash win write • True positives: bend boil burn change close cool dry end fly im-prove increase match melt open ring roll shut slam smash Mart stop tilt wake • False negatives: accelerate bang break cook crack decrease drop expand flood land march repeat rip rock shatter snap spill split spread stretch swing terminate proach, whilst being applicable only to RSAs, does turn not require human input specific to the alternation at hand. • False positives: ask attack believe catch expect help kick knit[REF_CITE]identifies participation in the dative know miss pay pull remember warn and benefactive alternations."
"Lapata&apos;s strategy is to identify participants using a shallow parser and vari- Interestingly, the errors for the LO measure tend ous linguistic and semantic cues, which are specified to be false negatives, rather than false positives."
The manually for these two alternations.
PP attachments LO measure is much more conservative than the ap- are resolved using Hindle and Rooth&apos;s (1993) lexical proach using the TCMS.
In this case the median association score.
"Compound nouns, which could be threshold produces better results. mistaken for the double object construction, were For the conative alternation, the lemma based filtered using the log-likelihood ratio test."
The se-method does not show a significant relationship be- mantic cues were obtained by manual analysis.
The tween participation and the LO scores.
"Moreover, relative frequency of a SCF for a verb, compared to there is no difference between the sums of the ranks the total frequency of the verb, was used for filtering of the two groups for the Mann Whitney U test. out erroneous SCFs."
The mean produces an accuracy of 58% whilst the Lapata does not report recall and precision fig-median produces an accuracy of 50%. ures against a gold standard.
The emphasis is on the phenomena actually evident in the corpus data. 5 Related Work Many of the verbs listed in Levin as taking the al-
"There has been some recent interest in observing ternation were not observed with this alternation alternations in corpora (McCarthy and Korhonen, in the corpus data."
"This amounted to 44% of the 1998;[REF_CITE]) and predicting related verb verbs for the benefactive, and 52% for the dative. classifications[REF_CITE]."
These figures only take into account the verbs for lier work[REF_CITE]demonstrated a link be- which at least one of the SCFS were observed. 54% tween selectional preference strength and participa- of the verbs listed for the dative and benefactive by tion in alternations where the direct object is omit- Levin were not acquired with either of the target ted.
Resnik used syntactic information from the SCFs.
"Conversely, many verbs not listed in Levin bracketing within the Penn Treebank corpus."
Re- were identified as taking the benefactive or dative search into the identification of other diathesis al- alternation using Lapata&apos;s criteria.
Manual analysis ternations has been advanced by the availability of these verbs revealed 18 false positives out of 52 of automatic syntactic processing.
Most work us- candidates. ing corpus evidence for verb classification has re-
Our ap- direct object.
"These three classes were chosen be- cause a few well defined features, specified a pri-ori, can distinguish the three groups."
Twenty verbs from Levin&apos;s classification were used in each class.
They were selected by virtue of having sufficient fre-quency in a combined corpus (from the Brown and the wsJ) of 65 million words.
The verbs were also chosen for having one predominant intended sense in the corpus.
Stevenson and Merlo used four linguisti-cally motivated features to distinguish these groups.
Counts from the corpus data for each of the four fea-tures were normalised to give a score on a scale of 1 to I00.
One feature was the causative non-causative distinction.
"For this feature, a measure similar to our LO measure was used."
The four features were identified in the corpus using automatic POS tagging and parsing of the data.
"The data for half of the verbs in each class was subject to manual scrutiny, after initial automatic processing."
The rest of the data was produced fully automatically.
The verbs were classified automatically using the four features.
"The accuracy of automatic classification was 52% us-ing all four features, compared to a baseline of 33%."
The best result was obtained using a combination of three features.
This gave an accuracy of 66%.
This method relied on an estimation of the cost of us-ing TCMS to encode the argument head data at a target slot.
The sum of the costs for the two target slots was compared to the cost of a TCM for encoding the union of the argument head data over the two slots.
Results are reported for the causative alterna-tion with 15 verbs.
"This method depends on there being similar quantities of data at the alternating slots, otherwise the data at the more frequent slot overwhelms the data at the less frequent slot."
"How-ever, many alternations involve SCFs with substan-tially different relative frequencies, especially when one SCF is specific to a particular preposition."
We carried out some experiments using the MDL method and our TCMs.
"For the causative, we used a sample of 110 verbs and obtained 63% accuracy."
"For the conative, a sample of 16 verbs was used and this time accuracy was only 56%."
"Notably, only one negative decision was made because of the disparate frame frequencies, which reduces the cost of combining the argument head data."
"We have discovered a significant relationship be-tween the similarity of selectional preferences at the target slots, and participation in the causative and conative alternations."
"A threshold, such as the mean or median can be used to obtain a level of accuracy well above the baseline."
A lemma based similarity score does not always indicate a significant relation-ship and generally produces a lower accuracy.
There are patterns of diathesis behaviour among verb groups[REF_CITE].
"Accuracy may be im-proved by considering severM alternations collec-tively, rather than in isolation."
"Complementary techniques to identify alternations, for example[REF_CITE], might be combined with ours."
"Although we have reported results on only two RSAS, our method is applicable to other such alter-nations."
"Furthermore, such application requires no human endeavour, apart from that required for eval-uation."
"However, a considerably larger corpus would be required to overcome the sparse data problem for other RSA alternations."
Some funding for this work was provided by UK EP-SRC project GR/[REF_CITE]&apos;PSET: Practical Simplifi-cation of English Text&apos;.
We also acknowledge Gerald Gazdar for his helpful comments on this paper.
In this section we introduce the grammar formalism we investigate in this paper.
"This formalism, origi-nally presented[REF_CITE], is an ab-straction of the language models adopted by several state-of-the-art real-world parsers (see Section 1)."
"We specify a non-stochastic version of the formal-ism, noting that probabilities may be attached to the rewrite rules exactly as in stochastic CFG[REF_CITE]."
We assume that the reader is familiar with context-free grammars.
Here we follow the notation[REF_CITE].
"A context-free grammar (CFG) is a tuple G = (VN,VT, P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respec-tively, S E VN is the start symbol, and P is a finite set of productions having the form"
"A -+ a, where A E VN and a E (VN t3 VT)*."
"A &quot;derives&quot; relation, written ::~, is associated with a CFG as usual."
"We use the reflexive and transitive closure of =~, writ-ten =~*, and define L(G) accordingly."
The size of a CFG G is defined as IGI = ~(A--+a)EP ]
"If every production in P has the form A --+BC or A --+ a, for A,B,C E VN,a E VT, then G is said to be in Chomsky Normal Form (CNF)."
"A CFG G = (VN,VT,P, S[$]) in CNF is called a bilexical context-free grammar if there exists a set VD, called the set of delexicalized nontermi-nals, such that nonterminals from VNare of the form A[a], consisting of A E VD and a E VT, and every production in P has one of the following two forms: (i) A[a] ~ B[b] C[c], a E {b, c); (ii) A[a] ~ a."
A nonterminal A[a] is said to have terminal symbol a as its lexlcal head.
"Note that in a parse tree for G, the lexical head of a nonterminal is always &quot;in-herited&quot; from some daughter symbol (i.e., from some symbol in the right-hand side of a production)."
"In the sequel, we also refer to the set VT as the lexicon of the grammar."
A bilexical CFG can encode lexically specific pref-erences in the form of binary relations on lexi-cal items.
"For instance, one might specify P as to contain the production VP[solve] -+ V[solve] NP[puzzles] but not the production VP[eat] --4 V[eat] NP[puzzles]."
"This will allow derivation of some VP constituents such as &quot;solve two puzzles&quot;, while forbidding &quot;eat two puzzles&quot;."
See[REF_CITE]for further discussion.
The cost of this expressiveness is a very large grammar.
"Indeed, we have IGI = O(]VD[3&quot; IVT[2), and in practical applications ]VTI&gt;&gt; IVDI&gt; 1."
"Thus, the grammar size is dominated in its growth by the square of the size of the working lexicon."
"Even if we conveniently group lexical items with distributional similarities into the same category, in practical ap-plications the resulting grammar might have several thousand productions."
Parsing strategies that can-not work in sublinear time with respect to the size of the lexicon and with respect to the size of the whole input grammar are very inefficient in these cases. 3 Correct-prefix property
So called left-to-right strategies are standa~dly adopted in algorithms for natural language pars-ing.
"Although intuitive, the notion of left-to-right parsing is a concept with no precise mathematical meaning."
"Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then per-form syntactic analysis with a non-left-to-right strat-egy."
"In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing[REF_CITE]."
"Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algor!thm[REF_CITE], tab-ular left-corner and PLR parsing[REF_CITE]and tabular LR parsing[REF_CITE]."
Let VT be some alphabet.
"A generic string over VT is denoted as w = al &quot;-an, with n _&gt;0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i,j] to denote string aiai+l&quot; .aj; if i &gt; j, we define w[i, j] = c."
"Let G -- (VN,VT,P,S) be a CFG and let w = al ... an with n _&gt;0 be some string over VT."
"A rec-ognizer for the CFG class is an algorithm R that, on input (G,w), decides whether w E L(G)."
We say that R satisfies the correct-prefix property (CPP) if the following condition holds.
"Algorithm R processes the input string from left-to-right, &quot;con-suming&quot; one symbol ai at a time."
"If for some i, 0 &lt; i &lt; n, the set of derivations in G having the form S ~* w[1, i]7, 7 E (VN U VT)*, is empty, then R rejects and halts, and it does so before consuming symbol ai+l, if i &lt; n. In this case, we say that R has detected an error at position i in w. Note that not require grammar precompilation is perhaps Ear- the above property forces the recognizer to do rele- ley&apos;s algorithm[REF_CITE]."
"We show here that vant computation for each terminal symbol that is methods in this family cannot be extended to work consumed. in time independent of the size of the lexicon, in"
"We say that w[1,i] is a correct-prefix for a lan- contrast with bidirectional recognition algorithms. guage L if there exists a string z such that w[1,i]z E"
The result presented below rests on the follow- L.
"In the natural language parsing literature, the ing, quite obvious, assumption."
"There exists a con- CPP is sometimes defined with the following condi- stant c, depending on the underlying computation tion in place of the above."
"If for some i, 0 &lt; i &lt; n, model, such that in k &gt; 0 elementary computation w[1, if is not a correct prefix for L(G), then R rejects steps any recognizer can only read up to c. k pro-and halts, and it does so before consuming symbol ductions from set P. In what follows, and without ai+i,~ if i &lt; n. Note that the latter definition asks any loss of generality, we assume c = 1."
"Apart from for a stronger condition, and the two definitions are this assumption, no other restriction is imposed on equivalent only in case the input grammar G is re- the representation of the input grammar or on the duced,i While the above mentioned parsing algo- access to the elements of sets VN, VT and P. rithms satisfy the former definition of CPP, they do Theorem 1 Let f be any function of two variables not satisfy the latter."
"Actually, we are not aware of defined on natural numbers."
"No recognizer for bilexi-any practically used parsing algorithm that satisfies cal context-free grammars that satisfies the CPP can the latter definition of CPP. run on input (G,wl in an amount of time bounded One needs to distinguish CPP parsing from Some by f([VDI, [W[), where VD is the set of delexicatized well known parsing algorithms in the literature that nonterminals of G. process symbols in the right-hand sides of each gram- mar production from left to right, but that do not Proof."
"Assume the existence of a recognizer R sat- exhibit any left-to-right dependency between differ- isfying the CPP and running in I(IVDI, IwL) steps or ent productions."
"In particular, processing of the less."
We show how to derive a contradiction. right-hand side of some production may be initi-
Let q &gt;_ 1 be an integer.
"Define a bilexical CFG ated at some input position without consultation of Ga = (Vr~,V~, Pq, d[bi]) where V~ contains q + 2 productions or parts of productions that may have distinct symbols {bi,...,bq+2} and been found to cover parts of the input to the left of that position."
"These algorithms may also consult V~ = {A[b,]l l&lt;i&lt;q+l}u{T[b]lbeV~}, input symbols from left to right, but the processing and where set pa contains all and only the following that takes place to the right of some position i does productions: not strictly depend on the processing that has taken place to the left of i. Examples are pure bottom-up (i) A[b,] --+"
"A[b,+i] T[b,], 1 &lt; i &lt; q; methods, such as left-corner parsing without top- down filtering[REF_CITE]. (ii) A[bq+i] -+ T[bq+2]"
"Algorithms that do satisfy the CPP make use of (iii) T[b]-+ b, b E V~. some form of top-down prediction."
"Top-down pre- Productions in (i) are called bridging productions. diction can be implemented at parse-time as in the Note that there are q bridging productions in Gq. case of Earley&apos;s algorithm by means of the &quot;predic- Also, note that V~ = {A,T} does not depend on tor&quot; step, or can be precompiled, as in the case of the choice of q. Thus, we will simply write VD. left-corner parsing[REF_CITE], Choose q &gt; max{f(IVD[,2),l }."
"On input by means of the left-corner relation, or as in the case (Gq, bq+2bq+i), R does not detect any error at posi- of LR parsers[REF_CITE], tion 1, that is after having read the first symbol bq+2 through the closure function used in the construc-of the input string."
"This is because A[bl] ~* bq+2~/ tion of LR states. 4 Recognition without precompilation with 3&apos; =- T[ba+i]T[ba]T[bq-i]&apos;&quot;T[bi] is a valid derivation in G. Since R executes no more than f(IVD] ,2) steps, from our assumption that reading a production takes unit time it follows that there must be an integer k, 1 &lt; k &lt; q, such that bridging In this section we consider recognition algorithms production A[bk] --+"
A[bk+i] T[bk] is not read from that do not require off-line compilation of the input Gq.
Construct then a new grammar GI~ by replacing grammar.
"Among algorithms that satisfy the CPP, in Gq the production A[bk] --+"
A[bk+l] T[bk] with the most popular example of a recognizer that does the new production A[bk] --+T[bk]
"A[bk+i], leaving i A context-free grammar G is reduced if every nonterminal everything else unchanged."
"It follows that, on in-of G can be part of at least one derivation that rewrites the put (G~, ba+2ba+i), R behaves exactly as before and start symbol into some string of terminal symbols. does not detect any error at position 1."
"But this is a contradiction, since there is no derivation in G~ of the form A[bl] =~* bq+2&quot;Y,7 E (VN U VT)*, as can be easily verified. •"
We can use the above result in the comparison of left-to-right and bidirectional recognizers.
The recognition of bilexical context-free languages can be carried out by existing bidirectional algorithms in time independent of the size of the lexicon and without any precompilation of the input bilexical grammar.
"For instance, the algorithms presented[REF_CITE]allow recognition in time O(IVDI3IwI4).2 Theorem 1 states that this time bound cannot be met if we require the CPP and if the input grammar is not precompiled."
"In the next section, we will consider the possibility that the in-put grammar is in a precompiled form."
"In this section we consider recognition algorithms that satisfy the CPP and allow off-line, polynomial-time compilation of the working grammar."
We focus on a class of bilexical context-free grammars where recognition requires the stacking of a number of un-resolved lexical dependencies that is proportional to the length of the input string.
We provide evidence that the above class of recognizers perform much less efficiently for these grammars than existing bidirec-tional recognizers.
We assume that the reader is familiar with the notions of deterministic and nondeterministic finite automata.
We follow here the notation[REF_CITE].
"A nondeterministic finite au-tomaton (FA) is a tuple M = (Q, E, 5,q0,F), where Q and P. are finite, disjoint sets of state and alphabet symbols, respectively, qo E Q and F _CQ are the ini-tial state and the set of final states, respectively, and is a total function mapping Q x ~ to 2Q, the power-set of Q. Function 5 represents the transitions of the automaton."
"Given a string w = al &quot;&quot;an, n &gt; O, an accepting computation in M for w is a sequence qo, al,ql,a2,q2 .... ,an,q,, such that qi E 5(qi-l,ai) for 1 &lt;i &lt; n, and q~ E F. The languageL(M) is the set of all strings in E* that admit at least one accepting computation in M. The size of M is de-fined as ]M] = ~qeQ,ae~ I~(q,a)l."
"The automaton M is deterministic if, for every q E Q and a E ~, we have IS(q, a)] = 1."
We call quasi-determinizer any algorithm A that satisfies the following two conditions: 1.
"A takes as input a nondeterministic FA M --= (Q, ~, 5, qo,F) and produces as output a device DM that, when given a string w as input, de-cides whether w E L(M); and 2. there exists a polynomial PA such that every DM runs in an amount of time bounded by PA(Iwl)."
"We remark that, given a nondeterministic FA M specified as above, known algorithms allow simula-tion of M on an input string w in time O(IM I IwI) (see for instance ([REF_CITE]Thin. 9.5) or ([REF_CITE]Thm. 3.38))."
"In contrast, a quasi-determinizer produces a device that simulates M in an amount of time independent of the size of M itself."
"A standard example of a quasi-determinizer is the so called power-set construction, used to convert a nondeterministic FA into a language-equivalent deterministic FA (see for instance ([REF_CITE]Thin. 2.1) or ([REF_CITE]Thm. 3.30))."
"In fact, there exist constants c and d such that any deterministic FA can be simulated on input string w in an amount of time bounded by c ]wI+ d."
"This requires function to be stored as a IQ] x ]El, [Footnote_2]-dimensional array with values in Q."
"2More precisely, the running time for these algorithms is O(IVDI3Iw[3min{[VT[, [w[}). In cases of practical interest, we always have Iw[ &lt; IVT[."
This is a standard representation for automata-like structures; see ([REF_CITE]:Sect. 6.5) for discussion.
"We now pose the question of the time efficiency of a quasi-determinizer, and consider the amount of time needed in the construction of DM."
"In[REF_CITE]it is shown that there exist (infinitely many) nonde-terministic FAs with state set Q, such that any language-equivalent deterministic FA must have at least 2IQ}states."
This means that the power-set con-struction cannot work in polynomial time in the size of the input FA.
"Despite of much effort, no algo-rithm has been found, up to the authors&apos; knowledge, that can simulate a nondeterministic FA on an input string w in linear time in&apos; Iwl and independently of IMI, if only polynomial-time precompilation of M is allowed."
"Even in case we relax the linear-time re-striction and consider recognition of w in polynomial time, for some fixed polynomial, it seems unlikely that the problem can be solved if only polynomial-time precompilation of M is allowed."
"Furthermore, if we consider precompilation of nondeterministic FAs into &quot;partially determinized&quot; FAs that would allow recognition in polynomial (or even exponen-tial) time in Iw], it seems unlikely that the analysis required for this precompilation could consider less than exponentially many combinations of states that may be active at the same time for the original non-deterministic FA."
"Finally, although more powerful formalisms have been shown to represent some regu-lar languages much more succinctly than FAs[REF_CITE], while allowing polynomial-time parsing, it seem unlikely that this could hold for reg-ular languages in general."
"Conjecture There is no quasi-determinizer that works in polynomial time in the size of the input c[($,q0)l automaton."
"Before turning to our main result, we need develop some additional machinery."
"Let M (Q,E,6, qo,F) be a nondeterministic FA and w = al..-an E L(M), where n &gt; 0. cat CFG GM -- (VN, VT, e~ C[($, qo)]) is specified as follows:"
"Figure I: A derivation in GM for string ala2a3#(a3, qs)(a2, q2)(ax, ql)($, q0). (i) vN = {TIff I ~ • VT} U {C[~],C&apos;[.]"
"I ~ • a}; (ii) VT = A U ~ U {•}; (iii) P contains all and only the following produc- ff and only ff tions: (a) for each a • VT, T[a] -, a;"
"C[($, qo)] ~* al&apos;.&quot; akg[Cak, qk)](at¢-l, qk-x) •••($, qo)."
"The claim can be proved by induction on k, using (b) for each (a,q),(a&apos;,q*) •"
"A such that q* • productions (a) to (c) from Definition 1. 5(q,a&apos;),"
"C[(a, q)] ~ C&apos;[(a&apos;, 4)] T[(a, q)]; (C) for each (a,q) •"
"C&apos;[(a,q)] ~ T[a]"
"C[(a,q)]; (d) .for each (a, q) •"
"A such that q • F, C[(a, q)] -, T[#] T[(a, q)]."
"Let R denote the reverse operator on strings.3 From the above claim and using production (d) from Definition 1, one can easily show that"
"L(GM) = {w#u [ w E L(M), uR encodes an accepting computation for w}."
The lemma directly follows from this relation. • We give an example of the above construc-
We can now provide the main result of this sec-tion.
Consider an automaton M and a string tion.
"To this end, we refine the definition of rec-w = ala2a3 such that w • L(M)."
Let ognizer presented in Section 3.
"A recognizer for the ($,qo)(al,ql)(a2,q2)(a3,q3) be the encoding of an CFG class is an algorithm R that has random access accepting computation in M for w."
"Then the to some data structure C(G) obtained by means of string ala2a3~(a3, q3)(a2, q2)(al, ql)($, qo) belongs some off-line precompilation of a CFG G."
On in-to L(GM).
"The tree depicted in Figure I represents put w, which is a string on the terminal symbols of a derivation in GM of such a string."
"G, R decides whether w E L(G)."
The definition of The following fact will be used below. the CPP extends in the obvious way to recognizers Lemma 1
"For each w • E*, w# is a correct-prefix working with precompiled grammars. for L(GM) if and only if w • L(M)."
Theorem 2 Let p be any polynomial in two vari-
Outline of the proof.
We claim the following ables. 1] the conjecture about quasi-determinizers fact.
"For each k &gt; 0, al,a2,...,ak • ~ qo, ql , . . . , qk • Q we have qi • 6(qi-l,ai), for all i (1 &lt; i &lt; k), and holds true, then no recognizer exists that 3Note that R does not affect individual symbolsin a string."
"Thus (a, q)R = (a, q). (i) has random access to data structure C(G) pre-compiled from a bilexical CFG G in polynomial time in IGI, (ii) runs in an amount of time bounded by p(IVDI, Iwl), where VD is the set of delexicalized nonterminals of G and w is the input string, and (iii) satisfies the CPP."
Assume there exists a recognizer R that sat-isfies conditions (i) to (iii) in the statement of the theorem.
We show how this entails that the conjec-ture about quasi-determinizers is false.
We use algorithm R to specify a quasi-determinizer A.
"Given a nondeterministic FA M, A goes through the following steps. 1."
A constructs grammar GM as in Definition 1. 2.
"A precompiles GM as required by R, producing data structure C(GM). 3."
A returns a device DM specified as follows.
"Given a string w as input, DM runs R on string w~."
"If R detects an error at any position i, 0 &lt; i &lt; Iw#l, then DM rejects and halts, oth-erwise DM accepts and halts."
From Lemma 1 we have that DM accepts w if and only if w E L(M).
"Since R runs in time P(]VDI, Iwl) and since GM has a set of delexicalized nonterminals independent of M, we have that there exists a poly-nomial PA such that every DM works in an amount of time bounded by pA(IWl)."
We therefore conclude that A is a quasi-determinizer.
It remains to be shown that A works in polyno-mial time in IMI.
Step 1 can be carried out in time O(IM[).
"The compilation at Step 2 takes polynomial time in IGM], following our hypotheses on R, and hence polynomial time in IMI, since IGMI = O(IMI)."
"Finally, the construction of DM at Step 3 can easily be carried out in time O(IMI) as well. •"
"In addition to Theorem 1, Theorem 2 states that, even in case the input grammar is compiled off-line and in polynomial time, we cannot perform CPP recognition for bilexical context-free grammars in time polynomial in the grammar and the input string but independent of the lexicon size."
This is true with at least the same evidence that sup-ports the conjecture on quasi-determinizers.
"Again, this should be contrasted with the time performance of existing bidirectional algorithms, allowing recog-nition for bilexical context-free grammars in time O(IVDI3 Iwl )."
"In order to complete our investigation of the above problem, in Appendix A we show that, when we drop the polynomial-time restriction on the grammar pre-compilation, it is indeed possible to get rid of any IVT] factor from the running time of the recognizer."
Empirical results presented in the literature show that bidirectional parsing strategies can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items.
In this paper we have provided an original mathe-matical argument in favour of this thesis.
Our re-sults hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1).
"We perceive that these results can be ex-tended to other language models that properly em-bed bilexical context-free grammars, as for instance the more general history-based models used[REF_CITE]and[REF_CITE]."
We leave this for future work.
Acknowledgments are relatively rare in human-computer interaction.
"Are people unwilling to use this human convention when talking to a machine, or is their scarcity due to the way that spoken-lan-guage interfaces are designed?"
"We found that, given a simple spoken-language interface that pro-vided opportunities for and responded to acknowl-edgments, about half of our subjects used acknowledgments at least once and nearly 30% used them extensively during the interaction."
"As our ability to build robust and flexible spoken-language interfaces increases, it is worthwhile to ask to what extent we should incorporate various human-human discourse phenomena into our dia-logue models."
"Many studies have shown that peo-ple alter their dialogue techniques when they believe that they are talking to a computer (e.g.,[REF_CITE]), so it is not clear that observations of human-human conversation will provide us with the guidance we need."
"At the same time, we cannot always look to current systems to determine which discourse phenomena should be supported."
"Cur-rent-generation interfaces are still relatively fragile, and so designers of spoken-language systems go to some effort to structure dialogues and create prompts that guide the user toward short, content-ful, in-vocabulary responses (e.g.,[REF_CITE])."
One result of this approach is the suppression of meta-dialogue acts such as acknowledgement and repeti-tion.
"The term &quot;acknowledgment&quot; is[REF_CITE], who describe a hierarchy of meth- heeman@cse, ogi. edu ods by which one conversant may signal that another&apos;s contribution has been understood well enough to allow the conversation to proceed."
Acknowledgments often appear in American English conversation as an &quot;okay&quot; or &quot;uh-huh&quot; that signals understanding but not necessarily agree-ment.
"These are also called &quot;back channels&quot; or &quot;prompts&quot; (e.g., Chu-Carroll &amp;[REF_CITE]),"
"Closely related to acknowledgments are repeti-tions, in which the conversant provides a stronger signal that a contribution has been understood by repeating part or all of the other&apos;s contribution."
"Repetitions are also referred to as &quot;paraphrases&quot; (Traum &amp;[REF_CITE]), &quot;echoing&quot;[REF_CITE], and &quot;demonstration&quot; (Clark &amp;[REF_CITE])."
"Repetitions are often seen when one is con-veying complex information, such as when one copies an address or telephone number."
"Neither acknowledgments nor repetitions con-tribute new domain information to the conversa-tion, but they serve to assure the speaker that information has been conveyed successfully."
"Acknowledgments also play a role in managing turn-taking in mixed-initiative dialogue; although acknowledgments may preface a new contribution by the same speaker (Novick &amp;[REF_CITE]), often they occur alone as a single-phrase turn that appears to serve the purpose of explicitly declining an opportunity to take a turn[REF_CITE]."
Acknowledgments and repetitions are ubiqui-tous in many types of human-human conversation.
"In a corpus of problem-solving spoken dialogues, for example,[REF_CITE]found that 51% of turns began with or consisted of an explicit acknowledgment."
"Given this, one would expect that acknowledgments should be modeled in dia-logue models for spoken-language systems, and indeed some research models are beginning to acknowledgments, perhaps even gain an advantage incorporate acknowledgments, e.g., Kita et al from doing so, while still keeping the behavior (1996),[REF_CITE], Iwase &amp;[REF_CITE]. optional."
Typical human-computer dialogue models are We decided to focus on a somewhat narrow use structured in ways that suppress the use of of acknowledgments.
Conversants are especially acknowledgments.
"In many systems turn-taking is likely to offer acknowledgments and repetitions completely controlled by one conversant, e.g., the when complex information is being presented, user responds to system prompts, which tends to especially when the conversant is copying the eliminate the need for acknowledgments as a turn- information."
While this is certainly explainable in taking mechanism.
"In other systems, the use of terms of mutuality of understanding, this particular barge-in defeats the common interpretation of an use of acknowledgment may be viewed from a acknowledgment; if the user speaks, the system more mechanical standpoint as regulating the pace contribution is cut off before the user utterance is at which information is presented."
This insight sug-interpreted.
"If that utterance was intended to signal gested to us that a fruitful task for this study might that the contribution should continue, the effect is be one in which the subject is asked to write down exactly the opposite of the one desired. verbally-presented information, as when taking"
"Thus, current design practices both discourage messages over the telephone. and render meaningless the standard uses of 2.2 Task acknowledgments."
If these impediments were
"We selected the domain of telephone interface to removed, would people choose to use acknowledg- email and designed a task in which subjects were ments when interacting with a computer interface? asked to transcribe items of information from the messages."
"Writing is slow in comparison to speak- 2 Experiment ing, so we anticipated that subjects would require a This study was designed as a pilot to our larger slower pace of information presentation when they investigation into the effects of incorporating were writing."
The messages included information acknowledgement behavior in dialogue models for not asked for on the question list to simulate &quot;unin-spoken-language interfaces.
Before we attempted teresting&quot; material that the subject would want to to compare interfaces with and without acknowl- move through at a faster pace.
"In this way we edgement behavior, we wanted to understand hoped to motivate subjects to try to control the whether people are willing to use this sort of meta- pace at which information was presented. dialogue behavior when interacting with a com- The email was presented in segments roughly puter. corresponding to a long phrase."
"After each seg-ment, the system paused to give the subject time to 2.1 Approach make notes."
"If the subject said nothing, the system In this study we hypothesized that subjects will would continue by presenting the next message choose to use acknowledgments in human-com- segment."
"Subjects could accept--and perhaps puter interaction if they are given an interface that make use of--this delay, or they could reduce it by provides opportunities for and responds to acknowledging the contribution, e.g., &quot;okay,&quot; or by acknowledgments. commanding the system to continue, e.g., &quot;go on.&quot;"
"In designing the study, we assumed that it The system signalled the possibility of controlling would not immediately occur to subjects that they the delay by prompting the subject &quot;Are you ready could use acknowledgments to a computer."
At the to go on?&quot; after the first pause.
"This prompting was same time, we did not want to explicitly instruct or repeated for every third pause in which the subject require subjects to use acknowledgment behavior, said nothing."
In this way we hoped to suggest to as that would tell us nothing about their prefer- the subjects that they could control the wait time if ences.
"We therefore decided against a comparison/ desired without explicitly telling them how to do control-group experimental design for this initial SO. study and instead focused on creating a situation in On the surface, there is no functional differ-which subjects would have a reason to use ence in system behavior between a subject&apos;s use of a command to move the system onward (e.g., &quot;go on&quot; &quot;next&quot;, &quot;continue&quot;) and the use of an acknowl-edgment (&quot;okay,&quot; &quot;uh-huh&quot;, or a repetition)."
"In either case, the system responds by presenting the next message segment, and in fact it eventually presents the next segment even if the subject says nothing at all."
"Thus, the design allows the subject to choose freely between accepting the system&apos;s pace (system initiative), or commanding the system to continue (user initiative), or acknowledging the presentations in a fashion more typical of mixed-initiative human conversation."
"In this way, we hoped to understand how the subject preferred to interact with the computer."
"Subjects were told that the study&apos;s purpose was to assess the understandability and usability of the interface, and that their task was to find the answers to a list of questions."
"They were given no instructions in the use of the program beyond the information that they could talk to it using normal, everyday speech."
"Ten were female, four were male."
Ages ranged from 13 to 57.
"All used comput-ers, typically office software and games, but none had significant programming experience."
"Each ses-sion lasted about 45 minutes total, and each subject was paid $10.00."
One subject declined payment.
"As mentioned earlier, one difficulty with recogniz-ing acknowledgements in spoken-language inter-faces is that the use of barge-in tends to defeat the purpose of acknowledgments when they occur in overlapped speech."
We used a Wizard of Oz proto-col as a simple way to allow the system to respond to such utterances and to provide robustness in handling repetitions.
The wizard&apos;s interface was constructed using the Rapid Application Developer in the Center for Spoken Language Understanding Toolkit[REF_CITE].
A simple button panel allowed the wizard to select the appropriate response from the actions supported by the application.
The applica-tion functionality was deliberately limited to sug-gest realistic abilities for a current spoken-language interface.
"Using messages pre-recorded in a synthesized voice, the wizard was able to direct the system to: • Read a list of all messages. • Begin reading a particular message. • Read the next message segment. • Repeat the current message segment. • Repeat the previous message segment. • Ask the subject whether the program should continue reading the current message. • Ask the subject to what to do next. • End the program. • Play one of several error and help messages."
"The texts of the email messages were pre-sented in phrases of varying lengths, with each phrase followed by a pause of about five seconds."
"Preliminary tests showed that the combined response time of the wizard and the interface was between one and two seconds, and that pauses of less than five seconds were not obviously different from the normal pace of system response."
"Five sec-onds is a long response time, uncomfortably so for human-human conversation, so we hoped that this lengthy pause would encourage the subjects to take the initiative in controlling the pace of the interac-tion."
The messages were divided into segments by hand.
"The divisions were intended to simulate a phrase-level presentation, although some short phrases were combined to make the presentation less choppy."
An example of one message and its division into phrases may be seen in Figure 1.
Synthesized speech from the Festival speech synthesizer[REF_CITE]was used through-out the interface.
"The message texts were presented in a synthesized male voice, while the control por-tions of the interface used a synthesized female voice."
"Default pronunciations were used except when the default was incorrect, e.g., &quot;read&quot; defaulted to the past-tense pronunciation in all con-texts."
"Also, there was minor use of the SABLE markup language[REF_CITE]to flatten the pitch range at the end of phrases in list items; the intent was to suggest the prosody of list contin-uation rather than the default sentence-final drop."
"Message six is from Jo at teleport dot com, about, please stop by store on your way home."
"I&apos;m going to be late getting home tonight, so would you please stop by the store on your way home?"
"We need milk, eggs, a bunch of spinach, fresh ginger, green onions, maple syrup, a pound of coos-coos, mild curry powder, a pound of coffee, and a package of seventy five watt light bulbs."
See you tonight.
The subject&apos;s list of questions included &quot;What items are you supposed to pick up at the store?&quot;
"To improve the understandability, both voices were 2.6 slowed slightly to 90% of the default speaking rate."
"Although we had not formed any hypothesis about other dialogue behaviors, we noticed several inter-esting dialogue behaviors that we had not antici-pated."
"We were surprised at the number of subjects who exhibited politeness behavior toward the inter-face, either saying &quot;please&quot; when issuing com-mands to the computer or responding to the program&apos;s &quot;good-bye&quot; at the end of the session."
"One subject used &quot;please&quot; throughout the interac-tion, but a more common pattern was to use &quot;please&quot; at the beginning of the session and to drop the behavior as the interface became more familiar."
"Politeness did not seem to be strongly associated with a willingness to use acknowledgments, how-ever; four of the nine subjects who exhibited politeness did not use any acknowledgments in their interaction."
"Despite the deliberately-artificial interface, several subjects responded at least once to the mes-sage content as if they were talking to the message"
Subject thanks the interface.
The system is reading the text of one of the messages. sender.
"In the excerpt shown in Figure 2., for exam-"
"Furthermore, the interface did not offer acknowl-ple, the subject replied &quot;I&apos;hank you&quot; to the message edgments to the subjects, and the subjects were text&apos;s &quot;thank you.&quot; This did not appear to be a mat- given no instructions suggesting that the interface ter of misunderstanding the capabilities of the understood acknowledgments."
"In fact two subjects interface; the subject later reported that despite the who did use acknowledgments expressed surprise synthesized voices she had briefly forgotten that that they had worked, and two who had not used she wasn&apos;t talking to her secretary. acknowledgments reported that they would proba- Three subjects also made one or more meta- bly have used them if they had known it would comments, e.g., &quot;ah, there it is&quot; when finding a par- work. ticular piece of information."
These may have been at least partially an artifact of the &quot;treasure hunt&quot;
It is interesting to consider these results in light of those reported[REF_CITE].
They nature of the task.
"When questioned in the post-describe a Japanese-language Wizard-of-Oz study experiment interview, subjects didn&apos;t seem aware in which the subjects were given some instruction that they&apos;d made these comments."
"All but one of on using the system, and in which the system both these instances were followed immediately by a presented and accepted back-channel feedback. command, so the wizard responded to the com-"
They found that even when the interface offered mand and ignored the meta-comment.
The one back channels itself the rate of subject back-chan-stand-alone meta-comment was treated as an nels was somewhat lower in human-computer unrecognized command (an error message was interaction than in comparable human-human con-played). versation.
This makes the fact that our interface elicited acknowledgments without offering them 4 Discussion even more encouraging.
"Clearly, some people are willing to utilize this human conversational con- Subjects were provided with three methods for vention in human-computer dialogue."
"Our post-controlling the pace at which information was pre-experiment interviews suggest, however, that some sented: silence, command, or acknowledgment/ people will find the use of acknowledgements repetition."
"The majority of the subjects used com-strange or uncomfortable in human-computer inter-mands more than they used acknowledgments, but action."
"While self-reports of attitudes toward hypo-over one half used an acknowledgment or repeti-thetical situations must be treated with some tion at least once during their interaction and caution, it seems reasonable to assume that even nearly 30% used acknowledgments in preference when such interfaces become available there will to commands."
"This occurred despite the fact that be users who will prefer to interact with computers subjects were given no reason to think that this using commands. behavior would be effective: the interface was deliberately limited in functionality, and voice syn- thesis was used instead of recorded voice to"
Will attitudes and conversational behavior change as people gain experience with more emphasize the artificial nature of the interaction. advanced spoken-language interfaces?
Despite the relatively short duration of this test--most subjects completed the task itself in 15-20 minutes--some changes in behavior could be observed over the course of the dialogue.
"In particular, politeness behaviors were likely to be seen early in the dia-logues and then diminish as the subjects became more comfortable with their interaction."
We specu-late that the use of politeness words did not reflect a strong underlying politeness toward the computer so much as a falling back on human conventions when faced with an unfamiliar dialogue situation.
"One subject who had used &quot;please&quot; 21 times dur-ing the interaction, for example, simply hung up without warning when she had finished."
"This con-trasts, however, with the findings[REF_CITE]that people do offer socially-desirable behavior to computers."
Would a better voice increase the incidence of acknowledgment behavior?
"Several subjects thought it would, and even with the current synthe-sized voices we saw several examples of subjects seemingly forgetting briefly that they were not talking to a human."
We plan to explore this ques-tion in future work.
"We conducted a preliminary study to examine the willingness of subjects to use a particular dialogue act, acknowledgment, in human-computer interac-tion."
"Although the number of subjects was small, we saw that about half of our subjects used acknowledgements or repetition at least occasion-ally to control the pace at which information was presented, and about 29% used acknowledgments more frequently than they used commands for that purpose."
Our immediate plans include extending this study to a larger and gender-balanced group of sub-jects so that we can draw firmer quantitative con-clusions about the percentage of people who are likely to prefer this style of interaction.
"In particu-lar, we cannot conclude from the current study&apos;s small sample how strong the preference for using acknowledgment might be, especially among male subjects."
"Also, in our current study the subject achieved no functional benefit in using acknowl-edgments."
"With better estimates of subject prefer-ences, we can then proceed to our larger goal of comparing the usefulness and user acceptability of spoken language dialogue models with and without acknowledgment behavior (c.f.[REF_CITE])."
We also plan to explore the effect of the quality of the synthesized voice on the incidence of acknowledg-ment behavior.
Alignment of phonetic sequences is a necessary step in many applications in computational phonol-ogy.
"After discussing various approaches to pho-netic alignment, I present a new algorithm that com-bines a number of techniques developed for se-quence comparison with a scoring scheme for com-puting phonetic similarity on the basis of multival-ued features."
"The algorithm performs better on cog-nate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature."
Identification of the corresponding segments in se-quences of phones is a necessary step in many appli-cations in both diachronic and synchronic phonol-ogy.
"Usually we are interested in aligning sequences that represent forms that are related in some way: a pair of cognates, or the underlying and the sur-face forms of a word, or the intended and the ac-tual pronunciations of a word."
"Alignment of pho-netic sequences presupposes transcription of sounds into discrete phonetic segments, and so differs from matching of utterances in speech recognition."
"On the other hand, it has much in common with the alignment of proteins and DNA sequences."
Many methods developed for molecular biology can be adapted to perform accurate phonetic alignment.
"Alignment algorithms usually contain two main components: a metric for measuring distance be-tween phones, and a procedure for finding the op-timal alignment."
The former is often calculated on the basis of phonological features that encode cer-tain properties of phones.
"An obvious candidate for the latter is a well-known dynamic programming (DP) algorithm for string alignment[REF_CITE], although other algorithms can used as well."
The task of finding the optimal alignment is closely linked to the task of calculating the distance between two sequences.
The basic DP algorithm accomplishes both tasks.
"Depending on the appli-cation, either of the results, or both, can be used."
"Within the last few years, several different ap-proaches to phonetic alignment have been reported."
"In a follow-up paper[REF_CITE], he extended the algorithm to align words from more than two languages."
Some characteristics of these implementations are juxta-posed in Table 1.
"In this paper, I present a new algorithm for the alignment of cognates."
It combines various tech-niques developed for sequence comparison with an appropriate scoring scheme for computing phonetic similarity on the basis of multivalued features.
"The new algorithm performs better, in terms of accuracy and efficiency, than comparable algorithms reported[REF_CITE]and[REF_CITE]."
"Although the main focus of this paper is diachronic phonol-ogy, the techniques proposed here can also be ap-plied in other contexts where it is necessary to align phonetic sequences."
"To align phonetic sequences, we first need a func-tion for calculating the distance between individual phones."
"The numerical value assigned by the func-tion to a pair of segments is referred to as the cost, or penalty, of substitution."
"The function is often ex-tended to cover pairs consisting of a segment and the null character, which correspond to the opera- lions of insertion and deletion (also called indels). a good indication of their proximity."
A distance function that satisfies the following ax- ments are represented by binary vectors in which ioms is called a metric: every entry stands for a single articulatory feature.
The penalty for a substitution is defined as the Ham-1.
"Va, b :d(a, b) &gt;_0 (nonnegative property) ming distance between two feature vectors."
The penalty for indels is established more or less arbi- 2.
"Va, b :d(a,b) = 0 ¢~ a = b (zero property) trarily.2 A distance function defined in such a way 3."
"Va, b :d(a,b) = d(b,a) (symmetry) satisfies all metric axioms. 4."
"Va, b, c :d(a, b) + d(b, c) &gt; d(a, c) (triangle in-"
It is interesting to compare the values of Cov-equality) ington&apos;s distance function with the average Ham-ming distances produced by a feature-based met- 2.1 Covington&apos;s Distance Function vs. ric.
"Since neither[REF_CITE]nor Feature-Based[REF_CITE]present their fea-ture vectors in sufficient detail to perform the cal-[REF_CITE], for his cognate alignment algo-rithm, constructed a special distance function."
"It culations, I adopted a fairly standard set of 17 bi-nary features[REF_CITE].3 The average was developed by trial and error on a test set of 82 cognate pairs from various related languages."
"The feature distances between pairs of segments corre-distance function is very simple; it uses no phono- sponding to every clause in Covington&apos;s distance logical features and distinguishes only three types function are given in Table 2, next to Covington&apos;s of segments: consonants, vowels, and glides."
"By definition, the Hamming distance important characteristics of sounds, such as place between identical segments is zero."
"The distance or manner of articulation, are ignored."
"For example, between the segments covered by clause #3 is also both yacht and will are treated identically as a glide- constant and equal to one (the feature in question vowel-consonant sequence."
The function&apos;s values being [long] or [syllabic]).
The remaining average for substitutions are listed in the &quot;penalty&quot; column feature distances were calculated using a set of most in Table 2.
The penalty for an indel is 40 if it is pre- frequent phonemes represented by 25 letters of the Latin alphabet (all but q).
"In order to facilitate com-ceded by another indel, and 50 otherwise."
The very high correlation (0.998)[REF_CITE]and Nerbonne between Covington&apos;s penalties and the average dis-and[REF_CITE]use distance functions based tances demonstrates that feature-based phonology on binary features.
Such functions have the ability provides a theoretical basis for Covington&apos;s manu-to distinguish a large number of different phones. ally constructed distance function.
The underlying assumption is that the number of bi-nary features by which two given sounds differ is
"Although binary features are elegant and widely used, they might not be optimal for phonetic align-ment."
Their primary motivation is to classify phonological oppositions rather than to reflect the phonetic characteristics of sounds.
"In a strictly bi-nary system, sounds that are similar often differ in a disproportionately large number of features."
It can be argued that allowing features to have several pos-sible values results in a more natural and phoneti-caUy adequate system.
"For example, there are many possible places of articulation, which form a near-continuum ranging from [labial] to [glottal]."
This system has been adapted[REF_CITE]and implemented[REF_CITE].
It contains about 20 features with values between 0 and 1.
"Some of them can take as many as ten different values (e.g. [place]), while others are basically binary oppositions (e.g. [nasal])."
Table 3 contains examples of multivalued features.
"The main problem with both Somers&apos;s and Con-nolly&apos;s approaches is that they do not differenti-ate the weights, or saliences, that express the rel-ative importance of individual features."
"For ex-ample, they assign the same salience to the fea-ture [place] as to the feature [aspiration], which results in a smaller distance between [p] and [k] than between [p] and [phi."
"I found that in order to avoid such incongruous outcomes, the salience values need to be carefully differentiated; specifi-cally, the features [place] and [manner] should be assigned significantly higher saliences than other features (the actual values used in my algorithm are given in Table 4)."
The question of how to derive salience values in a principled manner is still open.
"Although all four algorithms listed in Table 1 mea-sure relatedness between phones by means of a dis-tance function, such an approach does not seem to be the best for dealing with phonetic units."
"The fact that Covington&apos;s distance function is not a metric is not an accidental oversight; rather, it reflects certain inherent characteristics of phones."
"Since vowels are in general more volatile than consonants, the pref-erence for matching identical consonants over iden-tical vowels is justified."
"This insight cannot be ex-pressed by a metric, which, by definition, assigns a zero distance to all identical pairs of segments."
Nor is it certain that the triangle inequality should hold for phonetic segments.
"A phone that has two dif-ferent places of articulation, such as labio-velar [w], can be close to two phones that are distant from each other, such as labial [b] and velar [g]."
"In my algorithm, below, I employ an alternative approach to comparing segments, which is based on the notion of similarity."
A similarity scoring scheme assigns large positive scores to pairs of related seg-ments; large negative scores to pairs of dissimilar segments; and small negative scores to indels.
The optimal alignment is the one that maximizes the overall score.
"Under the similarity approach, the score obtained by two identical segments does not have to be constant."
"Another important advantage of the similarity approach is the possibility of perform-ing local alignment of phonetic sequences, which is discussed in the following section."
"Once an appropriate function for measuring simi-larity between pairs of segments has been designed,"
"The efficiency of the algorithm might not be rel-evant in the simple case of comparing two words, we need an algorithm for finding the optimal align- but if the algorithm is to be of practical use, it will ment of phonetic sequences."
While the DP algo- have to operate on large bilingual wordlists.
"More-rithm, which operates in quadratic time, seems to over, combining the alignment algorithm with some be optimal for the task, both Somers and Covington sort of strategy for identifying cognates on the basis opt for exhaustive search strategies."
"In my opinion, of phonetic similarity is likely to require comparing this is unwarranted. thousands of words against one another."
Somers&apos;s algorithm is unusual because the se- polynomially bound algorithm in the core of such a lected alignment is not necessarily the one that system is crucial.
"In any case, since the DP algo-minimizes the sum of distances between individ- rithm involves neither significantly larger overhead ual segments."
"Instead, it recursively selects the nor greater programming effort, there is no reason most similar segments, or &quot;anchor points&quot;, in the to avoid using it even for relatively small data sets. sequences being compared."
Such an approach has The DP algorithm is also sufficiently flexible to a serious flaw.
Suppose that the sequences to be accommodate most of the required extensions with-aligned are tewos and divut.
Even though the corre- out compromising its polynomial complexity.
"A sponding segments are slightly different, the align- simple modification will produce all alignments that ment is straightforward."
"However, an algorithm that are within e of the optimal distance[REF_CITE]. looks for the best matching segments first, will er-"
By applying methods from the operations research roneously align the two t&apos;s.
"Because of its recursive literature[REF_CITE], the algorithm can be adapted nature, the algorithm has no chance of recovering to deliver the n best solutions."
"Moreover, the basic from such an error.[Footnote_4] set of editing operations (substitutions and indels) can be extended to include both transpositions of ad-jacent segments (metathesis)[REF_CITE]and compressions and expansions[REF_CITE]."
4The criticism applies regardless of the method of choosing 5Covington does not elaborate on the nature of the modifi-the best matching segments (see also Section 5). cations.
Other extensions of the DP algorithm that are applicable to the problem of phonetic align-ment include affine gap scores and local compari-son.
The motivation for generalized gap scores arises from the fact that in diachronic phonology not only individual segments but also entire morphemes and syllables are sometimes deleted.
"In order to take this fact into account, the penalty for a gap can be calculated as a function of its length, rather than as a simple sum of individual deletions."
"One solution is to use an affine function of the form gap(x) = r + sx, where r is the penalty for the introduction of a gap, and s is the penalty for each symbol in the gap."
"Incidentally, Covington&apos;s penalties for indels can be expressed by an affine gap function with r -- 10 and s= 40."
Local comparis[REF_CITE]is made possible by using both positive and neg-ative similarity scores.
"In local, as opposed to global, comparison, only similar subsequences are matched, rather than entire sequences."
This often has the beneficial effect of separating inflectional and derivational affixes from the roots.
Such affixes tend to make finding the proper alignment more dif-ficult.
"It would be unreasonable to expect affixes to be stripped before applying the algorithm to the data, because one of the very reasons to use an au-tomatic aligner is to avoid analyzing every word in-dividually. 4 The algorithm"
Many of the ideas discussed in previous sections have been incorporated into the new algorithm for the alignment of phonetic sequences (ALINE).
"Sim-ilarity rather than distance is used to determine a set of best local alignments that fall within E of the optimal alignment. [Footnote_6] The set of operations con-tains insertions/deletions, substitutions, and expan-sions/compressions."
"6Global and serniglobal comparison can also be used. In a semiglobal comparison, the leading and trailing indels are assigned a score of zero."
Multivalued features are em-ployed to calculate similarity of phonetic segments.
Affine gaps were found to make little difference when local comparison is used and they were subse- quently removed from ALINE.[Footnote_7]
"7They may be necessary, however, when dealing with lan-guages that are rich in infixes."
The algorithm has been implemented in C++ and will be made avail-able in the near future.
Figure 1 contains the main components of the algorithm.
"First, the DP approach is applied to compute the similarity matrix S using the G scor-ing functions."
The optimal score is the maximum entry in the whole matrix.
A recursive procedure Retrieve (Figure 2) is called on every matrix en-try that exceeds the threshold score T. The align-ments are retrieved by traversing the matrix until a zero entry is encountered.
"The scoring functions for indels, substitutions and expansions are defined in Figure 3."
"Cskip, Csub, and Cexp are the maximum scores for indels, substitutions, and expansions, re-spectively."
Cvwt determines the relative weight of consonants and vowels.
"The default values are Cskip = -10, Csub = 35, Cexp = 45 and Cvwt = 10."
"The diff function returns the difference between segments p and q for a given feature f. Set Rv contains fea-tures relevant for comparing two vowels: Syllabic, Nasal, Retroflex, High, Back, Round, and Long."
Set are currently used by ALINE.
"Feature values are encoded as floating-point numbers in the range [0.0, 1.0]."
The numerical values of four principal features are listed in Table 3.
The numbers are based on the measurements performed[REF_CITE].
"The remaining features have exactly two possible values, 0.0 and 1.0."
"A special fea-ture &apos;Double&apos;, which has the same values as &apos;Place&apos;, indicates the second place of articulation."
"Thanks to its continuous nature, the system of features and their values can easily be adjusted and augmented."
"Latin gramen obtained with global, semiglobal, and The best alignments are obtained when local com- local comparison."
The double bars delimit the parison is used.
"For example, when aligning En- aligned subsequences. quence of substitution and deletion as compression is unsatisfactory because it cannot be distinguished from an actual sequence of substitution and dele-tion."
ALINE posits this operation particularly fre-quently in cases of diphthongization of vowels (see the alignments in Table 6).
Covington&apos;s data set of 82 cognates provides a convenient test for the algorithm.
"His English/Latin set is particularly interesting, because these two languages are not closely related."
Some of the alignments produced by Covington&apos;s algorithm and ALINE are shown in Table 6.
ALINE accurately discards inflectional affixes in piscis and flare.
"In fish/piscis, Covington&apos;s aligner produces four alter-native alignments, while ALINE selects the cor-rect one."
"Both algorithms are technically wrong on tooth/dentis, but this is hardly an error consid-ering that only the information contained in the phonetic string is available to the aligners."
"On Covington&apos;s Spanish/French data, ALINE does not make any mistakes."
"Unlike Covington&apos;s aligner, it properly aligns [1] in arbol with the second [r] in arbre."
"On his English/German data, it selects the correct alignment in those cases where Coving-ton&apos;s aligner produces two alternatives."
"In the fi-nal, mixed set, ALINE makes a single mistake in daughter/thugat~r, in which it posits a dropped pre-fix rather than a syncopated syllable; in all other cases, it is fight on target."
"Overall, ALINE clearly"
"The goal of my current research is to combine the new alignment algorithm with a cognate identifica-tion procedure, The alignment of cognates is possi- ble only after the pairs of words that are suspected Kruskal, editors, Time warps, string edits, and of being cognate have been identified."
"Identification macromolecules: the theory and practice of se-of cognates is, however, an even more difficult task quence comparison, pages 1-44."
"Reading, Mass.: than the alignment itself."
"Moreover, it is hardly fea- Addison-Wesley. sible without some kind of pre-alignment between Peter Ladefoged. 1995."
A Course in Phonetics. candidate lexemes.
A high alignment score of two New York: Harcourt Brace Jovanovich. words should indicate whether they are related.
An Roy Lowrance and Robert A. Wagner. 1975.
An integrated cognate identification algorithm would extension of the string-to-string correction prob-take as input unordered wordlists from two or more lem.
"Journal of the Association for Computing related languages, and produce a list of aligned cog-[REF_CITE]:177-183. nate pairs as output."
Such an algorithm would be a Eugene W. Myers. 1995.
"Seeing conserved signals. step towards developing a fully automated language In Eric S. Lander and Michael S. Waterman, edi-reconstruction system. tors, Calculating the Secrets of Life, pages 56-89."
"Washington, D.C.: National Academy Press."
"Reduplication, a central instance of prosodic mor- the copy language ww, which is context-sensitive. phology, is particularly challenging for state-of-"
"In the rest of this paper I will lay out a proposal the-art computational morphology, since it involves for handling reduplication with finite-state methods. copying of some part of a phonological string."
"As a starting point, I adopt Bird &amp;[REF_CITE]&apos;sIn this paper I advocate a finite-state method that com- One-Level Phonology, a monostratal constraint-bines enriched lexical representations via intersec- based framework where phonological representa- tion to implement the copying."
"The proposal tions, morphemes and generalizations are all finite-in- cludes a resource-conscious variant of automata and state automata (FSAs) and constraint combination can benefit from the existence of lazy algorithms. is accomplished via automata intersection."
"While Finally, the implementation of a complex case from it is possible to transfer much of the present pro-"
Koasati is presented.
"As motivated in §2, an appropriate automaton repre-sentation of morphemes that may undergo redupli-cation should provide generic support for three key operations: (i) copying or repetition of symbols, (ii) truncation or skipping, and (iii) infixation."
"For copying, the idea is to enrich the FSA rep-resenting a morpheme by encoding stepwise repeti-tion locally."
"For every content arc i 2~ j we add a reverse repeat arc j repe~t i. Following repeat arcs, we can now move backwards within a string, as we shall see in more detail below."
"For truncation, a similar local encoding is avail-able: For every content arc i --%j, add another skip arc i ski~j. This allows us to move forward while suppressing the spellout of e."
A generic recipe for infixation ensures that seg- mental material can be inserted anywhere within an existing morpheme FSA.
A possible representa-tional enrichment therefore adds a self loop i ~ i labelled with the symbol alphabet E to every state i of the FSA.2 Each of the three enrichments presupposes an epsilon-free automaton in order to be wellbehaved.
"This requirement in particular ensures that techni-cal arcs (skip, repeat) are in 1:1 correspondence with content arcs, which is essential for unambigu-ous positional movement: e.g. add_skips(a eb) would ambiguously require 1 or [Footnote_2] skips to supress the spellout of b, because it creates a disjunction of the empty string e with skip."
"2Thiscan be seenas an applicationof the ignore operator[REF_CITE],whereE* is beingignored."
It is perhaps worth emphasizing that there is no special interpretation whatsoever for these technical arcs: the standard au-tomaton semantics is unaffected.
"As a consequence, skip and repeat will be a visible part of the output in word form generation and must be allowed in the input for parsing as well."
"Taken together, the three enrichments yield an automaton for Bambara wulu, shown in figure 1.a."
"While skipping is not necessary for this example, 4 ~ 4 is: it will host the fixed-melody/o/."
"The repeat arcs will of course facilitate copying, as we shall see in a moment. a."
Ig I; I; I; I; : repeat seg:O
"Bird &amp;[REF_CITE]came close to discovering a useful device for reduplication when they noted that automaton intersection has at least indexed-grammar power (ibid., p.48)."
They demonstrated their claim by showing that odd-length strings of indefinite length like the one described by the regular expression (abcde f g)+ can be repeated by intersecting them with an automaton accept-ing only strings of even length: the result is (abede f gabede f g)+.
"Generalizing from their artifical example, let us first make one additional minor enrichment by tag-ging the edges of the reduplicative portion of a base with synchronization bits :1, while using the opposite value :0 for the interior part (see fig-ure 1.a)."
This gives us a segment-independent handle on those edges and a regular expression seg:l seg:o*seg:l for the whole synchronized portion (seg abbreviates the set of phonological segments).
"Assuming repeat-enriched bases, a total redupli-cation morpheme can now be seen as a partial word specification which mentions two synchronized por-tions separated by an arbitrary-length move back-wards: (4) seg: lseg:o*seg:l repeat* seg:lseg:o*seg:l"
"Moreover, total reduplicative copying now simply is intersection of the base and (4), or - in the Bam-bara case - a simple variant that adds the/o/(figure 1.b)."
"Disregarding serf loops for the moment, the reader may verify that no expansion of the kleene-starred repeat that traverses less than Ibase[ seg-ments will satisfy the demand for two synchronized portions."
Semai requires another slight variant of (4) which skips the interior of the base in the redu-plicant: (5) seg:l skip*seg: l repeat* seg:lseg:o*seg:l
"The identification of copying with intersection not only allows for great flexibility in describing the full range of actual reduplicative constructions with reg-ular expressions, it also reuses the central operation for constraint combination that is independently re-quired for one-level morphology and phonology."
Any improvement in efficient implementation of intersection therefore has immediate benefits for grammar computation as a whole.
"In contrast, a hypothetical setup where a dedicated total copy de-vice is sandwiched between finite-state transducers seems much less elegant and may require additional machinery to detect copies during parsing."
"Note that it is in fact possible to compute reduplication-as-intersection over an entire lexicon of bases (see figure 3 for an example), provided that repeat arcs are added individually to each base, En-riched base FSAs can then be unioned together and undergo further automaton transformations such as determinization or minimization."
This restriction is necessary because our finite-state method cannot express token identity as normally required in string repetition.
"Rather than identifying the same token, it addresses the same string position, using the weaker notion of type identity."
"Therefore, application of the method is only safe ff strings are effectively isolated from one another, which is exactly what per-base enrichment achieves."
See §3.4 for a suggestion on how to lift the restriction in practice.
"One pays a certain price for allowing general repe-tition and infixation: because of its self loops and technical arcs, the automaton of figure 1.a over-generates wildly."
"Also, during intersection, self loops can absorb other morphemes in unexpected ways."
A possible diagnosis of the underlying de-fect is that we need to distinguish between produc-ers and consumers of information.
"In analogy to LFG&apos;s constraint vs constraining equations, infor-mation may only be consumed if it has been pro-duced at least once."
"For automata, let us spend a P/C bit per arc, with P/C=I for producers and P/C=O for consumer arcs."
"In open interpretation mode, then, intersection com-bines the P/C bits of compatible arcs via logical OR, making producers dominant."
"It follows that a re-source may be multiply consumed, which has obvi-ous advantages for our application, the multiple re-alization of string symbols."
"A final step of closed in- terpretation prunes all consumer-only arcs that sur-vived constraint interaction, in what may be seen as intersection with the universal producer language under logical-AND combination of P/C bits."
"Using these resource-conscious notions, we can now model both the default absence of material and purely contextual requirements as consumer-type information: unless satisfied by lexical resources that have been explicitly produced, the correspond-ing arcs will not be part of the result."
"By convention, producers are displayed in bold."
"Thus, the exact re-sult of figure 1.a 71 1.b after closed interpretation is:"
W:I U:0/:o U:o o repeat [Footnote_4] repeat* W:l u:o l:o U:l
"4See Walther (submitted) for further details. With determin-istic automata, the question of how to recover from a wrong string hypothesis during parsing is not an issue."
"This expression also illustrates that, for parsing, strings like wuluowulu need to be consumer-self-loop-enriched via a small preprocessing step, be-cause intersection with the grammar would other-wise fail due to unmentioned technical arcs such as repeat."
"Because our proposal is fully declarative, parsing then reduces to intersecting the enriched parse string with the grammar-and-lexicon automa-ton (whose construction will itself involve intersec-tion) in closed interpretation mode, followed by a check for nonemptiness of the result."
"Whereas the original parse string was underspecified for mor-phological categories, the parse result for a realis-tic morphology system will, in addition to technical arcs, contain fully specified category arcs in some predefined linearization order, which can be effi-ciently retrieved if desired."
"It is clear that the above method is particularly at-tractive if some of its operations can be performed online, since a fullform lexicon of productive redu-plications is clearly undesirable e.g. for Bambara."
I therefore consider briefly questions of efficient im-plementation of these operations.
3A second condition is that no state is visited that has not been discovered from the start state. It is easy to implement (6) so that this condition is fulfilled as well.
Such imple-mentations are very advantageous when large in-termediate automata may be constructed but only a small part of them is visited for any particular in-put.
"They show that such a rule exists for composi- tion o, hence also for our operation of intersection (An B = range(identity(A) o identity(B)))."
"Fortunately, the three enrichment steps all have local computation rules as well: e repeat (6) a. q~-+ q2 ~ q2 ) q~ ski~ b. ql-~ q2 ~ ql q2 c. q ~ q-~+q"
"The impact of the existence of lazy implementa-tions for enrichment operations is twofold: we can (a) now maintain minimized base lexicons for stor-age efficiency and add enrichments lazily to the cur-rently pursued string hypothesis only, possibly mod-ulated by exception diacritics that control when en-richment should or should not happen.[Footnote_4]"
"4See Walther (submitted) for further details. With determin-istic automata, the question of how to recover from a wrong string hypothesis during parsing is not an issue."
"And (b), laziness suffices to make the proposed reduplication method reasonably time-efficient, despite the larger number of online operations."
"Actual benchmarks from a pilot implementation are reported elsewhere (Walther, submitted)."
In this section I show how to implement the Koasati case from (3) using the FSA Utilities toolbox (van[REF_CITE]).
FSA Utilities is a Prolog-based finite-state toolkit and extendible regular expression compiler.
It is freely available and encourages rapid prototyping.
Figure 2 displays the regular expression opera-tors that will be used (italicized operators are mod-ifications or extensions).
The grammar will be pre- 1 I
"Starting with the definition of stems (line 1), we Note that both the constituent before ( t:l a h a s:l ) add the three enrichments to the bare phonological / I string (2)."
"However, the innermost producer-type and after (p:l i n:~) the infixation site need to be string constructed by stringToAutomaton (3) is marked."
"Also, it turns out to be useful to classify intersected with phonological constraints (5,6) that base string positions for easy reference in the redu-need to see the string only, minus its enrichments. plicative morpheme, which motivates lines 32- 34."
"The main part now is the reduplicative morpheme itself (35), which looks like a mixture of Bambara and Semai: the spellout of the base is followed by it-erated repeats (36) to move back to its synchronized initial position (37), which - recall/h/- is required to be consonantal."
The rest of the base is skipped before insertion of the fixed-melody part/o(o)/oc-
"The following constraint (15-18) enriches a Finally, some obvious word_level_con- prosodically underspecified string with moras straints need to be defined (45-54), before the # - abstract units of syllable weight (Hayes, central intersection of Stem and punctual-aspect 1995) -, a prerequisite to locating ([Footnote_20]-[Footnote_24]) and reduplication (57) completes our Koasati fragment: synchronization-marking ([Footnote_25]-31) the first heavy syllable after which the reduplicative infix will be 45 word_level_constraints := inserted. 15 moraification := 16 ( vowel --&gt; ( mora / sigma ) )&amp; 17 ( consonant --&gt; ( mora / consonant ) 18 ( consonant --&gt; ( (~ mora) / vowel ) 19"
"20 first_(X) := [not_contains(X), X]."
25 mark__first__heavy_syllable :=
"Space precludes the description of a final automa- *, ton operation called Bounded Local Optimizati[REF_CITE]that turns out to be useful here to tahas-too-pin, ak-ho(o)-latlin ban unattested free length variation, as found e.g. in ak-ho(o)-latlin where the length of o is yet to be determined."
"Suffice to say that a parametriza-tion of Bounded Local Optimization would prune the moraic arc 16 ~ 19 in figure 3 by considering it costlier than the non-moraic arc 16 --~ 18, thereby eliminating the last source of indeterminacy."
"This paper has presented a novel finite-state method for reduplication that is applicable for both un-bounded total cases, truncated or otherwise phono-logically modified types and infixing instances."
"The key ingredients of the proposal are suitably en-riched automaton representations, the identification of reduplicative copying with automaton intersec-tion and a resource-conscious interpretation that differentiates between two types of arc symbols, namely producers and consumers of information."
"After demonstrating the existence of efficient on-demand algorithms to reduplication&apos;s central oper-ations, a case study from Koasati has shown that all of the above ingredients may be necessary in the analysis of a single complex piece of prosodic mor-phology."
It is worth mentioning that our method can be transferred into a two-level transducer setting with-out major difficulties ([REF_CITE]appendix B).
I conclude that the one-level approach to redu-plicative prosodic morphology presents an attractive way of extending finite-state techniques to difficult phenomena that hitherto resisted elegant computa-tional analyses.
"A finite-state method, based on leftmost longest-match replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes."
A small set of hand-crafted conver-sion rules for Dutch achieves a phoneme accuracy of over 93%.
The accuracy of the system is further im-proved by using transformation-based learning.
"The phoneme accuracy of the best system (using a large rule and a &apos;lazy&apos; variant of Brill&apos;s algoritm), trained on only 40K words, reaches 99%."
"Automatic grapheme to phoneme conversion (i.e. the conversion of a string of characters into a string of phonemes) is essential for applications of text to speech synthesis dealing with unrestricted text, where the input may contain words which do not occur in the system dictionary."
"Furthermore, a transducer for grapheme to phoneme conversion can be used to generate candidate replacements in a (pronunciation-sensitive) spelling correction sys-tem."
"When given the pronunciation of a misspelled word, the inverse of the grapheme to phoneme trans-ducer will generate all identically pronounced words."
"Below, we present a method for developing such grapheme to phoneme transducers based on a com-bination of hand-crafted conversion rules, imple-mented using finite state calculus, and automatically induced rules."
The hand-crafted system is defined as a two-step procedure: segmentation of the input into a sequence of graphemes (i.e. sequences of one or more characters typically corresponding to a sin-gle phoneme) and conversion of graphemes into (se-quences of) phonemes.
"The composition of the transducer which performs segmentation and the transducer defined by the conversion rules, is a transducer which converts sequences of characters into sequences of phonemes."
Specifying the conversion rules is a difficult task.
"Although segmentation of the input can in princi-ple be dispensed with, we found that writing con-version rules for segmented input substantially re- duces the context-sensitivity and order-dependence of such rules."
We manually developed a grapheme to phoneme transducer for Dutch data obtained from CELEX[REF_CITE]and achieved a word ac-curacy of 60.6% and a phoneme accuracy of 93.6%.
"To improve the performance of our system, we used transformation-based learning (TBL)[REF_CITE]."
Training data are obtained by aligning the output of the hand-crafted finite state transducer with the correct phoneme strings.
"These data can then be used as input for TBL, provided that suit-able rule templates are available."
"We performed sev-eral experiments, in which the amount of&apos;training data, the algorithm (Brill&apos;s original formulation and &apos;lazy&apos; variants[REF_CITE]), and the num-ber of rule templates varied."
"The best experiment (40K words, using a &apos;lazy&apos; strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.0% phoneme accuracy."
"This result, obtained using a relatively small set of training data, compares well with that of other systems."
"As argued[REF_CITE],[REF_CITE],[REF_CITE], and elsewhere, many of the rules used in phonology and morphol-ogy can be analysed as special cases of regular ex-pressions."
"By extending the language of regular ex-pressions with operators which capture the interpre-tation of linguistic rule systems, high-level linguis-tic descriptions can be compiled into finite state au-tomata directly."
"Furthermore, such automata can be combined with other finite state automata perform-ing low-level tasks such as tokenization or lexical-lookup, or more advanced tasks such as shallow pars-ing."
Composition of the individual components into a single transducer may lead to highly efficient pro-cessing.
"The system described below was implemented us-ing FSA Utilities,1 a package for implementing and manipulating finite state automata, which provides possibilities for defining new regular expression oper- [] the empty string [R~,..., R,,] concatenation {R1,... ,Rn} disjunction"
"R - optionality ignore (A,B) ignore: A interspersed with elements of B AxB cross-product: the transducer which maps all strings in A to all strings in B. identity(A) identity: the transducer which maps each element, in A onto itself."
"ToU composition of the transducers T and U. macro (Term, R) use Term as an abbreviation for R (where Term and R may contain variables)."
Figure 1: A fragment of FSA regular expression syntax.
"A and B are regular expressions denoting recognizers, T and U transducers, and R can be either. ators."
"The part of FSA&apos;s built-in regular expression wherever a consonant can be final in a coda or initial syntax relevant to this paper, is listed in figure 1. in the next onset, it is in fact added to the onset."
One particular useful extension of the basic syn- The segmentation task discussed below makes cru-tax of regular expressions is the replace-operator. cial use of longest match.
Although several implementations of the replace-
"Grapheme to phoneme conversion is implemented as operator are proposed, the most relevant case for the composition of four transducers: our purposes is so-called &apos;leftmost longest-match&apos; re-placement."
"In case of overlapping rule targets in the macro (graph2phon, input, this operator will replace the leftmost target, segmentation 7, segment the input and in cases where a rule target contains a prefix o mark_begin_end 7, add &apos;#&apos; which is also a potential target, the longer sequence o conversion 7. apply rules will be replaced."
Gerdemann and van[REF_CITE]o clean_up ).
Z remove markers implement leftmost longest-match replacement in
"FSA as the operator replace(Target, LeftContext,RightContext),"
"An example of conversion including the in-termediate steps is given below for the word aanknopingspunt (connection-point). where Target is a transducer defining the actual re- input: aanknopingspunt placement, and LeftContext and RightContext are aa-n-k-n-o-p-i-ng-s-p-u-n-t- s: regular expressions defining the left- and rightcon- m: #-aa-n-k-n-o-p-i-ng-s-p-u-n-t-# text of the rule, respectively. co: #-a+N+k-n-o-p-I+N+s-p-}+n-t-#"
An example where leftmost replacement is use- aNknopINsp}nt cl: ful is hyphenation.
"Hyphenation of (non-compound) words in Dutch amounts to segmenting a word The first transducer (segmentation) takes as into syllables, separated by hyphens."
In cases its input a sequence of characters and groups where (the written form of) a word can in prin- these into segments.
"The second transducer ciple be segmented in several ways (i.e. the se- (mark_begin_end) adds a marker (&apos;~&apos;) to the be-quence alfabet can be segmented as al-fa-bet, ginning and end of the sequence of segments."
"The al-fab-et, all-a-bet, or alf-ab-et), the seg- third transducer (conversion) performs the actual mentation which maximizes onsets is in general the conversion step."
It converts each segment into a correct one (i.e. al-fa-bet).
This property of hy- sequence of (zero or more) phonemes.
"The final phenation is captured by leftmost replacement: macro(hyphenate, replace([] x -, syllable, syllable)). step (clean_up) removes all markers."
The output is a list of phonemes in the notation used by CELEX (which can be easily translated into the more com-mon SAMPA-notation).
"Leftmost replacement ensures that hyphens are in- 3.1 Segmentation troduced &apos;eagerly&apos;, i.e. as early as possible."
"Given The goal of segmentation is to divide a word into a a suitable definition of syllable, this ensures that sequence of graphemes, providing a convenient input level of representation for the actual grapheme to phoneme conversion rules."
"While there are many letter-combinations which are realized as a single phoneme (ch, ng, aa, bb, .. ), it is only rarely the case that a single letter is mapped onto more than one phoneme (x), or that a letter receives no pronunciation at all (such as word-final n in Dutch, which is elided if it is proceeded by a schwa)."
"As the number of cases where multiple letters have to be mapped onto a single phoneme is relatively high, it is natural to model a letter to phoneme system as involving two subtasks: segmen-tation and conversion."
"Segmentation splits an input string into graphemes, where each grapheme typ-ically, but not necessarily, corresponds to a single phoneme."
"Segmentation is defined as: macro(segmentation, replace( [identity(graphemes), [] x ],[],[]) - )."
The macro graphemes defines the set of graphemes.
Segmentation attaches the marker &apos;-&apos; to each grapheme.
"Segmentation, as it is defined here, is not context-sensitive, and thus the second and third arguments of replace are simply empty."
"As the set of graphemes contains many elements which are substrings of other graphemes (i.e. e is a substring of ei, eau, etc.), longest-match is es-sential: the segmentation of beiaardier (caril-lon player) should be b-ei-aa-r-d-ie-r- and not b-e-i-a-a-r-d-i-e-r-."
This effect can be ob-tained by making the segment itself part of the tar-get of the replace statement.
"Targets are identi-fied using leftmost longest-match, and thus at each point in the input, only the longest valid segment is marked."
The set of graphemes contains a number of ele-ments which might seem superfluous.
"The grapheme aa±, for instance, translates as aj, a sequence which could also be derived on the basis of two graphemes aa and ±. However, if we leave out the segment aa±, segmentation (using leftmost longest match) of words such as waaien (to blow) would lead to the segmentation w-aa-ie-n, which is unnatural, as it would require an extra conversion rule for ±e."
Us-ing the grapheme aai allows for two conversion rules which always map aai to aj and ±e goes to ±.
"Segmentation as defined above provides the in-tuitively correct result in almost all cases, given a cases which are less natural, but which do not nec-essarily lead to errors."
"The grapheme eu, for in-stance, almost always goes to &apos;l&apos;, but translates as &apos;e,j,}&apos; in (loan-) words such as museum and petroleum."
"One might argue that a segmentation e-u- is therefore required, but a special conver-sion rule which covers these exceptional cases (i.e. eu followed by m) can easily be formulated."
"Simi-larly, ng almost always translates as N, but in some cases actually represents the two graphemes n-g-, as in aaneengesloten (connected), where it should be translated as NG."
"This case is harder to detect, and is a potential source of errors."
"The g2p operator is designed to facilitate the formu-lation of conversion rules for segmented input: macro(g2p(Target,LtCont,RtCont), replace([Target, - x +], [ignore(LtCont,{+,-}), {-,+}], ignore(RtCont,{+,-}) ) )."
The g2p-operator implements a special pro:pose ver-sion of the replace-operator.
"The replacement of the marker &apos;-&apos; by &apos;+&apos; in the target ensures that g[Footnote_2]p-conversion rules cannot apply in sequence to the same grapheme.2 Second, each target of the g2p-operator must be a grapheme (and not some sub-string of it)."
"2Note that the input and output alphabet are not disjoint, and thus rules applying in sequence to the same part of the input are not excluded in principle."
This is a consequence of the fact that the final element of the left-context must be a marker and the target itself ends in &apos;-&apos;.
"Finally, the ig-nore statements in the left and right context imply that the rule contexts can abstract over the potential presence of markers."
An overview of the conversion rules we used for Dutch is given in Figure 2.
"As the rules are ap-plied in sequence, exceptional rules can be ordered before the regular cases, thus allowing the regular cases to be specified with little or no context."
The special_vowel_rules deal with exceptional trans-lations of graphemes such as eu or cases where i or ij goes to &apos;©&apos;.
"The short_vowel_rules treat sin-gle vowels preceding two consonants, or a word final consonant."
"One problematic case is e, which can be translated either as &apos;E&apos; or &apos;~&apos;."
"Here, an ap-proximation is attempted which specifies tile con-text where e goes &apos;E&apos;, and subsumes the other case under the general rule for short vowels."
Tile special_consonant_rules address devoicing and a few other exceptional cases.
The default_rules supply a default mapping for a large number of graphemes.
The target of this rule is a long disjunc- ered by browsing through the test results of previ-tion of grapheme-phoneme mappings.
As this rule- ous runs.
"At this point, switching from a linguistics-set applies after all more specific: cases have been oriented to a data-oriented methodology, seemed ap-dealt with, no context restrictions need to be speci- propriate. fied."
"Depending somewhat on how one counts, the full 4 Transformation-based grapheme set of conversion rules for Dutch contains approxi- to phoneme conversion mately 80 conversion rules, more than 40 of which[REF_CITE]demonstrates that accurate part-of-are default mappings requiring no context.[Footnote_3] Compi-speech tagging can be learned by using a two-step lation of the complete system results in a (minimal, process."
3It should be noted that we only considered words which
"First, a simple system is used which as-deterministic) transducer with 747 states and 20,123 signs the most probable tag to each word."
The re-transitions. sults of the system are aligned with the correct tags
"The accuracy of the hand-crafted system was evM- sensitive) transformation rules are selected from a uated by testing it on all of tile words wihtout pool of rule patterns, which replace erroneous tagsdi- acritics in the CELEXlexical database which have a by correct tags."
The rule with the largest benefit on phonetic transcription.
"After several development the training data (i.e. the rule for which the number cycles, we achieved a word accuracy of 60.6% and of corrections minus the number of newly introduced a phonenle accuracy (measured as the edit distance mistakes, is the largest) is learned and applied to between the phoneme string produced by the sys- the training data."
"This process continues until no tem and the correct string, divided by the number more rules can be found which lead to improvement of phonemes in the correct string) of 93.6%. (above a certain threshold)."
There have been relatively few attempts at devel- Transformation-based learning (TBL) can be ap-oping grapheme to phoneme conversion systems us- plied to the present problem as well.4
"In this case, ing finite state technology alone."
"The rules are also implemented in a two-level these results are aligned with the correct transcrip-system, PC-KIMMO,[REF_CITE], but this still tions."
"In combination with suitable rule patterns, requires over 400 rules."
TBL requires aligned data for training and testing. a weighted finite state transducer) for the grapheme-
While alignment is mostly trivial for part-of-speech to-phoneme conversion step.
"These numbers suggest tagging, this is not the case for the present task. that our implementation (which contains around 80 Aligning data for grapheme-to-phoneme conversion rules in total) benefits considerably from the flexibil-amounts to aligning each part of the input (a se-ity and high-level of abstraction made available by finite state calculus. quence of characters) with a part of the output (a sequence of phonemes)."
"As the length of both se- One might suspect that a two-level approach to quences is not guaranteed to be equal, it must be grapheme to phoneme conversion is more appropri-possible to align more than one character with a ate than the sequential approach used here."
"Some-single phoneme (the usual case) or a single character what surprisingly, however, Williams concludes that with more than one phoneme (the exceptional case, a sequential approach is preferable."
The formulation i.e. &apos;x&apos;).
"The alignment problem is often solved (Du-of rules in the latter approach is more intuitive, and toit, 1997; Daelemans and van den[REF_CITE]) by rule ordering provides a way of dealing with excep-allowing &apos;null&apos; symbols in the phoneme string, and tional cases which is not easily available in a two- level system. introducing &apos;compound&apos; phonemes, such as &apos;ks&apos; to account for exceptional cases where a single charac-"
"While further improvements would definitely have ter must be aligned with two phonemes. been possible at this point, it becomes increasingly As our finite state system already segments the difficult to do this on the basis of linguistic knowl-input into graphemes, we have adopted a strategy edge alone."
"That is, most of the rules which have to be added deal with highly idiosyncratic cases where graphemes instead of characters are aligned (of- with phoneme strings (see Lawrence and Kaye ten related to loan-words) which can only be discov- (1986) for a similar approach)."
"The correspondence between graphemes and phonemes is usually one to one, but it is no problem to align a grapheme with two or more phonemes."
"Null symbols are only intro-duced in the output if a grapheme, such as word-final &apos;n&apos;, is not realized phonologically."
"For TBL, the input actually has to be aligned both with the system output as well as with the correct phoneme string."
"The first task can be solved triv-ially: since our finite state system proceeds by first segmenting the input into graphemes (sequences of characters), and then transduces each grapheme into a sequence of phonemes, we can obtain aligned data by simply aligning each grapheme with its con&apos;e-sponding phoneme string."
The input is segmented into graphemes by doing the segmentation step of the finite state transducer only.
"The corresponding phoneme strings can be identified by applying the conversion transducer to the segmented input, while keeping the boundary symbols &apos;-&apos; and &apos;+&apos;."
"As a con-sequence of the design of the conversion-rules, the resulting sequence of separated phonemes sequences stands in a one-to-one relationship to the graphemes."
"An example is shown in figure 3, where GR represents the grapheme segmented string, and sP the (system) phoneme strings produced by the finite state trans-ducer."
"Note that the final sP cell contains only a boundary marker, indicating that the grapheme &apos;n&apos; is translated into the null phoneme."
"For the alignment between graphemes (and, idi- rectly, the system output) and the correct phoneme strings (as found in Celex), we used the &apos;hand-seeded&apos; probabilistic alignment procedure described[REF_CITE]~.From the finite state conver-sion rules, a set of possible grapheme --+phoneme se-quence mappings can be derived."
"This allowables-set was extended with (exceptional) mappings present in the correct data, but not in the haml-crafted sys-tem."
We computed all possible aligmnents between (segmented) words and correct phoneme strings li-cenced by the allowables-set.
"Next, probabilities for all allowed mappings were estimated on the basis of all possible alignments, and the data was parsed again, now picking the most probable alignment for each word."
"To minimize the number of words that could not be aligned, a maximum of one unseen map-ping (which was assigned a low probability) was al-lowed per word."
"With this modification, only one out of 1000 words on average could not be aligned.&apos;~ These words were discarded."
"The aJigned phoneme string for the example in figure 3 is shown in the of being sampled at some point, higher scoring rules bottom line."
"Note that the final cell is empty, rep- are more likely to be generated than lower scoring resenting the null phoneme. 4.2 The experiments For the experiments with TBL we used the #-T rules, but no exhaustive search is required."
We ex-perimented with sampling sizes 5 and 10.
"As CPU requirements are more modest, we managed to per- BL- form experiments on 60K words in this case, which package[REF_CITE]."
This Prolog implementation lead to results which are comparable with Brill&apos;s al-of TBL is considerably more efficient (up to ten goritm applied to 40K words. times faster) than Brill&apos;s original (C) implementa-
Apart from being able to work with larger data tion.
"The speed-up results mainly from using Pro- sets, the &apos;lazy&apos; strategy also has the advantage that log&apos;s first-argument indexing to access large quanti- it can cope with larger sets of rule templates."
Brill&apos;s ties of data efficiently. algorithm slows down quickly when the set of rule
"We constructed a set of 22 rule templates which templates is extended, but for an algorithm based on replace a predicted phoneme with a (corrected) rule sampling, this effect is much less severe."
"Thus, phoneme on the basis of the underlying segment, we also constructed a set of 500 rule templates, con-and a context consisting either of phoneme strings, taining transformation rules which allowed up to with a maximum length of two on either side, or three graphemes or phoneme sequences as left or a context consisting of graphemes, with a maximal right context, and also allowed for disjunctive con-length of 1 on either side."
We used this
"Brill&apos;s algorithm, we achieved a phoneme accuracy rule set in combination with a &apos;lazy&apos; strategy with of 98.0% (see figure 4) on a test set of 20K words of sampling size 5 (lazy(5)+ in figure 4)."
"This led to a unseen data. 6[REF_CITE]K words resulted in 98 4%. further improvement of phoneme accuracy to 99.0%, phoneme accuracy."
"Note, however, that in spite of and word accuracy of 92.6%, using only 40K words the relative efficiency of the implementation, CPU of training material. time also goes up sharply."
"Finally, we investigated what the contribution was"
The heavy computation costs of TBL are due to of using a relatively accurate training set.
"To this the fact that for each error in the training data, all end, we constructed an alternative training set, in possible instantiations of the rule templates which which every segment was associated with its most correct this error are generated, and for each of these probable phoneme (where frequencies were obtained instantiated rules the score on the whole training set from the aligned CELEXdata)."
"As shown in figure 5, has to be computed."
The exper-on Monte Carlo sampling of the rules.
"For each er- imental results, for the &apos;lazy&apos; algorithm with sam-ror in the training set, only a sample of the rules pling size 5, show that the phoneme accuracy for is considered which might correct it."
As rules which training on 20K words is 0.3% less than for the cor-correct a high number of errors have a higher chance responding experiment in figure 4.
"As might be expected, the number of induced rules is much higher now, and thus cPu-requirements also increase substantially."
"We have presented a method for grapheme to phoneme conversion, which combines a hand-crafted finite state transducer with rules induced by a transformation-based learning."
An advantage of this method is that it is able to achieve a high level of accuracy using relatively small training sets.
"Given the result[REF_CITE], an obvious next step is to compile the induced rules into an actual transducer, and to compose this with the hand-crafted transducer."
"It should be noted, how-ever, that the number of induced rules is quite large in some of the experiments, so that the compilation procedure may require some attention."
"This paper describes AUTOSEM, a robust semantic interpretation framework that can operate both at parse time and repair time."
The evaluation demon-strates that AUTOSEM achieves a high level of ro-bustness efficiently and without requiring any hand coded knowledge dedicated to repair.
"In order for an approach to robust interpretation to be practical it must be efficient, address the ma-jor types of disfluencies that plague spontaneously produced language input, and be domain indepen-dent so thatachieving robustness in a particular do-main does not require an additional knowledge en-gineering effort."
"This paper describes AUTOSEM, a semantic interpretation framework that possesses these three qualities."
"While previous approaches to robust interpretation have offered robust parsers paired with separate repair modules~ with separate knowledge sources for each, AUTOSEM is a single unified framework that can operate both at parse time and repair time."
"AUTOSEM is integrated with the LCFLEx robust parser (Ros@and Lavie, to ap-pear; Lavie and Ros@, 2000)."
"Together AUTOSEM and LCFLEx constitute the robust understanding engine within the CARMEL natural language un-derstanding component developed in the context of the Atlas intelligent tutoring project (Freedman at al., to appear)."
The evaluation reported here demon-strates that AUTOSEM&apos;s repair approach operates 200 times faster than the most similar competing ap-proach while producing hypotheses of better quality.
AUTOSEM provides an interface to allow seman-tic interpretation to operate in parallel with syntac-tic interpretation at parse time in a lexicon driven fashion.
Domain specific semantic knowledge is en-coded declaratively within a meaning representation specification.
Semantic constructor functions are compiled automatically from this specification and then linked into lexical entries as in the Glue Lan-guage Semantics approach to interpretati[REF_CITE].
"Based on syntactic head/argument relationships assigned at parse time, the construc- tot functions enforce semantic selectiona] restric-tions and assemble meaning representation struc-tures by composing the meaning representation asso-ciated with the constructor function with the mean-ing representation of each of its arguments."
AUTOSEM first attempts to construct analy-ses that satisfy both syntactic and semantic well-formedness conditions.
The LCFLEx parser has the ability to efficiently relax syntactic constraints as needed and as allowed by its parameterized flexi-bility settings.
"For sentences remaining beyond the parser&apos;s coverage, AUTOSEM&apos;s repair algorithm re-lies entirely on semantic knowledge to compose the partial analyses produced by the parser."
Each se-mantic representation built by AUTOSEM&apos;s inter-pretation framework contains a pointer to the con-structor function that built it.
"Thus, each partial analysis can be treated as a constructor function with built in knowledge about how the associated partial analysis can be combined with other par-tial analyses in a semantically meaningful way."
Ge-netic programming search[REF_CITE]is used to efficiently compose the fragments pro-duced by the parser.
The function definitions com-piled from the meaning representation specification allow the genetic search to use semantic constraints to make effective use of its search space.
"Thus, AU-TOSEM operates efficiently, free of any hand coded repair rules or any knowledge specifically dedicated to repair unlike other approaches to recovery from parser failure ([REF_CITE]; Van[REF_CITE])."
At the heart of AUTOSEM is its interpretation framework composed of semantic constructor func-tions compiled from a meaning representation spec-ification.
These semantic constructor functions can be used at parse time to build up semantic represen-tations.
These same constructor functions can then be used in a repair stage to compose the fragments returned by the parser in the cases where the parser is not able to obtain a complete analysis for an ex- (:type &lt;*state&gt; :isa (&lt;&gt;) :instances nil :vars (entity time duration polarity) :spec ((who &lt;*entity&gt; entity) (when &lt;*when&gt; time) slot filler comes from.
"This third piece of information can be either a variable name, indicating that what-ever is bound to that variable is what should fill that slot, or a function call to another semantic construc-tor function, allowing types to specify constraints at more than one level of embedding."
"Similar to the (how-long &lt;*time-length&gt; duration) :spec field, the :instances field associates a list of (negation [+/-] polarity))) (:type &lt;*personal-state&gt; :isa (&lt;*state&gt;) :instances nil :vars () :spec ((who &lt;*who&gt; entity))) (:type &lt;busy&gt; :isa (&lt;*personal-state&gt;) :instances nil :vars (activity) :spec ((frame *busy) (event &lt;*event&gt; activity))) (:type [+/-] :isa (&lt;&gt;) :instances (+ -) :vars nil :spec nil) atomic values with a type."
Inheritance relations are defined via the :isa field.
"Types inherit the values of each subsuming type&apos;s :instances, :vars, and : spec fields. 3 Semantic Interpretation at Parse"
Time (:type &lt;cancel&gt; specification entries tragrammatical input sentence.
The meaning representation specification vides a venue for expressing domain specific se- (object activity) mantic information declaratively.
AUTOSEM pro- (tempadjunct time) duces frame-based meaning representation struc- (negation polarity)))) tures.
"Thus, each domain specific meaning repre-sentation specification must define a set of semantic types that together specify the set of frames and Figure 3: Lexical entry for the verb &quot;cancel&quot; atomic feature values that make up the domain spe-cific frame-based language, which slots are associ- As an extension to LCFLEx&apos;s LFG-like pseudo-ated with each frame, and what range of frames and unification grammar formalism, AUTOSEM pro-atomic feature values may fill each of those slots. vides the insert-role function as an interface to AUTOSEM provides a simple formalism for defin- allow semantic interpretation to operate in parallel ing meaning representations."
Each entry corre- with syntactic interpretation at parse time.
"When sponds to a semantic type and contains five fields: the insert-role function is used to insert a child :type, :isa, :instances, :vars, and :spec."
"Some constituent into the slot corresponding to its syntac-sample entries for the appointment scheduling do- tic functional role in a parent constituent, the child main are displayed in Figure 1."
Some details are constituent&apos;s semantic representation is passed in to omitted for simplicity.
The :type field simply con- the parent constituent&apos;s semantic constructor func- talus the name of the type.
"The :vars field contains tion as in the Glue Language Semantics approach a list of variables, each corresponding to a semantic to interpretati[REF_CITE]."
The :spec field associates a frame and set of lexicon formalism allows semantic constructor func- slots with a type.
"For each slot, the :spec field con- tions to be linked into lexical entries by means of rains the name of the slot, the most general type re- the semtag feature."
"Each semtag feature value cor-striction on the slot, and a specification of where the responds to a semantic constructor function and mappings between syntactic functional roles such as subject, direct object, and indirect object and semantic roles such as agent, activity, or time."
See Figures 2 and 3 discussed further be-low.
Note that the syntactic features that appear in this example are taken from the COMLEX lex-ic[REF_CITE].
"In order to provide consistent input to the semantic constructor func-tious, AUTOSEM assumes a syntactic approach in which deep syntactic functional roles are assigned as in CARMEL&apos;s syntactic parsing grammar evaluated in (Freedman et al., to appear)."
"In this way, for ex-ample, the roles assigned within an active sentence and its corresponding passive sentence remain the same."
"Since the same constructor function is called with different arguments a number of times in order to construct an analysis incrementally, an argument is included in every constructor function that allows a &quot;result so far&quot; to be passed in and augmented."
"Its default value, which is used the first time the constructor function is executed, is the representa-tion associated with the corresponding type in the absence of any arguments being instantiated."
"Each time the constructor function is executed, each of its arguments that are instantiated are first checked to be certain that the structures they are instantiated with match all of the type restrictions on all of the slots that are bound to that argument."
"If they are, the instantiated arguments&apos; structures are inserted into the corresponding slots in the &quot;result so far&quot;."
Otherwise the constructor function fails.
"Take as an example the sentence &quot;The meeting I had scheduled was canceled by you.&quot; as it is pro-cessed by AUTOSEM using the CARMEL grammar and lexicon, which is built on top of the COMLEX lexic[REF_CITE]."
The grammar as-signs deep syntactic functional roles to constituents.
"Thus, &quot;you&quot; is the deep subject of &quot;cancel&quot;, and &quot;the meeting&quot; is the direct object both of &quot;cancel&quot; and of &quot;schedule&quot;."
"The detailed subcategorization classes associated with verbs, nouns, and adjectives in COMLEX make it possible to determine what these relationships should be."
The meaning repre-sentation entry for &lt;cancel&gt; as well as the lexical entry for the verb &quot;cancel&quot; are found in Figures 2 and 3 respectively.
Some details are left out for sim-plicity.
"When &quot;the meeting I had scheduled&quot; is ana-lyzed as the surface subject of &quot;was canceled&quot;, it is assigned the deep syntactic role of object since &quot;was canceled&quot; is passive."
"The verb &quot;cancel&quot; has cancell as its semtag value in the lexicon, cancel 1 is defined there as being associated with the type &lt;cancel&gt;, and the object syntactic role is associated with the activity argument."
"Thus, the &lt;cancel&gt; function is called with its activity argument instantiated with the meaning of &quot;the meeting I had scheduled&quot;."
"Next, when &quot;by you&quot; is attached, &quot;you&quot; is assigned the deep syntactic role of subject of &quot;cancel&quot;."
The subject role is associated with the agent argument in the definition of cancell.
"Thus, &lt;cancel&gt; is called a again, this time with &quot;you&quot; instantiating the agent argument and the result from the last call to &lt;cancel&gt; passed in through the &quot;result so far&quot; argument."
"While the LCFLEX parser has been demonstrated to robustly parse a variety of disfluencies found in spontaneously generated language (Ros~ and Lavie, to appear), sentences still remain that are beyond its coverage."
"Previous research involving the ear-lier GLR* parser[REF_CITE]and an earlier repair module[REF_CITE]has demonstrated that divid-ing the task of robust interpretation into two stages, namely parsing and repair, provides a better trade off between run time and coverage than attempt-ing to place the entire burden of robustness on the parser alone (Ros$, 1997)."
"Thus, when the flexibility allowed at parse time is not sufficient to construct an analysis of an entire sentence for any reason, a frag-mentary analysis is passed into the repair module."
For each pair of vertices in the chart the best single clause level and noun phrase level analysis accord-ing to LCFLEx&apos;s statistical disambiguation scores is included in the set of fragmentary analyses passed on to the repair stage.
An example1 is displayed in
Here the sentence &quot;Why don&apos;t we make it from like eleven to one?&quot; failed to parse.
"In this case, the problem is that the insertion of &quot;like&quot; causes the sentence to be ungrammatical."
"When the parser&apos;s flexibility settings are such that it is constrained to build analyses only for contiguous portions of text, such an insertion would prevent the parser from con-structing an analysis covering the entire sentence."
"Nevertheless, it is able to construct analyses for a number of grammatical subsets of it."
Genetic programming search[REF_CITE]is used to search for different ways to combine the fragments.
Genetic programming is an oppor-tunistic search algorithm used for constructing com-puter programs to solve particular problems.
"Among its desirable properties is its ability to search a large space efficiently by first sampling widely and shal-lowly, and then narrowing in on the regions sur-rounding tile most promising looking points."
ZThls example was generated with the grammar used in the evaluation.
See Section 6.
The AUTOSEM repair algo-rithm can he used with grammars that do not make use of AUTOSEM&apos;s parse-time interface by using a simple conver-sion program that automatically builds a function for each partial analysis corresponding to its semantic type.
It first takes a list of functions and terminal sym-bols and randomly generates a population of pro-grams.
It then evaluates each program for its &quot;fit-hess&quot; according to some predetermined set of crite-ria.
"The most fit programs are then paired up and used to produce the next generation by means of a crossover operation whereby a pair of subprograms, one from each parent program, are swapped."
"The new generation is evaluated for its fitness, and the process continues for a preset number of generations."
"As mentioned earlier, because each semantic rep-resentation built by AUTOSEM contains a pointer to the constructor function that built it, each partial analysis can itself be treated as a constructor func-tion."
"Thus, the function set made available to the genetic programming search for each sentence need-ing repair is derived from the set of partial anal-yses extracted from the parser&apos;s chart."
A number of the functions produced for the example are dis-played in Figure 4.
Some functions have been omit-ted for brevity.
"The functions are displayed as func-tion calls, with the name of the function followed by its arguments."
The name of each function corre-sponds to the semantic type from the meaning rep-resentation that corresponds to the associated par-tial analysis.
"Following this is a list of place holders corresponding to each argument position associated with the semantic type, as described in Section 2."
"Each place holder is either nil if it is an open place holder, or [Footnote_1]: if the position has already been filled in the corresponding partial analysis."
1 is first assigned to each program corresponding to
The STR field contains the corresponding partial analysis.
This is the &quot;result so far&quot; parameter discussed in Section 3.
The C0V field lists the positions in the sentence covered by the partial analysis.
"Note that in the example sentence, the word &quot;don&apos;t&quot; covers both po-sitions 2 and 3 since the parser expands the con-traction before parsing."
"The SCOREfield contains the statistical score assigned by the parser&apos;s statis-tical disambiguation procedure described in (Ros~ and Lavie, to appear)."
The repair process begins as the genetic program-ming algorithm composes the function definitions into programs that assemble the fragments produced by the parser.
The genetic programming algorithm has access to a list of type restrictions that are placed on each argument position by the meaning repre-sentation specification.
"Thus, the algorithm ensures that the programs that are generated do not vio-late any of the meaning representation&apos;s type restric-tions."
"Once a population of programs is generated ran-domly, each program is evaluated for its fitness."
A simple function implements a preference for pro-grams that cover more of the sentence with fewer steps while using the analyses the parser assigned the best statistical scores to.
A score between 0 and the percentage of the input sentence it covers.
A second score between 0 and 1 estimates how com-plicated the program is by dividing the number of function calls by the length of the sentence and sub-tracting this number from 1.
A third score is as-signed the average of the statistical scores assigned by the parser to the fragments used in the program.
"Using coefficients based on an intuitive assignment of relative importance to the three scores, the final fit-ness value of each program is 1- [(.55 * coverageS) + (.25 • complexityS) + (.2 • statisticalS)]."
A typed version of the original crossover algorithm described[REF_CITE]was used to ensure that new programs would not violate any type restrictions or include more than one partial analysis covering the same span of text.
This was accomplished by first making for each subprogram a list of the subprograms from the alternate program it could be inserted into without violating any seman-tic constraints.
From these two lists it is possible to generate a list of all quadruples that specify a sub-program from each parent program to be removed and which subprogram from the alternate parent program they could be inserted into.
"From this list, all quadruples were removed that would either cause a span of text to be covered more than once in a resulting program or would require a subprogram to be inserted into a subprogram that would have been removed."
"From the remaining list, a quadruple was selected randomly."
The corresponding crossover operation was then executed and the resulting two new programs were returned.
"While this typed vet-sion of crossover is more complex than the original crossover operation, it can be executed very rapidly in practice because the programs are relatively small and the semantic type restrictions ensure than the initial lists generated are correspondingly small. 5 Related Work"
Recent approaches to robust parsing focus on shal-low or partial parsing techniques[REF_CITE].
"Rather than attempting to construct a parse covering an entire ungrammatical sentence as[REF_CITE], these approaches at-tempt to construct analyses for maximal contiguous portions of the input."
The weakness of these partial parsing approaches is that part of the original mean-ing of the utterance may be discarded with the por-tion(s) of the utterance that are skipped in order to find a parsable subset.
Information communicated by the relationships between these fragments within the original text is lost if these fragments are not combined.
"Thus, these less powerful algorithms es-sentially trade effectiveness for efficiency."
Their goal is to introduce enough flexibility to gain an accept- able level of coverage at an acceptable computational sentence. expense.
Each sentence was parsed in two different modes.
Some partial parsing approaches have been cou-
"In LC w/restarts mode, the parser was allowed to pled with a post-parsing repair stage (Danieli and construct analyses for contiguous portions of input[REF_CITE]; starting at any point in the sentence."
"The goal mode, the parser was allowed to start an analysis at behind these two stage approaches is to increase any point and skip up to three words within the anal-the coverage over partial parsing alone at a rea- ysis."
Because the AUTOSEM repair algorithm runs sonable computational cost.
"Until the introduction significantly faster than the ROSE repair algorithm, of AUTOSEM, the ROSE approach, introduced in repair was attempted after every parse rather than[REF_CITE], was unique in that it achieved this only when a parse quality heuristic indicated a need goal without either requiring hand coded knowledge as in the ROSE approach[REF_CITE]."
We com-specifically dedicated to repair or excessive amounts pared the results of both AUTOSEM and ROSE in of interaction with the user.
"However, although conjunction with the LC w/restarts parsing mode.[REF_CITE]demonstrates that the two stage ROSE"
The results are displayed in Figures 5 and 7.
"Be-approach is significantly faster than attempting to cause the ROSE approach only runs the full repair achieve the same quality of results in a single stage algorithm when its parse quality heuristic indicates parsing approach, our evaluation demonstrates that a need and the parser returns more than one par-it remains computationally intractable, requiring on tial analysis, it only attempted repair for 14% of the average 67 seconds to repair a single parse on a 330 sentences in the corpus."
"Nevertheless, although the[REF_CITE]."
"In contrast, we demonstrate AUTOSEM repair algorithm ran for each sentence, that AUTOSEM is on average 200 times faster, tak-"
Figure 5 demonstrates that processing time for pars-ing only .33 seconds on average to repair a single ing plus repair in the AUTOSEM condition was dra- parse while achieving results of superior quality. matically faster on average than with ROSE.
Aver-age processing time for the ROSE algorithm was 200 times slower than that for AUTOSEM on sentences where both repair algorithms were used.
"In addition An experiment was conducted to evaluate AU-to the advantage in terms of speed, the AUTOSEM TOSEM&apos;s robustness by comparing the effectiveness repair approach achieved an acceptable grade (Okay and efficiency of AUTOSEM&apos;s repair approach with or Perfect) on approximately 4% more sentences. that of the alternative ROSE approach."
The test set used for this evaluation contains 750 sentences Parsing in LC w/restarts mode plus repair was extracted from a corpus of spontaneous scheduling also compared with parsing in LCFLEx mode with dialogues collected in English.
For both repair ap- skipping up to three words.
"Again, LC w/restarts proaches we used the meaning representation devel- + AUTOSEM repair achieved a slightly higher oped for the appointment scheduling domain that number of acceptable grades, although LCFLEx was used in previous evaluations of the ROSE ap- achieved a slightly higher number of Perfect grades. proach[REF_CITE]."
"It consists of 260 semantic On long sentences (between 15 and 20 words), types, each expressing domain specific concepts LCFLEx mode required almost three times as muchfor the appointment scheduling domain such as busy, time as LC w/restarts mode plus AUTOSEM re-cancel, and out-of-~;own."
The ROSE meaning rep- pair.
This evaluation confirms our previous results resentation specification was easily converted to the that two stage approaches offer a better processing format used in AUTOSEM.
"Because a pre-existing time versus robustness trade-off. semantic grammar was available that parsed directly The primary difference between ROSE and AU-onto this meaning representation, that grammar was TOSEM is that ROSE uses a single repair function, used in the parsing stage to construct analyses."
"The MY-C0bIB,to combine any two fragments by referring final meaning representation structures for the first to the meaning representation specification."
"Each result was graded as either Bad, mantic restrictions imposed by the meaning repre-"
"Okay, or Perfect, where Perfect indicates that the re- sentation."
These restrictions are visible only locally sult was fluent and communicated the idea from the within individual applications of the I~Y-COMBfunc-original sentence.
A grade of Okay indicates that tion.
"Thus, FIY-COMBmust be able to cope with the the result communicated the correct information, case where the arguments passed in cannot be com-but not fluently."
Those graded Bad either communi- bined.
Large portions of the programs generated by cated incorrect information or were missing part or ROSE as repair hypotheses do not end up contribut-all of the information communicated in the original ing to the resulting structure.
The programs gener- ated by ROSE must therefore be much larger than in AUTOSEM in order to obtain the same results.
"Fur-thermore, the fitness of each repair hypothesis can only be computed by executing the program to ob-tain a result."
The combination of all of these things makes the process of fitness evaluation in ROSE far more costly than in AUTOSEM.
"In contrast, AU-TOSEM&apos;s constructor function definitions make it possible for the genetic search to make use of seman-tic restrictions to speed up the process of converging on a high quality repair hypothesis."
The tremendous speed-up offered by the AUTOSEM approach makes for Alternative Strategies it practical to apply repair more often and to use a larger generation size (50 individuals as opposed to 32) and a larger number of generations (5 as opposed to 4) for the genetic search.
"In this paper we described AUTOSEM, a new robust semantic interpretation framework."
Our evaluation demonstrates that AUTOSEM achieves a greater level of robustness 200 times more efficiently than the most similar competing approach.
"In AUTOSEM, the mapping between syntactic cases, such as with copular constructions and with Ph.D. thesis, Dept. of Computer Science, Duke adjunct prepositional phrases, it would be useful to University. introduce some non-determinism so that, for exam- W. Kasper, B. Kiefer, H. Krieger, C. Rupp, and ple, semantic selectional restrictions between the ob- K. Worm. 1999."
Charting the depths of robust ject. of the preposition and the semantic structure speech parsing.
In Proceedings of the 37th An-that the prepositional phrase is attaching to can nual Meeting of the Association for Computa-more easily play a role in selecting the appropri- tional Linguistics. ate semantic relationship.
Exploring approaches J. Koza. 1992.
On the Pro- for achieving this non-determinism efficiently is one of gramming of Computers by Means of Natural Se- our current objectives. 8 Acknowledgements lection.
"Although natural language is ambiguous, vari-ous linguistic and extra-linguistic factors often help determine a preferred reading."
"In this pa-per, we show that model generation can be used to model this process in the case of reciprocal statements."
"The proposed analysis builds on in-sights[REF_CITE]and is shown to provide an integrated, computational account of the interplay between model theoretic inter-pretation, knowledge-based reasoning and pref-erences that characterises the interpretation of reciprocals."
"Although there is widespread agreement that inference is an essential component of natural language processing, little work has been done so far on whether existing automated reason-ing systems such as theorem provers and model builders could be fruitfully put to work in the area of natural language interpretation."
"In this paper, we focus on the inference prob-lems raised by the reciprocal expression each other and show that model generation provides an adequate tool for modeling them."
The paper is structured as follows.
Section 3 discusses the meaning of reciprocal statements and proposes a formal semantics for each other.
Section 2 shows how model generation can be used to provide this semantics with a compu-tational interpretation.
"Section 4 compares our approach with the account of reciprocals which inspired it in the first place namely,[REF_CITE]."
Section 5 concludes with pointers for further research.
"In the linguistic literature, the reciprocal ex-pression each other is often taken to denote a dyadic quantifier over a first-order set, which we will call the antecedent set, and a binary first-order relation, which we will call the scope re-lation."
"In what follows, we assume an approach of this type and will use the symbol RcP for such reciprocal quantifiers so that the seman-tic representation of e.g. Jon and Bill saw each other will be: (1) RcP({jon, saw(x,y))"
"When antecedent sets of just two members are considered, each set member is required to stand in the scope relation to each other mem-ber."
"For larger sets however, research on recip-rocal statements has uncovered a variety of log-ical contributions that the reciprocal can pro-vide."
Here are some examples. (2) The students like each other.
Vx (std(x) -+
"Vy (x • y A std(y) -+ like(x,y)) (3) The students stare at each other in sur-prise."
Vx (std(x) -+ ~y (x ¢ y A std(y)
"A stare_at(x, y )) (4) The students gave each other measles."
"Vx (std(x) -+ 3y (x ¢ y A std(y) A (gave_measles(x, y) V gave_measle(y, x))))"
We can accept (2) to be true only if for each pair x and y of two students it holds that x likes y.
"But an analogous interpretation would be invalid in the case of (3) and (4) where not all pairs in the antecedent set the students can consistently stand in the scope relation (one can only stare at most at one person at a time, and one can only get measles from at most one per- sentence (4) above and corresponds to the fol-son)."
"More generally, ([REF_CITE]; Dal- lowing definition of RcP. rymple et al., 1998) convincingly argues that (5) RCPIAO ~-- APAR ([P[ &gt; 2 A Vx (P(x) =v different reciprocal statements can have very 3y p(y) ^ x # y ^ (R(x, v) v R(V,x)))) different truth conditions."
The challenge to be addressed is thus the following: How can we This definitiononly adequately characterises ex- determine a (computational) semantics for amples such as (4).
It does not cover thethe reciprocal expressions each other that accounts stronger meanings of the reciprocal in sentences for these multiplicity of meanings while predict- such as (2) and (3).
"However, each known form ing the specific meaning of a particular recipro- of reciprocity entails RCPIAO&apos;S truth conditions, cal statement? and RCPIAO therefore provides us with a mini-"
"Clearly knowledge based reasoning plays an mal semantics for reciprocals. important role: only those readings are possible Further, we observe that given a particular that are consistent with our knowledge about reciprocal statement, there seems to be a pref-the situation and the world."
"Specifically, knowl- erence for consistent interpretations where the edge based reasoning constrains the strength of number of pairs that are in the scope relation the truth conditions of a reciprocal statement. is as large as possible."
"For instance in (3), not Thus if we abstract away from the specific scope every student can stare at every other student relations, the truth conditions of examples such (one can stare at at most one person), but intu-as (2),(3) and (4) are ordered through entail- itively, the sentence requires that every student ment as follows (with A the antecedent set and stares at some other student."
While such an
R the scope relation):
Vx (A(x) --~Vy (A(y) --).R(xy))
"Vx (A(x) ~ 9y (A(y) A e(xy)) interpretation is much weaker than that of (2), this maximisation of the scope relation yields a reading that is also much stronger than the min- imal IAO interpretation of (4)."
"More generally, Vx (A(x) --~ 3y (A(y) A (e(xy) V (e(yx))) while IAO provides us with a lower bound for the interpretation of reciprocal statements, we Specifically, example (2), which does not in-will see in section 3 that the maximisation of the volve any strong knowledge based constraint, scope relation that is consistent with contextual has the strongest truth-conditions of the three knowledge yields the upper bound for the inter-examples."
"By contrast in (3), the knowledge pretation of a particular reciprocal statement that one can stare at most at one person, forces i.e., its meaning. a V3 reading while in (4), a weaker meaning still Based on these observations, the principle de-is imposed by knowledge based constraints: the termining the actual logical contribution of a x gave y measles relation is asymmetric hence reciprocal statement can be stated as follows: the k~/reading is ruled out; moreover, since one cannot be infected twice, some students in the Maximise Meaning Hypothesis group will be infected but not pass on the dis- (MMH): The valid interpretations of ease to anyone."
Hence the strongest truth con-a reciprocal sentence S in a context F ditions that can be assigned the sentence are the (where I&quot; includes knowledge about the
"VS disjunctive reading indicated in (4). previous discourse, the discourse situ-"
But are there any other constraints on the ation and the world) are those which interpretation process than these knowledge (a) are consistent both with the IAO based constraints?
And which meaning shall form of reciprocity and the informa-we assign a reciprocal expression?
"The compu- tion provided by F, and (b) whose con-tational semantics we will propose is inspired tributions to the scope relation are the[REF_CITE]and relies on the strongest. following observations."
"First, we note th[REF_CITE]"
"The MMH selects from the set of interpreta-identifies a lower bound for the truth conditions tions that are consistent with IAO and contex-of reciprocal sentences which they dub Inclusive tual knowledge, those that maximise the scope Alternative Ordering (IAO)."
It is exemplified by relation.
"Crucially, this view of reciprocals leads to an inference method that can actually com-pute the preferred interpretations of reciprocal sentences."
We now turn to this.
"Generation In Discourse Representation Theory (DRT,[REF_CITE]), a sen-tence with semantic representation (I) is true with respect to a model M iff there is an embed-ding of (I) onto M. Intuitively, this requirement says that a sub-model M&apos; of M must be found which satisfies (I)."
"So for instance, sentence (6a) is true in M iff there are two individuals bugs and bunny in M such that bugs and bunny stand in the love relation; or in other words, iff the partial model sketched in (6b) is part of M. (6) a. Bugs likes Bunny. b. {love(bugs, bunny)}"
"As shown in (Gardent and Konrad, To ap-pear), model generators (i.e., programs that compute some of the models satisfying a finite set of logical formulas) can be used to provide DRT, and more generally model-theoretic ap-proaches to natural language semantics, with a procedural interpretation: Given the semantic representation of a discourse and the relevant world knowledge (I) (i.e., a finite set of logical formulas), a model generator proves that (I) is satisfiable by generating some of its models."
"Intuitively, satisfying models explain how dis-courses can be made true."
They give an abstract representation of how (part of) the world should be for a discourse to be true.
"Concretely, satisfying models can be seen as capturing the meaning of discourses: data-bases that can be queried e.g. as part of a query/answer system or to interpret subse-quent discourse."
"Satisfying models are also remininiscent of Johnson-Laird&apos;s mental mod-els[REF_CITE]and in essence, mental models are very much like the Herbrand models we are making use of here."
"Formally, a model is a mathematical struc-ture that describes how the symbols of a logi-cal theory are interpreted."
"Given a first-order language £, a model is a pair (I, D) with D a non-empty set of entities (the domain of indi-viduals) and I an interpretation function which maps relation symbols in £ to relations of ap-propriate arity in D and constant symbols in £: to elements of D. Here we identify these mod-els with sets of positive assumptions that unam-biguously define the interpretation of the rela-tion symbols and fix the interpretation of terms to first-order entities that carry a unique name."
These are known in the literature as Herbrand models.
The set (7c) is such a model for the logical form (7b) which is a semantic representation of the sentence (7a). (7) a.
"Jon likes his cousin. b. 3x cousin_of(x, jon)"
"A like(ion, x) c. ~&apos;il = {cousin_of(cl,jon), like(jon, cl) }"
"The model A41 defines an interpretation of the predicates cousin and like over the universe of discourse 7) = {jon, cl }."
"It can also be taken as a valid interpretation of (7a).There are, how-ever, infinitely many models for (7b) that do not correspond to such interpretations e.g. (8) .M2 = {cousin_of(jon, jon), like(jon, jon)} (9) Ad3 = {cousin_of(c1, jon), fike(jon, C1), like(cl ,jon )}"
The model ..A42 explains the truth of (Ta) by declaring Jon as his own cousin.
This is a re-sult of the inappropriate semantic representa-tion (7b) which fails to specify that the relation expressed by the noun cousin is irreflexive.
"In the case of A43, the model contains superfluous information."
"While it is consistent to assume like(cl,jon) it is not necessary for explaining the truth of the input."
"For applications to natural-language, we are in-terested in exactly those models that capture the meaning of a discourse, or at least capture the preferred interpretations that a hearer asso-ciates with it."
"As discussed in (Gardent and Webber,[REF_CITE]), obtaining only these models requires eliminating both models that are &quot;too small&quot; (e.g. A42) and models that are &quot;too big&quot; (e.g. J~43)."
Models such as A42 can be eliminated simply by using more appropriate truth conditions for NL expressions (e.g. 3x cousin(x)
"A of(x, jon)"
"A x ~ jon A like(jon, x) for (7a))."
"In general how-ever, eliminating models that are &quot;too small&quot; is a non-trivial task which involves the interac-tion of model-theoretic interpretation not only with world knowledge reasoning but also with seems at first sight incompatible with local min-syntax, prosody and pragmatics."
The issue is imality.
"However, a simple method to combine discussed at some length (though not solved (Gardent and Webber,[REF_CITE]). ) in the MMH and model minimality is to consider the maximisation of reciprocal relations as a"
"To eliminate models that are &quot;too big&quot;, some minimisation of their complement sets."
After notion of minimality must be resorted to.
"For all, the difference in acceptability between (10c) instance, ([REF_CITE]as models for (10a) is due to exactly and Konrad, To appear) argues that local min- those pairs (x, y) (with x ~ y) that are not in imality is an adequate form of minimality for the like relation."
"To capture this intuition, we interpreting definite descriptions."
Local mini- introduce a special predicate $R that indicates mality is defined as follows. assumptions whose truth is considered &quot;costly&quot;.
Local Minimality: Let ~ be a set of first-
"In our case, these assumptions correspond to the order formulas and D be the set of Herbrand pairs of individuals that are not in the scope re-models of • that use some finite domain lation."
The semantic representation of recipro- D whose size is minimal.
"Then a model (I,D D is locally minimal iff there is no model (It, D~) E D such that I I C I."
Locally minimal models are models that ) cal each other is then as follows.
E other (11) RcP =__ )~P)~R (RCPIAo(P)(R) A VxVy (e(x)
A x ¢ y
"A -~R(x, y) ~=~ sat- $R(x, y))) isfy some input • within a minimal domain :D of individuals and are subset-minimal with respect The first conjunct says that a reciprocal sen-to all other domain minimal models."
These tence has as weakest possible meaning an IAO models are the simplest in the sense of Occam&apos;s reading.
"Since IAO is entailed by other identi- Razor and often the best explanation for the fied meaning for reciprocal statements, this is truth of an observation."
"In particular, if we as- compatible with the fact that reciprocal sen-sume that A42 is ruled out by a more appro- tences can have other, stronger meanings."
"The priate semantics for the word cousin, local min- second conjunct says that each pair (x, y) (with imality rules out -1~3 as non locally minimal and x ¢ y) that is not in the like relation is in therefore A41 is correctly identified as giving the the $R relation."
This encoding leads to mod- preferred interpretation for example (7).
Constraint els like (12b) and (12c) for (12a).
"We say that model (125) has a $R-cost of 4 ($R4), while model (12c) has a cost of 0."
"In the case of reciprocals, local minimality (12) a. RcP({jon, bill, dan})(XyXx like(x, y)) is clearly not a characterisation of preferred b. {like(ion, bill), like(ion, dan), $R(bill, dan), interpretations."
"Our semantic representation $R(bill, jon), SR(dan, bill), SR(dan, ion)} RCPIA0 will only capture a reciprocal&apos;s mean- $R4 ing if the reciprocal group has exactly two mem- c. {like(ion, bill), like(ion, dan), like(bill, dan), bers or if the input meets IAO, the weakest form like(bill, ion), like(dan, bill), like(dan, ion)} of reciprocity."
"For instance, the locally minimal $[REF_CITE]of formula (10b) is too weak to con-"
We now introduce a new form of minimality stitute an acceptable interpretation of (10a).
"In-whose definition is as follows. stead, the model capturing the meaning of (10a)"
"Conservative Minimality: Let ~ be a set is the model given in (10d). (10) a. Jon, Bill and Dan like each other. of first-order formulas and D be the set of Her-brand models of ~2 with a minimal domain T)."
"Then D has a subset C of models that carry b. RCPIAO({jon, bill, dan})()~y)~x like(x, y)) c. {like(yon, bill), like(bill, dan)} a minimal cost."
"A model (I,D) E C is con- d. {like(ion, bill), like(jon, dan), like(bill, dan), servative minimal iff there is no other model like(bill, jon), like(dan, bill), like(dan, ion)} (I&apos;, D&apos;) E C such that I&apos; C. I."
Conservative minimality is a conservative ex-
"Since the MMH ma.ximises rather than min- tension of local minimality: if there are no imises the logical contribution of formulas, it costs at all, then all local minimal models are also conservative models."
Conservative mini-reality is a combination of local minimality and cost minimisation that correctly identifies the preferred interpretation of reciprocal sentences.
"For instance since (12c) carries a minimal cost, it is a conservative minimal model for (12a) whereas (12b) isn&apos;t."
"Intuitively the approach works as follows: the more pairs there are that do not stand in the scope relation of the re-ciprocal, the bigger the SR predicate and the more costly (i.e. the least preferred) the model."
"That is, the combined use of a $R-predicate and of conservative minimality allows us to enforce a preference for interpretations (i.e. models) maximising R."
KIMBA[REF_CITE]is a finite model generator for first-order and higher-order logical theories that is based on a translation of logics into constraint problems over finite-domain integer variables.
KIMBA uses an effi-cient constraint solver to produce solutions that can be translated back into Herbrand models of the input.
We have tailored KIMBA such that it enumer-ates the conservative models of its input.
"In-tuitively, this works as follows."
"First, KIMBA searches for some arbitrary model of the input that mentions a minimum number of individu-als."
"Then, it takes the SR-cost of this model as an upper bound for the cost of all successor models and further minimises the cost as fax as possible by branch-and-bound search."
"After KIMBA has determined the lowest cost possi-ble, it restarts the model search and eliminates those models from the search space that have a non-minimal cost."
"For each model .h/ [ that it identifies as a cost-minimal one, it proves by refutation that there is no other cost-minimal model A/lt that uses only a subset of the pos-itive assumptions in A/[."
Each succesful proof yields a conservative minimal model.
All the examples discussed in this paper have been tested on Kimba and can be tried out at[URL_CITE]projects /lisa/kimba.html
Let us see in more detail what the predictions of our analysis are.
"As we saw in section 2, recip- rocal statements can have very different truth conditions."
"Intuitively, these truth-conditions lie on a spectrum from the weakest IAO inter-pretation (A is the antecedent set and R the scope relation):"
"IAl &gt;_2A Vx E A(x) 3y (A(y)A x ¢ y ^(R(x, y) v R(y, to the strongest so-called Strong Reciprocity (SR) interpretation namely:"
"IAI &gt; 2AVx g(x)Vy A(y)(x ~ y ==vR(x,y))"
We now see how the MMH allows us to cap-ture this spectrum.
Let us start with example (2) whose truth-conditions are the strongest Strong Reciprocity conditions: every distinct x and y in the an-tecedent set are related to each other by the scope relation.
"In this case, there is no con-straining world knowledge hence the content of the like relation can be fully maximised."
"For instance if there are five students, the cheapest model is one in which the cardinality of like is twenty (and consequently the cardinatity of $R is zero). (13) {like(sl, s2),/ike(sl, s3), like(sl, s4), /ike(sl, sh),/ike(s2, sl),/ike(s2, s3), like(s2, s4), like(s2, sh), like(s3, sl), like(s3, s2), like(s3, s4), like(s3, sh), I;ke(s4, sl), like(s4, s3), like(s4, s2), like(s4, sh), like(sh, sl), like(sh, s3), like(sh, s2), like(sh, s4) } $RO"
"By contrast, example (3) has a much weaker meaning."
"In this case there is a strong world knowledge constraint at work, namely that one can stare at only one other person at some time."
The cheapest models compatible with this knowledge are models in which every student stare at exactly one other student.
"Thus in a universe with five students, the preferred inter-pretations are models in which the cardinality of the scope relation x stares at y in surprise is five."
The following are example models.
"For simplicity we ommit the $R propositions and give the cost of the model instead (i.e. the car-dinality of the complement set of the scope re-lation). (14) {stare_at(sl, s2), stare_at(s2, s3), stare_at(s3, s4), stare_at(s4, sh), stare_at(sh, s3)} $[REF_CITE](stare_at(sl, s2), stare_at(s2, s3), stare_at(s3, s4), stare_at(s4, s5), stare_at(s5, sl)}"
One-way Weak Reciprocity (OWR)
Vx E A 3y e A (xRy) $R15
Intermediate Alternative Reciprocity (IAR)
"Sentence (4) illustrates an intermediate case Vx, y E A3zl, . . . 3Zm E A(x ~ y -+ with respect to strength of truth conditions. (xRzl Y zlRx) A ..."
A (zmRy Y yRzm))
World knowledge implies that the scope rela-tion x give y measles is assymetric and further Inclusive Alternative Ordering (IAO) that every individual is given measles by at most Vx E A Sy E A(xRy Y yRx) one other individual.
"Given a set of five stu-dents, model (16) and (17) are both acceptable To predict the meaning of a specific recip-interpretations of (4), (16) being the preferred rocal sentence, DKKMP then postulate the interpretation."
"Strongest Meaning Hypothesis which says that the meaning of a reciprocal sentence is the log- (16) {gave_measles(sl, s2), gave_meas]es(sl, s3), ically strongest meaning consistent with world gave_measles(s2, s4), gave_measles(s3, s5)} and contextual knowledge. $R16 The main difference between the DKKMP ap-proach and the present approach lies in how (17) (gave_measles(sl, s2), gave_measles(s2, s4), gave_measles(s3, s5)~} the best reading is determined: it is the logi- $R17 cally strongest of the five postulated meanings"
"In short, these examples show the MMH at in DKKMP, whereas in our approach, it is that work."
"They show how given a single seman- reading which maximises the scope relation of tic representation for reciprocals, a variety of the reciprocal."
This difference has both empiri-meanings can be derived as required by each cal and computational consequences. specific reciprocal statement.
"Two elements are Empirically, the predictions are the same in crucial to the account: the use of model build- most cases because maximising the scope rela-ing, and that of minimality as an implemen- tion often results in yielding a logically stronger tation of preferences."
Model building allows meaning.
"In particular, as is illustrated by the us to compute all the finite interpretations of examples in section 2, the present approach cap-a sentence that are consistent with contextual tures the five meanings postulated by DKKMP. knowledge and with an IAO interpretation[REF_CITE]exemplifies an SR reading, the reciprocal expression."
Preferences on the model (15) an IR reading and model (14) an other hand (i.e. the use of the cost predicate OWR reading.
"Further, model (16) is an IAR $R and the search for conservative mininal mod- interpretation while model (17) shows an IAO els), permits choosing among these interpreta- reading. tions the most likely one(s) namely, the inter-"
But as the examples also show there are pretation(s) maximising the scope relation. 4 Related Work cases where the predictions differ.
"In particu-lar, in the DKKMP approach, sentence (3) is assigned the IR reading represented by model (15)."
"However as they themselves observe, the[REF_CITE](henceforth DKKMP) sentence also has a natural OWR interpretation proposes the following taxonomy of mean-namely, one as depicted in model (14), in which ings for reciprocal statements (A stands for some pairs of students reciprocally stare at each the antecedent set and R for the scope relation): other."
This is predicted by the present approach
"Strong Reciprocity (SR) Vx, y E"
A(x ¢ y ~ xRy).
"Intermediate reciprocity (IR) Vx, y E A 3zl,... 3Zm E A(x ~= y xRzl A . . ."
A ZmRy) which says that models (14) and (15) are equally plausible since they both maximise the stare at relation to cardinality five.
"On the other hand, the DKKMP account is more appropriate for examples such as: --+ (18)"
The students sat next to each other a. forming a nice cercle. b. filling the bench. c. some to the front and others to the back of the church.
An IR interpretation is predicted for (18) which is compatible with both continuation (18a) and continuation (18b).
"By contrast, the model generation approach predicts that the preferred interpretation is a model in which the students form a circle, an interpretation com-patible with continuation (18a) but not with continuations (18b-c)."
"However, both approaches fail to predict the reading made explicit by continuation (18c) since this corresponds to the weaker OWR in-terpretation under the DKKMP account and to a model which fails to maximise the scope re-lation under the present approach."
"More gen-erally, both approaches fail to capture the se-mantic vagueness of reciprocal statements illus-trated by the following examples[Footnote_1]: (19) a."
1I am thankful to an anonymous NAACL referree for these examples.
The students often help each other with their homework. b.
"In the closing minutes of the game, the members of the losing team tried to encour-age each other."
"In both cases, the sentences can be true with-out maximising either the strength of its truth conditions (Strong Reciprocity) or the scope re-lation."
"This suggests that an empirically more correct analysis of reciprocals should involve prototypical and probabilistic knowledge - as it is essentially a computational approximation of the DKKMP approach, the present account does not integrate such information though it is compatible with it: just as we restrict the set of generated models to the set of conservative minimal models, we could restrict it to the set of models having some minimal probability."
"Computationally, the difference between the DKKMP and the present approach is as fol-lows."
"In essence, the DKKMP approach re-quires that each of the five possible readings (together with the relevant world knoweldge) be checked for consistency: some will be con-sistent, others will not."
"Since the first order consistency and validity problems are not de-cidable, we know that there can be no method guaranteed to always return a result."
"In order to implement the DKKMP approach, one must therefore resort to the technique advocated[REF_CITE]and use both a theo-rem prover and a model builder: for each possi-ble meaning Mi, the theorem is asked to prove ~Mi and the model builder to satisfy Mi."
"Mi is inconsistent if the theorem prover succeeds, and consistent if the model builder does."
"Theoreti-cally however, cases may remain where neither theorem proving nor model building will return an answer."
"If these cases occur in practice, the approach simply is not an option."
"Further, the approach is linguistically unappealing as it in essence requires the reciprocal each other to be five-way ambiguous."
"By contrast, the model generation approach assigns a single semantic representation to each other."
The approach strengthens the logical contribution of the weak semantic representa-tion as a process based on computational con-straints on a set of effectivelyenumerable mod-els.
"As a result, we will never encounter un-decidable logical problems as long as the repre-sented discourse is consistent."
"The model gener-ator is the only computational tool that we need for determining preferable readings, and our ex-periment shows that for the examples discussed in this paper, it returns preferred readings in a few seconds on standard PCs as long as the background theory and the size of the domain remain managably small."
We have argued that model building can be used to provide a computational approximation of DKKMP&apos;s analysis of reciprocals.
"One crucial feature of the account is that it permits building, comparing and ranking of natural-language interpretations against each other."
"In the case of reciprocals, the ranking is given by the size of the scope relation, but other ranking criteria have already been identified in the literature as well."
"For instance, (Gardent and Konrad, To appear) shows that in the case of definite descriptions, the ranking defined by local minimality permits capturing the prefer-ence of binding over bridging, over accomoda-tion."
Similarly[REF_CITE]shows that a predicate minimisation together with a preference for logically consequent reso- lutions can be used to model the interpretation References of pronominal anaphora.
Peter Baumgartner and Michael Kiihn. 1999.
"This suggests that one of the most promising Abductive coreference by model construction. application of model generators is as a device for In ICoS-1 Inference in Computational Se-developing and testing preference systems for mantics, Institute for Logic, Language and the interpretation of natural language."
"Infer- Computation, University of Amsterdam, Au-ence and knowledge based reasoning are needed gust. in NLP not only to check for consistency and P. Blackburn, J. Bos, M. Kohlhase, and informativity (as illustrated in e.g. (Blackburn H. de Neville. 1999."
"Inference and Com-et al., 1999)), but also to express preferences putational Semantics."
"In Third Interna-between, or constraints on, possible interpreta-tional Workshop on Computational Seman-tions."
"For this, finite model builders are natural tics (IWCS-3), Tilburg, The Netherlands. tools."
"Mary Dalrymple, Makoto Kanasawa, Yookyung"
Another area that deserves further investi-
"Kim, Sam Mchombo, and Stanley Peters. gation concerns the use of minimality for dis- 1998."
Reciprocal expressions and the con-ambiguation.
"In this paper, conservative min- cept of reciprocity."
"Linguistics and Philoso-imality is used to choose among the possible phy, 21(2):159-210, April. interpretations of a particular reciprocal state- Claire Gardent and Karsten Konrad. 1999. ment."
"On the other hand, (Gardent and Web- Definites and the proper treatment ofrabbits. ber,[REF_CITE]) shows that minimality is In Proceedings of ICOS."
"Also CLAUS Report also an important tool for disambiguating noun- 111,[URL_CITE]compounds, logical metonymy and definite de- Claire Gardent and Karsten Konrad."
"As the paper shows though, many pear."
Interpreting Definites using Model questions remains open about this use of mini- Generation.
Journal of Language and Com-mality for disambiguation which are well worth putation. investigating.
Claire Gardent and Bonnie Webber.
"In further work, we intend to look at other uary 2000."
Automated deduction and ambiguous natural language constructs and to discourse disambiguation.
Submitted for identify and model the ranking criteria deter-
Also[REF_CITE]mining their preferred interpretation.
Plurals[URL_CITE]are a first obvious choice.
"But more generally, P.N. Johnson-Laird and Ruth M.J. Byrne. we hope that looking at a wider range of data 1991."
Lawrence Erlbaum Asso-will unveil a broader picture of what the gen-ciates Publishers. eral biases are which help determine a preferred
"underlying our approach and then focus on &apos;its em- Semantic interpretation has been an actively investi- pirical assessment for two basic syntactic structures gated issue on the research agenda of the logic-based of the German language, viz. genitives and auxiliary paradigm of NLP in the late eighties (e.g., Charniak constructions, in two domains. and[REF_CITE],[REF_CITE],[REF_CITE])."
"With the emergence of empirical methodologies in the early nineties, attention has al-most completely shifted away from this topic."
"Since then, semantic issues have mainly been dealt with under a lexical perspective, viz. in terms of the res-olution of lexico-semantie ambiguities (e.g.,[REF_CITE],[REF_CITE]) and the gener-ation of lexical hierarchies from large text corpora (e.g.,[REF_CITE],[REF_CITE]) massively using statistical techniques."
The research on semantic interpretation that was conducted in the pre-empiricist age of NLP was mainly driven by an interest in logical formalisms as carriers for appropriate semantic representations of NL utterances.
"With this representational bias, computational matters -- how can semantic repre-sentation structures be properly derived from parse trees for a large variety of linguistic phenomena? --became a secondary issue."
"In particular, this re-search lacked entirely quantitative data reflecting the accuracy of the proposed semantic interpreta-tion mechanisms on real-world language data."
One might be tempted to argue that recent eval- 2 The Basic Model for Semantic
The problem of semantic interpretation can be de-scribed as the mapping from syntactic to semantic (or conceptual) representation structures.
"In our ap-proach, the syntactic representation structures are given as dependency graphs[REF_CITE]."
"Un-like constituency-based syntactic descriptions, de-pendency graphs consist of lexical nodes only, and these nodes are connected by vertices, each one of which is labeled by a particular dependency relation (cf."
"For the purpose of semantic interpretation, de-pendency graphs can be decomposed into semanti-cally interpretable subgraphs3 Basically, two types of semantically interpretable subgraphs can be dis-tinguished."
"The first one consists of lexical nodes which are labeled by content words only (lexical in-stances of verbs, nouns, adjectives or adverbs) and which are directly linked by a single dependency re-lation of any type whatsoever."
Such a subgraph is illustrated in Figure 1by 8peicher- genatt - Com- uation efforts within the field of information extrac- puters.
"The second type of subgraph is also delim-tion (IE) systems[REF_CITE]are going to ited by labels of content words but, in addition, a remedy this shortcoming."
"Given, however, the fixed series of n -- 1... 4 intermediary lexical nodes may number of knowledge templates and the restricted 1This notion and all subsequent criteria for interpretation types of entities, locations, and events they encode are formally described[REF_CITE]."
"Figure 2: Concept Graph for a Sample Sentence appear between these content words, all of which are labeled by non-content words (such as auxiliary or modal verbs, prepositions)."
"Hence, in contrast to As an example, consider the description for the direct linkage we speak here of indirect linkage be-concept COMPUTER-SYSTEM."
It may be character-tween content words.
"Such a subgraph, with two ized by a set of roles, such as HAS-HARD-DISKOr HAS-intervening non-content words - the modal &quot;kann&quot; WORKING-MEMORY,with corresponding restrictions and the auxiliary &quot;werden&quot; -, is given in Figure 1 by on the concept types of potential role fillers."
"HAS-Speieher- subject - kann- verbpart - werden- WORKING-MEMORY, e.g., sanctions only fillers of verbpart - erweitert."
Another subgraph with just the concept type MEMORY.
"These conceptual con-one intervening non-content word - the preposition straints are used for semantic filtering, i.e., for the &quot;mit&quot; - is illustrated by erweitert- ppadjunct - mit elimination of syntactically admissible dependency - pobject - SDRAM-Modulen."
"From these consid-graphs which, nevertheless, do not have a valid se-erations follows that, e.g., the subgraph spanned by mantic interpretation."
"Speieher and SDRAM-Modulen does not form a Semantic interpretation, in effect, boils down to semantically interpretable subgraph, since the con-finding appropriate conceptual relations in the do-tent word erweitert intervenes on the linking path. main knowledge that link the conceptual correlates Our approach to semantic interpretation sub-of the two content words spanning the semanti-scribes to the principles of locality and composition-cally interpretable subgraph, irrespective of whether ality."
"It operates on discrete and well-defined units a direct or an indirect linkage holds at the syn- (subgraphs) of the parse tree, and the results of se-tactic level."
"Accordingly, Figure 2 depicts the se-mantic interpretation are incrementally combined by mantic/conceptual interpretation of the dependency fusing semantically interpretable subgraphs. structure given in Figure 1."
Instances represent- As semantic target language we have chosen the ing the concrete discourse entities and events in framework of KL-ONE-type description logics (DL) the sample sentence are visualized as solid rectan-[REF_CITE].
"Since these logics are gles containing a unique identifier (e.g., COMPUTER-characterized by a settheoretical semantics we stay SYSTEM.02)."
Labeled and directed edges indicate on solid formal ground.
"Fhrthermore, we take ad-instance roles."
"Dashed rectangles characterize sym-vantage of the powerful inference engine of DL sys-bols used as makers for tense and modality.2 tems, the description classifier, which turns out to be Note that in Figure 2 each tuple of content words essential for embedded reasoning during the seman-which configures a minimal subgraph in Figure 1 tic interpretation process."
"By equating the semantic has already received an interpretation in terms of a representation language with the conceptual one, we relation linking the conceptual correlates."
"For exam-follow arguments discussed[REF_CITE]. ple, Speicher- genatt - Computers (cf."
"Figure 1, The basic idea for semantic interpretation is as box 1) is mapped to COMPUTER-SYSTEM.02 HAS-follows: Each lexical surface form of a content word"
WORKING-MEMORY MEMORY.01 (cf.
"Figure 2, box is associated with a set of concept identifiers repre- 1)."
"However, the search for a valid conceptual rela-senting its (different) lexical meanings."
"This way, tion is not only limited to a simple one-link slot-filler lexical ambiguity is accounted for."
"We rather may determine conceptual re-tual correlates are internal to the domain knowledge lation paths between conceptual correlates of lexical base, where they are described by a list of attributes items, the length of which may be greater than 1. or conceptual roles, and corresponding restrictions on permitted attribute values or role fillers are asso- 2We currently do not further interpret the information con-ciated with them. tained in tense or modality markers. (Thus, the need for role composition in the DL lan- VERBTRANS, like &quot;erweitern&quot; (extend), inherit the guage becomes evident.)"
The directed search in the corresponding constraint.
"However, there are lexeme concept graph of the domain knowledge requires so- classes such as NOUN which do not render any con-phisticated structural and topological constraints to straints for dependency relations such as evidenced be manageable at all."
These constraints are encap- by gen[itive] att[ribute] (cf. Fig. 3). sulated in a special path finding and path evaluation It may even happen that such restrictions can only algorithm specified[REF_CITE]. be attached to concrete lexemes in order to avoid
Besides these conceptual constraints holding in overgeneralization.
"Fortunately, we observed that the domain knowledge, we further attempt to reduce this only happened to be the case for closed-class, the search space for finding relation paths by two i.e., non-content words."
"Accordingly, in Figure 3 kinds of syntactic criteria."
"First, the search may be the preposition &quot;with&quot; is characterized by the con-constrained by the type of dependency relation hold- straint that only the conceptual roles HAS-PART, IN-ing between the content words of the currently con- STRUMENT, etc. must be taken into consideration for sidered semantically interpretable subgraph (direct semantic interpretation. linkage), or it may be constrained by the intervening Since the constraints at the lexeme class or the lex-lexical material, viz. the non-content words (indirect eme level are hard-wired in the class hierarchy, we linkage)."
Each of these syntactic constraints has an refer to the mapping of dependency relations (or id-immediate mapping to conceptual ones. iosyncratic lexemes) to a set of conceptual relations
"For some dependency configurations, however, no (expanded to their transitive closure) as static inter-syntactic constraints may apply."
Such a case of un- pretation.
"In contradistinction, the computation of constrained semantic interpretation (e.g., for geni- relation paths for tuples of concepts during the sen-tive attributes directly linked by the genatt relation) tence analysis process is called dynamic interpreta-leads to an exhaustive directed search in the knowl- tion, since the latter process incorporates additional edge base in order to find all conceptually compati- conceptual constraints on the fly. ble role fillings among the two concepts involved."
The above-mentioned conventions allow the
Syntactic restrictions on semantic interpretation specification of high-level semantic interpretation either come from lexeme classes or concrete lexemes. schemata covering a large variety of different syntac-
They are organized in terms of the lexeme class hi- tic constructions by a single schema.
"For instance, erarchy superimposed on the fully lexicalized depen- each syntactic construction for which no conceptual dency grammar we use[REF_CITE]."
"In the constraints apply (e.g., the interpretation of geni-fragment depicted in Figure 3, the lexeme class of tives, most adjectives, etc.) receives its semantic transitive verbs, VERBTRANS, requires that when- interpretation by instantiating the same interpreta-ever a subject dependency relation is encountered, tion schema[REF_CITE]."
The power of semantic interpretation is constrained to the con- this approach comes from the fact that these high-ceptual roles AGENT or PATIENTand all their sub- level schemata are instantiated in the course of the relations (such as EXTENSION-PATIENT).
All other parsing process by exploiting the dense specifications conceptual roles are excluded from the subsequent of the inheritance hierarchies both at the grammar semantic interpretation.
"Exploiting the property in- level (the lexeme class hierarchy), as well as the con-heritance mechanisms provided by the hierarchic or- ceptual level (the concept and role hierarchies). ganization of the lexicalized dependency grammar, We currently supply up to ten semantic interpre-all concrete lexemes subsumed by the lexeme class tation schemata for declaratives, relatives, and pas- sives at the clause level, complement subcategoriza- whether so-called static constraints, effected by the tion via PPs, auxiliaries, all tenses at the VP level, mapping from a single dependency relation to one pre- and and postnominal modifiers at the NP level, or more conceptual relations, are valid (cf."
Figure 3 and anaphoric expressions.
We currently do not ac- for restrictions of this type).
"Second, one may in-count for control verbs (work in progress), coordina- vestigate the appropriateness of the results from the tion and quantification. search of the domain knowledge base, i.e., whether a relation between two concepts can be determined at always assume a correct parse to be delivered for the here evaluate the adequacy of the conceptual rep-semantic interpretation process. resentation structures relating, in principle (only re-"
"We took a random selection of 54 texts (compris-stricted, of course, by the limits of the knowledge ac-ing 18,500 words) from the two text corpora, viz. quisition devices), to the entire domain of discourse, IT test reports and MEDical finding reports."
For with all qualifications mentioned in a text.
"Whether evaluation purposes (cf. Table 1), we concentrated these are relevant or not for a particular application on the interpretation of genitives (as an instance of has to be determined by subsequent data/knowledge direct linkage; GEN) and on the interpretation of cleansing."
"In this sense, semantic interpretation periphrastic verbal complexes, i.e., passive, tempo-might deliver the raw data for transformation into ral and modal constructions (as instances of indirect appropriate IE target structures."
"Only because linkage; MODAUX). of feasibility reasons, the designers of IE systems The choice of these two grammatical patterns al-equate IE with SI."
"The cross-linking of IE and SI lows us to ignore the problems caused by syntac-tasks, however, bears the risk of having to determine, tic ambiguity, since in our data no structural am-in advance, what will be relevant or not for later re-trieval processes, assumptions which are likely to be 3Note that computations at the domain knowledge level flawed by the dynamics of domains and the unpre- which go beyond mere type checking are usually located out-dictability of the full range of interests of prospective side the scope the semantic considerations."
This is due to users. the fact that encyclopedic knowledge and its repercussions on the understanding process are typically not considered part 3.1 Methodological Issues of the semantic interpretation task proper.
"While this may be true from a strict linguistic point of view, from the com- Our methodology to deal with the evaluation of se- putational perspective of NLP this position cannot seriously mantic interpretation is based on a triple division of be maintained."
"Even more so, when semantic and conceptual test conditions."
The first category relates to checks representations are collapsed. biguities occurred.
"If one were to investigate the &quot;the surface of the storage medium&quot;), and abstract combined effects of syntactic ambiguity and seman- notions (e.g., &quot;the acceptance of IT technology&quot;). tic interpretation the evaluation scenario had to be Finally, we further distinguished evaluative expres-changed."
"Methodologically, the first step were to ex- sions (e.g., &quot;the advantages of plasma display&quot;) from plore the precision of a semantic interpretation task figurative language, including idiomatic expressions without structural ambiguities (as we do) and then, (e.g., &quot;the heart of the notebook&quot;). in the next step, incorporate the treatment of syn- At first glance, the choice of genitives may appear tactic ambiguities (e.g., by semantic filtering devices, somewhat trivial."
"From a syntactic point of view, cf."
Several guidelines were defined for the evaluation an easy case to deal with at the dependency level. procedure.
"A major issue dealt with the correctness From a conceptual perspective, however, they pro- of a semantic interpretation."
"In cases with interpre-tation, we considered a semantic interpretation to be a correct one, if the conceptual relation between the two concepts involved was considered adequate by introspection (otherwise, incorrect)."
"This qualifi-cation is not as subjective as it may sound, since we applied really strict conditions adjusted to the fine-grained domain knowledge.4"
"Interpretations were considered to be correct in those cases which con-tained exactly one relation, as well as cases of se-mantical/conceptual ambiguities (up to three read-ings, the most), presumed the relation set contained the correct one.[Footnote_5] A special case of incorrectness, called nil, occurred when no relation path could be determined though the two concepts under scrutiny were contained in the domain knowledge base and an interpretation should have been computed."
"5At the level of semantic interpretation, the notion of se-mantic ambiguity relates to the fact that the search algorithm"
We further categorized the cases where the sys-tem failed to produce an interpretation due to at least one concept specification missing (with respect to the two linked content words in a semantically interpretable subgraph).
"In all those cases with-out interpretation, insufficient coverage of the upper model was contrasted with that of the two domain models in focus, MED and IT, and with cases in which concepts referred to other domains, e.g., fash- vide a real challenge."
Since no static constraints are involved in the interpretation of genitives (cf.
"Figure 3, lexeme class NOUN) and, hence, no prescriptions of (dis)allowed conceptual relations are made, an un-constrained search (apart from connectivity condi-tions imposed on the emerging role chains) of the domain knowledge base is started."
"Hence, the main burden rests on the dynamic constraint processing part of semantic interpretation, i.e., the path find-ing procedure muddling through the complete do-main knowledge base in order to select the adequate conceptual reading(s)."
"Therefore, genitives make a strong case for test category II mentioned above."
"Dependency graphs involving modal verbs or aux-iliaries are certainly more complex at the syntac-tic level, since the corresponding semantically in-terpretable subgraphs may be composed of up to six lexical nodes."
"However, all intervening non-content-word nodes accumulate constraints for the search of a valid relation for semantic interpretations and, hence, allows us to test category III phenom-ena."
"The search space is usually pruned, since only those relations that are sanctioned by the interven-ing nodes have to be taken into consideration."
ion or food.
Ontological subareas that could nei-
"We considered a total of almost 250 genitives in all ther be assigned to the upper model nor to partic- these texts, from which about 59%/33% (MED/IT) ular domains were denoted by phrases referring to received an interpretation.6 Out of the total loss due time (e.g., &quot;the beginning of the year&quot;), space (e.g., to incomplete conceptual coverage, 56%/58% (23 of 41 genitives/57 of 98 genitives) can be attributed to 4The majority of cases were easy to judge."
"For instance, insufficient coverage of the domain models."
"Only the &quot;the infiltration of the stroma&quot; resulted in a correct reading - STROMA being the PATIENT of the INFILTRATION event -, remaining 44%/42% are due to the residual factors as well as in an incorrect one - being the AGENT of the IN-FILTRATION."
"Among the incorrect semantic interpretations we also categorized, e.g., the interpretation of the expression &quot;the prices of the manufacturers&quot; as a conceptual linkage from PRICE via PRICE-OF to PRODUCT via HAS-MANUFACTURER to MANUFACTURER (this type of role chaining can be considered listed in Table 1."
"In our sample, the number of syntactic construc-tions containing modal verbs or auxiliaries amout to 292 examples."
"Compared to genitives, we obtained a more favorable recall for both domains: 66%[REF_CITE]% for IT."
"As for genitives, lacking in-terpretations, in the majority of cases, can be at-tributed to insufficient conceptual coverage."
"For the IT domain, however, a dramatic increase in the num-ber of missing concepts is due to gaps in the upper model (78 or 63%) indicating that a large number of essential concepts for verbs were not modeled."
"Also, MED IT figurative speech plays a more important role in IT 4-nodes 4-nodes with 24 occurrences."
Both observations mirror the recall - 59% fact that IT reports are linguistically far less con- precision - 85% strained and are rhetorically more advanced than # occurrences ... 3 22 their MED counterparts. • .. with interpretation 3 13 ....... correct 3 11 Another interesting observation which is not made explicit in Table 1 concerns the distribution of modal Table 2: Interpretation Results for Semantically In-verbs and auxiliaries.
"In MED, we encountered 57 terpretable Graphs Consisting of Four Nodes passives and just one modal verb and no temporal auxiliaries, i.e., our data are in line with prevailing findings about the basic patterns of medical sublan- 59% recall and 85% precision."
If we compare this guage[REF_CITE].
"For the IT domain, cor- to the overall figures for recall (40%) and precision responding occurrences were far less biased, viz. 80 (85%), the data might indicate a gain in recall for passives, 131 modal verbs, and 23 temporal auxil- longer subgraphs, while precision keeps stable. iaries."
"Finally, for the two domains 25 samples con-"
"The results we have worked out are just a first step tained both modal verbs and auxiliaries, thus form- into a larger series of broader and deeper evaluation ing semantically interpretable subgraphs with four efforts."
"The concrete values we present, sobering as word nodes. they may be for recall (57%/31% for genitives and One might be tempted to formulate a null hy- 66%/40% for modal verbs and auxiliaries), encour-pothesis concerning the detrimental impact of the aging, however, for precision (97%/94% for genitives length of semantically interpretable subgraphs (i.e., and 95%/85% for modal verbs and auxiliaries), can the number of intervening lexical nodes carrying only be interpreted relative to other data still lacking non-content words) on the quality of semantic inter- on a broader scale. pretation."
"In order to assess the role of the length As with any such evaluation, idiosyncrasies of the of the path in a dependency graph, we separately coverage of the knowledge bases are inevitably tied investigated the results for these subclasses of com- with the results and, thus, put limits on too far-bined verbal complexes• From the entire four-node reaching generalizations."
"However, our data reflect set (cf."
"Table 2) with 25 occurrences (3 for MED and the intention to submit a knowledge-intensive text 22 for IT), 16 received an interpretation (3 for MED, understander to a realistic, i.e., conceptually un- 13 for IT)."
"While we neglect the MED data due to constrained and therefore &quot;unfriendly&quot; test environ-the small absolute numbers, the IT domain revealed ment."
"Judged from the figures of our recall data, there have been carried out, some of which under severe is no doubt, whatsoever, that conceptual coverage restrictions."
"For instance,[REF_CITE]nar-of the domain constitutes the bottleneck for any row semantic interpretation down to a very limited knowledge-based approach to NLP.~ Sublanguage range of spatial relations in anatomy, while Gomez et differences are also mirrored systematically in these al. (1997) bias the result by preselecting only those data, since medical texts adhere more closely to well- phrases that were already covered by their domain established concept taxonomies and writing stan- models, thus optimizing for precision while shunting dards than magazine articles in the IT domain. aside recall considerations."
A recent study[REF_CITE]comes 4 Related Work closest to a serious confrontation with a wide range of real-world data (Dutch dialogues on a train travel After a period of active research within the logic-domain).
"This study proceeds from a corpus of based paradigm (e.g., Charniak and Goldman annotated parse trees to which are assigned type- (1988),[REF_CITE],[REF_CITE]), logical formulae which express the corresponding se-work on semantic interpretation has almost ceased mantic interpretation."
The goal of this work is to with the emergence of the empiricist movement in compute the most probable semantic interpretation NLP (cf.
"Accuracy (i.e., precision) is studies dealing with logic-based semantic interpreta-rather high and ranges between 89,2%-92,3% de-tion in the framework of the VERBMOBIL project). pending on the training size and depth of the parse"
"Only few methodological proposals for semantic computations were made since then (e.g., higher-order colored unification as a mechanism to avoid over-generation inherent to unconstrained higher-order unificati[REF_CITE])."
An issue which has lately received more focused at-tention are ways to cope with the tremendous com-plexity of semantic interpretations in the light of an exploding number of (scope) ambiguities.
"Within the underspecification framework of semantic repre-sentations, e.g.,[REF_CITE]proposes a polynomial algorithm which constructs packed semantic repre-sentations directly from parse forests."
"All the previously mentioned studies (with the ex-ception of the experimental setup[REF_CITE]), however, lack an empirical foundation of their var-ious claims."
"Though the MUC evaluation rounds[REF_CITE]yield the flavor of an empiri-cal assessment of semantic structures, their scope is far too limited to count as an adequate evaluation platform for semantic interpretation."
"Our accuracy criterion is weaker (the intended meaning must be included in the set of all read-ings), which might explain the slightly higher rates we achieve for precision."
"However, this study does not distinguish between different syntactic construc-tions that undergo semantic interpretation, nor does it consider the level of conceptual interpretation (we focus on) as distinguished from the level of semantic interpretation to which Bonnema et al. refer."
The evaluation of the quality and adequacy of se-mantic interpretation data is still in its infancy.
"Our approach which confronts semantic interpretation devices with a random sample of textual real-world data, without intentionally constraining the selec-tion of these language data, is a real challenge for the proposed methodology and it is unique in its experimental rigor."
"However, our work is just a step in the right di-rection rather than giving a complete picture or al-lowing final conclusions."
Two reasons may be given for the lack of such experiments.
"First, interest in the deeper conceptual aspects of text interpretation guistic modules are concerned."
"More generally, in has ceased in the past years, with almost all efforts this paper the rationale underlying size (of the lex-devoted to robust and shallow syntactic processing icons, knowledge or rule bases) as the major assess-of large data sets."
This also results in a lack of so-ment category is questioned.
"Rather dimensions re-phisticated semantic and conceptual specifications, lating to the depth and breadth of the knowledge in particular, for larger text analysis systems."
"Sec-sources involved in complex system behavior should ond, providing a gold standard for semantic inter-be taken more seriously into consideration."
"This is pretation is, in itself, an incredibly underconstrained exactly what we intended to provide in this paper. and time-consuming process for which almost no re-"
"As far as evaluation studies are concerned dealing sources have been allocated in the NLP community with the assessment of semantic interpretations, few up to now. 7At least for the medical domain, we are currently actively pursuing researchon the semiautomatic creation oflarge-scale Acknowledgements."
We want to thank the mem-ontologies from weak knowledge sources (medical terminolo- bers of the ~-~ group for close cooperation.
Martin Ro-gies); cf.
