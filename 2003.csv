sentence
Recent work in Question Answering has focused on web-based systems that extract answers using simple lexico-syntactic patterns.
"We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions."
"We evaluate our strategy on a challenging subset of questions, i.e. “Who is …” questions, against a state of the art web-based Question Answering system."
Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.
Many of the recent advances in Question Answering have followed from the insight that systems can benefit by exploiting the redundancy of information in large corpora.
"While the Web is a powerful resource, its usefulness in Question Answering is not without limits."
"The Web, while nearly infinite in content, is not a complete repository of useful information."
"Most newspaper texts, for example, do not remain accessible on the Web for more than a few weeks."
"Further, while Information Retrieval techniques are relatively successful at managing the vast quantity of text available on the Web, the exactness required of Question Answering systems makes them too slow and impractical for ordinary users."
"In order to combat these inadequacies, we propose a strategy in which information is extracted automatically from electronic texts offline, and stored for quick and easy access."
"We borrow techniques from Text Mining in order to extract semantic relations (e.g., concept-instance relations) between lexical items."
We enhance these techniques by increasing the yield and precision of the relations that we extract.
Our strategy is to collect a large sample of newspaper text (15GB) and use multiple part of speech patterns to extract the semantic relations.
We then filter out the noise from these extracted relations using a machine-learned classifier.
This process generates a high precision repository of information that can be accessed quickly and easily.
"We test the feasibility of this strategy on one semantic relation and a challenging subset of questions, i.e., “Who is …” questions, in which either a concept is presented and an instance is requested (e.g., “Who is the mayor of Boston?”), or an instance is presented and a concept is requested (e.g., “Who is Jennifer Capriati?”)."
By choosing this subset of questions we are able to focus only on answers given by concept-instance relationships.
"While this paper examines only this type of relation, the techniques we propose are easily extensible to other question types."
Evaluations are conducted using a set of “Who is …” questions collected over the period of a few months from the commercial question-based search engine[URL_CITE]
"We extract approximately 2,000,000 concept-instance relations from newspaper text using syntactic patterns and machine-learned filters (e.g., “president Bill Clinton” and “Bill Clinton, president of the USA,”)."
"We then compare answers based on these relations to answers given by TextMap[REF_CITE], a state of the art web-based question answering system."
"Finally, we discuss the results of this evaluation and the implications and limitations of our strategy."
A great deal of work has examined the problem of extracting semantic relations from unstructured text.
"Using patterns involving the phrase “such as”, she reports finding only 46 relations in 20M of New York Times text."
"Finally,[REF_CITE]describes a method for extracting instances from text that takes advantage of part of speech patterns involving proper nouns."
"Mann reports extracting 200,000 concept-instance pairs from 1GB of Associated Press text, only 60% of which were found to be legitimate descriptions."
These studies indicate two distinct problems associated with using patterns to extract semantic information from text.
"First, the patterns yield only a small amount of the information that may be present in a text (the Recall problem)."
"Second, only a small fraction of the information that the patterns yield is reliable (the Precision problem)."
Our approach follows closely[REF_CITE].
"However, we extend this work by directly addressing the two problems stated above."
"In order to address the Recall problem, we extend the list of patterns used for extraction to take advantage of appositions."
"Further, following[REF_CITE], we increase our yield by increasing the amount of data used by an order of magnitude over previously published work."
"Finally, in order to address the Precision problem, we use machine learning techniques to filter the output of the part of speech patterns, thus purifying the extracted instances."
"News (.5GB), the AP newswire (~2GB), the Los Angeles Times (~.5GB), the New York Times (~2GB), Reuters (~.8GB), the Wall Street Journal (~1.2GB), and various online news websites (~.7GB)."
"The text was cleaned of HTML (when necessary), word and sentence segmented, and part of speech tagged using Brill’s tagger[REF_CITE]."
Part of speech patterns were generated to take advantage of two syntactic constructions that often indicate concept-instance relationships: common noun/proper noun constructions (CN/PN) and appositions (APOS).
Such patterns (e.g. “president George Bush”) are very productive and occur 40 times more often than patterns employed[REF_CITE].
Table 1 shows the regular expression used to extract such patterns along with examples of extracted patterns.
"In addition to the CN/PN pattern[REF_CITE], we extracted syntactic appositions (APOS)."
"This pattern detects phrases such as “Bill Gates, chairman of Microsoft,”."
Table 2 shows the regular expression used to extract appositions and examples of extracted patterns.
These regular expressions are not meant to be exhaustive of all possible varieties of patterns construed as CN/PN or APOS.
"They are “quick and dirty” implementations meant to extract a large proportion of the patterns in a text, acknowledging that some bad examples may leak through."
The concept-instance pairs extracted using the above patterns are very noisy.
"In samples of approximately 5000 pairs, 79% of the APOS extracted relations were legitimate, and only 45% of the CN/PN extracted relations were legitimate."
"This noise is primarily due to overgeneralization of the patterns (e.g., “Berlin Wall, the end of the Cold War,”) and to errors in the part of speech tagger (e.g., “Winnebago/CN Industries/PN”)."
"Further, some extracted relations were considered either incomplete (e.g., “political commentator Mr. Bruce”) or too general (e.g., “meeting site Bourbon Street”) to be useful."
"For the purposes of learning a filter, these patterns were treated as illegitimate."
"In order to filter out these noisy concept-instance pairs, 5000 outputs from each pattern were hand tagged as either legitimate or illegitimate, and used to train a binary classifier."
"The annotated examples were split into a training set (4000 examples), a validation set (500 examples); and a held out test set (500 examples)."
"The WEKA machine learning package[REF_CITE]was used to test the performance of various learning and meta-learning algorithms, including Naïve Bayes, Decision Tree, Decision List, Support Vector Machines, Boosting, and Bagging."
Table 4 shows the list of features used to describe each concept-instance pair for training the CN/PN filter.
"Features are split between those that deal with the entire pattern, only the concept, only the instance, and the pattern’s overall orthography."
The most powerful of these features examines an Ontology in order to exploit semantic information about the concept’s head.
"This semantic information is found by examining the super-concept relations of the concept head in the 110,000 node Omega Ontology (Hovy et al., in prep.)."
"Figure 1 shows the performance of different machine learning algorithms, trained on 4000 extracted CN/PN concept-instance pairs, and tested on a validation set of 500."
"Naïve Bayes, Support Vector Machine, Decision List and Decision Tree algorithms were all evaluated and the Decision Tree algorithm (which scored highest of all the algorithms) was further tested with Boosting and Bagging meta-learning techniques."
The algorithms are compared to a baseline filter that accepts concept-instance pairs if and only if the concept head is a descendent of either the concept “Human” or the concept “Occupation” in Omega.
It is clear from the figure that the Decision Tree algorithm plus Bagging gives the highest precision and overall F-score.
All subsequent experiments are run using this technique. [Footnote_1]
"1 Precision and Recall here refer only to the output of the extraction patterns.[REF_CITE]% recall indicates that all legitimate concept-instance pairs that were extracted using the patterns, were classified as legitimate by the filter. It does not indicate that all concept-instance information in the text was extracted. Precision is to be understood similarly."
"Since high precision is the most important criterion for the filter, we also examine the performance of the classifier as it is applied with a threshold."
"Thus, a probability cutoff is set such that only positive classifications that exceed this cutoff are actually classified as legitimate."
Figure 2 shows a plot of the precision/recall tradeoff as this threshold is changed.
"As the threshold is raised, precision increases while recall decreases."
Based on this graph we choose to set the threshold at 0.9.
"Applying the Decision Tree algorithm with Bagging, using the pre-determined threshold, to the held out test set of 500 examples extracted with the CN/PN pattern yields a precision of .95 and a recall of .718."
"Under these same conditions, but applied to a held out test set of 500 examples extracted with the APOS pattern, the filter has a precision of .95 and a recall of .92."
The CN/PN and APOS filters were used to extract concept-instance pairs from unstructured text.
"The output of this process is approximately [Footnote_2],000,000 concept-instance pairs."
"2 Uniqueness of instances is judged here solely on the basis of surface orthography. Thus, “Bill Clinton” and “William Clinton” are considered two distinct instances. The effects of collapsing such cases will be considered in future work."
"3 As with instances, concept uniqueness is judged solely on the basis of orthography. Thus, “Steven Spielberg” and “J. Edgar Hoover” are both considered instances of the single concept"
Table 3 shows examples of this output.
"A sample of 100 concept-instance pairs was randomly selected from the 2,000,000 extracted pairs and hand annotated. 93% of these were judged legitimate concept-instance pairs."
A large number of questions were collected over the period of a few months[URL_CITE]100 questions of the form “Who is x” were randomly selected from this set.
"The questions queried concept-instance relations through both instance centered queries (e.g., “Who is Jennifer Capriati?”) and concept centered queries (e.g., “Who is the mayor of Boston?”)."
"Answers to these questions were then automatically generated both by look-up in the 2,000,000 extracted concept-instance pairs and by TextMap, a state of the art web-based Question Answering system which ranked among the top 10 systems in the[REF_CITE]Question Answering track[REF_CITE]."
"Although both systems supply multiple possible answers for a question, evaluations were conducted on only one answer. [Footnote_4]"
4 Integration of multiple answers is an open research question and is not addressed in this work.
"For TextMap, this answer is just the output with highest confidence, i.e., the system’s first answer."
"For the extracted instances, the answer was that concept-instance pair that appeared most frequently in the list of extracted examples."
"If all pairs appear with equal frequency, a selection is made at random."
"Answers for both systems are then classified by hand into three categories based upon their information content. [Footnote_5] Answers that unequivocally identify an instance’s celebrity (e.g., “Jennifer Capriati is a tennis star”) are marked correct."
"5 Evaluation of such “definition questions” is an active research challenge and the subject of a recent TREC pilot study. While the criteria presented here are not ideal, they are consistent, and sufficient for a system comparison."
"Answers that provide some, but insufficient, evidence to identify the instance’s celebrity (e.g., “Jennifer Capriati is a defending champion”) are marked partially correct."
"Answers that provide no information to identify the instance’s celebrity (e.g., “Jennifer Capriati is a daughter”) are marked incorrect. [Footnote_6] Table 5 shows example answers and judgments for both systems."
"6 While TextMap is guaranteed to return some answer for every question posed, there is no guarantee that an answer will be found amongst the extracted concept-instance pairs. When such a case arises, the look-up method’s answer is counted as incorrect."
Results of this comparison are presented in Figure 3.
The simple look-up of extracted concept-instance pairs generated 8% more partially correct answers and 25% more entirely correct answers than TextMap.
This suggests that over half of the questions that TextMap got wrong could have benefited from information in the concept-instance pairs.
"Finally, while the look-up of extracted pairs took approximately ten seconds for all 100 questions, TextMap took approximately 9 hours."
This difference represents a time speed up of three orders of magnitude.
There are a number of reasons why the state of the art system performed poorly compared to the simple extraction method.
"First, as mentioned above, the lack of newspaper text on the web means that TextMap did not have access to the same information-rich resources that the extraction method exploited."
"Further, the simplicity of the extraction method makes it more resilient to the noise (such as parser error) that is introduced by the many modules employed by TextMap."
"And finally, because it is designed to answer any type of question, not just “Who is…“ questions, TextMap is not as precise as the extraction technique."
"This is due to both its lack of tailor made patterns for specific question types, as well as, its inability to filter those patterns with high precision."
The information repository approach to Question Answering offers possibilities of increased speed and accuracy for current systems.
"By collecting information offline, on text not readily available to search engines, and storing it to be accessible quickly and easily, Question Answering systems will be able to operate more efficiently and more effectively."
"In order to achieve real-time, accurate Question Answering, repositories of data much larger than that described here must be generated."
"We imagine huge data warehouses where each repository contains relations, such as birthplace-of, location-of, creator-of, etc."
"These repositories would be automatically filled by a system that continuously watches various online news sources, scouring them for useful information."
Such a system would have a large library of extraction patterns for many different types of relations.
"These patterns could be manually generated, such as the ones described here, or learned from text, as described[REF_CITE]."
Each pattern would have a machine-learned filter in order to insure high precision output relations.
These relations would then be stored in repositories that could be quickly and easily searched to answer user queries. [Footnote_7]
"7 An important addition to this system would be the inclusion of time/date stamp and data source information. For, while “George Bush” is “president” today, he will not be forever."
"In this way, we envision a system similar[REF_CITE]."
"However, instead of relying on costly structured databases and pain stakingly generated wrappers, repositories are automatically filled with information from many different patterns."
"Access to these repositories does not require wrapper generation, because all information is stored in easily accessible natural language text."
The key here is the use of learned filters which insure that the information in the repository is clean and reliable.
"Such a system is not meant to be complete by itself, however."
Many aspects of Question Answering remain to be addressed.
"For example, question classification is necessary in order to determine which repositories (i.e., which relations) are associated with which questions."
"Further, many question types require post processing."
Even for “Who is …” questions multiple answers need to be integrated before final output is presented.
"An interesting corollary to using this offline strategy is that each extracted instance has with it a frequency distribution of associated concepts (e.g., for “Bill Clinton”: 105 “US president”; 52 “candidate”; 4 “nominee”)."
This distribution can be used in conjunction with time/stamp information to formulate mini biographies as answers to “Who is …” questions.
We believe that generating and maintaining information repositories will advance many aspects of Natural Language Processing.
Their uses in data driven Question Answering are clear.
"In addition, concept-instance pairs could be useful in disambiguating references in text, which is a challenge in Machine Translation and Text Summarization."
"In order to facilitate further research, we have made the extracted pairs described here publicly available[URL_CITE]"
"In order to maximize the utility of these pairs, we are integrating them into an Ontology, where they can be more efficiently stored, cross-correlated, and shared."
"In this paper we present a novel, cus-tomizable IE paradigm that takes advan-tage of predicate-argument structures."
"We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm."
It is based on: (1) an extended set of features; and (2) inductive decision tree learning.
The experimental results prove our claim that accurate predicate-argument struc-tures enable high quality IE results.
"The goal of recent Information Extraction (IE) tasks was to provide event-level indexing into news stories, including news wire, radio and television sources."
"In this context, the purpose of the HUB Event-99 evaluations[REF_CITE]was to capture information on some newsworthy classes of events, e.g. natural disasters, deaths, bombings, elections, financial fluctuations or illness outbreaks."
The identification and selective extraction of rele-vant information is dictated by templettes.
"Event templettes are frame-like structures with slots rep-resenting the event basic information, such as main event participants, event outcome, time and loca-tion."
"For each type of event, a separate templette is defined."
The slots fills consist of excerpts from text with pointers back into the original source mate-rial.
Templettes are designed to support event-based browsing and search.
Figure 1 illustrates a templette defined for “market changes” as well as the source of the slot fillers.
"To date, some of the most successful IE tech-niques are built around a set of domain relevant lin-guistic patterns based on select verbs (e.g. fall, gain or lose for the “market change” topic)."
These pat-terns are matched against documents for identifying and extracting domain-relevant information.
Such patterns are either handcrafted or acquired automat-ically.
A rich literature covers methods of automati-cally acquiring IE patterns.
Some of the most recent methods were reported[REF_CITE].
"To process texts efficiently and fast, domain pat-terns are ideally implemented as finite state au-tomata (FSAs), a methodology pioneered in the F ASTUS IE system[REF_CITE]."
"Although this paradigm is simple and elegant, it has the dis-advantage that it is not easily portable from one do-main of interest to the next."
"In contrast, a new, truly domain-independent IE paradigm may be designed if we know (a) predicates relevant to a domain; and (b) which of their argu- ments fill templette slots."
Central to this new way of extracting information from texts are systems that label predicate-argument structures on the output of full parsers.
"One such augmented parser, trained on data available from the PropBank project has been recently presented[REF_CITE]."
In this paper we describe a domain-independent IE paradigm that is based on predicate-argument struc-tures identified automatically by two different meth-ods: (1) the statistical method reported[REF_CITE]; and (2) a new method based on inductive learning which obtains 17% higher F-score over the first method when tested on the same data.
The accuracy enhancement of predicate argu-ment recognition determines up to 14% better IE re-sults.
These results enforce our claim that predicate argument information for IE needs to be recognized with high accuracy.
The remainder of this paper is organized as fol-lows.
Section 2 reports on the parser that produces predicate-argument labels and compares it against the parser introduced[REF_CITE].
Section 3 describes the pattern-free IE paradigm and compares it against FSA-based IE methods.
Section 4 describes the integration of predicate-argument parsers into the IE paradigm and compares the re-sults against a FSA-based IE system.
Section 5 sum-marizes the conclusions.
Proposition Bank or PropBank is a one mil-lion word corpus annotated with predicate-argument structures.
The corpus consists of the Penn Treebank 2 Wall Street Journal texts[URL_CITE]treebank).
"The PropBank annotations, performed at University of Pennsyl-vania[URL_CITE]ace) were described[REF_CITE]."
"To date PropBank has addressed only predicates lexicalized by verbs, proceeding from the most to the least common verbs while annotating verb predicates in the corpus."
"For any given predicate, a survey was made to determine the predicate usage and if required, the usages were divided in major senses."
"However, the senses are divided more on syntactic grounds than semantic, under the fundamental assumption that syntactic frames are direct reflections of underlying semantics."
"The set of syntactic frames are determined by diathesis alternations, as defined[REF_CITE]."
Each of these syntactic frames reflect underlying semantic components that constrain allowable ar-guments of predicates.
The expected arguments of each predicate are numbered sequentially from Arg0 to Arg5.
"Regardless of the syntactic frame or verb sense, the arguments are similarly labeled to determine near-similarity of the predicates."
The general procedure was to select for each verb the roles that seem to occur most frequently and use these roles as mnemonics for the predicate argu-ments.
"Generally, Arg0 would stand for agent, Arg1 for direct object or theme whereas Arg2 rep-resents indirect object, benefactive or instrument, but mnemonics tend to be verb specific."
"For example, when retrieving the argument structure for the verb-predicate assail with the sense ”to tear attack”[URL_CITE]fmt.cgi, we find Arg0:agent, Arg1:entity assailed and Arg2:assailed for."
"Additionally, the ar-gument may include functional tags from Treebank, e.g. ArgM-DIR indicates a directional, ArgM-LOC indicates a locative, and ArgM-TMP stands for a temporal."
"In previous work using the PropBank corpus,[REF_CITE]proposed a model pre-dicting argument roles using the same statistical method as the one employed[REF_CITE]for predicting semantic roles based on the FrameNet corpus[REF_CITE]."
This statis-tical technique of labeling predicate argument oper-ates on the output of the probabilistic parser reported[REF_CITE].
It consists of two tasks: (1) iden-tifying the parse tree constituents corresponding to arguments of each predicate encoded in PropBank; and (2) recognizing the role corresponding to each argument.
Each task can be cast a separate classifier.
"For example, the result of the first classifier on the sentence illustrated in Figure 2 is the identification of the two NPs as arguments."
The second classifier assigns the specific roles ARG1 and ARG0 given the predicate “assailed”.
Statistical methods in general are hindered by the data sparsity problem.
To achieve high accuracy and resolve the data sparsity problem the method reported[REF_CITE]employed a backoff solution based on a lattice that combines the model features.
"For practical reasons, this solution restricts the size of the feature sets."
"For example, the backoff lattice[REF_CITE]consists of eight con-nected nodes for a five-feature set."
A larger set of features will determine a very complex backoff lat-tice.
"Consequently, no new intuitions may be tested as no new features can be easily added to the model."
In our studies we found that inductive learning through decision trees enabled us to easily test large sets of features and study the impact of each feature on the augmented parser that outputs predicate ar-gument structures.
For this reason we used the C5 inductive decision tree learning algorithm[REF_CITE]to implement both the classifier that identifies argument constituents and the classifier that labels arguments with their roles.
Our model considers two sets of features: Feature Set 1 (FS1): features used in the work reported[REF_CITE]and[REF_CITE]; and Feature Set 2 (FS2): a novel set of features introduced in this paper.
FS1 is illustrated in Figure 3 and FS2 is illustrated in Figure 4.
In developing FS2 we used the following obser-vations:
The results presented in this paper were obtained by training on Proposition Bank (PropBank) release 2002/7/15[REF_CITE].
Syntactic infor-mation was extracted from the gold-standard parses in TreeBank Release 2.
"As named entity information is not available in PropBank/TreeBank we tagged the training corpus with NE information using an open-domain NE recognizer, having 96% F-measure on the MUC6 [Footnote_1] data."
1 The Message Understanding Conferences (MUC) were IE evaluation exercises in the 90s. Starting with MUC6 named entity data was available.
"We reserved section 23 of Prop-Bank/TreeBank for testing, and we trained on the rest."
"Due to memory limitations on our hardware, for the argument finding task we trained on the first 150 KB of TreeBank (about 11% of TreeBank), and for the role assignment task on the first 75 KB of argument constituents (about 60% of PropBank an-notations)."
Table 1 shows the results obtained by our induc-tive learning approach.
The first column describes the feature sets used in each of the 7 experiments performed.
"The following three columns indicate  the precision (P), recall (R), and F-measure ( ) [URL_CITE] obtained for the task of identifying argument con-stituents."
The last column shows the accuracy (A) for the role assignment task using known argument constituents.
The first row in Table 1 lists the re-sults obtained when using only the FS1 features.
The next five lines list the individual contributions of each of the newly added features when combined with the FS1 features.
The last line shows the re-sults obtained when all features from FS1 and FS2 were used.
"Table 1 shows that the new features increase the argument identification F-measure by 3.61%, and the role assignment accuracy with 4.29%."
"For the argument identification task, the head and content word features have a significant contribution for the task precision, whereas NE features contribute sig-nificantly to the task recall."
"For the role assignment task the best features from the feature set FS2 are the content word features (cw and cPos) and the Boolean NE flags, which show that semantic infor-mation, even if minimal, is important for role clas-sification."
"Surprisingly, the phrasal verb collocation features did not help for any of the tasks, but they were useful for boosting the decision trees."
Deci-sion tree learning provided by C5[REF_CITE]has built in support for boosting.
We used it and obtained improvements for both tasks.
The best F-measure obtained for argument constituent identifi-cation was 88.98% in the fifth iteration (a 0.76% im-provement).
The best accuracy for role assignment was 83.74% in the eight iteration (a 0.69% improve-ment) 3 .
We further analyzed the boosted trees and noticed that phrasal verb collocation features were mainly responsible for the improvements.
This is the rationale for including them in the FS2 set.
We also were interested in comparing the results of the decision-tree-based method against the re-sults obtained by the statistical approach reported[REF_CITE].
Table 2 summarizes the results.[REF_CITE]report the re-sults listed on the first line of Table 2.
"Because no F-scores were reported for the argument identification task, we re-implemented the model and obtained the results listed on the second line."
"It looks like we had some implementation differences, and our re-sults for the argument role classification task were slightly worse."
"However, we used our results for the statistical model for comparing with the inductive learning model because we used the same feature ex-traction code for both models."
"Lines [Footnote_3] and 4 list the results of the inductive learning model with boosting enabled, when the features were only from FS1, and from FS1 and FS2 respectively."
"3 These results, listed also on the last line of Table 2, dif-fer from those in Table 1 because they were produced after the boosting took place."
"When comparing the results obtained for both models when using only features from FS1, we find that almost the same re-sults were obtained for role classification, but an en-hancement of almost 13% was obtained when recog-nizing argument constituents."
"When comparing the statistical model with the inductive model that uses all features, there is an enhancement of 17.12% for argument identification and 4.87% for argument role recognition."
Another significant advantage of our inductive learning approach is that it scales better to un- known predicates.
"The statistical model introduced[REF_CITE]uses predicate lex-ical information at most levels in the probability lattice, hence its scalability to unknown predicates is limited."
"In contrast, the decision tree approach uses predicate lexical information only for 5% of the branching decisions recorded when testing the role assignment task, and only for 0.01% of the branch-ing decisions seen during the argument constituent identification evaluation."
Figure 7(a) illustrates an IE architecture that em-ploys predicate argument structures.
"Documents are processed in parallel to: (1) parse them syntactically, and (2) recognize the NEs."
The full parser first per-forms part-of-speech (POS) tagging using transfor-mation based learning (TBL)[REF_CITE].
"Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported[REF_CITE]."
"At last, the dependency parser presented[REF_CITE]is used to generate the full parse."
This approach allows us to parse the sen-tences with less than 40 words from TreeBank sec-tion 23 with an F-measure slightly over 85% at an average of 0.12 seconds/sentence on a 2GHz Pen-tium IV computer.
"The parse texts marked with NE tags are passed to a module that identifies entity coreference in docu-ments, resolving pronominal and nominal anaphors and normalizing coreferring expressions."
The parses are also used by a module that recognizes predi-cate argument structures with any of the methods described in Section 2.
For each templette modeling a different do-main a mapping between predicate arguments and templette slots is produced.
Figure 8 illus-trates the mapping produced for two[REF_CITE]do- mains.
The “market change” domain monitors changes (AMOUNT CHANGE) and current values (CURRENT VALUE) for financial instruments (IN-STRUMENT).
"The “death” domain extracts the de-scription of the person deceased (DECEASED), the manner of death (MANNER OF DEATH), and, if ap-plicable, the person to whom the death is attributed (AGENT OF DEATH)."
"To produce the mappings we used training data that consists of: (1) texts, and (2) their correspond-ing filled templettes."
Each templette has pointers back to the source text similarly to the example pre-sented in Figure 1.
"When the predicate argument structures were identified, the mappings were col-lected as illustrated in Figure 9."
Figure 9(a) shows an interesting aspect of the mappings.
"Although the role classification of the last argument is incorrect (it should have been identified as ARG4), it is mapped into the C URRENT -V ALUE slot."
This shows how the mappings resolve incorrect but consistent classifica-tions.
Figure 9(b) shows the flexibility of the system to identify and classify constituents that are not close to the predicate phrase (ARG0).
"This is a clear ad- vantage over the FSA-based system, which in fact missed the A GENT -O F -D EATH in this sentence."
"Be-cause several templettes might describe the same event, event coreference is processed and, based on the results, templettes are merged when necessary."
The IE architecture in Figure 7(a) may be com-pared with the IE architecture with cascaded FSA represented in Figure 7(b) and reported[REF_CITE].
"Both architectures share the same NER, coreference and merging modules."
"Specific to the FSA-based architec-ture are the phrasal parser, which identifies simple phrases such as basic noun or verb phrases (some of them domain specific), the combiner, which builds domain-dependent complex phrases, and the event recognizer, which detects the domain-specific Subject-Verb-Object (SVO) patterns."
An example of a pattern used by the FSA-based architecture is: D EATH -C AUSE K ILL -V
"ERB P ERSON , where D EATH -C AUSE may identify more than 20 lexemes, e.g. wreck, catastrophe, malpractice, and more than 20 verbs are K ILL -V ERBS , e.g. murder, execute, be-head, slay."
"Most importantly, each pattern must rec-ognize up to 26 syntactic variations, e.g. determined by the active or passive form of the verb, relative subjects or objects etc."
Predicate argument struc-tures offer the great advantage that syntactic vari-ations do not need to be accounted by IE systems anymore.
"Because entity and event coreference, as well as templette merging will attempt to recover from par-tial patterns or predicate argument recognitions, and our goal is to compare the usage of FSA patterns versus predicate argument structures, we decided to disable the coreference and merging modules."
This explains why in Figure 7 these modules are repre- sented with dashed lines.
"To evaluate the proposed IE paradigm we selected two[REF_CITE]domains: “market change”, which tracks changes in stock indexes, and “death”, which extracts all manners of human deaths."
These do-mains were selected because most of the domain in-formation can be processed without needing entity or event coreference.
"Moreover, one of the domains (market change) uses verbs commonly used in Prop-Bank/TreeBank, while the other (death) uses rela-tively unknown verbs, so we can also evaluate how well the system scales to verbs unseen in training."
Table 3 lists the F-scores for the two domains.
The first line of the Table lists the results obtained by the IE architecture illustrated in Figure 7(a) when the predicate argument structures were identified by the statistical model.
The next line shows the same results for the inductive learning model.
The last line shows the results for the IE architecture in Fig-ure 7(b).
"The results obtained by the FSA-based IE were the best, but they were made possible by hand-crafted patterns requiring an effort of 10 person days per domain."
"The only human effort necessary in the new IE paradigm was imposed by the genera-tion of mappings between arguments and templette slots, accomplished in less than 2 hours per domain, given that the training templettes are known."
"Addi-tionally, it is easier to automatically learn these map-pings than to acquire FSA patterns."
Table 3 also shows that the new IE paradigm per-forms better when the predicate argument structures are recognized with the inductive learning model.
The cause is the substantial difference in quality of the argument identification task between the two models.
"The Table shows that the new IE paradigm with the inductive learning model achieves about 90% of the performance of the FSA-based system for both domains, even though one of the domains uses mainly verbs rarely seen in training (e.g. “die” appears 5 times in PropBank)."
Another way of evaluating the integration of pred-icate argument structures in IE is by comparing the number of events identified by each architecture.
Ta-ble 4 shows the results.
"Once again, the new IE paradigm performs better when the predicate argu-ment structures are recognized with the inductive learning model."
More events are missed by the sta-tistical model which does not recognize argument constituents as well the inductive learning model.
This paper reports on a novel inductive learning method for identifying predicate argument struc-tures in text.
"The proposed approach achieves over 88% F-measure for the problem of identifying argu-ment constituents, and over 83% accuracy for the task of assigning roles to pre-identified argument constituents."
"Because predicate lexical information is used for less than 5% of the branching decisions, the generated classifier scales better than the statisti-cal method[REF_CITE]to un-known predicates."
This way of identifying pred-icate argument structures is a central piece of an IE paradigm easily customizable to new domains.
The performance degradation of this paradigm when compared to IE systems based on hand-crafted pat-terns is only 10%.
We introduce a probabilistic noisy-channel model for question answering and we show how it can be exploited in the context of an end-to-end QA system.
Our noisy-channel system outperforms a state-of-the-art rule-based QA system that uses similar resources.
"We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing."
Current state-of-the-art Question Answering (QA) systems are extremely complex.
"They contain tens of modules that do everything from information retrieval, sentence parsing ([REF_CITE];"
"They access external resources such as the WordNet ([REF_CITE]), the web[REF_CITE], structured, and semi-structured databases[REF_CITE]."
"They contain feedback loops, ranking, and re-ranking modules."
"Given their complexity, it is often difficult (and sometimes impossible) to understand what contributes to the performance of a system and what doesn’t."
"In this paper, we propose a new approach to QA in which the contribution of various resources and components can be easily assessed."
"The fundamental insight of our approach, which departs significantly from the current architectures, is that, at its core, a QA system is a pipeline of only two modules: • An IR engine that retrieves a set of M documents/N sentences that may contain answers to a given question Q. • And an answer identifier module that given a question Q and a sentence S (from the set of sentences retrieved by the IR engine) identifies a sub-string S A of S that is likely to be an answer to Q and assigns a score to it."
"Once one has these two modules, one has a QA system because finding the answer to a question Q amounts to selecting the sub-string S A of highest score."
"Although this view is not made explicit by QA researchers, it is implicitly present in all systems we are aware of."
"In its simplest form, if one accepts a whole sentence as an answer (S A = S), one can assess the likelihood that a sentence S contains the answer to a question Q by measuring the cosine similarity between Q and S. However, as research in QA demonstrates, word-overlap is not a good enough metric for determining whether a sentence contains the answer to a question."
"Consider, for example, the question “Who is the leader of France?”"
"The sentence “Henri Hadjenberg, who is the leader of France’s Jewish community, endorsed confronting the specter of the Vichy past” overlaps with all question terms, but it does not contain the correct answer; while the sentence “Bush later met with French President Jacques Chirac” does not overlap with any question term, but it does contain the correct answer."
"To circumvent this limitation of word-based similarity metrics, QA researchers have developed methods through which they first map questions and sentences that may contain answers in different spaces, and then compute the “similarity” between them there."
"For example, the systems developed at IBM and ISI map questions and answer sentences into parse trees and surface-based semantic labels and measure the similarity between questions and answer sentences in this syntactic/semantic space, using QA-motivated metrics."
The systems developed by CYC and LCC map questions and answer sentences into logical forms and compute the “similarity” between them using inference rules.
And systems such as those developed by IBM and BBN map questions and answers into feature sets and compute the similarity between them using maximum entropy models that are trained on question-answer corpora.
"From this perspective then, the fundamental problem of question answering is that of finding spaces where the distance between questions and sentences that contain correct answers is small and where the distance between questions and sentences that contain incorrect answers is large."
"In this paper, we propose a new space and a new metric for computing this distance."
"Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recogniti[REF_CITE], part of speech tagging[REF_CITE], machine translati[REF_CITE], information retrieval[REF_CITE], and text summarizati[REF_CITE], we develop a noisy channel model for QA."
This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations.
"Given a corpus of question-answer pairs (Q, S A ), we can train a probabilistic model for estimating the conditional probability P(Q | S A )."
"Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by an IR engine, one can find the sentence S i ∈ Σ and an answer in it A i,j by searching for the S i,A that maximizes the i,j conditional probability P(Q | S i,A ). i,j"
"In Section 2, we first present the noisy-channel model that we propose for this task."
"In Section 3, we describe how we generate training examples."
"In Section 4, we describe how we use the learned models to answer factoid questions, we evaluate the performance of our system using a variety of experimental conditions, and we compare it with a rule-based system that we have previously used in several TREC evaluations."
"In Section 5, we demonstrate that the framework we propose is flexible enough to accommodate a wide range of resources and techniques that have been employed in state-of-the-art QA systems."
Assume that we want to explain why “1977” in sentence S in Figure 1 is a good answer for the question “When did Elvis Presley die?”
"To do this, we build a noisy channel model that makes explicit how answer sentence parse trees are mapped into questions."
"Consider, for example, the automatically derived answer sentence parse tree in Figure 1, which associates to nodes both syntactic and shallow semantic, named-entity-specific tags."
"In order to rewrite this tree into a question, we assume the following generative story: 1."
"In general, answer sentences are much longer than typical factoid questions."
"To reduce the length gap between questions and answers and to increase the likelihood that our models can be adequately trained, we first make a “cut” in the answer parse tree and select a sequence of words, syntactic, and semantic tags."
The “cut” is made so that every word in the answer sentence or one of its ancestors belongs to the “cut” and no two nodes on a path from a word to the root of the tree are in the “cut”.
Figure 1 depicts graphically such a cut. 2.
"Once the “cut” has been identified, we mark one of its elements as the answer string."
"In Figure 1, we decide to mark DATE as the answer string (A_DATE). 3."
There is no guarantee that the number of words in the cut and the number of words in the question match.
"To account for this, we stochastically assign to every element s i in a cut a fertility according to table n(φ | s i )."
"We delete elements of fertility 0 and duplicate elements of fertility 2, etc."
With probability p 1 we also increment the fertility of an invisible word NULL.
"NULL and fertile words, i.e. words with fertility strictly greater than 1 enable us to align long questions with short answers."
Zero fertility words enable us to align short questions with long answers. 4.
"Next, we replace answer words (including the NULL word) with question words according to the table t(q i | s j ). 5."
"In the last step, we permute the question words according to a distortion table d, in order to obtain a well-formed, grammatical question."
The probability P(Q | S A ) is computed by multiplying the probabilities in all the steps of our generative story (Figure 1 lists some of the factors specific to this computation.)
The readers familiar with the statistical machine translation (SMT) literature should recognize that steps 3 to 5 are nothing but a one-to-one reproduction of the generative story proposed in the SMT context by Brown et al. (see[REF_CITE]for a detailed mathematical description of the model and the formula for computing the probability of an alignment and target string given a source string). [Footnote_1]
1 The distortion probabilities depicted in Figure 1 are a simplification of the distortions used in the IBM Model 4 model[REF_CITE]. We chose this watered down representation only for illustrative purposes. Our QA system implements the full-blown Model 4 statistical model described by Brown et al.
"To simplify our work and to enable us exploit existing off-the-shelf software, in the experiments we carried out in conjunction with this paper, we assumed a flat distribution for the two steps in our generative story."
"That is, we assumed that it is equally likely to take any cut in the tree and equally likely to choose as Answer any syntactic/semantic element in an answer sentence."
Assume that the question-answer pair in Figure 1 appears in our training corpus.
"When this happens, we know that 1977 is the correct answer."
"To generate a training example from this pair, we tokenize the question, we parse the answer sentence, we identify the question terms and answer in the parse tree, and then we make a &quot;cut&quot; in the tree that satisfies the following conditions: a) Terms overlapping with the question are preserved as surface text b)"
"The answer is reduced to its semantic or syntactic class prefixed with the symbol “A_” c) Non-leaves, which don’t have any question term or answer offspring, are reduced to their semantic or syntactic class. d) All remaining nodes (leaves) are preserved as surface text."
Condition a) ensures that the question terms will be identified in the sentence.
Condition b) helps learn answer types.
Condition c) brings the sentence closer to the question by compacting portions that are syntactically far from question terms and answer.
And finally the importance of lexical cues around question terms and answer motivates condition d).
"For the question-answer pair in Figure 1, the algorithm above generates the following training example: 



"
Figure 2 represents graphically the conditions that led to this training example being generated.
Our algorithm for generating training pairs implements deterministically the first two steps in our generative story.
The algorithm is constructed so as to be consistent with our intuition that a generative process that makes the question and answer as similar-looking as possible is most likely to enable us learn a useful model.
Each question- answer pair results in one training example.
It is the examples generated through this procedure that we use to estimate the parameters of our model.
Assume now that the sentence in Figure 1 is returned by an IR engine as a potential candidate for finding the answer to the question “When did Elvis Presley die?”
"In this case, we don’t know what the answer is, so we assume that any semantic/syntactic node in the answer sentence can be the answer, with the exception of the nodes that subsume question terms and stop words."
"In this case, given a question and a potential answer sentence, we generate an exhaustive set of question-answer test cases, each test case labeling as answer (A_) a different syntactic/semantic node."
Here are some of the test cases we consider for the question-answer pair in Figure 1:
"If we learned a good model, we would expect it to assign a higher probability to P(Q | S ai ) than to P(Q | S a1 ) and P(Q | S aj )."
"For training, we use three different sets. (i)"
The TREC9-10 set consists of the questions used[REF_CITE].
"We automatically generate answer-tagged sentences using the[REF_CITE]judgment sets, which are lists of answer-document pairs evaluated as either correct or wrong."
"For every question, we first identify in the judgment sets a list of documents containing the correct answer."
"For every document, we keep only the sentences that overlap with the question terms and contain the correct answer. (ii)"
"In order to have more variation of sentences containing the answer, we have automatically extended the first data set using the Web."
"For every TREC9-10 question/answer pair, we used our Web-based IR to retrieve sentences that overlap with the question terms and contain the answer."
We call this data set TREC9-10Web. (iii)
The third data set consists of 2381 question/answer pairs collected[URL_CITE]
We use the same method to automatically enhance this set by retrieving from the web sentences containing answers to the questions.
We call this data set Quiz-Zone.
Table 1 shows the size of the three training corpora:
"To train our QA noisy-channel model, we apply the algorithm described in Section 3.1 to generate training cases for all QA pairs in the three corpora."
"To help our model learn that it is desirable to copy answer words into the question, we add to each corpus a list of identical dictionary word pairs w i-w i ."
"For each corpus, we use GIZA[REF_CITE], a publicly available SMT package that implements the IBM models[REF_CITE], to train a QA noisy-channel model that maps flattened answer parse trees, obtained using the “cut” procedure described in Section 3.1, into questions."
We used two different data sets for the purpose of testing.
The first set consists of the 500 questions used[REF_CITE]; the second set consists of 500 questions that were randomly selected from the Knowledge Master (KM) repository[URL_CITE]
The KM questions tend to be longer and quite different in style compared to the TREC questions.
Our QA system is straightforward.
"It has only two modules: an IR module, and an answer-identifier/ranker module."
The IR module is the same we used in previous participations at TREC.
"As the learner, the answer-identifier/ranker module is also publicly available – the GIZA package can be configured to automatically compute the probability of the Viterbi alignment between a flattened answer parse tree and a question."
"For each test question, we automatically generate a web query and use the top 300 answer sentences returned by our IR engine to look for an answer."
"For each question Q and for each answer sentence S i , we use the algorithm described in Section 3.2 to exhaustively generate all Q- S i,Ai,j pairs."
Hence we examine all syntactic constituents in a sentence and use GIZA to assess their likelihood of being a correct answer.
"We select the answer A i,j that maximizes P(Q | S i,A ) for all answer sentences S i i,j and all answers A i,j that can be found in list retrieved by the IR module."
Figure 3 depicts graphically our noisy-channel-based QA system.
We evaluate the results by generating automatically the mean reciprocal rank (MRR) using the[REF_CITE]patterns and QuizZone original answers when testing[REF_CITE]and QuizZone test sets respectively.
"Our baseline is a state of the art QA system, QA-base, which was ranked from second to seventh in the last 3 years at TREC."
"To ensure a fair comparison, we use the same Web-based IR system in all experiments with no answer retrofitting."
"For the same reason, we use the QA-base system with the post-processing module disabled. (This module re-ranks the answers produced by QA-base on the basis of their redundancy, frequency on the web, etc.)"
Table 2 summarizes results of different combinations of training and test sets:
"For the[REF_CITE]corpus, the relatively low MRRs are due to the small answer coverage of the[REF_CITE]patterns."
"For the KM corpus, the relatively low MRRs are explained by two factors: (i) for this corpus, each evaluation pattern consists of only one string – the original answer; (ii) the KM questions are more complex than TREC questions (What piece of furniture is associated with Modred, Percival, Gawain, Arthur, and Lancelot?)."
"It is interesting to see that using only the TREC9-10 data as training (system A in Table 2), we are able to beat the baseline when testing[REF_CITE]questions; however, this is not true when testing on KM questions."
This can be explained by the fact that the TREC9-10 training set is similar to the[REF_CITE]test set while it is significantly different from the KM test set.
"We also notice that expanding the training to TREC9- 10Web (System B) and then to Quiz-Zone (System C) improved the performance on both test sets, which confirms that both the variability across answer tagged sentences (Trec9-10Web) and the abundance of distinct questions (Quiz-Zone) contribute to the diversity of a QA training corpus, and implicitly to the performance of our system."
Another characteristic of our framework is its flexibility.
We can easily extend it to span other question-answering resources and techniques that have been employed in state-of-the art QA systems.
"In the rest of this section, we assess the impact of such resources and techniques in the context of three case studies."
The LCC[REF_CITE]QA system[REF_CITE]implements a reasoning mechanism for justifying answers.
"In the LCC framework, questions and answers are first mapped into logical forms."
A resolution-based module then proves that the question logically follows from the answer using a set of axioms that are automatically extracted from the WordNet glosses.
"For example, to prove the logical form of “What is the age of our solar system?” from the logical form of the answer “The solar system is 4.6 billion years old.”, the LCC theorem prover shows that the atomic formula that corresponds to the question term “age” can be inferred from the atomic formula that corresponds to the answer term “old” using an axiom that connects “old” and “age”, because the WordNet gloss for “old” contains the word “age”."
"Similarly, the LCC system can prove that “Voting is mandatory for all Argentines aged over 18” provides a good justification for the question “What is the legal age to vote in Argentina?” because it can establish through logical deduction using axioms induced from WordNet glosses that “legal” is related to “rule”, which in turn is related to “mandatory”; that “age” is related to “aged”; and that “Argentine” is related to “Argentina”."
It is not difficult to see by now that these logical relations can be represented graphically as alignments between question and answer terms (see Figure 4).
"The exploitation of WordNet synonyms, which is part of many QA systems[REF_CITE], is a particular case of building such alignments between question and answer terms."
"For example, using WordNet synonymy relations, it is possible to establish a connection between “U.S.” and “United States” and between “buy” and “purchase” in the question-answer pair (Figure 5), thus increasing the confidence that the sentence contains a correct answer."
The noisy channel framework we proposed in this paper can approximate the reasoning mechanism employed by LCC and accommodate the exploitation of gloss- and synonymy-based relations found in WordNet.
"In fact, if we had a very large training corpus, we would expect such connections to be learned automatically from the data."
"However, since we have a relatively small training corpus available, we rewrite the WordNet glosses into a dictionary by creating word-pair entries that establish connections between all Wordnet words and the content words in their glosses."
"For example, from the word “age” and its gloss “a historic period”, we create the dictionary entries “age - historic” and “age – period”."
"To exploit synonymy relations, for every WordNet synset S i , we add to our training data all possible combinations of synonym pairs W i,x -W i,y ."
"Our dictionary creation procedure is a crude version of the axiom extraction algorithm described[REF_CITE]; and our exploitation of the glosses in the noisy-channel framework amounts to a simplified, statistical version of the semantic proofs implemented by LCC."
Table 3 shows the impact of WordNet synonyms (WNsyn) and WordNet glosses (WNgloss) on our system.
Adding WordNet synonyms and glosses improved slightly the performance on the KM questions.
"On the other hand, it is surprising to see that the performance has dropped when testing[REF_CITE]questions."
"To make use of this technique, we extend our training data set by expanding every question-answer pair Q-S A to a list (Q r -S A ), Q r ⊂ Θ where Θ is the set of question reformulations. [Footnote_2] We also expand in a similar way the answer candidates in the test corpus."
2 We are grateful to Ulf Hermjakob for sharing his reformulations with us.
Using reformulations improved the performance of our system on the[REF_CITE]test set while it was not beneficial for the KM test set (see Table 4).
"We believe this is explained by the fact that the reformulation engine was fine tuned on TREC-specific questions, which are significantly different from KM questions. 5.3 Exploiting data in structured -and semi-structured databases"
Structured and semi-structured databases were proved to be very useful for question-answering systems.
We adopted a different approach to exploit external knowledge bases.
"In our work, we first generated a natural language collection of factoids by mining different structured and semi-structured databases (World Fact Book, Biography.com, WordNet…)."
"The generation is based on manually written question-factoid template pairs, which are applied on the different sources to yield simple natural language question-factoid pairs."
"Consider, for example, the following two factoid-question template pairs:"
Q t1 :
What is the capital of _c?
S t1 :
The capital of _c is capital(_c).
Q t2 : How did _p die?
S t2 : _p died of causeDeath(_p).
"Using extraction patterns[REF_CITE], we apply these two templates on the World Fact Book database and on biography.com pages to instantiate question and answer-tagged sentence pairs such as:"
Q 1 : What is the capital of Greece?
S 1 : The capital of Greece is Athens.
Q 2 : How did Jean-Paul Sartre die?
S 2 : Jean-Paul Sartre died of a lung ailment.
These question-factoid pairs are useful both in training and testing.
"In training, we simply add all these pairs to the training data set."
"In testing, for every question Q, we select factoids that overlap sufficiently enough with Q as sentences that potentially contain the answer."
"For example, given the question “Where was Sartre born?” we will select the following factoids: 1-Jean-Paul Sartre was born in 1905. 2-Jean-Paul Sartre died in 1980. 3-Jean-Paul Sartre was born in Paris. 4-Jean-Paul Sartre died of a lung ailment."
"Up to now, we have collected about 100,000 question-factoid pairs."
We found out that these pairs cover only 24 of the 500[REF_CITE]questions.
"And so, in order to evaluate the value of these factoids, we reran our system C on these 24 questions and then, we used the question-factoid pairs as the only resource for both training and testing as described earlier (System D)."
Table 5 shows the MRRs for systems C and D on the 24 questions covered by the factoids.
It is very interesting to see that system D outperforms significantly system C.
"This shows that, in our framework, in order to benefit from external databases, we do not need any additional machinery (question classifiers, answer type identifiers, wrapper selectors, SQL query generators, etc.)"
All we need is a one-time conversion of external structured resources to simple natural language factoids.
"The results in Table 5 also suggest that collecting natural language factoids is a useful research direction: if we collect all the factoids in the world, we could probably achieve much higher MRR scores on the entire TREC collection."
"In this paper, we proposed a noisy-channel model for QA that can accommodate within a unified framework the exploitation of a large number of resources and QA-specific techniques."
We believe that our work will lead to a better understanding of the similarities and differences between the approaches that make up today’s QA research landscape.
"We also hope that our paper will reduce the high barrier to entry that is explained by the complexity of current QA systems and increase the number of researchers working in this field: because our QA system uses only publicly available software components (an IR engine; a parser; and a statistical MT system), it can be easily reproduced by other researchers."
"However, one has to recognize that the reliance of our system on publicly available components is not ideal."
The generative story that our noisy-channel employs is rudimentary; we have chosen it only because we wanted to exploit to the best extent possible existing software components (GIZA).
The empirical results we obtained are extremely encouraging: our noisy-channel system is already outperforming a state-of-the-art rule-based system that took many person years to develop.
"It is remarkable that a statistical machine translation system can do so well in a totally different context, in question answering."
"However, building dedicated systems that employ more sophisticated, QA-motivated generative stories is likely to yield significant improvements."
This work was supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program under contract number[REF_CITE]-02-C-0007.
"Kernel-based learning (e.g., Support Vec-tor Machines) has been successfully ap-plied to many hard problems in Natural Language Processing (NLP)."
"In NLP, al-though feature combinations are crucial to improving performance, they are heuris-tically selected."
Kernel methods change this situation.
The merit of the kernel methods is that effective feature combina-tion is implicitly expanded without loss of generality and increasing the compu-tational costs.
"Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis."
"In this paper, we extend a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier."
"Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Depen-dency Parsing show that our new classi-fiers are about 30 to 300 times faster than the standard kernel-based classifiers."
"Kernel methods (e.g., Support Vector Machines[REF_CITE]) attract a great deal of attention re-cently."
"In the field of Natural Language Process-ing, many successes have been reported."
Examples include Part-of-Speech tagging[REF_CITE]
"Text Chunking[REF_CITE], Named Entity Recogniti[REF_CITE], and Japanese Dependency Parsing[REF_CITE]."
It is known in NLP that combination of features contributes to a significant improvement in accuracy.
"For instance, in the task of dependency parsing, it would be hard to confirm a correct dependency re-lation with only a single set of features from either a head or its modifier."
"Rather, dependency relations should be determined by at least information from both of two phrases."
"In previous research, feature combination has been selected manually, and the performance significantly depended on these selec-tions."
This is not the case with kernel-based method-ology.
"For instance, if we use a polynomial ker-nel, all feature combinations are implicitly expanded without loss of generality and increasing the compu-tational costs."
"Although the mapped feature space is quite large, the maximal margin strategy[REF_CITE]of SVMs gives us a good generalization per-formance compared to the previous manual feature selection."
This is the main reason why kernel-based learning has delivered great results to the field of NLP.
"Kernel-based text analysis shows an excellent per-formance in terms in accuracy; however, its inef-ficiency in actual analysis limits practical applica-tion."
"For example, an SVM-based NE-chunker runs at a rate of only 85 byte/sec, while previous rule-based system can process several kilobytes per sec-ond[REF_CITE]."
"Such slow exe-cution time is inadequate for Information Retrieval, Question Answering, or Text Mining, where fast analysis of large quantities of text is indispensable."
This paper presents two novel methods that make the kernel-based text analyzers substantially faster.
These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector.
"More specifically, we focus on a Polynomial Ker-nel of degree d, which can attain feature combina-tions that are crucial to improving the performance of tasks in NLP."
"Second, we introduce two fast clas-sification algorithms for this kernel."
"One is PKI (Polynomial Kernel Inverted), which is an exten-sion of Inverted Index in Information Retrieval."
"The other is PKE (Polynomial Kernel Expanded), where all feature combinations are explicitly expanded."
"By applying PKE, we can convert a kernel-based clas-sifier into a simple and fast liner classifier."
"In order to build PKE, we extend the PrefixSpan[REF_CITE], an efficient Basket Mining algorithm, to enu-merate effective feature combinations from a set of support examples."
"Experiments on English BaseNP Chunking, Japanese Word Segmentation and Japanese Depen-dency Parsing show that PKI and PKE perform re-spectively 2 to 13 times and 30 to 300 times faster than standard kernel-based systems, without a dis-cernible change in accuracy."
"Suppose we have a set of training data for a binary classification problem: (x 1 , y 1 ), . .. , (x L , y L ) x j ∈ &lt; N , y j ∈ {+1, −1}, where x j is a feature vector of the j-th training sam-ple, and y j is the class label associated with this training sample."
"The decision function of SVMs is defined by ³X ´ y(x) = sgn y j α j φ(x j ) · φ(x) + b , (1) j∈SV where: (A) φ is a non-liner mapping function from &lt; N to &lt; H (N ¿ H). (B) α j , b ∈ &lt;, α j ≥ 0."
The mapping function φ should be designed such that all training examples are linearly separable in &lt; H space.
"Since H is much larger than N, it re-quires heavy computation to evaluate the dot prod-ucts φ(x i ) · φ(x) in an explicit form."
This problem can be overcome by noticing that both construction of optimal parameter α i (we will omit the details of this construction here) and the calculation of the decision function only require the evaluation of dot products φ(x i )·φ(x).
"This is critical, since, in some cases, the dot products can be evaluated by a simple Kernel Function: K(x 1 , x 2 ) = φ(x 1 ) · φ(x 2 )."
"Sub-stituting kernel function into (1) , we have the fol-lowing decision function. ³X ´ y(x) = sgn y j α j K(x j , x) + b (2) j∈SV"
"One of the advantages of kernels is that they are not limited to vectorial object x, but that they are appli-cable to any kind of object representation, just given the dot products."
"For many tasks in NLP, the training and test ex-amples are represented in binary vectors; or sets, since examples in NLP are usually represented in so-called Feature Structures."
"Here, we focus on such cases 1 ."
"Suppose a feature set F = {1, 2, . . . , N} and training examples X j (j = 1, 2, . .. , L), all of which are subsets of F (i.e., X j ⊆ F)."
"In this case, X j can be regarded as a binary vector x j = (x j1 , x j2 , . . . , x jN ) where x ji = 1 if i ∈ X j , x ji = 0 otherwise."
The dot product of x 1 and x 2 is given by x 1 · x 2 = |X 1 ∩ X 2 |.
"Definition 1 Polynomial Kernel of degree d Given sets X and Y , corresponding to binary fea-ture vectors x and y, Polynomial Kernel of degree d K d (X, Y ) is given by"
"K d (x, y) ="
"K d (X, Y ) = (1 + |X ∩ Y |) d , (3) where d = 1, 2, 3, . . .."
"In this paper, (3) will be referred to as an implicit form of the Polynomial Kernel. 1 In the Maximum Entropy model widely applied in NLP, we usually suppose binary feature functions f i (X j ) ∈ {0, 1}."
This formalization is exactly same as representing an example X j in a set {k|f k (X j ) = 1}.
"It is known in NLP that a combination of features, a subset of feature set F in general, contributes to overall accuracy."
"In previous research, feature com-bination has been selected manually."
"The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in compu-tational costs, since the Polynomial Kernel of degree d implicitly maps the original feature space F into F d space. (i.e., φ : F → F d )."
"This property is critical and some reports say that, in NLP, the poly-nomial kernel outperforms the simple linear kernel[REF_CITE]."
"Here, we will give an explicit form of the Polyno-mial Kernel to show the mapping function φ(·)."
"In this section, we introduce two fast classification algorithms for the Polynomial Kernel of degree d."
"Before describing them, we give the baseline clas-sifier (PKB): ³X ´ y(X) = sgn y j α j · (1 + |X j ∩ X|) d + b . (5) j∈SV"
"The complexity of PKB is O(|X| · |SV |), since it takes O(|X|) to calculate (1 + |X j ∩ X|) d and there are a total of |SV | support examples."
"Given an item i ∈ F, if we know in advance the set of support examples which contain item i ∈ F, we do not need to calculate |X j ∩ X| for all support examples."
This is a naive extension of Inverted In-dexing in Information Retrieval.
Figure 1 shows the pseudo code of the algorithm PKI.
The function h(i) is a pre-compiled table and returns a set of support examples which contain item i.
"The complexity of the PKI is O(|X| · B + |SV |), where B is an average of |h(i)| over all item i ∈ F ."
"The PKI can make the classification speed drasti-cally faster when B is small, in other words, when feature space is relatively sparse (i.e., B ¿ |SV |)."
"The feature space is often sparse in many tasks in NLP, since lexical entries are used as features."
The algorithm PKI does not change the final ac-curacy of the classification.
"Using Lemma 1, we can represent the decision function (5) in an explicit form: ³X ´¡X d ¢ y(X) = sgn y j α j c d (r) · |P r (X j ∩ X)| + b . (6) j∈SV r=0"
"If we, in advance, calculate"
"X w(s) = y j α j c d (|s|)I(s ∈ P |s| (X j )) j∈SV (where S I(t) is an indicator function 2 ) for all subsets s ∈ dr=0 P r (F ), (6) can be written as the following simple linear form: ³ X ´ y(X) = sgn w(s) + b . (7) s∈Γ d (X)"
S where Γ d (X) = dr=0 P r (X).
The classification algorithm given by (7) will be referred to as PKE.
"The complexity of PKE is O(|Γ d (X)|) = O(|X| d ), independent on the num-ber of support examples |SV |."
"To apply the PKE, we first calculate |Γ d (F)| de-gree of vectors w = (w(s 1 ), w(s 2 ), . . . , w(s |Γ d (F)| ))."
"This calculation is trivial only when we use a Quadratic Kernel, since we just project the origi-nal feature space F into F × F space, which is small enough to be calculated by a naive exhaustive method."
"However, if we, for instance, use a poly-nomial kernel of degree 3 or higher, this calculation becomes not trivial, since the size of feature space exponentially increases."
"Here we take the following strategy: 1. Instead of using the original vector w, we use w 0 , an approximation of w. [Footnote_2]."
"2 I(t) returns 1 if t is true,returns 0 otherwise."
We apply the Subset Mining algorithm to cal-culate w 0 efficiently.
"To demonstrate performances of PKI and PKE, we examined three NLP tasks: English BaseNP Chunk-ing (EBC), Japanese Word Segmentation (JWS) and"
Japanese Dependency Parsing (JDP).
"A more de-tailed description of each task, training and test data, the system parameters, and feature sets are presented in the following subsections."
"Table 1 summarizes the detail information of support examples (e.g., size of SVs, size of feature set etc.)."
"Our preliminary experiments show that a Quadratic Kernel performs the best in EBC, and a Cubic Kernel performs the best in JWS and JDP."
"The experiments using a Cubic Kernel are suitable to evaluate the effectiveness of the basket mining approach applied in the PKE, since a Cubic Kernel projects the original feature space F into F 3 space, which is too large to be handled only using a naive exhaustive method."
All experiments were conducted under Linux us-ing XEON 2.4 Ghz dual processors and 3.5 Gbyte of main memory.
All systems are implemented in C++.
Text Chunking is a fundamental task in NLP – divid-ing sentences into non-overlapping phrases.
BaseNP chunking deals with a part of this task and recog-nizes the chunks that form noun phrases.
Here is an example sentence: [He] reckons [the current account deficit] will narrow to [only $ 1.8 billion] .
A BaseNP chunk is represented as sequence of words between square brackets.
"BaseNP chunking task is usually formulated as a simple tagging task, where we represent chunks with three types of tags: B: beginning of a chunk."
I: non-initial word.
O: outside of the chunk.
"In our experiments, we used the same settings[REF_CITE]."
We use a standard data set[REF_CITE]consisting of sections 15-19 of the WSJ cor-pus as training and section 20 as testing.
"Since there are no explicit spaces between words in Japanese sentences, we must first identify the word boundaries before analyzing deep structure of a sen-tence."
Japanese word segmentation is formalized as a simple classification task.
"Let s = c 1 c 2 ···c m be a sequence of Japanese characters, t = t 1 t 2 · · · t m be a sequence of Japanese character types 3 associated with each character, and y i ∈ {+1, −1}, (i = (1, 2, . . . , m−1)) be a boundary marker."
"If there is a boundary between c i and c i+1 , y i = 1, otherwise y i = −1."
"The feature set of example x i is given by all characters as well as character types in some constant window (e.g., 5): {c i−2 , c i−1 , · · · , c i+2 , c i+3 , t i−2 , t i−1 , · · · , t i+2 , t i+3 }."
Note that we distinguish the relative position of each character and character type.
"We use the Kyoto University Corpus[REF_CITE], 7,958 sentences in the articles on January 1st to January 7th are used as training data, and 1,246 sentences in the articles on January 9th are used as the test data."
The task of Japanese dependency parsing is to iden-tify a correct dependency of each Bunsetsu (base phrase in Japanese).
"In previous research, we pre-sented a state-of-the-art SVMs-based Japanese de-pendency parser[REF_CITE]."
"We combined SVMs into an efficient parsing algorithm, Cascaded Chunking Model, which parses a sentence deterministically only by deciding whether the cur-rent chunk modifies the chunk on its immediate right hand side."
"The input for this algorithm consists of a set of the linguistic features related to the head and modifier (e.g., word, part-of-speech, and inflec-tions), and the output from the algorithm is either of the value +1 (dependent) or -1 (independent)."
"We use a standard data set, which is the same corpus de-scribed in the Japanese Word Segmentation."
"Tables 2, 3 and 4 show the execution time, accu-racy 4 , and |Ω| (size of extracted subsets), by chang-ing σ from 0.01 to 0.0005."
The PKI leads to about 2 to 12 times improve-ments over the PKB.
"In JDP, the improvement is sig-nificant."
"This is because B, the average of h(i) over all items i ∈ F, is relatively small in JDP."
The im-provement significantly depends on the sparsity of the given support examples.
The improvements of the PKE are more signifi-cant than the PKI.
"The running time of the[REF_CITE]to 300 times faster than the PKB, when we set an appropriate σ, (e.g., σ = 0.005 for EBC and JWS, σ = 0.0005 for JDP)."
"In these settings, we could preserve the final accuracies for test data."
"The PKE with a Cubic Kernel tends to make Ω large (e.g., |Ω| = 2.32 million for JWS, |Ω| = 8.26 mil-lion for JDP)."
"To reduce the size of Ω, we examined sim-ple frequency-based pruning experiments."
"Our ex-tension is to simply give a prior threshold ξ(= 1, 2, [Footnote_3], [Footnote_4] . . .), and erase all subsets which occur in less than ξ support examples."
"3 Usually, in Japanese, word boundaries are highly con-strained by character types, such as hiragana and katakana (both are phonetic characters in Japanese), Chinese characters, English alphabets and numbers."
"4 In EBC, accuracy is evaluated using F measure, harmonic mean between precision and recall."
The calculation of fre-quency can be similarly conducted by the PrefixS-pan algorithm.
"Tables 5 and 6 show the results of frequency-based pruning, when we fix σ=0.005 for JWS, and σ=0.0005 for JDP."
"In JDP, we can make the size of set Ω about one third of the original size."
This reduction gives us not only a slight speed increase but an improvement of accuracy (89.29%→89.34%).
Frequency-based pruning allows us to remove subsets that have large weight and small frequency.
"Such subsets may be generated from errors or special outliers in the train-ing examples, which sometimes cause an overfitting in training."
"In JWS, the frequency-based pruning does not work well."
"Although we can reduce the size of Ω by half, the accuracy is also reduced (97.94%→97.83%)."
"It implies that, in JWS, features even with frequency of one contribute to the final de-cision hyperplane."
There have been several studies for efficient classi-fication of SVMs.
Isozaki et al. propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast[REF_CITE].
XQK can be subsumed into PKE.
Both XQK and PKE share the basic idea; all feature combinations are explicitly expanded and we convert the kernel-based classifier into a simple lin-ear classifier.
The explicit difference between XQK and PKE is that XQK is designed only for Quadratic Kernel.
It implies that XQK can only deal with feature com-bination of size up to two.
"On the other hand, PKE is more general and can also be applied not only to the Quadratic Kernel but also to the general-style of polynomial kernels (1 + |X ∩ Y |) d ."
"In PKE, there are no theoretical constrains to limit the size of com-binations."
"In addition, Isozaki et al. did not mention how to expand the feature combinations."
"They seem to use a naive exhaustive method to expand them, which is not always scalable and efficient for extracting three or more feature combinations."
PKE takes a basket mining approach to enumerating effective feature combinations more efficiently than their exhaustive method.
"We focused on a Polynomial Kernel of degree d, which has been widely applied in many tasks in NLP and can attain feature combination that is crucial to improving the performance of tasks in NLP."
"Then, we introduced two fast classification algorithms for this kernel."
"One is PKI (Polynomial Kernel In-verted), which is an extension of Inverted Index."
"The other is PKE (Polynomial Kernel Expanded), where all feature combinations are explicitly expanded."
"The concept in PKE can also be applicable to ker-nels for discrete data structures, such as String Ker-nel[REF_CITE]and Tree Kernel[REF_CITE]."
"For instance, Tree Kernel gives a dot product of an ordered-tree, and maps the original ordered-tree onto its all sub-tree space."
"To apply the PKE, we must efficiently enumerate the effective sub-trees from a set of support examples."
We can similarly apply a sub-tree mining algorithm[REF_CITE]to this problem.
Appendix A.: Lemma 1 and its proof
X d µ d ¶³X r µ ¶´ c d (r) = (−1) r−m · m l r l m . l=r m=0
"Let X, Y be subsets of F = {1, 2, . . . , N}."
"In this case, |X ∩ Y | is same as the dot product of vector x, y, where x = {x 1 , x 2 , . .. , x N }, y = {y 1 , y 2 , . . . , y N } (x j , y j ∈ {0, 1}) x j = 1 if j ∈ X, x j = 0 otherwise. (1 + |X ∩ Y |) d = (1 + x · y) d can be expanded as follows"
X d µ d ¶³X N ´ l (1 + x · y) d = x j y j l l=0 j=1 X d µ d ¶ = l · τ(l) l=0 where k 1 +.
X ..+k N =l l! τ(l) = k 1 . . . (x N y N ) k N . k 1 ! . .. k N ! (x 1 y 1 ) k n ≥0
"Note that x jk j is binary (i.e.,x kj j ∈ {0,1}), the num-ber of r-size subsets can be given by a coefficient of (x 1 y 1 x 2 y 2 . . . x r y r )."
"X d µ d ¶µ k +...+k =l ¶ 1 X r l! c d (r) = l k 1 ! . . . k r ! l=r k n ≥1,n=1,2,...,r"
X d µ d ¶µ µ ¶ µ ¶ ¶ = r l − r (r−1) l + r (r−2) l − . . . l 1 2 l=r X d µ d ¶³X r µ ¶´ (−1) r−m · m l r m . 2 = l l=r m=0
This paper proposes the “Hierarchical Di-rected Acyclic Graph (HDAG) Kernel” for structured natural language data.
"The HDAG Kernel directly accepts several lev-els of both chunks and their relations, and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs."
We applied the proposed method to question classifica-tion and sentence alignment tasks to eval-uate its performance as a similarity mea-sure and a kernel function.
The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods.
"As it has become easy to get structured corpora such as annotated texts, many researchers have applied statistical and machine learning techniques to NLP tasks, thus the accuracies of basic NLP tools, such as POS taggers, NP chunkers, named entities tag-gers and dependency analyzers, have been improved to the point that they can realize practical applica-tions in NLP."
The motivation of this paper is to identify and use richer information within texts that will improve the performance of NLP applications; this is in con-trast to using feature vectors constructed by a bag-of-words[REF_CITE].
We now are focusing on the methods that use nu-merical feature vectors to represent the features of natural language data.
"In this case, since the orig-inal natural language data is symbolic, researchers convert the symbolic data into numeric data."
"This process, feature extraction, is ad-hoc in nature and differs with each NLP task; there has been no neat formulation for generating feature vectors from the semantic and grammatical structures inside texts."
Kernel methods[REF_CITE]suitable for NLP have recently been devised.
"Convolution Kernels[REF_CITE]demonstrate how to build kernels over discrete struc-tures such as strings, trees, and graphs."
One of the most remarkable properties of this kernel method-ology is that it retains the original representation of objects and algorithms manipulate the objects simply by computing kernel functions from the in-ner products between pairs of objects.
"This means that we do not have to map texts to the feature vectors by explicitly representing them, as long as an efficient calculation for the inner products be-tween a pair of texts is defined."
"The kernel method is widely adopted in Machine Learning methods, such as the Support Vector Machine (SVM)[REF_CITE]."
"In addition, kernel function  has been described as a similarity function that satisfies certain properties[REF_CITE]."
"The similarity measure between texts is one of the most important factors for some tasks in the application areas of NLP such as Machine Trans-lation, Text Categorization, Information Retrieval, and Question Answering."
This paper proposes the Hierarchical Directed Acyclic Graph (HDAG) Kernel.
It can handle sev-eral of the structures found within texts and can cal- culate the similarity with regard to these structures at practical cost and time.
"The HDAG Kernel can be widely applied to learning, clustering and similarity measures in NLP tasks."
The following sections define the HDAG Kernel and introduce an algorithm that implements it.
The results of applying the HDAG Kernel to the tasks of question classification and sentence alignment are then discussed.
Convolution Kernels were proposed as a concept of kernels for a discrete structure.
This framework de-fines a kernel function between input objects by ap-plying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. be a positive integer and  
"Let be nonempty, separable metric spaces."
This paper focuses on the special case that  countable sets.
"We start with  as  are a composite structure and    as its “parts”, where ! ( (&quot;( #&amp; $)&amp;. % is defined as a relation on the set &amp; such that *% +,  is true if are the “parts” of . %.- / is defined as 0-% /, % ,  &lt; Suppose  :; , and = bebe thethe partsparts ofof , with with &gt;?  ."
"Then, the similarity @/= be-tween and is defined as the following general-ized convolution: $BDCFEHGJILK  W [Z W _] a`\^ A ] BDC ] EbG ] I7c A (1)"
"We note that Convolution Kernels are abstract con-cepts, and that instances of them are determined by the definition of sub-kernel #/ JY ."
"The Tree Kernel[REF_CITE]and String Subse-quence Kernel (SSK)[REF_CITE], developed in the NLP field, are examples of Convolution Ker-nels instances."
An explicit definition of both the Tree Kernel and SSK @/ is written as:
A $;q ^pa` h q BDCoIokrh q BDG9I7c (2)
"Conceptually, we enumerate all sub-structures oc- and , where s curring in represents the to-tal number of possible sub-structures in the ob-jects. t , the feature mapping from the sample space to the feature space, is given by &gt; t /"
"In the case of the Tree Kernel, and be trees."
The Tree Kernel computes the number of common subtrees in two trees and . tdw_ is defined as the number of occurrences of the x ’th enumerated subtree in tree .
"In the case of SSK, input objects and are string sequences, and the kernel function computes the sum of the occurrences of x ’th common subse-quence t w / weighted according to the length of the subsequence."
"These two kernels make polynomial-time calculations, based on efficient recursive cal-culation, possible, see equation (1)."
Our proposed method uses the framework of Convolution Kernels.
This paper defines HDAG as a Directed Acyclic Graph (DAG) with hierarchical structures.
"That is, certain nodes contain DAGs within themselves."
"In basic NLP tasks, chunking and parsing are used to analyze the text semantically or grammatically."
"There are several levels of chunks, such as phrases, named entities and sentences, and these are bound by relation structures, such as dependency structure, anaphora, and coreference."
"HDAG is designed to enable the representation of all of these structures inside texts, hierarchical structures for chunks and DAG structures for the relations of chunks."
"We be-lieve this richer representation is extremely useful to improve the performance of similarity measure be-tween texts, moreover, learning and clustering tasks in the application areas of NLP."
Figure 1 shows an example of the text structures that can be handled by HDAG.
Figure 2 contains simple examples of HDAG that elucidate the calcu-lation of similarity.
"As shown in Figures 1 and 2, the nodes are al-lowed to have more than zero attributes, because nodes in texts usually have several kinds of at-tributes."
"For example, attributes include words, part-of-speech tags, semantic information such as Word- "
"Net, and class of the named entity."
"First of y{ all z , we define the set of nodes in HDAGs y as | and } , respectively, ~ and  repre- and sent | nodes in the graph that are defined as 2~,~ w  J  | 8 and 2J } #   } 8 , respectively."
We use the expression ~  ~J  ~f to represent the path from ~ to ~  through ~  .
We define “attribute sequence” as a sequence of attributes extracted from nodes included in a sub-path.
The attribute sequence is expressed as ‘A-B’ or ‘A-(C-B)’ where ( ) represents a chunk.
"As a ba-sic example of the z extraction   of attribute sequences from a sub-path,  in Figure 2 contains the and   ."
"Section 3.3 explains in detail the method of z four attribute sequences ‘e-b’, ‘e-V’, ‘N-b’ and ‘N-V’, which are the combinations of all attributes in extracting attribute sequences from sub-paths."
"Next, we define “terminated nodes” z as those that do not contain any graph, such as ~ , l~ ; “non-terminated nodes” are those that do, such as  ,  ."
"Since HDAGs treat not only exact matching of sub-structures but also approximate matching, we allow node skips according to decay factor !  $J when extracting attribute sequences from the  sub-paths."
"This framework makes similarity evalua-tion robust; the similar sub-structures can be eval-uated in the value of similarity, in contrast to ex-act matching that never evaluate the similar sub-structure."
"Next, we define parameter  (   ) as the number of attributes combined in the attribute sequence."
"When calculating similarity, we consider only combination lengths of up to  ."
"Given the above discussion y , t the y feature _@ vector y of HDAG is written as   , where t represents the explicit feature mapping of HDAG and s represents the number of all possible attribute combinations."
The value of t  is the in HDAG ; each attribute sequence is weighted ac-number of y occurrences of the x ’th attribute sequence cording to the node skip.
"The similarity between HDAGs, which is the definition of the HDAG Ker-nel, follows y equation y{z (2) where input objects andare and , respectively."
"According to this ap-proach, the HDAG Kernel calculates the inner prod-uct of the common attribute sequences weighted ac-cording to their node skips y and y the z occurrence be-tween the two HDAGs, and ."
"We note that, in general, if the dimension of the feature space becomes very high or approaches in-finity, it becomes computationally y infeasible to gen-erate feature vector  explicitly."
"To improve the reader’s understanding of what the HDAG Kernel calculates, before we introduce our efficient calcu-lation method, the next section details the attribute sequences that become elements of the feature vec-tor if the calculation is explicit."
We describe the details of the attribute sequences that are elements y of the y z feature vector of the HDAGand in Figure 2.Kernel using
"First, we determine  ¶  , which returns the sum of the common attribute sequences of the Â -combination of attributes between nodes ~ and  ."
"T  Ú N qHÓ TVÕ7W otherwise Þdßaà /~ returns the number of common attributes (4) of nodes ~ and  , not including the attributes of nodes inside ~ and  ."
We define function +x ~f as re-turning a set of nodes inside a non-terminated node ~ . x+f~  means node z ~~  ~  8 is a terminated z node.
"For example, x+!~ ~ We define functions ã{.À ~ ,and ã¹äÀx+/~ ~ , . and ã äÀ ä /~ to calculate /~ ."
"Function ïFð à ~f returns the set of nodes that have have direct links to ¶ . ïFð à /~!j direct links to node ~ . ïFð à /f~  means 2~ z j8~ no nodes ïFð à ~ , . and Next, we define @~ as representing the sum of the common attribute sequences that are the Â - þ combinations of attributes extracted from the sub-paths whose sinks are ~ and  , respectively."
"Functions /~ , ô äÀ /~ and ô Àä ä ~ , needed for the recursive calculation of À ~ , are , ã äÀ /~ and written äÀ ä /~ inrespectivelythe same form, exceptas ã&apos;À&quot;/~ã for the boundary con-dition of ô À /~ , which is written as: if Í  ó Ä B E « IìK Ã Ä B E «[REF_CITE]"
"Finally, an efficient similarity calculation formula is written as"
"Aõ \öl÷ B » ` E » £ ILK Ä Ó^ ` ÛmNOø Õ_NOù .A ,Ä B E «[REF_CITE]"
"According to equation (13), given the recursive definition of $À./~ , the similarity between two HDAGs can be calculated in /* |e } time . [Footnote_1]"
"1 We can easily rewrite the equation to calculate all combi-nations Biÿ &apos;ÿVÿ Åÿ of [I attributes, but the order of calculation time becomes ."
We will now elucidate an efficient processing algo-rithm.
"First, as a pre-process, the nodes are sorted under the following condition: all nodes that have a path to the focused node and are in the graph in-side the focused node should be set before the fo-cused node."
We can get at least one set of ordered nodes y since we are treating z an HDAG.
"In the case , we can get ûÔ~ , ~J , J~ , ~ , ~  , ~lü , ~!ý ."
"We of can rewrite the recursive calculation formula in “for loops”, if we follow the sorted order."
Figure 3 shows the algorithm of the HDAG kernel.
"Dynamic pro-gramming technique is used to compute the HDAG Kernel very efficiently because when following the sorted order, the values that are needed to calculate the focused pair of nodes are already calculated in the previous calculation."
We can calculate the table by following the order of the nodes from left to right and top to bottom.
We normalize the computed kernels before their use within the algorithms.
The normalization cor-responds to the standard unit norm normalization of examples in the feature space corresponding to the kernel space[REF_CITE].
We evaluated the performance of the proposed method in an actual application of NLP; the data set is written in Japanese.
"We compared HDAG and DAG (the latter had no hierarchy structure) to the String Subsequence Ker-nel (SSK) for word sequence, Dependency Structure"
"Kernel (DSK)[REF_CITE](a special case of the Tree Kernel), and Cosine measure for feature vectors consisting of the occurrence of at-tributes (BOA), and the same as BOA, but only the attributes of noun and unknown word (BOA’)were used."
We expanded SSK and DSK to improve the total performance of the experiments.
We denote them as SSK’ and DSK’ respectively.
The original SSK treats only exact  string combinations based on pa-rameter  .
We consider string combinations of up to for SSK’.
The original DSK was specifically con-structed for parse tree use.
We expanded it to be able to treat the  combinations of nodes and the free or-der of child node matching.
"Figure 4 shows some input objects for each eval-uated kernel, (a) for HDAG, (b) for DAG and DSK’, and (c) for SSK’."
"Note, though DAG and DSK’ treat the same input objects, their kernel calculation methods differ as do the return values."
"We used the words and semantic information of “Goi-taikei”[REF_CITE], which is similar to WordNet in English, as the attributes of the node."
"The chunks and their relations in the texts were an-alyzed by cabocha[REF_CITE], and named entities were analyzed by the method[REF_CITE]."
We tested each  -combination case with changing parameter  from 0.1 through 0.9 in the step of 0.1.
Only the best performance achieved under parame-ter  is shown in each case.
The experiments in this paper were designed to eval-uated how the similarity measure reflects the seman-tic information of texts.
"In the task of Question Clas-sification, a given question is classified into Ques- tion Type, which reflects the intention of the ques-tion."
The Sentence Alignment task evaluates which sentence is the most semantically similar to a given sentence.
The HDAG Kernel showed the best performance in the experiments as a similarity measure and as a kernel of the learning algorithm.
This proves the usefulness of the HDAG Kernel in determining the similarity measure of texts and in providing an SVM kernel for resolving classification problems in NLP tasks.
"These results indicate that our approach, in-corporating richer structures within texts, is well suited to the tasks that require evaluation of the se-mantical similarity between texts."
"The potential use of the HDAG Kernel is very wider in NLP tasks, and we believe it will be adopted in other practical NLP applications such as Text Categorization and Ques-tion Answering."
Our experiments indicate that the optimal param-eters of combination number  and decay factor  depend the task at hand.
They can be determined by experiments.
"The original DSK requires exact matching of the tree structure, even when expanded (DSK’) for flex-ible matching."
This is why DSK’ showed the worst performance.
"Moreover, in Sentence Alignment task, paraphrasing or different expressions with the same meaning is common, and the structures of the parse tree widely differ in general."
"Unlike DSK’, SSK’ and HDAG Kernel offer approximate match-ing which produces better performance."
"The structure of HDAG approaches that of DAG, if we do not consider the hierarchical structure."
"In addition, the structure of sequences (strings) is en-tirely included in that of DAG."
"Thus, the framework of the HDAG Kernel covers DAG Kernel and SSK."
"This paper proposed the HDAG Kernel, which can reflect the richer information present within texts."
Our proposed method is a very generalized frame-work for handling the structure inside a text.
We evaluated the performance of the HDAG Ker-nel both as a similarity measure and as a kernel func-tion.
"Our experiments showed that HDAG Kernel offers better performance than SSK, DSK, and the baseline method of the Cosine measure for feature vectors, because HDAG Kernel better utilizes the richer structure present within texts."
Recent text and speech processing applications such as speech mining raise new and more general problems re-lated to the construction of language models.
We present and describe in detail several new and efficient algorithms to address these more general problems and report ex-perimental results demonstrating their usefulness.
"We give an algorithm for computing efficiently the expected counts of any sequence in a word lattice output by a speech recognizer or any arbitrary weighted automaton; describe a new technique for creating exact representa-tions of -gram language models by weighted automata whose size is practical for offline use even for a vocab-ulary size of about 500,000 words and an -gram order  ; and present a simple and more general technique for constructing class-based language models that allows each class to represent an arbitrary weighted automaton."
"An efficient implementation of our algorithms and tech-niques has been incorporated in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities."
"Statistical language models are crucial components of many modern natural language processing systems such as speech recognition, information extraction, machine translation, or document classification."
"In all cases, a language model is used in combination with other in-formation sources to rank alternative hypotheses by as-signing them some probabilities."
There are classical techniques for constructing language models such as -gram models with various smoothing techniques (see[REF_CITE]and the references therein for a survey and comparison of these techniques).
"In some recent text and speech processing applications, several new and more general problems arise that are re-lated to the construction of language models."
We present new and efficient algorithms to address these more gen-eral problems.
Classical language models are constructed by deriving statistics from large input texts.
"In speech mining applications or for adaptation purposes, one often needs to construct a language model based on the out-put of a speech recognition system."
"But, the output of a recognition system is not just text."
"Indeed, the word er- ror rate of conversational speech recognition systems is still too high in many tasks to rely only on the one-best output of the recognizer."
"Thus, the word lattice output by speech recognition systems is used instead because it contains the correct transcription in most cases."
A word lattice is a weighted finite automaton (WFA) output by the recognizer for a particular utterance.
It contains typically a very large set of alternative transcrip-tion sentences for that utterance with the corresponding weights or probabilities.
A necessary step for construct-ing a language model based on a word lattice is to derive the statistics for any given sequence from the lattices or WFAs output by the recognizer.
This cannot be done by simply enumerating each path of the lattice and counting the number of occurrences of the sequence considered in each path since the number of paths of even a small au-tomaton may be more than four billion.
We present a simple and efficient algorithm for computing the expected count of any given sequence in a WFA and report experi-mental results demonstrating its efficiency.
Representation of language models by WFAs.
Clas-sical -gram language models admit a natural representa-tion by WFAs in which each state encodes a left context of width less than .
"However, the size of that represen-tation makes it impractical for offline optimizations such as those used in large-vocabulary speech recognition or general information extraction systems."
Most offline rep-resentations of these models are based instead on an ap-proximation to limit their size.
"We describe a new tech-nique for creating an exact representation of -gram lan-guage models by WFAs whose size is practical for offline words and for  . use even in tasks with a vocabulary size of about 500,000"
"In many applications, it is nat-ural and convenient to construct class-based language models, that is models based on classes of words[REF_CITE]."
Such models are also often more robust since they may include words that belong to a class but that were not found in the corpus.
Classical class-based models are based on simple classes such as a list of words.
But new clustering algorithms allow one to create more general and more complex classes that may be reg-ular languages.
Very large and complex classes can also be defined using regular expressions.
We present a simple and more general approach to class-based language mod-els based on general weighted context-dependent rules[REF_CITE].
Our approach allows us to deal efficiently with more complex classes such as weighted regular languages.
"We have fully implemented the algorithms just men-tioned and incorporated them in a general software li-brary for language modeling, the GRM Library, that in-cludes many other text and grammar processing function-alities[REF_CITE]."
"In the following, we will present in detail these algorithms and briefly describe the corresponding GRM utilities."
Definition1 A system is a semiring[REF_CITE]if: tive monoid with identity element ;  is  aiscommuta-a monoid with identity element ; distributes over ; and is an annihilator for : for all  ! &quot; #$.
"Thus, a semiring is a ring that may lack negation."
"Two semirings often used in speech processing are: the log semiring &amp;%  (*),.+ -0/1 2 3 45[REF_CITE]semiring 879( 5:; &lt;= via a &gt;@1A? morphism with, for which is isomorphic to the familiar real or probability all  E( )"
"F.+ -0/ : ; 2 3G4 &gt;@1AJ? LKNMPOQ,KTMPOQ and ID@&gt; ? the  convention, and thethattropical: KTMPOQUI -# Z[( \7 ) and .-01/"
G]!^@_` which can besemiringderived from the log + semiring using the Viterbi approximation.
Definition 2 A weighted finite-state ed  transducer !
GiHa jk lQ over a semiring where: d isistheanfinite8-tupleinput alphabet of the transducer; f is the finite output alphabet; g is a finite set of states; h0nog the set of initial states; ipnqg the set of final states; jrn0gsD: ) u+ =/ y: fz)uwvN+ ./ {:k\Vg: a finite set of transitions; |&lt;}~ the initial weight function; and | } the final weight function mapping i to .
A Weighted automaton &apos;ed !
GiH jk lQ is de-fined in a similar way by simply omitting the output la-bels.
We denote by    the set of strings accepted by an automaton  and similarly by  the strings de-scribed by a regular expression  .
"Given a transition  , we denote by  = its input label, x =  its origin or previous state and  = its desti-nation state or next state, &quot; N its weight, P =  its output label (transducer case)."
"Given a state  , we denote by jV  the set of transitions leaving  ."
"A path =NU. is an element of jk with con-secutive transitions:    wZ , =NN=  ."
We extend and  to paths by setting:      and ¡   .  .
A cycle  is a path whose origin and destination states coincide:   ;   .
We denote by £
"Z¤£ . the set of paths from  to ¤¥ and by £ Z¤U§¡ and Z¤U§¡U©  the set of paths from  to  with in-put label &amp;§ &amp;d and output label © (transducer case). can be extended £ to subsets $ª ªk¥¡n«g , These £ definitions G§Q ) Z¤U§¡G. by: ing functions  (and similarly  ) and the weight."
"The label-func-tion  can also be extended to paths by defining the la-bel of a path as the concatenation of the labels of its constituent transitions, and the weight of a path as the    -product    of  the  weights   , &quot;   of its  constituent   = transitions &quot;    ."
We: also &quot; ;¶  extend  °º &quot;    to any finite set of paths ¶ by setting:  .
"The output weight associated by to each input string  is:  St [ ¹ °5¼{» @½ =¾ ² ¿5²  lQÂ¡    &lt;     St is defined to be when £ U§Q  larly, the output weight associated by a transducer a to a ."
"Simi-pair of input-output string L§¡U ©&lt; is:  ;a ÅL§QG©&lt; ¹ °5¼8½@» ¾=² ¿² Æw² ÀQÁ l¡Çx  ,&quot;   ,mB   £  aSÅL§¡U©&lt; when U§¡U©  ."
A successful path in a weighted automaton or transducer È is a path from an initial state to a final state.
È is unambiguous if for any string  there is at most one successful path labeled with § .
"Thus, an unambiguous transducer defines a function."
"For any transducer a , denote by ¶!É1Za the automaton obtained by projecting a on its output, that is by omitting its input labels."
Note that the second operation of the tropical semiring identical.
Thus the weight of a path in an automaton  and the log semiring as well as their identity elements are over the tropical semiring does not change if  is viewed as a weighted automaton over the log semiring or vice-versa.
This section describes a counting algorithm based  on general gkGiH weighted .Ê
GË  automatabe an arbitraryalgorithmsweighted.
Letautoma-ton over the probability semiring and let  be a regular expression defined over the alphabet d .
We are interested in counting the occurrences of the sequences §##  in  while taking into account the weight of the paths where they appear.
"When  is deterministic and pushed, or £ stochastic, it canbe viewed as a probability distribution over all strings at state are all equal to . d; . 1 The £ weight  St associated by  to each string § is then  ."
"Thus, we define the count of the sequence § in  , Í L§ , as:"
"Í [Ï Î °5ÐBÑ Ò Ó{Ò ¿  SÅL§ where ÓÒ {Ó Ò ¿ denotes the number of occurrences of § in the string , i.e., the expected number of occurrences of § given  ."
"More generally, we will define the count of § as above regardless of whether  is stochastic or not."
"In most speech processing applications,  may be an acyclic automaton called a phone or a word lattice out-general and does not assume  to be acyclic."
We describe our algorithm for computing the expected counts of the sequences   and give the proof of its correctness.
Let Ô be the formal power series[REF_CITE]
"Ô over the probability semiring defined by Ô Õ ¬:: Õ , where §y  ."
An efficient implementation of the counting algorithm was incorporated in the GRM library[REF_CITE].
"The GRM utility grmcount can be used in par-ticular to generate a compact representation of the ex-pected counts of the -gram sequences appearing in a word lattice (of which a string encoded as an automaton is a special case), whose order is less or equal to a given integer."
"As an example, the following command line: grmcount -n3 foo.fsm &gt; count.fsm creates an encoded representation count.fsm of the -gram sequences,  , which can be used to construct a trigram model."
The encoded representation itself is also given as an automaton that we do not describe here.
The counting utility of the GRM library is used in a va-riety of language modeling and training adaptation tasks.
Our experiments show that grmcount is quite efficient.
"We tested this utility with 41,000 weighted automata out-puts of our speech recognition system for the same num-ber of speech utterances."
The total number of transitions of these automata was =æJ æ
"M. It took about 1h52m, in-of all -gram,  , appearing in all these automata cluding I/O, to compute the accumulated expected counts on a single processor of a 1GHz Intel Pentium processor Linux cluster with 2GB of memory and 256 KB cache  ."
"The time to compute these counts represents just Ué ê th of the total duration of the 41,000 speech utterances used in our experiment."
"Standard smoothed -gram models, including back[REF_CITE]and interpolated[REF_CITE]models, admit a natural representation by WFAs in which each state encodes a conditioning history of length less than ."
The size of that representation is often pro- .
"Indeed, the corresponding Òì automaton may have hibitive Ò d Ò ì  Òd states and transitions."
"Thus, even if the vo-cabulary size is just 1,000, the representation of a classi-cal trigram model may require in the worst case up to one billion transitions."
"Clearly, this representation is even less adequate for realistic natural language processing appli-cations where the vocabulary size is in the order of several hundred thousand words."
"In the past, two methods have been used to deal with this problem."
One consists of expanding that WFA on-demand.
"Thus, in some speech recognition systems, the states and transitions of the language model automaton are constructed as needed based on the particular input speech utterances."
The disadvantage of that method is that it cannot benefit from offline optimization techniques that can substantially improve the efficiency of a rec-ognizer[REF_CITE].
A similar drawback af-fects other systems where several information sources are combined such as a complex information extraction sys-tem.
An alternative method commonly used in many ap-plications consists of constructing instead an approxima-tion of that weighted automaton whose size is practical for offline optimizations.
This method is used in many large-vocabulary speech recognition systems.
"In this section, we present a new method for creat-ing an exact representation of -gram language models with WFAs whose size is practical even for very large-vocabulary tasks and for relatively high -gram orders."
"Thus, our representation does not suffer from the disad-vantages just pointed out for the two classical methods."
We first briefly present the classical definitions of -gram language models and several smoothing techniques commonly used.
We then describe a natural representa-tion of -gram language models using failure transitions.
This is equivalent to the on-demand construction referred to above but it helps us introduce both the approximate solution commonly used and our solution for an exact of-fline representation.
 ê =NRS is given as the product of conditional proba-
"In an -gram model, the joint probability of a string bilities: íàî Z ê N=R   ï  íî L "
"Òð  ð @ Ù ê (1) where the conditioning history  consists of zero or more words immediately preceding   and is dictated ð by the order of Í the ð  -gram model. ñ denote the count of -gram  and let given , estimated from counts. íî Let L Ò ðð be the maximum likelihood ñíî probability of  is often adjusted to reserve some probability íî mass Ò ð for unseen -gram se-quences."
Denote by ò Z the adjusted conditional probability.
Katz or absolute íî discounting both lead to an adjustedprobability ð ò . ð
"For all -grams ð « ¥ where ð    ð , we refer to ¥ as the backoff -gram of for."
Conditionalsome  probabilities in a backoff model are of the form: ürý  õTö¨ô`÷ õTø ùPö¨÷ ú ø ù ¨ú þ ÿ  öLõ ÷úþ   ø ùPúcû (2) where  is a factor that ensures a normalized model.
Conditional probabilities in a deleted interpolation model are of the form:   ö   `ô õNö¨ú ô`õTø ù ö¨÷­ú ø ù¤ú   ù ­ú#þ ÿ  õ þ !&quot; (3) where  is the mixing parameter between zero and one.
"In practice, as mentioned before, for numerical sta-bility, ID&gt;¦5A? probabilities are used."
"Furthermore, due ing applications, the weight associated to a string § by a the Viterbi approximation used in most speech process-mum weight of a path labeled with § ."
"Thus, an -gram weighted automaton representing the model is the mini-language model is represented by a WFA over the tropical semiring."
Both backoff and interpolated models can be naturally represented using default or failure transitions.
A fail-ure transition is labeled with a distinct symbol # .
It is the default transition taken at state  when  does not admit an outgoing transition labeled with the word considered.
"Thus, failure transitions have the semantics of otherwise."
The set of states of the WFA representing a backoff or interpolated model is defined by associating a state  $ to each sequence of length less than found in the corpus: + | Òðx &amp; Ò % (  &apos; _ *)
"Í ð , * + &lt;/"
Its transition set j is defined as the union of the following set of failure transitions: ¤+ Z   .# ¡NID@&gt; ? /     |1  $/ and the following set of regular transitions: 1+ Z  JG9NID¦&gt; 5AB? íî L Ò ð   5|  !
"Í ð  ,  + / where  is defined by: þÇÿ 9; * : ø  &lt;: 4 û ü 877 5 ³ 5 þÇ{ÿ ø÷ ø û 4 = õ   &gt; ­ù 4  (4)"
Figure 2 illustrates this construction for a trigram model.
"Treating v -transitions as regular symbols, this is a deterministic automaton."
"Figure 3 shows a complete Katz backoff bigram model built from counts taken from the following toy corpus and using failure transitions: 



 where s @ denotes the start symbol and /s @ the end ? sym-bol for each sentence."
"Note that the start symbol ? s @ does not label any transition, it encodes the history ? s @ ."
All transitions labeled with the end symbol /s @ lead to the single final state of the automaton.
The common method used for an offline representation of an -gram language model can be easily derived from the ing each # -transition by an v -transition.
"Thus, a transition representation using failure transitions by simply replac-that could only be taken in the absence of any other alter-native in the exact representation can now be taken re-gardless of whether there exists an alternative transition."
Thus the approximate representation may contain paths whose weight does not correspond to the exact probabil-ity of the string labeling that path according to the model.
"Consider ? for example the start state in figure 3, labeled with s @ ."
"In a failure transition model, there exists only one path from the start state to the state labeled , with a cost of 1.108, since the # transition cannot be traversed with an input of ."
"If the # transition is replaced by an v -transition, there is a second path to the state labeled – taking the v -transition to the history-less state, then the transition out of the history-less state."
This path is not part of the probabilistic model – we shall refer to it as an invalid path.
"In this case, there is a problem, because the cost of the invalid path to the state – the sum of the two path."
Hence the WFA with v -transitions gives a lower transition costs (0.672) – is lower than the cost of the true cost (higher probability) to all strings beginning with the symbol ? .
"Note that the invalid path from the state labeled s @ to the state labeled C has a higher cost than the correct path, which is not a problem in the tropical semiring."
This section presents a method for constructing an ex-act offline representation of an -gram language model whose size remains practical for large-vocabulary tasks.
The main idea behind our new construction is to mod-ify the topology of the WFA to remove any path contain-ing v -transitions whose cost is lower than the correct cost associated by the model to the string labeling that path.
"Since, as a result, the low cost path for each string will have the correct cost, this will guarantee the correctness of the representation in the tropical semiring."
"Our construction admits two parts: the detection of the invalid paths of the WFA, and the modification of the topology by splitting states to remove the invalid paths."
"To detect invalid paths, we determine first their initial non- v transitions."
Let j BA denote £¯ the set of v -transitions of the original automaton  .
"Let =NU. $j A ,  (+  , leadingbe the setto stateof all  pathssuch that for all  , =G , ¡    is the destination state of some v -transition."
"Lemma 2 £ For an -gram language model, the Ò £ &amp; Ò number % of paths in ¯ is less than the -gram order: ¯"
"For all `! £ ¯ . there is some ¥  A such, letthat &quot;.¥ ¥ w x w¨ ."
"By definition, definition of v -transitions in the model, Ò ð  9 Ò %  ."
By for all  .
It follows from  the definition ð of regular &amp; ð E ð transitions that  wL[   .
"Hence,  , i.e. w; , and either ( % i) &quot; G £   HG and &quot;    , and &quot;    or (ii) ."
"Then, =+ | £ ¯  P5/ ) . £ + E / , for all  GU E ."
"The history-less state Ò £¯ therefore, by recursion, ¯ has Ò ¯"
"I  no Ò £ incoming Ò Ò non- ð  Ò J v % paths, ."
"We now define transition sets K ¯U¯ ³ (originally empty) : for all states  G and all following =NG this . procedure £ LF , if there exists another path ¥ and transition  ¡¥Ç ,  j ;A such that  =   , ¡ ¡¥@ N , and &quot;   % and &quot;  @ or (ii) there, andexistseither ¥[ ( , i) j"
"A {    ¡ .  and  . ¥Â    and &quot; =   % &quot; Q¥¨w¥Â such,thatthen we add   to the set: KNM Þ w¹ ß M Þ ¹ ³ ß O KNM Þ w¹ ß M Þ ¹ ³ ß ) +.w/ ."
See figure 4 for an illustration of this condition.
"Using this procedure, we can determine P the set: ju + &quot;   | P.¥e RK k¯U¯U³Å/ ."
This set provides the first non- v transition of each invalid path.
"Thus, we can use these transitions to eliminate in- 2 Ò@Ò The Ò cost of the construction of jV P  valid paths."
"Proposition  is É Ò d g for all , where is the n-gram £ order ¯ ."
"For each ,\g and each  Proof."
Ò d Ò ¡ most =.¥ possibleand   states.
"It ¥ , there are at such that for some # A , is trivial to see from the proof of lemma 2 that the maximum length of  Ò is Ò ."
"Hence, the cost of finding É Ò all d Ò¦{Ò g ¥ Ò for a given  is d ."
"For all non-empty P jV P   the total cost is . , we create P a new state  P and for ¤ P  all &lt;   we set ¡ =  I j SA ."
"We create a  transition = , set  = ,  and  P for."
Forallall #j A suchsuchthatthat  =  and we Ò K ¯
M Þ T ß Ò  we set Ò =  P .
For all *j
"A such that   and, Ò K ¯"
"M Þ T ß  + and set  = , ` V we  U create; then afornewall intermediate   P  , if backoff w¥ !"
"RW K ¯ state M Þ T ß , we  U add a transition  X   P 5¥ÇeG&quot; w¥@t  to j ."
Proposition 3 The WFA over the tropical semiring mod-ified following the procedure just outlined is equivalent to the exact online representation with failure transitions.
Assume that there P exists a string Y for which the WFA returns a weight  ZY  less than the correct weight &quot; ZY  that would have been assigned to Y by the exact call an v -transition  within a path   NN   in-online representation with failure transitions.
"We will valid if the next non- v transition  E , \ [ o + , has the la-bel  , and there is a transition  with ¡ = ¡  and  =  ."
"Let  be a path through the WFA such that   ; VY and &quot;   ;  P ZY  , and  has the least number of invalid  P ZY v -transitions  be ofthealllastpathsinvalidlabeled v -transitionwith Y with weight  ."
"Let x¥ be the valid path leaving x ¤¨ suchtakenthat in   path !  @ 7¡ NN    . &quot;  Q¥Â ( ¸ + &quot;   N=G   , otherwise there would P be a path with fewer invalid v -transitions with weight  ZY  ."
Let G be the first state E where paths ¥ and  @7x N=G  intersect.
Then LF £ &quot; G    for some ([ 0 + .
"By definition, ¦7¡x=NG E before any v -transitions are,traversedsince intersectionin  ."
"Thenwillitoccurmust be the case that  ]K ì Þ T C ß M Þ T C ß , requiring the path to be removed from the WFA."
This is a contradiction.
"Note that some of the new intermediate backoff states (  U ) can be fully or partially merged, to reduce the space re-quirements of the model."
"Finding the optimal configu-ration of these states, however, is an NP-hard problem."
"For our experiments, we used a simple greedy approach to sharing structure, which helped reduce space dramati-cally."
"Figure 5 shows our example bigram model, after ap-plication of the algorithm."
"Notice that there  andare  P innowthetwoal-history-less states  U was, whichrequiredcorrespond)."
"The starttostate backs off to gorithm  , which(doesno not include a transition to the state labeled , thus eliminating the invalid path."
"Table 1 gives the sizes of three models in terms of transitions v -transitionandencodingstates,offorthebothmodelthe.failureThe DARPAtransitionNorthand American Business News (NAB) corpus contains 250 million words, with a vocabulary of 463,331 words."
"The Switchboard training corpus has 3.1 million words, and a vocabulary of 45,643."
"The number of transitions needed for the exact offline representation in each case was be-tween 2 and 3 times the number of transitions used in the representation with failure transitions, and the number of states was less than twice the original number of states."
This shows that our technique is practical even for very large tasks.
Efficient implementations of model building algo-rithms have been incorporated into the GRM library.
"The GRM utility grmmake produces basic backoff models, using Katz or Absolute discounting[REF_CITE]methods, in the topology shown in fig- ure 3, with v -transitions in the place of failure tran-sitions."
The utility grmshrink removes transitions from the model according to the shrinking methods[REF_CITE]or[REF_CITE].
The utility grmconvert takes a backoff model produced by grmmake or grmshrink and converts it into an exact model using either failure transitions or the algorithm just described.
It also converts the model to an interpolated model for use in the tropical semiring.
"As an example, the following command line: grmmake -n3 counts.fsm &gt; model.fsm creates a basic Katz backoff trigram model from the counts produced by the command line example in the ear-lier section."
The command: grmshrink -c1 model.fsm &gt; m.s1.fsm shrinks the trigram model using the weighted difference method[REF_CITE]with a threshold of 1.
"Finally, the command: grmconvert -tfail m.s1.fsm &gt; f.s1.fsm outputs the model represented with failure transitions."
Standard class-based or phrase-based language models are based on simple classes often reduced to a short list of words or expressions.
New spoken-dialog applications require the use of more sophisticated classes either de-rived from a series of regular expressions or using general clustering algorithms.
Regular expressions can be used to define classes with an infinite number of elements.
"Such classes can naturally arise, e.g., dates form an infinite set since the year field is unbounded, but they can be eas-ily represented or approximated by a regular expression."
"Also, representing a class by an automaton can be much more compact than specifying them as a list, especially when dealing with classes representing phone numbers or a list of names or addresses."
This section describes a simple and efficient method for constructing class-based language models where each class may represent an arbitrary (weighted) regular lan-
Let Í .
Í ì be a set of guage. that each class Í corresponds to a classesstochasticandweightedassume automaton   defined over the log semiring.
"Thus, the weight   ZtZ associated by  to a string  can £ be Ò in-terpreted as ID&gt;¦?5A of the conditional probability L Í  ."
Each class Í  defines a weighted transduction:   IB} Í 
This can be viewed as a specific obligatory weighted context-dependent rewrite rule where the left and right contexts are not restricted ([REF_CITE]; Mohri to the class Í can be viewed as the application of the fol-and[REF_CITE]).
"Thus, the transduction corresponding lowing obligatory weighted rewrite rule:   } Í  _ v v"
"The direction of application of the rule, left-to-right or right-to-left, can be chosen depending on the task [Footnote_2] ."
2 The simultaneous case is equivalent to the left-to-right one here.
"Thus, these classes can be viewed as a set of batch rewrite rules[REF_CITE]which can be compiled into weighted transducers."
The utilities of the GRM Library can be used to compile such a batch set of rewrite rules efficiently[REF_CITE].
Let a be the weighted transducer obtained by compil-be represented as a finite automaton  .
To apply the rules ing the rules corresponding to the classes.
"The corpus can defining the classes to the input corpus, we just need to compose the automaton  with a and project the result on the output: q X \LÜ[a  X can be made stochastic using a pushing algorithm[REF_CITE]."
"In general, the transducer a may not be unambiguous."
"Thus, the result of the application of the class rules to the corpus may not be a single text but an automaton representing a set of alternative sequences."
"However, this is not an issue since we can use the gen-eral counting algorithm previously described to construct ) ì¦Ù¡ Z  a language model based on a weighted automaton."
"When , the language defined by the classes, is a code, the transducer ` a is unambiguous."
Denote now by X the language model constructed from the new corpus `  X .
"To construct our final class- ` based language ` and projectmodelthe,resultwe simplyon thehaveoutputto composeside: X with a ` \ ` X [Ü a   ducers a¡ and  , the first one to be applied to the corpus A more general approach would be to have two trans-and the second one to the language model."
"In a proba- , 8a  should represent the probability bilistic interpretation £ Í  Ò  distribution £ L ÒÍ  ."
"By using a  and  a É the probability ` , wedistributionare in fact and a É  makingthe assumptions £ Í    that Ò the classes £ Z Ò Í  _ d E Ù¡ L Í are equally ì £ prob- Ò E More generally, the weights of aà and  could be the re-. able and thus that sults of an iterative learning process."
"Note however that we are not limited to this probabilistic interpretation and that our approach can still be used if a[ and  do not represent probability distributions ` , since we can always push  X and normalize ."
We illustrate this construction in the simple case of the following class containing movie titles: % movie + + batman   batman returns &lt;  a /
The compilation of the rewrite rule defined by this class and applied left to right leads to the weighted transducer a given by figure 6.
Our corpus simply consists of the tomaton  given by figure 7.
The corpus  X obtained by sentence “batman returns” and is represented by the au-composing  with a is given by figure 7.
We presented several new and efficient algorithms to deal with more general problems related to the construc-tion of language models found in new language process-ing applications and reported experimental results show-ing their practicality for constructing very large models.
"These algorithms and many others related to the construc-tion of weighted grammars have been fully implemented and incorporated in a general grammar software library, the GRM Library[REF_CITE]."
7cK¢¡*£&quot;¥¤N¦S£*§£&quot;&quot;¯°K®*± ² *° ² ¦S£&quot; ©µ¡&quot;*&quot;¥°N &quot; *&quot; ² ± &quot;&quot;ÂZ¡**£  ²  ² ¬BÃ¥°ÂK¢º *£  £&quot;&quot;&quot;°m©s¢¡&quot;¡ÄÁÅo¡~¦ &quot;K°NÃ¥K¢¡~©s®&quot;¢¦S£&quot; ¦S¶»Ã¡*&quot;£ *Âm¡**£  ² &lt;¹ jQ¦KÃ¥± ´B¦S£*°N­£&quot;)@ÏS±PÐ§Ñ z¤NÃ¡&quot;¨Ç¦KÃ«ÃÒ± &quot;¦K¡&quot;&quot;&quot;&quot;® Q¦KÃ¥± ´B¦S£&quot;¥°NÕ°K¯&apos;£&quot;&quot;* £&quot;°N:&quot;£&quot;¥°NÖ¡*Âm¡**£  ² &quot;«z¤&lt; Ù /z¤NÃ«¡&quot;¨|^&quot;¨B¦S£ ­¾m¿%*° ² ¬B®&quot;°K &quot;£ ¨z &quot;£&quot;¥°N &quot;¯°K® ² ¦KB ©sKÄ Ü Ý 6ÌIKN= ;//F ZINHÞ@; 6 m*&quot;¥°N:«z¯X°K® ² ¦S£&quot;¥°NÖ«¡³m¥£&quot;&quot;]±¾ ©s¢¡&quot;*¡ ?&quot; K®Æ ² K¢Ã¥°K¬ ² ¢c£Ì°K¯
Ã ¦S®&quot;¤K ¡&quot;*  **¢¡©^ ¡  ² K®£&quot; ² &quot; &quot;£  ² &quot;*&quot;¥°N  &quot;° mºz3£&quot;&quot;¢Ã¦S£&quot;¥ &quot;*&quot; ² ]À3¯°K®Á¦
"We present a supervised machine learning algorithm for metonymy resolution, which exploits the similarity between examples of conventional metonymy."
We show that syntactic head-modifier relations are a high precision feature for metonymy recognition but suffer from data sparse-ness.
"We partially overcome this problem by integrating a thesaurus and introduc-ing simpler grammatical features, thereby preserving precision and increasing recall."
Our algorithm generalises over two levels of contextual similarity.
Resulting infer-ences exceed the complexity of inferences undertaken in word sense disambiguation.
We also compare automatic and manual methods for syntactic feature extraction.
"Metonymy is a figure of speech, in which one ex-pression is used to refer to the standard referent of a related one[REF_CITE]."
"In (1), [Footnote_1] “seat 19” refers to the person occupying seat 19."
1 (1) was actually uttered by a flight attendant on a plane.
"The importance of resolving metonymies has been shown for a variety of NLP tasks, e.g., ma-chine translati[REF_CITE], ques-tion answering[REF_CITE]and anaphora reso-luti[REF_CITE]."
"In order to recognise and interpret the metonymy in (1), a large amount of knowledge and contextual inference is necessary (e.g. seats cannot be ques-tioned, people occupy seats, people can be ques-tioned)."
"Metonymic readings are also potentially open-ended[REF_CITE], so that developing a machine learning algorithm based on previous ex-amples does not seem feasible."
"However, it has long been recognised that many metonymic readings are actually quite regular[REF_CITE]. [Footnote_2]"
"2 Due to its regularity, conventional metonymy is also known as regular polysemy[REF_CITE]. We use the term “metonymy” to encompass both conventional and uncon-ventional readings."
"In (2), “Pakistan”, the name of a location, refers to one of its national sports teams. [Footnote_3]"
"3 All following examples are from the British National Cor-pus (BNC,[URL_CITE]"
Similar examples can be regularly found for many other location names (see (3) and (4)).
"In contrast to (1), the regularity of these exam-ples can be exploited by a supervised machine learn-ing algorithm, although this method is not pursued in standard approaches to regular polysemy and metonymy (with the exception of our own previous work[REF_CITE])."
Such an al-gorithm needs to infer from examples like (2) (when labelled as a metonymy) that “England” and “Scot-land” in (3) and (4) are also metonymic.
"In order to draw this inference, two levels of similarity need to be taken into account."
"One concerns the similarity of the words to be recognised as metonymic or literal (Possibly Metonymic Words, PMWs)."
"In the above examples, the PMWs are “Pakistan”, “England” and “Scotland”."
The other level pertains to the similar-ity between the PMW’s contexts (“&lt;subject&gt; (had) won the World Cup” and “&lt;subject&gt; lost in the semi-final”).
"In this paper, we show how a machine learning algorithm can exploit both similarities."
"Our corpus study on the semantic class of lo-cations confirms that regular metonymic patterns, e.g., using a place name for any of its sports teams, cover most metonymies, whereas unconventional metonymies like (1) are very rare (Section 2)."
"Thus, we can recast metonymy resolution as a classifica-tion task operating on semantic classes (Section 3)."
"In Section 4, we restrict the classifier’s features to head-modifier relations involving the PMW."
"In both (2) and (3), the context is reduced to subj-of-win."
"This allows the inference from (2) to (3), as they have the same feature value."
"Although the remain-ing context is discarded, this feature achieves high precision."
"In Section 5, we generalize context simi-larity to draw inferences from (2) or (3) to (4)."
"We exploit both the similarity of the heads in the gram-matical relation (e.g., “win” and “lose”) and that of the grammatical role (e.g. subject)."
Figure 1 illus-trates context reduction and similarity levels.
We evaluate the impact of automatic extraction of head-modifier relations in Section 6.
"Finally, we dis-cuss related work and our contributions."
We summarize[REF_CITE]’s an-notation scheme for location names and present an annotated corpus of occurrences of country names.
"We identify literal, metonymic, and mixed readings."
The literal reading comprises a locative (5) and a political entity interpretation (6). (5) coral coast of Papua New Guinea (6) Britain’s current account deficit
We distinguish the following metonymic patterns (see also[REF_CITE]).
"In a place-for-people pattern, a place stands for any persons/organisations associ-ated with it, e.g., for sports teams in (2), (3), and (4), and for the government in (7). [Footnote_4] (7) a cardinal element in Iran’s strategy when Iranian naval craft [...] bombarded [...]"
"4 As the explicit referent is often underspecified, we intro-duce place-for-people as a supertype category and we evaluate our system on supertype classification in this paper. In the annotation, we further specify the different groups of people referred to, whenever possible[REF_CITE]."
"In a place-for-event pattern, a location name refers to an event that occurred there (e.g., us-ing the word Vietnam for the Vietnam war)."
"In a place-for-product pattern a place stands for a product manufactured there (e.g., the word Bor-deaux referring to the local wine)."
"The category othermet covers unconventional metonymies, as (1), and is only used if none of the other categories fits[REF_CITE]."
"We also found examples where two predicates are involved, each triggering a different reading. (8) they arrived in Nigeria, hitherto a leading critic of the South African regime"
"In (8), both a literal (triggered by “arriving in”) and a place-for-people reading (triggered by “leading critic”) are invoked."
We introduced the cat-egory mixed to deal with these cases.
"Using Gsearch[REF_CITE], we randomly extracted 1000 occurrences of country names from the BNC, allowing any country name and its variants listed in the CIA factbook [URL_CITE] or WordNet[REF_CITE]to occur."
Each country name is surrounded by three sentences of context.
The annotation can be considered reliable[REF_CITE]with 95% agreement and a kappa[REF_CITE]of .88.
"Our corpus for testing and training the algorithm includes only the examples which both annotators could agree on and which were not marked as noise (e.g. homonyms, as “Professor Greenland”), for a total of 925."
Table 1 reports the reading distribution.
The corpus distribution confirms that metonymies that do not follow established metonymic patterns (othermet) are very rare.
"This seems to be the case for other kinds of metonymies, too[REF_CITE]."
We can therefore reformulate metonymy res-olution as a classification task between the literal reading and a fixed set of metonymic patterns that can be identified in advance for particular semantic classes.
"This approach makes the task comparable to classic word sense disambiguation (WSD), which is also concerned with distinguishing between possible word senses/interpretations."
"However, whereas a classic (supervised) WSD algorithm is trained on a set of labelled instances of one particular word and assigns word senses to new test instances of the same word, (supervised) metonymy recognition can be trained on a set of labelled instances of different words of one seman-tic class and assign literal readings and metonymic patterns to new test instances of possibly different words of the same semantic class."
"This class-based approach enables one to, for example, infer the read-ing of (3) from that of (2)."
We use a decision list (DL) classifier.
All features encountered in the training data are ranked in the DL (best evidence first) according to the following log-likelihood ratio[REF_CITE]:
Log P Pr(reading i |feature k ) j6 =i Pr(reading j |feature k )
"We estimated probabilities via maximum likeli-hood, adopting a simple smoothing method[REF_CITE]: 0.1 is added to both the de-nominator and numerator."
"The target readings to be distinguished are literal, place-for-people, place-for-event, place-for-product, othermet and mixed."
"All our algorithms are tested on our an-notated corpus, employing 10-fold cross-validation."
We evaluate accuracy and coverage:
Acc = # correct decisions made # decisions made Cov = # decisions made # test data
We also use a backing-off strategy to the most fre-quent reading (literal) for the cases where no decision can be made.
We report the results as ac-curacy backoff (Acc b ); coverage backoff is always 1.
We are also interested in the algorithm’s perfor-mance in recognising non-literal readings.
"There-fore, we compute precision (P), recall (R), and F-measure (F), where A is the number of non-literal readings correctly identified as non-literal (true pos-itives) and B the number of literal readings that are incorrectly identified as non-literal (false positives):"
P = A/(A + B) A R = #non-literal examples in the test data F = 2PR/(R + P)
The baseline used for comparison is the assign-ment of the most frequent reading literal.
We show that reducing the context to head-modifier relations involving the Possibly Metonymic Word achieves high precision metonymy recognition. [Footnote_6]
"6 In[REF_CITE], we also considered local and topical cooccurrences as contextual features. They con-stantly achieved lower precision than grammatical features."
"We represent each example in our corpus by a sin-gle feature role-of-head, expressing the grammat-ical role of the PMW (limited to (active) subject, passive subject, direct object, modifier in a prenom-inal genitive, other nominal premodifier, dependent in a prepositional phrase) and its lemmatised lexi-cal head within a dependency grammar framework. [Footnote_7] Table 2 shows example values and Table 3 the role distribution in our corpus."
"7 We consider only one link per PMW, although cases like (8) would benefit from including all links the PMW participates in."
We trained and tested our algorithm with this fea-ture (hmr). [Footnote_8] Results for hmr are reported in the first line of Table 5.
"8 The feature values were manually annotated for the follow-ing experiments, adapting the guidelines[REF_CITE]. The effect of automatic feature extraction is described in Section 6."
The reasonably high precision (74.5%) and accuracy (90.2%) indicate that reduc-ing the context to a head-modifier feature does not cause loss of crucial information in most cases.
Low recall is mainly due to low coverage (see Problem 2 below).
We identified two main problems.
"The feature can be too simplistic, so that decisions based on the head-modifier relation can assign the wrong reading in the following cases: • “Bad” heads: Some lexical heads are semanti-cally empty, thus failing to provide strong evi-dence for any reading and lowering both recall and precision."
"Bad predictors are the verbs “to have” and “to be” and some prepositions such as “with”, which can be used with metonymic (talk with Hungary) and literal (border with Hungary) readings."
"This problem is more se-rious for function than for content word heads: precision on the set of subjects and objects is 81.8%, but only 73.3% on PPs. • “Bad” relations: The premod relation suffers from noun-noun compound ambiguity."
"US op- 



 • Other cases: Very rarely neglecting the ing context leads to errors, even for “ lexical heads and relations."
"Inferring metonymy in (4) that “Germany” in “ lost a fifth of its territory” is also e.g., is wrong and lowers precision."
"However, wrong assignments (based on head-modifier relations) do not constitute a major problem as accuracy is very high (90.2%)."
The algorithm is often unable to make any decision that is based on the head-modifier re-lation.
"This is by far the more frequent problem, which we adress in the remainder of the paper."
"The feature role-of-head accounts for the similarity be-tween (2) and (3) only, as classification of a test in-stance with a particular feature value relies on hav-ing seen exactly the same feature value in the train-ing data."
"Therefore, we have not tackled the infer-ence from (2) or (3) to (4)."
"This problem manifests itself in data sparseness and low recall and coverage, as many heads are encountered only once in the cor-pus."
"As hmr’s coverage is only 63.1%, backoff to a literal reading is required in 36.9% of the cases."
In order to draw the more complex inference from (2) or (3) to (4) we need to generalise context sim-ilarity.
"We relax the identity constraint of the orig-inal algorithm (the same role-of-head value of the test instance must be found in the DL), exploiting two similarity levels."
"Firstly, we allow to draw infer-ences over similar values of lexical heads (e.g. from subj-of-win to subj-of-lose), rather than over iden-tical ones only."
"Secondly, we allow to discard the 



"
We regard two feature values r-of-h and r-of-h as similar if h and h 0 are similar.
In order to capture the similarity between h and h 0 we integrate a thesaurus[REF_CITE]in our algorithm’s testing phase.
"In Lin’s thesaurus, similarity between words is determined by their distribution in dependency relations in a newswire corpus."
"For a content word h (e.g., “lose”) of a specific part-of-speech a set of similar words Σ h of the same part-of-speech is given."
The set mem-bers are ranked in decreasing order by a similarity score.
Table 4 reports example entries. [Footnote_9]
"9 In the original thesaurus, each Σ h is subdivided into clus-ters. We do not take these divisions into account."
"Our modified algorithm (relax I) is as follows: 1. train DL with role-of-head as in hmr; for each test in-stance observe the following procedure (r-of-h indicates the feature value of the test instance); 2. if r-of-h is found in the DL, apply the corresponding rule and stop; 2 0 otherwise choose a number n ≥ 1 and set i = 1; (a) extract the i th most similar word h i to h from the thesaurus; (b) if i &gt; n or the similarity score of h i &lt; 0.10, assign no reading and stop; (b’) otherwise: if r-of-h i is found in the DL, apply cor-responding rule and stop; if r-of-h i is not found in the DL, increase i by 1 and go to (a);"
The examples already covered by hmr are clas-sified in exactly the same way by relax I (see Step 2).
"Let us therefore assume we encounter the test instance (4), its feature value subj-of-lose has not been seen in the training data (so that Step 2 fails 0 and Step 2 has to be applied) and subj-of-win is in the DL."
"For all n ≥ 1, relax I will use the rule for subj-of-win to assign a reading to “Scotland” in (4) as “win” is the most similar word to “lose” in the thesaurus (see Table 4)."
In this case (2b’) is only applied once as already the first iteration over the thesaurus finds a word h 1 with r-of-h 1 in the DL.
"The classification of “Turkey” with feature value gen-of-attitude in (9) required 17 iterations to find a word h 17 (“strategy”; see Example (7)) similar to “attitude”, with r-of-h 17 (gen-of-strategy) in the DL. (9) To say that this sums up Turkey’s attitude as a whole would nevertheless be untrue"
"Precision, recall and F-measure for n ∈ {1, ..., 10, 15, 20, 25, 30, 40, 50} are visualised in Figure 2."
"Both precision and recall increase with n. Recall more than doubles from 18.6% in hmr to 41% and precision increases from 74.5% in hmr to 80.2%, yielding an increase in F-measure from 29.8% to 54.2% (n = 50)."
Coverage rises to 78.9% and accuracy backoff to 85.1% (Table 5).
"Whereas the increase in coverage and recall is quite intuitive, the high precision achieved by re-lax I requires further explanation."
Let S be the set of examples that relax I covers.
"It consists of two subsets: S1 is the subset already covered by hmr and its treatment does not change in relax I, yielding the same precision."
S2 is the set of examples that re-lax I covers in addition to hmr.
The examples in S2 consist of cases with highly predictive content word heads as (a) function words are not included in the thesaurus and (b) unpredictive content word heads like “have” or “be” are very frequent and normally already covered by hmr (they are therefore members of S1).
Precision on S2 is very high (84%) and raises the overall precision on the set S.
"Cases that relax I does not cover are mainly due to (a) missing thesaurus entries (e.g., many proper"
Table 5: Results summary for manual annotation.
"For relax I and combination we report best results (50 thesaurus iterations). algorithm Acc Cov Acc b P R F hmr .902 .631 .817 .745 .186 .298 relax I .877 .789 .851 .802 .410 .542 relax II .865 .903 .859 .813 .441 .572 combination .894 .797 .870 .814 .510 .627 baseline .797 1.00 .797 n/a .000 n/a names or alternative spelling), (b) the small num-ber of training instances for some grammatical roles (e.g. dobj), so that even after 50 thesaurus iterations no similar role-of-head value could be found that is covered in the DL, or (c) grammatical roles that are not covered (other in Table 3)."
"Another way of capturing the similarity between (3) and (4), or (7) and (9) is to ignore lexical heads and generalise over the grammatical role (role) of the PMW (with the feature values as in Table 3: subj, subjp, dobj, gen, premod, ppmod)."
"We therefore de-veloped the algorithm relax II. 1. train decision lists: (a) DL1 with role-of-head as in hmr (b) DL2 with role; for each test instance observe the following procedure (r-of-h and r are the feature values of the test instance); 2. if r-of-h is found in the DL1, apply the corresponding rule and stop; 2’ otherwise, if r is found in DL2, apply the corresponding rule."
"Let us assume we encounter the test instance (4), subj-of-lose is not in DL1 (so that Step 2 fails 0 and Step 2 has to be applied) and subj is in DL2."
"The algorithm relax II will assign a place-for-people reading to “Scotland”, as most subjects in our corpus are metonymic (see Table 3)."
"Generalising over the grammatical role outper-forms hmr, achieving 81.3% precision, 44.1% re-call, and 57.2% F-measure (see Table 5)."
"The algo-rithm relax II also yields fewer false negatives than relax I (and therefore higher recall) since all sub-jects not covered in DL1 are assigned a metonymic reading, which is not true for relax I."
There are several ways of combining the algorithms we introduced.
"In our experiments, the most suc-cessful one exploits the facts that relax II performs better than relax I on subjects and that relax I per-forms better on the other roles."
"Therefore the algo-rithm combination uses relax II if the test instance is a subject, and relax I otherwise."
"This yields the best results so far, with 87% accuracy backoff and 62.7% F-measure (Table 5)."
The results obtained by training and testing our clas-sifier with manually annotated grammatical relations are the upper bound of what can be achieved by us-ing these features.
"To evaluate the influence pars-ing has on the results, we used the RASP toolkit[REF_CITE]that includes a pipeline of tokenisation, tagging and state-of-the-art statisti-cal parsing, allowing multiple word tags."
"The toolkit also maps parse trees to representations of gram-matical relations, which we in turn could map in a straightforward way to our role categories."
RASP produces at least partial parses for 96% of our examples.
"However, some of these parses do not assign any role of our roleset to the PMW — only 76.9% of the PMWs are assigned such a role by RASP (in contrast to 90.2% in the manual anno-tation; see Table 3)."
RASP recognises PMW sub-jects with 79% precision and 81% recall.
"For PMW direct objects, precision is 60% and recall 86%. [Footnote_10]"
10 We did not evaluate RASP’s performance on relations that do not involve the PMW.
We reproduced all experiments using the auto-matically extracted relations.
"Although the relative performance of the algorithms remains mostly un-changed, most of the resulting F-measures are more than 10% lower than for hand annotated roles (Ta-ble 6)."
"This is in line with results[REF_CITE], who compare the effect of man-ual and automatic parsing on semantic predicate-argument recognition."
Previous Approaches to Metonymy Recognition.
"Our approach is the first machine learning algorithm to metonymy recognition, building on our previous work[REF_CITE]."
"The current ap-proach expands on it by including a larger number of grammatical relations, thesaurus integration, and an assessment of the influence of parsing."
Best F-measure for manual annotated roles increased from 46.7% to 62.7% on the same dataset.
Most other traditional approaches rely on hand-crafted knowledge bases or lexica and use vi-olations of hand-modelled selectional restrictions (plus sometimes syntactic violations) for metonymy recogniti[REF_CITE]. [Footnote_11]
"11[REF_CITE]and[REF_CITE]en-hance this with anaphoric information.[REF_CITE]propose using frequency information besides syn-tactic/semantic restrictions, but use only a priori sense frequen-cies without contextual features."
"In these approaches, selectional restric-tions (SRs) are not seen as preferences but as ab-solute constraints."
"If and only if such an absolute constraint is violated, a non-literal reading is pro-posed."
"Our system, instead, does not have any a priori knowledge of semantic predicate-argument re-strictions."
"Rather, it refers to previously seen train-ing examples in head-modifier relations and their la-belled senses and computes the likelihood of each sense using this distribution."
This is an advantage as our algorithm also resolved metonymies without SR violations in our experiments.
An empirical compar-ison between our approach[REF_CITE][Footnote_12] and an SRs violation approach showed that our approach performed better.
12 Note that our current approach even outperforms[REF_CITE].
"In contrast to previous approaches[REF_CITE], we use a corpus reliably annotated for metonymy for evaluation, moving the field towards more objective evaluation procedures."
Word Sense Disambiguation.
"We compared our approach to supervised WSD in Section 3, stressing word-to-word vs. class-to-class inference."
This al-lows for a level of abstraction not present in standard supervised WSD.
"We can infer readings for words that have not been seen in the training data before, allow an easy treatment of rare words that undergo regular sense alternations and do not have to anno-tate and train separately for every individual word to treat regular sense distinctions. [Footnote_13]"
"13 Incorporating knowledge about particular PMWs (e.g., as a prior) will probably improve performance, as word idiosyn-cracies — which can still exist even when treating regular sense distinctions — could be accounted for. In addition, knowledge about the individual word is necessary to assign its original se-mantic class."
"By exploiting additional similarity levels and inte-grating a thesaurus we further generalise the kind of inferences we can make and limit the size of anno-tated training data: as our sampling frame contains 553 different names, an annotated data set of 925 samples is quite small."
"These generalisations over context and collocates are also applicable to stan-dard WSD and can supplement those achieved e.g., by subcategorisation frames[REF_CITE]."
Our approach to word similarity to overcome data sparseness is perhaps most similar[REF_CITE].
"However, they mainly focus on the computation of similarity measures from the train-ing data."
We instead use an off-the-shelf resource without adding much computational complexity and achieve a considerable improvement in our results.
"We presented a supervised classification algorithm for metonymy recognition, which exploits the simi-larity between examples of conventional metonymy, operates on semantic classes and thereby enables complex inferences from training to test examples."
We showed that syntactic head-modifier relations are a high precision feature for metonymy recogni-tion.
"However, basing inferences only on the lex-ical heads seen in the training data leads to data sparseness due to the large number of different lex-ical heads encountered in natural language texts."
In order to overcome this problem we have integrated a thesaurus that allows us to draw inferences be- tween examples with similar but not identical lex-ical heads.
We also explored the use of simpler grammatical role features that allow further gener-alisations.
"The results show a substantial increase in precision, recall and F-measure."
"In the future, we will experiment with combining grammatical fea-tures and local/topical cooccurrences."
"The use of semantic classes and lexical head similarity gener-alises over two levels of contextual similarity, which exceeds the complexity of inferences undertaken in standard supervised word sense disambiguation."
The research reported in this paper was supported by ESRC[REF_CITE].
Katja Markert is funded by an Emmy Noether Fel-lowship of the Deutsche Forschungsgemeinschaft (DFG).
We thank three anonymous reviewers for their comments and suggestions.
Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated cor-pus data.
We describe a new approach which involves clustering subcategoriza-tion frame ( SCF ) distributions using the Information Bottleneck and nearest neigh-bour methods.
"In contrast to previous work, we particularly focus on cluster-ing polysemic verbs."
"A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, of-fering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data."
Classifications which aim to capture the close rela-tion between the syntax and semantics of verbs have attracted a considerable research interest in both lin-guistics and computational linguistics (e.g.[REF_CITE]).
"While such classifications may not provide a means for full semantic inferencing, they can capture gen-eralizations over a range of linguistic properties, and can therefore be used as a means of reducing redun-dancy in the lexicon and for filling gaps in lexical knowledge."
"Verb classifications have, in fact, been used to support many natural language processing ( NLP ) tasks, such as language generation, machine transla-ti[REF_CITE], document classificati[REF_CITE], word sense disambiguati[REF_CITE]and subcategorization acquisiti[REF_CITE]."
"One attractive property of these classifications is that they make it possible, to a certain extent, to in-fer the semantics of a verb on the basis of its syn-tactic behaviour."
In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data ([REF_CITE]; Schulte im[REF_CITE]).
"In this paper, we focus on the particular task of classifying subcategorization frame ( SCF ) distri-butions in a semantically motivated manner."
Pre-vious research has demonstrated that clustering can be useful in inferring Levin-style semantic classes[REF_CITE]from both English and Ger-man verb subcategorization information (Brew and Schulte im[REF_CITE];
Schulte im[REF_CITE]; Schulte im[REF_CITE]).
"We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexi-con extracted automatically using the comprehen-sive system[REF_CITE]and (ii) applying a clustering mechanism to this informa-tion."
"We use clustering methods that process raw distributional data directly, avoiding complex pre-processing steps required by many advanced meth-ods (e.g. Brew and Schulte im[REF_CITE])."
"In contrast to earlier work, we give special empha- sis to polysemy."
Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not).
The rel-atively good clustering results obtained suggest that many polysemic verbs do have some predominating sense in corpus data.
"However, this sense can vary across corpora[REF_CITE], and assuming a single sense is inadequate for an important group of medium and high frequency verbs whose distribu-tion of senses in balanced corpus data is flat rather than zipfian[REF_CITE]."
"To allow for sense variation, we introduce a new evaluation scheme against a polysemic gold stan-dard."
This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated SCF data semantically.
We discuss our gold standards and the choice of test verbs in section 2.
Section 3 describes the method for subcategorization acquisition and sec-tion 4 presents the approach to clustering.
Details of the experimental evaluation are supplied in sec-tion 5.
Section 6 concludes with directions for future work.
"Levin’s taxonomy of verbs and their classes[REF_CITE]is the largest syntactic-semantic verb classifi-cation in English, employed widely in evaluation of automatic classifications."
"It provides a classification of 3,024 verbs (4,186 senses) into 48 broad / 192 fine grained classes."
"Although it is quite extensive, it is not exhaustive."
"As it primarily concentrates on verbs taking NP and PP complements and does not provide a comprehensive set of senses for verbs, it is not suitable for evaluation of polysemic classifi-cations."
We employed as a gold standard a substan-tially extended version of Levin’s classification constructed[REF_CITE].
"This incorpo-rates Levin’s classes, 26 additional classes[REF_CITE][Footnote_1] , and 57 new classes for verb types not covered comprehensively by Levin or Dorr. 110 test verbs were chosen from this gold stan-dard, 78 polysemic and 32 monosemous ones."
1 These classes are incorporated in the ’ LCS database’ ([URL_CITE]
Some low frequency verbs were included to investigate the effect of sparse data on clustering performance.
"To ensure that our gold standard covers all (or most) senses of these verbs, we looked into WordNet[REF_CITE]and assigned all the WordNet senses of the verbs to gold standard classes. [Footnote_2]"
"2 As WordNet incorporates particularly fine grained sense distinctions, some senses were found which did not appear in our gold standard. As many of them appeared marginal and/or low in frequency, we did not consider these additional senses in our experiment."
Two versions of the gold standard were created: monosemous and polysemic.
"The monosemous one lists only a single sense for each test verb, that cor-responding to its predominant (most frequent) sense in WordNet."
The polysemic one provides a compre-hensive list of senses for each verb.
The test verbs and their classes are shown in table 1.
"The classes are indicated by number codes from the classifica-tions of Levin, Dorr (the classes starting with 0) and"
Korhonen (the classes starting with A). [Footnote_3] The pre-dominant sense is indicated by bold font.
3 The gold standard assumes Levin’s broad classes (e.g. class 10) instead of possible fine-grained ones (e.g. class 10.1).
We obtain our SCF data using the subcategorization acquisition system[REF_CITE].
"We expect the use of this system to be benefi-cial: it employs a robust statistical parser[REF_CITE]which yields complete though shallow parses, and a comprehensive SCF classifier, which incorporates 163 SCF distinctions, a super-set of those found in the ANLT[REF_CITE]and COMLEX[REF_CITE]dictio-naries."
"The SCF s abstract over specific lexically-governed particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency con-structions, such as particle and dative movement. 78 of these ‘coarse-grained’ SCF s appeared in our data."
"In addition, a set of 160 fine grained frames were employed."
These were obtained by parameter-izing two high frequency SCF s for prepositions: the simple PP and NP + PP frames.
The scope was re-stricted to these two frames to prevent sparse data problems in clustering.
A SCF lexicon was acquired using this system from the British National Corpus ([REF_CITE]BNC ) so that the maximum of 7000 citations were used per test verb.
The lexicon was evaluated against manually analysed corpus data after an empirically defined threshold of 0.025 was set on relative fre-quencies of SCF s to remove noisy SCF s.
The method yielded 71.8% precision and 34.5% recall.
"When we removed the filtering threshold, and evaluated the noisy distribution, F-measure [Footnote_4] dropped from 44.9 to 38.51. [Footnote_5]"
4 F = 2·precision·recallprecision+recall
"5 These figures are not particularly impressive because our evaluation is exceptionally hard. We use 1) highly polysemic test verbs, 2) a high number of SCF s and 3) evaluate against manually analysed data rather than dictionaries (the latter have high precision but low recall)."
"Data clustering is a process which aims to partition a given set into subsets (clusters) of elements that are similar to one another, while ensuring that elements that are not similar are assigned to different clusters."
We use clustering for partitioning a set of verbs.
Our hypothesis is that information about SCF s and their associated frequencies is relevant for identifying se-mantically related verbs.
"Hence, we use SCF s as rel-evance features to guide the clustering process. [Footnote_6]"
6 The relevance of the features to the task is evident when comparing the probability of a randomly chosen pair of verbs verb i and verb j to share the same predominant sense (4.5%) with the probability obtained when verb j is the JS-divergence
"We chose two clustering methods which do not in-volve task-oriented tuning (such as pre-fixed thresh-olds or restricted cluster sizes) and which approach data straightforwardly, in its distributional form: (i) a simple hard method that collects the nearest neigh-bours ( NN ) of each verb (figure 1), and (ii) the In-formation Bottleneck ( IB ), an iterative soft method[REF_CITE]based on information-theoretic grounds."
"The NN method is very simple, but it has some disadvantages."
"It outputs only one clustering config-uration, and therefore does not allow examination of different cluster granularities."
It is also highly sensitive to noise.
Few exceptional neighbourhood relations contradicting the typical trends in the data are enough to cause the formation of a single cluster which encompasses all elements.
Therefore we employed the more sophisticated IB method as well.
"The IB quantifies the rele-vance information of a SCF distribution with re-spect to output clusters, through their mutual infor-mation I(Clusters; SCF s)."
"The relevance informa-tion is maximized, while the compression informa-tion I(Clusters;V erbs) is minimized."
This en-sures optimal compression of data through clusters.
The tradeoff between the two constraints is realized through minimizing the cost term:
"L = I(Clusters; V erbs) − βI(Clusters; SCF s) , where β is a parameter that balances the constraints."
The IB iterative algorithm finds a local minimum of the above cost term.
"It takes three inputs: (i) SCF -verb distributions, (ii) the desired number of clusters K, and (iii) the value of β."
"Starting from a random configuration, the algo-rithm repeatedly calculates, for each cluster K, verb V and SCF S, the following probabilities: (i) the marginal proportion of the cluster p(K); (ii) the probability p(S|K) for a SCF to occur with mem-bers of the cluster; and (iii) the probability p(K|V ) for a verb to be assigned to the cluster."
"These prob-abilities are used, each in its turn, for calculating the other probabilities (figure 2)."
The collection of all p(S|K)’s for a fixed cluster K can be regarded as a probabilistic center (centroid) of that cluster in the SCF space.
The IB method gives an indication of the most informative values of K. [Footnote_7] Intensifying the weight β attached to the relevance information I(Clusters; SCF s) allows us to increase the num-ber K of distinct clusters being produced (while too small β would cause some of the output clusters to be identical to one another).
"7 Most works on clustering ignore this issue and refer to an arbitrarily chosen number of clusters, or to the number of gold standard classes, which cannot be assumed in realistic applica-tions."
"Hence, the relevance in-formation grows with K. Accordingly, we consider as the most informative output configurations those for which the relevance information increases more sharply between K − 1 and K clusters than between K and K + 1."
"When the weight of relevance grows, the assign-ment to clusters is more constrained and p(K|V ) be-comes more similar to hard clustering."
K(V ) = argmax p(K|V )
K denote the most probable cluster of a verb V.
"For K ≥ 30, more than 85% of the verbs have p(K(V )|V ) &gt; 90% which makes the output cluster-ing approximately hard."
"For this reason, we decided to use only K(V ) as output and defer a further ex-ploration of the soft output to future work."
The input data to clustering was obtained from the automatically acquired SCF lexicon for our 110 test verbs (section 2).
The counts were extracted from unfiltered (noisy) SCF distributions in this lexicon. [Footnote_8] The NN algorithm produced 24 clusters on this in-put.
"8 This yielded better results, which might indicate that the unfiltered “noisy” SCF s contain information which is valuable for the task."
"From the IB algorithm, we requested K = 2 to 60 clusters."
The upper limit was chosen so as to slightly exceed the case when the average clus-ter size 110/K = 2.
"We chose for evaluation the IB results for K = 25, 35 and 42."
"For these val-ues, the SCF relevance satisfies our criterion for a notable improvement in cluster quality (section 4)."
The value K =35 is very close to the actual number (34) of predominant senses in the gold standard.
"In this way, the IB yields structural information beyond clustering."
A number of different strategies have been proposed for evaluation of clustering.
"We concentrate here on those which deliver a numerical value which is easy to interpret, and do not introduce biases towards spe-cific numbers of classes or class sizes."
As we cur-rently assign a single sense to each polysemic verb (sec. 5.4) the measures we use are also applicable for evaluation against a polysemous gold standard.
"Our first measure, the adjusted pairwise preci-sion ( APP ), evaluates clusters in terms of verb pairs (Schulte im[REF_CITE]) [Footnote_9] :"
9 Our definition differs by a factor of 2 from that of Schulte im[REF_CITE].
APP = K1 P K num. of correct pairs in k i · |k |−1 . i i=1 num. of pairs in k i |k i |+1
APP is the average proportion of all within-cluster pairs that are correctly co-assigned.
It is multiplied by a factor that increases with cluster size.
This fac-tor compensates for a bias towards small clusters.
"Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size[REF_CITE]."
"We associate with each cluster its most prevalent semantic class, and denote the number of verbs in a cluster K that take its prevalent class by n prevalent (K)."
Verbs that do not take this class are considered as errors.
"Given our task, we are only interested in classes which con-tain two or more verbs."
We therefore disregard those clusters where n prevalent (K) = 1.
This leads us to define modified purity:
P n prevalent (k i ) nprevalent(ki)≥2 m PUR = . number of verbs
The modification we introduce to purity removes the bias towards the trivial configuration comprised of only singletons.
"We first evaluated the clusters against the predom-inant sense, i.e. using the monosemous gold stan-dard."
"The results, shown in Table 2, demonstrate that both clustering methods perform significantly better on the task than our random clustering base-line."
"Both methods show clearly better performance with fine-grained SCF s (with prepositions, + PP ) than with coarse-grained ones (- PP )."
"Surprisingly, the simple NN method performs very similarly to the more sophisticated IB ."
"Being based on pairwise similarities, it shows better per-formance than IB on the pairwise measure."
"The IB is, however, slightly better according to the global measure (2% with K = 42)."
The fact that the NN method performs better than the IB with similar K values ( NN K = 24 vs. IB K = 25) seems to suggest that the JS divergence provides a better model for the predominant class than the compression model of the IB .
"However, it is likely that the IB perfor-mance suffered due to our choice of test data."
"As the method is global, it performs better when the target classes are represented by a high number of verbs."
"In our experiment, many semantic classes were rep-resented by two verbs only (section 2)."
"Nevertheless, the IB method has the clear advan-tage that it allows for more clusters to be produced."
At best it classified half of the verbs correctly ac-cording to their predominant sense (m PUR = 50%).
"Although this leaves room for improvement, the re-sult compares favourably to previously published re-sults [Footnote_10] ."
"10 Due to differences in task definition and experimental setup, a direct comparison with earlier results is impossible. For example,[REF_CITE]report an accuracy of 29% (which implies m PUR ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure."
"We argue, however, that evaluation against a monosemous gold standard reveals only part of the picture."
"In evaluation against the polysemic gold standard, we assume that a verb which is polysemous in our corpus data may appear in a cluster with verbs that share any of its senses."
"In order to evaluate the clus-ters against polysemous data, we assigned each pol-ysemic verb V a single sense: the one it shares with the highest number of verbs in the cluster K(V )."
Table 3 shows the results against polysemic and monosemous gold standards.
The former are notice-ably better than the latter (e.g. IB with K = 42 is 9% better).
"Clearly, allowing for multiple gold standard classes makes it easier to obtain better results with evaluation."
"In order to show that polysemy makes a non-trivial contribution in shaping the clusters, we mea-sured the improvement that can be due to pure chance by creating randomly polysemous gold stan-dards."
"In each iteration, the verbs kept their original predominant senses, but the set of additional senses was taken entirely from another verb - chosen at ran-dom."
"By doing so, we preserved the dominant sense of each verb, the total frequency of all senses and the correlations between the additional senses."
"The results included in table 3 indicate, with 99.5% confidence (3σ and above), that the improve-ment obtained with the polysemous gold standard is not artificial (except in two cases with 95% confi-dence)."
We performed qualitative analysis to further inves-tigate the effect of polysemy on clustering perfor- mance.
"The results in table 4 demonstrate that the more two verbs differ in their senses, the lower their chance of ending up in the same cluster."
From the figures in table 5 we see that the probability of two verbs to appear in the same cluster increases with the number of senses they share.
"Interestingly, it is not only the degree of polysemy which influences the results, but also the type."
"For verb pairs where at least one of the members displays ‘irregular’ poly-semy (i.e. it does not share its full set of senses with any other verb), the probability of co-occurrence in the same cluster is far lower than for verbs which are polysemic in a ‘regular’ manner (Table 5)."
Manual cluster analysis against the polysemic gold standard revealed a yet more comprehensive picture.
Consider the following clusters (the IB out-put with K = 42):
"A1: talk (37), speak (37)"
"A2: look (30, 35), stare (30)"
"A3: focus (31, 45), concentrate (31, 45)"
"A4: add (22, 37, A56)"
We identified a close relation between the clus-tering performance and the following patterns of se-mantic behaviour: 1) Monosemy:
In 8 cases there was a clear indication in the data (when examining SCF s and the selectional preferences on argument heads) that the polysemous verb indeed had its predominant sense in the rele-vant class and that the co-occurrence was not due to noise. 3) Regular Polysemy: Several clusters were pro-duced which represent linguistically plausible inter-sective classes (e.g. A3)[REF_CITE]rather than single classes. 4) Irregular Polysemy: Verbs with irregular pol-ysemy [Footnote_11] were frequently assigned to singleton clus-ters.
"11 Recall our definition of irregular polysemy, section 5.4."
"For example, add (A4) has a ‘combining and attaching’ sense in class 22 which involves NP and PP SCF s and another ‘communication’ sense in 37 which takes sentential SCF s. Irregular polysemy was not a marginal phenomenon: it explains 5 of the 10 singletons in our data."
These observations confirm that evaluation against a polysemic gold standard is necessary in order to fully explain the results from clustering.
"Finally, to provide feedback for further development of our verb classification approach, we performed a qualitative analysis of errors not resulting from poly-semy."
Consider the following clusters (the IB output for K = 42):
"B1: place (9), build (26, 45), publish (26, 25), carve (21, 25, 26)"
"B2: sin (003), rain (57), snow (57, 002)"
"B3: agree (36, 22, A42), appear (020, 48, 29), begin (55), continue (55, 47, 51)"
"B4: beg (015, 32)"
"Three main error types were identified: 1) Syntactic idiosyncracy: This was the most fre-quent error type, exemplified in B1, where place is incorrectly clustered with build, publish and carve merely because it takes similar prepositions to these verbs (e.g. in, on, into). 2) Sparse data: Many of the low frequency verbs (we had 12 with frequency less than 300) performed poorly."
"In B2, sin (which had 53 occurrences) is classified with rain and snow because it does not occur in our data with the preposition against -the ‘hallmark’ of its gold standard class (’Conspire Verbs’). 3) Problems in SCF acquisition: These were not numerous but occurred e.g. when the system could not distinguish between different control (e.g. sub-ject/object equi/raising) constructions (B3)."
This paper has presented a novel approach to auto-matic semantic classification of verbs.
This involved applying the NN and IB methods to cluster polysemic SCF distributions extracted from corpus data using Briscoe and Carroll’s (1997) system.
A principled evaluation scheme was introduced which enabled us to investigate the effect of polysemy on the resulting classification.
Our investigation revealed that polysemy has a considerable impact on the clusters formed: pol-ysemic verbs with a clear predominant sense and those with similar regular polysemy are frequently classified together.
Homonymic verbs or verbs with strong irregular polysemy tend to resist any classifi-cation.
"While it is clear that evaluation should account for these cases rather than ignore them, the issue of polysemy is related to another, bigger issue: the po-tential and limitations of clustering in inducing se-mantic information from polysemic SCF data."
Our results show that it is unrealistic to expect that the ‘important’ (high frequency) verbs in language fall into classes corresponding to single senses.
"How-ever, they also suggest that clustering can be used for novel, previously unexplored purposes: to de-tect from corpus data general patterns of seman-tic behaviour (monosemy, predominant sense, reg-ular/irregular polysemy)."
"In the future, we plan to investigate the use of soft clustering (without hardening the output) and de-velop methods for evaluating the soft output against polysemous gold standards."
"We also plan to work on improving the accuracy of subcategorization ac-quisition, investigating the role of noise (irregular / regular) in clustering, examining whether different syntactic/semantic verb types require different ap- proaches in clustering, developing our gold standard classification further, and extending our experiments to a larger number of verbs and verb classes."
We have aligned Japanese and English news articles and sentences to make a large parallel corpus.
We first used a method based on cross-language informa-tion retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles.
"How-ever, the results included many incorrect alignments."
"To remove these, we pro-pose two measures (scores) that evaluate the validity of alignments."
The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similar-ities in articles aligned by CLIR.
They enhance each other to improve the accu-racy of alignment.
"Using these measures, we have successfully constructed a large-scale article and sentence alignment cor-pus available to the public."
A large-scale Japanese-English parallel corpus is an invaluable resource in the study of natural language processing (NLP) such as machine translation and cross-language information retrieval (CLIR).
It is also valuable for language education.
"However, no such corpus has been available to the public."
We recently have obtained a noisy parallel cor-pus of Japanese and English newspapers consisting of issues published over more than a decade and have tried to align their articles and sentences.
We first aligned the articles using a method based on CLIR[REF_CITE]and then aligned the sentences in these articles by using a method based on dynamic programming (DP) matching[REF_CITE].
"However, the results included many in-correct alignments due to noise in the corpus."
"To remove these, we propose two measures (scores) that evaluate the validity of article and sen-tence alignments."
"Using these, we can selectively extract valid alignments."
"In this paper, we first discuss the basic statistics on the Japanese and English newspapers."
We next explain methods and measures used for alignment.
We then evaluate the effectiveness of the proposed measures.
"Finally, we show that our aligned corpus has attracted people both inside and outside the NLP community."
The Japanese and English newspapers used as source data were the Yomiuri Shimbun and the Daily Yomiuri.
They cover the period[REF_CITE].
"The number of Japanese articles per year ranges from 100,000 to 350,000, while English articles ranges from 4,000 to 13,000."
"The total number of Japanese articles is about 2,000,000 and the total number of English articles is about 110,000."
The number of English articles rep-resents less than 6 percent that of Japanese articles.
"Therefore, we decided to search for the Japanese ar-ticles corresponding to each of the English articles."
"The English articles as of mid-[REF_CITE]have tags indicating whether they are translated from Japanese articles or not, though they don’t have explicit links to the original Japanese articles."
"Consequently, we only used the translated English articles for the arti-cle alignment."
"The number of English articles used was 35,318, which is 68 percent of all of the arti-cles."
"On the other hand, the English articles before mid-[REF_CITE]do not have such tags."
So we used all the articles for the period.
"The number of them was 59,086."
We call the set of articles before mid-[REF_CITE]“1989-1996” and call the set of articles after mid-[REF_CITE]“1996-2001.”
"If an English article is a translation of a Japanese article, then the publication date of the Japanese ar-ticle will be near that of the English article."
"So we searched for the original Japanese articles within [URL_CITE] days before and after the publication of each English article, i.e., the corresponding article of an English article was searched for from the Japanese articles of 5 days’ issues."
"The average number of English arti-cles per day was 24 and that of Japanese articles per 5 days was [URL_CITE],532 for 1989-1996."
"As there are many candidates for alignment with English articles, we need a reliable measure to estimate the validity of article alignments to search for appropriate Japanese articles from these ambiguous matches."
Correct article alignment does not guarantee the existence of one-to-one correspondence between English and Japanese sentences in article alignment because literal translations are exceptional.
"Original Japanese articles may be restructured to conform to the style of English newspapers, additional descrip-tions may be added to fill cultural gaps, and detailed descriptions may be omitted."
A typical example of a restructured English and Japanese article pair is:
"Part of an English article: he1i Two bullet holes were found at thehome[REF_CITE]residentofBungeiShunju,inAk-abane, Tokyo, by his wife[REF_CITE]at around 9 a.m. Monday. h/e1i he2i Police suspect right-wing activists, who have mounted criticism against articles about the Imperial family appearing in the Shukan Bunshun, the publisher’s weekly magazine, were re-sponsible for the shooting. h/e2i he3i Police received an anony-mous phone call shortly after 1 a.m. Monday by a caller who reported hearing gunfire near Tanaka’s residence. h/e3i he4i Po-lice found nothing after investigating the report, but later found a bullet in the Tanakas’ bedroom, where they were sleeping at the time of the shooting. h/e4i"
"Part of a literal translation of a Japanese article: hj1i At about 8:55 a.m. on the 29th,[REF_CITE]the wife of Bungei"
"Shunju’s president[REF_CITE]found bullet holes on the ku, Tokyo.h/j1i hj2i As a result of an investigation, the officers of the Akabane police station found two holes on the exterior wall of the bedroom and a bullet in the bedroom.h/j2i hj3i After receiv-ing an anonymous phone call shortly after 1 a.m. saying that two or three gunshots were heard near Tanaka’s residence, police offi-cers hurried to the scene for investigation, but no bullet holes were found.h/j[URL_CITE]i hj4i When gunshots were heard, Mr. and Mrs. Tanaka were sleeping in the bedroom.h/j4i hj5i"
"Since Shukan Bunshun, a weekly magazine published by Bungei Shunju, recently ran an ar-ticle criticizing the Imperial family, Akabane police suspect right-wing activists who have mounted criticism against the recent arti-cle to be responsible for the shooting and have been investigating the incident.h/j5i where there is a three-to-four correspondence be-tween {e1, e3, e4} and {j1, j2, j3, j4}, together with a one-to-one correspondence between e2 and j5."
Such sentence matches are of particular interest to researchers studying human translations and/or stylistic differences between English and Japanese newspapers.
"However, their usefulness as resources for NLP such as machine translation is limited for the time being."
It is therefore important to extract sentence alignments that are as literal as possible.
"To achieve this, a reliable measure of the validity of sentence alignments is necessary."
We adopt a standard strategy to align articles and sentences.
"First, we use a method based on CLIR to align Japanese and English articles[REF_CITE]and then a method based on DP matching to align Japanese and English sentences[REF_CITE]in these articles."
"As each of these methods uses existing NLP techniques, we describe them briefly focusing on basic similarity measures, which we will compare with our proposed measures in Section 5."
The sentences [Footnote_5] in the aligned Japanese and English articles are aligned by a method based on DP match-ing[REF_CITE].
5 We split the Japanese articles into sentences by using sim-ple heuristics and split the English articles into sentences by using MXTERMINATOR[REF_CITE].
We allow 1-to-n or n-to-1 (1 ≤ n ≤ 6) alignments when aligning the sentences.
Readers are referred[REF_CITE]for a concise description of the algorithm.
"Here, we only discuss the similarities between Japanese and English sentences for align-ment."
Let J i and E i be the words of Japanese and English sentences for i-th alignment.
"The similar-ity [Footnote_6] between J i and E i is: co(J i × E i ) + 1 SIM(J i , E i ) = l(J i ) + l(E i ) − 2co(J i × E i ) + 2 where"
"6 SIM(J i , E i ) is different from the similarity function used[REF_CITE]. We use SIM because it performed well in a preliminary experiment."
"P l(X) = x∈X f(x) f(x) is the frequency P of x in the sentences. co(J i × E i ) = (j,e)∈J i ×E i min(f(j), f(e))"
"J i × E i = {(j, e)|j ∈ J i , e ∈ E i } and J i × E i is a one-to-one correspondence between Japanese and English words."
J i and E i are obtained as follows.
"We use ChaSen to morphologically analyze the Japanese sentences and extract content words, which consists of J i ."
"We use Brill’s tagger[REF_CITE]to POS-tag the English sentences, extract content words, and use Word-Net’s library [URL_CITE] to obtain lemmas of the words, which consists of E i ."
"We use simple heuristics to obtain J i × E i , i.e., a one-to-one correspondence between the words in J i and E i , by looking up Japanese-English and English-Japanese dictionaries made up by combining entries in the EDR Japanese-English bilingual dictionary and the EDR English-Japanese bilingual dictionary."
"Each of the constructed dictio-naries has over 300,000 entries."
We evaluated the implemented program against a corpus consisting of manually aligned Japanese and English sentences.
The source texts were Japanese white papers[REF_CITE].
The style of translation was generally literal reflecting the nature of govern-ment documents.
The average number of Japanese sentences per text was 413 and that of English sentences was 495.
"The recall, R, and precision, P, of the program against this corpus were R = 0.982 and P = 0.986, respectively, where"
R = number of correctly aligned sentence pairs total number of sentence pairs aligned in corpus number of correctly aligned sentence pairs P = total number of sentence pairs proposed by program
"The number of pairs in a one-to-n alignment is n. For example, if sentences {J 1 } and {E 1 ,E 2 ,E 3 } are aligned, then three pairs hJ 1 , E 1 i, hJ 1 , E 2 i, and hJ 1 , E 3 i are obtained."
This recall and precision are quite good consid-ering the relatively large differences in the language structures between Japanese and English.
"We use[REF_CITE]and SIM to evaluate the similarity in articles and sentences, respectively."
"These mea-sures, however, cannot be used to reliably discrim-inate between correct and incorrect alignments as will be discussed in Section 5."
This motivated us to devise more reliable measures based on basic simi-larities.
It is not sensitive to differences in the order of sentences between two articles.
"To rem-edy this, we define a measure that uses the similari-ties in sentence alignments in the article alignment."
"We define AVSIM(J,E) as the similarity between Japanese article, J, and English article, E:"
"P m k=1 SIM(J k ,"
"E k ) AVSIM(J, E) = m where (J 1 , E 1 ), (J 2 , E 2 ), . .. (J m , E m ) are the sen-tence alignments obtained by the method described in Section 3.2."
The sentence alignments in a cor-rectly aligned article alignment should have more similarity than the ones in an incorrectly aligned ar-ticle alignment.
"Consequently, article alignments with high AVSIM are likely to be correct."
Our sentence alignment program aligns sentences accurately if the English sentences are literal trans-lations of the Japanese as discussed in Section 3.2.
"However, the relation between English news sen-tences and Japanese news sentences are not literal translations."
"Thus, the results for sentence align-ments include many incorrect alignments."
"To dis-criminate between correct and incorrect alignments, we take advantage of the similarity in article align-ments containing sentence alignments so that the sentence alignments in a similar article alignment will have a high value."
"SntScore(J i , E i ) ="
"AVSIM(J, E) ×"
"SIM(J i , E i )"
"SntScore(J i ,E i ) is the similarity in the i-th align-ment, (J i , E i ), in article alignment J and E. When we compare the validity of two sentence alignments in the same article alignment, the rank order of sen-tence alignments obtained by applying SntScore is the same as that of SIM because they share a com-mon AVSIM."
"However, when we compare the va-lidity of two sentence alignments in different article alignments, SntScore prefers the sentence alignment with the more similar (high AVSIM) article align-ment even if their SIM has the same value, while SIM cannot discriminate between the validity of two sentence alignments if their SIM has the same value."
"Therefore, SntScore is more appropriate than SIM if we want to compare sentence alignments in different article alignments, because, in general, a sentence alignment in a reliable article alignment is more re-liable than one in an unreliable article alignment."
The next section compares the effectiveness of AVSIM to that[REF_CITE]and that of SntScore to that of SIM.
"Here, we discuss the results of evaluating article and sentence alignments."
We first estimate the precision of article alignments by using randomly sampled alignments.
"Next, we sort them in descending order[REF_CITE]and AVSIM to see whether these measures can be used to provide correct alignments with a high ranking."
"Finally, we show that the absolute values of AVSIM correspond well with human judgment."
Sentence alignments in article alignments have many errors even if they have been obtained from correct article alignments due to free translation as discussed in Section 2.
"To extract only correct alignments, we sorted whole sentence alignments in whole article alignments in decreasing order of SntScore and selected only the higher ranked sen-tence alignments so that the selected alignments would be sufficiently precise to be useful as NLP resources."
"The number of whole sentence alignments was about 1,300,000."
The most important category for sentence alignment is one-to-one.
"Thus, we want to discard as many errors in this category as pos-sible."
"In the first step, we classified whole one-to-one alignments into two classes: the first con-sisted of alignments whose Japanese and English sentences ended with periods, question marks, ex-clamation marks, or other readily identifiable char-acteristics."
We call this class “one-to-one”.
The second class consisted of the one-to-one alignments not belonging to the first class.
"The alignments in this class, together with the whole one-to-n alignments, are called “one-to-many”."
"One-to-one had about 640,000 alignments and one-to-many had about 660,000 alignments."
We first evaluated the precision of one-to-one alignments by sorting them in decreasing order of SntScore.
"We randomly extracted 100 samples from each of 10 blocks ranked at the top-300,000 align-ments. (A block had 30,000 alignments.)"
"We clas-sified these 1000 samples into two classes: The first was “match” (A), the second was “not match” (D)."
We judged a sample as “A” if the Japanese and English sentences of the sample shared a common event (approximately a clause). “D” consisted of the samples not belonging to “A”.
The results of evalua-tion are in Table 6. [Footnote_9]
"9 Evaluations were done by the authors. We double checked all samples.[REF_CITE]samples, there were a maximum of two or three where the first and second evaluations were different."
This table shows that the number of A’s decreases rapidly as the rank increases.
This means that SntScore ranks appropriate one-to-one alignments highly.
"The table indicates that the top-150,000 one-to-one alignments are sufficiently reliable. [Footnote_10]"
"10 The notion of “appropriate (correct) sentence alignment” depends on applications. Machine translation, for example, may require more precise (literal) alignment. To get literal alignments beyond a sharing of a common event, we will select a set of alignments from the top of the sorted alignments that satisfies the required literalness. This is because, in general, higher ranked alignments are more literal translations, because those alignments tend to have many one-to-one corresponding words and to be contained in highly similar article alignments."
The ra-tio of A’s in these alignments was 0.982.
We then evaluated precision for one-to-many alignments by sorting them in decreasing order of SntScore.
"We classified one-to-many into three cat-egories: “1-90000”, “90001-180000”, and “180001- 270000”, each of which was covered by the range of SntScore of one-to-one that was presented in Table 6."
We randomly sampled 100 one-to-many align-ments from these categories and judged them to be A or D (see Table 7).
"Table 7 indicates that the 38,090 alignments in the range from “1-90000” are suffi-ciently reliable."
Tables 6 and 7 show that we can extract valid alignments by sorting alignments according to SntScore and by selecting only higher ranked sen-tence alignments.
Much work has been done on article alignment.
They revealed that DTL is superior to MT at high-recall levels.
"That is, if we want to ob-tain many article alignments, then DTL is more ap-propriate than MT."
"In a preliminary experiment, we also compared MT and DTL for the data in Table 1 and found that DTL was superior to MT. [Footnote_11]"
"11 We translated the English articles into Japanese with an MT system. We then used the translated English articles as queries and searched the database consisting of Japanese articles. The direction of translation was opposite to the one described in Section 3.1. Therefore this comparison is not as objective as it could be. However, it gives us some idea into a comparison of MT and DTL."
These experimental results indicate that DTL is more ap-propriate than MT in article alignment.
"Their method achieved a 97% preci-sion in aligning articles, which is quite high."
They also applied their method to NHK broadcast news.
"However, they obtained a lower precision of 69.8% for the NHK corpus."
"Thus, the precision of their method depends on the corpora."
"Therefore, it is not clear whether their method would have achieved a high accuracy in the Yomiuri corpus treated in this paper."
"There are two significant differences between our work and previous works. (1) We have proposed AVSIM, which uses sim-ilarities in sentences aligned by DP matching, as a reliable measure for article alignment."
"Previous works, on the other hand, have used measures based on bag-of-words. (2) A more important difference is that we have actually obtained not only article alignments but also sentence alignments on a large scale."
"In addition to that, we are distributing the alignment data for re-search and educational purposes."
This is the first attempt at a Japanese-English bilingual corpus.
As of late-[REF_CITE]we have been distributing the alignment data discussed in this paper for re-search and educational purposes. 12 All the informa-tion on the article and sentence alignments are nu-merically encoded so that users who have the Yomi-uri data can recover the results of alignments.
"The data also contains the top-150,000 one-to-one sen-tence alignments and the top-30,000 one-to-many sentence alignments as raw sentences."
The Yomiuri Shimbun generously allowed us to distribute them for research and educational purposes.
We have sent over 30 data sets to organizations on their request.
About half of these were NLP-related.
The other half were linguistics-related.
A few requests were from high-school and junior-high-school teachers of English.
A psycho-linguist was also included.
It is obvious that people from both in-side and outside the NLP community are interested in this Japanese-English alignment data.
We have proposed two measures for extracting valid article and sentence alignments.
The measure for ar-ticle alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR.
They enhance each other and allow valid article and sen-tence alignments to be reliably extracted from an ex-tremely noisy Japanese-English parallel corpus.
We are distributing the alignment data discussed in this paper so that it can be used for research and educational purposes.
It has attracted the attention of people both inside and outside the NLP community.
We have applied our measures to a Japanese and English bilingual corpus and these are language in-dependent.
"It is therefore reasonable to expect that they can be applied to any language pair and still re-tain good performance, particularly since their effec-tiveness has been demonstrated in such a disparate language pair as Japanese and English."
"We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keep-ing computational complexity polynomial in the sentence length."
This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree align-ment algorithms.
"Systems for automatic translation between lan-guages have been divided into transfer-based ap-proaches, which rely on interpreting the source string into an abstract semantic representation from which text is generated in the target lan-guage, and statistical approaches, pioneered[REF_CITE], which estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of par-allel bilingual text."
"Only recently have hybrid approaches begun to emerge, which apply prob-abilistic models to a structured representation of the source text."
"The use of explicit syntactic information for the target language in this model has led to excellent translation results[REF_CITE], and raises the prospect of training a statistical system using syntactic informa-tion for both sides of the parallel corpus."
Tree-to-tree alignment techniques such as prob-abilistic tree substitution grammars[REF_CITE]can be trained on parse trees from parallel treebanks.
"However, real bitexts generally do not exhibit parse-tree isomorphism, whether because of systematic differences between how languages ex-press a concept syntactically[REF_CITE], or simply because of relatively free translations in the training material."
"In this paper, we introduce “loosely” tree-based alignment techniques to address this problem."
"We present analogous extensions for both tree-to-string and tree-to-tree models that allow alignments not obeying the constraints of the original syntactic tree (or tree pair), although such alignments are dispre-ferred because they incur a cost in probability."
"This is achieved by introducing a clone operation, which copies an entire subtree of the source language syn-tactic structure, moving it anywhere in the target language sentence."
Careful parameterization of the probability model allows it to be estimated at no ad-ditional cost in computational complexity.
"We ex-pect our relatively unconstrained clone operation to allow for various types of structural divergence by providing a sort of hybrid between tree-based and unstructured, IBM-style models."
"We first present the tree-to-string model, followed by the tree-to-tree model, before moving on to align-ment results for a parallel syntactically annotated Korean-English corpus, measured in terms of align-ment perplexities on held-out test data, and agree-ment with human-annotated word-level alignments."
"We begin by summarizing the model[REF_CITE], which can be thought of as representing translation as an Alexander Calder mobile."
"If we follow the process of an English sentence’s transformation into French, the English sentence is first given a syntactic tree representation by a statistical parser[REF_CITE]."
"As the first step in the translation process, the children of each node in the tree can be re-ordered."
"For any node with m children, m! re-orderings are possible, each of which is assigned a probability P order conditioned on the syntactic categories of the parent node and its children."
"As the second step, French words can be inserted at each node of the parse tree."
"Insertions are modeled in two steps, the first predicting whether an insertion to the left, an insertion to the right, or no insertion takes place with probability P ins , conditioned on the syntactic category of the node and that of its parent."
"The second step is the choice of the inserted word P t (f|NULL), which is predicted without any conditioning information."
"The final step, a French translation of each original English word, at the leaves of the tree, is chosen according to a distribution P t (f|e)."
"The French word is predicted conditioned only on the English word, and each English word can generate at most one French word, or can generate a NULL symbol, representing deletion."
"Given the original tree, the re-ordering, insertion, and translation probabilities at each node are independent of the choices at any other node."
"These independence relations are analogous to those of a stochastic context-free grammar, and allow for efficient parameter estimation by an inside-outside Expectation Maximization (EM) algorithm."
"The computation of inside probabilities β, outlined below, considers possible reordering of nodes in the original tree in a bottom-up manner: for all nodes ε i in input tree T do for all k, l such that 1 &lt; k &lt; l &lt; N do for all orderings ρ of the children ε 1 ...ε m of ε i do for all partitions of span k, l into Q m k 1 , l 1 ...k m , l m do β(ε i , k, l)+= P order (ρ|ε i ) j=1 β(ε j , k j , l j ) end for end for end for end for"
"This algorithm has computational complexity O(|T |N m+2 ), where m is the maximum number of children of any node in the input tree T, and N the length of the input string."
"By storing partially completed arcs in the chart and interleaving the in-ner two loops, complexity of O(|T |n 3 m!2 m ) can be achieved."
"Thus, while the algorithm is exponential in m, the fan-out of the grammar, it is polynomial in the size of the input string."
"Assuming |T| = O(n), the algorithm is O(n 4 )."
"The model’s efficiency, however, comes at a cost."
"Not only are many independence assumptions made, but many alignments between source and target sen-tences simply cannot be represented."
"As a minimal example, take the tree:"
"Of the six possible re-orderings of the three ter-minals, the two which would involve crossing the bracketing of the original tree (XZY and YZX) are not allowed."
"While this constraint gives us a way of using syntactic information in translation, it may in many cases be too rigid."
"In part to deal with this problem,[REF_CITE]flat-ten the trees in a pre-processing step by collapsing nodes with the same lexical head-word."
"This allows, for example, an English subject-verb-object (SVO) structure, which is analyzed as having a VP node spanning the verb and object, to be re-ordered as VSO in a language such as Arabic."
"Larger syntactic divergences between the two trees may require fur-ther relaxation of this constraint, and in practice we expect such divergences to be frequent."
"For exam-ple, a nominal modifier in one language may show up as an adverbial in the other, or, due to choices such as which information is represented by a main verb, the syntactic correspondence between the two sentences may break down completely."
"In order to provide some flexibility, we modify the model in order to allow for a copy of a (translated) subtree from the English sentences to occur, with some cost, at any point in the resulting French sen-tence."
"For example, in the case of the input tree"
"This operation, combined with the deletion of the original node Z, produces the alignment (XZY) that was disallowed by the original tree reorder-ing model."
Figure 1 shows an example from our Korean-English corpus where the clone operation al-lows the model to handle a case of wh-movement in the English sentence that could not be realized by any reordering of subtrees of the Korean parse.
"The probability of adding a clone of original node ε i as a child of node ε j is calculated in two steps: first, the choice of whether to insert a clone under ε j , with probability P ins (clone|ε j ), and the choice of which original node to copy, with probability"
P clone (ε i |clone = 1) =
P P makeclone (ε i ) k P makeclone (ε k ) where P makeclone is the probability of an original node producing a copy.
"In our implementation, for simplicity, P ins (clone) is a single number, estimated by the EM algorithm but not conditioned on the par-ent node ε j , and P makeclone is a constant, meaning that the node to be copied is chosen from all the nodes in the original tree with uniform probability."
"It is important to note that P makeclone is not de-pendent on whether a clone of the node in ques-tion has already been made, and thus a node may be “reused” any number of times."
"This indepen-dence assumption is crucial to the computational tractability of the algorithm, as the model can be estimated using the dynamic programming method above, keeping counts for the expected number of times each node has been cloned, at no increase in computational complexity."
"Without such an assump-tion, the parameter estimation becomes a problem of parsing with crossing dependencies, which is ex-ponential in the length of the input string[REF_CITE]."
The tree-to-tree alignment model has tree transfor-mation operations similar to those of the tree-to-string model described above.
"However, the trans-formed tree must not only match the surface string of the target language, but also the tree structure as-signed to the string by the treebank annotators."
"In or-der to provide enough flexibility to make this possi-ble, additional tree transformation operations allow a single node in the source tree to produce two nodes in the target tree, or two nodes in the source tree to be grouped together and produce a single node in the target tree."
"The model can be thought of as a synchronous tree substitution grammar, with proba-bilities parameterized to generate the target tree con-ditioned on the structure of the source tree."
The probability P(T b |T a ) of transforming the source tree T a into target tree T b is modeled in a sequence of steps proceeding from the root of the target tree down.
At each level of the tree: 1.
"At most one of the current node’s children is grouped with the current node in a single ele-mentary tree, with probability P elem (t a |ε a ⇒ children(ε a )), conditioned on the current node ε a and its children (ie the CFG produc-tion expanding ε a ). 2."
"An alignment of the children of the current elementary tree is chosen, with probability P align (α|ε a ⇒ children(t a ))."
"This alignment operation is similar to the re-order operation in the tree-to-string model, with the extension that 1) the alignment α can include insertions and deletions of individual children, as nodes in either the source or target may not corre-spond to anything on the other side, and 2) in the case where two nodes have been grouped into t a , their children are re-ordered together in one step."
"In the final step of the process, as in the tree-to-string model, lexical items at the leaves of the tree are translated into the target language according to a distribution P t (f|e)."
Allowing non-1-to-1 correspondences between nodes in the two trees is necessary to handle the fact that the depth of corresponding words in the two trees often differs.
A further consequence of allowing elementary trees of size one or two is that some reorderings not allowed when reordering the children of each individual node separately are now possible.
"For example, with our simple tree if nodes A and B are considered as one elementary tree, with probability P elem (t a |A ⇒ BZ), their col-lective children will be reordered with probability"
"P align ({(1, 1)(2, 3)(3, 2)}|A ⇒ XYZ)"
X Z Y giving the desired word ordering XZY.
"However, computational complexity as well as data sparsity prevent us from considering arbitrarily large ele-mentary trees, and the number of nodes considered at once still limits the possible alignments."
"For ex-ample, with our maximum of two nodes, no trans-formation of the tree is capable of generating the alignment WYXZ."
"In order to generate the complete target tree, one more step is necessary to choose the structure on the target side, specifically whether the elementary tree has one or two nodes, what labels the nodes have, and, if there are two nodes, whether each child at-taches to the first or the second."
"Because we are ultimately interested in predicting the correct target string, regardless of its structure, we do not assign probabilities to these steps."
"The nonterminals on the target side are ignored entirely, and while the align-ment algorithm considers possible pairs of nodes as elementary trees on the target side during training, the generative probability model should be thought of as only generating single nodes on the target side."
"Thus, the alignment algorithm is constrained by the bracketing on the target side, but does not generate the entire target tree structure."
"While the probability model for tree transforma-tion operates from the top of the tree down, prob-ability estimation for aligning two trees takes place by iterating through pairs of nodes from each tree in bottom-up order, as sketched below: for all nodes ε a in source tree T a in bottom-up order do for all elementary trees t a rooted in ε a do for all nodes ε b in target tree T b in bottom-up order do for all elementary trees t b rooted in ε b do for all alignments α of the children of t a and t b do β(ε a , ε b )"
P elem (t a |ε a )P align (α|ε i )
"Q += (i,j)∈α β(ε i , ε j ) end for end for end for end for end for"
"The outer two loops, iterating over nodes in each tree, require O(|T| 2 )."
"Because we restrict our el-ementary trees to include at most one child of the root node on either side, choosing elementary trees for a node pair is O(m 2 ), where m refers to the max-imum number of children of a node."
"Computing the alignment between the 2m children of the elemen-tary tree on either side requires choosing which sub-set of source nodes to delete, O(2 2m ), which subset of target nodes to insert (or clone), O(2 2m ), and how to reorder the remaining nodes from source to target tree, O((2m)!)."
"Thus overall complexity of the algo-rithm is O(|T | 2 m 2 4 2m (2m)!), quadratic in the size of the input sentences, but exponential in the fan-out of the grammar."
"Allowing m-to-n matching of up to two nodes on either side of the parallel treebank allows for limited non-isomorphism between the trees, as[REF_CITE]."
"However, even given this flexi-bility, requiring alignments to match two input trees rather than one often makes tree-to-tree alignment more constrained than tree-to-string alignment."
"For example, even alignments with no change in word order may not be possible if the structures of the two trees are radically mismatched."
"This leads us to think it may be helpful to allow departures from the constraints of the parallel bracketing, if it can be done in without dramatically increasing compu-tational complexity."
"For this reason, we introduce a clone operation, which allows a copy of a node from the source tree to be made anywhere in the target tree."
"After the clone operation takes place, the transformation of source into target tree takes place using the tree decomposi-tion and subtree alignment operations as before."
"The basic algorithm of the previous section remains un-changed, with the exception that the alignments α between children of two elementary trees can now include cloned, as well as inserted, nodes on the tar-get side."
"Given that α specifies a new cloned node as a child of ε j , the choice of which node to clone is made as in the tree-to-string model:"
P clone (ε i |clone ∈ α) =
P P makeclone (ε i ) k P makeclone (ε k )
"Because a node from the source tree is cloned with equal probability regardless of whether it has al-ready been “used” or not, the probability of a clone operation can be computed under the same dynamic programming assumptions as the basic tree-to-tree model."
"As with the tree-to-string cloning operation, this independence assumption is essential to keep the complexity polynomial in the size of the input sentences."
"For reference, the parameterization of all four models is summarized in Table 1."
"For our experiments, we used a parallel Korean-English corpus from the military doma[REF_CITE]."
"Syntactic trees have been annotated by hand for both the Korean and English sentences; in this paper we will be using only the Korean trees, mod-eling their transformation into the English text."
"The corpus contains 5083 sentences, of which we used 4982 as training data, holding out 101 sentences for evaluation."
The average Korean sentence length was 13 words.
"Korean is an agglutinative language, and words often contain sequences of meaning-bearing suffixes."
"For the purposes of our model, we rep-resented the syntax trees using a fairly aggressive tokenization, breaking multimorphemic words into separate leaves of the tree."
This gave an average of 21 tokens for the Korean sentences.
The aver-age English sentence length was 16.
"The maximum number of children of a node in the Korean trees was 23 (this corresponds to a comma-separated list of items). 77% of the Korean trees had no more than four children at any node, 92% had no more than five children, and 96% no more than six chil-dren."
"The vocabulary size (number of unique types) was 4700 words in English, and 3279 in Korean — before splitting multi-morphemic words, the Korean vocabulary size was 10059."
"For reasons of compu-tation speed, trees with more than 5 children were excluded from the experiments described below."
We evaluate our translation models both in terms agreement with human-annotated word-level align-ments between the sentence pairs.
"For scoring the viterbi alignments of each system against gold-standard annotated alignments, we use the alignment error rate (AER)[REF_CITE], which measures agreement at the level of pairs of words: [Footnote_1] 2|A ∩ G| AER = 1 − |A| + |G| where A is the set of word pairs aligned by the au-tomatic system, and G the set aligned in the gold standard."
"1[REF_CITE]differentiate between sure and possible hand-annotated alignments, our gold standard align-ments come in only one variety."
We provide a comparison of the tree-based models with the sequence of successively more com-plex models[REF_CITE].
Results are shown in Table 2.
The error rates shown in Table 2 represent the minimum over training iterations; training was stopped for each model when error began to in-crease.
"IBM Models 1, 2, and 3 refer[REF_CITE]. “Tree-to-String” is the model[REF_CITE], and “Tree-to-String, Clone” allows the node cloning operation of Section 2.1. “Tree-to-Tree” indicates the model of Section 3, while “Tree-to-Tree, Clone” adds the node cloning operation of Section 3.1."
"Model 2 is initialized from the parameters of Model 1, and Model 3 is initialized from Model 2."
"The lexical translation probabilities P t (f|e) for each of our tree-based models are initial-ized from Model 1, and the node re-ordering proba-bilities are initialized uniformly."
"Figure 1 shows the viterbi alignment produced by the “Tree-to-String, Clone” system on one sentence from our test set."
We found better agreement with the human align-ments when fixing P ins (left) in the Tree-to-String model to a constant rather than letting it be deter-mined through the EM training.
"While the model learned by EM tends to overestimate the total num-ber of aligned word pairs, fixing a higher probability for insertions results in fewer total aligned pairs and therefore a better trade-off between precision and recall."
"As seen for other tasks[REF_CITE], the likelihood crite-rion used in EM training may not be optimal when evaluating a system against human labeling."
The approach of optimizing a small number of metapa-rameters has been applied to machine translation[REF_CITE].
It is likely that the IBM mod-els could similarly be optimized to minimize align-ment error – an open question is whether the opti-mization with respect to alignment error will corre-spond to optimization for translation accuracy.
"Within the strict EM framework, we found roughly equivalent performance between the IBM models and the two tree-based models when making use of the cloning operation."
"For both the tree-to-string and tree-to-tree models, the cloning operation improved results, indicating that adding the flexibil-ity to handle structural divergence is important when using syntax-based models."
"The improvement was particularly significant for the tree-to-tree model, be-cause using syntactic trees on both sides of the trans-lation pair, while desirable as an additional source of information, severely constrains possible alignments unless the cloning operation is allowed."
"The tree-to-tree model has better theoretical com-plexity than the tree-to-string model, being quadratic rather than quartic in sentence length, and we found this to be a significant advantage in practice."
This improvement in speed allows longer sentences and more data to be used in training syntax-based mod-els.
"We found that when training on sentences of up 60 words, the tree-to-tree alignment was 20 times faster than tree-to-string alignment."
"For reasons of speed,[REF_CITE]limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus."
Our loosely tree-based alignment techniques allow statistical models of machine translation to make use of syntactic information while retaining the flexibil-ity to handle cases of non-isomorphic source and tar-get trees.
This is achieved with a clone operation pa-rameterized in such a way that alignment probabili-ties can be computed with no increase in asymptotic computational complexity.
"We present versions of this technique both for tree-to-string models, making use of parse trees for one of the two languages, and tree-to-tree models, which make use of parallel parse trees."
Results in terms of alignment error rate indicate that the clone operation results in better alignments in both cases.
"On our Korean-English corpus, we found roughly equivalent performance for the unstructured IBM models, and the both the tree-to-string and tree-to-tree models when using cloning."
To our knowl-edge these are the first results in the literature for tree-to-tree statistical alignment.
"While we did not see a benefit in alignment error from using syntactic trees in both languages, there is a significant practi-cal benefit in computational efficiency."
"We remain hopeful that two trees can provide more information than one, and feel that extensions to the “loosely” tree-based approach are likely to demonstrate this using larger corpora."
"Another important question we plan to pursue is the degree to which these results will be borne out with larger corpora, and how the models may be re-fined as more training data is available."
"As one ex-ample, our tree representation is unlexicalized, but we expect conditioning the model on more lexical information to improve results, whether this is done by percolating lexical heads through the existing trees or by switching to a strict dependency repre-sentation."
Word alignment plays a crucial role in sta-tistical machine translation.
Word-aligned corpora have been found to be an excellent source of translation-related knowledge.
We present a statistical model for comput-ing the probability of an alignment given a sentence pair.
This model allows easy in-tegration of context-specific features.
Our experiments show that this model can be an effective tool for improving an existing word alignment.
Word alignments were first introduced as an in-termediate result of statistical machine translation systems[REF_CITE].
"Since their intro-duction, many researchers have become interested in word alignments as a knowledge source."
"For example, alignments can be used to learn transla-tion lexicons[REF_CITE], transfer rules[REF_CITE], and classifiers to find safe sentence segmentation points[REF_CITE]."
"In addition to the IBM models, researchers have proposed a number of alternative alignment meth-ods."
These methods often involve using a statistic such as φ 2[REF_CITE]or the log likeli-hood ratio[REF_CITE]to create a score to mea-sure the strength of correlation between source and target words.
Such measures can then be used to guide a constrained search to produce word align-ments[REF_CITE].
"It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the align-ment."
"For example Melamed uses competitive link-ing along with an explicit noise model[REF_CITE]to produce a new scoring metric, which in turn creates better alignments."
"In this paper, we present a simple, flexible, sta-tistical model that is designed to capture the infor-mation present in a baseline alignment."
This model allows us to compute the probability of an align-ment for a given sentence pair.
It also allows for the easy incorporation of context-specific knowl-edge into alignment probabilities.
"A critical reader may pose the question, “Why in-vent a new statistical model for this purpose, when existing, proven models are available to train on a given word alignment?”"
"We will demonstrate exper-imentally that, for the purposes of refinement, our model achieves better results than a comparable ex-isting alternative."
We will first present this model in its most general form.
"Next, we describe an alignment algorithm that integrates this model with linguistic constraints in order to produce high quality word alignments."
We will follow with our experimental results and dis-cussion.
We will close with a look at how our work relates to other similar systems and a discussion of possible future directions.
In this section we describe our probability model.
"To do so, we will first introduce some necessary no-tation."
"Let E be an English sentence e 1 , e 2 , . . . , e m and let F be a French sentence f 1 , f 2 , . .. , f n ."
"We define a link l(e i , f j ) to exist if e i and f j are a trans-lation (or part of a translation) of one another."
"We define the null link l(e i , f 0 ) to exist if e i does not correspond to a translation for any French word in F. The null link l(e 0 ,f j ) is defined similarly."
"An alignment A for two sentences E and F is a set of links such that every word in E and F participates in at least one link, and a word linked to e 0 or f 0 partic-ipates in no other links."
"If e occurs in E x times and f occurs in F y times, we say that e and f co-occur xy times in this sentence pair."
"We define the alignment problem as finding the alignment A that maximizes P (A|E, F )."
This cor-responds to finding the Viterbi alignment in the IBM translation systems.
"Those systems model P (F, A|E), which when maximized is equivalent to maximizing P (A|E, F )."
"We propose here a system which models P (A|E, F ) directly, using a different decomposition of terms."
"In the IBM models of translation, alignments exist as artifacts of which English words generated which French words."
Our model does not state that one sentence generates the other.
"Instead it takes both sentences as given, and uses the sentences to deter-mine an alignment."
"An alignment A consists of t links {l 1 , l 2 , . . . , l t }, where each l k = l(e i k , f j k ) for some i k and j k ."
"We will refer to consecutive subsets of A as l ij = {l i , l i+1 , . .. , l j }."
"Given this notation, P (A|E, F ) can be decomposed as follows: t P (A|E, F ) ="
"P (l 1t |E, F ) ="
"Y P (l k |E, F, l 1k−1 ) k=1"
"At this point, we must factor P (l k |E, F, l 1k−1 ) to make computation feasible."
"Let C k = {E, F, l 1k−1 } represent the context of l k ."
Note that both the con-text C k and the link l k imply the occurrence of e i k and f j k .
We can rewrite P (l k |C k ) as:
P (l k |C k ) =
"P (l k , C k ) ="
P (C k |l k )P (l k )
"P (C k ) P (C k , e i k , f j k ) P (C k |l k )"
"P (l k , e i k , f j k ) = × P (C k |e i k , f j k ) P (e i k , f j k )"
P (C k |l k ) =
"P (l k |e i k , f j k ) ×"
"P (C k |e i , f k j k )"
"Here P (l k |e i k , f j k ) is link probability given a co-occurrence of the two words, which is similar in spirit to Melamed’s explicit noise model[REF_CITE]."
This term depends only on the words in-
P(C k |l k ) volved directly in the link.
"The ratio P(C k |e i ,f ) k jk modifies the link probability, providing context-sensitive information."
"Up until this point, we have made no simplify-ing assumptions in our derivation."
"Unfortunately, C k = {E, F, l 1k−1 } is too complex to estimate con-text probabilities directly."
"Suppose FT k is a set of context-related features such that P(l k |C k ) can be approximated by P(l k |e i k , f j k , FT k )."
"Let C k0 = {e i k , f j k }∪F T k ."
P (l k |C k0 ) can then be decomposed using the same derivation as above.
P (C k0 |l k )
P (l k |C k0 ) =
"P (l k |e i k , f j k ) × P (C k0 |e i k , f j k ) P (F T k |l k ) ="
"P (l k |e i k , f j k ) ×"
"P (F T k |e i , f k j k )"
"In the second line of this derivation, we can drop e i k and f j k from C k0 , leaving only F T k , because they are implied by the events which the probabilities are conditionalized on."
"Now, we are left with the task of approximating P (F T k |l k ) and P (F T k |e i k , f j k )."
"To do so, we will assume that for all ft ∈ FT k , ft is conditionally independent given either l k or (e i k , f j k )."
"This allows us to approximate alignment probability P (A|E, F ) as follows: t  "
"Y P (ft|l k )  P (l k |e i k , f j k ) ×"
"Y k=1 P (ft|e i k , f j k )  ft∈FT k"
"In any context, only a few features will be ac-tive."
The inner product is understood to be only over those features ft that are present in the current con-text.
"This approximation will cause P(A|E, F) to no longer be a well-behaved probability distribution, though as in Naive Bayes, it can be an excellent es-timator for the purpose of ranking alignments."
"If we have an aligned training corpus, the prob-abilities needed for the above equation are quite easy to obtain."
"Link probabilities can be deter-mined directly from |l k | (link counts) and |e i k , f j,k | (co-occurrence counts)."
"For any co-occurring pair of words (e i k ,f j k ), we check whether it has the feature ft."
"If it does, we increment the count of |ft, e i k , f j k |."
"If this pair is also linked, then we in-crement the count of |ft, l k |."
Note that our definition of FT k allows for features that depend on previous links.
"For this reason, when determining whether or not a feature is present in a given context, one must impose an ordering on the links."
This ordering can be arbitrary as long as the same ordering is used in training 1 and probability evaluation.
A simple solu-tion would be to order links according their French words.
"We choose to order links according to the link probability P(l k |e i k , f j k ) as it has an intuitive appeal of allowing more certain links to provide con-text for others."
We store probabilities in two tables.
"The first ta-ble stores link probabilities P (l k |e i k , f j k )."
It has an entry for every word pair that was linked at least once in the training corpus.
Its size is the same as the translation table in the IBM models.
"The sec-ond table stores feature probabilities, P(ft|l k ) and P (ft|e i k , f j k )."
"For every linked word pair, this table has two entries for each active feature."
In the worst case this table will be of size 2×|F T |×|E|×|F |.
"In practice, it is much smaller as most contexts activate only a small number of features."
In the next subsection we will walk through a sim-ple example of this probability model in action.
We will describe the features used in our implementa-tion of this model in Section 3.2.
Figure 1 shows an aligned corpus consisting of one sentence pair.
"Suppose that we are concerned with only one feature ft that is active [Footnote_2] for e i k and f j k if an adjacent pair is an alignment, i.e., l(e i k −1 ,f j k −1 ) ∈ l k−11 or l(e i k +1 , f j k +1 ) ∈ l 1k−1 ."
"2 Throughout this paper we will assume that null alignments are special cases, and do not activate or participate in features unless otherwise stated in the feature description."
This example would produce the probability tables shown in Table 1.
"Note how ft is active for the (a,v) link, and is not active for the (b, u) link."
This is due to our se-lected ordering.
"Table 1 allows us to calculate the probability of this alignment as: (b) Feature Counts e i k f j k |ft, l k | |ft, e i k , f j k | a v 1 1 (c) Feature Probabilities e i k f j k P (ft|l k ) P (ft|e i k , f j k ) 1 a [Footnote_1]v 4"
"1 In our experiments, the ordering is not necessary during training to achieve good performance."
"P (A|E, F ) = P (l(b, u)|b, u)× P (l(a, f 0 )|a, f 0 )× P (l(e 0 , v)|e 0 , v)× P (l(a, v)|a, v) P(ft|l(a,v))P(ft|a,v) = 1 × 12 × 21 × 14 × 1 1 4 = 14"
"In this section, we describe a world-alignment al-gorithm guided by the alignment probability model derived above."
"In designing this algorithm we have selected constraints, features and a search method in order to achieve high performance."
"The model, however, is general, and could be used with any in-stantiation of the above three factors."
"This section will describe and motivate the selection of our con-straints, features and search method."
"The input to our word-alignment algorithm con-sists of a pair of sentences E and F , and the depen-dency tree T E for E. T E allows us to make use of features and constraints that are based on linguistic intuitions."
"The reader will note that our alignment model as de-scribed above has very few factors to prevent unde-sirable alignments, such as having all French words align to the same English word."
"To guide the model to correct alignments, we employ two constraints to limit our search for the most probable alignment."
The first constraint is the one-to-one constraint[REF_CITE]: every word (except the null words e 0 and f 0 ) participates in exactly one link.
"The second constraint, known as the cohesion constraint[REF_CITE], uses the dependency tree (Mel’čuk, 1987) of the English sentence to restrict possible link combinations."
"Given the dependency tree T E , the alignment can induce a dependency tree for F[REF_CITE]."
The cohesion constraint requires that this induced dependency tree does not have any crossing dependencies.
The details about how the cohesion constraint is implemented are out-side the scope of this paper. 3
Here we will use a sim-ple example to illustrate the effect of the constraint.
Consider the partial alignment in Figure 2.
"When the system attempts to link of and de, the new link will induce the dotted dependency, which crosses a previously induced dependency between service and données."
"Therefore, of and de will not be linked."
In this section we introduce two types of features that we use in our implementation of the probabil-ity model described in Section 2.
The first feature type ft a concerns surrounding links.
It has been ob-served that words close to each other in the source language tend to remain close to each other in the translati[REF_CITE].
"To capture this notion, for any word pair (e i , f j ), if a link l(e i 0 , f j 0 ) exists where i − 2 ≤ i 0 ≤ i + 2 and j − 2 ≤ j 0 ≤ j + 2, then we say that the feature ft a (i−i 0 , j−j 0 , e i 0 ) is active for this context."
We refer to these as adjacency features.
The second feature type ft d uses the English parse tree to capture regularities among grammati-cal relations between languages.
"For example, when dealing with French and English, the location of the determiner with respect to its governor [Footnote_4] is never swapped during translation, while the location of ad-jectives is swapped frequently."
4 The parent node in the dependency tree.
"For any word pair (e i , f j ), let e i 0 be the governor of e i , and let rel be the relationship between them."
"If a link l(e i 0 ,f j 0 ) exists, then we say that the feature ft d (j −j 0 , rel) is active for this context."
We refer to these as depen-dency features.
Take for example Figure 3 which shows a par-tial alignment with all links completed except for those involving ‘the’.
"Given this sentence pair and English parse tree, we can extract features of both types to assist in the alignment of the 1 ."
"The word pair (the 1 , l 0 ) will have an active adjacency feature ft a (+1, +1, host) as well as a dependency feature ft d (−1, det)."
These two features will work together to increase the probability of this correct link.
"In contrast, the incorrect link (the 1 , les) will have only ft d (+[Footnote_3],det), which will work to lower the link probability, since most determiners are located be- fore their governors."
3 The algorithm for checking the cohesion constraint is pre-sented in a separate paper which is currently under review.
"Due to our use of constraints, when seeking the highest probability alignment, we cannot rely on a method such as dynamic programming to (implic-itly) search the entire alignment space."
"Instead, we use a best-first search algorithm (with constant beam and agenda size) to search our constrained space of possible alignments."
A state in this space is a par-tial alignment.
A transition is defined as the addi-tion of a single link to the current state.
Any link which would create a state that does not violate any constraint is considered to be a valid transition.
"Our start state is the empty alignment, where all words in E and F are linked to null."
A terminal state is a state in which no more links can be added without violat-ing a constraint.
Our goal is to find the terminal state with highest probability.
"For the purposes of our best-first search, non-terminal states are evaluated according to a greedy completion of the partial alignment."
"We build this completion by adding valid links in the order of their unmodified link probabilities P (l|e, f) until no more links can be added."
The score the state receives is the probability of its greedy completion.
These completions are saved for later use (see Section 4.2).
"As was stated in Section 2, our probability model needs an initial alignment in order to create its prob-ability tables."
"Furthermore, to avoid having our model learn mistakes and noise, it helps to train on a set of possible alignments for each sentence, rather than one Viterbi alignment."
"In the following sub-sections we describe the creation of the initial align-ments used for our experiments, as well as our sam-pling method used in training."
"We produce an initial alignment using the same al-gorithm described in Section 3, except we maximize summed φ 2 link scores[REF_CITE], rather than alignment probability."
This produces a reasonable one-to-one word alignment that we can refine using our probability model.
Our use of the one-to-one constraint and the cohe-sion constraint precludes sampling directly from all possible alignments.
These constraints tie words in such a way that the space of alignments cannot be enumerated as in IBM models 1 and 2[REF_CITE].
"Taking our lead from IBM models 3, 4 and 5, we will sample from the space of those high-probability alignments that do not violate our con-straints, and then redistribute our probability mass among our sample."
"At each search state in our alignment algorithm, we consider a number of potential links, and select between them using a heuristic completion of the re-sulting state."
"Our sample S of possible alignments will be the most probable alignment, plus the greedy completions of the states visited during search."
"It is important to note that any sampling method that concentrates on complete, valid and high probabil-ity alignments will accomplish the same task."
"When collecting the statistics needed to calcu-late P(A|E, F) from our initial φ 2 alignment, we give each s ∈ S a uniform weight."
"This is rea-sonable, as we have no probability estimates at this point."
"When training from the alignments pro-duced by our model, we normalize P(s|E,F) so that P s∈S P (s|E, F ) = 1."
We then count links and features in S according to these normalized proba-bilities.
"We adopted the same evaluation methodology as[REF_CITE], which compared alignment outputs with manually aligned sentences."
Och and Ney classify manual alignments into two categories: Sure (S) and Possible (P ) (S⊆P ).
They defined the following metrics to evaluate an alignment A: recall = |A∩S|S| | precision = |A∩P|P| | alignment error rate (AER) = |A∩S|S||++||A∩PP| |
We trained our alignment program with the same 50K pairs of sentences[REF_CITE]and tested it on the same 500 manually aligned sen-tences.
Both the training and testing sentences are from the Hansard corpus.
We parsed the training and testing corpora with Minipar. 5 We then ran the training procedure in Section 4 for three iterations.
We conducted three experiments using this methodology.
The goal of the first experiment is to compare the algorithm in Section 3 to a state-of-the-art alignment system.
The second will determine the contributions of the features.
"The third experi-ment aims to keep all factors constant except for the model, in an attempt to determine its performance when compared to an obvious alternative."
"Table 2 compares the results of our algorithm with the results[REF_CITE], where an HMM model is used to bootstrap IBM Model 4."
The rows IBM-4 F→E and IBM-4 E→F are the results ob-tained by IBM Model 4 when treating French as the source and English as the target or vice versa.
The row IBM-4 Intersect shows the results obtained by taking the intersection of the alignments produced by IBM-4 E→F and IBM-4 F→E. The row IBM-4 Refined shows results obtained by refining the inter-section of alignments in order to increase recall.
Our algorithm achieved over 44% relative error reduction when compared with IBM-4 used in ei-ther direction and a 25% relative error rate reduc-tion when compared with IBM-4 Refined.
It also achieved a slight relative error reduction when com-pared with IBM-4 Intersect.
This demonstrates that we are competitive with the methods described[REF_CITE].
"In Table 2, one can see that our algorithm is high precision, low recall."
"This was expected as our algorithm uses the one-to-one con-straint, which rules out many of the possible align-ments present in the evaluation data."
Table 3 shows the contributions of features to our al-gorithm’s performance.
The initial (φ 2 ) row is the score for the algorithm (described in Section 4.1) that generates our initial alignment.
The without fea-tures row shows the score after 3 iterations of refine-ment with an empty feature set.
Here we can see that our model in its simplest form is capable of produc-ing a significant improvement in alignment quality.
The rows with ft d only and with ft a only describe the scores after 3 iterations of training using only de-pendency and adjacency features respectively.
"The two features provide significant contributions, with the adjacency feature being slightly more important."
"The final row shows that both features can work to-gether to create a greater improvement, despite the independence assumptions made in Section 2."
"Even though we have compared our algorithm to alignments created using IBM statistical models, it is not clear if our model is essential to our perfor-mance."
This experiment aims to determine if we could have achieved similar results using the same initial alignment and search algorithm with an alter-native model.
"Without using any features, our model is similar to IBM’s Model 1, in that they both take into account only the word types that participate in a given link."
"IBM Model 1 uses P(f|e), the probability of f be-ing generated by e, while our model uses P (l|e, f), the probability of a link existing between e and f. In this experiment, we set Model 1 translation prob-abilities according to our initial φ 2 alignment, sam-pling as we described in Section 4.2."
We then use the Q n P (f j |e a j ) to evaluate candidate alignments in j=1 a search that is otherwise identical to our algorithm.
We ran Model 1 refinement for three iterations and recorded the best results that it achieved.
It is clear from Table 4 that refining our initial φ 2 alignment using IBM’s Model 1 is less effective than using our model in the same manner.
"In fact, the Model 1 refinement receives a lower score than our initial alignment."
"When viewed with no features, our proba-bility model is most similar to the explicit noise model defined[REF_CITE]."
"In fact, Melamed defines a probability distribution P (links(u, v)|cooc(u, v), λ + , λ − ) which appears to make our work redundant."
"However, this distribu-tion refers to the probability that two word types u and v are linked links(u, v) times in the entire cor-pus."
"Our distribution P(l|e, f) refers to the proba-bility of linking a specific co-occurrence of the word tokens e and f. In Melamed’s work, these probabil-ities are used to compute a score based on a prob-ability ratio."
"In our work, we use the probabilities directly."
By far the most prominent probability models in machine translation are the IBM models and their extensions.
"When trying to determine whether two words are aligned, the IBM models ask, “What is the probability that this English word generated this French word?”"
"Our model asks instead, “If we are given this English word and this French word, what is the probability that they are linked?”"
"The dis-tinction is subtle, yet important, introducing many differences."
"For example, in our model, E and F are symmetrical."
"Furthermore, we model P (l|e, f 0 ) and P (l|e, f 00 ) as unrelated values, whereas the IBM model would associate them in the translation prob-abilities t(f 0 |e) and t(f 00 |e) through the constraint P t(f|e) = 1."
"Unfortunately, by conditionalizing f on both words, we eliminate a large inductive bias."
This prevents us from starting with uniform proba-bilities and estimating parameters with EM.
"This is why we must supply the model with a noisy initial alignment, while IBM can start from an unaligned corpus."
"In the IBM framework, when one needs the model to take new information into account, one must cre-ate an extended model which can base its parame-ters on the previous model."
"In our model, new in-formation can be incorporated modularly by adding features."
"This makes our work similar to maximum entropy-based machine translation methods, which also employ modular features."
"Maximum entropy can be used to improve IBM-style translation prob-abilities by using features, such as improvements to P(f|e)[REF_CITE]."
"By the same token we can use maximum entropy to improve our esti-mates of P (l k |e i k , f j k , C k )."
We are currently inves-tigating maximum entropy as an alternative to our current feature model which assumes conditional in-dependence among features.
There have been many recent proposals to leverage syntactic data in word alignment.
"Methods such[REF_CITE],[REF_CITE]and[REF_CITE]employ a synchronous parsing procedure to constrain a statistical alignment."
The work done[REF_CITE]measures statistics on operations that transform a parse tree from one lan-guage into another.
The alignment algorithm described here is incapable of creating alignments that are not one-to-one.
"The model we describe, however is not limited in the same manner."
The model is currently capable of creating many-to-one alignments so long as the null probabilities of the words added on the “many” side are less than the probabilities of the links that would be created.
"Under the current implementation, the training corpus is one-to-one, which gives our model no opportunity to learn many-to-one alignments."
We are pursuing methods to create an extended algorithm that can handle many-to-one alignments.
"This would involve training from an initial align-ment that allows for many-to-one links, such as one of the IBM models."
"Features that are related to multiple links should be added to our set of feature types, to guide intelligent placement of such links."
"We have presented a simple, flexible, statistical model for computing the probability of an alignment given a sentence pair."
This model allows easy in-tegration of context-specific features.
Our experi-ments show that this model can be an effective tool for improving an existing word alignment.
We present a probabilistic parsing model for German trained on the Negra tree-bank.
"We observe that existing lexicalized parsing models using head-head depen-dencies, while successful for English, fail to outperform an unlexicalized baseline model for German."
Learning curves show that this effect is not due to lack of training data.
We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
"This model out-performs the baseline, achieving a labeled precision and recall of up to 74%."
This in-dicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.
"Treebank-based probabilistic parsing has been the subject of intensive research over the past few years, resulting in parsing models that achieve both broad coverage and high parsing accuracy (e.g.,[REF_CITE])."
"However, most of the ex-isting models have been developed for English and trained on the Penn Treebank[REF_CITE], which raises the question whether these models generalize to other languages, and to annotation schemes that differ from the Penn Treebank markup."
"The present paper addresses this question by proposing a probabilistic parsing model trained on Negra[REF_CITE], a syntactically annotated corpus for German."
"German has a number of syn-tactic properties that set it apart from English, and the Negra annotation scheme differs in important re-spects from the Penn Treebank markup."
While Ne-gra has been used to build probabilistic chunkers ([REF_CITE];
Lexicalization can increase parsing performance dramatically for English ([REF_CITE];
"However, the resulting performance is significantly lower than the performance of the same model for English (see Ta-ble 1)."
This paper is structured as follows.
"Section 2 re-views the syntactic properties of German, focusing on its semi-flexible wordorder."
"Section 3 describes two standard lexicalized models[REF_CITE], as well as an unlexicalized baseline model."
Section 4 presents a series of experi-ments that compare the parsing performance of these three models (and several variants) on Negra.
The results show that both lexicalized models fail to out-perform the unlexicalized baseline.
This is at odds with what has been reported for English.
Learning curves show that the poor performance of the lexi-calized models is not due to lack of training data.
"Section 5 presents an error analysis for Collins’s (1997) lexicalized model, which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra."
We pro-pose an alternative model that uses sister-head de-pendencies instead.
"This model outperforms the two original lexicalized models, as well as the unlexical-ized baseline."
"Based on this result and on the review of the previous literature (Section 6), we argue (Sec-tion 7) that sister-head models are more appropriate for treebanks with very flat structures (such as Ne-gra), typically used to annotate languages with semi-free wordorder (such as German)."
"German exhibits a number of syntactic properties that distinguish it from English, the language that has been the focus of most research in parsing."
"Prominent among these properties is the semi-free various languages (dependency precision for Czech) wordorder, i.e., German wordorder is fixed in some respects, but variable in others."
"Verb order is largely fixed: in subordinate clauses such as (1a), both the finite verb hat ‘has’ and the non-finite verb kom-poniert ‘composed’ are in sentence final position. ([Footnote_1]) a. Weil er gestern Musik komponiert hat. because er yesterday music composed has ‘Because he has composed music yesterday.’ b. Hat er gestern Musik komponiert? c."
1[REF_CITE]proposes essentially the same model; we will nevertheless use the label ‘Carroll and Rooth model’ as we are using their implementation (see Section 4.1).
Er hat gestern Musik komponiert.
"In yes/no questions such as (1b), the finite verb is sentence initial, while the non-finite verb is sen-tence final."
"In declarative main clauses (see (1c)), on the other hand, the finite verb is in second position (i.e., preceded by exactly one constituent), while the non-finite verb is final."
"While verb order is fixed in German, the order of complements and adjuncts is variable, and influ-enced by a variety of syntactic and non-syntactic factors, including pronominalization, information structure, definiteness, and animacy (e.g.,[REF_CITE])."
"The first position in a declarative sen-tence, for example, can be occupied by various con-stituents, including the subject ( er ‘he’ in (1c)), the object ( Musik ‘music’ in (2a)), an adjunct ( gestern ‘yesterday’ in (2b)), or the non-finite verb ( kom-poniert ‘composed’ in (2c)). (2) a. Musik hat er gestern komponiert. b. Gestern hat er Musik komponiert . c. Komponiert hat er gestern Musik."
The semi-free wordorder in German means that a context-free grammar model has to contain more rules than for a fixed wordorder language.
"For tran-sitive verbs, for instance, we need the rules S → V NP NP, S → NP V NP, and S → NP NP V to account for verb initial, verb second, and verb final order (assuming a flat S, see Section 2.2)."
"The Negra corpus consists of around 350,000 words of German newspaper text (20,602 sentences)."
"The annotation scheme[REF_CITE]is modeled to a certain extent on that of the Penn Treebank[REF_CITE], with crucial differences."
"Most impor-tantly, Negra follows the dependency grammar tra-dition in assuming flat syntactic representations: (a) There is no S → NP VP rule."
"Rather, the sub-ject, the verb, and its objects are all sisters of each other, dominated by an S node."
This is a way of accounting for the semi-free wordorder of German (see Section 2.1): the first NP within an S need not be the subject. (b) There is no SBAR → Comp S rule.
"Main clauses, subordinate clauses, and relative clauses all share the category S in Negra; complementizers and relative pronouns are simply sisters of the verb. (c) There is no PP → P NP rule, i.e., the prepo- sition and the noun it selects (and determiners and adjectives, if present) are sisters, dominated by a PP node."
"An argument for this representation is that prepositions behave like case markers in German; a preposition and a determiner can merge into a single word (e.g., in dem ‘in the’ becomes im )."
Another idiosyncrasy of Negra is that it assumes special coordinate categories.
"A coordinated sen-tence has the category CS, a coordinate NP has the category CNP, etc."
"While this does not make the annotation more flat, it substantially increases the number of non-terminal labels."
Negra also contains grammatical function labels that augment phrasal and lexical categories.
"Example are MO (modifier), HD (head), SB (subject), and OC (clausal object)."
"Lexicalization has been shown to improve pars-ing performance for the Penn Treebank (e.g.,[REF_CITE])."
The aim of the present paper is to test if this finding carries over to German and to the Negra cor-pus.
We therefore use an unlexicalized model as our baseline against which to test the lexicalized models.
"More specifically, we used a standard proba-bilistic context-free grammar (PCFG; see[REF_CITE])."
Each context-free rule RHS → LHS is anno-tated with an expansion probability P(RHS|LHS).
"The probabilities for all rules with the same lefthand side have to sum to one, and the probability of a parse tree T is defined as the product of the prob-abilities of all rules applied in generating T ."
"The head-lexicalized PCFG model[REF_CITE]is a minimal departure from the stan-dard unlexicalized PCFG model, which makes it ideal for a direct comparison. 1"
"A grammar rule LHS → RHS can be written as P → C 1 ...C n , where P is the mother category, and C 1 ...C n are daughters."
Let l(C) be the lexical head of the constituent C.
The rule probability is then de-fined as (see also[REF_CITE]): (3) P(RHS|LHS) =
"P rule (C 1 ...C n |P,l(P)) n · ∏ P choice (l(C i )|C i ,P,l(P)) i=1"
"Here P rule (C 1 ...C n |P,l(P)) is the probability that category P with lexical head l(P) is expanded by the rule P → C 1 ...C n , and P choice (l(C)|C,P,l(P)) is the probability that the (non-head) category C has the lexical head l(C) given that its mother is P with lex-ical head l(P)."
"In contrast to Carroll and Rooth’s (1998) approach, the model proposed[REF_CITE]does not com-pute rule probabilities directly."
"Rather, they are gen-erated using a Markov process that makes certain in-dependence assumptions."
A grammar rule LHS → RHS can be written as P → L m ...L 1 H R 1 ...R n where P is the mother and H is the head daughter.
Let l(C) be the head word of C and t(C) the tag of the head word of C.
Then the probability of a rule is defined as: (4) P(RHS|LHS) =
P(L m ...L 1 H R 1 ...R n |P) =
"P h (H|P)P l (L m ...L 1 |P,H)P r (R 1 ...R n |P ,H) m n = P h (H|P) ∏ P l (L i |P,H,d(i)) ∏ P r (R i |P,H,d(i)) i=0 i=0"
"Here, P h is the probability of generating the head, and P l and P r are the probabilities of generating the nonterminals to the left and right of the head, re-spectively; d(i) is a distance measure. (L 0 and R 0 are stop categories.)"
"At this point, the model is still un-lexicalized."
"To add lexical sensitivity, the P h , P r and P l probability functions also take into account head words and their POS tags: (5) P(RHS|LHS) ="
"P h (H|P,t(P),l(P)) m · ∏ P l (L i ,t(L i ),l(L i )|P,H,t(H),l(H),d(i)) i=0 n · ∏ P r (R i ,t(R i ),l(R i )|P,H,t(H),l(H),d(i)) i=0"
This experiment was designed to compare the per-formance of the three models introduced in the last section.
Our main hypothesis was that the lex-icalized models will outperform the unlexicalized baseline model.
Another prediction was that adding Negra-specific information to the models will in-crease parsing performance.
"We therefore tested a model variant that included grammatical function la-bels, i.e., the set of categories was augmented by the function tags specified in Negra (see Section 2.2)."
Adding grammatical functions is a way of deal-ing with the wordorder facts of German (see Sec- tion 2.1) in the face of Negra’s very flat annota-tion scheme.
"For instance, subject and object NPs have different wordorder preferences (subjects tend to be preverbal, while objects tend to be postver-bal), a fact that is captured if subjects have the la-bel NP-SB, while objects are labeled NP-OA (ac-cusative object), NP-DA (dative object), etc."
"Also the fact that verb order differs between subordinate and main clauses is captured by the function labels: the former are labeled S, while the latter are labeled S-OC (object clause), S-RC (relative clause), etc."
"Another idiosyncrasy of the Negra annotation is that conjoined categories have separate labels (S and CS, NP and CNP, etc.), and that PPs do not contain an NP node."
We tested a variant of the[REF_CITE]model that takes this into account.
Data Sets All experiments reported in this paper used the treebank format of Negra.
"This format, which is included in the Negra distribution, was de-rived from the native format by replacing crossing branches with traces."
We split the corpus into three subsets.
"Of the remaining 2,000 sentences, the first 1,000 served as the test set, and the last 1000 as the development set."
"To increase parsing efficiency, we removed all sentences with more than 40 words."
This resulted in a test set of 968 sentences and a development set of 975 sentences.
"Early versions of the models were tested on the development set, and the test set remained unseen until all parameters were fixed."
"The final results reported this paper were obtained on the test set, unless stated otherwise."
"Grammar Induction For the unlexicalized PCFG model (henceforth baseline model), we used the probabilistic left-corner parser Lopar[REF_CITE]."
"When run in unlexicalized mode, Lopar im-plements the model described in Section 3.1."
"A grammar and a lexicon for Lopar were read off the Negra training set, after removing all grammatical function labels."
"As Lopar cannot handle traces, these were also removed from the training data."
"The head-lexicalized model[REF_CITE](henceforth C&amp;R model) was again realized using Lopar, which in lexicalized mode implements the model in Section 3.2."
Lexicalization requires that each rule in a grammar has one of the categories on its righthand side annotated as the head.
"For the cate-gories S, VP, AP, and AVP, the head is marked in Ne-gra."
"For the other categories, we used rules to heuris-tically determine the head, as is standard practice for the Penn Treebank."
The lexicalized model proposed[REF_CITE](henceforth Collins model) was re-implemented by one of the authors.
"For training, empty categories were removed from the training data, as the model cannot handle them."
The same head finding strategy was applied as for the C&amp;R model.
"In this experiment, only head-head statistics were used (see (5))."
The original Collins model uses sister-head statistics for non-recursive NPs.
This will be discussed in detail in Section 5.
"Training and Testing For all three models, the model parameters were estimated using maximum likelihood estimation."
Both Lopar and the Collins model use various backoff distributions to smooth the estimates.
The reader is referred[REF_CITE]and[REF_CITE]for details.
"For the C&amp;R model, we used a cutoff of one for rule frequencies P rule and lexical choice frequencies P choice (the cutoff value was optimized on the development set)."
"We also tested variants of the baseline model and the C&amp;R model that include grammatical function information, as we hypothesized that this informa-tion might help the model to handle wordorder vari-ation more adequately, as explained above."
"Finally, we tested variant of the C&amp;R model that uses Lopar’s parameter pooling feature."
This fea-ture makes it possible to collapse the lexical choice distribution P choice for either the daughter or the mother categories of a rule (see Section 3.2).
"We pooled the estimates for pairs of conjoined and non-conjoined daughter categories (S and CS, NP and CNP, etc.): these categories should be treated as the same daughters; e.g., there should be no difference between S → NP V and S → CNP V. We also pooled the estimates for the mother categories NPs and PPs."
This is a way of dealing with the fact that there is no separate NP node within PPs in Negra.
Lopar and the Collins model differ in their han-dling of unknown words.
"In Lopar, a POS tag distri-bution for unknown words has to be specified, which is then used to tag unknown words in the test data."
The Collins model treats any word seen fewer than five times in the training data as unseen and uses an external POS tagger to tag unknown words.
"In order to make the models comparable, we used a uniform approach to unknown words."
"All models were run on POS-tagged input; this input was created by tag-ging the test set with a separate POS tagger, for both known and unknown words."
"We used TnT[REF_CITE], trained on the Negra training set."
The tagging accuracy was 97.12% on the development set.
"In order to obtain an upper bound for the perfor-mance of the parsing models, we also ran the parsers on the test set with the correct tags (as specified in Negra), again for both known and unknown words."
We will refer to this mode as ‘perfect tagging’.
All models were evaluated using standard PAR -
"We report labeled recall (LR) labeled precision (LP), average crossing brackets (CBs), zero crossing brackets (0CB), and two or less crossing brackets (≤2CB)."
"We also give the cover-age (Cov), i.e., the percentage of sentences that the parser was able to parse."
"The results for all three models and their variants are given in Table 2, for both TnT tags and per-fect tags."
The baseline model achieves 70.56%[REF_CITE].69% LP with TnT tags.
"Adding grammatical functions reduces both figures slightly, and cover-age drops by about 15%."
"The C&amp;R model performs worse than the baseline, at 68.04%[REF_CITE].07% LP (for TnT tags)."
Adding grammatical function again reduces performance slightly.
Parameter pool-ing increases both LR and LP by about 1%.
"The Collins models also performs worse than the base-line, at 67.91%[REF_CITE].07% LP."
Performance using perfect tags (an upper bound of model performance) is 2–3% higher for the base-line and for the C&amp;R model.
The Collins model gains only about 1%.
Perfect tagging results in a per-formance increase of over 10% for the models with grammatical functions.
"This is not surprising, as the perfect tags (but not the TnT tags) include grammat-ical function labels."
"However, we also observe a dra-matic reduction in coverage (to about 65%)."
"We added grammatical functions to both the base-line model and the C&amp;R model, as we predicted that this would allow the model to better capture the wordorder facts of German."
"However, this predic-tion was not borne out: performance with grammat-ical functions (on TnT tags) was slightly worse than without, and coverage dropped substantially."
"A pos-sible reason for this is sparse data: a grammar aug-mented with grammatical functions contains many additional categories, which means that many more parameters have to be estimated using the same training set."
"On the other hand, a performance in-crease occurs if the tagger also provides grammati-cal function labels (simulated in the perfect tags con-dition)."
"However, this comes at the price of an unac-ceptable reduction in coverage."
"When training the C&amp;R model, we included a variant that makes use of Lopar’s parameter pool-ing feature."
"We pooled the estimates for conjoined daughter categories, and for NP and PP mother cat-egories."
"This is a way of taking the idiosyncrasies of the Negra annotation into account, and resulted in a small improvement in performance."
The most surprising finding is that the best per-formance was achieved by the unlexicalized PCFG baseline model.
Both lexicalized models (C&amp;R and Collins) performed worse than the baseline.
"This re-sults is at odds with what has been found for En-glish, where lexicalization is standardly reported to increase performance by about 10%."
"The poor per-formance of the lexicalized models could be due to a lack of sufficient training data: our Negra training set contains approximately 18,000 sentences, and is therefore significantly smaller than the Penn Tree-bank training set (about 40,000 sentences)."
"Negra sentences are also shorter: they contain, on average, 15 words compared to 22 in the Penn Treebank."
We computed learning curves for the unmodified variants (without grammatical functions or parame-ter pooling) of all three models (on the development set).
The result (see Figure 1) shows that there is no evidence for an effect of sparse data.
"For both the baseline and the C&amp;R model, a fairly high f-score is achieved with only 10% of the training data."
A slow increase occurs as more training data is added.
The performance of the Collins model is even less affected by training set size.
"This is probably due to the fact that it does not use rule probabilities directly, but generates rules using a Markov chain."
"As we saw in the last section, lack of training data is not a plausible explanation for the sub-baseline per-formance of the lexicalized models."
"In this experi-ment, we therefore investigate an alternative hypoth-esis, viz., that the lexicalized models do not cope well with the fact that Negra rules are so flat (see Section 2.2)."
"We will focus on the Collins model, as it outperformed the C&amp;R model in Experiment 1."
An error analysis revealed that many of the errors of the Collins model in Experiment 1 are chunking errors.
"For example, the PP neben den Mitteln des Theaters should be analyzed as (6a)."
But instead the parser produces two constituents as in (6b)): (6) a. [PP neben den Mitteln [NP des Theaters]] apart the means the theater’s ‘apart from the means of the theater’. b. [PP neben den Mitteln] [NP des Theaters]
"The reason for this problem is that neben is the head of the constituent in (6), and the Collins model uses a crude distance measure together with head-head dependencies to decide if additional constituents should be added to the PP."
The distance measure is inadequate for finding PPs with high precision.
The chunking problem is more widespread than PPs.
"The error analysis shows that other con-stituents, including Ss and VPs, also have the wrong boundary."
"This problem is compounded by the fact that the rules in Negra are substantially flatter than the rules in the Penn Treebank, for which the Collins model was developed."
Table 3 compares the average number of daughters in both corpora.
The flatness of PPs is easy to reduce.
"As detailed in Section 2.2, PPs lack an intermediate NP projec-tion, which can be inserted straightforwardly using the following rule:"
"In the present experiment, we investigated if parsing performance improves if we test and train on a ver-sion of Negra on which the transformation in (7) has been applied."
"In a second series of experiments, we investigated a more general way of dealing with the flatness of"
"Negra, based on Collins’s (1997) model for non-recursive NPs in the Penn Treebank (which are also flat)."
"For non-recursive NPs,[REF_CITE]does not use the probability function in (5), but instead sub-stitutes P r (and, by analogy, P l ) by: (8) P r (R i ,t(R i ),l(R i )|P,R i−1 ,t(R i−1 ),l(R i−1 ),d(i))"
Here the head H is substituted by the sister R i−1 (and L i−1 ).
"In the literature, the version of P r in (5) is said to capture head-head relationships."
We will refer to the alternative model in (8) as capturing sister-head relationships.
Using sister-head relationships is a way of coun-teracting the flatness of the grammar productions; it implicitly adds binary branching to the grammar.
Our proposal is to extend the use of sister-head re-lationship from non-recursive NPs (as proposed by Collins) to all categories.
"Table 4 shows the linguistic features of the result-ing model compared to the models[REF_CITE],[REF_CITE], and[REF_CITE]."
"The C&amp;R model effectively includes category infor-mation about all previous sisters, as it uses context-free rules."
"Charniak’s (2000) model extends this to higher order Markov chains (first to third order), and therefore includes category information about previ-ous sisters."
"The current model differs from all these proposals: it does not use any information about the head sister, but instead includes the category, head word, and head tag of the previous sister, effectively treating it as the head."
We first trained the original Collins model on a mod-ified versions of the training test from Experiment 1 in which the PPs were split by applying rule (7).
"In a second series of experiments, we tested a range of models that use sister-head dependencies instead of head-head dependencies for different cat-egories."
"We first added sister-head dependencies for NPs (following Collins’s (1997) original proposal) and then for PPs, which are flat in Negra, and thus similar in structure to NPs (see Section 2.2)."
Then we tested a model in which sister-head relationships are applied to all categories.
"In a third series of experiments, we trained mod-els that use sister-head relationships everywhere ex-cept for one category."
This makes it possible to de-termine which sister-head dependencies are crucial for improving performance of the model.
The results of the PP experiment are listed in Ta-ble 5.
"Again, we give results obtained using TnT tags and using perfect tags."
The row ‘Split PP’ contains the performance figures obtained by including split PPs in both the training and in the testing set.
This leads to a substantial increase in LR (6–7%) and LP (around 8%) for both tagging schemes.
"Note, how-ever, that these figures are not directly comparable to the performance of the unmodified Collins model: it is possible that the additional brackets artificially in-flate LR and LP."
"Presumably, the brackets for split PPs are easy to detect, as they are always adjacent to a preposition."
"An honest evaluation should therefore train on the modified training set (with split PPs), but collapse the split categories for testing, i.e., test on the unmodified test set."
The results for this evalu-ation are listed in rows ‘Collapsed PP’.
"Now there is no increase in performance compared to the unmod-ified Collins model; rather, a slight drop in LR and LP is observed."
Table 5 also displays the results of our exper-iments with the sister-head model.
"For TnT tags, we observe that using sister-head dependencies for NPs leads to a small decrease in performance com-pared to the unmodified Collins model, resulting in 67.84%[REF_CITE].96% LP."
"Sister-head dependen-cies for PPs, however, increase performance sub-stantially to 70.27%[REF_CITE].45% LP."
"The high-est improvement is observed if head-sister depen-dencies are used for all categories; this results in 71.32%[REF_CITE].93% LP, which corresponds to an improvement of 3% in LP and 5% in LR compared to the unmodified Collins model."
Performance with perfect tags is around 2–4% higher than with TnT tags.
"For perfect tags, sister-head dependencies lead to an improvement for NPs, PPs, and all categories."
The third series of experiments was designed to determine which categories are crucial for achiev-ing this performance gain.
This was done by train-ing models that use sister-head dependencies for all categories but one.
Table 6 shows the change in LR and LP that was found for each individual category (again for TnT tags and perfect tags).
The highest drop in performance (around 3%) is observed when the PP category is reverted to head-head dependen-cies.
"For S and for the coordinated categories (CS,"
"CNP, etc.), a drop in performance of around 1% each is observed."
A slight drop is observed also for VP (around 0.5%).
"Only minimal fluctuations in perfor-mance are observed when the other categories are removed (AP, AVP, and NP): there is a small effect (around 0.5%) if TnT tags are used, and almost no effect for perfect tags."
We showed that splitting PPs to make Negra less flat does not improve parsing performance if test-ing is carried out on the collapsed categories.
"How-ever, we observed that LR and LP are artificially in-flated if split PPs are used for testing."
"This finding goes some way towards explaining why the parsing performance reported for the Penn Treebank is sub-stantially higher than the results for Negra: the Penn Treebank contains split PPs, which means that there are lot of brackets that are easy to get right."
"The re-sulting performance figures are not directly compa-rable to figures obtained on Negra, or other corpora with flat PPs. [Footnote_2]"
"2 This result generalizes to Ss, which are also flat in Negra (see Section 2.2). We conducted an experiment in which we added an SBAR above the S. No increase in performance was obtained if the evaluation was carried using collapsed Ss."
We also obtained a positive result: we demon-strated that a sister-head model outperforms the un-lexicalized baseline model (unlike the C&amp;R model and the Collins model in Experiment 1).
LR was about 1% higher and LP about 4% higher than the baseline if lexical sister-head dependencies are used for all categories.
This holds both for TnT tags and for perfect tags (compare Tables 2 and 5).
We also found that using lexical sister-head dependencies for all categories leads to a larger improvement than us-ing them only for NPs or PPs (see Table 5).
"This result was confirmed by a second series of experi-ments, where we reverted individual categories back to head-head dependencies, which triggered a de-crease in performance for all categories, with the ex-ception of NP, AP, and AVP (see Table 6)."
"On the whole, the results of Experiment [Footnote_2] are at odds with what is known about parsing for English."
"2 This result generalizes to Ss, which are also flat in Negra (see Section 2.2). We conducted an experiment in which we added an SBAR above the S. No increase in performance was obtained if the evaluation was carried using collapsed Ss."
"The progression in the probabilistic parsing litera-ture has been to start with lexical head-head depen-dencies[REF_CITE]and then add non-lexical sis- ter informati[REF_CITE], as illustrated in Table 4."
"Lexical sister-head dependencies have only been found useful in a limited way: in the original Collins model, they are used for non-recursive NPs."
"Our results show, however, that for parsing Ger-man, lexical sister-head information is more im-portant than lexical head-head information."
Only a model that replaced lexical head-head with lexical sister-head dependencies was able to outperform a baseline model that uses no lexicalization. [Footnote_3]
"3 It is unclear what effect bi-lexical statistics have on the sister-head model; while[REF_CITE]shows bi-lexical statis-tics are sparse for some grammars,[REF_CITE]found they play a greater role in binarized grammars."
"Based on the error analysis for Experiment 1, we claim that the reason for the success of the sister-head model is the fact that the rules in Negra are so flat; using a sister-head model is a way of binarizing the rules."
"There are currently no probabilistic, treebank-trained parsers available for German (to our knowl-edge)."
"A number of chunking models have been pro-posed, however."
"Using cascaded Markov models,[REF_CITE]reports an improved performance on the same task ([REF_CITE].4%,[REF_CITE].3%)."
They re-port an[REF_CITE]%.
"The head-lexicalized model[REF_CITE]has been applied to German by Beil et al. (1999, 2002)."
"However, this approach differs in the number of ways from the results reported here: (a) a hand-written grammar (instead of a treebank gram-mar) is used; (b) training is carried out on unan-notated data; (c) the grammar and the training set cover only subordinate and relative clauses, not un-restricted text."
They also report the results of a task-based eval-uation (extraction of sucategorization frames).
There is some research on treebank-based pars-ing of languages other than English.
The work[REF_CITE]and[REF_CITE]has demonstrated the applicability of the[REF_CITE]model for Czech and Chinese.
"The perfor-mance reported by these authors is substantially lower than the one reported for English, which might be due to the fact that less training data is avail-able for Czech and Chinese (see Table 1)."
"This hy-pothesis cannot be tested, as the authors do not present learning curves for their models."
"However, the learning curve for Negra (see Figure 1) indicates that the performance of the[REF_CITE]model is stable, even for small training sets."
"As Experiment 1 showed, this cannot be taken for granted."
"We presented the first probabilistic full parsing model for German trained on Negra, a syntactically annotated corpus."
"This model uses lexical sister-head dependencies, which makes it particularly suit-able for parsing Negra’s flat structures."
"The flatness of the Negra annotation reflects the syntactic proper-ties of German, in particular its semi-free wordorder."
"In Experiment 1, we applied three standard pars-ing models from the literature to Negra: an un-lexicalized PCFG model (the baseline), Carroll and Rooth’s (1998) head-lexicalized model, and Collins’s (1997) model based on head-head depen-dencies."
The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.
Both lexicalized models perform substan-tially worse.
This finding is at odds with what has been reported for parsing models trained on the Penn Treebank.
As a possible explanation we considered lack of training data: Negra is about half the size of the Penn Treebank.
"However, the learning curves for the three models failed to produce any evidence that they suffer from sparse data."
"In Experiment 2, we therefore investigated an al-ternative hypothesis: the poor performance of the lexicalized models is due to the fact that the rules in Negra are flatter than in the Penn Treebank, which makes lexical head-head dependencies less useful for correctly determining constituent boundaries."
"Based on this assumption, we proposed an alterna-tive model hat replaces lexical head-head dependen-cies with lexical sister-head dependencies."
This can the thought of as a way of binarizing the flat rules in Negra.
"The results show that sister-head dependen-cies improve parsing performance not only for NPs (which is well-known for English), but also for PPs, VPs, Ss, and coordinate categories."
The best perfor-mance was obtained for a model that uses sister-head dependencies for all categories.
"This model achieves up to 74% recall and precision, thus outperforming the unlexicalized baseline model."
It can be hypothesized that this finding carries over to other treebanks that are annotated with flat structures.
Such annotation schemes are often used for languages that (unlike English) have a free or semi-free wordorder.
Testing our sister-head model on these languages is a topic for future research.
"We present a novel, data-driven method for integrated shallow and deep parsing."
"Mediated by an XML-based multi-layer annotation architecture, we interleave a robust, but accurate stochastic topological field parser of German with a constraint-based HPSG parser."
"Our annotation-based method for dovetailing shallow and deep phrasal constraints is highly flexible, al-lowing targeted and fine-grained guidance of constraint-based parsing."
We conduct systematic experiments that demonstrate substantial performance gains. [Footnote_1]
1 This work was in part supported by a BMBF grant to the DFKI project WHITEBOARD ([REF_CITE]).
One of the strong points of deep processing (DNLP) technology such as HPSG or LFG parsers certainly lies with the high degree of precision as well as detailed linguistic analysis these systems are able to deliver.
"Although considerable progress has been made in the area of processing speed, DNLP systems still cannot rival shallow and medium depth tech-nologies in terms of throughput and robustness."
"As a net effect, the impact of deep parsing technology on application-oriented NLP is still fairly limited."
With the advent of XML-based hybrid shallow-deep architectures as presented[REF_CITE]it has become possible to integrate the added value of deep processing with the performance and robustness of shallow processing.
"So far, integration has largely focused on the lexical level, to improve upon the most urgent needs in increasing the robust-ness and coverage of deep parsing systems, namely lexical coverage."
"While integration[REF_CITE]was still restricted to morphologi-cal and PoS information,[REF_CITE]ex-tended shallow-deep integration at the lexical level to lexico-semantic information, and named entity expressions, including multiword expressions.[REF_CITE]assume a vertical, ‘pipeline’ scenario where shallow NLP tools provide XML annotations that are used by the DNLP system as a preprocessing and lexical interface."
"The per-spective opened up by a multi-layered, data-centric architecture is, however, much broader, in that it en-courages horizontal cross-fertilisation effects among complementary and/or competing components."
"One of the culprits for the relative inefficiency of DNLP parsers is the high degree of ambiguity found in large-scale grammars, which can often only be re-solved within a larger syntactic domain."
Within a hy-brid shallow-deep platform one can take advantage of partial knowledge provided by shallow parsers to pre-structure the search space of the deep parser.
"In this paper, we will thus complement the efforts made on the lexical side by integration at the phrasal level."
We will show that this may lead to considerable per-formance increase for the DNLP component.
"More specifically, we combine a probabilistic topological field parser for German[REF_CITE]with the HPSG parser[REF_CITE]."
"The HPSG grammar used is the one originally developed[REF_CITE], with significant per-formance enhancements by B. Crysmann."
In Section 2 we discuss the mapping problem involved with syntactic integration of shallow and deep analyses and motivate our choice to combine the HPSG system with a topological parser.
Sec-tion 3 outlines our basic approach towards syntactic shallow-deep integration.
"Section 4 introduces vari-ous confidence measures, to be used for fine-tuning of phrasal integration."
"Sections 5 and 6 report on experiments and results of integrated shallow-deep parsing, measuring the effect of various integra-tion parameters on performance gains for the DNLP component."
"Section 7 concludes and discusses pos-sible extensions, to address robustness issues."
The prime motivation for integrated shallow-deep processing is to combine the robustness and effi-ciency of shallow processing with the accuracy and fine-grainedness of deep processing.
"Shallow analy-ses could be used to pre-structure the search space of a deep parser, enhancing its efficiency."
"Even if deep analysis fails, shallow analysis could act as a guide to select partial analyses from the deep parser’s chart – enhancing the robustness of deep analysis, and the informativeness of the combined system."
"In this paper, we concentrate on the usage of shal-low information to increase the efficiency, and po-tentially the quality, of HPSG parsing."
"In particu-lar, we want to use analyses delivered by an effi-cient shallow parser to pre-structure the search space of HPSG parsing, thereby enhancing its efficiency, and guiding deep parsing towards a best-first analy-sis suggested by shallow analysis constraints."
"The search space of an HPSG chart parser can be effectively constrained by external knowledge sources if these deliver compatible partial subtrees, which would then only need to be checked for com-patibility with constituents derived in deep pars-ing."
Raw constituent span information can be used to guide the parsing process by penalizing con-stituents which are incompatible with the precom-puted ‘shape’.
"Additional information about pro-posed constituents, such as categorial or featural constraints, provide further criteria for prioritis-ing compatible, and penalising incompatible con-stituents in the deep parser’s chart."
An obvious challenge for our approach is thus to identify suitable shallow knowledge sources that can deliver compatible constraints for HPSG parsing.
"However, chunks delivered by state-of-the-art shal-low parsers are not isomorphic to deep syntactic analyses that explicitly encode phrasal embedding structures."
"As a consequence, the boundaries of deep grammar constituents in (1.a) cannot be pre-determined on the basis of a shallow chunk analy-sis (1.b)."
"Moreover, the prevailing greedy bottom-up processing strategies applied in chunk parsing do not take into account the macro-structure of sentences."
They are thus easily trapped in cases such as (2). (1) a. [ CL There was [ NP a rumor [ CL it was going to be bought by [ NP a French company [ CL that competes in supercomputers]]]]]. b. [ CL There was [ NP a rumor]] [ CL it was going to be bought by [ NP a French company]] [ CL that competes in supercomputers]. (2) Fred eats [ NP pizza and Mary] drinks wine.
"In sum, state-of-the-art chunk parsing does nei-ther provide sufficient detail, nor the required accu-racy to act as a ‘guide’ for deep syntactic analysis."
"Recently, there is revived interest in shallow anal-yses that determine the clausal macro-structure of sentences."
"The topological field model of (German) syntax[REF_CITE]divides basic clauses into dis-tinct fields – pre-, middle-, and post-fields – delim-ited by verbal or sentential markers, which consti-tute the left/right sentence brackets."
"This model of clause structure is underspecified, or partial as to non-sentential constituent structure, but provides a theory-neutral model of sentence macro-structure."
"Due to its linguistic underpinning, the topologi-cal field model provides a pre-partitioning of com-plex sentences that is (i) highly compatible with deep syntactic analysis, and thus (ii) maximally ef-fective to increase parsing efficiency if interleaved with deep syntactic analysis; (iii) partiality regarding the constituency of non-sentential material ensures robustness, coverage, and processing efficiency.[REF_CITE]explored a corpus-based stochastic approach to topological field pars-ing, by training a non-lexicalised PCFG on a topo-logical corpus derived from the NEGRA treebank of German."
"Measured on the basis of hand-corrected PoS-tagged input as provided by the NEGRA tree-bank, the parser achieves 100% coverage for length 40 (99.8% for all)."
Labelled precision and recall are around 93%.
Perfect match (full tree identity) is about 80% (cf.
"Table 1, disamb +)."
"In this paper, the topological parser was provided a tagger front-end for free text processing, using the TnT tagger[REF_CITE]."
The grammar was ported to the efficient LoPar parser[REF_CITE].
"Tag-ging inaccuracies lead to a drop of 5.1/4.7 percent- age points in LP/LR, and 8.3 percentage points in perfect match rate (Table 1, disamb )."
"As seen in Figure 1, the topological trees abstract away from non-sentential constituency – phrasal fields MF (middle-field) and VF (pre-field) directly expand to PoS tags."
"By contrast, they perfectly ren-der the clausal skeleton and embedding structure of complex sentences."
"In addition, parameterised cate-gory labels encode larger syntactic contexts, or ‘con-structions’, such as clause type ( CL - V 2, - SUBCL , - REL ), or inflectional patterns of verbal clusters ( RK - VFIN ,- VPART )."
"These properties, along with their high accuracy rate, make them perfect candidates for tight integration with deep syntactic analysis."
"Moreover, due to the combination of scrambling and discontinuous verb clusters in German syntax, a deep parser is confronted with a high degree of local ambiguity that can only be resolved at the clausal level."
"Highly lexicalised frameworks such as HPSG, however, do not lend themselves naturally to a top-down parsing strategy."
Using topological analyses to guide the HPSG will thus provide external top-down information for bottom-up parsing.
"Our work aims at integration of topological and HPSG parsing in a data-centric architecture, where each component acts independently [Footnote_2] – in contrast to the combination of different syntactic formalisms within a unified parsing process. [Footnote_3] Data-based inte-gration not only favours modularity, but facilitates flexible and targeted dovetailing of structures."
2 See Section 6 for comparison to recent work on integrated chunk-based and dependency parsing[REF_CITE].
"3 As, for example,[REF_CITE]."
"While structurally similar, topological trees are not fully isomorphic to HPSG structures."
"In Figure 1, e.g., the span from the verb ‘hätte’ to the end of the sentence forms a constituent in the HPSG analysis, while in the topological tree the same span is domi-nated by a sequence of categories: LK , MF , RK , NF ."
"Yet, due to its linguistic underpinning, the topo-logical tree can be used to systematically predict key constituents in the corresponding ‘target’ HPSG analysis."
"We know, for example, that the span from the fronted verb ( LK - VFIN ) till the end of its clause CL - V 2 corresponds to an HPSG phrase."
"Also, the first position that follows this verb, here the leftmost daughter of MF , demarcates the left edge of the tra-ditional VP. Spans of the vorfeld VF and clause cat-egories CL exactly match HPSG constituents."
"Cate-gory CL - V 2 tells us that we need to reckon with a fronted verb in position of its LK daughter, here 3, while in CL - SUBCL we expect a complementiser in the position of LK , and a finite verb within the right verbal complex RK , which spans positions 12 to 13."
"In order to communicate such structural con-straints to the deep parser, we scan the topological tree for relevant configurations, and extract the span information for the target HPSG constituents."
"The resulting ‘map constraints’ (Fig. 1) encode a bracket type name [Footnote_4] that identifies the target constituent and its left and right boundary, i.e. the concrete span in the sentence under consideration."
4 We currently extract 34 different bracket types.
"The span is en-coded by the word position index in the input, which is identical for the two parsing processes. [Footnote_5]"
"5 We currently assume identical tokenisation, but could ac-commodate for distinct tokenisation regimes, using map tables."
"In addition to pure constituency constraints, a skilled grammar writer will be able to associate spe-cific HPSG grammar constraints – positive or neg-ative – with these bracket types."
"These additional constraints will be globally defined, to permit fine-grained guidance of the parsing process."
This and further information (cf. Section 4) is communicated to the deep parser by way of an XML interface.
"In the annotation-based architecture[REF_CITE], XML-encoded analysis results of all components are stored in a multi-layer XML chart."
The architecture employed in this paper improves[REF_CITE]by providing a central Whiteboard Annotation Transformer (WHAT) that supports flexible and powerful access to and trans-formation of XML annotation based on standard XSLT engines [Footnote_6] (see[REF_CITE]for more de-tails on WHAT).
"6 Advantages we see in the XSLT approach are (i) minimised programming effort in the target implementation language for XML access, (ii) reuse of transformation rules in multiple mod-ules, (iii) fast integration of new XML-producing components."
Shallow-deep integration is thus fully annotation driven.
"Complex XSLT transforma-tions are applied to the various analyses, in order to extract or combine independent knowledge sources, including XPath access to information stored in shallow annotation, complex XSLT transformations to the output of the topological parser, and extraction of bracket constraints."
"The HPSG parser is an active bidirectional chart parser which allows flexible parsing strategies by us-ing an agenda for the parsing tasks. [Footnote_7] To compute pri-orities for the tasks, several information sources can be consulted, e.g. the estimated quality of the parti-cipating edges or external resources like PoS tagger results."
7 A parsing task encodes the possible combination of a pas-sive and an active chart edge.
"Object-oriented implementation of the prior-ity computation facilitates exchange and, moreover, combination of different ranking strategies."
"Extend-ing our current regime that uses PoS tagging for pri-oritisation, [Footnote_8] we are now utilising phrasal constraints (brackets) from topological analysis to enhance the hand-crafted parsing heuristic employed so far."
8 See e.g. (Prins and van[REF_CITE]) for related work.
Conditions for changing default priorities Ev-ery bracket pair br x computed from the topological analysis comes with a bracket type x that defines its behaviour in the priority computation.
Each bracket type can be associated with a set of positive and neg-ative constraints that state a set of permissible or for-bidden rules and/or feature structure configurations for the HPSG analysis.
"The bracket types fall into three main categories: left-, right-, and fully matching brackets."
"A right-matching bracket may affect the priority of tasks whose resulting edge will end at the right bracket of a pair, like, for example, a task that would combine edges C and F or C and D in Fig. 2."
Left-matching brackets work analogously.
"For fully matching brackets, only tasks that produce an edge that matches the span of the bracket pair can be af-fected, like, e.g., a task that combines edges B and C in Fig. 2."
"If, in addition, specified rule as well as fea-ture structure constraints hold, the task is rewarded if they are positive constraints, and penalised if they are negative ones."
"All tasks that produce crossing edges, i.e. where one endpoint lies strictly inside the bracket pair and the other lies strictly outside, are penalised, e.g., a task that combines edges A and B."
This behaviour can be implemented efficiently when we assume that the computation of a task pri- ority takes into account the priorities of the tasks it builds upon.
This guarantees that the effect of chang-ing one task in the parsing process will propagate to all depending tasks without having to check the bracket conditions repeatedly.
"For each task, it is sufficient to examine the start-and endpoints of the building edges to determine if its priority is affected by some bracket."
Only four cases can occur: 1.
The new edge spans a pair of brackets: a match 2.
"The new edge starts or ends at one of the brack-ets, but does not match: left or right hit 3."
One bracket of a pair is at the joint of the build-ing edges and a start- or endpoint lies strictly inside the brackets: a crossing (edges A and B in Fig. 2) 4.
No bracket at the endpoints of both edges: use the default priority
"For left-/right-matching brackets, a match behaves exactly like the corresponding left or right hit."
"Computing the new priority If the priority of a task is changed, the change is computed relative to the default priority."
"We use two alternative confi-dence values, and a hand-coded parameter ( x ) , to adjust the impact on the default priority heuristics. conf ent (br x ) specifies the confidence for a concrete bracket pair br x of type x in a given sentence, based on the tree entropy of the topological parse. conf pr specifies a measure of ’expected accuracy’ for each bracket type."
Sec. 4 will introduce these measures.
The priority p ( t ) of a task t involving a bracket br x is computed from the default priority p ~( t ) by: p ( t ) = p ~( t ) (1 conf ent( br x) conf pr( x ) ( x ))
This way of calculating priorities allows flexible pa-rameterisation for the integration of bracket con-straints.
"While the topological parser’s accuracy is high, we need to reckon with (partially) wrong anal-yses that could counter the expected performance gains."
"An important factor is therefore the confi-dence we can have, for any new sentence, into the best parse delivered by the topological parser:"
"If confidence is high, we want it to be fully considered for prioritisation – if it is low, we want to lower its impact, or completely ignore the proposed brackets."
"We will experiment with two alternative confi-dence measures: (i) expected accuracy of particular bracket types extracted from the best parse deliv-ered, and (ii) tree entropy based on the probability distribution encountered in a topological parse, as a measure of the overall accuracy of the best parse proposed – and thus the extracted brackets. [Footnote_9]"
"9 Further measures are conceivable: We could extract brack-ets from some n-best topological parses, associating them with weights, using methods similar[REF_CITE]."
"To determine a measure of ‘expected accuracy’ for the map constraints, we computed precision and re-call for the 34 bracket types by comparing the ex-tracted brackets from the suite of best delivered topological parses against the brackets we extracted from the trees in the manually annotated evalua-tion corpus[REF_CITE]."
We chose precision of extracted bracket types as a static confidence weight for prioritisation.
"Precision figures are distributed as follows: 26.5% of the bracket types have precision 90% (93.1% in avg, 53.5% of bracket mass), 50% have pre-cision 80% (88.9% avg, 77.7% bracket mass). 20.6% have precision 50% (41.26% in avg, 2.7% bracket mass)."
"For experiments using a threshold on conf pr (x) for bracket type x , we set a threshold value of 0.7, which excludes 32.35% of the low-confidence bracket types (and 22.1% bracket mass), and includes chunk-based brackets (see Section 5)."
"While precision over bracket types is a static mea-sure that is independent from the structural complex-ity of a particular sentence, tree entropy is defined as the entropy over the probability distribution of the set of parsed trees for a given sentence."
"It is a use-ful measure to assess how certain the parser is about the best analysis, e.g. to measure the training utility value of a data point in the context of sample selec-ti[REF_CITE]."
"We thus employ tree entropy as a confidence measure for the quality of the best topo-logical parse, and the extracted bracket constraints."
"We carry out an experiment to assess the effect of varying entropy thresholds on precision and re-call of topological parsing, in terms of perfect match rate, and show a way to determine an optimal value for ."
"We compute tree entropy over the full prob-ability distribution, and normalise the values to be distributed in a range between 0 and 1."
The normali-sation factor is empirically determined as the highest entropy over all sentences of the training set. [Footnote_10] Experimental setup We randomly split the man-ually corrected evaluation corpus[REF_CITE](for sentence length 40 ) into a train-ing set of 600 sentences and a test set of 408 sen-tences.
10 Possibly higher values in the test set will be clipped to 1.
"This yields the following values for the train-ing set (test set in brackets): initial perfect match rate is 73.5% (70.0%),[REF_CITE].8% (87.6%), and[REF_CITE].5% (87.8%). [Footnote_11][REF_CITE].8% for both."
"11 Evaluation figures for this experiment are given disregard-ing parameterisation (and punctuation), corresponding to the first row of figures in table 1."
"Evaluation measures For the task of identifying the perfect matches from a set of parses we give the following standard definitions: precision is the pro-portion of selected parses that have a perfect match – thus being the perfect match rate, and recall is the proportion of perfect matches that the system se-lected."
Coverage is usually defined as the proportion of attempted analyses with at least one parse.
We ex-tend this definition to treat successful analyses with a high tree entropy as being out of coverage.
"Fig. 3 shows the effect of decreasing entropy thresholds on precision, recall and coverage."
"The unfiltered set of all sentences is found at =1. Lowering in- creases precision, and decreases recall and coverage."
We determine f-measure as composite measure of precision and recall with equal weighting ( =0.5).
Results We use f-measure as a target function on the training set to determine a plausible .
"F-measure is maximal at =0.236 with 88.9%, see Figure 4."
Precision and recall are 83.7% and 94.8% resp. while coverage goes down to 83.0%.
"Applying the same on the test set, we get the following results: 80.5% precision, 93.0% recall."
Coverage goes down to 80.6%.
Confidence Measure We distribute the comple-ment of the associated tree entropy of a parse tree tr as a global confidence measure over all brackets br extracted from that parse: conf ent( br ) = 1 ent ( tr ) .
"For the thresholded version of conf ent( br ) , we set the threshold to 1 = 1 0 : 236 = 0 : 764 ."
"Experimental Setup In the experiments we use the subset of the NEGRA corpus (5060 sents, 24.57%) that is currently parsed by the HPSG gram-mar. [Footnote_12]"
12 This test set is different from the corpus used in Section 4.
"Average sentence length is 8.94, ignoring punctuation; average lexical ambiguity is 3.05 en-tries/word."
"As baseline, we performed a run with-out topological information, yet including PoS pri-oritisation from tagging. [Footnote_13]"
"13 In a comparative run without PoS-priorisation, we estab-lished a speed-up factor of 1.13 towards the baseline used in our experiment, with a slight increase in coverage (1%). This compares to a speed-up factor of 2.26 reported[REF_CITE], by integration of PoS guidance into a dependency parser."
A series of tests explores the effects of alternative parameter settings.
We fur-ther test the impact of chunk information.
"To this end, phrasal fields determined by topological pars-ing were fed to the chunk parser[REF_CITE]."
"Extracted NP and PP bracket constraints are defined as left-matching bracket types, to compen-sate for the non-embedding structure of chunks."
"Chunk brackets are tested in conjunction with topo-logical brackets, and in isolation, using the labelled precision value of 71.1%[REF_CITE]as a uniform confidence weight. [Footnote_14]"
"14 The experiments were run on a 700 MHz Pentium III ma-chine. For all runs, the maximum number of passive edges was set to the comparatively high value of 70000."
Measures For all runs we measure the absolute time and the number of parsing tasks needed to com-pute the first reading.
The times in the individual runs were normalised according to the number of executed tasks per second.
"We noticed that the cov-erage of some integrated runs decreased by up to 1% of the 5060 test items, with a typical loss of around 0.5%."
"To warrant that we are not just trading coverage for speed, we derived two measures from the primary data: an upper bound, where we asso-ciated every unsuccessful parse with the time and number of tasks used when the limit of 70000 pas-sive edges was hit, and a lower bound, where we removed the most expensive parses from each run, until we reached the same coverage."
"Whereas the upper bound is certainly more realistic in an applica-tion context, the lower bound gives us a worst case estimate of expectable speed-up."
Integration Parameters We explored the follow-ing range of weighting parameters for prioritisation (see Section 3.3 and Table 2).
We use two global settings for the heuristic pa-rameter .
"Setting to 1 drastically increases the influence of topological information, the priority for rewarded tasks is doubled and set to zero for penalized ones."
"The first two runs (rows with P E) ignore both confidence parameters (conf pr=ent =1), measur-ing only the effect of higher or lower influence of topological information."
"In the remaining six runs, the impact of the confidence measures conf pr=ent is tested individually, namely +P E and P +E, by setting the resp. alternative value to 1."
"For two runs, we set the resp. confidence values that drop below a certain threshold to zero (PT, ET) to exclude un- certain candidate brackets or bracket types."
"For runs including chunk bracketing constraints, we chose thresholded precision (PT) as confidence weights for topological and/or chunk brackets."
Table 2 summarises the results.
"A high impact on bracket constraints ( 1) results in lower perfor-mance gains than using a moderate impact ( 12 ) (rows 2,4,5 vs. 3,8,9)."
"A possible interpretation is that for high , wrong topological constraints and strong negative priorities can mislead the parser."
"Use of confidence weights yields the best per-formance gains (with 12 ), in particular, thresholded precision of bracket types PT, and tree entropy +E, with comparable speed-up of factor 2.2/2.3 and 2.27/2.23 (2.25 if averaged)."
Thresholded entropy ET yields slightly lower gains.
"This could be due to a non-optimal threshold, or the fact that – while pre-cision differentiates bracket types in terms of their confidence, such that only a small number of brack-ets are weakened – tree entropy as a global measure penalizes all brackets for a sentence on an equal ba-sis, neutralizing positive effects which – as seen in +/ P – may still contribute useful information."
"Additional use of chunk brackets (row 10) leads to a slight decrease, probably due to lower preci-sion of chunk brackets."
"Even more, isolated use of chunk information (row 11) does not yield signifi- cant gains over the baseline (0.89/1.1)."
Similar re-sults were reported[REF_CITE]for inte-gration of chunk- and dependency parsing. [Footnote_15]
"15[REF_CITE]report a gain of factor 2.76 relative to a non-PoS-guided baseline, which reduces to factor 1.21 relative to a PoS-prioritised baseline, as in our scenario."
"For PT -E 12 , Figure 5 shows substantial per-formance gains, with some outliers in the range of length 25–36. 962 sentences (length &gt; 3, avg. 11.09) took longer parse time as compared to the baseline (with 5% variance margin)."
"For coverage losses, we isolated two factors: while erroneous topological in-formation could lead the parser astray, we also found cases where topological information prevented spu-rious HPSG parses to surface."
This suggests that the integrated system bears the potential of cross-validation of different components.
"We demonstrated that integration of shallow topo-logical and deep HPSG processing results in signif-icant performance gains, of factor 2.25—at a high level of deep parser efficiency."
We show that macro-structural constraints derived from topological pars-ing improve significantly over chunk-based con-straints.
Fine-grained prioritisation in terms of con-fidence weights could further improve the results.
Our annotation-based architecture is now easily extended to address robustness issues beyond lexical matters.
"By extracting spans for clausal fragments from topological parses, in case of deep parsing fail- ure the chart can be inspected for spanning anal-yses for sub-sentential fragments."
"Further, we can simplify the input sentence, by pruning adjunct sub-clauses, and trigger reparsing on the pruned input."
The paper describes two parsing schemes: a shallow approach based on machine learning and a cascaded finite-state parser with a hand-crafted grammar.
It dis-cusses several ways to combine them and presents evaluation results for the two in-dividual approaches and their combina-tion.
An underspecification scheme for the output of the finite-state parser is intro-duced and shown to improve performance.
"In several areas of Natural Language Processing, a combination of different approaches has been found to give the best results."
"It is especially rewarding to combine deep and shallow systems, where the for-mer guarantees interpretability and high precision and the latter provides robustness and high recall."
This paper investigates such a combination consist-ing of an n-gram based shallow parser and a cas-caded finite-state parser [Footnote_1] with hand-crafted gram-mar and morphological checking.
"1 Although not everyone would agree that finite-state parsers constitute a ‘deep’ approach to parsing, they still are knowledge-based, require efforts of grammar-writing, a com-plex linguistic lexicon, manage without training data, etc."
"The respective strengths and weaknesses of these approaches are brought to light in an in-depth evaluation on a tree-bank of German newspaper texts[REF_CITE]containing ca. 340,000 tokens in 19,546 sentences."
The evaluation format chosen (dependency tuples) is used as the common denominator of the systems in building a hybrid parser with improved perfor-mance.
An underspecification scheme allows the finite-state parser partially ambiguous output.
It is shown that the other parser can in most cases suc-cessfully disambiguate such information.
"Section 2 discusses the evaluation format adopted (dependency structures), its advantages, but also some of its controversial points."
Section 3 formu-lates a classification problem on the basis of the evaluation format and applies a machine learner to it.
Section 4 describes the architecture of the cas-caded finite-state parser and its output in a novel underspecification format.
Section 5 explores sev-eral combination strategies and tests them on several variants of the two base components.
Section 6 pro-vides an in-depth evaluation of the component sys-tems and the hybrid parser.
Section 7 concludes.
The simplest method to evaluate a parser is to count the parse trees it gets correct.
"This measure is, how-ever, not very informative since most applications do not require one hundred percent correct parse trees."
"Thus, an important question in parser evaluation is how to break down parsing results."
"In the PARSEVAL evaluation scheme[REF_CITE], partially correct parses are gauged by the number of nodes they produce and have in com-mon with the gold standard (measured in precision and recall)."
Another figure (crossing brackets) only counts those incorrect nodes that change the partial order induced by the tree.
"A problematic aspect of the PARSEVAL approach is that the weight given to particular constructions is again grammar-specific, since some grammars may need more nodes to de-scribe them than others."
"Further, the approach does not pay sufficient heed to the fact that parsing de-cisions are often intricately twisted: One wrong de-cision may produce a whole series of other wrong decisions."
"Both these problems are circumvented when parsing results are evaluated on a more abstract level, viz. dependency structure[REF_CITE]."
"Dependency structure generally follows predicate-argument structure, but departs from it in that the basic building blocks are words rather than predi-cates."
"In terms of parser evaluation, the first property guarantees independence of decisions (every link is relevant also for the interpretation level), while the second property makes for a better empirical justifi-cation. for evaluation units."
"Dependency structure can be modelled by a directed acylic graph, with word tokens at the nodes."
"In labelled dependency structure, the links are furthermore classified into a certain set of grammatical roles."
Dependency can be easily determined from con-stituent structure if in every phrase structure rule a constituent is singled out as the head[REF_CITE].
"To derive a labelled dependency structure, all non-head constituents in a rule must be labelled with the grammatical role that links their head tokens to the head token of the head constituent."
"There are two cases where the divergence be-tween predicates and word tokens makes trouble: (1) predicates expressed by more than one token, and (2) predicates expressed by no token (as they occur in ellipsis)."
Case 1 frequently occurs within the verb complex (of both English and German).
"The solu-tion proposed in the literature[REF_CITE]is to define a normal form for depen-dency structure, where every adjunct or argument attaches to some distinguished part of the verb com-plex."
The underlying assumption is that those cases where scope decisions in the verb complex are se-mantically relevant (e.g. with modal verbs) are not resolvable in syntax anyway.
There is no generally accepted solution for case 2 (ellipsis).
"Most authors in the evaluation literature neglect it, perhaps due to its infrequency (in the NEGRA corpus, ellipsis only occurs in 1.2% of all dependency relations)."
"Robinson (1970, 280) proposes to promote one of the dependents (preferably an obligatory one) (1a) or even all dependents (1b) to head status. (1) a. the very brave b. John likes tea and Harry coffee."
A more sweeping solution to these problems is to abandon dependency structure at all and directly go for predicate-argument structure[REF_CITE].
"But as we argued above, moving to a more theoretical level is detrimental to comparabil-ity across grammatical frameworks."
"According to the dependency structure approach to evaluation, the task of the parser is to find the cor-rect dependency structure for a string, i.e. to as-sociate every word token with pairs of head token and grammatical role or else to designate it as inde-pendent."
"To make the learning task easier, the num-ber of classes should be reduced as much as possi-ble."
"For one, the task could be simplified by focus-ing on unlabelled dependency structure (measured in “unlabelled” precision and recall[REF_CITE]), which is, however, in general not suffi-cient for further semantic processing."
"Another possibility for reduction is to associate ev-ery word with at most one pair of head token and grammatical role, i.e. to only look at dependency trees rather than graphs."
There is one case where the tree property cannot easily be maintained: co-ordination.
"Conceptually, all the conjuncts are head constituents in coordination, since the conjunction could be missing, and selectional restrictions work on the individual conjuncts (2). (2) John ate (fish and chips|*wish and ships)."
"But if another word depends on the conjoined heads (see (4a)), the tree property is violated."
A way out of the dilemma is to select a specific conjunct as modification site[REF_CITE].
"But unless care is taken, semantically vi-tal information is lost in the process: Example (4) shows two readings which should be distinguished in dependency structure."
A comparison of the two readings shows that if either the first conjunct or the last conjunct is unconditionally selected certain readings become undistinguishable.
"Rather, in or-der to distinguish a maximum number of readings, pre-modifiers must attach to the last conjunct and post-modifiers and coordinating conjunctions to the first conjunct [Footnote_2] ."
"2 Even in this setting some readings cannot be distinguished (see e.g. (3) where a conjunction of three modifiers would be retrieved). Nevertheless, the proposed scheme fails in only 0.0017% of all dependency tuples."
The fact that the modifier refers to a conjunction rather than to the conjunct is recorded in the grammatical role (by adding c to it). (4) a. the [fans and supporters] of Arsenal b. [the fans] and [supporters of Arsenal]
Other constructions contradicting the tree property are arguably better treated in the lexicon anyway (e.g. control verbs[REF_CITE]) or could be solved by enriching the repertory of grammati-cal roles (e.g. relative clauses with null relative pro-nouns could be treated by adding the dependency re-lation between head verb and missing element to the one between head verb and modified noun).
"In a number of linguistic phenomena, dependency theorists disagree on which constituent should be chosen as the head."
A case in point are PPs.
Few grammars distinguish between adjunct and subcate-gorized PPs at the level of prepositions.
"In predicate-argument structure, however, the embedded NP is in one case related to the preposition, in the other to the subcategorizing verb."
"Accordingly, some ap-proaches take the preposition to be the head of a PP[REF_CITE], others the NP[REF_CITE]."
"Still other approaches[REF_CITE]conflate verb, preposition and head noun into a triple, and thus only count content words in the evaluation."
"For learning, the matter can be resolved empirically:"
"Taking prepositions as the head somewhat improves performance, so we took PPs to be headed by prepo-sitions."
Another question is how to encode the head to-ken.
"The simplest method, encoding the word by its string position, generates a large space of classes."
A more efficient approach uses the distance in string position between dependent and head token.
"Finally,[REF_CITE]proposes a third type of representation:"
"In his work, a head is described by its word type, an indication of the direction from the dependent (left or right) and the number of tokens of the same type that lie between head and dependent."
An illustrative representation would be »paper which refers to the second nearest token paper to the right of the cur-rent token.
"Obviously there are far too many word tokens, but we can use Part-Of-Speech tags instead."
"Furthermore information on inflection and type of noun (proper versus common nouns) is irrelevant, which cuts down the size even more."
We will call this approach nth-tag.
A further refinement of the nth-tag approach makes use of the fact that depen-dency structures are acylic.
"Hence, only those words with the same POS tag as the head between depen-dent and head must be counted that do not depend directly or indirectly on the dependent."
We will call this approach covered-nth-tag.
Figure 1 shows the number of classes the individ-ual approaches generate on the NEGRA Treebank.
Note that the longest sentence has 115 tokens (with punctuation marks) but that punctuation marks do not enter dependency structure.
The original tree-bank exhibits 31 non-head syntactic [Footnote_3] grammatical roles.
3 i.e. grammatical roles not merely used for tokenization
"We added three roles for marker comple-ments (CMP), specifiers (SPR), and floating quanti-fiers (NK+), and subtracted the roles for conjunction markers (CP) and coreference with expletive (RE). 22 roles were copied to mark reference to conjunc-tion."
"Thus, all in all there was a stock of 54 gram-matical roles."
We used -grams (3-grams and 5-grams) of POS tags as context and C4.5[REF_CITE]for ma-chine learning.
All results were subjected to 10-fold cross validation.
The learning algorithm always returns a result.
"We counted a result as not assigned, however, if it referred to a head token outside the sentence."
See Figure 2 for results [Footnote_4] of the learner.
"4 If the learner was given a chance to correct its errors, i.e. if it could train on its training results in a second round, there was a statistically significant gain in F-value with recall rising and precision falling (e.g. F-value .7314, precision .7397, recall .7232 for nth-tag trigrams, and F-value .7763, precision .7826, recall .7700 for nth-tag 5-grams)."
"The left column shows performance with POS tags from the treebank (ideal tags, I-tags), the right column values obtained with POS tags as generated automatically by a tag-ger with an accuracy of 95% (tagger tags, T-tags)."
The nth-tag head representation outperforms the distance representation by 10%.
"Considering acyclicity (cover) slightly improves performance, but the gain is not statistically significant (t-test with 99%)."
"The results are quite impressive as they stand, in particular the nth-tag 5-gram version seems to achieve quite good results."
"It should, however, be stressed that most of the dependencies correctly de-termined by the n-gram methods extend over no more than 3 tokens."
"With the distance method, such ‘short’ dependencies make up 98.90% of all depen-dencies correctly found, with the nth-tag method still 82%, but only 79.63% with the finite-state parser (see section 4) and 78.91% in the treebank."
"In addition to the learning approach, we used a cas-caded finite-state parser[REF_CITE], to extract dependency structures from the text."
"The layout of this parser is similar to Abney’s parser[REF_CITE]: First, a series of transducers extracts noun chunks on the basis of tokenized and POS-tagged text."
"Since center-embedding is frequent in German noun phrases, the same transducer is used several times over."
It also has access to inflectional informa-tion which is vital for checking agreement and deter-mining case for subsequent phases (see[REF_CITE]for a more thorough description).
"Second, a series of transducers extracts verb-final, verb-first, and verb-second clauses."
"In contrast to Abney, these are full clauses, not just simplex clause chunks, so that again recursion can occur."
"Third, the result-ing parse tree is refined and decorated with gram-matical roles, using non-deterministic ‘interpreta-tion’ transducers (the same technique is used[REF_CITE])."
"Fourth, verb complexes are exam-ined to find the head verb and auxiliary passive or raising verbs."
"Only then subcategorization frames can be checked on the clause elements via a non-deterministic transducer, giving them more specific grammatical roles if successful."
"Fifth, dependency tuples are extracted from the parse tree."
Some parsing decisions are known to be not resolv-able by grammar.
Such decisions are best handed over to subsequent modules equipped with the rel-evant knowledge.
"Thus, in chart parsing, an under-specified representation is constructed, from which all possible analyses can be easily and efficiently read off."
Example (5) illustrates this scheme. (5) I saw a man in a car on the hill.
The main drawback of this scheme is its overgener-ation.
"In fact, it allows six readings for example (5), which only has five readings (the speaker could not have been in the car, if the man was asserted to be on the hill)."
"A similar clause with 10 PPs at the end would receive 39,916,800 readings rather than 58,786."
"So a more elaborate scheme is called for, but one that is just as easy to generate."
A device that often comes in handy for under-specification are context variables[REF_CITE].
First let us give every sequence of prepositional phrases in every clause a specific name (e.g. 1B for the second sequence in the first clause).
Now we generate the ambiguous dependency relations ([REF_CITE]) but label them with context variables.
"Such context variables consist of the sequence name , a num-ber designating the dependent in left-to-right or-der (e.g. 0 for in , 1 for on in example (5)), and a number designating the head in left-to-right (e.g. 0 for saw , 1 for man , 2 for hill in (5))."
"If the links are stored with the dependents, the number can be left implicit."
"Generation of such a representation is straightforward and, in particular, does not lead to a higher class of complexity of the full system."
"Ex-ample (6) shows a tuple representation for the two prepositions of sentence (5). (6) in [1A00] saw ADJ, [1A01] man ADJ on [1A10] saw ADJ, [1A11] man ADJ, [1A12] car ADJ"
"In general, a dependent can modify  heads,  ."
Now we viz. the heads numbered put the following constraint on resolution: A depen-dent  can only modify a head  if no previous which could have attached to  (i.e. dependent   ) chose some head  to the left of rather than  .
The condition is formally expressed in (7).
"In example (6) there are only two dependents (  in ,  on )."
"If in attaches to saw , on cannot attach to a head between saw and in ; conversely if on attaches to man , in cannot attach to a head before man ."
"Nothing follows if on attaches to car . ./!&quot;$# %&amp;#(*)+&amp;# (7) Constraint: , -) for all PP sequences"
The cascaded parser described adopts this under-specification scheme for right modification.
Left modification (see (8)) is usually not stacked so the simpler scheme[REF_CITE]suffices. (8) They are usually competent people.
"German is a free word order language, so that sub-categorization can be ambiguous."
Such ambiguities should also be underspecified.
"Again we introduce a context variable for every ambiguous subcatego-rization frame (e.g. 1 in (9)) and count the individual readings 1 (with letters a,b in (9)). (9) Peter kennt Karl. (Peter knows Karl / Karl knows Peter.)"
Peter kennt [1a] SBJ/[1b] OA kennt TOP
Karl kennt [1a] OA/[1b] SBJ
"Since subcategorization ambiguity interacts with at-tachment ambiguity, context variables sometimes need to be coupled:[REF_CITE]the attachment ambiguity only occurs if the PP is read as adjunct. (10) Karl fügte einige Gedanken zu dem Werk hinzu. (Karl added some thoughts on/to the work.)"
Gedanken fügte [1a] OA/[1b] OA zu [1A0] fügte [1a] PP:zu/[1b]
ADJ [1A1] Gedanken PP:zu 1A1 &lt; 1b
"In evaluating underspecified representations,[REF_CITE]distinguish upper and lower bound, standing for optimal performance in disam-biguation and average performance, respectively."
"Figure 3, values are also given for the performance of the parser without underspecification, i.e. always favoring maximal attachment and word order with-out scrambling (direct)."
"Interestingly this method performs significantly better than average, an effect mainly due to the preference for high attachment."
"We considered several strategies to combine the re-sults of the diverse parsing approaches: simple vot-ing, weighted voting, Bayesian learning, Maximum Entropy, and greedy optimization of F-value."
The result predicted by the ma-jority of base classifiers is chosen.
"The finite-state parser, which may give more than one result, dis-tributes its vote evenly on the possible readings."
"In weighted voting, the result which gets the most votes is chosen, where the num-ber of votes given to a base classifier is correlated with its performance on a training set."
The Bayesian approach[REF_CITE]chooses the most probable predic-tion.
The probability   of a / predictionof the probabilityis computedof by the product given the predictions made by the individual base   / of a correct classifiers .
The probability prediction given a learned prediction is ap-proximated by relative frequency in a training set.
"Combining the results can also be seen as a classification task, with base pre-dictions added to the original set of features."
We used the Maximum Entropy approach 5[REF_CITE]as a machine learner for this task.
Un-derspecified features were assigned multiple values.
Greedy Optimization of F-value.
Another method uses a decision list of prediction–classifier pairs to choose a prediction by a classifier.
"The list is obtained by greedy optimization: In each step, the prediction–classifier pair whose addition results in the highest gain in F-value for the combined model on the training set is appended to the list."
The algorithm terminates when F-value cannot be improved by any of the remaining candidates.
A finer distinction is possible if the decision is made dependent on the POS tag as well.
"For greedy optimization, the predictions of the finite-state parser were classified only in grammatical roles, not head positions."
We tested the various combination strategies for the combination Finite-State parser (lower bound) and C4.5 [Footnote_5]-gram nth-tag on ideal tags (results in Fig-ure 4).
"5 More specifically, the OpenNLP implementation ([URL_CITE]was used with 10 iterations and a cut-off frequency for features of 10."
Both simple and weighted voting degrade the results of the base classifiers.
Greedy optimiza-tion outperforms all other strategies.
Indeed it comes near the best possible choice which would give an F-score of .9089 for 5-gram nth-tag and finite-state parser (upper bound) (cf.
Figure 5 shows results for some combinations with the greedy optimization strategy on ideal tags.
All combinations listed yield an improvement of more than 1% in F-value over the base classifiers.
"It is striking that combination with a shallow parser does not help the Finite-State parser much in cov-erage (upper bound), but that it helps both in dis-ambiguation (pushing up the lower bound to almost the level of upper bound) and robustness (remedy-ing at least some of the errors)."
The benefit of un-derspecification is visible when lower bound and di-rect are compared.
The nth-tag [Footnote_5]-gram method was the best method to combine the finite-state parser with.
"5 More specifically, the OpenNLP implementation ([URL_CITE]was used with 10 iterations and a cut-off frequency for features of 10."
"Even on T-tags, this combination achieved an F-score of .8520 (lower, upper: .8579, direct: .8329) without POS tag and an F-score of .8563 (lower, up-per: .8642, direct: .8535) with POS tags."
Figure 6 gives a survey of the performance of the parsing approaches relative to grammatical role.
These figures are more informative than overall F-score[REF_CITE].
"The first column gives the name of the grammatical role, as explained below."
The second column shows corpus frequency in per-cent.
The third column gives the standard devia-tion of distance between dependent and head.
"The three last columns give the performance (recall) of C4.5 with distance representation and 5-grams, C4.5 with nth-tag representation and 5-grams, and the cascaded finite-state parser, respectively."
"For the finite-state parser, the number shows performance with optimal disambiguation (upper bound) and, if the grammatical role allows underspecification, the number for average disambiguation (lower bound) in parentheses."
"Relations between function words and content words (e.g. specifier (SPR), marker complement (CMP), infinitival zu marker (PM)) are frequent and easy for all approaches."
"The cascaded parser has an edge over the learners with arguments (subject (SB), clausal (OC), accusative (OA), second accusative (OA2), genitive (OG), dative object (DA))."
"For all these argument roles a slight amount of ambigu-ity persists (as can be seen from the divergence be-tween upper and lower bound), which is due to free word order."
No ambiguity is found with reported speech (RS).
"The cascaded parser also performs quite well where verb complexes are concerned (separable verb prefix (SVP), governed verbs (OC), and predicative complements (PD, SP))."
"Another clearly discernible complex are adjuncts (modifier (MO), negation (NG), passive subject (SBP); one-place coordination (JUnctor) and discourse markers (DM); finally postnominal modifier (MNR), geni-tive (GR), or von -phrase (PG)), which all exhibit at-tachment ambiguities."
No attachment ambiguities are attested for prenominal genitives (GL).
"Some types of adjunction have not yet been implemented in the cascaded parser, so that it performs badly on them (e.g. relative clauses (RC), which are usu-ally extraposed to the right (average distance is - 11.[Footnote_6]) and thus quite difficult also for the learn-ers; comparative constructions (CC, CM), measure phrases (AMS), floating quantifiers (NK+))."
"6 Other relations classified as NK in the original tree-bank have been reclassified: prenominal determiners to SPR, prenominal adjective phrases to MO."
"Attach- ment ambiguities also occur with appositions (APP, NK 6 )."
"Notoriously difficult is coordination (attach- ment of conjunction to conjuncts (CD), and depen-dency on multiple heads ( c))."
Vocatives (VO) are not treated in the cascaded parser.
AC is the relation between parts of a circumposition.
"The paper has presented two approaches to German parsing (n-gram based machine learning and cas-caded finite-state parsing), and evaluated them on the basis of a large amount of data."
A new represen-tation format has been introduced that allows under-specification of select types of syntactic ambiguity (attachment and subcategorization) even in the ab-sence of a full-fledged chart.
Several methods have been discussed for combining the two approaches.
"It has been shown that while combination with the shallow approach can only marginally improve per-formance of the cascaded parser if ideal disambigua-tion is assumed, a quite substantial rise is registered in situations closer to the real world where POS tag-ging is deficient and resolution of attachment and subcategorization ambiguities less than perfect."
"In ongoing work, we look at integrating a statis-tic context-free parser called BitPar, which was writ-ten by Helmut Schmid and achieves .816 F-score on NEGRA."
"Interestingly, the performance goes up to .9474 F-score when BitPar is combined with the FS parser (upper bound) and .9443 for the lower bound."
"So at least for German, combining parsers seems to be a pretty good idea."
"Thanks are due to Helmut Schmid and Prof. C. Rohrer for discussions, and to the reviewers for their detailed comments."
"Automatically acquiring synonymous col-location pairs such as &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt; from corpora is a challenging task."
"For this task, we can, in general, have a large monolingual corpus and/or a very limited bilingual corpus."
Methods that use monolingual corpora alone or use bilingual corpora alone are apparently inadequate because of low pre-cision or low coverage.
"In this paper, we propose a method that uses both these re-sources to get an optimal compromise of precision and coverage."
"This method first gets candidates of synonymous collocation pairs based on a monolingual corpus and a word thesaurus, and then selects the ap-propriate pairs from the candidates using their translations in a second language."
The translations of the candidates are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus.
The translation information is proved as effective to select synonymous collocation pairs.
"Experi-mental results indicate that the average precision and recall of our approach are 74% and 64% respectively, which outper-form those methods that only use mono-lingual corpora and those that only use bi-lingual corpora."
This paper addresses the problem of automatically extracting English synonymous collocation pairs using translation information.
"A synonymous col-location pair includes two collocations which are similar in meaning, but not identical in wording."
"Throughout this paper, the term collocation refers to a lexically restricted word pair with a certain syntactic relation."
"For instance, &lt;turn on, OBJ, light&gt; is a collocation with a syntactic relation verb-object, and &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt; are a synonymous collocation pair."
"In this paper, translation information means trans-lations of collocations and their translation prob-abilities."
"Synonymous collocations can be considered as an extension of the concept of synonymous ex-pressions which conventionally include synony-mous words, phrases and sentence patterns."
Syn-onymous expressions are very useful in a number of NLP applications.
They are used in information retrieval and question answering[REF_CITE]to bridge the expres-sion gap between the query space and the document space.
"For instance, “buy book” extracted from the users’ query should also in some way match “order book” indexed in the documents."
"Besides, the synonymous expressions are also important in language generati[REF_CITE]and computer assisted authoring to produce vivid texts."
"Up to now, there have been few researches which directly address the problem of extracting synonymous collocations."
"However, a number of studies investigate the extraction of synonymous words from monolingual corpora[REF_CITE]."
The methods used the contexts around the investigated words to discover synonyms.
"The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous."
"In addition, some studies investigate the extraction of synony-mous words and/or patterns from bilingual corpora[REF_CITE]."
"However, these methods can only extract synonymous expressions which occur in the bilingual corpus."
"Due to the limited size of the bilingual corpus, the coverage of the extracted expressions is very low."
"Given the fact that we usually have large mono- lingual corpora (unlimited in some sense) and very limited bilingual corpora, this paper proposes a method that tries to make full use of these different resources to get an optimal compromise of preci-sion and coverage for synonymous collocation extraction."
We first obtain candidates of synony-mous collocation pairs based on a monolingual corpus and a word thesaurus.
We then select those appropriate candidates using their translations in a second language.
Each translation of the candidates is assigned a probability with a statistical translation model that is trained with a small bilingual corpus and a large monolingual corpus.
The similarity of two collocations is estimated by computing the similarity of their vectors constructed with their corresponding translations.
Those candidates with larger similarity scores are extracted as synony-mous collocations.
The basic assumption behind this method is that two collocations are synony-mous if their translations are similar.
"For example, &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt; are synonymous because both of them are translated into &lt; , OBJ, &gt; (&lt;kai[Footnote_1], OBJ, deng1&gt;) and &lt; , OBJ, &gt; (&lt;da3 kai1, OBJ, deng1&gt;) in Chinese."
"1 The NLPWIN parser is developed at Microsoft Re-search, which parses several languages including Chi-nese and English. Its output can be a phrase structure parse tree or a logical form which is represented with dependency triples."
"In order to evaluate the performance of our method, we conducted experiments on extracting three typical types of synonymous collocations."
"Experimental results indicate that our approach achieves 74% average precision and 64% recall respectively, which considerably outperform those methods that only use monolingual corpora or only use bilingual corpora."
The remainder of this paper is organized as fol-lows.
Section 2 describes our synonymous colloca-tion extraction method.
"Section 3 evaluates the proposed method, and the last section draws our conclusion and presents the future work."
Our method for synonymous collocation extraction comprises of three steps: (1) extract collocations from large monolingual corpora; (2) generate can-didates of synonymous collocation pairs with a word thesaurus WordNet; (3) select synonymous collocation candidates using their translations.
This section describes how to extract English col-locations.
"Since Chinese collocations will be used to train the language model in Section 2.3, they are also extracted in the same way."
"Collocations in this paper take some syntactical relations (dependency relations), such as &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt;, and &lt;verb, MOD, adv&gt;."
"These dependency triples, which embody the syntactic relationship between words in a sentence, are generated with a parser—we use NLPWIN in this paper 1 ."
"For example, the sentence “She owned this red coat” is transformed to the following four triples after parsing: &lt;own, SUBJ, she&gt;, &lt;own, OBJ, coat&gt;, &lt;coat, DET, this&gt;, and &lt;coat, ATTR, red&gt;."
"These triples are generally represented in the form of &lt;Head, Relation Type, Modifier&gt;."
"The measure we use to extract collocations from the parsed triples is weighted mutual infor-mation (WMI)[REF_CITE], as described as"
"WMI(w 1 ,r,w 2 ) = p(w 1 ,r,w 2 )log p(w p(w 1 ,r,w 2 ) 1 | r)p(w 2 | r)p(r)"
Those triples whose WMI values are larger than a given threshold are taken as collocations.
We do not use the point-wise mutual information because it tends to overestimate the association between two words with low frequencies.
"Weighted mutual information meliorates this effect by add-ing p(w 1 , r, w 2 ) ."
"For expository purposes, we will only look into three kinds of collocations for synonymous collo-cation extraction: &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt; and &lt;verb, MOD, adv&gt;."
"English Collocations Class #Type #Token verb, OBJ, noun 506,628 7,005,455 noun, ATTR, adj 333,234 4,747,970 verb, Mod, adv 40,748 483,911"
"The English collocations are extracted from Wall Street Journal (1987-1992) and Association Press (1988-1990), and the Chinese collocations are extracted from People’s Daily (1980-1998)."
The statistics of the extracted collocations are shown in Table 1 and 2.
The thresholds are set as 5 for both English and Chinese.
Token refers to the total number of collocation occurrences and Type refers to the number of unique collocations in the corpus.
"Candidate generation is based on the following assumption: For a collocation &lt;Head, Relation Type, Modifier&gt;, its synonymous expressions also take the form of &lt;Head, Relation Type, Modifier&gt; although sometimes they may also be a single word or a sentence pattern."
"The synonymous candidates of a collocation are obtained by expanding a collocation &lt;Head, Rela-tion Type, Modifier&gt; using the synonyms of Head and Modifier."
The synonyms of a word are obtained from WordNet 1.6.
"In WordNet, one synset consists of several synonyms which represent a single sense."
"Therefore, polysemous words occur in more than one synsets."
The synonyms of a given word are obtained from all the synsets including it.
"For ex-ample, the word “turn on” is a polysemous word and is included in several synsets."
"For the sense “cause to operate by flipping a switch”, “switch on” is one of its synonyms."
"For the sense “be contingent on”, “depend on” is one of its synonyms."
We take both of them as the synonyms of “turn on” regard-less of its meanings since we do not have sense tags for words in collocations.
If we use C w to indicate the synonym set of a word w and U to denote the English collocation set generated in Section 2.1.
The detail algorithm on generating candidates of synonymous collocation pairs is described in Figure 1.
"For example, given a collocation &lt;turn on, OBJ, light&gt;, we expand “turn on” to “switch on”, “depend on”, and then expand “light” to “lump”, “illumination”."
"With these synonyms and the relation type OBJ, we generate synonymous collocation candidates of &lt;turn on, OBJ, light&gt;."
"The candidates are &lt;switch on, OBJ, light&gt;, &lt;turn on, OBJ, lump&gt;, &lt;depend on, OBJ, illumination&gt;, &lt;depend on, OBJ, light&gt; etc."
"Both these candidates and the original collocation &lt;turn on, OBJ, light&gt; are used to generate the synony-mous collocation pairs."
"With the above method, we obtained candidates of synonymous collocation pairs."
"For example, &lt;switch on, OBJ, light&gt; and &lt;turn on, OBJ, light&gt; are a synonymous collocation pair."
"However, this method also produces wrong synonymous colloca-tion candidates."
"For example, &lt;depend on, OBJ, illumination&gt; and &lt;turn on, OBJ, light&gt; is not a synonymous pair."
"Thus, it is important to filter out these inappropriate candidates."
"In synonymous word extraction, the similarity of two words can be estimated based on the similarity of their contexts."
"However, this method cannot be effectively extended to collocation similarity esti-mation."
"For example, in sentences “They turned on the lights” and “They depend on the illumination”, the meaning of two collocations &lt;turn on, OBJ, light&gt; and &lt;depend on, OBJ, illumination&gt; are different although their contexts are the same."
"Therefore, monolingual information is not enough to estimate the similarity of two collocations."
"However, the meanings of the above two colloca-tions can be distinguished if they are translated into a second language (e.g., Chinese)."
"For example, &lt;turn on, OBJ, light&gt; is translated into &lt; , OBJ, &gt; (&lt;kai1, OBJ, deng1) and &lt; , OBJ, &gt; (&lt;da3 kai1, OBJ, deng1&gt;) in Chinese while &lt;depend on, OBJ, illumination&gt; is translated into &lt; , OBJ, &gt; (qu3 jue2 yu2, OBJ, guang1 zhao4 du4&gt;)."
"Thus, they are not synonymous pairs because their translations are completely different."
"In this paper, we select the synonymous collo-cation pairs from the candidates in the following way."
"First, given a candidate of synonymous col-location pair generated in section 2.2, we translate the two collocations into Chinese with a simple statistical translation model."
"Second, we calculate the similarity of two collocations with the feature vectors constructed with their translations."
A can-didate is selected as a synonymous collocation pair if its similarity exceeds a certain threshold.
"For an English collocation e col =&lt;e 1 , r e , e 2 &gt;, we translate it into Chinese collocations 2 using an English-Chinese dictionary."
"If the translation sets of e 1 and e 2 are represented as CS 1 and CS 2 respec-tively, the Chinese translations can be represented as S={&lt;c 1 , r c , c 2 &gt;| c 1 CS 1 , c 2"
"CS 2 , r c }, with R denoting the relation set."
"Given an English collocation e col =&lt;e 1 , r e , e 2 &gt; and one of its Chinese collocation c col =&lt;c 1 , r c , c 2 &gt; S , the probability that e col is translated into c col is calculated as in Equation (1). p(c col | e col ) = p(e 1 , r , e | c , r , c ) p(c 1 , r , c ) c 2 (1) e 2 1 c 2 p(e col )"
"According to Equation (1), we need to calculate the translation probability p(e col |c col ) and the target language probability p(c col )."
Calculating the trans-lation probability needs a bilingual corpus.
"If the above equation is used directly, we will run into the data sparseness problem."
"Thus, model simplifica-tion is necessary."
Our simplification is made according to the fol-lowing three assumptions.
"Assumption 1: For a Chinese collocation c col and r e , we assume that e 1 and e 2 are conditionally inde-pendent."
"The translation model is rewritten as: p(e col | c col ) = p(e 1 ,r e ,e 2 | c col ) (2) = p(e 1 | r e ,c col ) p(e 2 | r e ,c col ) p(r e | c col )"
"Assumption [Footnote_2]: Given a Chinese collocation &lt;c 1 , r c , c 2 &gt;, we assume that the translation probability p(e i |c col ) only depends on e i and c i (i=1,2), and p(r e |c col ) only depends on r e and r c ."
"2 Some English collocations can be translated into Chi-nese words, phrases or patterns. Here we only consider the case of being translated into collocations."
Equation (2) is rewritten as: p(e col | c col ) = p(e 1 | c col ) p(e 2 | c col ) p(r e | c col ) (3) = p(e 1 | c 1 ) p(e 2 | c 2 ) p(r e | r c )
"It is equal to a word translation model if we take the relation type in the collocations as an element like a word, which is similar to Model 1[REF_CITE]."
Assumption 3: We assume that one type of English collocation can only be translated to the same type of Chinese collocations [Footnote_3] .
3[REF_CITE]found that about 70% of the Chinese translations have the same relation type as the source English collocations.
"Thus, p(r e | r c ) =1 in our case."
Equation (3) is rewritten as: p(e col | c col ) = p(e 1 | c 1 )p(e 2 | c 2 )p(r e | r c ) (4) = p(e 1 | c 1 )p(e 2 | c 2 )
The language model p(c col ) is calculated with the Chinese collocation database extracted in section 2.1.
"In order to tackle with the data sparseness problem, we smooth the language model with an interpolation method."
"When the given Chinese collocation occurs in the corpus, we calculate it as in (5). p(c col ) = count(c ) (5) col N where count(c col ) represents the count of the Chi-nese collocation c col ."
N represents the total counts of all the Chinese collocations in the training cor-pus.
"For a collocation &lt;c 1 , r c , c 2 &gt;, if we assume that two words c 1 and c 2 are conditionally independent given the relation r c , Equation (5) can be rewritten as in (6). p(c col ) = p(c 1 | r c ) p(c 2 | r c ) p(r c ) (6) where p(c 1 | r c ) = count(c , r ,*) 1 c count(*, r c ,*) p(c 2 | r c ) = count(*, r , c ) c 2 , p(r c ) = count(*,r c ,*) count(*, r c ,*) N count(c 1 , r c ,*) : frequency of the collocations with c 1 as the head and r c as the relation type. count(*, r c ,c 2 ) : frequency of the collocations with c 2 as the modifier and r c as the relation type count(*, r c ,*) : frequency of the collocations with r c as the relation type."
"With Equation (5) and (6), we get the interpolated language model as shown in (7). p(c col ) = λ count(c ) col + (1- λ ) p(c 1 | r c ) p(c 2 | r c ) p(r c ) N (7) where 0 &lt; λ &lt;1 . λ is a constant so that the prob-abilities sum to 1."
Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora[REF_CITE].
"In this paper, we use a parallel bilingual corpus to train the word translation probabilities based on the result of word alignment with a bi-lingual Chinese-English dictionary."
The alignment method is described[REF_CITE].
"In order to deal with the problem of data sparseness, we conduct a simple smoothing by adding 0.5 to the counts of each translation pair as in (8). p(e | c) = countcount(c) +(0e.,5c*)|+trans0.5 _ e | (8) where | trans _ e | represents the number of Eng-lish translations for a given Chinese word c."
"For each synonymous collocation pair, we get its corresponding Chinese translations and calculate the translation probabilities as in section 2.3.1."
"These Chinese collocations with their correspond-ing translation probabilities are taken as feature vectors of the English collocations, which can be represented as:"
"Fe coli =&lt; (c coli1 , p icol1 ),(c coli2 , p coli2 ),...,(c colim , p colim ) &gt;"
The similarity of two collocations is defined as in (9).
"The candidate pairs whose similarity scores exceed a given threshold are selected. sim(e col1 ,e col2 ) = cos( Fe col1 , Fe col2 ) p col1i * p col2j ccol1i =c2colj (9) = ( p col1i ) 2 * ( p 2colj ) 2 i j"
"For example, given a synonymous collocation pair &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt;, we first get their corresponding feature vectors."
"The feature vector of &lt;turn on, OBJ, light&gt;: & lt; (&lt; , OBJ, &gt;, 0.04692), (&lt; , OBJ, &gt;, 0.01602), … , (&lt; , OBJ, &gt;, 0.0002710), (&lt; ,"
"OBJ, &gt;, 0.0000305) &gt;"
"The feature vector of &lt;switch on, OBJ, light&gt;: & lt; (&lt; , OBJ, &gt;, 0.04238), (&lt; , OBJ, &gt;, 0.01257), (&lt; , OBJ, &gt;, 0.002531), … , (&lt; , OBJ, &gt;, 0.00003542) &gt;"
The values in the feature vector are translation probabilities.
"With these two vectors, we get the similarity of &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt;, which is 0.2348."
"We use an English-Chinese dictionary to get the Chinese translations of collocations, which includes 219,404 English words."
Each source word has 3 translation words on average.
"The word translation probabilities are estimated from a bilingual corpus that obtains 170,025 pairs of Chinese-English sen-tences, including about 2.1 million English words and about 2.5 million Chinese words."
"With these data and the collocations in section 2.1, we produced 93,523 synonymous collocation pairs and filtered out 1,060,788 candidate pairs with our translation method if we set the similarity threshold to 0.01."
"To evaluate the effectiveness of our methods, two experiments have been conducted."
The first one is designed to compare our method with two methods that use monolingual corpora.
The second one is designed to compare our method with a method that uses a bilingual corpus.
We compared our approach with two methods that use monolingual corpora.
These two methods also employed the candidate generation described in section 2.2.
The difference is that the two methods use different strategies to select appropriate candi-dates.
The training corpus for these two methods is the same English one as in Section 2.1.
Method 1: This method uses monolingual contexts to select synonymous candidates.
The purpose of this experiment is to see whether the context method for synonymous word extraction can be effectively extended to synonymous collocation extraction.
The similarity of two collocations is calculated with their feature vectors.
The feature vector of a collocation is constructed by all words in sentences which surround the given collocation.
The context vector for collocation i is represented as in (10).
"Fe coli =&lt; (w i1 , p i1 ),(w i2 , p i2 ),...,(w im , p im ) &gt; (10) where p ij = count(w ij ,e coli ) N w ij : context word j of collocation i. p ij : probability of w ij co-occurring with e icol . count(w ij ,e coli ) : frequency of the context word w ij co-occurring with the collocation e coli"
N: all counts of the words in the training corpus.
"With the feature vectors, the similarity of two col-locations is calculated as in (11)."
"Those candidates whose similarities exceed a given threshold are selected as synonymous collocations. sim(e 1col , e col2 ) = cos( Fe 1col , Fe col2 ) p 1i * p 2 j w1i=w2 j (11) = ( p 1i ) 2 * ( p ) 2 2 j i j"
"Method 2: Instead of using contexts to calculate the similarity of two words, this method calculates the similarity of collocations with the similarity of their components."
"The formula is described[REF_CITE]. sim(e col1 ,e col2 ) = sim(e 11 , e 12 )*sim(e 12 , e 22 )*sim(rel 1 , rel 2 ) (12) where e coli = (e 1i , rel i ,e 2i ) ."
"We assume that the rela-tion type keeps the same, so sim(rel 1 , rel 2 ) = 1 ."
"The similarity of the words is calculated with the same method as described[REF_CITE], which is rewritten[REF_CITE]."
"The similarity of the words is calculated through the surrounding context words which have dependency relationships with the investigated words. (w(e 1 , rel, e) + w(e 2 , rel, e)) )"
"Sim(e 1 , e 2 ) = (rel,e)∈T(e1 w(e 1 , rel, e) + w(e 2 , rel, e) (rel,e)∈T(e1) (rel,e)∈T(e2) (13) where T(e i ) denotes the set of words which have the dependency relation rel with e i . w(e i , rel, e j ) p(e i , rel, e j ) = p(e i , rel, e j ) log p(e i | rel) p(e j | rel) p(rel)"
"With the candidate generation method as depicted in section 2.2, we generated 1,154,311 candidates of synonymous collocations pairs for 880,600 collocations, from which we randomly selected 1,300 pairs to construct a test set."
Each pair was evaluated independently by two judges to see if it is synonymous.
Only those agreed upon by two judges are considered as synonymous pairs.
The statistics of the test set is shown in Table 3.
"We evaluated three types of synonymous collocations: &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt;, &lt;verb, MOD, adv&gt;."
"For the type &lt;verb, OBJ, noun&gt;, among the 630 synonymous collocation candidate pairs, 197 pairs are correct."
"For &lt;noun, ATTR, adj&gt;, 163 pairs (among 324 pairs) are correct, and for &lt;verb, MOD, adv&gt;, 124 pairs (among 346 pairs) are correct."
"With the test set, we evaluate the performance of each method."
"The evaluation metrics are precision, recall, and f-measure."
A development set including 500 synonymous pairs is used to determine the thresholds of each method.
"For each method, the thresholds for getting highest f-measure scores on the development set are selected."
"As the result, the thresholds for Method 1, Method 2 and our approach are 0.02, 0.02, and 0.01 respectively."
"With these thresholds, the experi-mental results on the test set in Table 3 are shown in Table 4, Table 5 and Table 6."
It can be seen that our approach gets the highest precision (74% on average) for all the three types of synonymous collocations.
"Although the recall (64% on average) of our approach is below other methods, the f-measure scores, which combine both precision and recall, are the highest."
"In order to compare our methods with other methods under the same recall value, we conduct another experiment on the type &lt;verb, OBJ, noun&gt; 4 ."
"We set the recalls of the two methods to the same value of our method, which is 0.6396 in Table 4."
"The precisions are 0.3190, 0.4922, and 0.6811 for Method 1, Method 2, and our method, respectively."
"Thus, the precisions of our approach are higher than the other two methods even when their recalls are the same."
It proves that our method of using translation information to select the candidates is effective for synonymous collocation extraction.
The results of Method 1 show that it is difficult to extract synonymous collocations with monolin-gual contexts.
"Although Method 1 gets higher re-calls than the other methods, it brings a large number of wrong candidates, which results in lower precision."
"If we set higher thresholds to get com-parable precision, the recall is much lower than that of our approach."
This indicates that the contexts of collocations are not discriminative to extract syn-onymous collocations.
The results also show that Model 2 is not suit-able for the task.
"The main reason is that both high scores of sim(e 11 ,e 12 ) and sim(e 12 ,e 22 ) does not mean the high similarity of the two collocations."
"The reason that our method outperforms the other two methods is that when one collocation is translated into another language, its translations indirectly disambiguate the words’ senses in the collocation."
"For example, the probability of &lt;turn on, OBJ, light&gt; being translated into &lt; , OBJ, &gt; (&lt;da3 kai1, OBJ, deng1&gt;) is much higher than that of it being translated into &lt; , OBJ, &gt; (&lt;qu3 jue2 yu2, OBJ, guang1 zhao[Footnote_4] du4&gt;) while the situation is reversed for &lt;depend on, OBJ, il-lumination&gt;."
"4 The results of the other two types of collocations are the same as &lt;verb, OBJ, noun&gt;. We omit them because of the space limit."
"Thus, the similarity between &lt;turn on, OBJ, light&gt; and &lt;depend on, OBJ, illumination&gt; is low and, therefore, this candidate is filtered out."
"If the same source ex-pression has more than one different translation in the second language, these different translations are extracted as synonymous expressions."
"In order to compare our method with these methods that only use a bilingual corpus, we implement a method that is similar to the above two studies."
The detail proc-ess is described in Method 3.
"Method 3: The method is described as follows: (1) All the source and target sentences (here Chi-nese and English, respectively) are parsed; (2) extract the Chinese and English collocations in the bilingual corpus; (3) align Chinese collocations c col =&lt;c 1 , r c , c 2 &gt; and English collocations e col =&lt;e 1 , r e , e 2 &gt; if c 1 is aligned with e 1 and c 2 is aligned with e 2 ; (4) obtain two English synonymous collocations if two different English collocations are aligned with the same Chinese collocation and if they occur more than once in the corpus."
The training bilingual corpus is the same one described in Section 2.
"With Method 3, we get 9,368 synonymous collocation pairs in total."
"The number is only 10% of that extracted by our ap-proach, which extracts 93,523 pairs with the same bilingual corpus."
In order to evaluate Method 3 and our approach on the same test set.
We randomly select 100 collocations which have synonymous collocations in the bilingual corpus.
5 These synonymous collocation pairs are evaluated by two judges and only those agreed on by both are selected as correct pairs.
"Our method described in Section 2 gen-erates 556 synonymous collocation pairs with a threshold set in the above section, where 75% (417 among 556) are correct."
"If we set a higher threshold (0.08) for our method, we get 360 pairs where 295 are correct (82%)."
"If we use |A|, |B|, |C| to denote correct pairs extracted by Method 3, our method, both Method 3 and our method respectively, we get |A|=100, |B|=295, and C = | A| ∩ |"
B | = 78 .
"Thus, the syn-onymous collocation pairs extracted by our method cover 78% ( | C | | A | ) of those extracted by Method 3 while those extracted by Method 3 only cover 26% ( | C | | B | ) of those extracted by our method."
It can be seen that the coverage of Method 3 is much lower than that of our method even when their precisions are set to the same value.
This is mainly because Method 3 can only extract synonymous collocations which occur in the bilingual corpus.
"In contrast, our method uses the bilingual corpus to train the translation probabilities, where the trans-lations are not necessary to occur in the bilingual corpus."
The advantage of our method is that it can extract synonymous collocations not occurring in the bilingual corpus.
This paper proposes a novel method to automati-cally extract synonymous collocations by using translation information.
"Our contribution is that, given a large monolingual corpus and a very limited bilingual corpus, we can make full use of these resources to get an optimal compromise of preci-sion and recall."
"Especially, with a small bilingual corpus, a statistical translation model is trained for the translations of synonymous collocation candi-dates."
The translation information is used to select synonymous collocation pairs from the candidates obtained with a monolingual corpus.
Experimental results indicate that our approach extracts syn-onymous collocations with an average precision of 74% and recall of 64%.
"This result significantly outperforms those of the methods that only use monolingual corpora, and that only use a bilingual corpus."
Our future work will extend synonymous ex-pressions of the collocations to words and patterns besides collocations.
"In addition, we are also inter-ested in extending this method to the extraction of synonymous words so that “black” and “white”, “dog” and “cat” can be classified into different synsets."
Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning.
In this pa-per we present a novel approach for con-structing semantic spaces that takes syn-tactic relations into account.
We introduce a formalisation for this class of models and evaluate their adequacy on two mod-elling tasks: semantic priming and auto-matic discrimination of lexical relations.
"Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discriminati[REF_CITE], text segmentati[REF_CITE], contextual spelling correcti[REF_CITE], auto-matic thesaurus extracti[REF_CITE], and notably information retrieval[REF_CITE]."
Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling stud-ies ranging from similarity judgements[REF_CITE]to semantic priming[REF_CITE]and text comprehensi[REF_CITE].
In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides im-portant information about its meaning.
The semantic properties of words are represented by vectors that are constructed from the observed distributional pat-terns of co-occurrence of their neighbouring words.
"Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context."
"Contexts are defined as a small number of words surrounding the target word[REF_CITE]or as entire para-graphs, even documents[REF_CITE]."
"Context is typically treated as a set of unordered words, although in some cases syntac-tic information is taken into account[REF_CITE]."
A word can be thus viewed as a point in an n-dimensional semantic space.
The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance.
"In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink ) are not taken into account in the con-struction of the semantic space, although in some cases word lexemes are used rather than word sur-face forms[REF_CITE]."
Minimal assumptions are made with respect to syntactic dependencies among words.
In fact it is assumed that all context words within a certain dis-tance from the target word are semantically relevant.
The lack of syntactic information makes the build-ing of semantic space models relatively straightfor-ward and language independent (all that is needed is a corpus of written or spoken text).
"However, this entails that contextual information contributes indis-criminately to a word’s meaning."
Some studies have tried to incorporate syntactic information into vector-based models.
"In this view, the semantic space is constructed from words that bear a syntactic relationship to the target word of in-terest."
"This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant."
"However, existing models ei-ther concentrate on specific relations for construct-ing the semantic space such as objects (e.g.,[REF_CITE]) or collapse all types of syntactic relations available for a given target word[REF_CITE]."
"Although syntactic information is now used to select a word’s appropriate contexts, this in-formation is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing."
"A commonly raised criticism for both types of se-mantic space models (i.e., word-based and syntax-based) concerns the notion of semantic similarity."
Proximity between two words in the semantic space cannot indicate the nature of the lexical relations be-tween them.
"Distributionally similar words can be antonyms, synonyms, hyponyms or in some cases semantically unrelated."
This limits the application of semantic space models for NLP tasks which re-quire distinguishing between lexical relations.
In this paper we generalise semantic space models by proposing a flexible conceptualisation of context which is parametrisable in terms of syntactic rela-tions.
We develop a general framework for vector-based models which can be optimised for different tasks.
Our framework allows the construction of se-mantic space to take place over words or syntactic relations thus bridging the distance between word-based and syntax-based models.
"Furthermore, we show how our model can incorporate well-defined, informative contexts in a principled way which re-tains information about the syntactic relations avail-able for a given target word."
"We first evaluate our model on semantic prim-ing, a phenomenon that has received much attention in computational psycholinguistics and is typically modelled using word-based semantic spaces."
We next conduct a study that shows that our model is sensitive to different types of lexical relations.
"Once we move away from words as the basic con-text unit, the issue of representation of syntactic in-formation becomes pertinent."
Information about the dependency relations between words abstracts over word order and can be considered as an intermediate layer between surface syntax and semantics.
"More formally, dependencies are asymmetric binary rela-tionships between a head and a modifier[REF_CITE]."
The structure of a sentence can be repre-sented by a set of dependency relationships that form a tree as shown in Figure 1.
Here the head of the sen-tence is the verb carry which is in turn modified by its subject lorry and its object apples .
It is the dependencies in Figure 1 that will form the context over which the semantic space will be constructed.
"The construction mechanism sets out by identifying the local context of a target word, which is a subset of all dependency paths starting from it."
"The paths consist of the dependency edges of the tree labelled with dependency relations such as subj , obj , or aux (see Figure 1)."
"The paths can be ranked by a path value function which gives differ-ent weight to different dependency types (for exam-ple, it can be argued that subjects and objects convey more semantic information than determiners)."
Tar-get words are then represented in terms of syntactic features which form the dimensions of the seman-tic space.
Paths are mapped to features by the path equivalence relation and the appropriate cells in the matrix are incremented.
We assume the semantic space formalisation pro-posed[REF_CITE].
A semantic space is a matrix whose rows correspond to target words and columns to dimensions which Lowe calls basis elements : Definition 1.
"A Semantic Space Model is a matrix K = B × T, where b i ∈ B denotes the basis element of column i, t j ∈ T denotes the target word of row j, and K ij the cell (i, j)."
T is the set of words for which the matrix con-tains representations; this can be either word types or word tokens .
"In this paper, we assume that co-occurrence counts are constructed over word types, but the framework can be easily adapted to represent word tokens instead."
"In traditional semantic spaces, the cells K ij of the matrix correspond to word co-occurrence counts."
This is no longer the case for dependency-based models.
In the following we explain how co-occurrence counts are constructed.
The first step in constructing a semantic space from a large collection of dependency relations is to con-struct a word’s local context .
"The dependency parse p of a sentence s is an undirected graph p(s) = (V p ,E p )."
"The set of nodes corresponds to words of the sentence: V p = {w 1 ,...,w n }."
The set of edges is E p ⊆ V p ×V p .
"A class q is a three-tuple consisting of a POS-tag, a relation, and another POS-tag."
We write Q for the set of all classes Cat × R ×Cat.
"For each parse p, the labelling function L p : E p → Q as-signs a class to every edge of the parse."
"In Figure 1, the labelling function labels the left-most edge as L p (( a , lorry )) = hDet, det ,Ni."
Note that Det represents the POS-tag “determiner” and det the dependency relation “determiner”.
"In traditional models, the target words are sur-rounded by context words."
"In a dependency-based model, the target words are surrounded by depen-dency paths ."
"A path φ is an ordered tuple of edges he 1 ,...,e n i ∈ E pn so that ∀i : (e i−1 = (v 1 ,v 2 ) ∧ e i = (v 3 ,v 4 )) ⇒ v 2 = v 3"
"A path anchored at a word w is a path heΦ 1 ,...,e n i so that e 1 = (v 1 ,v 2 ) and w = v 1 ."
Write w for the set of all paths over E p anchored at w.
"In words, a path is a tuple of connected edges in a parse graph and it is anchored at w if it starts at w. In Figure 1, the set of paths anchored at lorry 1 is: {h(lorry,carry)i,h(lorry,carry),(carry,apples)i, h(lorry,a)i,h(lorry,carry),(carry,might)i,...}"
The local context of a word is the set or a subset of its anchored paths.
The class information can always be recovered by means of the labelling function.
A local context of a word w from a sentence s is a subset of the anchored paths at w. A function c : W → 2 Φ w which assigns a local context to a word is called a context specification function .
The context specification function allows to elim-inate paths on the basis of their classes.
"For exam-ple, it is possible to eliminate all paths from the set of anchored paths but those which contain immedi-ate subject and direct object relations."
"This can be formalised as: c(w) = {φ ∈ Φ w |φ = hei ∧ (L p (e) = hV, obj ,Ni ∨ L p (e) = hV, subj ,Ni)}"
"In Figure 1, the labels of the two edges which form paths of length 1 and conform to this context specification are marked in boldface."
"Notice that the local context of lorry contains only one anchored path (c( lorry ) = {h(lorry,carry)i})."
The second step in the construction of the dependency-based semantic models is to specify the relative importance of different paths.
Linguistic in-formation can be incorporated into our framework through the path value function .
The path value function v assigns a real number to a path: v : Φ → R.
"For instance, the path value function could pe-nalise longer paths for only expressing indirect re-lationships between words."
"An example of a length-based path value function is v(φ) = 1n where φ = he 1 ,...,e n i."
This function assigns a value of 1 to the one path from c( lorry ) and fractions to longer paths.
"Once the value of all paths in the local context is determined, the dimensions of the space must be specified."
"Unlike word-based models, our contexts contain syntactic information and dimensions can be defined in terms of syntactic features ."
The path equivalence relation combines functionally equiva-lent dependency paths that share a syntactic feature into equivalence classes.
Let ∼ be the path equivalence relation on Φ.
The partition induced by this equivalence re-lation is the set of basis elements B.
"For example, it is possible to combine all paths which end at the same word: A path which starts at w i and ends at w j , irrespectively of its length and class, will be the co-occurrence of w i and w j ."
"This word-based equivalence function can be defined in the following manner: h(v 1 ,v 2 ),...,(v n−1 ,v n )i ∼ h(v [Footnote_1] ,v 02 ),...,(v 0m−1 ,v 0m )i iff v n = v 0m"
"1 For the sake of brevity, we only show paths up to length 2."
This means that in Figure 1 the set of basis elements is the set of words at which paths end.
"Although co- occurrence counts are constructed over words like in traditional semantic space models, it is only words which stand in a syntactic relationship to the target that are taken into account."
"Once the value of all paths in the local context is determined, the local observed frequency for the co-occurrence of a basis element b with the target word w is just the sum of values of all paths φ in this context which express the basis element b."
The global observed frequency is the sum of the local observed frequencies for all occurrences of a target word type t and is therefore a measure for the co-occurrence of t and b over the whole corpus.
"Global observed frequency: f (b,t) = ∑ ∑ ˆ v(φ) w∈W(t) φ∈C(w)∧φ∼b"
"Due to the Zip-fian distribution of word types, words occurring with similar frequencies will be judged more similar than they actually are."
A lexical association func-tion can be used to explicitly factor out chance co-occurrences.
Write A for the lexical association function which computes the value of a cell of the matrix from a co-occurrence frequency:
K ij =
"A( fˆ(b i ,t j ))"
"All our experiments were conducted on the British National Corpus (BNC), a 100 million word col-lection of samples of written and spoken language[REF_CITE]."
We used Lin’s (1998) broad cover-age dependency parser MINIPAR to obtain a parsed version of the corpus.
"MINIPAR employs a man-ually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total)."
Lexicon entries con-tain part-of-speech and subcategorization informa-tion.
"The grammar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges (i.e., types of syntactic (dependency) relationships)."
MINIPAR uses a distributed chart parsing algorithm.
Grammar rules are implemented as constraints asso-ciated with the nodes and edges.
The dependency-based semantic space was con-structed with the word-based path equivalence func-tion from Section 2.3.
As basis elements for our se-mantic space the 1000 most frequent words in the BNC were used.
Each element of the resulting vec-tor was replaced with its log-likelihood value (see[REF_CITE]in Section 2.3) which can be consid-ered as an estimate of how surprising or distinctive a co-occurrence pair is[REF_CITE].
"We experimented with a variety of distance mea-sures such as cosine, Euclidean distance, L 1 norm, Jaccard’s coefficient, Kullback-Leibler divergence and the Skew divergence (see[REF_CITE]for an overview)."
We obtained the best results for co-sine (Experiment 1) and Skew divergence (Experi-ment 2).
The two measures are shown in Figure 2.
The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed[REF_CITE]as a linguistically motivated distance measure.
We use a value of α = .99.
We explored in detail the influence of different types and sizes of context by varying the context specification and path value functions.
Contexts were defined over a set of 23 most frequent depen-dency relations which accounted for half of the de-pendency edges found in our corpus.
"From these, we constructed four context specification functions: (a) minimum contexts containing paths of length 1 (in Figure 1 sweet and carry are the minimum con-text for apples ), (b) np context adds dependency in-formation relevant for noun compounds to minimum context, (c) wide takes into account paths of length longer than 1 that represent meaningful linguistic re-lations such as argument structure, but also prepo-sitional phrases and embedded clauses (in Figure 1 the wide context of apples is sweet, carry, lorry , and might ), and (d) maximum combined all of the above into a rich context representation."
"Four path valuation functions were used: (a) plain assigns the same value to every path, (b) length assigns a value inversely proportional to a path’s length, (c) oblique ranks paths according to the obliqueness hierarchy of grammatical relations[REF_CITE], and (d) oblength combines length and oblique ."
Length-based and length-neutral path value functions are collapsed for the minimum context specification since it only considers paths of length 1.
We further compare in Experiments 1 and 2 our dependency-based model against a state-of-the-art vector-based model where context is defined as a “bag of words”.
Note that considerable latitude is allowed in setting parameters for vector-based mod-els.
"In order to allow a fair comparison, we se-lected parameters for the traditional model that have been considered optimal in the literature[REF_CITE], namely a symmetric 10 word window and the most frequent 500 content words from the BNC as dimensions."
These parameters were similar to those used[REF_CITE](symmet-ric 10 word window and 536 content words).
Again the log-likelihood score is used to factor out chance co-occurrences.
A large number of modelling studies in psycholin-guistics have focused on simulating semantic prim-ing studies.
"The semantic priming paradigm pro-vides a natural test bed for semantic space models as it concentrates on the semantic similarity or dis-similarity between a prime and its target, and it is precisely this type of lexical relations that vector-based models capture."
In this experiment we focus on Balota and Lorch’s (1986) mediated priming study.
In semantic priming transient presentation of a prime word like tiger di-rectly facilitates pronunciation or lexical decision on a target word like lion .
"Mediated priming extends this paradigm by additionally allowing indirectly re-lated words as primes – like stripes , which is only related to lion by means of the intermediate concept tiger ."
"For the pronunciation task, re-action times were reduced significantly for both di-rect and mediated primes, however the effect was larger for direct primes."
There are at least two semantic space simulations that attempt to shed light on the mediated priming effect.
"In their study, mediated primes were farther from their targets than unrelated words."
Materials were taken form
"They consist of 48 target words, each paired with a related and a mediated prime (e.g., lion-tiger-stripes )."
Each related-mediated prime tuple was paired with an unrelated control randomly selected from the complement set of related primes.
"One stimulus was removed as it had a low cor-pus frequency (less than 100), which meant that the resulting vector would be unreliable."
"We con-structed vectors from the BNC for all stimuli with the dependency-based models and the traditional model, using the parametrisations given in Sec-tion 3.1 and cosine as a distance measure."
"We calcu-lated the distance in semantic space between targets and their direct primes (TarDirP), targets and their mediated primes (TarMedP), targets and their unre-lated controls (TarUnC) for both models."
"We carried out a one-way Analysis of Variance (A NOVA ) with the distance as dependent variable (TarDirP, TarMedP, TarUnC)."
Recall from Table 1 that we experimented with fourteen different con-text definitions.
A reliable effect of distance was observed for all models (p &lt; .001).
We used the η 2 statistic to calculate the amount of variance ac-counted for by the different models.
Figure 3 plots η 2 against the different contexts.
"The best result was obtained for model 7 which accounts for 23.1% of the variance (F(2,140) = 20.576, p &lt; .001) and corresponds to the wide context specification and the plain path value function."
"A reliable distance effect was also observed for the traditional vector-based model (F(2,138) = 9.384, p &lt; .001)."
Pairwise A NOVA s were further performed to ex-amine the size of the direct and mediated priming ef-fects individually (see Table 2).
"There was a reliable direct priming effect (F(1,94) = 25.290, p &lt; .001) but we failed to find a reliable mediated priming effect (F(1,93) = .001, p = .790)."
"A reliable di-rect priming effect (F(1,92) = 12.185, p = .001) but no mediated priming effect was also obtained for the traditional vector-based model."
We used the η 2 statistic to compare the effect sizes obtained for the dependency-based and traditional model.
"The best dependency-based model accounted for 23.1% of the variance, whereas the traditional model ac-counted for 12.2% (see also Table 2)."
Our results indicate that dependency-based mod-els are able to model direct priming across a wide range of parameters.
"Our results also show that larger contexts (see models 7 and 11 in Figure 3) are more informative than smaller contexts (see mod-els 1 and 3 in Figure 3), but note that the wide con-text specification performed better than maximum."
"At least for mediated priming, a uniform path value as assigned by the plain path value function outper-forms all other functions (see Figure 3)."
Neither our dependency-based model nor the tra-ditional model were able to replicate the mediated priming effect reported[REF_CITE](see L &amp; McD in Table 2).
"This may be due to differences in lemmatisation of the BNC, the parametrisations of the model or the choice of context words (Lowe and McDonald use a spe-cial procedure to identify “reliable” context words)."
"Our results also differ[REF_CITE]who found that mediated primes were fur-ther from their targets than unrelated controls, us-ing however a model and corpus different from the ones we employed for our comparative studies."
"In the dependency-based model, mediated primes were virtually indistinguishable from unrelated words."
"In sum, our results indicate that a model which takes syntactic information into account outper-forms a traditional vector-based model which sim-ply relies on word occurrences."
Our model is able to reproduce the well-established direct priming ef-fect but not the more controversial mediated prim-ing effect.
"Our results point to the need for further comparative studies among semantic space models where variables such as corpus choice and size as well as preprocessing (e.g., lemmatisation, tokeni-sation) are controlled for."
In this experiment we examine whether dependency-based models construct a semantic space that encap-sulates different lexical relations.
"More specifically, we will assess whether word pairs capturing differ-ent types of semantic relations (e.g., hyponymy, syn-onymy) can be distinguished in terms of their dis-tances in the semantic space."
"Our experimental materials were taken[REF_CITE]who in an attempt to investigate which types of lexical relations induce priming col-lected a set of 142 word pairs exemplifying the fol-lowing semantic relations: (a) synonymy (words with the same meaning, value and worth ), (b) su-perordination and subordination (one word is an in-stance of the kind expressed by the other word, pain and sensation ), (c) category coordination (words which express two instances of a common super-ordinate concept, truck and train ), (d) antonymy (words with opposite meaning, friend and enemy ), (e) conceptual association (the first word subjects produce in free association given the other word, leash and dog ), and (f) phrasal association (words which co-occur in phrases private and property )."
The pairs were selected to be unambiguous exam-ples of the relation type they instantiate and were matched for frequency.
"The pairs cover a wide range of parts of speech, like adjectives, verbs, and nouns. are given in Table 3."
"The mean distances for concep-tual associates ( CA ), phrasal associates ( PA ), super-ordinates/subordinates ( SUP ), category coordinates ( CO ), antonyms ( ANT ), and synonyms ( SYN ) are also shown in Table 3."
"There is no significant differ-ence between PA and CA , although SUP , CO , ANT , and SYN , are all significantly different from CA (see Table 3, where × indicates statistical significance, a = .05)."
"Furthermore, ANT and SYN are signifi-cantly different from PA . 



"
"As in Experiment 1, six words with low fre-quencies (less than 100) were removed from the materials."
"Vectors were computed for the re-maining 278 words for both the traditional and the dependency-based models, again with the parametrisations detailed in Section 3.1."
"We calcu-lated the semantic distance for every word pair, this time using Skew divergence as distance measure."
We carried out an A NOVA with the lexical rela-tion as factor and the distance as dependent variable.
"The lexical relation factor had six levels, namely the relations detailed in Section 3.3.1."
"We found no ef-fect of semantic distance for the traditional semantic space model (F(5,141) = 1.481, p = .200)."
The η 2 statistic revealed that only 5.2% of the variance was accounted for.
"On the other hand, a reliable effect of distance was observed for all dependency-based models (p &lt; .001)."
Model 7 (wide context specifi-cation and plain path value function) accounted for the highest amount of variance in our data (20.3%).
Our results can be seen in Figure 4.
We examined whether there are any significant differences among the six relations using Post-hoc Tukey tests.
"The pairwise comparisons for model 7 



"
"Our results suggest that dependency-based vector space models can, at least to a certain degree, dis-tinguish among different types of lexical relations, while this seems to be more difficult for traditional semantic space models."
The Tukey test revealed that category coordination is reliably distinguished from all other relations and that phrasal association is re-liably different from antonymy and synonymy.
"Tax-onomy related relations (e.g., synonymy, antonymy, hyponymy) can be reliably distinguished from con-ceptual and phrasal association."
"However, no reli-able differences were found between closely associ-ated relations such as antonymy and synonymy."
Our results further indicate that context encoding plays an important role in discriminating lexical re-lations.
As in Experiment 1 our best results were obtained with the wide context specification.
"Also, weighting schemes such as the obliqueness hierar-chy length again decreased the model’s performance (see conditions 2, 5, 9, and 13 in Figure 4), show-ing that dependency relations contribute equally to the representation of a word’s meaning."
This points to the fact that rich context encodings with a wide range of dependency relations are promising for cap-turing lexical semantic distinctions.
"However, the performance for maximum context specification was lower, which indicates that collapsing all depen-dency relations is not the optimal method, at least for the tasks attempted here."
In this paper we presented a novel semantic space model that enriches traditional vector-based models with syntactic information.
The model is highly gen-eral and can be optimised for different tasks.
"It ex-tends prior work on syntax-based models[REF_CITE], by providing a general framework for defining context so that a large num-ber of syntactic relations can be used in the construc-tion of the semantic space."
"Our approach differs[REF_CITE]in three important ways: (a) by introducing dependency paths we can capture non-immediate relationships between words (i.e., between subjects and objects), whereas Lin considers only local context (depen-dency edges in our terminology); the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; (b) Lin creates the semantic space from the set of dependency edges that are rel-evant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels (e.g., sub-ject, object, modifier) and parametrize the space ac-cordingly for different tasks; (c) considerable flexi-bility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words (see the leaves in Figure 1), parts of speech or dependency edges; in Lin’s approach, it is only dependency edges (features in his terminology) that form the dimensions of the semantic space."
Experiment 1 revealed that the dependency-based model adequately simulates semantic priming.
Ex-periment 2 showed that a model that relies on rich context specifications can reliably distinguish be-tween different types of lexical relations.
Our re-sults indicate that a number of NLP tasks could potentially benefit from dependency-based models.
"These are particularly relevant for word sense dis-crimination, automatic thesaurus construction, auto-matic clustering and in general similarity-based ap-proaches to NLP."
"Standard IR systems can process queries such as “web NOT internet”, enabling users who are interested in arachnids to avoid documents about computing."
The docu-ments retrieved for such a query should be irrelevant to the negated query term.
Most systems implement this by reprocessing re-sults after retrieval to remove documents containing the unwanted string of letters.
"This paper describes and evaluates a the-oretically motivated method for removing unwanted meanings directly from the orig-inal query in vector models, with the same vector negation operator as used in quan-tum logic."
"Irrelevance in vector spaces is modelled using orthogonality, so query vec-tors are made orthogonal to the negated term or terms."
"As well as removing unwanted terms, this form of vector negation reduces the occur-rence of synonyms and neighbours of the negated terms by as much as 76% compared with standard Boolean methods."
"By alter-ing the query vector itself, vector negation removes not only unwanted strings but un-wanted meanings."
Vector spaces enjoy widespread use in information retrieval ([REF_CITE]; Baeza-Yates and
"One benefit of these models is that the similarity between pairs of terms or between queries and documents is a continuous function, au-tomatically ranking results rather than giving just a YES/NO judgment."
"In addition, vector models can be freely built from unlabelled text and so are both entirely unsupervised, and an accurate reflection of the way words are used in practice."
"In vector models, terms are usually combined to form more complicated query statements by (weighted) vector addition."
"Because vector addition is commutative, terms are combined in a “bag of words” fashion."
"While this has proved to be effective, it certainly leaves room for improvement: any gen-uine natural language understanding of query state-ments cannot rely solely on commutative addition for building more complicated expressions out of primi-tives."
Other algebraic systems such as Boolean logic and set theory have well-known operations for building composite expressions out of more basic ones.
"Set-theoretic models for the logical connectives ‘AND’, ‘NOT’ and ‘OR’ are completely understood by most researchers, and used by Boolean IR systems for as-sembling the results to complicated queries."
"It is clearly desirable to develop a calculus which com-bines the flexible ranking of results in a vector model with the crisp efficiency of Boolean logic, a goal which has long been recognised[REF_CITE]and attempted mainly for conjunction and disjunc-tion."
"This paper proposes such a scheme for nega-tion, based upon well-known linear algebra, and which also implies a vector form of disjunction."
"It turns out that these vector connectives are precisely those used in quantum logic (Birkhoff and v[REF_CITE]), a development which is discussed in much more detail[REF_CITE]."
"Because of its simplicity, our model is easy to under-stand and to implement."
"Vector negation is based on the intuition that un-related meanings should be orthogonal to one an-other, which is to say that they should have no fea-tures in common at all."
Thus vector negation gener-ates a ‘meaning vector’ which is completely orthog-onal to the negated term.
Document retrieval ex-periments demonstrate that vector negation is not only effective at removing unwanted terms: it is also more effective than other methods at removing their synonyms and related terms.
"This justifies the claim that, by producing a single query vector for “a NOT b”, we remove not only unwanted strings but also unwanted meanings."
We describe the underlying motivation behind this model and define the vector negation and disjunc-tion operations in Section 2.
"In Section 3 we re-view other ways negation is implemented in Infor-mation Retrieval, comparing and contrasting with vector negation."
In Section 4 we describe experi-ments demonstrating the benefits and drawbacks of vector negation compared with two other methods for negation.
In this section we use well-known linear algebra to define vector negation in terms of orthogonality and disjunction as the linear sum of subspaces.
The mathematical apparatus is covered in greater detail[REF_CITE].
"If A is a set (in some universe of discourse U), then ‘NOT A’ corre-sponds to the complement A ⊥ of the set A in U (by definition)."
"By a simple analogy, let A be a vector subspace of a vector space V (equipped with a scalar product)."
Then the concept ‘NOT A’ should corre-spond to the orthogonal complement A ⊥ of A under the scalar product (Birkhoff and v[REF_CITE]§6).
"If we think of a basis for V as a set of features, this says that ‘NOT A’ refers to the subspace of V which has no features in common with A."
We make the following definitions.
Let V be a (real) vector space equipped with a scalar product.
We will use the notation A ≤ V to mean “A is a vector subspace of V .”
"For A ≤ V , define the or-thogonal subspace A ⊥ to be the subspace"
"A ⊥ ≡ {v ∈ V : ∀a ∈ A, a · v = 0}."
"For the purposes of modelling word-meanings, we might think of ‘orthogonal’ as a model for ‘com-pletely unrelated’ (having similarity score zero)."
"This makes perfect sense for information retrieval, where we assume (for example) that if two words never occur in the same document then they have no features in common."
"Definition 1 Let a,b ∈ V and A,B ≤ V. By NOT A we mean A ⊥ and by NOT a, we mean hai ⊥ , where hai = {λa : λ ∈ R} is the 1-dimensional subspace subspace generated by a. By a NOT B we mean the projection of a onto B ⊥ and by a NOT b we mean the projection of a onto hbi ⊥ ."
We now show how to use these notions to perform calculations with individual term or query vectors in a form which is simple to program and efficient to run.
"Theorem 1 Let a,b ∈ V ."
Then a NOT b is repre-sented by the vector a · b a NOT b ≡ a − |b| 2 b. where |b| 2 = b · b is the modulus of b. Proof.
A simple proof is given[REF_CITE].
"For normalised vectors, Theorem 1 takes the par-ticularly simple form a NOT b = a − (a · b)b, (1) which in practice is then renormalised for consis-tency."
"One computational benefit is that Theorem 1 gives a single vector for a NOT b, so finding the sim-ilarity between any other vector and a NOT b is just a single scalar product computation."
"Disjunction is also simple to envisage, the expres-sion b 1 OR ... OR b n being modelled by the sub-space"
B = {λ 1 b 1 + . . . + λ n b n : λ i ∈ R}.
"Theoretical motivation for this formulation can be found in (Birkhoff and v[REF_CITE]§1,§6) and[REF_CITE]: for example, B is the smallest subspace of V which contains the set {b j }."
"Computing the similarity between a vector a and this subspace B is computationally more expensive than for the negation of Theorem 1, because the scalar product of a with (up to) n vectors in an or-thogonal basis for B must be computed."
Thus the gain we get by comparing each document with the query a NOT b using only one scalar product oper-ation is absent for disjunction.
"However, this benefit is regained in the case of negated disjunction."
Suppose we negate not only one argument but several.
"If a user specifies that they want documents related to a but not b 1 , b 2 , . . . , b n , then (unless otherwise stated) it is clear that they only want documents related to none of the un-wanted terms b i (rather than, say, the average of these terms)."
"This motivates a process which can be thought of as a vector formulation of the classical de Morgan equivalence ∼ a∧ ∼ b ≡∼ (a ∨ b), by which the expression a AND NOT b 1 AND NOT b 2 . . ."
AND NOT b n is translated to a NOT (b 1 OR ... OR b n ). (2)
"Using Definition 1, this expression can be modelled with a unique vector which is orthogonal to all of the unwanted arguments {b 1 }."
"However, unless the vectors b 1 , . . . , b n are orthogonal (or identical), we need to obtain an orthogonal basis for the subspace b 1 OR . . ."
OR b n before we can implement a higher-dimensional version of Theorem 1.
"This is because the projection operators involved are in general non-commutative, one of the hallmark differences be-tween Boolean and quantum logic."
In this way vector negation generates a meaning-vector which takes into account the similarities and differences between the negative terms.
"A query for chip NOT computer, silicon is treated differently from a query for chip NOT computer, potato."
"Vector negation is capable of realising that for the first query, the two negative terms are referring to the same general topic area, but in the second case the task is to remove radically different meanings from the query."
"This technique has been used to remove several meanings from a query iteratively, al-lowing a user to ‘home in on’ the desired meaning by systematically pruning away unwanted features."
Our first experiments with vector negation were to determine whether the negation operator could find different senses of ambiguous words by negating a word closely related to one of the meanings.
"A vector space model was built using Latent Semantic Analy-sis, similar to the systems[REF_CITE]."
"The effect of LSA is to in-crease linear dependency between terms, and for this reason it is likely that LSA is a crucial step in our approach."
"Terms were indexed depending on their co-occurrence with 1000 frequent “content-bearing words” in a 15 word context-window, giving each term 1000 coordinates."
This was reduced to 100 di-mensions using singular value decomposition.
"Later on, document vectors were assigned in the usual manner by summation of term vectors using tf-idf weighting ([REF_CITE]p. 121)."
"Vectors were normalised, so that the standard (Euclidean) scalar product and cosine similarity coincided."
This scalar product was used as a measure of term-term and term-document similarity throughout our exper-iments.
"This method was used because it has been found to be effective at producing good term-term similarities for word-sense disambiguati[REF_CITE]and automatic lexical acquisiti[REF_CITE], and these similarities were used to generate in-teresting queries and to judge the effectiveness of dif-ferent forms of negation."
More details on the build-ing of this vector space model can be found[REF_CITE].
"Two early results using negation to find senses of ambiguous words are given in Table 1, showing that vector negation is very effective for removing the ‘le-gal’ meaning from the word suit and the ‘sporting’ meaning from the word play, leaving respectively the ‘clothing’ and ‘performance’ meanings."
Note that re- moving a particular word also removes concepts re-lated to the negated word.
"This gives credence to the claim that our mathematical model is removing the meaning of a word, rather than just a string of characters."
"This encouraged us to set up a larger scale experiment to test this hypothesis, which is de-scribed in Section 4."
"There have been rigourous studies of Boolean op-erators for information retrieval, including the p-norms[REF_CITE]and the matrix forms[REF_CITE], which have focussed partic-ularly on mathematical expressions for conjunction and disjunction."
"However, typical forms of negation (such as NOT p = 1−p) have not taken into account the relationship between the negated argument and the rest of the query."
Negation has been used in two main forms in IR systems: for the removal of unwanted documents af-ter retrieval and for negative relevance feedback.
We describe these methods and compare them with vec-tor negation.
A traditional Boolean search for documents related to the query a NOT b would return simply those doc-uments which contain the term a and do not contain the term b.
"More formally, let D be the document collection and let"
D i ⊂
D be the subset of docu-ments containing the term i.
"Then the results to the Boolean query for a NOT b would be the set D a ∩D 0b , where D b0 is the complement of D b in D. Variants of this are used within a vector model, by using vector retrieval to retrieve a (ranked) set of relevant docu-ments and then ‘throwing away’ documents contain-ing the unwanted terms ([REF_CITE]p. 26)."
This paper will refer to such methods under the general heading of ‘post-retrieval filtering’.
There are at least three reasons for preferring vec-tor negation to post-retrieval filtering.
"Firstly, post-retrieval filtering is not very principled and is subject to error: for example, it would remove a long docu-ment containing only one instance of the unwanted term."
"One might argue here that if a document contain-ing unwanted terms is given a ‘negative-score’ rather than just disqualified, this problem is avoided."
"This would leaves us considering a combined score, sim(d, a NOT b) = d · a − λd · b for some parameter λ."
"However, since this is the same as d · (a − λb), it is computationally more ef- ficient to treat a − λb as a single vector."
"This is exactly what vector negation accomplishes, and also determines a suitable value of λ from a and b."
Thus a second benefit for vector negation is that it pro-duces a combined vector for a NOT b which enables the relevance score of each document to be computed using just one scalar product operation.
"The third gain is that vector retrieval proves to be better at removing not only an unwanted term but also its synonyms and related words (see Section 4), which is clearly desirable if we wish to remove not only a string of characters but the meaning repre-sented by this string."
Relevance feedback has been shown to improve re-trieval[REF_CITE].
"In this process, documents judged to be relevant have (some multiple of) their document vector added to the query: docu-ments judged to be non-relevant have (some multiple of) their document vector subtracted from the query, producing a new query according to the formula"
"Q i+1 = αQ i + β X D i − γ X D i , |D i | |D i | rel nonrel where Q i is the i th query vector, D i is the set of doc-uments returned by Q i which has been partitioned into relevant and non-relevant subsets, and α, β, γ ∈ R are constants."
The positive feedback part of this process has become standard in many search engines with op-tions such as “More documents like this” or “Similar pages”.
The subtraction option (called ‘negative rel-evance feedback’) is much rarer.
"A widely held opin-ion is that that negative feedback is liable to harm retrieval, because it may move the query away from relevant as well as non-relevant documents ([REF_CITE]p. 160)."
The concepts behind negative relevance feedback are discussed instructively[REF_CITE].
"Neg-ative relevance feedback introduces the idea of sub-tracting an unwanted vector from a query, but gives no general method for deciding “how much to sub-tract”."
We shall refer to such methods as ‘Constant Subtraction’.
"Dunlop (1997, p. 139) gives an anal-ysis which leads to a very intuitive reason for pre-ferring vector negation over constant subtraction."
"If a user removes an unwanted term which the model deems to be closely related to the desired term, this should have a strong effect, because there is a sig-nificant ‘difference of opinion’ between the user and the model. (From an even more informal point of view, why would anyone take the trouble to remove a meaning that isn’t there anyway?)."
"With any kind of constant subtraction, however, the removal of dis-tant points has a greater effect on the final query-statement than the removal of nearby points."
Vector negation corrects this intuitive mismatch.
Recall from Equation 1 that (using normalised vec-tors for simplicity) the vector a NOT b is given by a − (a · b)b.
The similarity of a with a NOT b is therefore a · (a − (a · b)b) = 1 − (a · b) 2 .
"The closer a and b are, the greater the (a · b) 2 factor becomes, so the similarity of a with a NOT b be-comes smaller the closer a is to b."
This coincides ex-actly with Dunlop’s intuitive view: removing a con-cept which in the model is very close to the original query has a large effect on the outcome.
"Negative relevance feedback introduces the idea of subtract-ing an unwanted vector from a query, but gives no general method for deciding ‘how much to subtract’."
We shall refer to such methods as ‘Constant Subtrac-tion’.
"This section describes experiments which compare the three methods of negation described above (post-retrieval filtering, constant subtraction and vector negation) with the baseline alternative of no nega-tion at all."
The experiments were carried out using the vector space model described in Section 2.1.
"To judge the effectiveness of different methods at removing unwanted meanings, with a large number of queries, we made the following assumptions."
A document which is relevant to the meaning of ‘term a NOT term b’ should contain as many references to term a and as few references to term b as possible.
"Close neighbours and synonyms of term b are unde-sirable as well, since if they occur the document in question is likely to be related to the negated term even if the negated term itself does not appear."
1200 queries of the form ‘term a NOT term b’ were generated for 3 different document collections.
"The terms chosen were the 100 most frequently occurring (non-stop) words in the collection, 100 mid-frequency words (the 1001 st to 1100 th most frequent), and 100 low-frequency words (the 5001 st to 5100 th most fre-quent)."
The nearest neighbour (word with highest cosine similarity) to each positive term was taken to be the negated term. (This assumes that a user is most likely to want to remove a meaning closely related to the positive term: there is no point in re-moving unrelated information which would not be retrieved anyway.)
"In addition, for the 100 most fre-quent words, an extra retrieval task was performed with the roles of the positive term and the negated term reversed, so that in this case the system was be-ing asked to remove the very most common words in the collection from a query generated by their near-est neighbour."
"We anticipated that this would be an especially difficult task, and a particularly real-istic one, simulating a user who is swamped with information about a ‘popular topic’ in which they are not interested. 1"
"The document collections used were from the British National Corpus (published by Oxford University, the textual data consisting of ca 90M words, 85K documents), the New York Times News Syndicate (1994-96, from the North American News Text Corpus published by the Linguistic Data Consortium, ca 143M words, 370K documents) and the Ohsumed corpus of medical documents[REF_CITE](ca 40M words, 230K documents)."
The query was just the positive term and the negated term was ignored. • Post-retrieval filtering.
"After vector retrieval us-ing only the positive term as the query term, documents containing the negated term were eliminated. • Constant subtraction."
Experiments were per-formed with a variety of subtraction constants.
"The query a NOT b was thus given the vector a−λb for some λ ∈ [0, [Footnote_1]]."
"1 For reasons of space we do not show the retrieval per-formance on query terms of different frequencies in this paper, though more detailed results are available from the author on request."
"The results recorded in this paper were obtained using λ = 0.75, which gives a direct comparison with vector negation. • Vector negation, as described in this paper."
"For each set of retrieved documents, the following results were counted. • The relative frequency of the positive term. • The relative frequency of the negated term. • The relative frequency of the ten nearest neigh-bours of the negative term."
"One slight subtlety here is that the positive term was itself a close neighbour of the negated term: to avoid incon-sistency, we took as ‘negative neighbours’ only those which were closer to the negated term than to the positive term. • The relative frequency of the synonyms of the negated term, as given by the WordNet database[REF_CITE]."
"As above, words which were also synonyms of the positive term were dis-counted."
"On the whole fewer such synonyms were found in the Ohsumed and NYT docu-ments, which have many medical terms and proper names which are not in WordNet."
Additional experiments were carried out to com-pare the effectiveness of different forms of negation at removing several unwanted terms.
"For two negated terms, the post-retrieval filtering process worked by discarding documents containing either of the negative terms."
Constant subtraction worked by subtracting a constant multiple of each of the negated terms from the query.
"Vector nega-tion worked by making the query vector orthogonal to the plane generated by the two negated terms, as in Equation 2."
Results were collected in much the same way as the results for single-argument negation.
"Occurrences of each of the negated terms were added together, as were occurrences of the neighbours and WordNet synonyms of either of the negated words."
The results of our experiments are collected in Table 2 and summarised in Figure 1.
The results for a single negated term demonstrate the following points. • All forms of negation proved extremely good at removing the unwanted words.
"This is triv-ially true for post-retrieval filtering, which works by discarding any documents that contain the negated term."
"It is more interesting that con-stant subtraction and vector negation performed so well, cutting occurrences of the negated word by 82% and 85% respectively compared with the baseline of no negation. • On average, using no negation at all retrieved the most positive terms, though not in every case."
"While this upholds the claim that any form of negation is likely to remove relevant as well as irrelevant results, the damage done was only around 3% for post-retrieval filtering and 25% for constant and vector negation. • These observations alone would suggest that post-retrieval filtering is the best method for the simple goal of maximising occurrences of the positive term while minimising the occur-rences of the negated term."
"However, vec-tor negation and constant subtraction dramati-cally outperformed post-retrieval filtering at re-moving neighbours of the negated terms, and were reliably better at removing WordNet syn-onyms as well."
"We believe this to be good evidence that, while post-search filtering is by definition better at removing unwanted strings, the vector methods (either orthogonal or con-stant subtraction) are much better at removing unwanted meanings."
"Preliminary observations suggest that in the cases where vector negation retrieves fewer occurrences of the positive term than other methods, the other methods are of-ten retrieving documents that are still related in meaning to the negated term. • Constant subtraction can give similar results to vector negation on these queries (though the vector negation results are slightly better)."
"This is with queries where the negated term is the closest neighbour of the positive term, and the assumption that the similarity between these pairs is around 0.75 is a reasonable approxima-tion."
"However, further experiments with a va-riety of negated arguments chosen at random from a list of neighbours demonstrated that in this more general setting, the flexibility provided by vector negation produced conclusively better results than constant subtraction for any single fixed constant."
"In addition, the results for removing multiple negated terms demonstrate the following points. • Removing another negated term further reduces the retrieval of the positive term for all forms of negation."
"Constant subtraction is the worst af-fected, performing noticeably worse than vector negation. • All three forms of negation still remove many occurrences of the negated term."
Vector nega-tion and (trivially) post-search filtering perform as well as they do with a single negated term.
"However, constant subtraction performs much worse, retrieving more than twice as many un-wanted terms as vector negation. • Post-retrieval filtering was even less effective at removing neighbours of the negated term than with a single negated term."
Constant subtrac-tion also performed much less well.
Vector nega-tion was by far the best method for remov-ing negative neighbours.
"The same observation holds for WordNet synonyms, though the results are less pronounced."
"This shows that vector negation is capable of re-moving unwanted terms and their related words from retrieval results, while retaining more occurrences of the original query term than constant subtraction."
"Vector negation does much better than other meth-ods at removing neighbours and synonyms, and we therefore expect that it is better at removing doc-uments referring to unwanted meanings of ambigu-ous words."
Experiments with sense-tagged data are planned to test this hypothesis.
"The goal of these experiments was to evaluate the extent to which the different methods could remove unwanted meanings, which we measured by count-ing the frequency of unwanted terms and concepts in retrieved documents."
"This leaves the problems of determining the optimal scope for the negation quan-tifier for an IR system, and of developing a natural user interface for this process for complex queries."
"These important challenges are beyond the scope of this paper, but would need to be addressed to in-corporate vector negation into a state-of-the-art IR system."
Traditional branches of science have exploited the structure inherent in vector spaces and developed rigourous techniques which could contribute to nat-ural language processing.
"As an example of this po-tential fertility, we have adapted the negation and disjunction connectives used in quantum logic to the tasks of word-sense discrimination and information retrieval."
Experiments focussing on the use of vector nega-tion to remove individual and multiple terms from queries have shown that this is a powerful and ef-ficient tool for removing both unwanted terms and their related meanings from retrieved documents.
"Because it associates a unique vector to each query statement involving negation, the similarity between each document and the query can be calculated using just one scalar product computation, a considerable gain in efficiency over methods which involve some form of post-retrieval filtering."
"We hope that these preliminary aspects will be initial gains in developing a concrete and effective system for learning, representing and composing as-pects of lexical meaning."
"In statistical machine translation, the gen-eration of a translation hypothesis is com-putationally expensive."
"If arbitrary word-reorderings are permitted, the search prob-lem is NP-hard."
"On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm."
"In this paper, we compare two different re-ordering constraints, namely the ITG con-straints and the IBM constraints."
This comparison includes a theoretical dis-cussion on the permitted number of re-orderings for each of these constraints.
We show a connection between the ITG constraints and the since 1870 known Schröder numbers.
We evaluate these constraints on two tasks: the Verbmobil task and the Cana-dian Hansards task.
"The evaluation con-sists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these con-straints."
"Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses."
The experiments will show that the base-line ITG constraints are not sufficient on the Canadian Hansards task.
"There-fore, we present an extension to the ITG constraints."
These extended ITG con-straints increase the alignment coverage from about 87% to 96%.
"In statistical machine translation, we are given a source language (‘French’) sentence f 1J = f 1 . . . f j . . . f J , which is to be translated into a target language (‘English’) sentence e I1 = e 1 . . . e i . . . e"
"Among all possible target language sentences, we will choose the sentence with the highest probabil-ity: ê I1 = argmax {P r(e I1 |f 1J )} (1) e I1 = argmax {P r(e 1I ) ·"
P r(f 1J |e I1 )} (2) e I1
The decomposition into two knowledge sources in Eq. 2 is the so-called source-channel approach to statistical machine translati[REF_CITE].
It allows an independent modeling of tar-get language model Pr(e I1 ) and translation model Pr(f 1J |e I1 ).
The target language model describes the well-formedness of the target language sentence.
The translation model links the source language sen-tence to the target language sentence.
It can be fur-ther decomposed into alignment and lexicon model.
"The argmax operation denotes the search problem, i.e. the generation of the output sentence in the tar-get language."
We have to maximize over all possible target language sentences.
"In this paper, we will focus on the alignment problem, i.e. the mapping between source sen-tence positions and target sentence positions."
"As the word order in source and target language may differ, the search algorithm has to allow certain word-reorderings."
"If arbitrary word-reorderings are allowed, the search problem is NP-hard[REF_CITE]."
"Therefore, we have to restrict the possible reorderings in some way to make the search prob-lem feasible."
"Here, we will discuss two such con-straints in detail."
The first constraints are based on inversion transduction grammars (ITG)[REF_CITE].
"In the following, we will call these the ITG constraints."
The second constraints are the IBM constraints[REF_CITE].
"In the next section, we will describe these constraints from a theoretical point of view."
"Then, we will describe the resulting search algorithm and its extension for word graph generation."
"Afterwards, we will analyze the Viterbi alignments produced during the training of the align-ment models."
"Then, we will compare the translation results when restricting the search to either of these constraints."
"In this section, we will discuss the reordering con-straints from a theoretical point of view."
We will answer the question of how many word-reorderings are permitted for the ITG constraints as well as for the IBM constraints.
"Since we are only interested in the number of possible reorderings, the specific word identities are of no importance here."
"Further-more, we assume a one-to-one correspondence be-tween source and target words."
"Thus, we are inter-ested in the number of word-reorderings, i.e. permu-tations, that satisfy the chosen constraints."
"First, we will consider the ITG constraints."
"Afterwards, we will describe the IBM constraints."
Let us now consider the ITG constraints.
"Here, we interpret the input sentence as a sequence of blocks."
"In the beginning, each position is a block of its own."
"Then, the permutation process can be seen as fol-lows: we select two consecutive blocks and merge them to a single block by choosing between two op-tions: either keep them in monotone order or invert the order."
This idea is illustrated in Fig. 1.
The white boxes represent the two blocks to be merged.
"Now, we investigate, how many permutations are obtainable with this method."
A permutation derived by the above method can be represented as a binary tree where the inner nodes are colored either black or white.
At black nodes the resulting sequences of the children are inverted.
At white nodes they are kept in monotone order.
This representation is equivalent to the parse trees of the simple grammar[REF_CITE].
We observe that a given permutation may be con-structed in several ways by the above method.
"For instance, let us consider the identity permutation of 1, 2, ..., n. Any binary tree with n nodes and all in-ner nodes colored white (monotone order) is a pos-sible representation of this permutation."
"To obtain a unique representation, we pose an additional con-straint on the binary trees: if the right son of a node is an inner node, it has to be colored with the oppo-site color."
"With this constraint, each of these binary trees is unique and equivalent to a parse tree of the ’canonical-form’ grammar[REF_CITE]."
"In[REF_CITE], it is shown that the number of such binary trees with n nodes is the (n − 1)th large Schröder number S n−1 ."
The (small) Schröder numbers have been first described[REF_CITE]as the number of bracketings of a given sequence (Schröder’s second problem).
The large Schröder numbers are just twice the Schröder numbers.
Schröder remarked that the ratio between two√ consecutive Schröder numbers approaches 3 + 2 2 = 5.8284... .
"A second-order recurrence for the large Schröder numbers is: (n + 1)S n = 3(2n − 1)S n−1 − (n − 2)S n−2 with n ≥ 2 and S 0 = 1, S 1 = 2."
The Schröder numbers have many combinatori-cal interpretations.
"Here, we will mention only two of them."
The first one is another way of view-ing at the ITG constraints.
"The number of permu-tations of the sequence 1, 2, ..., n, which avoid the subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large Schröder number S n−1 ."
More details on forbidden subsequences can be found[REF_CITE].
The interesting point is that a search with the ITG con-straints cannot generate a word-reordering that con-tains one of these two subsequences.
"In[REF_CITE], these forbidden subsequences are called ’inside-out’ transpositions."
Another interpretation of the Schröder numbers is given[REF_CITE]:
The number of permutations that can be sorted with an output-restricted double-ended queue (deque) is exactly the large Schröder number.
"Additionally, Knuth presents an approxi-mation for the large Schröder numbers: √ S n ≈ c · (3 + 8) n · n − 32 (3) q √ where c is set to 12 (3 2 − 4)/π."
"This approxi-mation function confirms the√ result of Schröder, and we obtain S n ∈ Θ((3 +√ 8) n ), i.e. the Schröder numbers grow like (3 + 8) n ≈ 5.83 n ."
"In this section, we will describe the IBM constraints[REF_CITE]."
"Here, we mark each position in the source sentence either as covered or uncovered."
"In the beginning, all source positions are uncovered."
"Now, the target sentence is produced from bottom to top."
A target position must be aligned to one of the first k uncovered source positions.
The IBM con-straints are illustrated in Fig. 2.
For most of the target positions there are k per-mitted source positions.
Only towards the end of the sentence this is reduced to the number of remaining uncovered source positions.
Let n denote the length of the input sequence and let r n denote the permitted number of permutations with the IBM constraints.
"Then, we obtain: ½ k n−k · k! n &gt; k r n = (4) n! n ≤ k"
"Typically, k is set to 4."
"In this case, we obtain an asymptotic upper and lower bound of 4 n , i.e. r n ∈ Θ(4 n )."
"In Tab. 1, the ratio of the number of permitted re-orderings for the discussed constraints is listed as a function of the sentence length."
We see that for longer sentences the ITG constraints allow for more reorderings than the IBM constraints.
"For sentences of length 10 words, there are about twice as many reorderings for the ITG constraints than for the IBM constraints."
This ratio steadily increases.
"For longer sentences, the ITG constraints allow for much more flexibility than the IBM constraints."
"Now, let us get back to more practical aspects."
"Re-ordering constraints are more or less useless, if they do not allow the maximization of Eq. 2 to be per-formed in an efficient way."
"Therefore, in this sec-tion, we will describe different aspects of the search algorithm for the ITG constraints."
"First, we will present the dynamic programming equations and the resulting complexity."
"Then, we will describe prun-ing techniques to accelerate the search."
"Finally, we will extend the basic algorithm for the generation of word graphs."
The ITG constraints allow for a polynomial-time search algorithm.
It is based on the following dy-namic programming recursion equations.
"During the search a table Q j l ,j r ,e b ,e t is constructed."
"Here, Q j l ,j r ,e b ,e t denotes the probability of the best hy-pothesis translating the source words from position j l (left) to position j r (right) which begins with the target language word e b (bottom) and ends with the word e t (top)."
This is illustrated in Fig. 3.
"Here, we initialize this table with monotone trans-lations of IBM Model 4."
"Therefore, Q 0j l ,j r ,e b ,e t de-notes the probability of the best monotone hypothe-sis of IBM Model 4."
"Alternatively, we could use any other single-word based lexicon as well as phrase-based models for this initialization."
Our choice is the IBM Model4 to make the results as comparable as possible to the search with the IBM constraints.
"We introduce a new parameter p m (m=ˆ monotone), which denotes the probability of a monotone combi-nation of two partial hypotheses."
"Q j l ,j r ,e b ,e t n= (5) max Q 0j l ,j r ,e b ,e t , jl≤k&lt;jr, e0,e00"
"Q j l ,k,e b ,e 0 · Q k+1,j r ,e 00 ,e t · p(e 00 |e 0 ) · p m , o Q k+1,j r ,e b ,e 0 · Q j l ,k,e 00 ,e t · p(e 00 |e 0 ) · (1 − p m )"
"We formulated this equation for a bigram lan-guage model, but of course, the same method can also be applied for a trigram language model."
The resulting algorithm is similar to the CYK-parsing al-gorithm.
It has a worst-case complexity of O(J 3 · E 4 ).
"Here, J is the length of the source sentence and E is the vocabulary size of the target language."
"Although the described search algorithm has a polynomial-time complexity, even with a bigram language model the search space is very large."
A full search is possible but time consuming.
The situation gets even worse when a trigram language model is used.
"Therefore, pruning techniques are obligatory to reduce the translation time."
Pruning is applied to hypotheses that translate the same subsequence f jj lr of the source sentence.
We use pruning in the following two ways.
The first pruning technique is histogram pruning: we restrict the number of translation hypotheses per sequence f jj lr .
"For each sequence f jj lr , we keep only a fixed number of translation hypotheses."
The second prun-ing technique is threshold pruning: the idea is to re-move all hypotheses that have a low probability rela-tive to the best hypothesis.
"Therefore, we introduce a threshold pruning parameter q, with 0 ≤ q ≤ 1."
"Let Q ∗j l ,j r denote the maximum probability of all translation hypotheses for f jj lr ."
"Then, we prune a hypothesis iff:"
"Q j l ,j r ,e b ,e t &lt; q · Q j∗ l ,j r"
Applying these pruning techniques the computa-tional costs can be reduced significantly with almost no loss in translation quality.
The generation of word graphs for a bottom-top search with the IBM constraints is described[REF_CITE].
These methods cannot be applied to the CYK-style search for the ITG con-straints.
"Here, the idea for the generation of word graphs is the following: assuming we already have word graphs for the source sequences f jk l and f kj+ r 1 , then we can construct a word graph for the sequence f jj lr by concatenating the partial word graphs either in monotone or inverted order."
"Now, we describe this idea in a more formal way."
A word graph is a directed acyclic graph (dag) with one start and one end node.
The edges are annotated with target language words or phrases.
We also al-low ²-transitions.
These are edges annotated with the empty word.
"Additionally, edges may be anno-tated with probabilities of the language or translation model."
Each path from start node to end node rep-resents one translation hypothesis.
The probability of this hypothesis is calculated by multiplying the probabilities along the path.
"During the search, we have to combine two word graphs in either monotone or inverted order."
"This is done in the following way: we are given two word graphs w 1 and w 2 with start and end nodes (s 1 ,g 1 ) and (s 2 ,g 2 ), respectively."
"First, we add an ²-transition (g 1 ,s 2 ) from the end node of the first graph w 1 to the start node of the second graph w 2 and annotate this edge with the probability of a monotone concatenation p m ."
"Second, we create a copy of each of the original word graphs w 1 and w 2 ."
"Then, we add an ²-transition (g 2 , s 1 ) from the end node of the copied second graph to the start node of the copied first graph."
This edge is annotated with the probability of a inverted concatenation 1 − p m .
"Now, we have obtained two word graphs: one for a monotone and one for a inverted concatenation."
"The final word graphs is constructed by merging the two start nodes and the two end nodes, respectively."
"Let W(j l ,j r ) denote the word graph for the source sequence f jj lr ."
"This graph is constructed from the word graphs of all subsequences of (j l , j r )."
"Therefore, we assume, these word graphs have al-ready been produced."
"For all source positions k with j l ≤ k &lt; j r , we combine the word graphs W (j l , k) and W(k + 1, j r ) as described above."
"Finally, we merge all start nodes of these graphs as well as all end nodes."
"Now, we have obtained the word graph W (j l , j r ) for the source sequence f jj lr ."
"As initializa-tion, we use the word graphs of the monotone IBM4 search."
"In this section, we will extend the ITG constraints described in Sec. 2.1."
This extension will go beyond basic reordering constraints.
We already mentioned that the use of consecutive phrases within the ITG approach is straightforward.
The only thing we have to change is the initializa-tion of the Q-table.
"Now, we will extend this idea to phrases that are non-consecutive in the source lan-guage."
"For this purpose, we adopt the view of the ITG constraints as a bilingual grammar as, e.g.,[REF_CITE]."
"For the baseline ITG constraints, the resulting grammar is:"
A → [AA] | hAAi | f/e | f/² | ²/e
"Here, [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation."
Let us now consider the case of a source phrase consisting of two parts f 1 and f 2 .
Let e denote the corresponding target phrase.
We add the productions
A → [e/f 1 A ²/f 2 ] | he/f 1 A ²/f 2 i to the grammar.
"The probabilities of these pro-ductions are, dependent on the translation direction, p(e|f 1 ,f 2 ) or p(f 1 ,f 2 |e), respectively."
"Obviously, these productions are not in the normal form of an ITG, but with the method described[REF_CITE], they can be normalized."
In the following sections we will present results on two tasks.
"Therefore, in this section we will show the corpus statistics for each of these tasks."
The first task we will present results on is the Verb-mobil task[REF_CITE].
"The domain of this corpus is appointment scheduling, travel planning, and hotel reservation."
It consists of transcriptions of spontaneous speech.
Table 2 shows the corpus statistics of this corpus.
The training corpus (Train) was used to train the IBM model parameters.
"The remaining free parameters, i.e. p m and the model scaling factors[REF_CITE], were adjusted on the development corpus (Dev)."
The resulting sys-tem was evaluated on the test corpus (Test).
"Table 2: Statistics of training and test corpus for the Verbmobil task (PP=perplexity, SL=sentence length)."
"Additionally, we carried out experiments on the Canadian Hansards task."
"This task contains the pro-ceedings of the Canadian parliament, which are kept by law in both French and English."
About 3 million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium (LDC).
"Here, we use a subset of the data containing only sentences with a maximum length of 30 words."
Table 3 shows the training and test corpus statistics.
"In this section, we will investigate for each of the constraints the coverage of the training corpus align-ment."
"For this purpose, we compute the Viterbi alignment of IBM Model 5 with GIZA++[REF_CITE]."
This alignment is produced without any restrictions on word-reorderings.
"Then, we check for every sentence if the alignment satisfies each of the constraints."
The ratio of the number of satisfied alignments and the total number of sentences is re-ferred to as coverage.
Tab. 4 shows the results for the Verbmobil task and for the Canadian Hansards task.
"It contains the results for both translation direc-tions German-English (S→T) and English-German (T→S) for the Verbmobil task and French-English (S→T) and English-French (T→S) for the Canadian Hansards task, respectively."
"For the Verbmobil task, the baseline ITG con-straints and the IBM constraints result in a similar coverage."
It is about 91% for the German-English translation direction and about 88% for the English-German translation direction.
A significantly higher coverage of about 96% is obtained with the extended ITG constraints.
"Thus with the extended ITG con-straints, the coverage increases by about 8% abso-lute."
"For the Canadian Hansards task, the baseline ITG constraints yield a worse coverage than the IBM constraints."
"Especially for the English-French trans-lation direction, the ITG coverage of 73.6% is very low."
"Again, the extended ITG constraints obtained the best results."
"Here, the coverage increases from about 87% for the IBM constraints to about 96% for the extended ITG constraints."
"In our experiments, we use the following error crite-ria: • WER (word error rate): The WER is computed as the minimum num-ber of substitution, insertion and deletion oper-ations that have to be performed to convert the generated sentence into the target sentence. • PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order."
"The PER com-pares the words in the two sentences ignoring the word order. • mWER (multi-reference word error rate): For each test sentence, not only a single refer-ence translation is used, as for the WER, but a whole set of reference translations."
"For each translation hypothesis, the WER to the most similar sentence is calculated[REF_CITE]. • BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences[REF_CITE]."
"BLEU measures accuracy, i.e. large BLEU scores are better. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judg-ments by test persons are necessary."
Each translated sentence was judged by a human ex-aminer according to an error scale from 0.0 to 1.0[REF_CITE].
"In this section, we will present the translation results for both the IBM constraints and the baseline ITG constraints."
We used a single-word based search with IBM Model 4.
The initialization for the ITG constraints was done with monotone IBM Model 4 translations.
"So, the only difference between the two systems are the reordering constraints."
In Tab. 5 the results for the Verbmobil task are shown.
We see that the results on this task are sim-ilar.
The search with the ITG constraints yields slightly lower error rates.
Some translation examples of the Verbmobil task are shown in Tab. 6.
"We have to keep in mind, that the Verbmobil task consists of transcriptions of spontaneous speech."
"Therefore, the source sen-tences as well as the reference translations may have an unorthodox grammatical structure."
"In the first example, the German verb-group (“würde vorschla-gen”) is split into two parts."
The search with the ITG constraints is able to produce a correct transla-tion.
"With the IBM constraints, it is not possible to translate this verb-group correctly, because the dis-tance between the two parts is too large (more than four words)."
"As we see in the second example, in German the verb of a subordinate clause is placed at the end (“übernachten”)."
"The IBM search is not able to perform the necessary long-range reordering, as it is done with the ITG search."
The ITG constraints were introduced[REF_CITE].
"The applications were, for instance, the segmenta-tion of Chinese character sequences into Chinese “words” and the bracketing of the source sentence into sub-sentential chunks."
In[REF_CITE]the base-line ITG constraints were used for statistical ma-chine translation.
"The resulting algorithm is simi-lar to the one presented in Sect. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas[REF_CITE]a single-word based lexicon model is used."
In[REF_CITE]a model similar to Wu’s method was consid-ered.
We have described the ITG constraints in detail and compared them to the IBM constraints.
We draw the following conclusions: especially for long sentences the ITG constraints allow for higher flexibility in word-reordering than the IBM constraints.
"Regard-ing the Viterbi alignment in training, the baseline ITG constraints yield a similar coverage as the IBM constraints on the Verbmobil task."
On the Canadian Hansards task the baseline ITG constraints were not sufficient.
With the extended ITG constraints the coverage improves significantly on both tasks.
On the Canadian Hansards task the coverage increases from about 87% to about 96%.
We have presented a polynomial-time search al-gorithm for statistical machine translation based on the ITG constraints and its extension for the gen-eration of word graphs.
We have shown the trans-lation results for the Verbmobil task.
"On this task, the translation quality of the search with the base-line ITG constraints is already competitive with the results for the IBM constraints."
"Therefore, we ex-pect the search with the extended ITG constraints to outperform the search with the IBM constraints."
Future work will include the automatic extraction of the bilingual grammar as well as the use of this grammar for the translation process.
Truecasing is the process of restoring case information to badly-cased or non-cased text.
"This paper explores truecas-ing issues and proposes a statistical, lan-guage modeling based truecaser which achieves an accuracy of ∼98% on news articles."
Task based evaluation shows a 26% F-measure improvement in named entity recognition when using truecasing.
"In the context of automatic content ex-traction, mention detection on automatic speech recognition text is also improved by a factor of 8."
Truecasing also en-hances machine translation output legibil-ity and yields a BLEU score improvement of 80.2%.
This paper argues for the use of truecasing as a valuable component in text processing applications.
"While it is true that large, high quality text corpora are becoming a reality, it is also true that the digital world is flooded with enormous collections of low quality natural language text."
"Transcripts from var-ious audio sources, automatic speech recognition, optical character recognition, online messaging and gaming, email, and the web are just a few exam-ples of raw text sources with content often produced in a hurry, containing misspellings, insertions, dele-tions, grammatical errors, neologisms, jargon terms etc."
We want to enhance the quality of such sources in order to produce better rule-based systems and sharper statistical models.
"This paper focuses on truecasing, which is the process of restoring case information to raw text."
"Besides text rEaDaBILiTY, truecasing enhances the quality of case-carrying data, brings into the pic-ture new corpora originally considered too noisy for various NLP tasks, and performs case normalization across styles, sources, and genres."
Consider the following mildly ambiguous sen-tence “us rep. james pond showed up riding an it and going to a now meeting”.
The case-carrying al-ternative “US Rep. James Pond showed up riding an IT and going to a NOW meeting” is arguably better fit to be subjected to further processing.
Broadcast news transcripts contain casing errors which reduce the performance of tasks such as named entity tagging.
Automatic speech recognition produces non-cased text.
"Headlines, teasers, section headers - which carry high information content - are not properly cased for tasks such as question answer-ing."
Truecasing is an essential step in transforming these types of data into cleaner sources to be used by NLP applications. “the president” and “the President” are two viable surface forms that correctly convey the same infor-mation in the same context.
"Such discrepancies are usually due to differences in news source, authors, and stylistic choices."
"Truecasing can be used as a normalization tool across corpora in order to pro-duce consistent, context sensitive, case information; it consistently reduces expressions to their statistical canonical form."
"In this paper, we attempt to show the benefits of truecasing in general as a valuable building block for NLP applications rather than promoting a spe-cific implementation."
"We explore several truecasing issues and propose a statistical, language modeling based truecaser, showing its performance on news articles."
"Then, we present a straight forward appli-cation of truecasing on machine translation output."
"Finally, we demonstrate the considerable benefits of truecasing through task based evaluations on named entity tagging and automatic content extraction."
"Truecasing can be viewed in a lexical ambiguity res-olution framework[REF_CITE]as discriminat-ing among several versions of a word, which hap-pen to have different surface forms (casings)."
Word-sense disambiguation is a broad scope problem that has been tackled with fairly good results generally due to the fact that context is a very good pre-dictor when choosing the sense of a word.[REF_CITE]mention good results on limited case restoration experiments on toy problems with 100 words.
"They also observe that real world problems generally exhibit around 90% case restoration accu-racy.[REF_CITE]also approaches casing dis-ambiguation but models only instances when capi-talization is expected: first word in a sentence, after a period, and after quotes.[REF_CITE]attempted to extract named entities from non-cased text by using a weaker classifier but without focus-ing on regular text or case restoration."
Accents can be viewed as additional surface forms or alternate word casings.
"From this perspective, ei-ther accent identification can be extended to truecas-ing or truecasing can be extended to incorporate ac-cent restoration.[REF_CITE]reports good re-sults with statistical methods for Spanish and French accent restoration."
Truecasing is also a specialized method for spelling correction by relaxing the notion of casing to spelling variations.
There is a vast literature on spelling correcti[REF_CITE]using both linguistic and statis-tical approaches.
"Also,[REF_CITE]ap-ply a noisy channel model, based on generic string to string edits, to spelling correction."
In this paper we take a statistical approach to true-casing.
"First we present the baseline: a simple, straight forward unigram model which performs rea-sonably well in most cases."
"Then, we propose a bet-ter, more flexible statistical truecaser based on lan-guage modeling."
"From a truecasing perspective we observe four general classes of words: all lowercase (LC), first letter uppercase (UC), all letters uppercase (CA), and mixed case word MC)."
The MC class could be fur-ther refined into meaningful subclasses but for the purpose of this paper it is sufficient to correctly iden-tify specific true MC forms for each MC instance.
We are interested in correctly assigning case la-bels to words (tokens) in natural language text.
"This represents the ability to discriminate between class labels for the same lexical item, taking into account the surrounding words."
We are interested in casing word combinations observed during training as well as new phrases.
"The model requires the ability to generalize in order to recognize that even though the possibly misspelled token “lenon” has never been seen before, words in the same context usually take the UC form."
The goal of this paper is to show the benefits of true-casing in general.
The unigram baseline (presented below) is introduced in order to put task based eval-uations in perspective and not to be used as a straw-man baseline.
The vast majority of vocabulary items have only one surface form.
"Hence, it is only natural to adopt the unigram model as a baseline for truecasing."
"In most situations, the unigram model is a simple and efficient model for surface form restoration."
This method associates with each surface form a score based on the frequency of occurrence.
The decoding is very simple: the true case of a token is predicted by the most likely case of that token.
The unigram model’s upper bound on truecasing performance is given by the percentage of tokens that occur during decoding under their most frequent case.
Hence it is inevitable for the unigram model to fail on tokens such as “new”.
"Due to the over-whelming frequency of its LC form, “new” will take this particular form regardless of what token follows it."
"For both “information” and “york” as subsequent words, “new” will be labeled as LC."
"For the latter case, “new” occurs under one of its less frequent sur-face forms."
The truecasing strategy that we are proposing seeks to capture local context and bootstrap it across a sentence.
The case of a token will depend on the most likely meaning of the sentence - where local meaning is approximated by n-grams observed dur-ing training.
"However, the local context of a few words alone is not enough for case disambiguation."
Our proposed method employs sentence level con-text as well.
"We capture local context through a trigram lan-guage model, but the case label is decided at a sen-tence level."
A reasonable improvement over the un-igram model would have been to decide the word casing given the previous two lexical items and their corresponding case content.
"However, this greedy approach still disregards global cues."
Our goal is to maximize the probability of a larger text segment (i.e. a sentence) occurring under a certain surface form.
"Towards this goal, we first build a language model that can provide local context statistics."
Language modeling provides features for a label-ing scheme.
These features are based on the prob-ability of a lexical item and a case content condi-tioned on the history of previous two lexical items and their corresponding case content:
"P model (w 3 |w 2 , w 1 ) = λ trigram P (w 3 |w 2 , w 1 ) + λ bigram P (w 3 |w 2 ) + λ unigram P (w 3 ) + λ uniform P 0 (1) where trigram, bigram, unigram, and uniform prob-abilities are scaled by individual λ i s which are learned by observing training examples. w i repre-sents a word with a case tag treated as a unit for probability estimation."
Using the language model probabilities we de-code the case information at a sentence level.
We construct a trellis (figure 1) which incorporates all the sentence surface forms as well as the features computed during training.
"A node in this trellis con-sists of a lexical item, a position in the sentence, a possible casing, as well as a history of the previous two lexical items and their corresponding case con-tent."
"Hence, for each token, all surface forms will appear as nodes carrying additional context infor-mation."
"In the trellis, thicker arrows indicate higher transition probabilities."
The trellis can be viewed as a Hidden Markov Model (HMM) computing the state sequence which best explains the observations.
"The states (q 1 , q 2 , · · · , q n ) of the HMM are combinations of case and context information, the transition proba-bilities are the language model (λ) based features, and the observations (O 1 O 2 · · · O t ) are lexical items."
"During decoding, the Viterbi algorithm[REF_CITE]is used to compute the highest probability state sequence (q τ∗ at sentence level) that yields the desired case information: q τ∗ = argmax q i1 q i2 ···q it P (q i1 q i2 · · · q it |O 1 O 2 · · · O t , λ) (2) where P (q i1 q i2 · · · q it |O 1 O 2 · · · O t , λ) is the proba-bility of a given sequence conditioned on the obser-vation sequence and the model parameters."
"A more sophisticated approach could be envisioned, where either the observations or the states are more expres- sive."
These alternate design choices are not explored in this paper.
"Testing speed depends on the width and length of the trellis and the overall decoding complexity is: C decoding = O(SM H+1 ) where S is the sentence size, M is the number of surface forms we are will-ing to consider for each word, and H is the history size (H = 3 in the trigram case)."
In order for truecasing to be generalizable it must deal with unknown words — words not seen during training.
"For large training sets, an extreme assump-tion is that most words and corresponding casings possible in a language have been observed during training."
"Hence, most new tokens seen during de-coding are going to be either proper nouns or mis-spellings."
"The simplest strategy is to consider all unknown words as being of the UC form (i.e. peo-ple’s names, places, organizations)."
Another approach is to replace the less frequent vocabulary items with case-carrying special tokens.
"During training, the word mispeling is replaced with by UNKNOWN LC and the word Lenon with UN-KNOWN UC."
This transformation is based on the observation that similar types of infrequent words will occur during decoding.
This transformation cre-ates the precedent of unknown words of a particular format being observed in a certain context.
"When a truly unknown word will be seen in the same con-text, the most appropriate casing will be applied."
This was the method used in our experiments.
"A similar method is to apply the case-carrying special token transformation only to a small random sam-ple of all tokens, thus capturing context regardless of frequency of occurrence."
"A reasonable truecasing strategy is to focus on to-ken classification into three categories: LC, UC, and CA."
"In most text corpora mixed case tokens such as McCartney, CoOl, and TheBeatles occur with mod-erate frequency."
Some NLP tasks might prefer map-ping MC tokens starting with an uppercase letter into the UC surface form.
This technique will reduce the feature space and allow for sharper models.
"How-ever, the decoding process can be generalized to in-clude mixed cases in order to find a closer fit to the true sentence."
"In a clean version of the AQUAINT (ARDA) news stories corpus, ∼ 90% of the tokens occurred under the most frequent surface form (fig-ure 2)."
The expensive brute force approach will consider all possible casings of a word.
"Even with the full casing space covered, some mixed cases will not be seen during training and the language model prob-abilities for n-grams containing certain words will back off to an unknown word strategy."
"A more fea-sible method is to account only for the mixed case items observed during training, relying on a large enough training corpus."
A variable beam decod-ing will assign non-zero probabilities to all known casings of each word.
An n-best approximation is somewhat faster and easier to implement and is the approach employed in our experiments.
During the sentence-level decoding only the n-most-frequent mixed casings seen during training are considered.
"If the true capitalization is not among these n-best versions, the decoding is not correct."
Additional lex-ical and morphological features might be needed if identifying MC instances is critical.
The first word in a sentence is generally under the UC form.
This sentence-begin indicator is some-times ambiguous even when paired with sentence-end indicators such as the period.
"While sentence splitting is not within the scope of this paper, we want to emphasize the fact that many NLP tasks would benefit from knowing the true case of the first word in the sentence, thus avoiding having to learn the fact that beginning of sentences are artificially important."
"Since it is uneventful to convert the first letter of a sentence to uppercase, a more interest-ing problem from a truecasing perspective is to learn how to predict the correct case of the first word in a sentence (i.e. not always UC)."
"If the language model is built on clean sentences accounting for sentence boundaries, the decoding will most likely uppercase the first letter of any sen-tence."
"On the other hand, if the language model is trained on clean sentences disregarding sentence boundaries, the model will be less accurate since dif-ferent casings will be presented for the same context and artificial n-grams will be seen when transition-ing between sentences."
One way to obtain the de-sired effect is to discard the first n tokens in the train-ing sentences in order to escape the sentence-begin effect.
The language model is then built on smoother context.
A similar effect can be obtained by initial-izing the decoding with n-gram state probabilities so that the boundary information is masked.
"Both the unigram model and the language model based truecaser were trained on the AQUAINT (ARDA) and TREC (NIST) corpora, each consist-ing of 500M token news stories from various news agencies."
The truecaser was built using IBM’s ViaVoice TM language modeling tools.
These tools implement trigram language models using deleted interpolation for backing off if the trigram is not found in the training data.
The resulting model’s perplexity is 108.
"Since there is no absolute truth when truecasing a sentence, the experiments need to be built with some reference in mind."
Our assumption is that profes-sionally written news articles are very close to an intangible absolute truth in terms of casing.
"Fur-thermore, we ignore the impact of diverging stylistic forms, assuming the differences are minor."
Based on the above assumptions we judge the truecasing methods on four different test sets.
"The first test set (APR) consists of the[REF_CITE]∗ top 20 news stories from Associated Press and Reuters excluding titles, headlines, and sec-tion headers which together form the second test set (APR+)."
The third test set (ACE) consists of ear- lier news stories from AP and New York Times be-longing to the ACE dataset.
The last test set (MT) includes a set of machine translation references (i.e. human translations) of news articles from the Xin-hua agency.
"The sizes of the data sets are as follows: APR - 12k tokens, ACE - 90k tokens, and MT - 63k tokens."
"For both truecasing methods, we computed the agreement with the original news story consid-ered to be the ground truth."
The language model based truecaser consistently displayed a significant error reduction in case restoration over the unigram model (figure 3).
"On current news stories, the truecaser agreement with the original articles is ∼ 98%."
Titles and headlines usually have a higher con-centration of named entities than normal text.
This also means that they need a more complex model to assign case information more accurately.
The LM based truecaser performs better in this environment while the unigram model misses named entity com-ponents which happen to have a less frequent surface form.
The original reference articles are assumed to have the absolute true form.
"However, differences from these original articles and the truecased articles are not always casing errors."
The truecaser tends to modify the first word in a quotation if it is not proper name: “There has been” becomes “there has been”.
It also makes changes which could be con-sidered a correction of the original article: “Xinhua news agency” becomes “Xinhua News Agency” and “northern alliance” is truecased as “Northern Al-liance”.
In more ambiguous cases both the original version and the truecased fragment represent differ-ent stylistic forms: “prime minister Hekmatyar” be-comes “Prime Minister Hekmatyar”.
There are also cases where the truecaser described in this paper makes errors.
New movie names are sometimes miss-cased: “my big fat greek wedding” or “signs”.
"In conducive contexts, person names are correctly cased: “DeLay said in”."
"However, in ambiguous, adverse contexts they are considered to be common nouns: “pond” or “to delay that”."
Un-seen organization names which make perfectly nor-mal phrases are erroneously cased as well: “interna-tional security assistance force”.
We have applied truecasing as a post-processing step to a state of the art machine translation system in or-der to improve readability.
"For translation between Chinese and English, or Japanese and English, there is no transfer of case information."
In these situations the translation output has no case information and it is beneficial to apply truecasing as a post-processing step.
This makes the output more legible and the system performance increases if case information is required.
We have applied truecasing to Chinese-to-English translation output.
The data source consists of news stories (2500 sentences) from the Xinhua News Agency.
"The news stories are first translated, then subjected to truecasing."
"The translation output is evaluated with BLEU[REF_CITE], which is a robust, language independent automatic ma-chine translation evaluation method."
"BLEU scores are highly correlated to human judges scores, pro-viding a way to perform frequent and accurate au-tomated evaluations."
BLEU uses a modified n-gram precision metric and a weighting scheme that places more emphasis on longer n-grams.
"In table 1, both truecasing methods are applied to machine translation output with and without upper-casing the first letter in each sentence."
"The truecas-ing methods are compared against the all letters low-ercased version of the articles as well as against an existing rule-based system which is aware of a lim-ited number of entity casings such as dates, cities, and countries."
The LM based truecaser is very ef-fective in increasing the readability of articles and captures an important aspect that the BLEU score is sensitive to.
Truecasig the translation output yields an improvement † of 80.2% in BLEU score over the existing rule base system.
Case restoration and normalization can be employed for more complex tasks.
We have successfully lever-aged truecasing in improving named entity recogni-tion and automatic content extraction.
"In order to evaluate the effect of truecasing on ex-tracting named entity labels, we tested an existing named entity system on a test set that has signif-icant case mismatch to the training of the system."
"The base system is an HMM based tagger, similar[REF_CITE]."
The system has 31 semantic categories which are extensions on the MUC cate-gories.
The tagger creates a lattice of decisions cor-responding to tokenized words in the input stream.
"When tagging a word w i in a sentence of words w 0 ...w N , two possibilities."
"If a tag begins: p(t N1 |w N1 ) i = p(t i |t i−1 , w i−1 )p † (w i |t i , w i−1 )"
"If a tag continues: p(t N1 |w 1N ) i = p(w i |t i , w i−1 )"
The † indicates that the distribution is formed from words that are the first words of entities.
The p † dis-tribution predicts the probability of seeing that word given the tag and the previous word instead of the tag and previous tag.
"Each word has a set of fea-tures, some of which indicate the casing and embed-ded punctuation."
These models have several levels of back-off when the exact trigram has not been seen in training.
"A trellis spanning the 31 futures is built for each word in a sentence and the best path is de-rived using the Viterbi algorithm. † Truecasing improves legibility, not the translation itself"
The performance of the system shown in table 2 indicate an overall 26.52% F-measure improvement when using truecasing.
The alternative to truecas-ing text is to destroy case information in the train-ing material SNORIFY procedure[REF_CITE].
"Case is an important feature in detecting most named entities but particularly so for the title of a work, an organization, or an ambiguous word with two frequent cases."
"Truecasing the sentence is essential in detecting that “To Kill a Mockingbird” is the name of a book, especially if the quotation marks are left off."
Automatic Content Extraction (ACE) is task fo-cusing on the extraction of mentions of entities and relations between them from textual data.
"The tex-tual documents are from newswire, broadcast news with text derived from automatic speech recognition (ASR), and newspaper with text derived from optical character recognition (OCR) sources."
"The mention detection task (ace, 2001) comprises the extraction of named (e.g. ”Mr. Isaac Asimov”), nominal (e.g. ”the complete author”), and pronominal (e.g. ”him”) mentions of Persons, Organizations, Locations, Fa-cilities, and Geo-Political Entities."
"The automatically transcribed (using ASR) broad-cast news documents and the translated Xinhua News Agency (XINHUA) documents in the ACE corpus do not contain any case information, while human transcribed broadcast news documents con-tain casing errors (e.g. “George bush”)."
This prob-lem occurs especially when the data source is noisy or the articles are poorly written.
"For all documents from broadcast news (human transcribed and automatically transcribed) and XIN-HUA sources, we extracted mentions before and af-ter applying truecasing."
"The ASR transcribed broad-cast news data comprised 86 documents containing a total of 15,535 words, the human transcribed ver-sion contained 15,131 words."
There were only two XINHUA documents in the ACE test set containing a total of 601 words.
None of this data or any ACE data was used for training the truecasing models.
"Table 3 shows the result of running our ACE par-ticipating maximum entropy mention detection sys-tem on the raw text, as well as on truecased text."
"For ASR transcribed documents, we obtained an eight fold improvement in mention detection from 5% F-measure to 46% F-measure."
"The low baseline score is mostly due to the fact that our system has been trained on newswire stories available from previous ACE evaluations, while the latest test data included ASR output."
It is very likely that the improvement due to truecasing will be more modest for the next ACE evaluation when our system will be trained on ASR output as well.
"Although the statistical model we have considered performs very well, further improvements must go beyond language modeling, enhancing how expres-sive the model is."
"Additional features are needed during decoding to capture context outside of the current lexical item, medium range context, as well as discontinuous context."
"Another potentially help-ful feature to consider would provide a distribu-tion over similar lexical items, perhaps using an edit/phonetic distance."
Truecasing can be extended to cover a more gen-eral notion surface form to include accents.
"De-pending on the context, words might take different surface forms."
"Since punctuation is a notion exten-sion to surface form, shallow punctuation restora-tion (e.g. word followed by comma) can also be ad-dressed through truecasing."
"We have discussed truecasing, the process of restor-ing case information to badly-cased or non-cased text, and we have proposed a statistical, language modeling based truecaser which has an agreement of ∼98% with professionally written news articles."
"Although its most direct impact is improving legibil-ity, truecasing is useful in case normalization across styles, genres, and sources."
Truecasing is a valu- able component in further natural language process-ing.
Task based evaluation shows a 26% F-measure improvement in named entity recognition when us-ing truecasing.
"In the context of automatic content extraction, mention detection on automatic speech recognition text is improved by a factor of 8."
True-casing also enhances machine translation output leg-ibility and yields a BLEU score improvement of 80.2% over the original system.
"Often, the training procedure for statisti-cal machine translation models is based on maximum likelihood or related criteria."
A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.
"In this paper, we analyze various training criteria which directly optimize translation qual-ity."
These training criteria make use of re-cently proposed automatic evaluation met-rics.
We describe a new algorithm for effi-cient training an unsmoothed error count.
We show that significantly better results can often be obtained if the final evalua-tion criterion is taken directly into account as part of the training procedure.
Many tasks in natural language processing have evaluation criteria that go beyond simply count-ing the number of wrong decisions the system makes.
"Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked retrieval, and BLEU or multi-reference word error rate for statistical machine translation."
"The use of statistical techniques in natural language process-ing often starts out with the simplifying (often im-plicit) assumption that the final scoring is based on simply counting the number of wrong decisions, for instance, the number of sentences incorrectly trans-lated in machine translation."
"Hence, there is a mis-match between the basic assumptions of the used statistical approach and the final evaluation criterion used to measure success in a task."
"Ideally, we would like to train our model param-eters such that the end-to-end performance in some application is optimal."
"In this paper, we investigate methods to efficiently optimize model parameters with respect to machine translation quality as mea-sured by automatic evaluation criteria such as word error rate and BLEU."
"Let us assume  that we  are given a source  (‘French’) sentence , which is to be translated  into a target (‘English’) sentence  Among all possible target sentences, we will choose the sentence with the highest probability: 1  &quot;!%#$ &amp;"
"Pr )( +* ([Footnote_1]) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the tar-get language."
"1 The notational ,&apos;- . convention will be as follows. We use the symbol Pr to denote general probability distributions with 0/ &apos;-, . (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol ."
The decision in Eq. 1 minimizes the number of decision errors.
"Hence, under a so-called zero-one loss function this decision rule is optimal[REF_CITE]."
"Note that using a differ-ent loss function—for example, one induced by the BLEU metric—a different decision rule would be optimal."
"As the true probability distribution Pr &apos;( )(  is un- known, we have )( to  develop a model that ap-proximates Pr )(  .byWeusingdirectlya log-linearmodel themodelposterior."
"Inprobability Pr this  &apos; framework    , we have a. setForofeach featurefeaturefunctionsfunction, there exists a model parameter  ."
The direct translation probability is given by:
Pr )(  (      (2) exp   &quot;!    (3)  exp
"In this framework, the modeling problem amounts to developing suitable feature functions that capture the relevant properties of the translation task."
The training problem amounts to obtaining suitable pa-rameter values .
"A standard criterion for log-linear models is the MMI (maximum mutual infor-mation) criterion, which can be derived from the maximum entropy principle: +&quot;#! $ %# &amp;&apos; $  &apos; ( &apos; ,+ (4) *)("
"The optimization problem under this criterion has very nice properties: there is one unique global op-timum, and there are algorithms (e.g. gradient de-scent) that are guaranteed to converge to the global optimum."
"Yet, the ultimate goal is to obtain good translation quality on unseen test data."
"Experience shows that good results can be obtained using this approach, yet there is no reason to assume that an optimization of the model parameters using Eq. 4 yields parameters that are optimal with respect to translation quality."
"The goal of this paper is to investigate alterna-tive training criteria and corresponding training al-gorithms, which are directly related to translation quality measured with automatic evaluation criteria."
"In Section 3, we review various automatic evalua-tion criteria used in statistical machine translation."
"In Section 4, we present two different training crite-ria which try to directly optimize an error count."
"In Section 5, we sketch a new training algorithm which efficiently optimizes an unsmoothed error count."
"In Section 6, we describe the used feature functions and our approach to compute the candidate translations that are the basis for our training procedure."
"In Sec-tion 7, we evaluate the different training criteria in the context of several MT experiments."
"In recent years, various methods have been pro-posed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations."
"Examples of such methods are word error rate, position-independent word error rate[REF_CITE], generation string accu-racy[REF_CITE], multi-reference word error rate[REF_CITE], BLEU score[REF_CITE], NIST score[REF_CITE]."
All these criteria try to approximate human assess-ment and often achieve an astonishing degree of cor-relation to human subjective evaluation of fluency and adequacy[REF_CITE].
"In this paper, we use the following methods: - multi-reference word error rate (mWER):"
"When this method is used, the hypothesis trans-lation is compared to various reference transla-tions by computing the edit distance (minimum number of substitutions, insertions, deletions) between the hypothesis and the closest of the given reference translations. - multi-reference position independent error rate (mPER): This criterion ignores the word order by treating a sentence as a bag-of-words and computing the minimum number of substitu-tions, insertions, deletions needed to transform the hypothesis into the closest of the given ref-erence translations. - BLEU score: This criterion computes the ge-ometric mean of the precision of . -grams of various lengths between a hypothesis and a set of reference translations multiplied by a factor"
BP 0/ that penalizes short sentences:
BLEU BP 0/ / 1 $ 32547&amp; 6 *) 9 8 :( 8
Here 8 denotes the precision of 9 . -grams &lt;; in the hypothesis translation.
We use . - NIST score: This criterion computes a weighted precision of . -grams between a hy-pothesis and a set of reference translations mul-tiplied by a factor BP’ 0/ that penalizes short sentences:
NIST BP’ 0/ / &amp; 6 8 8
Here 8 denotes the weighted precision 9 ; of . -grams in the translation.
We use .
"Both, NIST and BLEU are accuracy measures, and thus larger values reflect better translation qual-ity."
"Note that NIST and BLEU scores are not addi-tive for different sentences, i.e. the score for a doc-ument cannot be obtained by simply summing over scores for individual sentences."
"In the following, we assume that we can measure the number of errors in sentence by comparing  it.with a reference sentence using a function E However, the following exposition can be easily adapted to accuracy metrics and to metrics that make use of multiple references."
We assume that $ the number of errors for a set is obtained by summing $ the $  of &apos; sentences &apos; &apos; er- rors $ for the individual sentences: .
"Our goal is to obtain $ a minimal error count on a representative &apos; $ &apos; corpus &apos; with given reference trans-different * candidate transla-lations &apos; and a set of &amp; tions for each input sentence . & apos; ) &apos; # &apos;&amp; $  &quot;! ,+ (5) &apos; &apos; &apos; ) &apos; # &amp;&apos; $ &amp;  &quot;! ,+ with &apos;   )( &apos; + +% &quot;  !#$ # &amp; (6)"
The above stated optimization criterion is not easy to handle: - It includes an argmax operation (Eq. 6).
"There-fore, it is not possible to compute a gradient and we cannot use gradient descent methods to perform optimization. - The objective function has many different local optima."
The optimization algorithm must han-dle this.
"In addition, even if we manage to solve the optimiza-tion problem, we might face the problem of overfit-ting the training data."
"In Section 5, we describe an efficient optimization algorithm."
"To be able to compute a gradient and to make the objective function smoother, we can use the follow-ing error criterion which is essentially a smoothed error count, with a parameter to adjust the smooth-ness: & apos; &apos; &apos; &apos; (   (   +&quot;! &apos; &apos;  &amp; &apos; (7)"
"In the extreme case, for &quot;! # , Eq. 7 converges to the unsmoothed criterion of Eq. 5 (except in the case of ties)."
"Note, that the resulting objective func-tion might still have local optima, which makes the optimization hard compared to using the objective function of Eq. 4 which does not have different lo-cal optima."
The use of this type of smoothed error count is a common approach in the speech commu-nity[REF_CITE].
Figure 1 shows the actual shape of the smoothed and the unsmoothed error count for two parame-ters in our translation system.
We see that the un-smoothed error count has many different local op-tima and is very unstable.
The smoothed error count is much more stable and has fewer local optima.
"But as we show in Section 7, the performance on our task obtained with the smoothed error count does not differ significantly from that obtained with the unsmoothed error count."
A standard algorithm for the optimization of the unsmoothed error count (Eq. 5) is Powells algo-rithm combined with a grid-based line optimiza-tion method[REF_CITE].
We start at a ran-dom point in the -dimensional parameter space and try to find a better scoring point in the param-eter space by making a one-dimensional line min-imization along the directions given by optimizing one parameter while keeping all other parameters fixed.
"To avoid finding a poor local optimum, we start from different initial parameter values."
A major problem with the standard approach is the fact that grid-based line optimization is hard to adjust such that both good performance and efficient search are guaranteed.
If a fine-grained grid is used then the algorithm is slow.
If a large grid is used then the optimal solution might be missed.
"In the following, we describe a new algorithm for efficient line optimization of the unsmoothed error count (Eq. 5) using a log-linear model (Eq. 3) which is guaranteed to find the optimal solution."
The new algorithm is much faster and more stable than the grid-based line optimization method.
Computing the most probable sentence  out * of a set of candidate translation / &amp; (see
Eq. 6) along a line  with parameter results in an optimization problem of the following functional form  % &quot;! &amp;   :  /  +* (8)
"Here, 0/ and 0/ are constants with respect to ."
"Hence, every candidate translation in corresponds  +* to a line."
The  function !%  &amp;    / (9) is piecewise li[REF_CITE].
This allows us to compute an efficient exhaustive representation of that function.
"In the following, we sketch the new algorithm quence of linear intervals constitutingto optimize Eq. 5: We compute the ordered  se-for ev-ery sentence together with the incremental change in error count from the previous to the next inter-val."
"Hence, we obtain for every sentence a se-quence    which denote the interval boundaries and a corresponding 6 sequence for the change in error count involved at the corre-sponding interval boundary  ."
"Here, 8 denotes the change in the error count 6  at 8  to the error count at position position 8 for all different sentences of our corpus, the complete set of interval boundaries and error count changes on the whole corpus are obtained."
The op-timal can now be computed easily by traversing the sequence of interval boundaries while updating an error count.
"It is straightforward to refine this algorithm to also handle the BLEU and NIST scores instead of sentence-level error counts by accumulating the rel-evant statistics for computing these scores (n-gram precision, translation length and reference length) ."
The basic feature functions of our model are iden-tical to the alignment template approach[REF_CITE].
"In this translation model, a sentence is translated by segmenting the input sentence into phrases, translating these phrases and reordering the translations in the target language."
"In addition to the feature functions described[REF_CITE], our system includes a phrase penalty (the number of alignment templates used) and special alignment features."
"Altogether, the log-linear model includes different features."
Note that many of the used feature functions are derived from probabilistic models: the feature func-tion is defined as the negative logarithm of the cor-responding probabilistic model.
"Therefore, the fea-ture functions are much more ’informative’ than for instance the binary feature functions used in stan-dard maximum entropy models in natural language processing."
"For search, we use a dynamic programming beam-search algorithm to explore a subset of all pos-sible translations[REF_CITE]and extract . -best candidate translations using A* search[REF_CITE]."
"Using an . -best approximation, we might face the problem that the parameters trained are good for the list of . translations used, but yield worse transla-tion results if these parameters are used in the dy-namic programming search."
"Hence, it is possible that our new search produces translations with more errors on the training corpus."
This can happen be-cause with the modified model scaling factors the . -best list can change significantly and can include sentences not in the existing . -best list.
"To avoid this problem, we adopt the following solution: First, we perform search (using a manually defined set of parameter values) and compute an . -best list, and use this . -best list to train the model parameters."
"Second, we use the new model parameters in a new search and compute a new . -best list, which is com-bined with the existing . -best list."
"Third, using this extended . -best list new model parameters are com-puted."
This is iterated until the resulting . -best list does not change.
"In this algorithm convergence is guaranteed as, in the limit, the . -best list will con-tain all possible translations."
"In our experiments, we compute in every iteration about 200 alternative translations."
"In practice, the algorithm converges af-ter about five to seven iterations."
"As a result, error rate cannot increase on the training corpus."
A major problem in applying the MMI criterion is the fact that the reference translations need to be part of the provided . -best list.
"Quite often, none of the given reference translations is part of the . -best list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence."
"To solve this problem, we define for the MMI train-ing new pseudo-references by selecting from the . -best list all the sentences which have a minimal num-ber of word errors with respect to any of the true ref-erences."
"Note that due to this selection approach, the results of the MMI criterion might be biased toward the mWER criterion."
It is a major advantage of the minimum error rate training that it is not necessary to choose pseudo-references.
We present results on the 2002 TIDES Chinese– English small data track task.
The goal is the trans-lation of news text from Chinese to English.
"Ta-ble 1 provides some statistics on the training, de-velopment and test corpus used."
"The system we use does not include rule-based components to translate numbers, dates or names."
The basic feature func-tions were trained using the training corpus.
The de-velopment corpus was used to optimize the parame-ters of the log-linear model.
Translation results are reported on the test corpus.
Table 2 shows the results obtained on the develop-ment corpus and Table 3 shows the results obtained on the test corpus.
Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant.
"For all error rates, we show the maximal occurring 95% confi-dence interval in any of the experiments for that col-umn."
The confidence intervals are computed using bootstrap resampling[REF_CITE].
The last column provides the number of words in the pro-duced translations which can be compared with the average number of reference words occurring in the development and test corpora given in Table 1.
"We observe that if we choose a certain error crite-rion in training, we obtain in most cases the best re-sults using the same criterion as the evaluation met-ric on the test data."
"The differences can be quite large: If we optimize with respect to word error rate, the results are mWER=68.3%, which is better than if we optimize with respect to BLEU or NIST and the difference is statistically significant."
"Between BLEU and NIST, the differences are more moderate, but by optimizing on NIST, we still obtain a large improvement when measured with NIST compared to optimizing on BLEU."
The MMI criterion produces significantly worse results on all error rates besides mWER.
"Note that, due to the re-definition of the notion of reference translation by using minimum edit distance, the re-sults of the MMI criterion are biased toward mWER."
"It can be expected that by using a suitably defined . -gram precision to define the pseudo-references for MMI instead of using edit distance, it is possible to obtain better BLEU or NIST scores."
An important part of the differences in the trans-lation scores is due to the different translation length (last column in Table 3).
The mWER and MMI cri-teria prefer shorter translations which are heavily pe-nalized by the BLEU and NIST brevity penalty.
We observe that the smoothed error count gives almost identical results to the unsmoothed error count.
This might be due to the fact that the number of parameters trained is small and no serious overfit-ting occurs using the unsmoothed error count.
The use of log-linear models for statistical machine translation was suggested[REF_CITE]and[REF_CITE].
The use of minimum classification error training and using a smoothed error count is common in the pattern recognition and speech recognition community[REF_CITE].
"A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq. 3."
The loss function is either identical or closely related to the final evalua-tion criterion.
"In contrast to the approach presented in this paper, the training criterion and the statisti-cal models used remain unchanged in the minimum Bayes risk approach."
In the field of natural language processing this approach has been applied for exam-ple in parsing[REF_CITE]and word alignment[REF_CITE].
We presented alternative training criteria for log-linear statistical machine translation models which are directly related to translation quality: an un-smoothed error count and a smoothed error count on a development corpus.
"For the unsmoothed er-ror count, we presented a new line optimization al-gorithm which can efficiently find the optimal solu-tion along a line."
We showed that this approach ob-tains significantly better results than using the MMI training criterion (with our method to define pseudo-references) and that optimizing error rate as part of the training criterion helps to obtain better error rate on unseen test data.
"As a result, we expect that ac-tual ’true’ translation quality is improved, as previ-ous work has shown that for some evaluation cri-teria there is a correlation with human subjective evaluation of fluency and adequacy[REF_CITE]."
"However, the different evaluation criteria yield quite different results on our Chinese–English translation task and therefore we expect that not all of them correlate equally well to human translation quality."
The following important questions should be an-swered in the future: - How many parameters can be reliably esti-mated using unsmoothed minimum error rate criteria using a given development corpus size?
We expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems.
Is it possible to optimize more parameters using the smoothed error rate criterion? - Which error rate should be optimized during training?
This relates to the important question of which automatic evaluation measure is opti-mally correlated to human assessment of trans-lation quality.
"Note, that this approach can be applied to any evaluation criterion."
"Hence, if an improved auto-matic evaluation criterion is developed that has an even better correlation with human judgments than BLEU and NIST, we can plug this alternative cri-terion directly into the training procedure and opti-mize the model parameters for it."
This means that improved translation evaluation measures lead di-rectly to improved machine translation quality.
"Of course, the approach presented here places a high demand on the fidelity of the measure being opti-mized."
"It might happen that by directly optimiz-ing an error measure in the way described above, weaknesses in the measure might be exploited that could yield better scores without improved transla-tion quality."
"Hence, this approach poses new chal-lenges for developers of automatic evaluation crite-ria."
"Many tasks in natural language processing, for in-stance summarization, have evaluation criteria that go beyond simply counting the number of wrong system decisions and the framework presented here might yield improved systems for these tasks as well."
We apply a decision tree based approach to pronoun resolution in spoken dialogue.
Our system deals with pronouns with NP-and non-NP-antecedents.
We present a set of features designed for pronoun resolu-tion in spoken dialogue and determine the most promising features.
We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system.
Corpus-based methods and machine learning tech-niques have been applied to anaphora resolution in written text with considerable success ([REF_CITE]; Ng &amp;[REF_CITE]among others).
It has been demonstrated that systems based on these ap-proaches achieve a performance that is comparable to hand-crafted systems.
Since they can easily be applied to new domains it seems also feasible to port a given corpus-based anaphora resolution sys-tem from written text to spoken dialogue.
This pa-per describes the extensions and adaptations needed for applying our anaphora resolution system[REF_CITE]to pronoun resolu-tion in spoken dialogue.
There are important differences between written text and spoken dialogue which have to be accounted for.
The most obvious difference is that in spo-ken dialogue there is an abundance of (personal and demonstrative) pronouns with non-NP-antecedents or no antecedents at all.
Corpus studies have shown that a significant amount of pronouns in spoken di-alogue have non-NP-antecedents: Byron &amp;[REF_CITE]report that about 50% of the pronouns in the[REF_CITE]corpus have non-NP-antecedents.
Eck-ert &amp;[REF_CITE]note that only about 45% of the pronouns in a set of Switchboard dialogues have NP-antecedents.
The remainder consists of 22% which have non-NP-antecedents and 33% without antecedents.
These studies suggest that the perfor-mance of a pronoun resolution algorithm can be im-proved considerably by enabling it to resolve also pronouns with non-NP-antecedents.
"Because of the difficulties a pronoun resolution algorithm encounters in spoken dialogue, previous approaches were applied only to tiny domains, they needed deep semantic analysis and discourse pro-cessing and relied on hand-crafted knowledge bases."
"In contrast, we build on our existing anaphora res-olution system and incrementally add new features specifically devised for spoken dialogue."
That way we are able to determine relatively powerful yet computationally cheap features.
To our knowledge the work presented here describes the first imple-mented system for corpus-based anaphora resolution dealing also with non-NP-antecedents.
Spoken dialogue contains more pronouns with non- NP-antecedents than written text does.
"However, pronouns with NP-antecedents (like 3rd pers. mas-culine/feminine pronouns, cf. he in the example be-low) still constitute the largest fraction of all coref-erential pronouns in the Switchboard corpus."
"In spoken dialogue there are considerable num-bers of pronouns that pick up different kinds of abstract objects from the previous discourse, e.g. events, states, concepts, propositions or facts[REF_CITE]."
These anaphors then have VP-antecedents (“it ” in (B6) below) or sentential antecedents (“that ” in (B5)).
A1: ... [he] ’s nine months old. ...
A2: [He] likes to dig around a little bit.
"A3: [His] mother comes in and says, why did you let [him] [play in the dirt] ,"
A:4 I guess [[he] ’s enjoying himself] .
B5: [That] ’s right.
"B6: [It] ’s healthy, ..."
A major problem for pronoun resolution in spo-ken dialogue is the large number of personal and demonstrative pronouns which are either not refer-ential at all (e.g. expletive pronouns) or for which a particular antecedent cannot easily be determined by humans (called vague anaphors by Eckert &amp;[REF_CITE]).
"In the following example, the “that ” in utter-ance (A3) refers back to utterance (A1)."
"As for the first two pronouns in (B4), following Eckert &amp;[REF_CITE]and[REF_CITE]we assume that re-ferring expressions in disfluencies, abandoned utter-ances etc. are excluded from the resolution."
The third pronoun in (B4) is an expletive.
The pronoun in (A5) is different in that it is indeed referential: it refers back to“that ” from (A3).
"A1: ... [There is a lot of theft, a lot of assault dealing with, uh, people trying to get money for drugs. ]"
"And, uh, I think [that ]’s a national problem, though."
"It, it, it’s pretty bad here, too."
A5: [It ]’s not unique ...
"Pronoun resolution in spoken dialogue also has to deal with the whole range of difficulties that come with processing spoken language: disfluen-cies, hesitations, abandoned utterances, interrup-tions, backchannels, etc."
These phenomena have to be taken into account when formulating constraints on e.g. the search space in which an anaphor looks for its antecedent.
"E.g., utterance (B2) in the previ-ous example does not contain any referring expres-sions."
So the demonstrative pronoun in (A3) has to have access not only to (B2) but also to (A1).
Our work is based on twenty randomly chosen Switchboard dialogues.
"Taken together, the dia-logues contain 30810 tokens (words and punctua-tion) in 3275 sentences / 1771 turns."
"The annotation consists of 16601 markables, i.e. sequences of words and attributes associated with them."
"On the top level, different types of markables are distinguished: NP-markables identify referring expressions like noun phrases, pronouns and proper names."
"Some of the attributes for these markables are derived from the Penn Treebank version of the Switchboard dia-logues, e.g. grammatical function, NP form, gram-matical case and depth of embedding in the syn-tactical structure."
"VP-markables are verb phrases, S-markables sentences."
Disfluency-markables are noun phrases or pronouns which occur in unfin-ished or abandoned utterances.
"Among other (type-dependent) attributes, markables contain a member attribute with the ID of the coreference class they are part of (if any)."
"If an expression is used to re-fer to an entity that is not referred to by any other expression, it is considered a singleton."
Table 1 gives the distribution of the npform at-tribute for NP-markables.
The second and third row give the number of non-singletons and singletons re-spectively that add up to the total number given in the first row.
"Table 2 shows the distribution of the agreement attribute (i.e. person, gender, and number) for the pronominal expressions in our corpus."
"The left fig-ure in each cell gives the total number of expres-sions, the right figure gives the number of non-singletons."
"Note the relatively high number of sin-gletons among the personal and demonstrative pro-nouns (223 for it, 60 for they and 82 for that)."
"These pronouns are either expletive or vague, and cause the most trouble for a pronoun resolution algorithm, which will usually attempt to find an antecedent nonetheless."
"Singleton they pronouns, in particu-lar, are typical for spoken language (as opposed to written text)."
The same is true for anaphors with non-NP-antecedents.
"However, while they are far more frequent in spoken language than in written text, they still constitute only a fraction of all coref-erential expressions in our corpus."
This defines an upper limit for what the resolution of these kinds of anaphors can contribute at all.
These facts have to be kept in mind when comparing our results to results of coreference resolution in written text.
Training and test data instances were generated from our corpus as follows.
"All markables were sorted in document order, and markables for first and sec-ond person pronouns were removed."
The resulting list was then processed from top to bottom.
"If the list contained an NP-markable at the current posi-tion and if this markable was not an indefinite noun phrase, it was considered a potential anaphor."
"In that case, pairs of potentially coreferring expressions were generated by combining the potential anaphor with each compatible [Footnote_1] NP-markable preceding [Footnote_2] it in the list."
1 Markables are considered compatible if they do not mis-match in terms of agreement.
2 We disregard the phenomenon of cataphor here.
"The resulting pairs were labelled P if both markables had the same (non-empty) value in their member attribute, N otherwise."
"For anaphors with non-NP-antecedents, additional training and test data instances had to be generated."
This process was triggered by the markable at the current position being it or that.
"In that case, a small set of poten-tial non-NP-antecedents was generated by selecting S- and VP-markables from the last two valid sen-tences preceding the potential anaphor."
The choice of the last two sentences was motivated pragmat-ically by considerations to keep the search space (and the number of instances) small.
"A sentence was considered valid if it was neither unfinished nor a backchannel utterance (like e.g. ”Uh-huh”, ”Yeah”, etc.)."
"From the selected markables, inac-cessible non-NP-expressions were automatically re-moved."
We considered an expression inaccessible if it ended before the sentence in which it was con-tained.
This was intended to be a rough approxi-mation of the concept of the right frontier[REF_CITE].
The remaining expressions were then com-bined with the potential anaphor.
"Finally, the result-ing pairs were labelled P or N and added to the in-stances generated with NP-antecedents."
"We distinguish two classes of features: NP-level features specify e.g. the grammatical function, NP form, morpho-syntax, grammatical case and the depth of embedding in the syntactical structure."
"For these features, each instance contains one value for the antecedent and one for the anaphor."
"Coreference-level features, on the other hand, de-scribe the relation between antecedent and anaphor in terms of e.g. distance (in words, markables and sentences), compatibility in terms of agreement and identity of syntactic function."
"For these features, each instance contains only one value."
"In addition, we introduce a set of features which is partly tailored to the processing of spoken dia-logue."
The feature ante exp type (17) is a rather obvious yet useful feature to distinguish NP- from non-NP-antecedents.
"The features ana np , vp and s pref (18, 19, 20) describe a verb’s preference for arguments of a particular type."
"Inspired by the work of Eckert &amp;[REF_CITE]and[REF_CITE], these features capture preferences for NP- or non- NP-antecedents by taking a pronoun’s predicative context into account."
"The underlying assumption is that if a verb preceding a personal or demonstrative pronoun preferentially subcategorizes sentences or VPs, then the pronoun will be likely to have a non- NP-antecedent."
The features are based on a verb list compiled from 553 Switchboard dialogues. [Footnote_3]
3 It seemed preferable to compile our own list instead of us-ing existing ones like Briscoe &amp;[REF_CITE].
"For ev-ery verb occurring in the corpus, this list contains up to three entries giving the absolute count of cases where the verb has a direct argument of type NP, VP or S. When the verb list was produced, pronominal arguments were ignored."
"The features mdist 3mf3p and mdist 3n (21, 22) are refinements of the mdist feature."
"They measure the distance in markables be-tween antecedent and anaphor, but in doing so they take the agreement value of the anaphor into ac-count."
"For anaphors with an agreement value of 3mf or 3p, mdist 3mf3p is measured as D = 1 + the num- ber of NP-markables between anaphor and potential antecedent."
"Anaphors with an agreement value of 3n, (i.e. it or that), on the other hand, potentially have non-NP-antecedents, so mdist 3n is measured as D + the number of anaphorically accessible [Footnote_4] S-and VP-markables between anaphor and potential antecedent."
"4 As mentioned earlier, the definition of accessibility of non- NP-antecedents is inspired by the concept of the right frontier[REF_CITE]."
The feature ante tfifd (23) is supposed to capture the relative importance of an expression for a dia-logue.
"The underlying assumption is that the higher the importance of a non-NP expression, the higher the probability of its being referred back to."
"For our purposes, we calculated TF for every word by counting its frequency in each of our twenty Switch-board dialogues separately."
The calculation of IDF was based on a set of 553 Switchboard dialogues.
"For every word, we calculated IDF as log(553/N ), with N =number of documents containing the word."
"For every non-NP-markable, an average TF*IDF value was calculated as the TF*IDF sum of all words comprising the markable, divided by the number of words in the markable."
The feature ante ic (24) as an alternative to ante tfidf is based on the same as-sumptions as the former.
"The information content of a non-NP-markable is calculated as follows, based on a set of 553 Switchboard dialogues: For each word in the markable, the IC value was calculated as the negative log of the total frequency of the word divided by the total number of words in all 553 dia-logues."
"The average IC value was then calculated as the IC sum of all words in the markable, divided by the number of words in the markable."
"Finally, the feature wdist ic (25) measures the word-based dis-tance between two expressions."
It does so in terms of the sum of the individual words’ IC.
The calcula-tion of the IC was done as described for the ante ic feature.
"All experiments were performed using the decision tree learner RPART (Therneau &amp;[REF_CITE]), which is a CART[REF_CITE]reimple-mentation for the S-Plus and R statistical comput-ing environments (we use R, Ihaka &amp;[REF_CITE],[URL_CITE]"
"We used the standard pruning and control settings for RPART (cp=0.0001, minsplit=20, minbucket=7)."
All results reported were obtained by performing 20-fold cross-validation.
"In the prediction phase, the trained classifier is ex-posed to unlabeled instances of test data."
The classi-fier’s task is to label each instance.
"When an instance is labeled as coreferring, the IDs of the anaphor and antecedent are kept in a response list for the evalua-tion according[REF_CITE]."
For determining the relevant feature set we fol-lowed an iterative procedure similar to the wrap-per approach for feature selection (Kohavi &amp;[REF_CITE]).
We start with a model based on a set of prede-fined baseline features.
Then we train models com-bining the baseline with all additional features sep-arately.
"We choose the best performing feature (f-measure according[REF_CITE]) , adding it to the model."
We then train classifiers combining the enhanced model with each of the remaining fea-tures separately.
We again choose the best perform-ing classifier and add the corresponding new feature to the model.
This process is repeated as long as significant improvement can be observed.
"In our experiments we split the data in three sets ac-cording to the agreement of the anaphor: third per-son masculine and feminine pronouns (3mf), third person neuter pronouns (3n), and third person plural pronouns (3p)."
"Since only 3n-pronouns have non- NP-antecedents, we were mainly interested in im-provements in this data set."
We used the same baseline model for each data set.
"The baseline model corresponds to a pronoun resolution algorithm commonly applied to written text, i.e., it uses only the features in the first two parts of Table 3."
For the baseline model we gener-ated training and test data which included only NP-antecedents.
Then we performed experiments using the fea-tures introduced for spoken dialogue.
The training and test data for the models using additional features included NP- and non-NP-antecedents.
For each data set we followed the iterative procedure outlined in Section 5.1.
In the following tables we present the results of our experiments.
"The first column gives the number of coreference links correctly found by the classifier, the second column gives the number of all corefer-ence links found."
The third column gives the total number of coreference links (1250) in the corpus.
"During evaluation, the list of all correct links is used as the key list against which the response list pro-duced by the classifier (cf. above) is compared."
"The remaining three columns show precision, recall and f-measure, respectively."
Table 4 gives the results for 3mf pronouns.
The baseline model performs very well on this data set (the low recall figure is due to the fact that the 3mf data set contains only a small subset of the coref-erence links expected by the evaluation).
The re-sults are comparable to any pronoun resolution al-gorithm dealing with written text.
This shows that our pronoun resolution system could be ported to the spoken dialogue domain without sacrificing perfor-mance.
Table 5 shows the results for 3n pronouns.
The baseline model does not perform very well.
"As men-tioned above, for evaluating the performance of the baseline model we removed all potential non-NP-antecedents from the data."
This corresponds to a naive application of a model developed for written text to spoken dialogue.
"First, we applied the same model to the data set containing all kinds of antecedents."
The perfor-mance drops somewhat as the classifier is exposed to non-NP-antecedents without being able to differ-entiate between NP- and non-NP-antecedents.
"By adding the feature ante exp type the classifier is en-abled to address NP- and non-NP-antecedents dif-ferently, which results in a considerable gain in per-formance."
Substituting the wdist feature with the wdist ic feature also improves the performance con-siderably.
The ante tfidf feature only contributes marginally to the overall performance. – These re-sults show that it pays off to consider features par-ticularly designed for spoken dialogue.
"Table 6 presents the results for 3p pronouns, which do not have non-NP-antecedents."
Many of these pronouns do not have an antecedent at all.
"Oth-ers are vague in that human annotators felt them to be referential, but could not determine an an-tecedent."
"Since we did not address that issue in depth, the classifier tries to find antecedents for these pronouns indiscriminately, which results in rather low precision figures, as compared to e.g. those for 3mf."
Only the feature wdist ic leads to an improve-ment over the baseline.
Table 7 shows the results for the combined clas-sifiers.
The improvement in f-measure is due to the increase in recall while the precision shows only a slight decrease.
"Though some of the features of the baseline model (features 1-16) still occur in the decision tree learned, the feature ante exp type divides ma-jor parts of the tree quite nicely (see Figure 1)."
Be-low that node the feature ana npform is used to dis-tinguish between negative (personal pronouns) and potential positive cases (demonstrative pronouns).
This confirms the hypothesis by Eckert &amp;[REF_CITE]and[REF_CITE]to give high priority to these features.
The decision tree fragment in Figure 1 correctly assigns the P label to 23-7=16 sentential antecedents.
"However, the most important problem is the large amount of pronouns without antecedents."
The model does find (wrong) antecedents for a lot of pro-nouns which should not have one.
"Only a small frac-tion of these pronouns are true expletives (i.e., they precede a “weather” verb or are in constructions like “It seems that . . . ”."
"The majority of these cases are referential, but have no antecedent in the data (i.e., they are vague pronouns)."
"The overall numbers for precision, recall and f-measure are fairly low."
One reason is that we did not attempt to resolve anaphoric definite NPs and proper names though these coreference links are contained in the evaluation key list.
"If we removed them from there, the recall of our experiments would approach the 51%[REF_CITE]mentioned for her system us-ing only domain-independent semantic restrictions."
Our approach for determining the feature set for pro-noun resolution resembles the so-called wrapper ap-proach for feature selection (Kohavi &amp;[REF_CITE]).
"This is in contrast to the majority of other work on feature selection for anaphora resolution, which was hardly ever done systematically."
"E.g.[REF_CITE]only compared baseline systems consisting of one feature each, only three of which yielded an f-measure greater than zero."
Then they combined these features and achieved results which were close to the best overall results they report.
"While this tells us which features contribute a lot, it does not give any information about potential (positive or nega-tive) influence of the rest."
"Ng &amp;[REF_CITE]select the set of features by hand, giving a preference to high precision features."
They admit that this method is quite subjective.
Corpus-based work about pronoun resolution in spoken dialogue is almost non-existent.
"However, there are a few papers dealing with neuter pronouns with NP-antecedents."
"E.g., Dagan &amp;[REF_CITE]pre-sented a corpus-based approach to the resolution of the pronoun it, but they use a written text corpus and do not mention non-NP-antecedents at all."
"For their exper-iments, however, they restricted anaphoric relations to those with NP-antecedents."
"Byron extends a pronoun resolution al-gorithm[REF_CITE]with semantic filtering, thus enabling it to resolve anaphors with non-NP-antecedents as well."
"Semantic filtering relies on knowledge about semantic restrictions associated with verbs, like semantic compatibility between sub-ject and predicative noun or predicative adjective."
An evaluation on ten[REF_CITE]dialogues with 80 3rd person pronouns and 100 demonstrative pro-nouns shows that semantic filtering and the im-plementation of different search strategies for per-sonal and demonstrative pronouns yields a suc-cess rate of 72%.
"As Byron admits, the ma-jor limitation of her algorithm is its dependence on domain-dependent resources which cover the domain entirely."
"When evaluating her algorithm with only domain-independent semantics,[REF_CITE]% success rate."
What is problematic with her approach is that she assumes the input to her algorithm to be only referential pronouns.
This simplifies the task considerably.
We presented a machine learning approach to pro-noun resolution in spoken dialogue.
We built upon a system we used for anaphora resolution in writ-ten text and extended it with a set of features de-signed for spoken dialogue.
We refined distance features and used metrics from information retrieval for determining non-NP-antecedents.
Inspired by the more linguistically oriented work by Eckert &amp;[REF_CITE]and[REF_CITE]we also evaluated the contribution of features which used the predica-tive context of the pronoun to be resolved.
"However, these features did not show up in the final models since they did not lead to an improvement."
"Instead, rather simple distance metrics were preferred."
"While we were (almost) satisfied with the performance of these features, the major problem for a spoken dia-logue pronoun resolution algorithm is the abundance of pronouns without antecedents."
Previous research could avoid dealing with this phenomenon by either applying the algorithm by hand (Eckert &amp;[REF_CITE]) or excluding these cases[REF_CITE]from the evaluation.
Because we included these cases in our evaluation we consider our approach at least comparable to Byron’s system when she uses only domain-independent semantics.
We believe that our system is more robust than hers and that it can more easily be ported to new domains.
The work presented here has been partially funded by the German Ministry of Research and Technology as part of the E MBASSI project (01[REF_CITE]D/2) and by the Klaus Tschira Foundation.
"We would like to thank Susanne Wilhelm and Lutz Wind for doing the annota-tions, Kerstin Schürmann, Torben Pastuch and Klaus Rothenhäusler for helping with the data prepara-tion."
In this paper we propose a competition learning approach to coreference resolu-tion.
"Traditionally, supervised machine learning approaches adopt the single-candidate model."
Nevertheless the prefer-ence relationship between the antecedent candidates cannot be determined accu-rately in this model.
"By contrast, our ap-proach adopts a twin-candidate learning model."
"Such a model can present the competition criterion for antecedent can-didates reliably, and ensure that the most preferred candidate is selected."
"Further-more, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution."
The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model.
Coreference resolution is the process of linking together multiple expressions of a given entity.
The key to solve this problem is to determine the ante-cedent for each referring expression in a document.
"In coreference resolution, it is common that two or more candidates compete to be the antecedent of an anaphor[REF_CITE]."
Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates.
"So far, various algorithms have been proposed to deter-mine the preference relationship between two can-didates."
"Mitkov’s knowledge-poor pronoun resolution method[REF_CITE], for example, uses the scores from a set of antecedent indicators + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates."
"And centering algorithms[REF_CITE], sort the antecedent candidates based on the ranking of the forward-looking or backward-looking centers."
"In recent years, supervised machine learning approaches have been widely used in coreference resoluti[REF_CITE], and have achieved significant success."
"Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a con-fidence value."
The confidence values are generally used as the competition criterion for the antecedent candidates.
"For example, the “Best-First” selection algorithms[REF_CITE]link the anaphor to the candidate with the maximal confidence value (above 0.5)."
"One problem of the single-candidate model, however, is that it only takes into account the rela-tionships between an anaphor and one individual candidate at a time, and overlooks the preference relationship between candidates."
"Consequently, the confidence values cannot accurately represent the true competition criterion for the candidates."
"In this paper, we present a competition learning approach to coreference resolution."
"Motivated by the research work[REF_CITE], our approach adopts a twin-candidate model to directly learn the competition criterion for the antecedent candidates."
"In such a model, a classifier is trained based on the instances formed by an anaphor and a pair of its antecedent candidates."
The classifier is then used to determine the preference between any two candidates of an anaphor encountered in a new document.
The candidate that wins the most com-parisons is selected as the antecedent.
"In order to reduce the computational cost and data noises, our approach also employs a candidate filter to elimi-nate the invalid or irrelevant candidates."
The layout of this paper is as follows.
Section 2 briefly describes the single-candidate model and analyzes its limitation.
Section 3 proposes in de-tails the twin-candidate model and Section 4 pre-sents our coreference resolution approach based on this model.
Section 5 reports and discusses the ex-perimental results.
Section 6 describes related re-search work.
"Finally, conclusion is given in Section 7."
The main idea of the single-candidate model for coreference resolution is to recast the resolution as a binary classification problem.
"During training, a set of training instances is generated for each anaphor in an annotated text."
An instance is formed by the anaphor and one of its antecedent candidates.
It is labeled as positive or negative based on whether or not the candidate is tagged in the same coreferential chain of the anaphor.
"After training, a classifier is ready to resolve the NPs 1 encountered in a new document."
"For each NP under consideration, every one of its antecedent candidates is paired with it to form a test instance."
The classifier returns a number between 0 and 1 that indicates the likelihood that the candidate is coreferential to the NP.
The returned confidence value is commonly used as the competition criterion to rank the candi-date.
"Normally, the candidates with confidences less than a selection threshold (e.g. 0.5) are dis-carded."
"Then some algorithms are applied to choose one of the remaining candidates, if any, as the antecedent."
"For example, “Closest-First”[REF_CITE]selects the candidate closest to the anaphor, while “Best-First”[REF_CITE]selects the candidate with the maximal confidence value."
"One limitation of this model, however, is that it only considers the relationships between a NP en-countered and one of its candidates at a time dur-ing its training and testing procedures."
"The confidence value reflects the probability that the candidate is coreferential to the NP in the overall distribution 2 , but not the conditional probability when the candidate is concurrent with other com-petitors."
"Consequently, the confidence values are unreliable to represent the true competition crite-rion for the candidates."
"To illustrate this problem, just suppose a data set where an instance could be described with four exclusive features: F1, F[Footnote_2], F3 and F4."
"2 Suppose we use C4.5 algorithm and the class value takes the smoothed ration, p +1 , where p is the number of positive t + 2 instances and t is the total number of instances contained in the corresponding leaf node."
The ranking of candidates obeys the following rule:
CS F1 &gt;&gt; CS F2 &gt;&gt; CS F3 &gt;&gt; CS F4
Here CS Fi ( [Footnote_1]≤ i ≤ 4 ) is the set of antecedent can-didates with the feature Fi on.
1 In this paper a NP corresponds to a Markable in MUC coreference resolution tasks.
"The mark of “&gt;&gt;” denotes the preference relationship, that is, the candidates in CS F1 is preferred to those in CS F2 , and to those in CS F3 and CS F4 ."
"Let CF 2 and CF 3 denote the class value of a leaf node “F[Footnote_2] = 1” and “F3 = 1”, respectively."
"2 Suppose we use C4.5 algorithm and the class value takes the smoothed ration, p +1 , where p is the number of positive t + 2 instances and t is the total number of instances contained in the corresponding leaf node."
"It is pos-sible that CF 2 &lt; CF 3 , if the anaphors whose candi-dates all belong to CS F3 or CS F4 take the majority in the training data set."
"In this case, a candidate in CS F3 would be assigned a larger confidence value than a candidate in CS F2 ."
This nevertheless contra-dicts the ranking rules.
"If during resolution, the candidates of an anaphor all come from CS F2 or CS F3 , the anaphor may be wrongly linked to a can-didate in CS F3 rather than in CS F2 ."
"Different from the single-candidate model, the twin-candidate model aims to learn the competition criterion for candidates."
"In this section, we will introduce the structure of the model in details."
"Consider an anaphor ana and its candidate set can-didate_set, {C 1 , C 2 , …, C k }, where C j is closer to ana than C i if j &gt; i. Suppose positive_set is the set of candidates that occur in the coreferential chain of ana, and negative_set is the set of candidates not in the chain, that is, negative_set = candidate_set - positive_set."
"The set of training instances based on ana, inst_set, is defined as follows: inst_ set= {inst (Ci,Cj,ana) |i &gt; j,C i ∈ positve_ set, C j ∈negative_ set} U {inst (Ci,Cj,ana) |i &gt; j,C i ∈negative_ set, C j ∈positve_ set}"
"From the above definition, an instance is formed by an anaphor, one positive candidate and one negative candidate."
"For each instance, inst (ci, cj, ana) , the candidate at the first position, C i , is closer to the anaphor than the candidate at the second position, C j ."
"A training instance inst (ci,cj,ana) is labeled as positive if C i ∈ positive-set and C j ∈ negative-set; or negative if C i ∈ negative-set and C j ∈ positive-set."
See the following example:
"Any design to link China&apos;s accession to the WTO with the missile tests 1 was doomed to failure. “If some countries 2 try to block China TO acces-sion, that will not be popular and will fail to win the support of other countries 3 ” she said."
"Although no governments 4 have suggested formal sanctions 5 on China over the missile tests 6 , the United States has called them 7 “provocative and reckless” and other countries said they could threaten Asian stability."
"In the above text segment, the antecedent can-didate set of the pronoun “them 7 ” consists of six candidates highlighted in Italics."
"Among the can-didates, Candidate 1 and 6 are in the coreferential chain of “them 7 ”, while Candidate 2, 3, 4, 5 are not."
"Thus, eight instances are formed for “them 7 ”: (2,1,7) (3,1,7) (4,1,7) (5,1,7) (6,5,7) (6,4,7) (6,3,7) (6,2,7)"
"Here the instances in the first line are negative, while those in the second line are all positive."
A feature vector is specified for each training or testing instance.
"Similar to those in the single-candidate model, the features may describe the lexical, syntactic, semantic and positional relation-ships of an anaphor and any one of its candidates."
"Besides, the feature set may also contain inter-candidate features characterizing the relationships between the pair of candidates, e.g. the distance between the candidates in the number distances or paragraphs."
"Based on the feature vectors generated for each anaphor encountered in the training data set, a classifier can be trained using a certain machine learning algorithm, such as C4.5, RIPPER, etc."
"Given the feature vector of a test instance inst (ci,cj,ana) (i &gt; j), the classifier returns the posi-tive class indicating that C i is preferred to C j as the antecedent of ana; or negative indicating that C j is preferred."
"Let CR( inst (ci,cj,ana) ) denote the classification re-sult for an instance inst (ci,cj,ana) ."
The antecedent of an anaphor is identified using the algorithm shown in Figure 1.
"Algorithm ANTE-SEL takes as input an ana-phor and its candidate set candidate_set, and re-turns one candidate as its antecedent."
"In the algorithm, each candidate is compared against any other candidate."
The classifier acts as a judge dur-ing each comparison.
The score of each candidate increases by one every time when it wins.
"In this way, the final score of a candidate records the total times it wins."
The candidate with the maximal score is singled out as the antecedent.
"If two or more candidates have the same maxi-mal score, the one closest to the anaphor would be selected."
"While the realization and the structure of the twin-candidate model are significantly different from the single-candidate model, the single-candidate model in fact can be regarded as a special case of the twin-candidate model."
"To illustrate this, just consider a virtual “blank” candidate C 0 such that we could convert an in-stance inst (ci,ana) in the single-candidate model to an instance inst (ci,c 0 ,ana) in the twin-candidate model."
"Let inst (ci,c 0 ,ana) have the same class label as inst (ci, ana) , that is, inst (ci, c 0 , ana) is positive if C i is the antecedent of ana; or negative if not."
"Apparently, the classifier trained on the in-stance set { inst (ci,ana) }, T1, is equivalent to that trained on { inst (ci,c 0 ,ana) }, T2."
"T1 and T2 would assign the same class label for the test instances inst (ci, ana) and inst (ci, c 0 , ana) , respectively."
"That is to say, determining whether C i is coreferential to ana by T1 in the single-candidate model equals to determining whether C i is better than C 0 w.r.t ana by T2 in the twin-candidate model."
Here we could take C 0 as a “standard candidate”.
"While the classification in the single-candidate model can find its interpretation in the twin-candidate model, it is not true vice versa."
"Conse-quently, we can safely draw the conclusion that the twin-candidate model is more powerful than the single-candidate model in characterizing the rela-tionships among an anaphor and its candidates."
Our competition learning approach adopts the twin-candidate model introduced in the Section 3.
The main process of the approach is as follows: 1.
"The raw input documents are preprocessed to obtain most, if not all, of the possible NPs. 2."
"During training, for each anaphoric NP, we create a set of candidates, and then generate the training instances as described in Section 3. 3."
"Based on the training instances, we make use of the C5.0 learning algorithm[REF_CITE]to train a classifier. 4."
"During resolution, for each NP encountered, we also construct a candidate set."
"If the set is empty, we left this NP unresolved; otherwise we apply the antecedent identification algo- rithm to choose the antecedent and then link the NP to it."
"To determine the boundary of the noun phrases, a pipeline of Nature Language Processing compo-nents are applied to an input raw text:"
Tokenization and sentence segmentation
Named entity recognition
Noun phrase chunking
"Among them, named entity recognition, part-of-speech tagging and text chunking apply the same Hidden Markov Model (HMM) based engine with error-driven learning capability ([REF_CITE]&amp; 2002)."
"The named entity recognition component recognizes various types of MUC-style named entities, i.e., organization, location, person, date, time, money and percentage."
"For our study, in this paper we only select those features that can be obtained with low annotation cost and high reliability."
All features are listed in Table 1 together with their respective possible val-ues.
"For a NP under consideration, all of its preceding NPs could be the antecedent candidates."
"Neverthe-less, since in the twin-candidate model the number of instances for a given anaphor is about the square of the number of its antecedent candidates, the computational cost would be prohibitively large if we include all the NPs in the candidate set."
"More-over, many of the preceding NPs are irrelevant or even invalid with regard to the anaphor."
"These data noises may hamper the training of a good-performanced classifier, and also damage the accu-racy of the antecedent selection: too many com-parisons are made between incorrect candidates."
"Therefore, in order to reduce the computational cost and data noises, an effective candidate filter-ing strategy must be applied in our approach."
"During training, we create the candidate set for each anaphor with the following filtering algorithm: 1."
"If the anaphor is a pronoun, (a) Add to the initial candidate set all the pre-ceding NPs in the current and the previous two sentences. (b) Remove from the candidate set those that disagree in number, gender, and person. (c) If the candidate set is empty, add the NPs in an earlier sentence and go to 1(b). 2."
"If the anaphor is a non-pronoun, (a) Add all the non-pronominal antecedents to the initial candidate set. (b) For each candidate added in 2(a), add the non-pronouns in the current, the previous and the next sentences into the candidate set."
"During resolution, we filter the candidates for each encountered pronoun in the same way as dur-ing training."
"That is, we only consider the NPs in the current and the preceding 2 sentences."
Such a context window is reasonable as the distance be-tween a pronominal anaphor and its antecedent is generally short.
"In the MUC-6 data set, for exam-ple, the immediate antecedents of 95% pronominal anaphors can be found within the above distance."
"Comparatively, candidate filtering for non-pronouns during resolution is complicated."
"A po-tential problem is that for each non-pronoun under consideration, the twin-candidate model always chooses a candidate as the antecedent, even though all of the candidates are “low-qualified”, that is, unlikely to be coreferential to the non-pronoun un-der consideration."
"In fact, the twin-candidate model in itself can identify the qualification of a candidate."
"We can compare every candidate with a virtual “standard candidate”, C 0 ."
"Only those better than C 0 are deemed qualified and allowed to enter the “round robin”, whereas the losers are eliminated."
"As we have discussed in Section 3.5, the classifier on the pairs of a candidate and C 0 is just a single-candidate classifier."
"Thus, we can safely adopt the single-candidate classifier as our candidate filter."
The candidate filtering algorithm during resolu-tion is as follows: 1.
"If the current NP is a pronoun, construct the candidate set in the same way as during training. 2."
"If the current NP is a non-pronoun, (a) Add all the preceding non-pronouns to the ini-tial candidate set. (b) Calculate the confidence value for each candi-date using the single-candidate classifier. (c) Remove the candidates with confidence value less than 0.5."
Our coreference resolution approach is evaluated on the standard[REF_CITE]and[REF_CITE]data set.
For[REF_CITE]“dry-run” documents an-notated with coreference information could be used as training data.
There are also 30 annotated train-ing documents from MUC-7.
"For testing, we util-ize the 30 standard test documents from MUC-6 and the 20 standard test documents from MUC-7."
In the experiment we compared our approach with the following research works: 1.
Strube’s S-list algorithm for pronoun resolu-ti[REF_CITE]. 2.
Ng and Cardie’s machine learning approach to coreference resoluti[REF_CITE]. 3.
Connolly et al.’s machine learning approach to anaphora resoluti[REF_CITE].
"Among them, S-List, a version of centering algorithm, uses well-defined heuristic rules to rank the antecedent candidates; Ng and Cardie’s ap-proach employs the standard single-candidate model and “Best-First” rule to select the antece- dent; Connolly et al.’s approach also adopts the twin-candidate model, but their approach lacks of candidate filtering strategy and uses greedy linear search to select the antecedent (See “Related work” for details)."
"We constructed three baseline systems based on the above three approaches, respectively."
"For com-parison, in the baseline system 2 and 3, we used the similar feature set as in our system (see table 1)."
"Table 2 and 3 show the performance of different approaches in the pronoun and non-pronoun reso-lution, respectively."
In these tables we focus on the abilities of different approaches in resolving an anaphor to its antecedent correctly.
"The recall measures the number of correctly resolved ana-phors over the total anaphors in the MUC test data set, and the precision measures the number of cor-rect anaphors over the total resolved anaphors."
The F-measure F=2*RP/(R+P) is the harmonic mean of precision and recall.
The experimental result demonstrates that our competition learning approach achieves a better performance than the baseline approaches in re-solving pronominal anaphors.
"As shown in Table 2, our approach outperforms Ng and Cardie’s single-candidate based approach by 3.7 and 5.4 in F-measure for MUC-6 and MUC-7, respectively."
"Besides, compared with Strube’s S-list algorithm, our approach also achieves gains in the F-measure by 3.2 (MUC-6), and 1.6 (MUC-7)."
"In particular, our approach obtains significant improvement (21.1 for MUC-6, and 13.1 for MUC-7) over Con-nolly et al.’s twin-candidate based approach."
"Compared with the gains in pronoun resolution, the improvement in non-pronoun resolution is slight."
"As shown in Table 3, our approach resolves non-pronominal anaphors with the recall of 51.3 (39.7) and the precision of 90.4 (87.6) for MUC-6 (MUC-7)."
"In contrast to Ng and Cardie’s approach, the performance of our approach improves only 0.3 (0.6) in recall and 0.5 (1.2) in precision."
"The rea-son may be that in non-pronoun resolution, the coreference of an anaphor and its candidate is usu-ally determined only by some strongly indicative features such as alias, apposition, string-matching, etc (this explains why we obtain a high precision but a low recall in non-pronoun resolution)."
"There-fore, most of the positive candidates are coreferen-tial to the anaphors even though they are not the “best”."
"As a result, we can only see comparatively slight difference between the performances of the two approaches."
"Although Connolly et al.’s approach also adopts the twin-candidate model, it achieves a poor per-formance for both pronoun resolution and non-pronoun resolution."
The main reason is the absence of candidate filtering strategy in their approach (this is why the recall equals to the precision in the tables).
"Without candidate filtering, the recall may rise as the correct antecedents would not be elimi-nated wrongly."
"Nevertheless, the precision drops largely due to the numerous invalid NPs in the candidate set."
"As a result, a significantly low F-measure is obtained in their approach."
Table 4 summarizes the overall performance of different approaches to coreference resolution.
"Dif-ferent from Table 2 and 3, here we focus on whether a coreferential chain could be correctly identified."
"For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program[REF_CITE]for the coreference resolution task."
"Here the recall means the correct resolved chains over the whole coreferential chains in the data set, and precision means the correct resolved chains over the whole resolved chains."
"In line with the previous experiments, we see reasonable improvement in the performance of the coreference resolution: compared with the baseline approach based on the single-candidate model, the F-measure of approach increases from 69.4 to 71.3 for MUC-6, and from 58.7 to 60.2 for MUC-7."
A similar twin-candidate model was adopted in the anaphoric resolution system[REF_CITE].
The differences between our approach and theirs are: (1)
"In Connolly et al.’s approach, all the preceding NPs of an anaphor are taken as the antecedent candidates, whereas in our approach we use candidate filters to eliminate invalid or irrele-vant candidates. (2) The antecedent identification in Connolly et al.’s approach is to apply the classifier to successive pairs of candidates, each time retaining the better candidate."
"However, due to the lack of strong assumption of transitivity, the selection procedure is in fact a greedy search."
"By contrast, our approach evaluates a candidate according to the times it wins over the other competitors."
"Comparatively this algorithm could lead to a better solution. (3) Our approach makes use of more indicative features, such as Appositive, Name Alias, String-matching, etc."
These features are effec-tive especially for non-pronoun resolution.
In this paper we have proposed a competition learning approach to coreference resolution.
We started with the introduction of the single-candidate model adopted by most supervised ma-chine learning approaches.
We argued that the con-fidence values returned by the single-candidate classifier are not reliable to be used as ranking cri-terion for antecedent candidates.
"Alternatively, we presented a twin-candidate model that learns the competition criterion for antecedent candidates directly."
We introduced how to adopt the twin-candidate model in our competition learning ap- proach to resolve the coreference problem.
"Particu-larly, we proposed a candidate filtering algorithm that can effectively reduce the computational cost and data noises."
The experimental results have proved the effec-tiveness of our approach.
"Compared with the base-line approach using the single-candidate model, the F-measure increases by 1.9 and 1.5 for MUC-6 and MUC-7 data set, respectively."
The gains in the pronoun resolution contribute most to the overall improvement of coreference resolution.
"Currently, we employ the single-candidate clas-sifier to filter the candidate set during resolution."
"While the filter guarantees the qualification of the candidates, it removes too many positive candi-dates, and thus the recall suffers."
"In our future work, we intend to adopt a looser filter together with an anaphoricity determination module[REF_CITE]."
"Only if an encountered NP is determined as an anaphor, we will select an antecedent from the candidate set generated by the looser filter."
"Furthermore, we would like to incorporate more syntactic features into our feature set, such as grammatical role or syntactic parallelism."
These features may be help-ful to improve the performance of pronoun resolu-tion.
"We introduce a MetaGrammar, which al-lows us to automatically generate, from a single and compact MetaGrammar hier-archy, parallel Lexical Functional Gram-mars (LFG) and Tree-Adjoining Gram-mars (TAG) for French and for English: the grammar writer specifies in compact manner syntactic properties that are po-tentially framework-, and to some extent language-independent (such as subcatego-rization, valency alternations and realiza-tion of syntactic functions), from which grammars for several frameworks and languages are automatically generated offline. 1"
"Expensive dedicated tools and resources (e.g. gram-mars, parsers, lexicons, etc.) have been developed for a variety of grammar formalisms, which all have the same goal: model the syntactic properties of nat-ural language, but resort to a different machinery to achieve that goal."
"However, there are some core syn-tactic phenomena on which a cross-framework (and to some extent a cross-language) consensus exists, such as the notions of subcategorization, valency al-ternations, syntactic function."
"From a theoretical perspective, a MetaGrammatical level of representa-tion allows one to encode such consensual pieces of syntactic knowledge and to compare different frame-works and languages."
"From a practical perspective, encoding syntactic phenomena at a metagrammati-cal level, from which grammars for different frame-works and languages are generated offline, has sev-eral advantages such as portability among grammat-ical frameworks, better parallelism, increased coher-ence and consistency in the grammars generated and less need for human intervention in the grammar de-velopment process."
"In section 2, we explain the notion of MetaGram-mar (MG), present the MG tool we use to gener-ate TAGs, and how we extend the approach to gen-erate LFGs."
"In section 3, we justify the use of a MetaGrammar for generating LFGs and explore sev-eral options, i.e. domains of locality, for doing so."
"In sections 4 and 5, we discus the handling of va-lency alternations without resorting to LFG lexical rules, and the treatment of long-distance dependen-cies."
"In sections 6 and 7, we discuss the advantages of a MG approach and the automatic generation of par-allel TAG-LFG grammars for English and for French with an explicit sharing of both cross-language and cross-framework syntactic knowledge in the MG."
"The notion of MetaGrammar was originally pre-sented[REF_CITE]to automatically generate wide-coverage TAGs for French and Italian 2 , using a compact higher-level layer of linguistic description which imposes a general organization for syntactic information in a three-dimensional hierarchy: • Dimension [Footnote_1]: initial subcategorization • Dimension [Footnote_2]: valency alternations and redistri-bution of functions • Dimension 3: surface realization of arguments."
1 We assume the reader has a basic knowledge of TAGs and LFGs and refer respectively[REF_CITE]and[REF_CITE]for an introduction to these frameworks.
2 A Similar MetaGrammar type of organization for TAGs was independently presented[REF_CITE]for English.
"Each terminal class in dimension 1 encodes an initial subcategorization (i.e. transitive, ditransitive etc...); Each terminal class in dimension 2 - a list of ordered redistributions of functions (e.g. to add an argument for causatives, to erase one for passive with no agents ...); Each terminal class in dimen-sion 3 - the surface realization of a syntactic func-tion (e.g. declares if a direct-object is pronominal-ized, wh-extracted, etc.)."
"Each class in the hierar-chy is associated to the partial description of a tree[REF_CITE]which encodes fa-ther, dominance, equality and precedence relations between nodes."
"A well-formed tree is generated by inheriting from exactly one terminal class from di-mension 1, one terminal class from dimension 2 3 , and n terminal classes from dimension 3 (where n is the number of arguments of the elementary tree being generated)."
"For instance, the elementary tree for “Par qui sera accompagnée Marie” (By whom will Mary be accompanied) is generated by inheriting from tran-sitive in dimension 1, from passive in dimension 2 and subject-nominal-inverted for its subject and Wh-questioned-object for its object in dimension [Footnote_3]."
"3 This terminal class may be the result of the crossing of sev-eral super-classes, to handle complex phenomena such as Pas-sive+Causative."
"This particular tool was used to develop from a com-pact hand-coded hierarchy of a few dozen nodes, a wide-coverage TAG[REF_CITE]elementary trees[REF_CITE], as well as a medium-size"
TAG for Italian[REF_CITE].
The compactness of the hierarchy is due to the fact that nodes are de-fined only for simple syntactic phenomena: classes for complex syntactic phenomena (e.g. Topicalized-object+Pronominalized) are generated by automatic crossings of classes for simple phenomena.
"In ad-dition to proposing a compact representation of syn-tactic knowledge,[REF_CITE]explored whether some components of the hierarchy could be re-used across similar languages (French and Italian)."
"How-ever, she developed two distinct hierarchies to gen-erate grammars for these two languages and gener-ated only TAG grammars."
We extend the use of the MetaGrammar to generate LFGs and also push fur-ther its cross-language and cross-framework potential by generating parallel TAGs and LFGs for English and French from one single hierarchy [Footnote_4] .
"4 We also generate Range Concatenation Grammars[REF_CITE], but do not develop this point here."
"The grammar rules we generate are sorted by syn-tactic phenomena, thanks to the notion of HyperTag, introduced[REF_CITE]."
"The main idea behind HyperTags is to keep track, when trees (i.e. grammar rules) are generated from a MetaGrammar hierarchy, of which terminal classes were used for generating the tree."
This allows one to obtain a framework-independent feature structure containing the salient syntactic characteristics of each grammar rule [Footnote_5] .
"5 The notion of HyperTag was inspired by that of supertags[REF_CITE], which consists in assigning a TAG elementary tree to lexical items, hence enriching traditional POS tagging. However, HyperTags are framework-independent."
"For instance, the verb give in A book was given to Mary could be assigned the HyperTag:  Ditransitive"
"Although we retain the linguistic insights pre-sented[REF_CITE], that is the three dimen-sions to model syntax, (subcategorization, valency alternation, realization of syntactic arguments), we slightly alter it, and add sub-dimensions for the real-ization of predicates as well as modifiers."
"Moreover, we use a different MetaGrammar tool which is less framework-dependent and supports the notion of Hy-perTag."
"To generate TAGs and LFGs, we use the MG com-piler presented[REF_CITE][Footnote_6] ."
6 This compiler is freely available[URL_CITE]
"Each class in the MG hierarchy encodes: • Its SuperClasse(s) • A HyperTag which captures the salient linguis-tic characteristics of that class. • What the class needs and provides. • A set of quasi-nodes (i.e. variables) • Topological relations between these nodes (fa-ther, dominates, precedes, equals) [Footnote_7] • A function for each quasi-nodes to decorate the tree (e.g. traditional agreement features and/or LFG functional equations)."
"7 We have augmented the tool to support free variables for nodes, optional resources, as well as additional relations such as sister and c-command. We do not detail these technical points for sake of brevity."
"The MG tool automatically crosses the nodes in the hierarchy, looking to create “balanced” classes, that is classes that do not need nor provide any re-source [Footnote_8] ."
8 Another way to see this is by analogy to a resource allocation graph.
"Then for each balanced terminal class, the HyperTags are unified, and the structural constraints between quasi-nodes are unified; If the unification succeeds, one or more &lt;HyperTag, tree&gt; pairs are generated."
"When generating a TAG, tree is inter-preted as a TAG elementary tree (i.e. a grammar rule)."
"When generating an LFG, tree is a tree deco-rated with traditional LFG functional annotations (in a way which is similar to constituent trees decorated with functional annotation e.g.[REF_CITE]), and is in a second step broken down into one or more LFG rules."
"Figure 1 illustrates how a simple dec-orated tree is generated with the MG compiler, and how the decorated tree corresponds to one TAG el-ementary tree and to two LFG rewriting rules for a canonical transitive construction."
"In addition, to facilitate the grammar-lexicon interface, each deco-rated tree yields an LFG lexical template (here, Sub-jObj:V (↑Pred=‘x&lt;(↑Subj)(↑Obj)&gt;’)."
"Because TAGs are a tree rewriting system, there are intrinsic redundancies in the rules of a TAG."
"E.g., all the rules for verbs with a canonical NP subject and a canonical realization of the verb will have a redun-dant piece of structure (S NP0↓ (VP (V⋄))) ."
"This piece of structure will be present not only for each new sub-categorization frame (intransitive, transitive, ditransi-tive...), but also for all related non-canonical syntactic constructions such as in each grammar rule encoding a Wh-extracted object."
This redundancy justifies the use of a MetaGrammar for TAGs.
"Since LFG rules rely on a context free backbone, it is generally admit-ted that there is less redundancy in LFG than in TAG."
"However, there are still redundancies, at the level of rewriting rules, at the level of functional equations, and at the level of lexical entries."
"To illustrate such redundancies, we take the example of French ditran-sitives with the insertion of one or more modifiers."
"The direct object is realized as an NP, the second ob-ject as a PP."
Both orders NP PP and PP NP are ac-ceptable.
"On top of that, one or more modifiers may be inserted before, after or between the two argu-ments, and can be of almost any category (PP, ADVP,"
Here is a non exhaustive list of acceptable word-order variations: - Jean donne une pomme à
Marie (lit: J. gives an apple to M.) - Jean donne à
Marie une pomme (lit: J. gives to M. an apple) - Jean aujourd’hui donne à
Marie une pomme (lit: J. today gives to M. an apple) - Jean donne à
Marie chaque matin une pomme avant le départ du train (lit: J gives to M. every morning an apple before the departure of the train) - Jean donne chaque matin à
Marie une pomme (lit: J. gives each morning to M. an apple) - Aujourd’hui Jean donne à
Marie une pomme (lit: Today J. gives to M. an apple)
"A first rule for VP expansion, accounting for the free order between the first and second object without modifiers, is shown below:"
VP → V (NP) PP (NP) ↑=↓ (↑Obj)=↓ (↑SecondObj)=↓ (↑Obj)=↓
"This VP rule is redundant: the NP is mentioned twice, with its associated functional equation."
"The NPs are both marked optional because at least one of them has to be not realized, else no well-formed F-structure could be built since the uniqueness condi-tion would be violated by the presence of two direct-objects: for a sentence such as “*Jean donne une pomme à"
"Mary une pomme”/J. gives an apple to M. an apple, a C-structure would be built but, as expected, no corresponding well-formed F-structure."
Let us now enrich the rule to account for modifier in-sertion.
This yields the VP expansion shown in 2(a).
"The rule for VP expansion is now highly redun-dant, although the syntactic phenomena handled by this rule are very simple ones: the NP for the di-rect object is repeated twice, along with its functional equation, the disjunction (ADVP|NP|PP) is repeated 5 times, again with its functional equation."
This gives us grounds to support a MetaGrammar type of orga-nization for LFG.
"In practice, as described[REF_CITE], additional LFG notation is available such as operators like “insert or ignore”, ”shuffle” ”ID/LP”, ”Macros” etc."
"However, these op-erators, which are motivated from a formal perspec-tive, but not so much from a linguistic perspective, yield two major problems: first, not all LFG parsers support those additional operators."
"Second, the pro-liferation of operators allows for a same rule to be expressed in many different ways, which is helpful for grammar writing purpose, but not so desirable for maintenance purpose [Footnote_9] ."
"9 This can be compared to computer programs written in Perl, which are easy to develop, but hard to read and maintain. A"
"Although nothing pre- vents the MG generator to create rules with opera-tors such as “ignore or insert”, we chose not to do so."
"Instead of generating rules with operators or rules like (2a), we generate two rules (2b) and (2c) in order to have uniqueness, completeness and coherence not only at the F-structure level but also at the C-structure level. [Footnote_10] ."
"10 Thus the grammars we generate exhibit redundancies for modifiers, but, since the MG hierarchy has relatively few redun-dancies, and since these grammars are automatically generated, the problem is minor."
"Moreover, for lexical organization, practical LFGs resort to the notion of lexical template but from a linguistic perspective, the lexicon is not cleanly or-ganized[REF_CITE]."
"We have seen in section 2.2 that the MG tool we use outputs &lt;HyperTag, tree&gt; pairs, where tree is dec-orated with functional equations and corresponds to one or more LFG rewriting rules (Figure 1)."
"In order to generate LFG rules with a MG, we have two options."
"The first option consists in generating “standard” LFG rules, that is trees of depth 1 deco-rated with functional equations."
"Figure 3 illustrates such as decorated tree, which yields one LFG rewrit-ing rule, and one lexical entry for French verbs such as “éloigner” ( take away from), which take an NP object and a PP object introduced by “de”. (Ex: “Pe-ter éloigne son enfant de la fenêtre”/ P. takes his child away from the window)."
"The second option, which is the one we have opted for, consists in generating con-stituent trees which may be of depth superior to one, decorated with feature equations."
It has the following advantages: • It allows for a more natural parallelism between the TAG and LFG grammars generated • It allows for a more natural encoding of syntax at the MetaGrammar level • It allows us to generate LFGs without Lexical Rules • It allows us to easily handle long-distance de-pendencies.
The trees decorated with LFG functional annota-tions are then decomposed into standard LFG rewrit-ing rules and lexical entries [Footnote_12] .
"12 Non terminal symbols symbols are renamed and, in a second phase, rules which differ only by the name of their non terminals are merged, in a manner similar to that used in (Hepple and van[REF_CITE]). For space reasons, we do not detail the algo-rithm here."
The grammar we ob-tain is then interfaced with a parser [Footnote_13] .
13 We use the freely available XLFG parser described[REF_CITE]and have also experimented with the Xerox parser[REF_CITE].
"Concerning the first point (TAG-LFG parallelism), the trees dec-orated with functional equations and TAG elemen-tary trees are very similar, as was first discussed[REF_CITE]."
"Concerning the second point (more natural encoding of the MetaGrammar level), the “resource model” of the MetaGrammar, based on “needs” and “provides”, allows for a natural encod-ing and enforcement of LFG coherence, complete-ness and uniqueness principles: A transitive verb needs exactly one resource “Subject” and one re-source “Object”."
Violations result in invalid classes which do not yield any rules.
"So from that perspec-tive, it makes little sense, apart from practical rea-sons such as interfacing the grammar with an existing parser, to force the rules generated to be trees of depth one."
"Moreover, classical completeness/coherence conditions have received a similar resource-sensitive re-interpretation in LFG to compute semantic struc-tures using linear logic[REF_CITE]."
We devote the next two sections to the third (lexical rules) and fourth (wh) points.
Traditional LFGs encode phrase structure realiza-tions of syntactic functions such as the wh-extraction or pronominalization of an object in phrase structure rules.
"In the MetaGrammar, these are encoded in the “Argument Realization” dimension (dimension 3 in Candito’s terminology)."
"For valency alternations, i.e. when initial syntactic functions are modified, LFG re-sorts to the additional machinery of lexical rules [Footnote_14] ."
"14 Or, alternatively, some notion of lexical mapping, which we do not discuss here."
"However, these valency alternations are encoded di-rectly in the MetaGrammar in the “valency alterna-tion” dimension (dimension 2 in Candito’s terminol-ogy)."
"Hence, when a rule is generated for a canonical transitive verb, rules are generated not only for all possible argument realization for the subject and di-rect object (wh-questioned, relativized, cliticized for French etc.), but also for all the valency alternations allowed for the subcategory frame concerned (here, passive with/without agent, causative etc)."
"Therefore, there is no need to generate usual LFG lexical rules, and the absence of lexical rules has no effect on inter-facing the grammars we generate with existing LFG parsers."
Fig. 4 illustrates the generation of a deco-rated tree for passive-with-no-agent.
"When generating TAGs and LFGs from a single MG hierarchy, we must make sure that long-distance phe-nomena are correctly handled."
"The only difference between TAG and LFG is that for TAG, we must make sure that bridge verbs are auxiliary trees, i.e. have a foot node, whereas for LFG we must make sure that extraction rules have a node decorated with a functional uncertainty equation."
"In TAGs, long distance dependencies are handled through the do-main of locality of elementary trees, the argument-predicate co-occurrence principle and the adjunction operati[REF_CITE]."
Figure 5 illustrates the TAG analysis of What did Mary say that John ate: the extracted element is in the same grammar rule as its predicate “ate” [Footnote_15] and the tree an-chored by the bridge verb is inserted in the “ate” tree thanks to the adjunction operation.
"15 Although a trace is present in rule for “ate”, following the convention of the Xtag project, it is not compulsory and not needed from a formal point of view."
"More trees can adjoin in to analyze What does P. think that M. said ... that John ate using the same mechanism, which we retain in the TAGs we generate by generating auxil-iary tree for bridge verbs (i.e. trees with a foot node)."
"In LFG, long-distance dependencies are handled by functional uncertainty[REF_CITE]."
Here is a small LFG grammar to analyze What did M. say that John ate.
"The extracted element (node NP o in rule 4) is asso-ciated to a function path (in bold characters), which is unknown since an arbitrary number of clauses can ap-pear between “NP o ” and its regent (V y in rule 6)."
"The result of the LFG analysis for What did M. say that J. ate, using this standard LFG grammar is shown in Figure 6."
A constituent structure is built using the the rewriting rules.
The functional equations associated to nodes compute an F-structure which ensures that each predicate of the sentence (i.e. “say” and “ate”) have their arguments realized.
"The need for func-tional uncertainty results from the fact that in LFG, contrary to TAGs, the extracted element (NP o ) and its governor (V y ) are located in different grammar rules."
"Hence, when generating LFGs, we must make sure that the decorated tree bears a functional uncertainty equation at the site of the extraction. 7 illustrates the generation of such a decorated tree (identical to the TAG tree for ”ate” modulo the functional equations), which will be decomposed into rules 4, 5 and 6. [Footnote_16]"
"16 Because the MG does not impose a restricted domain of lo-cality,[REF_CITE]proposes an alternative to functional un-certainty, which we do not present here for space reasons."
"A first advantage of using a MetaGrammar, dis-cussed[REF_CITE], is that the syntactic phenomena covered are quite system-atic: if rules are generated for “transitive-passive-whExtractedByPhrase” (e.g. By whom was the mouse eaten), and if the hierarchy includes ditran-sitive verbs, then the automatic crossing of phe-nomena ensures that sentences will be generated for “ditransitive-passive-whExtractedByPhrase” (i.e. By whom was Peter given a present)."
All rules for word order variations are automatically generated by un-derspecifying relations between quasi-nodes in the MG hierarchy (e.g. precedence relation between first and second object for ditransitives in French).
A sec-ond advantage of the MG is to minimize the need for human intervention in the grammar development process.
"Humans encode the linguistic knowledge in a compact manner i.e. the MG hierarchy, and then verify the validity of the rules generated."
"If some grammar rules are missing or incorrect, then changes are made directly in the MG hierarchy and never in the generated rules [Footnote_17] ."
17 Exceptionality is handled in the MG hierarchy as well. We do not have much to say about it: only that the MG does not impose any additional burden to handle syntactic “exceptions” compared to hand-crafted grammars.
This ensures a homogeneity not necessarily present with traditional hand-crafted grammars.
"A third and essential advantage is that it is straightforward to obtain from a single hierarchy parallel multi-lingual grammars similar to the paral-lel LFG grammars presented[REF_CITE]and[REF_CITE], but with an explicit sharing of classes 18 in the MetaGrammar hierarchy plus a cross-framework application. 19"
"So far, we have implemented a non trivial hierarchy which consists of 189 classes."
A fragment of the hi-erarchy is shown in Figure 8.
"From this hierarchy, we generate 550 decorated trees, which correspond to approx. 550 TAG trees and 140 LFG rules."
"We cover the following syntactic phenomena: 50 verb subcate-gorization frames (including auxiliaries, modals, sen-tential and infinitival complements), dative-shift for English, clitics (and their placement) for French, pas-sives with and without agent, long distance depen-dencies (relatives, wh-questions, clefts) and a few idiomatic expressions."
A more detailed presenta-tion of the LFG grammar is presented[REF_CITE].
"A more detailed discussion of the cross-language aspects with a comparison to re-lated work such as the LFG ParGram project, or HPSG matrix grammars[REF_CITE]may be found[REF_CITE][Footnote_20] ."
"20 The main difference with HPSG approaches such as Matrix is that HPSG type-hierarchies are an inherent part of the gram-mar, and deal only with one framework:HPSG, whereas our MG hierarchy is not an inherent part of the grammar, since it is used to generate cross-framework grammars offline."
The cross-language and cross-framework parallelism is insured by the HyperTags: Most classes in the hi-erarchy are shared for French and for English.
Lan-guage specific classes are marked using the binary features “English” and “French” in their HyperTag.
"So for instance, classes encoding clitic placement are marked [French=+;English=-] and classes pertain-ing to dative-shift are marked [French=-;English=+]."
This prevents the crossing of incompatible classes and hence the generation of incorrect rules (such as “Dative-shift-withCliticizedObject”).
"Similarly, most classes in the hierarchy are shared for TAGs and LFGs."
Classes specific to TAGs are marked [TAG=+;LFG=-] (and conversely for LFGs) [Footnote_21]
"21 We use binary features in order to add more languages and frameworks to the hierarchy. E.g. when adding German, some classes are shared for English and German, but not French and are marked [English=+;German=+;French=-]. This would not be possible if we had a non binary feature [Language=X]. The same reasoning applies for generating additional frameworks."
We have presented a MetaGrammar tool which al-lows us to automatically generate parallel TAG and LFG grammars for English and French.
We have discussed the handling of long-distance dependen-cies.
"We keep enriching our hierarchy in order to increase the coverage of our grammars, are adding new languages (German) and exploring the extension of the domain of locality to sentence level[REF_CITE]."
"The ultimate goal of this work is twofold: first, to maximize cross-language rule-sharing at the metagrammatical level; Second, to automatic extract MetaGrammars from a tree-bank[REF_CITE], and then automatically gener-ate grammars for different frameworks."
This paper proposes the application of finite-state approximation techniques on a unification-based grammar of word for-mation for a language like German.
"A refinement of an RTN-based approxima-tion algorithm is proposed, which extends the state space of the automaton by se-lectively adding distinctions based on the parsing history at the point of entering a context-free rule."
The selection of history items exploits the specific linguistic nature of word formation.
"As experiments show, this algorithm avoids an explosion of the size of the automaton in the approxima-tion construction."
"In English orthography, compounds following pro-ductive word formation patterns are spelled with spaces or hyphens separating the components (e.g., classic car repair workshop)."
"This is convenient from an NLP perspective, since most aspects of word formation can be ignored from the point of view of the conceptually simpler token-internal pro-cesses of inflectional morphology, for which stan-dard finite-state techniques can be applied. (Let us assume that to a first approximation, spaces and punctuation are used to identify token boundaries.)"
"It makes it also very easy to access one or more of the components of a compound (like classic car in the example), which is required in many NLP tech-niques (e.g., in a vector space model)."
"If an NLP task for English requires detailed in-formation about the structure of compounds (as complex multi-token units), it is natural to use the formalisms of computational syntax for English, i.e., context-free grammars, or possibly unification-based grammars."
"This makes it possible to deal with the bracketing structure of compounding, which would be impossible to cover in full generality in the finite-state setting."
"In languages like German, spelling conventions for compounds do not support such a convenient split between sub-token processing based on finite-state technology and multi-token processing based on context-free grammars or beyond—in German, even very complex compounds are written without spaces or hyphens: words like Verkehrswegepla-nungsbeschleunigungsgesetz (‘law for speeding up the planning of traffic routes’) appear in corpora."
"So, for a fully adequate and general account, the token-level analysis in German has to be done at least with a context-free grammar: [Footnote_1] For checking the selection features of derivational affixes, in the general case a tree or bracketing structure is required."
"1 For a fully general account of derivational morphology in English, the token-level analysis has to go beyond finite-state means too: the prefix non- in nonrealizability combines with the complex derived adjective realizable, not with the verbal stem realize (and non- could combine with a more complex form). However, since in English there is much less token-level inter-action between derivation and compounding, a finite-state ap-proximation of the relevant facts at token-level is more straight-forward than in German."
"For instance, the prefix Fehl- combines with nouns (compare (1)); however, it can appear linearly adjacent with a verb, including its own prefix, and only then do we get the suffix -ung, which turns the verb into a noun. (1) N N V  "
N V V N
Fehl ver arbeit ung mis work ‘misprocessing’
"Furthermore, context-free power is required to parse the internal bracketing structure of complex words like (2), which occur frequently and productively. (2) N N"
"As the results of the DeKo project on deriva-tional and compositional morphology of German show[REF_CITE], an adequate account of the word formation principles has to rely on a number of dimensions (or features/attributes) of the morphological units."
An affix’s selection of the el-ement it combines with is based on these dimen-sions.
"Besides part-of-speech category, the dimen-sions include origin of the morpheme (Germanic vs. classical, i.e., Latinate or Greek [Footnote_2] ), complexity of the unit (simplex/derived), and stem type (for many lemmata, different base stems, derivation stems and compounding stems are stored; e.g., träg in (2) is a derivational stem for the lemma trag(en) (‘bear’); heits is the compositional stem for the affix heit)."
"2 Of course, not the true ethymology is relevant here; ORIGIN is a category in the synchronic grammar of speakers, and for individual morphemes it may or may not be in accordance with diachronic facts."
"Given these dimensions in the affix feature selec-tion, we need a unification-based (attribute) gram-mar to capture the word formation principles explic-itly in a formal account."
"A slightly simplified such grammar is given in (3), presented in a PATR-II-style notation: [Footnote_3] (3) a. X0 X1 X2 X1 CAT = PREFIX"
3 An implementation of the DeKo rules in the unification for-malism YAP is discussed[REF_CITE].
X0 CAT = X1 MOTHER - CAT X0 COMPLEXITY = PREFIX - DERIVED
X1 SELECTION = X2 b. X0 X1 X2 X2 CAT = SUFFIX X0 CAT =
X2 MOTHER - CAT X0 COMPLEXITY = SUFFIX - DERIVED X2 SELECTION = X1 c. X0 X1 X2
X0 CAT = X2 CAT X0 COMPLEXITY = COMPOUND (4) Sample lexicon entries
"Applying the suffixation rule, we can derive intellektual.isier- (the stem of ‘intellectualize’) from the two sample lexicon entries in (4)."
"Note how the selection feature ( SELECTION ) of prefixes and af-fixes are unified with the selected category’s features (triggered by the last feature equation in the prefixa-tion and suffixation rules (3a,b))."
"Context-freeness Since the range of all atomic-valued features is finite and we can exclude lexicon entries specifying the SELECTION feature embedded in their own SELECTION value, the three attribute grammar rewrite rules can be compiled out into an equivalent context-free grammar."
"While there is linguistic justification for a context-free (or unification-based) model of word formation, there are a number of considerations that speak in favor of a finite-state account. (A basic assumption made here is that a morphological analyzer is typi-cally used in a variety of different system contexts, so broad usability, consistency, simplicity and gen-erality of the architecture are important criteria.)"
"First, there are a number of NLP applications for which a token-based finite-state analysis is stan-dardly used as the only linguistic analysis."
It would be impractical to move to a context-free technol-ogy in these areas; at the same time it is desirable to include an account of word formation in these tasks.
"In particular, it is important to be able to break down complex compounds into the individual com-ponents, in order to reach an effect similar to the way compounds are treated in English orthography."
"Second, inflectional morphology has mostly been treated in the finite-state two-level paradigm."
"Since any account of word formation has to be combined with inflectional morphology, using the same tech-nology for both parts guarantees consistency and re-usability. [Footnote_4]"
"4 An alternative is to construct an interface component be-tween a finite-state inflectional morphology and a context-free word formation component. While this can be conceivably done, it restricts the applicability of the resulting overall system, since many higher-level applications presuppose a finite-state analyzer; this is for instance the case for the Xerox Linguistic Environment ([URL_CITE]a de-velopment platform for syntactic Lexical-Functional Grammars[REF_CITE]."
"Third, when a morphological analyzer is used in a linguistically sophisticated application context, there will typically be other linguistic components, most notably a syntactic grammar."
"In these compo-nents, more linguistic information will be available to address derivation/compounding."
"Since the nec-essary generative capacity is available in the syntac-tic grammar anyway, it seems reasonable to leave more sophisticated aspects of morphological analy-sis to this component (very much like the syntax-based account of English compounds we discussed initially)."
"Given the first two arguments, we will however nevertheless aim for maximal exactness of the finite-state word formation component."
"Naturally, existing morphological analyzers of lan-guages like German include a treatment of compo-sitional morphology (e.g.,[REF_CITE])."
An over-generation strategy has been applied to ensure cov-erage of corpus data.
"Exactness was aspired to for the inflected head of a word (which is always right-peripheral in German), but not for the non-head part of a complex word."
The non-head may essentially be a flat concatenation of lexical elements or even an arbitrary sequence of symbols.
"Clearly, an account making use of morphological principles would be desirable."
"While the internal structure of a word is not relevant for the identification of the part-of-speech category and morphosyntactic agreement in-formation, it is certainly important for information extraction, information retrieval, and higher-level tasks like machine translation."
"An alternative strategy—putting emphasis on a linguistically satisfactory account of word forma-tion—is to compile out a higher-level word forma-tion grammar into a finite-state automaton (FSA), assuming a bound to the depth of recursive self-embedding."
"This strategy was used in a finite-state implementation of the rules in the DeKo project[REF_CITE], based on the AT&amp;T Lextools toolkit by Richard Sproat. [Footnote_5] The toolkit provides a compilation routine which transforms a certain class of regular-grammar-equivalent rewrite gram-mars into finite-state transducers."
"5 Lextools: a toolkit for finite-state linguistic analysis, AT&amp;T Labs Research;[URL_CITE]"
"Full context-free recursion has to be replaced by an explicit cascading of special category symbols (e.g., N1, N2, N3, etc.)."
"Unfortunately, the depth of embedding occur-ring in real examples is at least four, even if we assume that derivations like ver.träg.lich (‘com-patible’; in (2)) are stored in the lexicon as complex units: in the initially mentioned com-pound Verkehrs.wege.planungs.beschleunigungs.ge-setz (‘law for speeding up the planning of traffic routes’), we might assume that Verkehrs.wege (‘traf-fic routes’) is stored as a unit, but the remainder of the analysis is rule-based."
"With this depth of recursion (and a realistic morphological grammar), we get an unmanagable explosion of the number of states in the compiled (intermediate) FSA."
"We propose a refinement of finite-state approxima-tion techniques for context-free grammars, as they have been developed for syntax ([REF_CITE])."
Our strategy assumes that we want to express and develop the morphological grammar at the linguistically satisfactory level of a (context-free-equivalent) unification grammar.
"In process-ing, a finite-state approximation of this grammar is used."
"Exploiting specific facts about morphology, the number of states for the constructed FSA can be kept relatively low, while still being in a position to cover realistic corpus example in an exact way."
"The construction is based on the following obser-vation: Intuitively, context-free expressiveness is not needed to constrain grammaticality for most of the word formation combinations."
"This is because in most cases, either (i) morphological feature selec-tion is performed between string-adjacent terminal symbols, or (ii) there are no categorial restrictions on possible combinations. (i) is always the case for suffixation, since German morphology is exclu-sively right-headed. 6"
"So the head of the unit selected by the suffix is always adjacent to it, no matter how complex the unit is: (5) X Y   ..."
"Y X (i) is also the case for prefixes combining with a sim-ple unit. (ii) is the case for compounding: while affix-derivation is sensitive to the mentioned dimen-sions like category and origin, no such grammati-cal restrictions apply in compounding. [Footnote_7]"
"7 Of course, when speakers disambiguate the possible brack-etings of a complex compound, they can exclude many com-binations as implausible. But this is a defeasible world knowledge-based effect, which should not be modeled as strict selection in a morphological grammar."
"So the fact that in compounding, the heads of the two combined units may not be adjacent (since the right unit may be complex) does not imply that context-freeness is required to exclude impossible combinations: ([Footnote_6]) X oror XX X X X X X X X X X X X X X X X X"
"6 This may appear to be falsified by examples like ver- (V ) + Urteil (N, ‘judgement’) = verurteilen (V, ‘convict’); how-ever, in this case, a noun-to-verb conversion precedes the prefix derivation. Note that the inflectional marking is always right-peripheral."
The only configuration requiring context-freeness to exclude ungrammatical examples is the combina-tion of a prefix with a complex morphological unit:
X X (7)  X ... X
"As (1) showed, such examples do occur; so they should be given an exact treatment."
"However, the depth of recursive embeddings of this particular type (possibly with other embeddings intervening) in re-alistic text is limited."
"So a finite-state approximation keeping track of prefix embeddings in particular, but leaving the other operations unrestricted seems well justified."
"We will show in sec. 6 how such a tech-nique can be devised, building on the algorithm re-viewed in sec. 5."
A comprehensive overview and experimental com-parison of finite-state approximation techniques for context-free grammars is given[REF_CITE].
"In Nederhof’s approximation experiments based on an HPSG grammar, the so-called RTN method provided the best trade-off between exactness and the resources required in automaton construction. (Techniques that involve a heavy explosion of the number of states are impractical for non-trivial grammars.)"
"More specifically, a parameterized ver-sion of the RTN method, in which the FSA keeps track of possible derivational histories, was consid-ered most adequate."
The RTN method of finite-state approximation is inspired by recursive transition networks (RTNs).  
RTNs are collections of sub-automata.
"For each rule in a context-free grammar, a sub-automaton with states is constructed:   (8)   . . . . . ."
"As a symbol is processed in the automaton (say, ), the RTN control jumps to the respective sub-  automaton’s initial state (so, from in (8) to a state in the sub-automaton for ), keeping the return address on a stack representation."
"When the sub-automaton is in its final state ( ), control jumps back to the next state in the automaton: ."
"In the RTN-based finite-state approximation of a context-free grammar (which does not have an un-limited stack representation available), the jumps   to sub-automata are hard-wired, i.e., transitions for non-terminal symbols like the transition from to are replaced by direct -transitions to the ini-tial state and from the end state of the respective sub-automata: (9). (Of course, the resulting non-deterministic FSA is then determinized and mini-mized by standard techniques.) (9)   . . . . . .   . . .   . . ."
"The technique is approximative, since on jump-ing back, the automaton “forgets” where it had come  from, so if there are several rules with a right-hand side occurrence of, say , the automaton may non-deterministically jump back to the wrong rule."
"For instance, if our grammar consists of a recursive pro-duction B a B c for category B, and a production B b, we will get the following FSA:  (10) b  a c"
"The approximation loses the original balancing of a’s and c’s, so “abcc” is incorrectly accepted."
"In the parameterized version of the RTN method th[REF_CITE]proposes, the state space is enlarged: different copies of each state are created to keep track of what the derivational his-tory was at the point of entering the present sub-automaton."
"For representing the derivational his-tory, Nederhof uses a list of “dotted” productions, as known from Earley parsing."
"So, for state in (10), we would get copies  ,   , etc., likewise for the states &quot;! !"
"The -transitions for jumping to and from embedded categories observe the laws for legal context-free derivations, as far as recorded by the dotted rules. 8 Of course, the win-dow for looking back in history is bounded; there is a parameter (which Nederhof calls # ) for the size of the history list in the automaton construction."
"Be-yond the recorded history, the automaton’s approxi-mation will again get inexact. (11) shows the parameterized variant of (10), with parameter %# $&apos;&amp; , i.e., a maximal length of one ele-ment for , the .+ - history ( ( is used as a short-hand for item ) * 0/21 ). (11) will not accept “abcc” (but * it will accept “aabccc”)."
"The number of possible histories (and thus the number of states in the non-deterministic FSA) grows exponentially with the depth parameter, but only polynomially with the size of the grammar."
"Hence, with parameter [Footnote_8]# 9$ &amp; (“RTN2”), the tech-nique is usable for non-trivial syntactic grammars."
8 For the exact conditions see[REF_CITE].
"For each subgrammar, the RTN construction and FSA minimization is performed separately, so in the end, the relatively small mini-mized FSAs can be reassembled."
"In word formation, the split of the original gram-mar into subgrammars of mutually recursive (MR) categories has no great complexity-reducing effect (if any), contrary to the situation in syntax."
"Essen-tially, all recursive categories are part of a single large equivalence class of MR categories."
"Hence, the size of the grammar that has to be effectively ap-proximated is fairly large (recall that we are dealing with a compiled-out unification grammar)."
"For a re-alistic grammar, the parameterized RTN technique is unusable with parameter :# $ or higher."
"Moreover, a history of just two previous embeddings (as we get it with ;# $ ) is too limited in a heavily recursive setting like word formation: recursive embeddings of depth four occur in realistic text."
"However, we can exploit more effectively the “mildly context-free” characteristics of morpholog- ical grammars (at least of German) discussed in sec. 4."
"We propose a refined version of the parame-terized RTN-method, with a selective recording of derivational history."
We stipulate a distinction of   two types of rules: “historically important” h-rules  (written ) and non-h-rules (writ-ten ).
The h-rules are treated as in the parameterized RTN-method.
The non-h-rules are not recorded in the construction of history lists;  they are however taken into account in the determi- -nation of legal histories.
"For instance, ) 0* / 1 will appear as a legal history for the sub-automaton for some category D only if there is a derivation B D (i.e., a sequence of rule rewrites mak-ing use of non-h-rules)."
"By classifying certain rules as non-h-rules, we can concentrate record-keeping resources on a particular subset of rules."
"In sec. 4, we saw that for most rules in the compiled-out context-free grammar for German morphology (all rules compiled from (3b) and (3c)), the inexactness of the RTN-approximation does not have any negative effect (either due to head-adjacency, which is preserved by the non-parametric version of RTN, or due to lack of category-specific constraints, which means that no context-free bal-ancing is checked)."
"Hence, it is safe to classify these rules as non-h-rules."
The only rules in which the in-exactness may lead to overgeneration are the ones compiled from the prefix rule (3a).
"Marking these rules as h-rules and doing selective history-based RTN construction gives us exactly the desired effect: we will get an FSA that will accept a free alternation of all three word-formation types (as far as compat-ible with the lexical affixes’ selection), but stacking of prefixes is kept track of."
"Suffix derivations and compounding steps do not increase the length of our history list, so even with a %# $&apos;&amp; or # $ , we can get very far in exact coverage."
"Besides the selective history list construction, two further optimizations were applied to Nederhof’s (2000) parameterized RTN-method: First, Earley items with the same remainder to the right of the dot - -were collapsed ( ) 1 and ) 1 )."
"Since they are indistinguishable in terms of future behavior, making a distinction results in an unnec- essary increase of the state space. (Effectively, only the material to the right of the dot was used to build the history items.)"
"Second, for immedi-ate right-peripheral recursion, the history list was   collapsed; i.e., if the current history has the form  ) 1 ! , and the next item to be added -would be again ) 1 , the present list is left unchanged."
This is correct because completion of  ) 1 will automatically result in the com-pletion of all immediately stacked such items.
"Together, the two optimizations help to keep the number of different histories small, without losing relevant distinctions."
"Especially the second opti-mization is very effective in a selective history set-ting, since the “immediate” recursion need not be literally immediate, but an arbitrary number of non- h-rules may intervene."
"So if we find a noun pre- -fix [N N N], i.e., we are looking for a noun, we need not pay attention (in terms of coverage-relevant history distinctions) whether we are running into compounds or suffixations: we know, when we find another noun prefix (with the same selection features, i.e., origin etc.), one analysis will always be to close off both prefixations with the same noun: (12) N"
"Of course, the second prefixation need not have hap-pened on the right-most branch, so at the point of having accepted N N N, we may actually be in the configuration sketched in (13a):"
"Note however that in terms of grammatically le-gal continuations, this configuration is “subsumed” by (13b), which is compatible with (12) (the top ‘?’ category will be accessible using -transitions back from a completed N—recall that suffixation and compounding is not controlled by any history items)."
"So we can note that the only examples for which the approximating FSA is inexact are those where the stacking depth of distinct prefixes (i.e., selecting for a different set of features) is greater than our pa-rameter # ."
"Thanks to the second optimization, the relatively frequent case of stacking of two verbal prefixes as in vor.ver.arbeiten ‘preprocess’ counts as a single prefix for book-keeping purposes."
"We implemented the selective history-based RTN-construction in Prolog, as a conversion routine that takes as input a definite-clause grammar with compiled-out grounded feature values; it produces as output a Prolog representation of an FSA."
"The re-sulting automaton is determinized and minimized, using the FSA library for Prolog by Gertjan van No-ord. 9 Emphasis was put on identifying the most suit-able strategy for dealing with word formation taking into account the relative size of the FSAs generated (other techniques than the selective history strategy were tried out and discarded)."
"The algorithm was applied on a sample word for-mation grammar with 185 compiled-out context-free rules, displaying the principled mechanism of cat-egory and other feature selection, but not the full set of distinctions made in the DeKo project. [Footnote_9] of the rules were compiled from the prefixation rule, and were thus marked as h-rules for the selective method."
9 FSA6.2xx: Finite State Automata Utilities;
We ran a comparison between a version of the non-selective parameterized RTN-method[REF_CITE]and the selective history method proposed in this paper.
An overview of the results is given in fig. 1. [Footnote_10]
10 The fact that the minimized FSAs[URL_CITE]are identical for the selective method is an artefact of the sample grammar.
It should be noted that the op-timizations of sec. 7 were applied in both methods (the non-selective method was simulated by mark- ing all rules as h-rules).
"As the size results show, the non-deterministic FSAs constructed by the selective method are more complex (and hence resource-intensive in minimiza-tion) than the ones produced by the “plain” param-eterized version."
"However, the difference in exact-ness of the approximizations has to be taken into ac-count."
"As a tentative indication for this, note that the minimized FSA for ;# $ &amp; in the plain version has only two states; so obviously too many distinctions from the context-free grammar have been lost."
"In the plain version, all word formation operations are treated alike, hence the history list of length one or two is quickly filled up with items that need not be recorded."
"A comparison of the number of dif-ferent pairs of categories and history lists used in the construction shows that the selective method is more economical in the use of memory space as the depth parameter grows larger. (For # $ , the selec-tive method would even have fewer different cate-gory/history list pairs than the plain method, since the patterns become repetitive."
"However, the ap-proximations were impractical for #  $ .)"
"Since the selective method uses non-h-rules only in the deter-mination of legal histories (as discussed in sec. 6), it can actually “see” further back into the history than the length of the history list would suggest."
"What the comparison clearly indicates is that in terms of resource requirements, our selective method with a parameter # is much closer to the # -version of the plain RTN-method than to the next higher # version."
"But since the selective method focuses its record-keeping resources on the crucial aspects of the finite-state approximation, it brings about a much higher gain in exactness than just ex-tending the history list by one in the plain method."
We also ran the selective method on a more fine- grained morphological grammar with 403 rules (in-cluding 12 h-rules).
"Parameter # $ &amp; was ap-plicable, leading to a non-deterministic FSA with 7,345 states, which could be minimized."
"Param-eter # $ led to a non-deterministic[REF_CITE]601 states, for which minimization could not be completed due to a memory overflow."
It is one goal for future research to identify possible ways of breaking down the approximation construction into smaller subproblems for which minimization can be run separately (even though all categories belong to the same equivalence class of mutually recursive cat-egories). [Footnote_11] Another goal is to experiment with the use of transduction as a means of adding structural markings from which the analysis trees can be re-constructed (to the extent they are not underspecified by the finite-state approach); possible approaches are discussed[REF_CITE].
11 A related possibility pointed out by a reviewer would be to expand features from the original unification-grammar only where necessary (cf.[REF_CITE]).
"Inspection of the longest few hundred prefix-containing word forms in a large German newspaper corpus indicates that prefix stacking is rare. (If there are several prefixes in a word form, this tends to arise through compounding.)"
No instance of stacking of depth 3 was observed.
"So, the range of phenom-ena for which the approximation is inexact is of lit-tle practical relevance."
"For a full evaluation of the coverage and exactness of the approach, a compre-hensive implementation of the morphological gram-mar would be required."
"We ran a preliminary exper-iment with a small grammar, focusing on the cases that might be problematic: we extracted from the corpus a random sample of 100 word forms con-taining prefixes."
"After making sure that the re-quired affixes and stems were included in the lexicon of the grammar, we ran a comparison of exact pars-ing with the unification-based grammar and the se-lective history-based RTN-approximation, with pa-rameter # $ &amp; (which means that there is a history window of one item)."
The approximation does not lose any test items parsed by the full grammar.
"Some obvi-ous improvements should make it possible soon to run experiments with a larger history window, reach-ing exactness of the finite-state method for almost all relevant data."
"I’d like to thank my former colleagues at the Institut für Maschinelle Sprachverarbeitung at the Univer-sity of Stuttgart for invaluable discussion and input: Arne Fitschen, Anke Lüdeling, Bettina Säuberlich and the other people working in the DeKo project and the IMS lexicon group."
I’d also like to thank Christian Rohrer and Helmut Schmid for discussion and support.
This paper presents a new bottom-up chart parsing algorithm for Prolog along with a compilation procedure that reduces the amount of copying at run-time to a con-stant number (2) per edge.
"It has ap-plications to unification-based grammars with very large partially ordered cate-gories, in which copying is expensive, and can facilitate the use of more so-phisticated indexing strategies for retriev-ing such categories that may otherwise be overwhelmed by the cost of such copy-ing."
"It also provides a new perspective on “quick-checking” and related heuris-tics, which seems to confirm that forcing an early failure (as opposed to seeking an early guarantee of success) is in fact the best approach to use."
A preliminary empirical evaluation of its performance is also provided.
"This paper addresses the cost of copying edges in memoization-based, all-paths parsers for phrase-structure grammars."
"While there have been great ad-vances in probabilistic parsing methods in the last five years, which find one or a few most probable parses for a string relative to some grammar, all-paths parsing is still widely used in grammar devel-opment, and as a means of verifying the accuracy of syntactically more precise grammars, given a corpus or test suite."
Most if not all efficient all-paths phrase-structure-based parsers for natural language are chart-based because of the inherent ambiguity that exists in large-scale natural language grammars.
"Within WAM-based Prolog, memoization can be a fairly costly operation because, in addition to the cost of copying an edge into the memoization table, there is the additional cost of copying an edge out of the table onto the heap in order to be used as a premise in further deductions (phrase structure rule applica-tions)."
"All textbook bottom-up Prolog parsers copy edges out: once for every attempt to match an edge to a daughter category, based on a matching end-point node, which is usually the first-argument on which the memoization predicate is indexed."
"De-pending on the grammar and the empirical distri-bution of matching mother/lexical and daughter de-scriptions, this number could approach  copies for an edge added early to the chart, where is the length of the input to be parsed."
"For classical context-free grammars, the category information that must be copied is normally quite small in size."
"For feature-structure-based grammars and other highly lexicalized grammars with large categories, however, which have become consider-ably more popular since the advent of the standard parsing algorithms, it becomes quite significant."
"The ALE system[REF_CITE]attempts to reduce this by using an algorithm due to Carpen-ter that traverses the string breadth-first, right-to-left, but matches rule daughters rule depth-first, left-to-right in a failure-driven loop, which eliminates the need for active edges and keeps the sizes of the heap and call stack small."
"It still copies a candidate edge every time it tries to match it to a daughter descrip-tion, however, which can approach  because of its lack of active edges."
"The OVIS system (van[REF_CITE]) employs selective memoization, which tabulates only maximal projections in a head-corner parser — partial projections of a head are still re-computed."
"A chart parser with zero copying overhead has yet to be discovered, of course."
"This paper presents one that reduces this worst case to two copies per non-empty edge, regardless of the length of the in-put string or when the edge was added to the chart."
"Since textbook chart parsers require at least two copies per edge as well (assertion and potentially matching the next lexical edge to the left/right), this algorithm always achieves the best-case number of copies attainable by them on non-empty edges."
It is thus of some theoretical interest in that it proves that at least a constant bound is attainable within a Prolog setting.
"It does so by invoking a new kind of gram-mar transformation, called EFD-closure, which en-sures that a grammar need not match an empty cat-egory to the leftmost daughter of any rule."
"This transformation is similar to many of the myriad of earlier transformations proposed for exploring the decidability of recognition under various parsing control strategies, but the property it establishes is more conservative than brute-force epsilon elimi-nation for unification-based grammars[REF_CITE]."
"It also still treats empty categories distinctly from non-empty ones, unlike the linking tables pro-posed for treating leftmost daughters in left-corner parsing[REF_CITE]."
"Its motivation, the practical consideration of copying overhead, is also rather different, of course."
"The algorithm will be presented as an improved version of ALE’s parser, although other standard bottom-up parsers can be similarly adapted."
"This paper is not an attempt to show that a Prolog-based parser could be as fast as a phrase-structure parser implemented in an imperative pro-gramming language such as C. Indeed, if the cat-egories of a grammar are discretely ordered, chart edges can be used for further parsing in situ, i.e., with no copying out of the table, in an impera- tive programming language."
"Nevertheless, when the categories are partially ordered, as in unification-based grammars, there are certain breadth-first pars-ing control strategies that require even imperatively implemented parsers to copy edges out of their ta-bles."
What is more important is the tradeoff at stake between efficiency and expressiveness.
"By improv-ing the performance of Prolog-based parsing, the computational cost of its extra available expres-sive devices is effectively reduced."
"The alterna-tive, simple phrase-structure parsing, or extended phrase-structure-based parsing with categories such as typed feature structures, is extremely cumber-some for large-scale grammar design."
"Even in the handful of instances in which it does seem to have been successful, which includes the recent HPSG English Resource Grammar and a handful of Lexical-Functional Grammars, the results are by no means graceful, not at all modular, and arguably not reusable by anyone except their designers."
"The particular interest in Prolog’s expressiveness arises, of course, from the interest in generalized context-free parsing beginning with definite clause grammars[REF_CITE], as an in-stance of a logic programming control strategy."
"The connection between logic programming and parsing is well-known and has also been a very fruitful one for parsing, particularly with respect to the appli-cation of logic programming transformations[REF_CITE]and constraint logic programming tech-niques to more recent constraint-based grammati-cal theories."
Relational predicates also make gram-mars more modular and readable than pure phrase-structure-based grammars.
Commercial Prolog implementations are quite difficult to beat with imperative implementations when it is general logic programming that is re-quired.
This is no less true with respect to more re-cent data structures in lexicalized grammatical theo-ries.
"A recent comparis[REF_CITE]of a version between ALE (which is written in Prolog) that re-duces typed feature structures to Prolog term encod-ings, and LiLFeS[REF_CITE], the fastest imperative re-implementation of an ALE-like lan-guage, showed that ALE was slightly over 10 times faster on large-scale parses with its HPSG reference grammar than LiLFeS was with a slightly more effi- cient version of that grammar."
"Whether this algorithm will outperform standard Prolog parsers is also largely empirical, because: 1. one of the two copies is kept on the heap itself and not released until the end of the parse."
"For large parses over large data structures, that can increase the size of the heap significantly, and will result in a greater number of cache misses and page swaps. 2. the new algorithm also requires an off-line par-tial evaluation of the grammar rules that in-creases the number of rules that must be it-erated through at run-time during depth-first closure."
"This can result in redundant opera-tions being performed among rules and their partially evaluated instances to match daughter categories, unless those rules and their partial evaluations are folded together with local dis-junctions to share as much compiled code as possible."
A preliminary empirical evaluation is presented in Section 8.
"The results of the present study can only cautiously be compared to theirs so far, because of our lack of access to the suc-cessive stages of their implementations and the lack of a common grammar ported to all of the systems involved."
"Some parallels can be drawn, however, particularly with respect to the utility of indexing and the maintenance of active edges, which suggest that the algorithm presented below makes Prolog be-have in a more “C-like” manner on parsing tasks."
"The principal benefits of this algorithm are that: 1. it reduces copying, as mentioned above. 2. it does not suffer from a problem that text-book algorithms suffer from when running un-der non-ISO-compatible Prologs (which is to say most of them)."
"On such Prologs, asserted empty category edges that can match leftmost daughter descriptions of rules are not able to combine with the outputs of those rules. 3. keeping a copy of the chart on the heap allows for more sophisticated indexing strategies to apply to memoized categories that would oth-erwise be overwhelmed by the cost of copying an edge before matching it against an index."
Indexing is also briefly considered in Section 8.
"In-dexing is not the same thing as filtering[REF_CITE], which extracts an approximation grammar to parse with first, in order to increase the likelihood of early unification failure."
"If the filter parse succeeds, the system then proceeds to perform the entire unification operation, as if the approxima-tion had never been applied."
It is in fact only a greedy approxima-tion — the optimization problem is exponential in the number of feature paths used for the check.
True indexing re-orders required operations with-out repeating them.
"Nei-ther of these is suitable for indexing chart edges dur-ing parsing, because the edges are discarded after every sentence, before the expense of building the index can be satisfactorily amortized."
"There is a fair amount of relevant work in the database and pro-gramming language communities, but many of the results are negative[REF_CITE]— very little time can be spent on constructing the index."
"A moment’s thought reveals that the very notion of an active edge, tabulating the well-formed pre- fixes of rule right-hand-sides, presumes that copy-ing is not a significant enough issue to merit the overhead of more specialized indexing."
"While the present paper proceeds from Carpenter’s algorithm, in which no active edges are used, it will become clear from our evaluation that active edges or their equivalent within a more sophisticated indexing strategy are an issue that should be re-investigated now that the cost of copying can provably be re-duced in Prolog."
"In this section, it will be assumed that the phrase-structure grammar to be parsed with obeys the fol-lowing property:"
"Definition 1 An (extended) context-free grammar, , is empty-first-daughter-closed (EFD-closed) iff, for every production rule,    in ,  and there are no empty productions (empty categories) derivable from non-terminal ."
The next section will show how to transform any phrase-structure grammar into an EFD-closed gram-mar.
"This algorithm, like Carpenter’s algorithm, pro-ceeds breadth-first, right-to-left through the string, at each step applying the grammar rules depth-first, matching daughter categories left-to-right."
"The first step is then to reverse the input string, and compute its length (performed by reverse count/5) and initialize the chart: rec(Ws,FS) :-retractall(edge(_,_,_)), reverse_count(Ws,[],WsRev,0,Length), CLength is Length - 1, functor(Chart,chart,CLength), build(WsRev,Length,Chart), edge(0,Length,FS)."
Two copies of the chart are used in this presentation.
"One is represented by a term chart(E1,...,EL), where the th argument holds the list of edges whose left node is ."
"Edges at the beginning of the chart (left node 0) do not need to be stored in this copy, nor do edges beginning at the end of the chart (specifically, empty categories with left node and right node Length)."
This will be called the term copy of the chart.
"The other copy is kept in a dynamic predicate, edge/3, as a text-book Prolog chart parser would."
This will be called the asserted copy of the chart.
Neither copy of the chart stores empty categories.
"These are assumed to be available in a separate pred-icate, empty cat/1."
"Since the grammar is EFD-closed, no grammar rule can produce a new empty category."
Lexical items are assumed to be available in the predicate lex/2.
"The predicate, build/3, actually builds the chart:"
"The precondition upon each call to build(Ws,R,Chart) is that Chart con-tains the complete term copy of the non-loop edges of the parsing chart from node R to the end, while Ws contains the (reversed) input string from node R to the beginning."
"Each pass through the first clause of build/3 then decrements Right, and seeds the chart with every category for the lexical item that spans from R-1 to R. The predicate, add edge/4 actually adds the lexical edge to the asserted copy of the chart, and then closes the chart depth-first under rule applications in a failure-driven loop."
"When it has finished, if Ws is not empty (RMinus1 is not 0), then build/3 retracts all of the new edges from the asserted copy of the chart (with rebuild edges/2, described below) and adds them to the R-1st argument of the term copy before continuing to the next word. add edge/4 matches non-leftmost daughter de-scriptions from either the term copy of the chart, thus eliminating the need for additional copying of non-empty edges, or from empty cat/1."
"When-ever it adds an edge, however, it adds it to the as-serted copy of the chart."
"This is necessary because add edge/4 works in a failure-driven loop, and any edges added to the term copy of the chart would be removed during backtracking: add_edge(Left,Right,FS,Chart):-assert(edge(Left,Right,FS)), rule(FS,Left,Right,Chart). rule(FS,L,R,Chart) :- (Mother ===&gt; [FS|DtrsRest]), % PS rule match_rest(DtrsRest,R,Chart,Mother,L). match_rest([],R,Chart,Mother,L) :- % all Dtrs matched add_edge(L,R,Mother,Chart). match_rest([Dtr|Dtrs],R,Chart,Mother,L) :-arg(R,Chart,Edges), member(edge(Dtr,NewR),Edges), match_rest(Dtrs,NewR,Chart,Mother,L) ; empty_cat(Dtr), match_rest(Dtrs,R,Chart,Mother,L)."
Note that we never need to be concerned with up-dating the term copy of the chart during the opera-tion of add edge/4 because EFD-closure guaran-tees that all non-leftmost daughters must have left nodes strictly greater than the Left passed as the first argument to add edge/4.
"Moving new edges from the asserted copy to the term copy is straightforwardly achieved by re-build edges/2: rebuild_edges(Left,Edges) :-retract(edge(Left,R,FS)) -&gt; Edges = [edge(FS,R)|EdgesRest], rebuild_edges(Left,EdgesRest) ; Edges = []."
"The two copies required by this algorithm are thus: 1) copying a new edge to the asserted copy of the chart by add edge/4, and 2) copying new edges from the asserted copy of the chart to the term copy of the chart by rebuild edges/2."
The as-serted copy is only being used to protect the term copy from being unwound by backtracking.
"Asymptotically, this parsing algorithm has the same cubic complexity as standard chart parsers — only its memory consumption and copying behavior are different."
"To convert an (extended) context-free grammar to one in which EFD-closure holds, we must partially evaluate those rules for which empty categories could be the first daughter over the available empty categories."
"If all daughters can be empty categories in some rule, then that rule may create new empty categories, over which rules must be partially evalu-ated again, and so on."
The closure algorithm is pre-sented in Figure 1 in pseudo-code and assumes the existence of six auxiliary lists:
"Es — a list of empty categories over which par-tial evaluation is to occur,"
"Rs — a list of rules to be used in partial evalu-ation,"
"NEs — new empty categories, created by partial evaluation (when all daughters have matched empty categories),"
"NRs — new rules, created by partial evaluation (consisting of a rule to the leftmost daughter of which an empty category has applied, with only its non-leftmost daughters remaining),"
"EAs — an accumulator of empty categories al-ready partially evaluated once on Rs, and"
RAs — an accumulator of rules already used in partial evaluation once on Es.
Initialize Es to empty cats of grammar; initialize Rs to rules of input grammar; initialize the other four lists to []; loop:
Each pass through the while-loop attempts to match the empty categories in Es against the left-most daughter description of every rule in Rs.
"If new empty categories are created in the process (because some rule in Rs is unary and its daugh-ter matches), they are also attempted — EAs holds the others until they are done."
"Every time a rule’s leftmost daughter matches an empty category, this effectively creates a new rule consisting only of the non-leftmost daughters of the old rule."
"In a unification-based setting, these non-leftmost daugh-ters could also have some of their variables instan-tiated to information from the matching empty cate-gory."
"If the while-loop terminates (see the next section), then the rules of Rs are stored in an accumulator,"
"RAs, until the new rules, NRs, have had a chance to match their leftmost daughters against all of the empty categories that Rs has."
Partial evaluation with NRs may create new empty categories that Rs have never seen and therefore must be applied to.
This is taken care of within the while-loop when RAs are added back to Rs for second and subsequent passes through the loop.
The parsing algorithm itself always terminates be-cause the leftmost daughter always consumes input.
Off-line EFD-closure may not terminate when in-finitely many new empty categories can be produced by the production rules.
"We say that an extended context-free grammar, by which classical CFGs as well as unification-based phrase-structure grammars are implied, is -offline-parseable ( -OP) iff the empty string is not infinitely ambiguous in the grammar."
Every -OP grammar can be converted to a weakly equivalent grammar which has the EFD-closure property.
"The proof of this statement, which establishes the correctness of the algorithm, is omitted for brevity."
"EFD-closure bears some resemblance in its inten-tions to Greibach Normal Form, but: (1) it is far more conservative in the number of extra rules it must create; (2) it is linked directly to the deriv-able empty categories of the grammar, whereas GNF conversion proceeds from an already -eliminated grammar (EFD-closure of any -free grammar, in fact, is the grammar itself); (3) GNF is rather more difficult to define in the case of unification-based grammars than with classical CFGs, and in the one generalization we are aware[REF_CITE], EFD-closure is actually not guaranteed by it; and Dymetman’s generalization only works for classi-cally offline-parseable grammars."
"In the case of non- -OP grammars, a standard bottom-up parser without EFD-closure would not terminate at run-time either."
Our new algorithm is thus neither better nor worse than a textbook bottom-up parser with respect to termination.
A remain-ing topic for consideration is the adaptation of this method to strategies with better termination proper-ties than the pure bottom-up strategy.
"The details of how to integrate an indexing strategy for unification-based grammars into the EFD-based parsing algorithm are too numerous to present here, but a few empirical observations can be made."
"First, EFD-based parsing is faster than Carpenter’s algo-rithm even with atomic, CFG-like categories, where the cost of copying is at a minimum, even with no in-dexing."
"We defined several sizes of CFG by extract-ing local trees from successively increasing portions of the Penn Treebank II, as shown in Table 1, and then computed the average time to parse a corpus of sentences (5 times each) drawn from the initial sec-tion."
All of the parsers were written in SICStus Pro-log.
These average times are shown in Figure 2 as a function of the number of rules.
"Storing active edges is always the worst option, followed by Carpenter’s algorithm, followed by the EFD-based algorithm."
"In this atomic case, indexing simply takes on the form of a hash by phrase structure category."
This can be implemented on top of EFD because the overhead of copying has been reduced.
This fourth option is the fastest by a factor of approximately 2.18 on average over EFD without indexing.
"One may also refer to Table 2, in which the num- ber of successful and failed unifications (matches) was counted over the test suite for each rule set."
"Asymptotically, the success rate does not decrease by very much from rule set to rule set."
"There are so many more failures early on, however, that the sheer quantity of failed unifications makes it more impor-tant to dispense with these quickly."
"Of the grammars to which we have access that use larger categories, this ranking of parsing algorithms is generally preserved, although we have found no correlation between category size and the factor of improvement."
"John Carroll’s Prolog port of the Alvey grammar of English (Figure 3), for example, is EFD-closed, but the improvement of EFD over Carpenter’s algorithm is much smaller, presumably because there are so few edges when compared to the CFGs extracted from the Penn Treebank."
EFD-index is also slower than EFD without indexing be-cause of our poor choice of index for that gram-mar.
"With subsumption testing (Figure 4), the ac-tive edge algorithm and Carpenter’s algorithm are at an even greater disadvantage because edges must be copied to be compared for subsumption."
"On a pre-release version of MERGE (Figure 5), [Footnote_1] a modi-fication of the English Resource Grammar that uses more macros and fewer types, the sheer size of the categories combined with a scarcity of edges seems to cost EFD due to the loss of locality of reference, although that loss is more than compensated for by indexing."
"1 We are indebted to Kordula DeKuthy and Detmar Meurers of Ohio State University, for making this pre-release version available to us."
This paper has presented a bottom-up parsing algo-rithm for Prolog that reduces the copying of edges from either linear or quadratic to a constant num-ber of two per non-empty edge.
"Its termination properties and asymptotic complexity are the same as a standard bottom-up chart parser, but in prac-tice it performs better."
"Further optimizations can be incorporated by compiling rules in a way that lo-calizes the disjunctions that are implicit in the cre-ation of extra rules in the compile-time EFD-closure step, and by integrating automaton- or decision-tree-based indexing with this algorithm."
"With copying now being unnecessary for matching a daughter cat-egory description, these two areas should result in a substantial improvement to parse times for highly lexicalized grammars."
"The adaptation of this algo-rithm to active edges, other control strategies, and to scheduling concerns such as finding the first parse as quickly as possible remain interesting areas of fur-ther extension."
"Apart from this empirical issue, this algorithm is of theoretical interest in that it proves that a con-stant number of edge copies can be attained by an all-paths parser, even in the presence of partially or-dered categories."
"Many applications of natural language processing technologies involve analyzing texts that concern the psychological states and processes of people, including their beliefs, goals, predictions, explanations, and plans."
"In this paper, we describe our efforts to create a robust, large-scale lexi-cal-semantic resource for the recognition and classification of expressions of com-monsense psychology in English Text."
"We achieve high levels of precision and recall by hand-authoring sets of local grammars for commonsense psychology concepts, and show that this approach can achieve classification performance greater than that obtained by using machine learning techniques."
We demonstrate the utility of this resource for large-scale cor-pus analysis by identifying references to adversarial and competitive goals in po-litical speeches throughout U.S. history.
"Across all text genres it is common to find words and phrases that refer to the mental states of people (their beliefs, goals, plans, emotions, etc.) and their mental processes (remembering, imagining, priori-tizing, problem solving)."
These mental states and processes are among the broad range of concepts that people reason about every day as part of their commonsense understanding of human psychol-ogy.
"Commonsense psychology has been studied in many fields, sometimes using the terms Folk psychology or Theory of Mind, as both a set of be-liefs that people have about the mind and as a set of everyday reasoning abilities."
"Within the field of computational linguistics, the study of commonsense psychology has not re-ceived special attention, and is generally viewed as just one of the many conceptual areas that must be addressed in building large-scale lexical-semantic resources for language processing."
"Although there have been a number of projects that have included concepts of commonsense psychology as part of a larger lexical-semantic resource, e.g. the Berkeley FrameNet Project[REF_CITE], none have attempted to achieve a high degree of breadth or depth over the sorts of expressions that people use to refer to mental states and processes."
"The lack of a large-scale resource for the analy-sis of language for commonsense psychological concepts is seen as a barrier to the development of a range of potential computer applications that in-volve text analysis, including the following: • Natural language interfaces to mixed-initiative planning systems (Ferguson &amp;[REF_CITE]) require the ability to map ex-pressions of users’ beliefs, goals, and plans (among other commonsense psychology con-cepts) onto formalizations that can be ma-nipulated by automated planning algorithms. • Automated question answering systems (Voorhees &amp;[REF_CITE]) require the abil-ity to tag and index text corpora with the rele-vant commonsense psychology concepts in order to handle questions concerning the be-liefs, expectations, and intentions of people. • Research efforts within the field of psychology that employ automated corpus analysis tech-niques to investigate developmental and men-tal illness impacts on language production, e.g. Reboul &amp; Sabatier’s (2001) study of the dis-course of schizophrenic patients, require the ability to identify all references to certain psy-chological concepts in order to draw statistical comparisons."
"In order to enable future applications, we un-dertook a new effort to meet this need for a lin-guistic resource."
This paper describes our efforts in building a large-scale lexical-semantic resource for automated processing of natural language text about mental states and processes.
"Our aim was to build a system that would analyze natural language text and recognize, with high precision and recall, every expression therein related to commonsense psychology, even in the face of an extremely broad range of surface forms."
Each recognized expres-sion would be tagged with an appropriate concept from a broad set of those that participate in our commonsense psychological theories.
Section 2 demonstrates the utility of a lexical-semantic resource of commonsense psychology in automated corpus analysis through a study of the changes in mental state expressions over the course of over 200 years of U.S. Presidential State-of-the- Union Addresses.
"Section 3 of this paper describes the methodology that we followed to create this resource, which involved the hand authoring of local grammars on a large scale."
Section 4 de-scribes a set of evaluations to determine the per-formance levels that these local grammars could achieve and to compare these levels to those of machine learning approaches.
Section 5 concludes this paper with a discussion of the relative merits of this approach to the creation of lexical-semantic resources as compared to other approaches.
One of the primary applications of a lexical-semantic resource for commonsense psychology is toward the automated analysis of large text cor-pora.
"The research value of identifying common-sense psychology expressions has been demonstrated in work on children’s language use, where researchers have manually annotated large text corpora consisting of parent/child discourse transcripts (Barsch &amp;[REF_CITE]) and chil-dren’s storybooks[REF_CITE]."
"While these previous studies have yielded interesting results, they required enormous amounts of human effort to manually annotate texts."
"In this section we aim to show how a lexical-semantic resource for com-monsense psychology can be used to automate this annotation task, with an example not from the do- main of children’s language acquisition, but rather political discourse."
We conducted a study to determine how politi-cal speeches have been tailored over the course of U.S. history throughout changing climates of mili-tary action.
"Specifically, we wondered if politi-cians were more likely to talk about goals having to do with conflict, competition, and aggression during wartime than in peacetime."
"In order to automatically recognize references to goals of this sort in text, we used a set of local grammars authored using the methodology described in Sec-tion 3 of this paper."
The corpus we selected to ap-ply these concept recognizers was the U.S. State of the Uni[REF_CITE]to 2003.
The rea-sons for choosing this particular text corpus were its uniform distribution over time and its easy availability in electronic form from Project Guten-berg[URL_CITE]net).
"Our set of local gram-mars identified 4290 references to these goals in this text corpus, the vast majority of them begin references to goals of an adversarial nature (rather than competitive)."
"Examples of the references that were identified include the following: • They sought to use the rights and privileges they had obtained in the United Nations, to frustrate its purposes [adversarial-goal] and cut down its powers as an effective agent of world progress.[REF_CITE]• The nearer we come to vanquishing [adver-sarial-goal] our enemies the more we inevita-bly become conscious of differences among the victors.[REF_CITE]• Men have vied [competitive-goal] with each other to do their part and do it well.[REF_CITE]• I will submit to Congress comprehensive leg-islation to strengthen our hand in combating [adversarial-goal] terrorists.[REF_CITE]"
Figure 1 summarizes the results of applying our local grammars for adversarial and competitive goals to the U.S. State of the Union Addresses.
"For each year, the value that is plotted represents the number of references to these concepts that were identified per 100 words in the address."
The inter-esting result of this analysis is that references to adversarial and competitive goals in this corpus increase in frequency in a pattern that directly cor-responds to the major military conflicts that the U.S. has participated in throughout its history.
Each numbered peak in Figure 1 corresponds to a period in which the U.S. was involved in a mili-tary conflict.
"These are: 1) 1813,[REF_CITE]US and Britain; 2) 1847, Mexican American War; 3) 1864, Civil War; 4) 1898, Spanish American War; 5) 1917, World War I; 6) 1943, World War II; 7) 1952, Korean War; 8) 1966, Vietnam War; 9) 1991, Gulf War; 10) 2002, War on Terrorism."
The wide applicability of a lexical-semantic re-source for commonsense psychology will require that the identified concepts are well defined and are of broad enough scope to be relevant to a wide range of tasks.
"Additionally, such a resource must achieve high levels of accuracy in identifying these concepts in natural language text."
The remainder of this paper describes our efforts in authoring and evaluating such a resource.
The first challenge in building any lexical-semantic resource is to identify the concepts that are to be recognized in text and used as tags for indexing or markup.
"For expressions of commonsense psy-chology, these concepts must describe the broad scope of people’s mental states and processes."
An ontology of commonsense psychology with a high degree of both breadth and depth is described[REF_CITE].
"In this work, 635 commonsense psychology concepts were identified through an analysis of the representational requirements of a corpus of 372 planning strategies collected from 10 real-world planning domains."
"These concepts were grouped into 30 conceptual areas, corresponding to various reasoning functions, and full formal mod- els of each of these conceptual areas are being authored to support automated inference about commonsense psychology (Gordon &amp;[REF_CITE])."
We adopted this conceptual framework in our current project because of the broad scope of the concepts in this ontology and its potential for future integration into computational reasoning systems.
"The full list of the 30 concept areas identified is as follows: 1) Managing knowledge, 2) Similarity comparison, 3) Memory retrieval, 4) Emotions, 5) Explanations, 6) World envisionment, 7) Execu-tion envisionment, 8) Causes of failure, 9) Man-aging expectations, 10) Other agent reasoning, 11)[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])[REF_CITE])"
"Ob-servation of execution, and 30) Body interaction."
"Our aim for this lexical-semantic resource was to develop a system that could automatically iden-tify every expression of commonsense psychology in English text, and assign to them a tag corre-sponding to one of the 635 concepts in this ontol-ogy."
"For example, the following passage (from William Makepeace Thackeray’s 1848 novel, Vanity Fair) illustrates the format of the output of this system, where references to commonsense psychology concepts are underlined and followed by a tag indicating their specific concept type de-limited by square brackets:"
"Perhaps [partially-justified-proposition] she had mentioned the fact [proposition] already to Rebecca, but that young lady did not appear to [partially-justified-proposition] have remem-bered it [memory-retrieval]; indeed, vowed and protested that she expected [add-expectation] to see a number of Amelia&apos;s nephews and nieces."
She was quite disappointed [disappointment-emotion] that Mr. Sedley was not married; she was sure [justified-proposition]
"Amelia had said he was, and she doted so on [liking-emotion] lit-tle children."
The approach that we took was to author (by hand) a set of local grammars that could be used to identify each concept.
For this task we utilized the Intex Corpus Processor software developed by the Laboratoire d&apos;Automatique Documentaire et Lin-guistique (LADL) of the University of Paris 7[REF_CITE].
"This software allowed us to author a set of local grammars using a graphical user in-terface, producing lexical/syntactic structures that can be compiled into finite-state transducers."
"To simplify the authoring of these local grammars, Intex includes a large-coverage English dictionary compiled by Blandine Courtois, allowing us to specify them at a level that generalized over noun and verb forms."
"For example, there are a variety of ways of expressing in English the concept of reaf-firming a belief that is already held, as exemplified in the following sentences: 1) The finding was confirmed by the new data. 2) She told the truth, corroborating his story. 3) He reaffirms his love for her. 4) We need to verify the claim. 5) Make sure it is true."
"Although the verbs in these sentences differ in tense, the dictionaries in Intex allowed us to recog-nize each using the following simple description: (&lt;confirm&gt; by | &lt;corroborate&gt; | &lt;reaffirm&gt; | &lt;verify&gt; | &lt;make&gt; sure)"
"While constructing local grammars for each of the concepts in the original ontology of common-sense psychology, we identified several conceptual distinctions that were made in language that were not expressed in the specific concepts that Gordon had identified."
"For example, the original ontology included only three concepts in the conceptual area of memory retrieval (the sparsest of the 30 areas), namely memory, memory cue, and memory re-trieval."
"English expressions such as “to forget” and “repressed memory” could not be easily mapped directly to one of these three concepts, which prompted us to elaborate the original sets of con-cepts to accommodate these and other distinctions made in language."
"In the case of the conceptual area of memory retrieval, a total of twelve unique concepts were necessary to achieve coverage over the distinctions evident in English."
These local grammars were authored one con-ceptual area at a time.
"At the time of the writing of this paper, our group had completed 6 of the origi-nal 30 commonsense psychology conceptual areas."
"The remainder of this paper focuses on the first 4 of the 6 areas that were completed, which were evaluated to determine the recall and precision per-formance of our hand-authored rules."
"These four areas are Managing knowledge, Memory, Expla-nations, and Similarity judgments."
Figure 2 pre-sents each of these four areas with a single fabricated example of an English expression for each of the final set of concepts.
"Local grammars for the two additional conceptual areas, Goals (20 concepts) and Goal management (17 concepts), were authored using the same approach as the oth-ers, but were not completed in time to be included in our performance evaluation."
"After authoring these local grammars using the Intex Corpus Processor, finite-state transducers were compiled for each commonsense psychology concept in each of the different conceptual areas."
"To simplify the application of these transducers to text corpora and to aid in their evaluation, trans-ducers for individual concepts were combined into a single finite state machine (one for each concep-tual area)."
"By examining the number of states and transitions in the compiled finite state graphs, some indication of their relative size can be given for the four conceptual areas that we evaluated: Managing knowledge (348 states / 932 transitions), Memory (203 / 725), Explanations (208 / 530), and Similar-ity judgments (121 / 500)."
"In order to evaluate the utility of our set of hand-authored local grammars, we conducted a study of their precision and recall performance."
"In order to calculate the performance levels, it was first neces-sary to create a test corpus that contained refer-ences to the sorts of commonsense psychological concepts that our rules were designed to recognize."
"To accomplish this, we administered a survey to collect novel sentences that could be used for this purpose."
This survey was administered over the course of one day to anonymous adult volunteers who stopped by a table that we had set up on our uni-versity’s campus.
"We instructed the survey taker to author 3 sentences that included words or phrases related to a given concept, and 3 sentences that they felt did not contain any such references."
"Each survey taker was asked to generate these 6 sen-tences for each of the 4 concept areas that we were evaluating, described on the survey in the follow-ing manner: • Managing knowledge: Anything about the knowledge, assumptions, or beliefs that people have in their mind • Memory: When people remember things, for-get things, or are reminded of things • Explanations: When people come up with pos-sible explanations for unknown causes • Similarity judgments: When people find simi-larities or differences in things"
"A total of 99 people volunteered to take our survey, resulting in a corpus of 297 positive and 297 negative sentences for each conceptual area, with a few exceptions due to incomplete surveys."
"Using this survey data, we calculated the preci-sion and recall performance of our hand-authored local grammars."
Every sentence that had at least one concept detected for the corresponding concept area was treated as a “hit”.
Table 1 presents the precision and recall performance for each concept area.
"The results show that the precision of our system is very high, with marginal recall perform-ance."
The low recall scores raised a concern over the quality of our test data.
"In reviewing the sentences that were collected, it was apparent that some sur-vey participants were not able to complete the task as we had specified."
"To improve the validity of the test data, we enlisted six volunteers (native English speakers not members of our development team) to judge whether or not each sentence in the corpus was produced according to the instructions."
"The corpus of sentences was divided evenly among these six raters, and each sentence that the rater judged as not satisfying the instructions was fil-tered from the data set."
"In addition, each rater also judged half of the sentences given to a different rater in order to compute the degree of inter-rater agreement for this filtering task."
"After filtering sentences from the corpus, a second preci-sion/recall evaluation was performed."
"Table 2 pre-sents the results of our hand-authored local grammars on the filtered data set, and lists the in-ter-rater agreement for each conceptual area among our six raters."
"The results show that the system achieves a high level of precision, and the recall performance is much better than earlier indicated."
The performance of our hand-authored local grammars was then compared to the performance that could be obtained using more traditional ma-chine-learning approaches.
"In these comparisons, the recognition of commonsense psychology con-cepts was treated as a classification problem, where the task was to distinguish between positive and negative sentences for any given concept area."
"Sentences in the filtered data sets were used as training instances, and feature vectors for each sentence were composed of word-level unigram and bi-gram features, using no stop-lists and by ignoring punctuation and case."
"By using a toolkit of machine learning algorithms (Witten &amp;[REF_CITE]), we were able to compare the performance of a wide range of different techniques, including Naïve Bayes, C4.5 rule induction, and Support Vector Machines, through stratified cross-validation (10-fold) of the training data."
The high-est performance levels were achieved using a se-quential minimal optimization algorithm for training a support vector classifier using polyno-mial kernels[REF_CITE].
These performance re-sults are presented in Table 3.
The percentage correctness of classification (Pa) of our hand-authored local grammars (column A) was higher than could be attained using this machine-learning approach (column B) in three out of the four con-cept areas.
We then conducted an additional study to de-termine if the two approaches (hand-authored local grammars and machine learning) could be com-plimentary.
The concepts that are recognized by our hand-authored rules could be conceived as ad-ditional bimodal features for use in machine learning algorithms.
We constructed an additional set of support vector machine classifiers trained on the filtered data set that included these additional concept-level features in the feature vector of each instance along side the existing unigram and bi-gram features.
"Performance of these enhanced classifiers, also obtained through stratified cross-validation (10-fold), are also reported in Table 3 as well (column C)."
The results show that these en-hanced classifiers perform at a level that is the greater of that of each independent approach.
The most significant challenge facing developers of large-scale lexical-semantic resources is coming to some agreement on the way that natural lan-guage can be mapped onto specific concepts.
This challenge is particularly evident in consideration of our survey data and subsequent filtering.
The abilities that people have in producing and recog-nizing sentences containing related words or phrases differed significantly across concept areas.
"While raters could agree on what constitutes a sentence containing an expression about memory (Kappa=.8069), the agreement on expressions of managing knowledge is much lower than we would hope for (Kappa=.5636)."
"We would expect much greater inter-rater agreement if we had trained our six raters for the filtering task, that is, described exactly which concepts we were looking for and gave them examples of how these concepts can be realized in English text."
"However, this ap-proach would have invalidated our performance results on the filtered data set, as the task of the raters would be biased toward identifying exam-ples that our system would likely perform well on rather than identifying references to concepts of commonsense psychology."
Our inter-rater agreement concern is indicative of a larger problem in the construction of large-scale lexical-semantic resources.
"The deeper we delve into the meaning of natural language, the less we are likely to find strong agreement among un-trained people concerning the particular concepts that are expressed in any given text."
"Even with lexical-semantic resources about commonsense knowledge (e.g. commonsense psychology), finer distinctions in meaning will require the efforts of trained knowledge engineers to successfully map between language and concepts."
"While this will certainly create a problem for future preci- sion/recall performance evaluations , the concern is even more serious for other methodologies that rely on large amounts of hand-tagged text data to create the recognition rules in the first place."
"We expect that this problem will become more evident as projects using algorithms to induce local gram-mars from manually-tagged corpora, such as the Berkeley FrameNet efforts[REF_CITE], broaden and deepen their encodings in conceptual areas that are more abstract (e.g. commonsense psychology)."
The approach that we have taken in our re-search does not offer a solution to the growing problem of evaluating lexical-semantic resources.
"However, by hand-authoring local grammars for specific concepts rather than inducing them from tagged text, we have demonstrated a successful methodology for creating lexical-semantic re-sources with a high degree of conceptual breadth and depth."
By employing linguistic and knowledge engineering skills in a combined manner we have been able to make strong ontological commitments about the meaning of an important portion of the English language.
"We have demonstrated that the precision and recall performance of this approach is high, achieving classification performance greater than that of standard machine-learning techniques."
"Furthermore, we have shown that hand-authored local grammars can be used to identify concepts that can be easily combined with word-level features (e.g. unigrams, bi-grams) for integration into statistical natural language proc-essing systems."
"Our early exploration of the appli-cation of this work for corpus analysis (U.S. State of the Union Addresses) has produced interesting results, and we expect that the continued develop-ment of this resource will be important to the suc-cess of future corpus analysis and human-computer interaction projects."
"In this paper, we present a learning ap-proach to the scenario template task of information extraction, where information filling one template could come from mul-tiple sentences."
"When tested on the MUC-4 task, our learning approach achieves accuracy competitive to the best of the MUC-4 systems, which were all built with manually engineered rules."
Our analy-sis reveals that our use of full parsing and state-of-the-art learning algorithms have contributed to the good performance.
"To our knowledge, this is the first re-search to have demonstrated that a learn-ing approach to the full-scale informa-tion extraction task could achieve per-formance rivaling that of the knowledge-engineering approach."
"The explosive growth of online texts written in natu-ral language has prompted much research into infor-mation extraction (IE), the task of automatically ex-tracting specific information items of interest from natural language texts."
"The extracted information is used to fill database records, also known as tem-plates in the IE literature."
Research efforts on IE tackle a variety of tasks.
"They include extracting information from semi-structured texts, such as seminar announcements, rental and job advertisements, etc., as well as from free texts, such as newspaper articles[REF_CITE]."
"IE from semi-structured texts is easier than from free texts, since the layout and format of a semi-structured text provide additional useful clues"
OFFICIALS SAID THAT SHINING PATH MEMBERS WERE RESPONSIBLE FOR THE ATTACK ... ...
POLICE SOURCES STATED THAT THE BOMB ATTACK INVOLVING THE SHINING PATH CAUSED SERIOUS DAMAGES ... ... to aid in extraction.
Several benchmark data sets have been used to evaluate IE approaches on semi-structured texts[REF_CITE].
"For the task of extracting information from free texts, a series of Message Understanding Confer-ences (MUC) provided benchmark data sets for eval-uation."
Several subtasks for IE from free texts have been identified.
"The named entity (NE) task extracts person names, organization names, location names, etc."
"The template element (TE) task extracts infor-mation centered around an entity, like the acronym, category, and location of a company."
The template relation (TR) task extracts relations between enti-ties.
"Finally, the full-scale IE task, the scenario tem-plate (ST) task, deals with extracting generic infor-mation items from free texts."
"To tackle the full ST task, an IE system needs to merge information from multiple sentences in general, since the information needed to fill one template can come from multiple sentences, and thus discourse processing is needed."
The full-scale ST task is considerably harder than all the other IE tasks or subtasks outlined above.
"As is the case with many other natural language processing (NLP) tasks, there are two main ap-proaches to IE, namely the knowledge-engineering approach and the learning approach."
"Most early IE systems adopted the knowledge-engineering ap- proach, where manually engineered rules were used for IE."
"More recently, machine learning approaches have been used for IE from semi-structured texts[REF_CITE], named entity extracti[REF_CITE], template element extraction, and template relation extracti[REF_CITE]."
"These ma-chine learning approaches have been successful for these tasks, achieving accuracy comparable to the knowledge-engineering approach."
"However, for the full-scale ST task of generic IE from free texts, the best reported method to date is still the knowledge-engineering approach."
"For ex-ample, almost all participating IE systems in MUC used the knowledge-engineering approach for the full-scale ST task."
The one notable exception is the work of UMass at MUC-6[REF_CITE].
"Unfortunately, their learning approach did consider-ably worse than the best MUC-6 systems."
"In this paper, we present a learning approach to the full-scale ST task of extracting information from free texts."
The task we tackle is considerably more complex than that of ([REF_CITE];
We evaluated our learning approach on the MUC-4 task of extracting terrorist events from free texts.
"We chose the MUC-4 task since man-ually prepared templates required for training are available. 1 When trained and tested on the official benchmark data of MUC-4, our learning approach achieves accuracy competitive with the best MUC-4 systems, which were all built using manually engi-neered rules."
"To our knowledge, our work is the first learning-based approach to have achieved perfor-mance competitive with the knowledge-engineering approach on the full-scale ST task."
The task addressed in this paper is the Scenario Tem-plate (ST) task defined in the Fourth Message Un-derstanding Conference (MUC-4). [Footnote_2]
"2 The full-scale IE task is called the ST task only in MUC-6 and MUC-7, when other subtasks like NE and TE tasks were defined. Here, we adopted this terminology also in describing the full-scale IE task for MUC-4."
The objective of this task is to extract information on terrorist events occurring in Latin American countries from free text documents.
"For example, given the input document in Figure 1, an IE system is to extract information items related to any terrorist events to fill zero or more database records, or templates."
Each distinct terrorist event is to fill one template.
An example of an output template is shown in Figure 2.
"Each of the 25 fields in the template is called a slot, and the string or value that fills a slot is called a slot fill."
Different slots in the MUC-4 template need to be treated differently.
"Besides slot 0 (MESSAGE: ID) and slot [Footnote_1] (MESSAGE: TEMPLATE), the other 23 slots have to be extracted or inferred from the text document."
1[URL_CITE]muc data/muc data index.html
These slots can be divided into the fol-lowing categories:
"These slots are filled using strings extracted directly from the text document (slot 6, 9, 10, 12, 18, 19)."
Text Conversion Slots.
"These slots have to be inferred from strings in the document (slot 2, 14, 17, 21, 24)."
"For example, INCIDENT: DATE has to be inferred from temporal expressions such as “TO-"
"DAY”, “LAST WEEK”, etc."
Set Fill Slots.
This category includes the rest of the slots.
The value of a set fill slot comes from a finite set of possible values.
They often have to be inferred from the document.
Our supervised learning approach is illustrated in Figure 3.
"Our system, called A LICE (Automated Learning-based Information Content Extraction), requires manually extracted templates paired with their corresponding documents that contain terrorist events for training."
"After the training phase, A LICE is then able to extract relevant templates from new documents, using the model learnt during training."
"In the training phase, each input training docu-ment is first preprocessed through a chain of prepro-cessing modules."
"The outcome of the preprocessing is a full parse tree for each sentence, and corefer-ence chains linking various coreferring noun phrases both within and across sentences."
The core of A L - ICE uses supervised learning to build one classifier for each string slot.
The candidates to fill a template slot are base (non-recursive) noun phrases.
A noun phrase  that occurs in a training document and fills a template slot is used to generate one positive training example for the classifier of slot .
Other noun phrases in the training document are neg-ative training examples for the classifier of slot .
"The features of a training example generated from  are the verbs and other noun phrases (serving roles like agent and patient) related to  in the same sentence, as well as similar features for coreferring noun phrases of  ."
"Thus, our features for a tem-plate slot classifier encode semantic (agent and pa-tient roles) and discourse (coreference) information."
Our experimental results in this paper demonstrate that such features are effective in learning what to fill a template slot.
"During testing, a new document is preprocessed through the same chain of preprocessing modules."
"Each candidate noun phrase  generates one test example, and it is presented to the classifier of a tem-plate slot to determine whether  fills the slot ."
"A separate template manager decides whether a new template should be created to include slot , or slot should fill the existing template."
All the preprocessing modules of A LICE were built with supervised learning techniques.
"They include sentence segmentati[REF_CITE], part-of-speech tagging[REF_CITE], named en-tity recogniti[REF_CITE], full parsing[REF_CITE], and coreference resoluti[REF_CITE]."
"Each module performs at or near state-of-the-art accuracy, but errors are unavoidable, and later modules in the preprocessing chain have to deal with errors made by the previous modules."
"As mentioned earlier, the features of an example are generated based on a base noun phrase (denoted as baseNP), which is a candidate for filling a template slot."
"While most strings that fill a string slot are base noun phrases, this is not always the case."
"For in-stance, consider the two examples in Figure 4."
"In the first example, “BOMB” should fill the string slot IN-CIDENT: INSTRUMENT ID, while in the second example, “FMLN” should fill the string slot PERP: ORGANIZATION ID."
"However, “BOMB” is itself not a baseNP (the baseNP is “A BOMB EXPLO-SION”)."
Similarly for “FMLN”.
"As such, a string that fills a template slot but is itself not a baseNP (like “BOMB”) is also used to generate a training example, by using its smallest en-compassing noun phrase (like “A BOMB EXPLO- (1) ONE PERSON WAS KILLED TONIGHT"
"AS THE RE-SULT OF A BOMB EXPLOSION IN SAN SALVADOR. (2) FORTUNATELY, NO CASUALTIES WERE REPORTED AS A RESULT OF THIS INCIDENT, FOR WHICH THE FMLN GUERRILLAS ARE BEING HELD RESPONSIBLE. (1) MEMBERS OF THAT SECURITY GROUP ARE COMB-ING THE AREA TO DETERMINE THE FINAL OUTCOME OF THE FIGHTING. (2) A BOMB WAS THROWN AT THE HOUSE OF FRE-DEMO CANDIDATE FOR DEPUTY MIGUEL ANGEL BARTRA BY TERRORISTS."
SION”) to generate the training example features.
"During training, a list of such words is compiled for slots 6 and 10 from the training templates."
"During testing, these words are also used as candidates for generating test examples for slots 6 and 10, in addi-tion to base NPs."
The features of an example are derived from the treebank-style parse tree output by an implementa-tion of Collins&apos; parser[REF_CITE].
"In particular, we traverse the full parse tree to determine the verbs, agents, patients, and indirect objects related to a noun phrase candidate  ."
"While a machine learning approach is used[REF_CITE]to determine general semantic roles, we used a simple rule-based traversal of the parse tree instead, which could also reliably determine the generic agent and patient role of a sentence, and this suffices for our current purpose.  Specifically, the following, forgroupsa givenof featuresnoun phraseare usedcandidate:"
"Verb of Agent NP (VAg) When  is an agent in a sentence, each of its associated verbs is a VAg feature."
"For example, in sentence (1) of Figure 5, if  is MEMBERS, then its VAg features are COMB and DETERMINE."
"Verb of Patient NP (VPa) When  is a patient in a sentence, each of its associated verbs is a VPa feature  is BOMB."
"For example, then its, VPain sentencefeature(is2)THROWof Figure. 5, if Verb-Preposition of NP-in-PP (V-Prep) When  is the NP in a prepositional phrase PP, then this feature is the main verb and the preposition of PP."
"For example, in sentence (2) of Figure 5, if  is HOUSE, its V-Prep feature is THROW-AT."
VPa and related NPs/PPs (VPaRel)
"If  is a patient in a sentence, each of its VPa may have its own agents (Ag) and prepositional phrases (Prep-NP)."
"In this case, the tuples (VPa, Ag) and (VPa, Prep-NP) are used as features."
"For example, in “GUARDS WERE SHOT TO DEATH”, if  is GUARDS, then its VPa SHOOT, and the preposi-tional phrase TO-DEATH form the feature (SHOOT, TO-DEATH)."
VAg and related NPs/PPs (VAgRel)
"This is sim-ilar to VPa above, but for VAg."
V-Prep and related NPs (V-PrepRel)
"When  is the NP in a prepositional phrase PP, then the main verb (V) may have its own agents (Ag) and pa-tients (Pa)."
"In this case, the tuples (Ag, V-Prep) and (V-Prep, Pa) are used as features."
"For example, HOUSE in sentence (2) of Figure 5 will have the fea-tures (TERRORIST, THROW-AT) and (THROW-AT, BOMB)."
Noun-Preposition (N-Prep) This feature aims at capturing information in phrases such as “MUR-DER OF THE PRIESTS”.
"If  is PRIESTS, this feature will be MURDER-OF."
Head Word (H)
The head word of each  is also used as a feature.
"In a parse tree, there is a head word at each tree node."
"In cases where a phrase does not fit into a parse tree node, the last word of the phrase is used as the head word."
This feature is useful as the system has no information of the semantic class of  .
"From the head word, the system can get some clue to help decide if  is a possible candidate for a slot."
"For example, an  with head word PEASANT is more likely to fill the human target slot compared to another  with head word CLASH."
Named Entity Class (NE)
The named entity class of  is used as a feature.
Real Head (RH)
"For a phrase that does not fit into a parse node, the head word feature is taken to be the last word of the phrase."
The real head word of its encompassing parse node is used as another feature.
"For example, in the NP “FMLN GUERRILLAS”, “FMLN” is a positive example for slot 10, with head word “FMLN” and real head “GUERRILLA”."
Coreference features Coreference chains found by our coreference resolution module based on de-cision tree learning are used to determine the noun phrases that corefer with  .
"In particular, we use the two noun phrases  and   , where  (   ) is the noun phrase that corefers with  and immediately precedes (follows)  ."
"If such a pre-ceding (or following) noun phrase   exists, we generate the following features based on  : VAg, VPa, and N-Prep."
"To give an idea of the informative features used in the classifier of a slot, we rank the features used for a slot classifier according to their correlation met-ric values[REF_CITE], where informa-tive features are ranked higher."
Table 1 shows the top-ranking features for a few feature groups and template slots.
"The bracketed number behind each feature indicates the rank of this feature for that slot classifier, ordered by the correlation metric value."
We observed that certain feature groups are more useful for certain slots.
"For example, DIE is the top VAg verb for the human target slot, and is ranked 12 among all features used for the human target slot."
"On the other hand, VAg is so unimportant for the physical target slot that the top VAg verb is due to a preprocessing error that made MONSERRAT a verb."
We evaluated four supervised learning algorithms.
"The maximum entropy (ME) framework is a re-cent learning approach which has been successfully used in various NLP tasks such as sentence segmen-tation, part-of-speech tagging, and parsing[REF_CITE]."
"However, to our knowledge, ours is the first research effort to have applied ME learn-ing to the full-scale ST task."
We used the imple-mentation of maximum entropy modeling from the opennlp.maxent package. [URL_CITE] .
Support Vector Machine (Alice-SVM)
The Support Vector Machine (SVM)[REF_CITE]has been successfully used in many recent applications such as text categorization and handwritten digit recognition.
The learning algorithm finds a hyper-plane that separates the training data with the largest margin.
We used a linear kernel for all our experi-ments.
Naive Bayes (Alice-NB)
The Naive Bayes (NB) algorithm[REF_CITE]assumes the inde-pendence of features given the class and assigns a test example to the class which has the highest pos-terior probability.
Add-one smoothing was used.
Decision Tree (Alice-DT)
The decision tree (DT) algorithm[REF_CITE]partitions training exam-ples using the feature with the highest information gain.
It repeats this process recursively for each par-tition until all examples in each partition belong to one class.
"We used the WEKA package [URL_CITE] for the implemen-tation of SVM, NB, and DT algorithms."
A feature cutoff is used for each algorithm: fea-tures occurring less than times are rejected.
"For all experiments, is set to 3."
"For ME and SVM, no other feature selection is applied."
"For NB and DT, the top 100 features as determined by chi-square are selected."
"While not trying to do a serious compari-son of machine learning algorithms, ME and SVM seem to be able to perform well without feature selection, whereas NB and DT require some form of feature selection in order to perform reasonably well."
"As each sentence is processed, phrases classified as positive for any of the string slots are sent to the Template Manager (TM), which will decide if a new template should be created when it receives a new slot fill."
The system first attempts to attach a date and a lo-cation to each slot fill  .
"Dates and locations are first attached to their syntactically nearest verb, by traversing the parse tree."
"Then, for each string fill  , we search its syntactically nearest verb in the same manner and assign the date and location at-tached to to  ."
"When a new slot fill is found, the Template Man-ager will decide to start a new template if one of the following conditions is true:"
Date The date attached to the current slot fill is different from the date of the current template.
Location The location attached to the current slot fill is not compatible with the location of the current template (one location does not contain the other).
"This is determined by using location lists provided by the MUC-4 conference, which specify whether one location is contained in another."
"An entry in this list has the format of “PLACE-NAME1:PLACE-NAME2”, where PLACE-NAME2 is contained in PLACE-NAME1 (e.g., CUBA: HAVANA (CITY))."
The sentence of the current slot fill contains a seed word for a different incident type.
"A number of seed words are automatically learned for each of the incident types ATTACK, BOMBING, and KIDNAPPING."
They are automatically derived based on the correlation metric value used[REF_CITE].
"For the remaining incident types, there are too few incidents in the training data for seed words to be collected."
The seeds words used are shown in Table 2.
"In the last stage before output, the template content is further enriched in the following manner:"
"Removal of redundant slot fills For each slot in the template, there might be several slot fills refer-ring to the same thing."
"For example, for HUM TGT:"
"DESCRIPTION, the system might have found both “PRIESTS” and “JESUIT PRIESTS”."
A slot fill that is a substring of another slot fill will be removed from the template.
"Effect/Confidence and Type Classifiers are also trained for effect and confidence slots 11, 16, and 23 (ES slots), as well as type slots 7, 13, and 20 (TS slots)."
"ES slots used exactly the same features as string slots, while TS slots used only head words and adjectives as features."
"For such slots, each entry refers to another slot fill."
"For example, slot 23 may contain the entry “DEATH” : “PRIESTS”, where “PRIESTS” fills slot 19."
"During training, each train-ing example is a fill of a reference slot (e.g., for slot 23, the reference slots are slot 18 and 19)."
"During testing, slot fills of reference slots will be classified to determine if they should have an entry in an ES or a TS slot."
Date and Location.
"If the system is unable to fill the DATE or LOCATION slot of a template, it will use as default value the date and country of the city in the dateline of the document."
The remaining slots are filled with default values.
"For example, slot 5 has the default value “ACCOMPLISHED”, and slot 8 “TERROR-IST ACT” (except when the perpetrator contains strings such as “GOVERNMENT”, in which case it will be changed to “STATE-SPONSORED VIO-LENCE”)."
"There are 1,300 training documents, of which 700 are relevant (i.e., have one or more event templates)."
"There are two official test sets, i.e., TST3 and TST4, containing 100 documents each."
"We trained our sys-tem A LICE using the 700 documents with relevant templates, and then tested it on the two official test sets."
The output templates were scored using the scorer provided on the official website.
"The accuracy figures of A LICE (with different learning algorithms) on string slots and all slots are listed in Table 3 and Table 4, respectively."
"Accu-racy is measured in terms of recall (R), precision (P), and F-measure (F)."
We also list in the two tables the accuracy figures of the top 7 (out of a total of 17) systems that participated in MUC-4.
"The accuracy figures in the two tables are obtained by running the official scorer on the output templates of A LICE , and those of the MUC-4 participating systems (available on the official web site)."
The same history file down-loaded from the official web site is uniformly used for scoring the output templates of all systems (the history file contains the arbitration decisions for am-biguous cases).
"We conducted statistical significance test, using the approximate randomization method adopted in MUC-4."
Table 5 shows the systems that are not sig-nificantly different from Alice-ME.
"Our system A LICE -ME, using a learning ap-proach, is able to achieve accuracy competitive to the best of the MUC-4 participating systems, which were all built using manually engineered rules."
"We also observed that ME and SVM, the more recent machine learning algorithms, performed better than DT and NB."
"To illustrate the benefit of full pars-ing, we conducted experiments using a subset of fea-tures, with and without full parsing."
We used ME as the learning algorithm in these experiments.
The re-sults on string slots are summarized in Table 6.
"The baseline system used only two features, head word (H) and named entity class (NE)."
"Next, we added three features, VAg, VPa, and V-Prep."
"Without full parsing, these verbs were obtained based on the im-mediately preceding (or following) verb of a noun phrase, and the voice of the verb."
"With full pars-ing, these verbs were obtained based on traversing the full parse tree."
"The results indicate that verb fea-tures contribute to the performance of the system, even without full parsing."
"With full parsing, verbs can be determined more accurately, leading to better overall performance."
"Although the best MUC-4 participating systems, GE/GE-CMU, still outperform A LICE -ME, it must be noted that for GE, “10 1/2 person months” were spent on MUC-4 using the GE NLTOOLSET , af-ter spending “15 person months” on MUC-3[REF_CITE]."
"With a learning approach, IE systems are more portable across domains."
Not all occurrences of a string in a document that match a slot fill of a template provide good positive training examples.
"For example, in the same docu-ment, there might be the following sentences “THE MNR REPORTS THE KIDNAPPING OF OQUELI COLINDRES...”, followed by “OQUELI COLIN-DRES ARRIVED IN[REF_CITE]JAN-UARY”."
"In this case, only the first occurrence of OQUELI COLINDRES should be used as a positive example for the human target slot."
"However, A LICE does not have access to such information, since the MUC-4 training documents are not annotated (i.e., only templates are provided, but the text strings in a document are not marked)."
"Thus, A LICE currently uses all occurrences of “OQUELI COLINDRES” as positive training examples, which introduces noise in the training data."
We believe that annotating the string occurrences in training documents will pro-vide higher quality training data for the learning ap-proach and hence further improve accuracy.
"Although part-of-speech taggers often boast of accuracy over 95%, the errors they make can be fatal to the parsing of sentences."
"For example, they often tend to confuse “VBN” with “VBD”, which could change the entire parse tree."
"The MUC-4 corpus was provided as uppercase text, and this also has a negative impact on the named entity recognizer and part-of-speech tagger, which both make use of case information."
Learning approaches have been shown to perform on par or even outperform knowledge-engineering approaches in many NLP tasks.
"However, the full-scale scenario template IE task was still dom-inated by knowledge-engineering approaches."
"In this paper, we demonstrate that using both state-of-art learning algorithms and full parsing, learning approaches can rival knowledge-engineering ones, bringing us a step closer to building full-scale IE systems in a domain-independent fashion with state-of-the-art accuracy."
Several approaches have been described for the automatic unsupervised acquisi-tion of patterns for information extraction.
"Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a de-pendency chain."
The effect of these al-ternative models has not been previously studied.
"In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary sub-trees of dependency trees."
We describe a discovery procedure for this model and demonstrate experimentally an improve-ment in recall using Subtree patterns.
Information Extraction (IE) is the process of identi-fying events or actions of interest and their partici-pating entities from a text.
"As the field of IE has de-veloped, the focus of study has moved towards au-tomatic knowledge acquisition for information ex-traction, including domain-specific lexicons[REF_CITE]and extraction pat-terns[REF_CITE]."
"In particular, methods have recently emerged for the acquisition of event extraction pat-terns without corpus annotation in view of the cost of manual labor for annotation."
"However, there has been little study of alternative representation models of extraction patterns for unsupervised acquisition."
"In the prior work on extraction pattern acquisition, the representation model of the patterns was based on a fixed set of pattern templates[REF_CITE], or predicate-argument relations, such as subject-verb, and object-verb[REF_CITE]."
The model of our previous work[REF_CITE]was based on the paths from predicate nodes in dependency trees.
"In this paper, we discuss the limitations of prior extraction pattern representation models in relation to their ability to capture the participating entities in scenarios."
"We present an alternative model based on subtrees of dependency trees, so as to extract enti-ties beyond direct predicate-argument relations."
An evaluation on scenario-template tasks shows that the proposed Subtree model outperforms the previous models.
Section 2 describes the Subtree model for extrac-tion pattern representation.
Section 3 shows the method for automatic acquisition.
Section 4 gives the experimental results of the comparison to other methods and Section 5 presents an analysis of these results.
"Finally, Section 6 provides some concluding remarks and perspective on future research."
Our research on improved representation models for extraction patterns is motivated by the limitations of the prior extraction pattern representations.
"In this section, we review two of the previous models in detail, namely the Predicate-Argument model[REF_CITE]and the Chain model[REF_CITE]."
"The main cause of difficulty in finding entities by extraction patterns is the fact that the participating entities can appear not only as an argument of the predicate that describes the event type, but also in other places within the sentence or in the prior text."
"In the MUC-3 terrorism scenario, WEAPON entities occur in many different relations to event predicates in the documents."
"Even if WEAPON entities appear in the same sentence with the event predicate, they rarely serve as a direct argument of such predicates. (e.g., “One person was killed as the result of a bomb explosion.”)"
The Predicate-Argument model is based on a direct syntactic rela-tion between a predicate and its arguments [Footnote_1][REF_CITE].
"1 Since the case marking for a nominalized predicate is sig-nificantly different from the verbal predicate, which makes it hard to regularize the nominalized predicates automatically, the constraint for the Predicate-Argument model requires the root node to be a verbal predicate."
"In general, a predicate provides a strong context for its arguments, which leads to good accuracy."
"However, this model has two major limitations in terms of its coverage, clausal bound-aries and embedded entities inside a predicate’s arguments."
"Figure 1 [Footnote_2] shows an example of an extraction task in the terrorism domain where the event template consists of perpetrator, date, location and victim."
"2 Throughout this paper, extraction patterns are defined as one or more word classes with their context in the dependency tree, where the actual word matched with the class is associ-ated to one of the slots in the template. The notation of the patterns in this paper is based on a dependency tree where ( ( - )..( - )) denotes is the head, and, for each in , is its argument and the relation between and is labeled with . The labels introduced in this paper are SBJ (subject), OBJ (object), ADV (adverbial adjunct), REL (relative), APPOS (apposition) and prepositions (IN, OF, etc.). Also, we assume that the order of the arguments does not matter. Symbols begin-ning with C- represent NE (Named Entity) types."
"With the extraction patterns based on the Predicate-Argument model, only perpetrator and victim can be extracted."
"The location (downtown Jerusalem) is embedded as a modifier of the noun (heart) within the prepositional phrase, which is an adjunct of the main predicate, triggered [Footnote_3] ."
3 Yangarber refers this as a noun phrase pattern[REF_CITE].
"Furthermore, it is not clear whether the extracted entities are related to the same event, because of the clausal boundaries. [Footnote_4]"
"4 This is the problem of merging the result of entity extrac-tion. Most IE systems have hard-coded inference rules, such"
"Chain model Our previous work, the Chain model[REF_CITE][Footnote_5] attempts to remedy the limitations of the Predicate-Argument model."
5 Originally we called it “Tree-Based Representation of Pat-terns”. We renamed it to avoid confusion with the proposed approach that is also based on dependency trees.
The extraction patterns generated by the Chain model are any chain-shaped paths in the dependency tree. [Footnote_6]
"6[REF_CITE]required the root node of the chain to be a verbal predicate, but we have relaxed that constraint for our experiments."
Thus it successfully avoids the clausal boundary and embedded entity limitation.
We reported a 5% gain in recall at the same precision level in the MUC-6 management succession task compared to the Predicate-Argument model.
"However, the Chain model also has its own weak-ness in terms of accuracy due to the lack of context."
"For example, in Figure 1(c), (triggered ( C-DATE - ADV)) is needed to extract the date entity."
"However, the same pattern is likely to be applied to texts in other domains as well, such as “The Mexican peso was devalued and triggered a national financial cri-sis last week.”"
"Subtree model The Subtree model is a general-ization of previous models, such that any subtree of a dependency tree in the source sentence can be re-garded as an extraction pattern candidate."
"As shown in Figure 1(d), the Subtree model, by its defini-tion, contains all the patterns permitted by either the Predicate-Argument model or the Chain model."
"It is also capable of providing more relevant context, such as (triggered (explosion-OBJ)( C-DATE -ADV)) ."
"The obvious advantage of the Subtree model is the flexibility it affords in creating suitable patterns, spanning multiple levels and multiple branches."
Pat-tern coverage is further improved by relaxing the constraint that the root of the pattern tree be a pred-icate node.
"However, this flexibility can also be a disadvantage, since it means that a very large num-ber of pattern candidates — all possible subtrees of the dependency tree of each sentence in the corpus — must be considered."
An efficient procedure is re-quired to select the appropriate patterns from among the candidates.
"Also, as the number of pattern candidates in-creases, the amount of noise and complexity in- creases."
"In particular, many of the pattern candidates overlap one another."
"For a given set of extraction patterns, if pattern A subsumes pattern B (say, A is (shoot ( C-PERSON -OBJ)(to death)) and B is (shoot ( C-PERSON -OBJ)) ), there is no added contribution for extraction by pattern matching with A (since all the matches with pattern A must be covered with pattern B)."
"Therefore, we need to pay special attention to the ranking function for pattern candidates, so that pat-terns with more relevant contexts get higher score."
This section discusses an automatic procedure to learn extraction patterns.
"Given a narrative descrip- tion of the scenario and a set of source documents, the following three stages obtain the relevant extrac-tion patterns for the scenario; preprocessing, docu-ment retrieval, and ranking pattern candidates."
Morphological analysis and Named Entities (NE) tagging are performed at this stage. [Footnote_7] Then all the sentences are converted into dependency trees by an appropriate dependency analyzer. [Footnote_8]
"7 We used Extended NE hierarchy based[REF_CITE], which is structured and contains 150 classes."
"8 Any degree of detail can be chosen through entire proce-dure, from lexicalized dependency to chunk-level dependency. For the following experiment in Japanese, we define a node in"
"The NE tagging replaces named entities by their class, so the result-ing dependency trees contain some NE class names as leaf nodes."
"This is crucial to identifying common patterns, and to applying these patterns to new text."
"The procedure retrieves a set of documents that de-scribe the events of the scenario of interest, the rel-evant document set."
A set of narrative sentences de-scribing the scenario is selected to create a query for the retrieval.
Any IR system of sufficient accu-racy can be used at this stage.
"For this experiment, we retrieved the documents using CRL’s stochastic-model-based IR system[REF_CITE]."
"Given the dependency trees of parsed sentences in the relevant document set, all the possible subtrees can be candidates for extraction patterns."
The rank-ing of pattern candidates is inspired by TF/IDF scor-ing in IR literature; a pattern is more relevant when it appears more in the relevant document set and less across the entire collection of source documents.
The right-most expansion base subtree discovery algorithm[REF_CITE]was implemented to cal-culate term frequency (raw frequency of a pattern) and document frequency (the number of documents where a pattern appears) for each pattern candidate.
"The algorithm finds the subtrees appearing more fre-quently than a given threshold by constructing the subtrees level by level, while keeping track of their occurrence in the corpus."
"Thus, it efficiently avoids the construction of duplicate patterns and runs al-most linearly in the total size of the maximal tree patterns contained in the corpus."
The following ranking function was used to rank each pattern candidate.
"The score of subtree ,  , is     ! (1) where  is the number of times that subtree appears across the documents in the relevant docu-ment set, &quot; . # $ is the set of subtrees that appear in &quot; . is the number of documents in the collection containing subtree , and is the total number of the dependency tree as a bunsetsu, phrasal unit. documents in the collection."
The first term roughly corresponds to the term frequency and the second term to the inverse document frequency in TF/IDF scoring. % is used to control the weight on the IDF portion of this scoring function.
The % in Equation (1) is used to parameterize the weight on the IDF portion of the ranking function.
"As we pointed out in Section 2, we need to pay spe-cial attention to overlapping patterns; the more rele-vant context a pattern contains, the higher it should be ranked."
The weight % serves to focus on how specific a pattern is to a given scenario.
"Therefore, for high % value, (triggered (explosion-OBJ)( C-DATE - ADV)) is ranked higher than (triggered ( C-DATE - ADV)) in the terrorism scenario, for example."
Fig-ure 2 shows the improvement of the extraction per-formance by tuning % on the entity extraction task which will be discussed in the next section.
"For unsupervised tuning of % , we used a pseudo-extraction task, instead of using held-out data for su-pervised learning."
"We used an unsupervised version of the text classification task to optimize % , assum-ing that all the documents retrieved by the IR sys-tem are relevant to the scenario and the pattern set that performs well on the text classification task also works well on the entity extraction task."
"The unsupervised text classification task is to measure how close a pattern matching system, given a set of extraction patterns, simulates the document retrieval of the same IR system as in the previous sub-section."
The % value is optimized so that the cu-mulative performance of the precision-recall curve over the entire range of recall for the text classifica-tion task is maximized.
"The document set for text classification is com-posed of the documents retrieved by the same IR system as in Section 3.2 plus the same number of documents picked up randomly, where all the docu-ments are taken from a different document set from the one used for pattern learning."
"The pattern match-ing system, given a set of extraction patterns, clas-sifies a document as retrieved if any of the patterns match any portion of the document, and as random otherwise."
"Thus, we can get the performance of text classification of the pattern matching system in the form of a precision-recall curve, without any super-vision."
"Next, the area of the precision-recall curve is computed by connecting every point in the precision-recall curve from 0 to the maximum re-call the pattern matching system reached, and we compare the area for each possible % value."
"Finally, the % value which gets the greatest area under the precision-recall curve is used for extraction."
The comparison to the same procedure based on the precision-recall curve of the actual extraction performance shows that this tuning has high correla-tion with the extraction performance (Spearman cor-relation coefficient  with 2% confidence).
"For efficiency and to eliminate low-frequency noise, we filtered out the pattern candidates that appear in less than 3 documents throughout the entire collec-tion."
"Also, since the patterns with too much con-text are unlikely to match with new text, we added another filtering criterion based on the number of nodes in a pattern candidate; the maximum number of nodes is 8."
"Since all the slot-fillers in the extraction task of our experiment are assumed to be instances of the 150 classes in the extended Named Entity hierar-chy[REF_CITE], further filtering was done by requiring a pattern candidate to contain at least one Named Entity class."
The experiment of this study is focused on compar-ing the performance of the earlier extraction pattern models to the proposed Subtree Model (SUBT).
"The compared models are the direct predicate-argument model (PA) [Footnote_9] , and the Chain model (CH)[REF_CITE]."
"9 This is a restricted version[REF_CITE]con-strained to have a single place-holder for each pattern, while[REF_CITE]allowed more than one place-holder. However, the difference does not matter for the entity extrac-tion task which does not require merging entities in a single template."
"The task for this experiment is entity extraction, which is to identify all the entities participating in relevant events in a set of given Japanese texts."
"Note that all NEs in the test documents were identified manually, so that the task can measure only how well extraction patterns can distinguish the participating entities from the entities that are not related to any events."
This task does not involve grouping entities associated with the same event into a single template to avoid possible effect of merging failure on extrac-tion performance for entities.
"We accumulated the test set of documents of two scenarios; the Manage-ment Succession scenario[REF_CITE], with a simpler template structure, where corporate man-agers assumed and/or left their posts, and the Mur-derer Arrest scenario, where a law enforcement or-ganization arrested a murder suspect."
"The source document set from which the ex-traction patterns are learned consists of 117,109 Mainichi Newspaper articles from 1995."
All the sentences are morphologically analyzed by JU-MAN[REF_CITE]and converted into depen-dency trees by KNP[REF_CITE].
"Regardless of the model of extraction patterns, the pattern acquisition follows the procedure described in Section 3."
"The association of NE classes and slots in the template is made automatically; Person, Organi-zation, Post (slots) correspond to C-PERSON, C-ORG, C-POST (NE-classes), respectively, in the Succession scenario, and Suspect, Arresting Agency, Charge (slots) correspond to C-PERSON, C-ORG, C-OFFENCE (NE-classes), respectively, in the Ar- rest scenario. 10"
"For each model, we get a list of the pattern candi-dates ordered by the ranking function discussed in Section 3.3 after filtering."
The result of the per-formance is shown (Figure 3) as a precision-recall graph for each subset of top- ranked patterns where ranges from 1 to the number of the pattern candi-dates.
"The test set was accumulated from Mainichi[REF_CITE]by a simple keyword search, with some additional irrelevant documents. (See Ta-ble 1 for detail.)"
Figure 3(a) shows the precision-recall curve of top- relevant extraction patterns for each model on the Succession Scenario.
"At lower recall levels (up to 35%), all the models performed similarly."
"How-ever, the precision of Chain patterns dropped sud-denly by 20% at recall level 38%, while the SUBT patterns keep the precision significantly higher than Chain patterns until it reaches 58% recall."
"Even after SUBT hit the drop at 56%, SUBT is consistently a few percent higher in precision than Chain patterns for most recall levels."
Figure 3(a) also shows that although PA keeps high precision at low recall level it has a significantly lower ceiling of recall (52%) compared to other models.
Figure 3(b) shows the extraction performance on the Arrest scenario task.
"Again, the Predicate-Argument model has a much lower recall ceiling (25%)."
The difference in the performance between the Subtree model and the Chain model does not seem as obvious as in the Succession task.
"However, it is still observable that the Subtree model gains a few percent precision over the Chain model at re-call levels around 40%."
A possible explanation of the subtleness in performance difference in this sce-nario is the smaller number of contributing patterns compared to the Succession scenario.
One of the advantages of the proposed model is the ability to capture more varied context.
The Predicate-Argument model relies for its context on the predicate and its direct arguments.
"However, some Predicate-Argument patterns may be too gen-eral, so that they could be applied to texts about a different scenario and mistakenly detect entities from them."
"For example, (( C-ORG -SBJ) happyo-suru), “ C-ORG reports” may be the pattern used to ex-tract an Organization in the Succession scenario but it is too general — it could match irrelevant sen-tences by mistake."
The proposed Subtree Model can acquire a more scenario-specific pattern (( C-ORG - SBJ)((shunin-suru-REL) jinji-OBJ) happyo-suru) “ C-ORG reports a personnel affair to appoint” .
"Any scoring func-tion that penalizes the generality of a pattern match, such as inverse document frequency, can success-fully lessen the significance of too general patterns."
The detailed analysis of the experiment revealed that the overly-general patterns are more severely penalized in the Subtree model compared to the Chain model.
"Although both models penalize gen-eral patterns in the same way, the Subtree model also promotes more scenario-specific patterns than the Chain model."
"In Figure 3, the large drop was caused by the pattern (( C-DATE -ON) C-POST ) , which was mainly used to describe the date of ap-pointment to the C-POST in the list of one’s pro-fessional history (which is not regarded as a Suc-cession event), but also used in other scenarios in the business domain (18% precision by itself)."
"Although the scoring function described in Sec-tion 3.3 is the same for both models, the Subtree model can also produce contributing patterns, such as (( C-PERSON C-POST -SBJ)( C-POST -TO) shunin-suru) “ C-PERSON C-POST was appointed to C-POST ” whose ranks were higher than the problematic pat-tern."
"Without generalizing case marking for nominal-ized predicates, the Predicate-Argument model ex-cludes some highly contributing patterns with nomi-nalized predicates, as some example patterns show in Figure 4."
"Also, chains of modifiers could be extracted only by the Subtree and Chain models."
A typical and highly relevant expression for the Succession scenario is (((daihyo-ken-SBJ) aru-REL) C-POST ) “ C-POST with ministerial authority” .
"Although, in the Arrest scenario, the superiority of the Subtree model to the other models is not clear, the general discussion about the capability of cap-turing additional context still holds."
"In Figure 4, the short pattern (( C-PERSON C-POST -APPOS) C-NUM ) , which is used for a general description of a person with his/her occupation and age, has rela-tively low precision (71%)."
"However, with more rel-evant context, such as “arrest” or “unemployed”, the patterns become more relevant to Arrest scenario."
"In this paper, we explored alternative models for the automatic acquisition of extraction patterns."
We pro-posed a model based on arbitrary subtrees of depen-dency trees.
The result of the experiment confirmed that the Subtree model allows a gain in recall while preserving high precision.
We also discussed the effect of the weight tuning in TF/IDF scoring and showed an unsupervised way of adjusting it.
There are several ways in which our pattern model may be further improved.
"In particular, we would like to relax the restraint that all the fills must be tagged with their proper NE tags by introducing a GENERIC place-holder into the extraction patterns."
"By allowing a GENERIC place-holder to match with anything as long as the context of the pattern is matched, the extraction patterns can extract the enti-ties that are not tagged properly."
Also patterns with a GENERIC place-holder can be applied to slots that are not names.
"Thus, the acquisition method de-scribed in Section 3 can be used to find the patterns for any type of slot fill."
Acknowledgments Thanks to Taku Kudo for his implementation of the subtree discovery algorithm and the anonymous reviewers for useful comments.
"This research is supported by the Defense Advanced Research Projects Agency as part of the Translin-gual Information Detection, Extraction and Summa-rization (TIDES) program,[REF_CITE]-00- 1-8917 from the Space and Naval Warfare Systems Center San Diego."
Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection.
In this paper we formulate story link detec-tion and new event detection as informa-tion retrieval task and hypothesize on the impact of precision and recall on both sys-tems.
"Motivated by these arguments, we introduce a number of new performance enhancing techniques including part of speech tagging, new similarity measures and expanded stop lists."
Experimental re-sults validate our hypothesis.
"Topic Detection and Tracking (TDT) research is sponsored by the DARPA Translingual Information Detection, Extraction, and Summarization (TIDES) program."
"The research has five tasks related to organizing streams of data such as newswire and broadcast news[REF_CITE]: story segmentation, topic tracking, topic detection, new event detection (NED), and link detection (LNK)."
"A link detection system detects whether two stories are “linked”, or discuss the same event."
A story about a plane crash and another story about the funeral of the crash vic-tims are considered to be linked.
"In contrast, a story about hurricane Andrew and a story about hurricane Agnes are not linked because they are two different events."
A new event detection system detects when a story discusses a previously unseen or “not linked” event.
Link detection is considered to be a core tech-nology for new event detection and the other tasks.
Several groups are performing research in the TDT tasks of link detection and new event detection.
"Based on their findings, we incorporated a number of their ideas into our baseline system."
CMU[REF_CITE]and UMass[REF_CITE]found that for new event detection it was better to com-pare a new story against all previously seen stories than to cluster previously seen stories and compare a new story against the clusters.
"CMU[REF_CITE]found that NED results could be im-proved by developing separate models for different news sources to that could capture idiosyncrasies of different sources, which we also extended to link de-tection."
UMass reported on adapting a tracking sys-tem for NED detecti[REF_CITE].
"Allan et. al ,[REF_CITE]developed a NED system based upon a tracking technology and showed that to achieve high-quality first story detection, tracking effectiveness must improve to a degree that experi-ence suggests is unlikely."
"In this paper, while we reach a similar conclusion[REF_CITE]for LNK and NED systems , we give specific directions for improving each system separately."
We compare the link detection and new event detection tasks and discuss ways in which we have observed that tech-niques developed for one task do not always perform similarly for the other task.
This section describes those parts of the process-ing steps and the models that are the same for New Event Detection and for Link Detection.
"For pre-processing, we tokenize the data, recog-nize abbreviations, normalize abbreviations, remove stop-words, replace spelled-out numbers by digits, add part-of-speech tags, replace the tokens by their stems, and then generate term-frequency vectors."
Our similarity calculations of documents are based on an incremental TF-IDF model.
"In a TF-IDF model, the frequency of a term in a document (TF) is weighted by the inverse document frequency (IDF)."
"In  the incremental model, document frequencies are not static but change in time steps ."
"At time , a new set of test documents is added to the model by updating the frequencies      (1) where denote the document frequencies in the newly added set of  documents."
The initial docu-ment frequencies are generated from a (pos-sibly emtpy) training set.
"In a static TF-IDF model, new words (i.e., those words, that did not occur in the training set) are ignored in further computations."
An incremental TF-IDF model uses the new vocab-ulary in similarity calculations.
This is an advantage because new events often contain new vocabulary.
Very low frequency terms tend to be uninfor-mative  .  
We therefore set a threshold  .
Only terms #&quot; with ! are used at time .
We use  .
The document frequencies as described in the pre-vious section are used to calculate weights for the terms in the documents .
"At time , we use &amp;$ (% &apos;*) ,+. / ,+ 9 0  (2)   where 0 *9 is the total number of documents at time . is a normalization value such that either the weights sum to 1 (if we use Hellinger distance, KL-divergence, or Clarity-based distance), or their squares sum to 1 (if we use cosine distance)."
The vectors consisting of normalized term weights :$5(% *&apos; ) are used to calculate the similarity between two documents and ; .
"In our current implementa-tion, we use the the Clarity metric which was intro-duced[REF_CITE]and gets its name from the distance to general En-glish, which is called Clarity."
"We used a symmetric version that is computed as: & lt; &gt;% ?= @+ ; CBEDGF:IH7H ; : B DJF: H7H   ; (3) ; :5$ %(*&apos; ) ,+ @+ :$5%(&apos;*) @+ &amp; ; &amp;$ %(&apos;*) +,Z ; (4) DJF where  “ ” is the Kullback-Leibler divergence, is the probability distribution of words for “gen-eral English” as derived from the training corpus."
"The idea behind this metric is that we want to give credit to similar pairs of documents that are very different from general English, and we want to dis-count similar pairs of documents that are close to general English (which can be interpreted as being the noise)."
The motivation for using the clarity met-ric will given in section 6.1.
"Another metric is Hellinger distance &lt; %&gt;= ,+ ; RQ \S [ :5$ (% &apos;*) ,+&amp;:$5(% &apos;*) + ; Z (5)"
"Other possible similarity metrics are the cosine dis-tance, the Kullback-Leibler divergence, or the sym-metric form of it, Jensen-Shannon distance."
"Documents in the stream of news stories may stem from different sources, e.g., there are 20 different sources in the data[REF_CITE](ABC News, As-sociated Press, New York Times, etc)."
Each source might use the vocabulary differently.
"For example, the names of the sources, names of shows, or names of news anchors are much more frequent in their own source than in the other ones."
"In order to re-flect the source-specific differences we do not build one incremental TF-IDF model, but as many as we have different sources and use frequencies -] ^  (6) for source &lt; at time ."
"The frequencies are updated according to equation (1), but only using those doc-uments in that are from the same source &lt; ."
"As a consequence, a term like “CNN” receives a high document frequency (thus low weight) in the model for the source CNN and a low document frequency (thus high weight) in the New York Times model.  Instead  of the overall document frequencies  ] ^  , we now use the source specific when calculating the term weights in equation (2)."
"Sources &lt; for which no training data is available  -] ^  (i.e., no data to generate is available) might be initialized in two different ways:  ] ^  1. Use an empty model: for all ; 2. Identify one or more other but similar sources  &lt; for which training data is available and use   ]-^ Q   ] ^  Z (7) ]"
"Due to stylistic differences between various sources, e.g., news paper vs. broadcast news, translation er-rors, and automatic speech recognition errors[REF_CITE], the similarity measures for both on-topic and off-topic pairs will in general depend on the source pair."
"Errors due to these differences can be reduced by using thresholds conditioned on the sources[REF_CITE], or, as we do, by normalizing the similarity values based on similari-ties for the source pairs found in the story history."
"In order to decide whether a new document ; that is added to the collection at time describes a new event, it is individually compared to all previous documents using the steps  described in section 2."
We identify the document with highest similarity: 8  &lt; %T= +  ; Z (8) The value  &lt;  X $ ;  / B &lt; &gt;% = + ; is used to de-termine whether a document ; is about a new event and at the same time is an indication of the confi-dence ] in our decision.
"If the score exceeds a thresh-old , then there is no sufficiently similar previous document, thus ; describes a new ] event  (decision YES)."
"If the score is smaller than , then is suf-ficiently similar, thus ; describes ] an old event (de-cision NO)."
The threshold can be determined by using labeled training data and calculating similar-ity scores for document pairs on the same event and on different events.
"In order to decide whether a pair of stories and ; are linked, we identify a set of similarity metrics that capture the similarity between the two docu-ments using Clarity and Hellinger metrics: ,+   &lt; %&gt;  = @+ ; + &lt; %&gt;  = @@+ ;  Z (9) ; @+"
The value ; is used to determine whether sto-ries “q” and “d” are linked.
If the similarity exceeds a threshold  we the two stories are sufficiently similar (decision YES).
If the similarity is smaller than  we the two stories are sufficiently differ-ent (decision NO).
The Threshold  can be deter-mined using labeled training data.
"All TDT systems are evaluated by calculating a De-tection Cost: & quot;$! # &amp;%(&apos; ] ] 2 ) %&amp;&apos; ] ] 2 ) +*-,/. # 1032 2 ) 032 2 )  +6* ,/. # Z (10) where (% &apos; ] ] and 032 are the costs of a miss and a false alarm."
"They ) are ] ] set to ) 1 and 0.1, respec-tively, for all tasks. %(&apos; and 032 are the condi-tional probabilities ) of a miss ) and a false alarm in the system output. +6* /, . # and  +6* ,/. # a the a priori target and non-target probabilities."
They are set to 0.02 and 0.98 for LNK and NED.
"The detection cost is normalized such that a perfect system scores 0, and a random baseline scores 1: & quot;$! # /7 4 , % &quot;$! # min 1%(&apos; ] ] 2 ) +6* /, . # + 1032 2 )  +*6,/. # (11)"
TDT evaluates all systems with a topic-weighted method: error probabilities are accumulated sepa-rately for each topic and then averaged.
This is mo-tivated by the different sizes of the topics.
The evaluation yields two costs: the detection cost is the cost when using the actual decisions made by the system; the minimum detection cost is the cost when using the confidence scores that each system has to emit with each decision and selecting the op-timal threshold based on the score.
"In the[REF_CITE]evaluation, our Link Detec-tion system /7 4 was , % the Z best /  ofandthree = % P systems   /7 , 4 yield- , % ing &quot;  ."
"Our New Event Detection system /7 4 , % was Z / ranked second = % P of four with 7 4 , % costs Z of . #&amp; # Z / and"
"In this section, we draw on Information retrieval tools to analyze LNK and NED tasks."
"Motivated by the results of this analysis, we compare a number of techniques in the LNK and NED tasks in particular we compare the utility of two similarity measures, part-of-speech tagging, stop wording, and normal-izing abbreviations and numerals."
"The comparisons were performed on corpora developed for TDT, in-cluding TDT2 and TDT3."
The conditions for false alarms and misses are re-versed for LNK and NED tasks.
"In the LNK task, incorrectly flagging two stories as being on the same event is considered a false alarm."
"In contrast in the NED task, incorrectly flagging two stories as being on the same event will cause the true first story to be missed."
"Conversely, in LNK incorrectly labeling two stories that are on the same event as not linked is a miss, but in the NED task, incorrectly labeling two stories on the same event as not linked can result in a false alarm where a story is incorrectly identified as a new event."
"The detection cost in Eqn.10 which ] ] 2 ) assigns a higher &quot;*+ cost 2 ) to  false +*6,/. # alarm Z  %( . &apos; A LNK +6* /, . system #032 Z wants to minimize false alarms and to do this it should identify stories as being linked only if they are linked, which translates to high precision."
"In contrast a NED system, will minimize false alarms by identifying all stories that are linked which trans-lates to high recall."
"Motivated by this discussion, we investigated the use of number of precision and re-call enhancing techniques with the LNK and NED system."
We investigated the use of the Clarity met-ric[REF_CITE]which was shown to cor-relate positively with precision.
We investigated the use of part-of-speech tagging which was shown by Allan and Raghavan[REF_CITE]to improve query clarity.
In section 6.2.1 we will show how POS helps recall.
We also investigated the use of expanded stop-list which improves precision.
We also investigated normalizing abbreviations and transforming spelled out numbers into numbers.
On the one hand the enhanced processing list includes most of the term in the ASR stop-list and remov-ing these terms will improve precision.
"On the other hand normalizing these terms will have the same ef-fect as stemming a recall enhancing device[REF_CITE],[REF_CITE]."
"In ad-dition to these techniques, we also investigated the use of different similarity measures."
The systems developed for TDT primarily use co-sine similarity as the similarity measure.
We have developed systems based on cosine similarity[REF_CITE].
"In work on text segmentation,[REF_CITE]observed that the system performance was much better when the Hellinger measure was used instead."
"In this work, we decided to use the clarity metric, a precision enhancing device[REF_CITE]."
"For both our LNK and NED systems, we compared the performance of the systems using each of the similarity measures separately."
"Table 1 shows that for LNK, the system based on Clarity similar-ity performed better the system based on Hellinger similarity; in contrast, for NED, the system based on"
Hellinger similarity performed better.
Figure 1 shows the cumulative density function for the Hellinger and Clarity similarities for on-topic (about the same event) and off-topic (about different events) pairs for the LNK task.
"While there are a number of statistics to measure the overall difference between tow cumulative distribution functions, we used the Kolmogorov-Smirnov distance (K-S dis-tance; the largest difference between two cumula-tive distributions) for two reasons."
"First, the K-S distance is invariant under re-parametrization."
"Sec-ond, the significance of the K-S distance in case of the null hypothesis (data sets are drawn from same distribution) can be calculated[REF_CITE]."
"The K-S distance between the on-topic and off-topic similarities is larger for Clarity similarity (cf. table 2), indicating that it is the better metric for LNK."
Figure 2 shows the cumulative distribution func-tions for Hellinger and Clarity similarities in the NED task.
The plot is based on pairs that contain the current story and its most similar story in the story history.
"When the most similar story is on the same event (approx. 75% of the cases), its similarity is part of the on-topic distribution, otherwise (approx. 25% of the cases) it is plotted as off-topic."
The K-S dis-tance between the Hellinger on-topic and off-topic CDFs is larger than those for Clarity (cf. table 2).
"For both NED and LNK, we can reject the null hy-pothesis for both metrics with over 99.99 % confi-dence."
"To get the high precision required for LNK sys-tem, we need to have a large separation between the on-topic and off-topic distributions."
"Examining Fig-ure 1 and Table 2 , indicates that the Clarity metric has a larger separation than the Hellinger metric."
"At high recall required by NED system (low CDF val-ues for on-topic), there is a greater separation with the Hellinger metric."
"For example, at 10% recall, the Hellinger metric has 71 % false alarm rate as com-pared to 75 % for the Clarity metric."
We explored the idea that noting the part-of-speech of the terms in a document may help to re-duce confusion among some of the senses of a word.
"During pre-processing, we tagged the terms as one of five categories: adjective, noun, proper nouns, verb, or other."
A “tagged term” was then created by combining the stem and part-of-speech.
"For ex-ample, ‘N train’ represents the term ‘train’ when used as a noun, and ‘V train’ represents the term ‘train’ when used as a verb."
We then ran our NED and LNK systems using the tagged terms.
The sys-tems were tested in the[REF_CITE]TDT data.
A comparison of the performance of the systems when part-of-speech is used against a baseline sys-
Table 4: Comparison of using an “ASR stop-list” and “enhanced preprocessing” for handling ASR tem when part-of-speech is not used is shown in Ta-ble 3.
"For Story Link Detection, performance de-creases by 38.3%, while for New Event Detection, performance improves by 8.3%."
"Since POS tagging helps differentiates between the different senses of the same root, it also reduces the number of match-ing terms between two documents."
"In the LNK task for example, the total number of matches drops from 177,550 to 151,132."
"This has the effect of placing a higher weight on terms that match, i.e. terms that have the same sense and for the TDT corpus will increase recall and decrease."
Consider for example matching “food server to “food service” and “java server”.
When using POS both terms will have the same similarity to the query and the use of POS will retrieve the relevant documents but will also retrieve other documents that share the same sense.
A large portion of the documents in the TDT col-lection has been automatically transcribed using Au-tomatic Speech Recognition (ASR) systems which can achieve over 95% accuracies.
"However, some of the words not recognized by the ASR tend to be very informative words that can significantly impact the detection performance[REF_CITE]."
"Fur-thermore, there are systematic differences between ASR and manually transcribed text, e.g., numbers are often spelled out thus “30” will be spelled out “thirty”."
"Another situation where ASR is different from transcribed text is abbreviations, e.g. ASR sys-tem will recognize ‘CNN” as three separate tokens “C”, “N”, and “N”."
"In order to account for these differences, we iden-tified the set of tokens that are problematic for ASR."
"Our approach was to identify a parallel corpus of manually and automatically transcribed documents, the TDT2 corpus, and then use a statistical approach[REF_CITE]to identify tokens with significantly 



"
"In[REF_CITE]we investigated normalizing abbreviations and transforming spelled-out numbers into numerals, “enhanced preprocessing”, and then compared this approach with using an “ASR stop-list”."
The previous two sections examined the impact of four different techniques on the performance of LNK and NED systems.
The Part-of-speech is a re-call enhancing devices while the ASR stop-list is a precision enhancing device.
The enhanced prepro-cessing improves precision and recall.
The results which are summarized in Table 5 indicate that pre-cision enhancing devices improved the performance of the LNK task while recall enhancing devices im-proved the NED task.
"In the extreme case, a perfect link detection system performs perfectly on the NED task."
We gave em-pirical evidence that there is not necessarily such a correlation at lower accuracies.
These findings are in accordance with the results reported[REF_CITE]for topic tracking and first story detection.
"To test the impact of the cost function on the per-formance of LNK and NED ] ] systems, we repeated the evaluation with &quot;&amp;% &apos; * both set to 1,and and we found that the difference between the two re- sults decreases from 30.24% to 14.73%."
"B The result indicates that the setting (Hel, PoS, ASRstop) is better at recall B (identifying same-event stories), while (Clarity, PoS, ASRstop) is better at pre-cision (identifying different-event stories)."
"In addition to the different costs assigned to misses and false alarms, there is a difference in the number of positives and negatives in the &quot; data set (the TDT cost function uses +*6,/. # Z )."
This might explain part of the remaining difference of 14.73%.
"Another view on the differences is that a NED system must perform very well on the higher penal-ized first stories when it does not have any training data for the new event, event though it may perform worse on follow-up stories."
"A LNK system, how-ever, can afford to perform worse on the first story if it compensates by performing well on follow-up sto-ries (because here not flagged follow-up stories are considered misses and thus higher penalized than in NED)."
This view explains the benefits of using part-of-speech information and the negative effect of the ASR stop-list on NED : different part-of-speech tags help discriminate new events from old events; re-moving words by using the ASR stoplist makes it harder to discriminate new events.
"We conjecture that the Hellinger metric helps improve recall, and in a study similar[REF_CITE]we plan to further evaluate the impact of the Hellinger metric on a closed collection e.g. TREC."
We have compared the effect of several techniques on the performance of a story link detection system and a new event detection system.
"Although many of the processing techniques used by our systems are the same, a number of core technologies affect the performance of the LNK and NED systems differ-ently."
"The Clarity similarity measure was more ef-fective for LNK, Hellinger similarity measure was more effective for NED, part-of-speech was more useful for NED, and stop-list adjustment was more useful for LNK."
"These differences may be due in part to a reversal in the tasks: a miss in LNK means the system does not flag two stories as being on the same event when they actually are, while a miss in NED means the system does flag two stories as be-ing on the same event when actually they are not."
"In future work, we plan to evaluate the impact of the Hellinger metric on recall."
"In addition, we plan to use Anaphora resolution which was shown to im-prove recall[REF_CITE]to enhance the NED system."
"For spoken dialogue systems to correctly understand user intentions to achieve certain tasks while con-versing with users, the dialogue state has to be ap-propriately updated[REF_CITE]after each user utterance."
"Here, a dialogue state means all the information that the system possesses concern-ing the dialogue."
"For example, a dialogue state in-cludes intention recognition results after each user utterance, the user utterance history, the system ut-terance history, and so forth."
"Obtaining the user in-tention and the content of an utterance using only the single utterance is called speech understanding, and updating the dialogue state based on both the previ-ous utterance and the current dialogue state is called discourse understanding."
"In general, the result of speech understanding can be ambiguous, because it is currently difficult to uniquely decide on a single speech recognition result out of the many recogni-tion candidates available, and because the syntac-tic and semantic analysis process normally produce multiple hypotheses."
"The system, however, has to be able to uniquely determine the understanding result after each user utterance in order to respond to the user."
The system therefore must be able to choose the appropriate speech understanding result by re-ferring to the dialogue state.
"Most conventional systems uniquely determine the result of the discourse understanding, i.e., the dialogue state, after each user utterance."
"However, multiple dialogue states are created from the current dialogue state and the speech understanding results corresponding to the user utterance, which leads to ambiguity."
"When this ambiguity is ignored, the dis- course understanding accuracy is likely to decrease."
Our idea for improving the discourse understanding accuracy is to make the system hold multiple dia-logue states after a user utterance and use succeed-ing utterances to resolve the ambiguity among di-alogue states.
"Although the concept of combining multiple dialogue states and speech understanding results has already been reported[REF_CITE], they use intuition-based hand-crafted rules for the disambiguation of dialogue states, which are costly and sometimes lead to inaccuracy."
"To resolve the ambiguity of dialogue states and reduce the cost of rule making, we propose using statistical infor-mation obtained from dialogue corpora, which com-prise dialogues conducted between the system and users."
The next section briefly illustrates the basic ar-chitecture of a spoken dialogue system.
Section 3 describes the problem to be solved in detail.
"Then after introducing related work, our approach is de-scribed with an example dialogue."
"After that, we describe the experiments we performed to verify our approach, and discuss the results."
The last section summarizes the main points and mentions future work.
"Here, we describe the basic architecture of a spoken dialogue system (Figure 1)."
"When receiving a user utterance, the system behaves as follows. [Footnote_1]."
"1 In general, a dialogue act corresponds to one sentence. However, in dialogues where user utterances are unrestricted, smaller units, such as phrases, can be regarded as dialogue acts."
The speech recognizer receives a user utterance and outputs a speech recognition hypothesis. 2.
The language understanding component re-ceives the speech recognition hypothesis.
The syntactic and semantic analysis is performed to convert it into a form called a dialogue act.
Table 1 shows an example of a dialogue act.
"In the example, “refer-start-and-end-time” is called the dialogue act type, which briefly describes the meaning of a dialogue act, and “start=14:00” and “end=15:00” are add-on in-formation. 1 3."
"The discourse understanding component re-ceives the dialogue act, refers to the current di-alogue state, and updates the dialogue state. 4."
"The dialogue manager receives the current dia-logue state, decides the next utterance, and out-puts the next words to speak."
The dialogue state is updated at the same time so that it contains the content of system utterances. 5.
The speech synthesizer receives the output of the dialogue manager and responds to the user by speech.
This paper deals with the discourse understand-ing component.
"Since we are resolving the ambi-guity of speech understanding from the discourse point of view and not within the speech understand-ing candidates, we assume that a dialogue state is uniquely determined given a dialogue state and the next dialogue act, which means that a dialogue act is a command to change a dialogue state."
We also assume that the relationship between the dialogue act and the way to update the dialogue state can be easily described without expertise in dialogue sys-tem research.
We found that these assumptions are reasonable from our experience in system develop-ment.
Note also that this paper does not separately deal with reference resolution; we assume that it is performed by a command.
A speech understanding result is considered to be equal to a dialogue act in this article.
"In this paper, we consider frames as representa-tions of dialogue states."
"To represent dialogue states, plans have often been used[REF_CITE]."
"Traditionally, plan-based discourse understanding methods have been imple-mented mostly in keyboard-based dialogue systems, although there are some recent attempts to apply them to spoken dialogue systems as well[REF_CITE]; however, considering the current performance of speech recognizers and the limitations in task domains, we believe frame-based discourse understanding and dialogue man-agement are sufficient[REF_CITE]."
Most conventional spoken dialogue systems uniquely determine the dialogue state after a user utterance.
"Normally, however, there are multiple candidates for the result of speech understanding, which leads to the creation of multiple dialogue state candidates."
We believe that there are cases where it is better to hold more than one dialogue state and resolve the ambiguity as the dialogue progresses rather than to decide on a single dialogue state after each user utterance.
"As an example, consider a piece of dialogue in which the user utterance “from two p.m.” has been misrecognized as “uh two p.m.” (Figure 2)."
"Fig-ure 3 shows the description of the example dia-logue in detail including the system’s inner states, such as dialogue acts corresponding to the speech recognition hypotheses [Footnote_2] and the intention recogni-tion results. 3 After receiving the speech recogni-tion hypothesis “uh two p.m.,” the system cannot tell whether the user utterance corresponds to a dia-logue act specifying the start time or the end time (da1,da2)."
"2 In this example, for convenience of explanation, the n-best speech recognition input is not considered."
"Therefore, the system tries to obtain further information about the time."
"In this case, the system utters a backchannel to prompt the next user utterance to resolve the ambiguity from the dis-course. [Footnote_4] At this stage, the system holds two dialogue states having different intention recognition results (ds1,ds2)."
4 A yes/no question may be an appropriate choice as well.
"The next utterance, “to three p.m.,” is one that uniquely corresponds to a dialogue act spec-ifying the end time (da3), and thus updates the two current dialogue states."
"As a result, two dialogue states still remain (ds3,ds4)."
"If the system can tell that the previous dialogue act was about the start time at this moment, it can understand the user in-tention correctly."
"The correct understanding result, ds[Footnote_3], is derived from the combination of ds1 and da[Footnote_3], where ds1 is induced by ds0 and da1."
3 An intention recognition result is one of the elements of a dialogue state.
3 An intention recognition result is one of the elements of a dialogue state.
"As shown here, holding multiple understanding results can be better than just deciding on the best speech understanding hypothesis and discarding other pos-sibilities."
"In this paper, we consider a discourse understand-ing component that deals with multiple dialogue states."
Such a component must choose the best com-bination of a dialogue state and a dialogue act out of all possibilities.
An appropriate scoring method for the dialogue states is therefore required.
"They used a metric similar to the concept error rate for the evalu- ation of discourse accuracy, comparing reference di-alogue states with hypothesis dialogue states."
Both these methods employ hand-crafted rules to score the dialogue states to decide the best dialogue state.
"Creating such rules requires expert knowledge, and is also time consuming."
There are approaches that propose statistically es-timating the dialogue act type from several previous dialogue act types using N-gram probability[REF_CITE].
"Although their approaches can be used for disam-biguating user utterance using discourse informa-tion, they do not consider holding multiple dialogue states."
"In the context of plan-based utterance understand-ing[REF_CITE], when there is ambiguity in the understanding re-sult of a user utterance, an interpretation best suited to the estimated plan should be selected."
"In ad-dition, the system must choose the most plausible plans from multiple possible candidates."
"Although we do not adopt plan-based representation of dia-logue states as noted before, this problem is close to what we are dealing with."
"Unfortunately, however, it seems that no systematic ways to score the candi-dates for disambiguation have been proposed."
The discourse understanding method that we pro-pose takes the same approach[REF_CITE].
"However, our method is different in that, when ordering the multiple dialogue states, the sta-tistical information derived from the dialogue cor-pora is used."
"We propose using two kinds of statisti-cal information: 1. the probability of a dialogue act type sequence, and 2. the collocation probability of a dialogue state and the next dialogue act."
"Probability of a dialogue act type sequence Based on the same idea[REF_CITE]and[REF_CITE], we use the probability of a dialogue act type sequence, namely, the N-gram probability of dialogue act types."
"Sys-tem utterances and the transcription of user utter-ances are both converted to dialogue acts using a di-alogue act conversion parser, then the N-gram prob-ability of the dialogue act types is calculated."
"Collocation probability of a dialogue state and the next dialogue act From the dialogue corpora, dialogue states and the succeeding user utterances are extracted."
"Then, pairs comprising a dialogue state and a dialogue act are created after convert-ing user utterances into dialogue acts."
"Contrary to the probability of sequential patterns of dialogue act types that represents a brief flow of a dialogue, this collocation information expresses a local detailed flow of a dialogue, such as dialogue state changes caused by the dialogue act."
"The simple bigram of dialogue states and dialogue acts is not sufficient due to the complexity of the data that a dialogue state possesses, which can cause data sparseness problems."
"Therefore, we classify the ways that di-alogue states are changed by dialogue acts into 64 classes characterized by seven binary attributes (Ta-ble 2) and compute the occurrence probability of each class in the corpora."
"We assume that the un-derstanding result of the user intention contained in a dialogue state is expressed as a frame, which is common in many systems[REF_CITE]."
A frame is a bundle of slots that consist of attribute-value pairs concerning a certain domain.
Each speech recognition hypothesis is converted to a dialogue act or acts.
"When there are several di-alogue acts corresponding to a speech recognition hypothesis, all possible dialogue acts are created as in Figure 3, where the utterance “uh two p.m.” pro-duces two dialogue act candidates."
Each dialogue act is given a score using its linguistic and acous-tic scores.
"The linguistic score represents the gram-matical adequacy of a speech recognition hypothe-sis from which the dialogue act originates, and the acoustic score the acoustic reliability of a dialogue act."
"Sometimes, there is a case that a dialogue act has such a low acoustic or linguistic score and that it is better to ignore the act."
"We therefore create a dialogue act called null act, and add this null act to our list of dialogue acts."
A null act is a dialogue act that does not change the dialogue state at all.
"Since the dialogue state is uniquely updated by a di-alogue act, if there are l dialogue acts derived from speech understanding and m dialogue states, m × l new dialogue states are created."
"In this case, we de-fine the score of a dialogue state S t+1 as"
"S t+1 = S t + α · s act + β · s ngram + γ · s col where S t is the score of a dialogue state just before the update, s act the score of a dialogue act, s ngram the score concerning the probability of a dialogue act type sequence, s col the score concerning the col-location probability of dialogue states and dialogue acts, and α, β, and γ are the weighting factors."
The newly created dialogue states are ordered based on the score.
"The dialogue state that has the best score is regarded as the most probable one, and the system responds to the user by referring to it."
The maximum number of dialogue states is needed in order to drop low-score dialogue states and thereby perform the operation in real time.
"This dropping process can be considered as a beam search in view of the entire discourse process, thus we name the maximum number of dialogue states the dialogue state beam width."
"Dialogue Corpus We analyzed a corpus of dia-logues between naive users and a Japanese spoken dialogue system, which were collected in acousti-cally insulated booths."
The task domain was meet-ing room reservation.
Subjects were instructed to reserve a meeting room on a certain date from a cer-tain time to a certain time.
"As a speech recognition engine, Julius3.1p1[REF_CITE]was used with its attached acoustic model."
"For the language model, we used a trigram trained from randomly generated texts of acceptable phrases."
"For system response, NTT’s speech synthesis engine FinalFluet[REF_CITE]was used."
"The system had a vocabulary of 168 words, each registered with a category and a semantic feature in its lexicon."
The system used hand-crafted rules for discourse understanding.
"The corpus consists of 240 dialogues from 15 subjects (10 males and 5 females), each one performing 16 dialogues."
Dialogues that took more than three min-utes were regarded as failures.
The task completion rate was 78.3% (188/240).
"Extraction of Statistical Information From the transcription, we created a trigram of dialogue act types using the CMU-Cambridge Toolkit[REF_CITE]."
Figure 3 shows an example of the trigram information starting from {refer-start-time backchannel}.
The bigram information used for smoothing is also shown.
The collocation proba-bility was obtained from the recorded dialogue states and the transcription following them.
"Taking the case of the example dialogue in Figure 3, it happened that the sequence {refer-start-time backchannel refer-end-time} does not appear in the corpus; thus, the probability is calculated based on the bigram probability using the backoff weight, which is 0.006."
The trigram probability for {refer-end-time backchannel refer-end-time} is 0.031.
"The collocation probability of the sequence ds1 + da3 → ds3 fits collocation pattern 12, where a slot having no value was changed."
"The sequence ds2 + da3 → ds4 fits collocation pattern 17, where a slot having a value was changed to have a differ-ent value."
"The probabilities were 0.155 and 0.009, respectively."
"By the simple adding of the two proba-bilities in common logarithms in each case, ds3 has the probability score -3.015 and ds4 -3.549, sug-gesting that the sequence ds3 is the most probable discourse understanding result after U2."
"To verify the effectiveness of the proposed ap-proach, we built a Japanese spoken dialogue system in the meeting reservation domain that employs the proposed discourse understanding method and per-formed dialogue experiments."
The speech recognition engine was Julius3.3p1[REF_CITE]with its attached acoustic models.
"For the language model, we made a trigram from the transcription obtained from the corpora."
The system had a vocabulary of 243.
The recognition engine outputs 5-best recognition hypotheses.
"This time, values for s act , s ngram , s col are the logarithm of the inverse number of n-best ranks, [Footnote_6] the log like-lihood of dialogue act type trigram probability, and the common logarithm of the collocation probabil-ity, respectively."
"6 In this experiment, only the acoustic score of a dialogue act was considered."
"For the experiment, weighting fac-tors are all set to one (α = β = γ = 1)."
The di-alogue state beam width was 15.
The speech recognition accuracy (word error rate) was 65.18%.
Dialogues that took more than five minutes were regarded as failures.
The task com-pletion rate was 88.3% (226/256). [Footnote_7]
"7 It should be noted that due to the creation of an enormous number of dialogue states in discourse understanding, the pro-posed system takes a few seconds to respond after the user in-put."
"From all user speech intervals, the number of times that dialogue states below second place be-came first place was 120 (7.68%), showing a relative frequency of shuffling within the dialogue states."
The main reason that we developed the proposed corpus-based discourse understanding method was that it is difficult to manually create rules to deal with multiple dialogue states.
"It is yet to be exam-ined, however, whether holding multiple dialogue states is really effective for accurate discourse un-derstanding."
"To verify that holding multiple dialogue states is effective, we fixed the speech recognizer’s output to 1-best, and studied the system performance changes when the dialogue state beam width was changed from 1 to 30."
"When the dialogue state beam width is too large, the computational cost becomes high and the system cannot respond in real time."
We therefore selected 30 for empirical reasons.
The task domain and other settings were the same as in the previous experiment except for the dialogue state beam width changes.
"Each subject was instructed to reserve the same meeting room twice, once with the 1-beam-width system and again with 30-beam-width system."
The order of what room to reserve and what system to use was randomized.
The speech recognition accuracy was 69.17%.
Di-alogues that took more than five minutes were re-garded as failures.
"The task completion rates for the 1-beam-width system and the 30-beam-width sys-tem were 88.3% and 91.0%, and the average task completion times were 107.66 seconds and 95.86 seconds, respectively."
"A statistical hypothesis test showed that times taken to carry out a task with the 30-beam-width system are significantly shorter than those with the 1-beam-width system (Z = −2.01, p &lt; .05)."
"In this test, we used a kind of censored mean computed by taking the mean of the times only for subjects that completed the tasks with both systems."
The population distribution was estimated by the bootstrap method[REF_CITE].
"It may be possible to evaluate the discourse understanding by comparing the best dialogue state with the reference dialogue state, and calculate a metric such as the CER (concept error rate)[REF_CITE]do; however it is not clear whether the discourse understanding can be evaluated this way, since it is not certain whether the CER correlates closely with the system’s performance[REF_CITE]."
"Therefore, this time, we used the task completion time and the task completion rate for comparison."
Cost of creating the discourse understanding component The best task completion rate in the ex-periments was 91.0% (the case of 1-best recognition input and a 30 dialogue state beam width).
This high rate suggests that the proposed approach is effective in reducing the cost of creating the discourse un-derstanding component in that no hand-crafted rules are necessary.
"For statistical discourse understand-ing, an initial system, e.g., a system that employs the proposed approach with only s act for scoring the dialogue states, is needed in order to create the di-alogue corpus; however, once it has been made, the creation of the discourse understanding component requires no expert knowledge."
Effectiveness of holding multiple dialogue states The result of the examination of dialogue state beam width changes suggests that holding multiple dia-logue states shortens the task completion time.
"As far as task-oriented spoken dialogue systems are concerned, holding multiple dialogue states con-tributes to the accuracy of discourse understanding."
We proposed a new discourse understanding method that orders multiple dialogue states created from multiple dialogue states and the succeeding speech understanding results based on statistical informa-tion obtained from dialogue corpora.
"The results of the experiments show that our approach is effective in reducing the cost of creating the discourse under-standing component, and the advantage of keeping multiple dialogue states was also shown."
There still remain several issues that we need to explore.
"These include the use of statistical informa-tion other than the probability of a dialogue act type sequence and the collocation probability of dialogue states and dialogue acts, the optimization of weight-ing factors α, β, γ, other default parameters that we used in the experiments, and more experiments in larger domains."
"Despite these issues, the present re-sults have shown that our approach is promising."
This paper discusses the challenges and pro-poses a solution to performing information re-trieval on the Web using Chinese natural language speech query.
The main contribution of this re-search is in devising a divide-and-conquer strategy to alleviate the speech recognition errors.
It uses the query model to facilitate the extraction of main core semantic string (CSS) from the Chinese natu-ral language speech query.
"It then breaks the CSS into basic components corresponding to phrases, and uses a multi-tier strategy to map the basic components to known phrases in order to further eliminate the errors."
The resulting system has been found to be effective.
"We are entering an information era, where infor-mation has become one of the major resources in our daily activities."
"With its wide spread adoption, Internet has become the largest information wealth for all to share."
"Currently, most (Chinese) search engines can only support term-based information retrieval, where the users are required to enter the queries directly through keyboards in front of the computer."
"However, there is a large segment of population in China and the rest of the world who are illiterate and do not have the skills to use the computer."
They are thus unable to take advantage of the vast amount of freely available information.
"Since almost every person can speak and under-stand spoken language, the research on “(Chinese) natural language speech query retrieval” would enable average persons to access information using the current search engines without the need to learn special computer skills or training."
"They can sim-ply access the search engine using common de-vices that they are familiar with such as the telephone, PDA and so on."
"In order to implement a speech-based informa-tion retrieval system, one of the most important challenges is how to obtain the correct query terms from the spoken natural language query that con-vey the main semantics of the query."
This requires the integration of natural language query process-ing and speech recognition research.
Natural language query processing has been an active area of research for many years and many techniques have been developed[REF_CITE].
"Most of these techniques, however, focus only on written language, with few devoted to the study of spoken language query processing."
Speech recognition involves the conversion of acoustic speech signals to a stream of text.
"Because of the complexity of human vocal tract, the speech signals being observed are different, even for mul-tiple utterances of the same sequence of words by the same pers[REF_CITE]."
"Furthermore, the speech signals can be influenced by the differences across different speakers, dialects, transmission distortions, and speaking environments."
These have contributed to the noise and variability of speech signals.
"As one of the main sources of er-rors in Chinese speech recognition come from sub-stituti[REF_CITE], in which a wrong but similar sounding term is used in place of the correct term, confusion matrix has been used to record confused sound pairs in an attempt to elimi-nate this error."
Confusion matrix has been em-ployed effectively in spoken document retrieval ([REF_CITE]and[REF_CITE]) and to minimize speech recognition errors[REF_CITE].
"However, when such method is used di-rectly to correct speech recognition errors, it tends to bring in too many irrelevant terms[REF_CITE]."
"Because important terms in a long document are often repeated several times, there is a good chance that such terms will be correctly recognized at least once by a speech recognition engine with a reason-able level of word recognition rate."
Many spoken document retrieval (SDR) systems took advantage of this fact in reducing the speech recognition and matching errors[REF_CITE].
"In contrast to SDR, very little work has been done on Chinese spoken query processing (SQP), which is the use of spoken que-ries to retrieval textual documents."
"Moreover, spo-ken queries in SQP tend to be very short with few repeated terms."
"In this paper, we aim to integrate the spoken language and natural language research to process spoken queries with speech recognition errors."
The main contribution of this research is in devising a divide-and-conquer strategy to alleviate the speech recognition errors.
It first employs the Chinese query model to isolate the Core Semantic String (CSS) that conveys the semantics of the spoken query.
"It then breaks the CSS into basic compo-nents corresponding to phrases, and uses a multi-tier strategy to map the basic components to known phrases in a dictionary in order to further eliminate the errors."
"In the rest of this paper, an overview of the pro-posed approach is introduced in Section 2."
"Section 3 describes the query model, while Section 4 out-lines the use of multi-tier approach to eliminate errors in CSS."
Section 5 discusses the experimental setup and results.
"Finally, Section 6 contains our concluding remarks."
There are many challenges in supporting surfing of Web by speech queries.
"One of the main challenges is that the current speech recognition technology is not very good, especially for average users that do not have any speech trainings."
"For such unlimited user group, the speech recognition engine could achieve an accuracy of less than 50%."
"Because of this, the key phrases we derived from the speech query could be in error or missing the main seman-tic of the query altogether."
This would affect the effectiveness of the resulting system tremendously.
"Given the speech-to-text output with errors, the key issue is on how to analyze the query in order to grasp the Core Semantic String (CSS) as accurately as possible."
CSS is defined as the key term se-quence in the query that conveys the main seman-tics  of the ! &quot; query  .
"For # $ % &amp; () example  , given the query:  “ ” (Please tell me the information on how the U.S. separates the most-favored-nation status from human rights is-sue in china)."
The CSS in the query is underlined.
"We can segment the CSS into several basic com-ponents that correspond to key  concepts + such as: * (U.S.), ! &quot; # (China $ ), (human rights issue % ), &amp; (the most-favored-nation status) and (separate)."
"Because of the difficulty in handling speech recognition errors involving multiple segments of CSSs, we limit our research to queries that contain only one CSS string."
"However, we allow a CSS to include multiple basic components as depicted in the above example."
This is reasonable as most que-ries posed by the users on the Web tend to be short with only a few characters[REF_CITE].
Thus the accurate extraction of CSS and its separation into basic components is essential to alleviate the speech recognition errors.
"First of all, isolating CSS from the rest of speech enables us to ignore errors in other parts of speech, such as the greetings and polite remarks, which have no effects on the outcome of the query."
"Second, by separating the CSS into basic components, we can limit the propagation of errors, and employ the set of known phrases in the domain to help correct the errors in these components separately."
"To achieve this, we process the query in three main stages as illustrated in Figure 1."
"First, given the user’s oral query, the system uses a speech rec-ognition engine to convert the speech to text."
"Sec-ond, we analyze the query using a query model (QM) to extract CSS from the query with mini-mum errors."
QM defines the structures and some of the standard phrases used in typical queries.
"Third, we divide the CSS into basic components, and employ a multi-tier approach to match the ba- sic components to the nearest known phrases in order to correct the speech recognition errors."
The aim here is to improve recall without excessive lost in precision.
The resulting key components are then used as query to standard search engine.
The following sections describe the details of our approach.
Query model (QM) is used to analyze the query and extract the core semantic string (CSS) that contains the main semantic of the query.
There are two main components for a query model.
"The first is query component dictionary, which is a set of phrases that has certain semantic functions, such as the polite remarks, prepositions, time etc."
"The other component is the query structure, which de-fines a sequence of acceptable semantically tagged tokens, such as “Begin, Core Semantic String, Question Phrase, and End”."
Each query structure also includes its occurrence probability within the query corpus.
Table 2 gives some examples of query structures.
"In order to come up with a set of generalized query structures, we use a query log of typical queries posed by users."
"The query log consists of 557 que-ries, collected from twenty-eight human subjects at the Shanghai Jiao Tong University[REF_CITE]."
Each subject is asked to pose 20 separate queries to retrieve general information from the Web.
"After analyzing the queries, we derive a query model comprising 51 query structures and a set of query components."
"For each query structure, we compute its probability of occurrence, which is used to determine the more likely structure con-taining CSS in case there are multiple CSSs found."
"As part of the analysis of the query log, we classify the query components into ten classes, as listed in Table 1."
These ten classes are called semantic tags.
They can be further divided into two main catego-ries: the closed class and open class.
Closed classes are those that have relatively fixed word lists.
"These include question phrases, quantifiers, polite remarks, prepositions, time and commonly used verb and subject-verb phrases."
We collect all the phrases belonging to closed classes from the query log and store them in the query component diction-ary.
"The open class is the CSS, which we do not know in advance."
"CSS typically includes person’s names, events and country’s names etc."
"Given the set of sample queries, a heuristic rule-based approach is used to analyze the queries, and break them into basic components with assigned semantic tags by matching the words listed in Ta-ble 1."
Any sequences of words or phrases not found in the closed class are tagged as CSS (with Semantic Tag 9).
We can thus derive the query structures of the form given in Table 2.
"Due to speech recognition errors, we do not expect the query components and hence the query struc-ture to be recognized correctly."
"Instead, we parse the query structure in order to isolate and extract CSS."
"To facilitate this, we employ the Finite State Automata (FSA) to model the query structure."
"FSA models the expected sequences of tokens in typical queries and annotate the semantic tags, including CSS."
A FSA is defined for each of the 51 query structures.
An example of FSA is given in Figure 2.
"Because CSS is an open set, we do not know its content in advance."
"Instead, we use the following two rules to determine the candidates for CSS: (a) it is an unknown string not present in the Query Component Dictionary; and (b) its length is not less than two, as the average length of concepts in Chinese is greater than one[REF_CITE]."
"At each stage of parsing the query using FSA[REF_CITE], we need to make decision on which state to proceed and how to handle unex-pected tokens in the query."
"Thus at each stage, FSA needs to perform three functions: a) Goto function: It maps a pair consisting of a state and an input symbol into a new state or the fail state."
"We use G(N,X) =N’ to define the goto function from State N to State N’, given the occurrence of token X. b) Fail function: It is consulted whenever the goto function reports a failure when encoun-tering an unexpected token."
"We use f(N) =N’ to represent the fail function. c) Output function: In the FSA, certain states are designated as output states, which indi-cate that a sequence of tokens has been found and are tagged with the appropriate semantic tag."
"To construct a goto function, we begin with a graph consisting of one vertex which represents State 0.We then enter each token X into the graph by adding a directed path to the graph that begins at the start state."
"New vertices and edges are added to the graph so that there will be, starting at the start state, a path in the graph that spells out the token X. The token X is added to the output func-tion of the state at which the path terminates."
"For example, suppose that our Query Component Dictionary  consists of seven phrases as follows  : (please help me);  (about  );  “ (some); (news); (collect); (tell me) ; (what do you have)”."
Adding these tokens into the graph will result in a FSA as shown in Figure 2.
"The  path from State 0 to State 3 spells(Please help me)”, and onout the phrase “ with semantic tag 6."
"Similarly, the output of “  completion of this path, we associate its output (some)” is associated with State 5, and semantic tag 4, and so on."
We now use an example to illustrate the process of parsing the query.
Suppose  the  speech query: ” user  issues a ” (please help me to collect some information about Bin Laden).
"However, the result of speech (help  ) recognition with errors is: ” (please) (me) (receive) (send) (some) (about) (half) (pull) (light) (of) (news)”."
Note that there are 4 mis-recognized characters which are underlined.
The FSA begins with State 0.
"When the system encounters the sequence of characters (please) (help) (me), the state changes from 0 to 1, 2 and eventually to 3."
"At State 3, the system recog-nizes a polite remark phrase and output a token with semantic tag 6."
"Next, the system meets the character (receive), it will transit[REF_CITE]because of g(0, )=10."
"When the system sees the next character (send), which does not have a corresponding transition rule, the goto function reports a failure."
"Because the length of the string is 2 and the string is not in the Query Component Dictionary, the semantic tag 9 is assigned to token” ” according to the defi-nition of CSS."
"By repeating the above process, we obtain the following   result  : 6 49 7 9 3"
Here the semantic tags are as defined in Table 1.
"It is noted that because of speech recognition errors, the system detected two CSSs, and both of them contain speech recognition errors."
"Given that we may find multiple CSSs, the next stage is to analyze the CSSs found along with their surrounding context in order to determine the most probable CSS."
The approach is based on the prem-ise that choosing the best sense for an input vector amounts to choosing the most probable sense given that vector.
"The input vector i has three compo-nents: left context (L i ), the CSS itself (CSS i ), and right context (R i )."
"The probability of such a struc-ture occurring in the Query Model is as follows: n s i = (C ij * p j ) (1) j=0 where C ij is set to 1 if the input vector i (L i , R i ) matches the two corresponding left and right CSS context of the query structure j, and 0 otherwise. p j is the possibility of occurrence of the j th query structure, and n is the total number of the structures in the Query Model."
Note that Equation (1) gives a detected CSS higher weight if it matches to more query structures with higher occurrence probabili-ties.
We simply select the best CSS i such that arg max (s i ) according to Eqn(1). i
"For illustration, let’s consider the above example with 2 detected CSSs."
"The two CSS vectors are: [6, 9, 4] and [7, 9, 3]."
"From the Query Model, we know that the probability of occurrence, p j , of structure [6, 9, 4] is 0, and that of structure [7, 9, 3] is 0.03, with the latter matches to only one struc-ture."
Hence the s i values for them are 0 and 0.03 respectively.
"Thus the most probable core semantic structure is [7, 9, 3] and the CSS “ (half) (pull) (light)” is extracted."
"Because of speech recognition error, the CSS ob-tained is likely to contain error, or in the worse case, missing the main semantics of the query alto-gether."
We now discuss how we alleviate the errors in CSS for the former case.
"We will first break the CSS into one or more basic semantic parts, and then apply the multi-tier method to map the query components to known phrases."
"In many cases, the CSS obtained may be made up of several semantic components equivalent to base noun phrases."
Here we employ a technique based on Chinese cut marks[REF_CITE]to perform the segmentation.
The Chinese cut marks are tokens that can separate a Chinese sentence into several semantic parts.
"By separating the CSS into basic key components, we can limit the propagation of errors."
"In order to further eliminate the speech recognition errors, we propose a multi-tier approach to map the basic components in CSS into known phrases by using a combination of matching techniques."
"To do this, we need to build up a phrase dictionary con-taining typical concepts used in general and spe-cific domains."
Most basic CSS components should be mapped to one of these phrases.
"Thus even if a basic component contains errors, as long as we can find a sufficiently similar phrase in the phrase dic-tionary, we can use this in place of the erroneous CSS component, thus eliminating the errors."
"We collected a phrase dictionary containing about 32,842 phrases, covering mostly base noun phrase and named entity."
The phrases are derived from two sources.
We first derived a set of com-mon phrases from the digital dictionary and the logs in the search engine used at the Shanghai Jiao Tong University.
We also derived a set of domain specific phrases by extracting the base noun phrases and named entities from the on-line news articles obtained during the period.
This approach is reasonable as in practice we can use recent web or news articles to extract concepts to update the phrase dictionary.
"Given the phrase dictionary, the next problem then is to map the basic CSS components to the nearest phrases in the dictionary."
"As the basic components may contain errors, we cannot match them exactly just at the character level."
We thus propose to match each basic component with the known phrases in the dictionary at three levels: (a) character level; (b) syllable string level; and (c) confusion syllable string level.
The purpose of matching at levels b and c is to overcome  thehomophone problem in CSS.
"For example, “ (Laden)” is wrongly recognized as “ (pull lamp)” by the speech recognition engine."
"Such er-rors cannot be re-solved at the character matching level, but it can probably be matched at the syllable string level."
The confusion matrix is used to further reduce the effect of speech recognition errors due to similar sounding characters.
"To account for possible errors in CSS compo-nents, we perform similarity, instead of exact, matching at the three levels."
"Given the basic CSS component q i , and a phrase c j in the dictionary, we compute:"
"Sim(q i ,c i ) ="
"LCS(q i ,c i ) * LCS(q i ,c i )"
"M k (2) max{| q i |,| c i |} k=0 where LCS(q i ,c j ) gives the number of characters/ syllable matched between q i and c i in the order of their appearance using the longest common subse-quence matching (LCS) algorithm[REF_CITE]."
"M k is introduced to accounts for the similar-ity between the two matching units, and is depend-ent on the level of matching."
"If the matching is performed at the character or syllable string levels, the basic matching unit is one character or one syl-lable and the similarity between the two matching units is 1."
"If the matching is done at the confusion syllable string level, M k is the corresponding coef-ficients in the confusion matrix."
"Hence LCS (q i ,c j ) gives the degree of match between q i and c j , nor-malized by the maximum length of q i or c j ; and Σ M gives the degree of similarity between the units being matched."
"The three level of matching also ranges from be-ing more exact at the character level, to less exact at the confusion syllable level."
"Thus if we can find a relevant phrase with sim(q i ,c j )&gt; at the higher character level, we will not perform further match-ing at the lower levels."
"Otherwise, we will relax the constraint to perform the matching at succes-sively lower levels, probably at the expense of pre-cision."
The detail of algorithm is listed as follows:
"Input: Basic CSS Component, q i a. Match q i with phrases in dictionary at character level using Eqn.(2). b."
"If we cannot find a match, then match q i with phrases at the syllable level using Eqn.(2). c."
"If we still cannot find a match, match q i with phrases at the confusion syllable level using Eqn.(2). d."
"If we found a match, set q’ i =c j ; otherwise set For  example, given a query: “   q’ i =q i . ” (please tell me some news about Iraq)."
"If  the query is wrongly recognized as “ could correctly extract the CSS “ (Iraq) from this mis-recognized query, then we could ig-nore the speech recognition errors in other parts of the above query."
"Even if there are errors in the CSS extracted, such as “ (chen)  (waterside)” instead of “  (chen shui bian)”, we could ap-ply the syllable string level matching to correct the homophone errors."
"For CSS errors such as “ ! (corrupt) &quot; (usually)” instead of the correct CSS “ # $ % (Taliban)”, which could not be corrected at the syllable string matching level, we could ap-ply the confusion syllable string matching to over-come this error."
"As our system aims to correct the errors and ex-tract CSS components in spoken queries, it is im-portant to demonstrate that our system is able to handle queries of different characteristics."
"To this end, we devised two sets of test queries as follows."
"The test results show that by using our query model, we could correctly extract 99% and 96% of CSSs from the spoken queries for the short and long query category respectively."
"The errors are mainly due to the wrong tagging of some query components, which caused the query model to miss the correct query structure, or match to a wrong   structure. # $ % For example  :”given(pleasethetellqueryme “some news about Taliban   )."
If it  is wrongly $ recognized %  as: 9 7 9 10 which is a nonsensical sentence.
"Since the prob-abilities of occurrence both query structures [0,9,7] and [7,9,10] are 0, we could not find the CSS at all. the last query component “  (news)” to “  This error is mainly due to the mis-recognition of (afternoon)”."
"It confuses the Query Model, which could not find the correct CSS."
The overall results indicate that there are fewer errors in short queries as such queries contain only one CSS component.
This is encouraging as in practice most users issue only short queries.
"In order to test the accuracy of extracting basic query components, we asked one subject to manu-ally divide the CSS into basic components, and used that as the ground truth."
"We compared the following two methods of extracting CSS compo-nents: a) As a baseline, we simply performed the stan-dard stop word removal and divided the query into components with the help of a dictionary."
"However, there is no attempt to correct the speech recognition errors in these components."
Here we assume that the natural language query is a bag of words with stop word removed[REF_CITE].
"Currently, most search engines are based on this approach. b) We applied our query model to extract CSS and employed the multi-tier mapping approach to extract and correct the errors in the basic CSS components."
"Tables 3 and 4 give the comparisons between Methods (a) and (b), which clearly show that our method outperforms the baseline method by over 20.2% and 20 % in F 1 measure for the short and long queries respectively. 



"
"Although research on natural language query proc-essing and speech recognition has been carried out for many years, the combination of these two ap-proaches to help a large population of infrequent users to “surf the web by voice” has been relatively recent."
"This paper outlines a divide-and-conquer approach to alleviate the effect of speech recogni-tion error, and in extracting key CSS components for use in a standard search engine to retrieve rele-vant documents."
The main innovative steps in our system are: (a) we use a query model to isolate CSS in speech queries; (b) we break the CSS into basic components; and (c) we employ a multi-tier approach to map the basic components to known phrases in the dictionary.
The tests demonstrate that our approach is effective.
The work is only the beginning.
Further research can be carried out as follows.
"First, as most of the queries are about named entities such as the per-sons or organizations, we need to perform named entity analysis on the queries to better extract its structure, and in mapping to known named entities."
"Second, most speech recognition engine will return a list of probable words for each syllable."
This could be incorporated into our framework to facili-tate multi-tier mapping.
We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems.
"Un-like previous studies that focus on user’s knowledge or typical kinds of users, the user model we propose is more compre-hensive."
"Specifically, we set up three di-mensions of user models: skill level to the system, knowledge level on the tar-get domain and the degree of hastiness."
"Moreover, the models are automatically derived by decision tree learning using real dialogue data collected by the sys-tem."
We obtained reasonable classifica-tion accuracy for all dimensions.
Dia-logue strategies based on the user model-ing are implemented in Kyoto city bus in-formation system that has been developed at our laboratory.
Experimental evalua-tion shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increas-ing the dialogue duration for skilled users.
A spoken dialogue system is one of the promising applications of the speech recognition and natural language understanding technologies.
A typical task of spoken dialogue systems is database retrieval.
Some IVR (interactive voice response) systems us-ing the speech recognition technology are being put into practical use as its simplest form.
"According to the spread of cellular phones, spoken dialogue sys-tems via telephone enable us to obtain information from various places without any other special appa-ratuses."
"However, the speech interface involves two in-evitable problems: one is speech recognition er-rors, and the other is that much information can-not be conveyed at once in speech communications."
"Therefore, the dialogue strategies, which determine when to make guidance and what the system should tell to the user, are the essential factors."
"To cope with speech recognition errors, several confirma-tion strategies have been proposed: confirmation management methods based on confidence measures of speech recognition results[REF_CITE]and implicit con-firmation that includes previous recognition results into system’s prompts[REF_CITE]."
"In terms of determining what to say to the user, several stud-ies have been done not only to output answers cor-responding to user’s questions but also to generate cooperative responses[REF_CITE]."
"Furthermore, methods have also been proposed to change the di-alogue initiative based on various cues[REF_CITE]."
"Nevertheless, whether a particular response is co-operative or not depends on individual user’s char-acteristics."
"For example, when a user says nothing, the appropriate response should be different whether he/ she is not accustomed to using the spoken dia-logue systems or he/she does not know much about the target domain."
"Unless we detect the cause of the silence, the system may fall into the same situation repeatedly."
"In order to adapt the system’s behavior to individ-ual users, it is necessary to model the user’s patterns[REF_CITE]."
Most of conventional stud-ies on user models have focused on the knowledge of users.
Others tried to infer and utilize user’s goals to generate responses adapted to the user (van[REF_CITE]).
"However, these studies depend on knowledge of the target domain greatly, and therefore the user models need to be deliberated manually to be ap-plied to new domains."
"Moreover, they assumed that the input is text only, which does not contain errors."
"On the other hand, spoken utterances include various information such as the interval between utterances, the presence of barge-in and so on, which can be utilized to judge the user’s character."
These features also possess generality in spoken dialogue systems because they are not dependent on domain-specific knowledge.
We propose more comprehensive user models to generate user-adapted responses in spoken dialogue systems taking account of all available information specific to spoken dialogue.
The models change both the dialogue initiative and the generated re-sponse.
"In[REF_CITE], typical users’ be-haviors are defined to evaluate spoken dialogue sys-tems by simulation, and stereotypes of users are as-sumed such as patient, submissive and experienced."
"We introduce user models not for defining users’ be-haviors beforehand, but for detecting users’ patterns in real-time interaction."
"We define three dimensions in the user models: ‘skill level to the system’, ‘knowledge level on the target domain’ and ‘degree of hastiness’."
The for-mer two are related to the strategies in manage-ment of the initiative and the response generation.
"These two enable the system to adaptively gener-ate dialogue management information and domain-specific information, respectively."
The last one is used to manage the situation when users are in hurry.
"Namely, it controls generation of the additive con-tents based on the former two user models."
Handling such a situation becomes more crucial in speech communications using cellular phones.
The user models are trained by decision tree learning algorithm using real data collected from the Kyoto city bus information system.
"Then, we imple-ment the user models and adaptive dialogue strate-gies on the system and evaluate them using data col-lected with 20 novice users."
"We have developed the Kyoto City Bus Information System, which locates the bus a user wants to take, and tells him/her how long it will take before its arrival."
The system can be accessed via telephone including cellular phones [Footnote_1] .
"From any places, users can easily get the bus information that changes ev-ery minute."
"Users are requested to input the bus stop to get on, the destination, or the bus route number by speech, and get the corresponding bus informa-tion."
The bus stops can be specified by the name of famous places or public facilities nearby.
Figure 1 shows a simple example of the dialogue.
Figure 2 shows an overview of the system.
The system operates by generating VoiceXML scripts dynamically.
"The real-time bus information database is provided on the Web, and can be ac-cessed via Internet."
"Then, we explain the modules in the following."
VWS (Voice Web Server)
The Voice Web Server drives the speech recog-nition engine and the TTS (Text-To-Speech) module according to the specifications by the generated VoiceXML.
"Speech Recognizer The speech recognizer decodes user utterances based on specified grammar rules and vocabu-lary, which are defined by VoiceXML at each dialogue state."
The dialogue manager generates response sen-tences based on speech recognition results (bus stop names or a route number) received from the VWS.
"If sufficient information to locate a bus is obtained, it retrieves the corresponding information from the real-time bus information database."
"VoiceXML Generator This module dynamically generates VoiceXML files that contain response sentences and spec-ifications of speech recognition grammars, which are given by the dialogue manager."
User Model Identifier This module classifies user’s characters based on the user models using features specific to spoken dialogue as well as semantic attributes.
"The obtained user profiles are sent to the dia-logue manager, and are utilized in the dialogue management and response generation."
We define three dimensions as user models listed be-low.
Skill level to the system
Knowledge level on the target domain
Degree of hastiness
"Next, we describe the response generation strategies adapted to individual users based on the proposed user models: skill level, knowledge level and hasti-ness."
"Basic design of dialogue management is based on mixed-initiative dialogue, in which the system makes follow-up questions and guidance if neces-sary while allowing a user to utter freely."
It is in-vestigated to add various contents to the system re-sponses as cooperative responses[REF_CITE].
"Such additive information is usually cooperative, but some people may feel such a response redundant."
"Thus, we introduce the user models and control the generation of additive information."
"By introduc-ing the proposed user models, the system changes generated responses by the following two aspects: dialogue procedure and contents of responses."
"In order to implement the proposed user models as a classifier, we adopt a decision tree."
It is constructed by decision tree learning algorithm
C5.0[REF_CITE]with data collected by our dialogue system.
Figure 3 shows the derived decision tree for the skill level.
We use the features listed in Figure 4.
They in-clude not only semantic information contained in the utterances but also information specific to spoken dialogue systems such as the silence duration prior to the utterance and the presence of barge-in.
"Ex-cept for the last category of Figure 4 including “at-tribute of specified bus stops”, most of the features are domain-independent."
The classification of each dimension is done for every user utterance except for knowledge level.
The model of a user can change during a dialogue.
Fea-tures extracted from utterances are accumulated as history information during the session.
Figure 5 shows an example of the system behav- ior with the proposed user models.
"The skill level is classified as being low by the decision tree, because the first user’s utterance includes only one content word."
"Then, dialogue procedure is changed to the system-initiated one."
"Similarly, the hastiness is clas-sified as being low by the decision tree, and the sys-tem includes the explanation on the dialogue pro-cedure and instruction on the expression in the re-sponses."
They are omitted if the hastiness is identi-fied as high.
We train and evaluate the decision tree for the user models using dialogue data collected by our system.
The data was collected[REF_CITE]to
"The number of the sessions (tele-phone calls) is 215, and the total number of utter-ances included in the sessions is 1492."
We anno-tated the subjective labels by hand.
The annotator judges the user models for every utterances based on recorded speech data and logs.
"The labels were given to the three dimensions described in section 3.3 among ’high’, ’indeterminable’ or ’low’."
"It is possible that annotated models of a user change dur-ing a dialogue, especially from ’indeterminable’ to ’low’ or ’high’."
The number of labeled utterances is shown in Table 1.
"Using the labeled data, we evaluated the classi-fication accuracy of the proposed user models."
All the experiments were carried out by the method of 10-fold cross validation.
"The process, in which one tenth of all data is used as the test data and the re-mainder is used as the training data, is repeated ten times, and the average of the accuracy is computed."
The result is shown in Table 2.
"The conditions #1, #2 and #3 in Table 2 are described as follows. #1:[REF_CITE]-fold cross validation is carried out per utterance. #2:[REF_CITE]-fold cross validation is carried out per session (call). #3: We calculate the accuracy under more realis-tic condition."
The accuracy is calculated not in three classes (high / indeterminable / low) but in two classes that actually affect the dia-logue strategies.
"For example, the accuracy for the skill level is calculated for the two classes: low and the others."
"As to the classification of knowledge level, the accuracy is calculated for dialogue sessions because the features such as the attribute of a specified bus stop are not ob-tained in every utterance."
"Moreover, in order to smooth unbalanced distribution of the train-ing data, a cost corresponding to the reciprocal ratio of the number of samples in each class is introduced."
"By the cost, the chance rate of two classes becomes 50%."
The difference between condition #1 and #2 is that the training was carried out in a speaker-closed or speaker-open manner.
The former shows better per-formance.
The result in condition #3 shows useful accuracy in the skill level.
"The following features play im-portant part in the decision tree for the skill level: the number of filled slots by the current utterance, presence of barge-in and ratio of no input."
"For the knowledge level, recognition result (something rec-ognized / uncertain / no input), ratio of no input and the way to specify bus stops (whether a bus stop is specified by its exact name or not) are effective."
"The hastiness is classified mainly by the three features: presence of barge-in, ratio of no input and lapsed time of the current utterance."
We evaluated the system with the proposed user models using 20 novice subjects who had not used the system.
The experiment was performed in the laboratory under adequate control.
"For the speech input, the headset microphone was used."
"First, we explained the outline of the system to sub-jects and gave the document in which experiment conditions and the scenarios were described."
We prepared two sets of eight scenarios.
Subjects were requested to acquire the bus information using the system with/without the user models.
"In the sce-narios, neither the concrete names of bus stops nor the bus number were given."
"For example, one of the scenarios was as follows: “You are in Kyoto for sightseeing."
"After visiting the Ginkakuji temple, you go to Maruyama Park."
"Supposing such a situa-tion, please get information on the bus.”"
We also set the constraint in order to vary the subjects’ hastiness such as “Please hurry as much as possible in order to save the charge of your cellular phone.”
"The subjects were also told to look over question-naire items before the experiment, and filled in them after using each system."
This aims to reduce the sub-ject’s cognitive load and possible confusion due to switching the systems[REF_CITE].
"The question-naire consisted of eight items, for example, “When the dialogue did not go well, did the system guide in-telligibly?”"
"We set seven steps for evaluation about each item, and the subject selected one of them."
"Furthermore, subjects were asked to write down the obtained information: the name of the bus stop to get on, the bus number and how much time it takes before the bus arrives."
"With this procedure, we planned to make the experiment condition close to the realistic one."
"The subjects were divided into two groups; a half (group 1) used the system in the order of “with user models ! without user models”, the other half (group 2) used in the reverse order."
The dialogue management in the system without user models is also based on the mixed-initiative di-alogue.
"The system generates follow-up questions and guidance if necessary, but behaves in a fixed manner."
"Namely, additive cooperative contents cor-responding to skill level described in section 3.2 are not generated and the dialogue procedure is changed only after recognition errors occur."
"The system with-out user models behaves equivalently to the initial state of the user models: the hastiness is low, the knowledge level is low and the skill level is high."
"All of the subjects successfully completed the given task, although they had been allowed to give up if the system did not work well."
"Namely, the task success rate is 100%."
Average dialogue duration and the number of turns in respective cases are shown in Table 3.
"Though the users had not experienced the system at all, they got accustomed to the system very rapidly."
"Therefore, as shown in Table 3, both the duration and the number of turns were decreased obviously in the latter half of the experiment in either group."
"However, in the initial half of the experiment, the group 1 completed with significantly shorter dia-logue than group 2."
This means that the incorpora-tion of the user models is effective for novice users.
Table 4 shows a ratio of utterances for which the skill level was identified as high.
The ratio is calcu-lated by dividing the number of utterances that were judged as high skill level by the number of all utter-ances in the eight sessions.
The ratio is much larger for group 1 who initially used the system with user models.
"This fact means that novice users got ac-customed to the system more rapidly with the user models, because they were instructed on the usage by cooperative responses generated when the skill level is low."
The results demonstrate that coopera-tive responses generated according to the proposed user models can serve as good guidance for novice users.
"In the latter half of the experiment, the dialogue duration and the number of turns were almost same between the two groups."
"This result shows that the proposed models prevent the dialogue from becom-ing redundant for skilled users, although generating cooperative responses for all users made the dia-logue verbose in general."
It suggests that the pro-posed user models appropriately control the genera-tion of cooperative responses by detecting characters of individual users.
We have proposed and evaluated user models for generating cooperative responses adaptively to in-dividual users.
"The proposed user models consist of the three dimensions: skill level to the system, knowledge level on the target domain and the de-gree of hastiness."
The user models are identified us-ing features specific to spoken dialogue systems as well as semantic attributes.
"They are automatically derived by decision tree learning, and all features used for skill level and hastiness are independent of domain-specific knowledge."
"So, it is expected that the derived user models can be used in other do-mains generally."
"The experimental evaluation with 20 novice users shows that the skill level of novice users was im-proved more rapidly by incorporating the user mod-els, and accordingly the dialogue duration becomes shorter more immediately."
The result is achieved by the generated cooperative responses based on the proposed user models.
The proposed user models also suppress the redundancy by changing the dia-logue procedure and selecting contents of responses.
"Thus, they realize user-adaptive dialogue strategies, in which the generated cooperative responses serve as good guidance for novice users without increas-ing the dialogue duration for skilled users."
"Pipelined Natural Language Generation (NLG) systems have grown increasingly complex as architectural modules were added to support language functionali-ties such as referring expressions, lexical choice, and revision."
This has given rise to discussions about the relative placement of these new modules in the overall archi-tecture.
"Recent work on another aspect of multi-paragraph text, discourse mark-ers, indicates it is time to consider where a discourse marker insertion algorithm fits in."
"We present examples which suggest that in a pipelined NLG architecture, the best approach is to strongly tie it to a revi-sion component."
"Finally, we evaluate the approach in a working multi-page system."
"Historically, work on NLG architecture has focused on integrating major disparate architectural modules such as discourse and sentence planners and sur-face realizers."
"More recently, as it was discovered that these components by themselves did not cre-ate highly readable prose, new types of architectural modules were introduced to deal with newly desired linguistic phenomena such as referring expressions, lexical choice, revision, and pronominalization."
"Adding each new module typically entailed that an NLG system designer would justify not only the reason for including the new module (i.e., what lin- guistic phenomena it produced that had been pre-viously unattainable) but how it was integrated into their architecture and why its placement was reason-ably optimal (cf.,[REF_CITE], pp. 4–7)."
"At the same time,[REF_CITE]argued that im-plemented NLG systems were converging toward a de facto pipelined architecture (Figure 1) with minimal-to-nonexistent feedback between modules."
"Although several NLG architectures were pro-posed in opposition to such a linear arrangement[REF_CITE], these re-search projects have not continued while pipelined architectures are still actively being pursued."
"In addition, Reiter concludes that although com-plete integration of architectural components is the-oretically a good idea, in practical engineering terms such a system would be too inefficient to operate and too complex to actually implement."
"Significantly, Reiter states that fully interconnecting every module would entail constructing N ( N 1) interfaces be-tween them."
"As the number of modules rises (i.e., as the number of large-scale features an NLG engineer wants to implement rises) the implementation cost rises exponentially."
"Moreover, this cost does not in-clude modifications that are not component specific, such as multilingualism."
"As text planners scale up to produce ever larger texts, the switch to multi-page prose will introduce new features, and consequentially the number of architectural modules will increase."
"For example, Mooney’s E EG system[REF_CITE], which cre-ated a full-page description of the Three-Mile Island nuclear plant disaster, contains components for dis-course knowledge, discourse organization, rhetori- cal relation structuring, sentence planning, and sur-face realization."
"Similarly, the S TORY B OOK system[REF_CITE], which generated 2 to 3 pages of narrative prose in the Little Red Riding Hood fairy tale domain, contained seven separate components."
"This paper examines the interactions of two lin-guistic phenomena at the paragraph level: revision (specifically, clause aggregation, migration and de-motion) and discourse markers."
Clause aggregation involves the syntactic joining of two simple sen-tences into a more complex sentence.
Discourse markers link two sentences semantically without necessarily joining them syntactically.
"Because both of these phenomena produce changes in the text at the clause-level, a lack of coordination between them can produce interference effects."
We thus hypothesize that the architectural mod-ules corresponding to revision and discourse marker selection should be tightly coupled.
"We then first summarize current work in discourse markers and revision, provide examples where these phenomena interfere with each other, describe an implemented technique for integrating the two, and report on a preliminary system evaluation."
"Discourse markers, or cue words, are single words or small phrases which mark specific semantic rela-tions between adjacent sentences or small groups of sentences in a text."
"Typical examples include words like however, next, and because."
Discourse markers pose a problem for both the parsing and generation of clauses in a way similar to the problems that re-ferring expressions pose to noun phrases: changing the lexicalization of a discourse marker can change the semantic interpretation of the clauses affected.
Recent work in the analysis of both the distribu-tion and role of discourse markers has greatly ex-tended our knowledge over even the most expansive previous accounts of discourse connectives[REF_CITE]from previous decades.
"For example, using a large scale corpus analysis and human sub-jects employing a substitution test over the corpus sentences containing discourse markers,[REF_CITE]distilled a taxonomy of individual lexical discourse markers and 8 binary-valued fea-tures that could be used to drive a discourse marker selection algorithm."
"Other work often focuses on particular semantic categories, such as temporal discourse markers."
"For instance,[REF_CITE]attempted to create declar-ative lexicons that contain applicability conditions and other constraints to aid in the process of dis-course marker selection."
"Other theoretical research consists, for example, of adapting existing grammat-ical formalisms such as TAGs[REF_CITE]for discourse-level phenomena."
"Alternatively, there are several implemented sys-tems that automatically insert discourse markers into multi-sentential text."
"In an early instance,[REF_CITE]followed Quirk’s pre-existing non-computational account of discourse connectives to produce single argumentative discourse markers inside a functional unification surface realizer (and thereby postponing lexicalization till the last possi-ble moment)."
More recent approaches have tended to move the decision time for marker lexicalization higher up the pipelined architecture.
"For example, the M OOSE system[REF_CITE]lexicalized discourse markers at the sentence planning level by pushing them directly into the lexicon."
"Similarly,[REF_CITE]produce multiple discourse markers for Patient Information Leaflets using a constraint-based method applied to RST trees during sentence planning."
"Finally, in the C IRC -S IM intelligent tutoring sys-tem[REF_CITE]that generates connected di- alogues for students studying heart ailments, dis-course marker lexicalization has been pushed all the way up to the discourse planning level."
"In this case, C IRC -S IM lexicalizes discourse markers inside of the discourse schema templates themselves."
"Given that these different implemented discourse marker insertion algorithms lexicalize their markers at three distinct places in a pipelined NLG archi-tecture, it is not clear if lexicalization can occur at any point without restriction, or if it is in fact tied to the particular architectural modules that a system designer chooses to include."
"The answer becomes clearer after noting that none of the implemented discourse marker algorithms de-scribed above have been incorporated into a com-prehensive NLG architecture containing additional significant components such as revision (with the exception of M OOSE ’s lexical choice component, which Stede considers to be a submodule of the sen-tence planner)."
"Revision (or clause aggregation) is principally con-cerned with taking sets of small, single-proposition sentences and finding ways to combine them into more fluent, multiple-proposition sentences."
"Sen-tences can be combined using a wide range of differ-ent syntactic forms, such as conjunction with “and”, making relative clauses with noun phrases common to both sentences, and introducing ellipsis."
"Typically, revision modules arise because of dis-satisfaction with the quality of text produced by a simple pipelined NLG system."
"As noted[REF_CITE], there is a wide variety in re-vision definitions, objectives, operating level, and type."
"Similarly,[REF_CITE]tried to distinguish between different revision parameters by having users perform revision thought experiments and proposing rules in RST form which mimic the behavior they observed."
"While neither of these were implemented revi-sion systems, there have been several attempts to im-prove the quality of text from existing NLG systems."
"There are two approaches to the architectural posi-tion of revision systems: those that operate on se-mantic representations before the sentence planning level, of which a prototypical example is[REF_CITE], and those placed after the sentence planner, operating on syntactic/linguistic data."
"Here we treat mainly the second type, which have typically been conceived of as “add-on” components to existing pipelined architectures."
An important implication of this architectural order is that the revision compo-nents expect to receive lexicalized sentence plans.
"Of these systems, Robin’s S TREAK system[REF_CITE]is the only one that accepts both lex-icalized and non-lexicalized data."
"After a sentence planner produces the required lexicalized informa-tion that can form a complete and grammatical sen-tence, S TREAK attempts to gradually aggregate that data."
"It then proceeds to try to opportunistically in-clude additional optional information from a data set of statistics, performing aggregation operations at various syntactic levels."
"Because S TREAK only produces single sentences, it does not attempt to add discourse markers."
"In addition, there is no a priori way to determine whether adjacent propositions in the input will remain adjacent in the final sentence."
"The R EVISOR system[REF_CITE]takes an entire sentence plan at once and it-erates through it in paragraph-sized chunks, em-ploying clause- and phrase-level aggregation and re-ordering operations before passing a revised sen-tence plan to the surface realizer."
"However, at no point does it add information that previously did not exist in the sentence plan."
"The RTPI system[REF_CITE]takes in sets of multiple, lexicalized sentential plans over a number of medi-cal diagnoses from different critiquing systems and produces a single, unified sentence plan which is both coherent and cohesive."
"Like S TREAK , Shaw’s C ASPER system[REF_CITE]produces single sentences from sets of sen-tences and doesn’t attempt to deal with discourse markers."
C ASPER also delays lexicalization when aggregating by looking at the lexicon twice during the revision process.
This is due mainly to the effi-ciency costs of the unification procedure.
"However, C ASPER ’s sentence planner essentially uses the first lexicon lookup to find a “set of lexicalizations” be-fore eventually selecting a particular one."
An important similarity of these pipelined revi-sion systems is that they all manipulate lexical-ized representations at the clause level.
"Given that both aggregation and reordering operators may sep- arate clauses that were previously adjacent upon leaving the sentence planner, the inclusion of a re-vision component has important implications for any upstream architectural module which assumed that initially adjacent clauses would remain adjacent throughout the generation process."
"The current state of the art in NLG can be described as small pipelined generation systems that incorpo-rate some, but not all, of the available pipelined NLG modules."
"Specifically, there is no system to-date which both revises its output and inserts ap-propriate discourse markers."
"Additionally, there are no systems which utilize the latest theoretical work in discourse markers described in Section 2."
"But as NLG systems begin to reach toward multi-page text, combining both modules into a single architec-ture will quickly become a necessity if such systems are to achieve the quality of prose that is routinely achieved by human authors."
This integration will not come without con-straints.
"For instance, discourse marker insertion al-gorithms assume that sentence plans are static ob-jects."
Thus any change to the static nature of sen-tence plans will inevitably disrupt them.
"On the other hand, revision systems currently do not add in-formation not specified by the discourse planner, and do not perform true lexicalization: any new lexemes not present in the sentence plan are merely delayed lexicon entry lookups."
"Finally, because revision is potentially destructive, the sentence elements that lead to a particular discourse marker being chosen may be significantly altered or may not even exist in a post-revision sentence plan."
These factors lead to two partial order constraints on a system that both inserts discourse markers and revises at the clause level after sentence planning:
Discourse marker lexicalization cannot pre-cede revision
Revision cannot precede discourse marker lexicalization
"In the first case, assume that a sentence plan ar-rives at the revision module with discourse mark-ers already lexicalized."
Then the original discourse marker may not be appropraite in the revised sen-tence plan.
"For example, consider how the applica-tion of the following revision types requires different lexicalizations for the initial discourse markers:"
Clause Aggregation: The merging of two main clauses into one main clause and one sub-ordinate clause:
John had always liked to ride motorbikes.
"On account of this, his wife passionately hated motorbikes. )"
"John had always liked to ride motorbikes, which his wife f * on account of this j thus g passionately hated."
Reordering: Two originally adjacent main clauses no longer have the same fixed position relative to each other:
Diesel motors are well known for emitting ex-cessive pollutants.
"Furthermore, diesel is often transported unsafely."
"However, diesel motors are becoming cleaner. )"
"Diesel motors are well known for emitting ex-cessive pollutants, f * however j although g they are becoming cleaner."
"Furthermore, diesel is often transported unsafely."
Clause Demotion: Two main clauses are merged where one of them no longer has a clause structure:
The happy man went home.
"However, the man was poor. )"
The happy f * however j but g poor man went home.
"These examples show that if discourse marker lexicalization occurs before clause revision, the changes that the revision module makes can render those discourse markers undesirable or even gram-matically incorrect."
"Furthermore, these effects span a wide range of potential revision types."
"In the second case, assume that a sentence plan is passed to the revision component, which performs various revision operations before discourse mark-ers are considered."
"In order to insert appropriate dis-course markers, the insertion algorithm must access the appropriate rhetorical structure produced by the discourse planner."
"However, there is no guarantee that the revision module has not altered the initial organization imposed by the discourse planner."
"In such a case, the underlying data used for discourse marker selection may no longer be valid."
"For example, consider the following generically represented discourse plan:"
"C1: “John and his friends went to the party.” [ temporal “before” relation, time(C1, C2) ]"
"C2: “John and his friends gathered at the mall.” [ causal relation, cause(C2, C3) ]"
C3: “John had been grounded.”
"One possible revision that preserved the discourse plan might be: “Before John and his friends went to the party, they gathered at the mall since he had been grounded.”"
"In this case, the discourse marker algorithm has selected “before” and “since” as lexicalized dis-course markers prior to revision."
"But there are other possible revisions that would destroy the ordering established by the discourse plan and make the se-lected discourse markers unwieldy: “John, f * since j g who had been grounded, gathered with his friends at the mall before go-ing to the party.” “ f *"
"Since j Because g he had been grounded, John and his friends gathered at the mall and f * before j then g went to the party.”"
Reordering sentences without updating the dis-course relations in the discourse plan itself would result in many wrong or misplaced discourse marker lexicalizations.
"Given that discourse markers can-not be lexicalized before clause revision is enacted, and that clause revision may alter the original dis-course plan upon which a later discourse marker in-sertion algorithm may rely, it follows that the revi-sion algorithm should update the discourse plan as it progresses, and the discourse marker insertion al-gorithm should be responsive to these changes, thus delaying discourse marker lexicalization."
"To demonstrate the application of this problem to real world discourse, we took the S TORY B OOK[REF_CITE]"
"NLG system that generates multi-page text in the form of Little Red Riding Hood stories and New York Times articles, using a pipelined architec-ture with a large number of modules such as revisi[REF_CITE]."
"But although it was ca-pable of inserting discourse markers, it did so in an ad-hoc way, and required that the document author notice possible interferences between revision and discourse marker insertion and hard-wire the docu-ment representation accordingly."
"Upon adding a principled discourse marker selec-tion algorithm to the system, we soon noticed vari-ous unwanted interactions between revision and dis-course markers of the type described in Section 4 above."
"Thus, in addition to the other constraints al-ready considered during clause aggregation, we al-tered the revision module to also take into account the information available to our discourse marker in-sertion algorithm (in our case, intention and rhetori-cal predicates)."
We were thus able to incorporate the discourse marker selection algorithm into the revi-sion module itself.
This is contrary to most NLG systems where dis-course marker lexicalization is performed as late as possible using the modified discourse plan leaves af-ter the revision rules have reorganized all the origi-nal clauses.
"In an architecture that doesn’t consider discourse markers, a generic revision rule without access to the original discourse plan might appear like this (where type refers to the main clause syn-tax, and rhetorical type refers to its intention):"
"If type(clause1) = &lt; type &gt; type(clause2) = &lt; type &gt; subject(clause1) = subject(clause2) then make-subject-relative-clause(clause1, clause2)"
"But by making available the intentional and rhetorical information from the discourse plan, our modified revision rules instead have this form:"
"If rhetorical-type(clause1) = &lt; type &gt; rhetorical-type(clause2) = &lt; type &gt; where the function lexicalize-discourse-marker de-termines the appropriate discourse marker lexical-ization given a set of features such as those de-scribed[REF_CITE]or[REF_CITE], and update-rhetorical-relation causes the appropriate changes to be made to the running discourse plan so that future revision rules can take those alterations into account."
"S TORY B OOK takes a discourse plan augmented with appropriate low-level (i.e., unlexicalized, or conceptual) rhetorical features and produces a sen-tence plan without discarding rhetorical informa-tion."
It then revises and lexicalizes discourse mark-ers concurrently before passing the results to the sur-face realization module for production of the surface text.
"Consider the following sentences in a short text plan produced by the generation system: 1. “In this case, Mr. Curtis could no longer be tried for the shooting of his former girlfriend’s companion.” &lt; agent-action &gt; [ causal relation ] 2. “There is a five-year statute of limitations on that crime.” &lt; existential &gt; [ opposition relation ] 3. “There is no statute of limitations in murder cases.” &lt; existential &gt;"
"Without revision, a discourse marker insertion al-gorithm is only capable of adding discourse markers before or after a clause boundary: “In this case, Mr. Curtis could no longer be tried for the shooting of his former girlfriend’s compan-ion."
This is because there is a five-year statute of limitations on that crime.
"However, there is no statute of limitations in murder cases.”"
"But a revised version with access to the discourse plan and integrating discourse markers that our sys-tem generates is: “In this case, Mr. Curtis could no longer be tried for the shooting of his former girlfriend’s compan-ion, because there is a five-year statute of limita-tions on that crime even though there is no statue of limitations in murder cases.”"
"A revision module without access to the discourse plan and a method for lexicalizing discourse mark-ers will be unable to generate the second, improved version."
"Furthermore, a discourse marker insertion algorithm that lexicalizes before the revision algo-rithm begins will not have enough basis to decide and frequently produce wrong lexicalizations."
The actual implemented rules in our system (which gen-erate the example above) are consistent with the ab-stract rule presented earlier.
"Revising sentence 1 with 2: f opposition, simple g ) update-rhetorical-relation(clause1, clause2, existential, existential, current-relations)"
"Given these parameters, the discourse markers will be lexicalized as because and even though respectively, and the revision component will be able to combine all three base sentences plus the discourse markers into the single sentence shown above."
"Evaluation of multi-paragraph text generation is ex-ceedingly difficult, as empirically-driven methods are not sufficiently sophisticated, and subjective hu-man evaluations that require multiple comparisons of large quantities of text is both difficult to control for and time-consuming."
Evaluating our approach is even more difficult in that the interference between discourse markers and revision is not a highly fre- quent occurrence in multi-page text.
"For instance, in our corpora we found that these interference effects occurred 23% of the time for revised clauses and 56% of the time with discourse markers."
"In other words, almost one of every four clause revisions po-tentially forces a change in discourse marker lexi-calizations and one in every two discourse markers occur near a clause revision boundary."
"However, the “penalty” associated with incor-rectly selecting discourse markers is fairly high lead-ing to confusing sentences, although there is no cog-nitive science evidence that states exactly how high for a typical reader, despite recent work in this direc-ti[REF_CITE]."
"Furthermore, there is little agreement on exactly what constitutes a dis-course marker, especially between the spoken and written dialogue communities (e.g., many members of the latter consider “uh” to be a discourse marker)."
We thus present an analysis of the frequencies of various features from three separate New York Times articles generated by the S TORY B OOK sys-tem.
We then describe the results of running our combined revision and discourse marker module with the discourse plans used to generate them.
"While three NYT articles is not a substantial enough evaluation in ideal terms, the cost of evaluation in such a knowledge-intensive undertaking will con-tinue to be prohibitive until large-scale automatic or semiautomatic techniques are developed."
The left side of table 1 presents an analysis of the frequencies of revisions and discourse markers as found in each of the three NYT articles.
"In addition, we have indicated the number of times in our opin-ion that revisions and discourse markers co-occurred (i.e., a discourse marker was present at the junction site of the clauses being aggregated)."
"The right side of the table indicates the differ-ence between the accuracy of two different versions of the system: separate signifies the initial configu-ration of the S TORY B OOK system where discourse marker insertion and revision were performed as separate process, while integrated signifies that dis-course markers were lexicalized during revision as described in this paper."
The difference between these two numbers thus represents the number of times per article that the integrated clause aggrega-tion and discourse marker module was able to im-prove the resulting text.
Efficiency and software engineering considerations dictate that current large-scale NLG systems must be constructed in a pipeline fashion that minimizes backtracking and communication between modules.
"Yet discourse markers and revision both operate at the clause level, which leads to the potential of inter-ference effects if they are not resolved at the same lo-cation in a pipelined architecture."
"We have analyzed recent theoretical and applied work in both discourse markers and revision, showing that although no pre-vious NLG system has yet integrated both compo-nents into a single architecture, an architecture for multi-paragraph generation which separated the two into distinct, unlinked modules would not be able to guarantee that the final text contained appropri-ately lexicalized discourse markers."
"Instead, our combined revision and discourse marker module in an implemented pipelined NLG system is able to correctly insert appropriate discourse markers de-spite changes made by the revision system."
A cor-pus analysis indicated that significant interference effects between revision and discourse marker lex-icalization are possible.
Future work may show that similar interference effects are possible as succes-sive modules are added to pipelined NLG systems.
This paper presents a Chinese word segmen-tation system that uses improved source-channel models of Chinese sentence genera-tion.
"Chinese words are defined as one of the following four types: lexicon words, mor-phologically derived words, factoids, and named entities."
"Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: ([Footnote_1]) word segmentation, (2) morphological analy-sis, (3) factoid detection, and (4) named entity recognition."
"1 We would like to thank Ashley Chang, Jian-Yun Nie, Andi Wu and Ming Zhou for many useful discussions, and for comments on earlier versions of this paper. We would also like to thank Xiaoshan Fang, Jianfeng Li, Wenfeng Yang and Xiaodan Zhu for their help with evaluating our system."
"The performance of the system is evaluated on a manually annotated test set, and is also compared with several state-of-the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system."
"Chinese word segmentation is the initial step of many Chinese language processing tasks, and has attracted a lot of attention in the research commu-nity."
It is a challenging problem due to the fact that there is no standard definition of Chinese words.
"In this paper, we define Chinese words as one of the following four types: entries in a lexicon, mor-phologically derived words, factoids, and named entities."
"We then present a Chinese word segmen-tation system which provides a solution to the four fundamental problems of word-level Chinese lan-guage processing: word segmentation, morpho-logical analysis, factoid detection, and named entity recognition (NER)."
There are no word boundaries in written Chinese text.
"Therefore, unlike English, it may not be de-sirable to separate the solution to word segmenta-tion from the solutions to the other three problems."
"Ideally, we would like to propose a unified ap-proach to all the four problems."
"The unified ap-proach we used in our system is based on the im-proved source-channel models of Chinese sentence generation, with two components: a source model and a set of channel models."
"The source model is used to estimate the generative probability of a word sequence, in which each word belongs to one word type."
"For each word type, a channel model is used to estimate the generative probability of a character string given the word type."
So there are multiple channel models.
We shall show in this paper that our models provide a statistical frame-work to corporate a wide variety linguistic knowl-edge and statistical models in a unified way.
We evaluate the performance of our system us-ing an annotated test set.
"We also compare our system with several state-of-the-art systems, taking into account the fact that the definition of Chinese words often varies from system to system."
In the rest of this paper: Section 2 discusses previous work.
Section 3 gives the detailed defini-tion of Chinese words.
Sections 4 to 6 describe in detail the improved source-channel models.
Section 8 describes the evaluation results.
Section 9 pre-sents our conclusion.
Many methods of Chinese word segmentation have been proposed: reviews include[REF_CITE].
"These methods can be roughly classified into dictionary-based methods and statistical-based methods, while many state-of-the-art systems use hybrid approaches."
"In dictionary-based methods (e.g.[REF_CITE]), given an input character string, only words that are stored in the dictionary can be identified."
"The performance of these methods thus depends to a large degree upon the coverage of the dictionary, which unfortunately may never be complete be-cause new words appear constantly."
"Therefore, in addition to the dictionary, many systems also con-tain special components for unknown word identi-fication."
"In particular, statistical methods have been widely applied because they utilize a probabilistic or cost-based scoring mechanism, instead of the dictionary, to segment the text."
"These methods however, suffer from three drawbacks."
"First, some of these methods (e.g.[REF_CITE]) identify unknown words without identifying their types."
"For instance, one would identify a string as a unit, but not identify whether it is a person name."
This is not always sufficient.
"Second, the probabilistic models used in these methods (e.g.[REF_CITE]) are trained on a segmented corpus which is not always available."
"Third, the identified unknown words are likely to be linguistically implausible (e.g.[REF_CITE]), and additional manual checking is needed for some subsequent tasks such as parsing."
We believe that the identification of unknown words should not be defined as a separate problem from word segmentation.
These two problems are better solved simultaneously in a unified approach.
"One example of such approaches is[REF_CITE], which is based on weighted finite-state transducers (FSTs)."
"Our approach is motivated by the same inspiration, but is based on a different mechanism: the improved source-channel models."
"As we shall see, these models provide a more flexible framework to incorporate various kinds of lexical and statistical information."
Some types of unknown words that are not discussed in Sproat’s system are dealt with in our system.
"There is no standard definition of Chinese words – linguists may define words from many aspects (e.g.[REF_CITE]), but none of these definitions will completely line up with any other."
"Fortunately, this may not matter in practice because the definition that is most useful will depend to a large degree upon how one uses and processes these words."
"We define Chinese words in this paper as one of the following four types: (1) entries in a lexicon (lexicon words below), (2) morphologically derived words, (3) factoids, and (4) named entities, because these four types of words have different function-alities in Chinese language processing, and are processed in different ways in our system."
"For example, the plausible word segmentation for the sentence in Figure 1(a) is as shown."
"Figure 1(b) is the output of our system, where words of different types are processed in different ways: boundaries. (b) An output of our word segmentation system."
"Square brackets indicate word boundaries. + indicates a morpheme boundary. • For lexicon words, word boundaries are de-tected. • For morphologically derived words, their morphological patterns are detected, e.g. 朋友 们 ‘friend+s’ is derived by affixation of the plural affix 们 to the noun 朋友 (MA_S in-dicates a suffixation pattern), and 高高兴兴 ‘happily’ is a reduplication of 高兴 ‘happy’ (MR_AABB indicates an AABB reduplica-tion pattern). • For factoids, their types and normalized forms are detected, e.g. 12:30 is the normal-ized form of the time expression 十二点三十 分 (TIME indicates a time expression). • For named entities, their types are detected, e.g. 李俊生 ‘Li Junsheng’ is a person name (PN indicates a person name)."
"In our system, we use a unified approach to de-tecting and processing the above four types of words."
This approach is based on the improved source-channel models described below.
"Let S be a Chinese sentence, which is a character string."
"For all possible word segmentations W, we will choose the most likely one W * which achieves the highest conditional probability P(W|S): W * = argmax w P(W|S)."
"According to Bayes’ decision rule and dropping the constant denominator, we can equivalently perform the following maximization: (1) W * = arg max P(W )"
P(S |W ) .
"Following the Chinese word definition in Section 3, we define word class C as follows: (1) Each lexicon word is defined as a class; (2) each morphologically derived word is defined as a class; (3) each type of factoids is defined as a class, e.g. all time expres-sions belong to a class TIME; and (4) each type of named entities is defined as a class, e.g. all person names belong to a class PN."
We therefore convert the word segmentation W into a word class se-quence C. Eq. 1 can then be rewritten as:
C * = arg max
P(C)P(S | C) . ([Footnote_2]) C
"2 In our system, we define ten types of factoid: date, time (TIME), number, and WWW."
Eq. 2 is the basic form of the source-channel models for Chinese word segmentation.
"The models assume that a Chinese sentence S is generated as follows: First, a person chooses a sequence of concepts (i.e., word classes C) to output, according to the prob-ability distribution P(C); then the person attempts to express each concept by choosing a sequence of characters, according to the probability distribution P(S|C)."
The source-channel models can be interpreted in another way as follows: P(C) is a stochastic model estimating the probability of word class sequence.
"It indicates, given a context, how likely a word class occurs."
"For example, person names are more likely to occur before a title such as 教授 ‘professor’."
So P(C) is also referred to as context model afterwards.
P(S|C) is a generative model estimating how likely a character string is generated given a word class.
"For example, the character string 李俊生 is more likely to be a person name than 里俊生 ‘Li Jun-sheng’ because 李 is a common family name in China while 里 is not."
So P(S|C) is also referred to as class model afterwards.
"In our system, we use the improved source-channel models, which contains one context model (i.e., a trigram language model in our case) and a set of class models of different types, each of which is for one class of words, as shown in Figure 2."
"Although Eq. 2 suggests that class model prob-ability and context model probability can be com-bined through simple multiplication, in practice some weighting is desirable."
There are two reasons.
"First, some class models are poorly estimated, owing to the sub-optimal assumptions we make for simplicity and the insufficiency of the training corpus."
Combining the context model probability with poorly estimated class model probabilities according to Eq. 2 would give the context model too little weight.
"Second, as seen in Figure 2, the class models of different word classes are constructed in different ways (e.g. name entity models are n-gram models trained on corpora, and factoid models are compiled using linguistic knowledge)."
"Therefore, the quantities of class model probabilities are likely to have vastly different dynamic ranges among different word classes."
"One way to balance these probability quantities is to add several class model weight CW, each for one word class, to adjust the class model probability P(S|C) to P(S|C) CW ."
"In our experiments, these class model weights are deter-mined empirically to optimize the word segmenta-tion performance on a development set."
"Given the source-channel models, the procedure of word segmentation in our system involves two steps: First, given an input string S, all word can-didates are generated (and stored in a lattice)."
"Each candidate is tagged with its word class and the class model probability P(S’|C), where S’ is any substring of S. Second, Viterbi search is used to select (from the lattice) the most probable word segmentation (i.e. word class sequence C * ) according to Eq. ([Footnote_2])."
"2 In our system, we define ten types of factoid: date, time (TIME), number, and WWW."
"Given an input string S, all class models in Figure 2 are applied simultaneously to generate word class candidates whose class model probabilities are assigned using the corresponding class models: • Lexicon words: For any substring S’ ⊆ S, we assume P(S’|C) = 1 and tagged the class as lexicon word if S’ forms an entry in the word lexicon, P(S’|C) = 0 otherwise. • Morphologically derived words: Similar to lexicon words, but a morph-lexicon is used instead of the word lexicon (see Section 5.1). • Factoids: For each type of factoid, we compile a set of finite-state grammars G, represented as FSTs."
"For all S’ ⊆ S, if it can be parsed using G, we assume P(S’|FT) = 1, and tagged S ’ as a factoid candidate."
"As the example in Figure 1 shows, 十二点三十分 is a factoid (time) can-didate with the class model probability P( 十二 点三十分 |TIME) =1, and 十二 and 三十 are also factoid (number) candidates, with P( 十二 |NUM) ="
"P( 三十 |NUM) =1 • Named entities: For each type of named enti-ties, we use a set of grammars and statistical models to generate candidates as described in Section 5.2."
"In our system, the morphologically derived words are generated using five morphological patterns: (1) affixation: 朋友们 (friend - plural) ‘friends’; (2) reduplication: 高兴 ‘happy’ ! 高高兴兴 ‘happily’; (3) merging: 上班 ‘on duty’ + 下班 ‘off duty’ ! 上 下班 ‘on-off duty’; (4) head particle (i.e. expres-sions that are verb + comp): 走 ‘walk’ + 出去 ‘out’ ! 走出去 ‘walk out’; and (5) split (i.e. a set of expressions that are separate words at the syntactic level but single words at the semantic level): 吃了饭 ‘already ate’, where the bi-character word 吃饭 ‘eat’ is split by the particle 了 ‘already’."
"It is difficult to simply extend the well-known techniques for English (i.e., finite-state morphology) to Chinese due to two reasons."
"First, Chinese mor- morphological rules are not as ‘general’ as their English counterparts."
"For example, English plural nouns can be in general generated using the rule ‘noun + s ! plural noun’."
But only a small subset of Chinese nouns can be pluralized (e.g. 朋友们 ) using its Chinese counterpart ‘noun + 们 ! plural noun’ whereas others (e.g. 南瓜 ‘pumpkins’) cannot.
"Second, the operations required by Chinese mor-phological analysis such as copying in reduplication, merging and splitting, cannot be implemented using the current finite-state networks [Footnote_3] ."
3[REF_CITE]also studied such problems (with the same example) and uses weighted FSTs to deal with the affixation.
Our solution is the extended lexicalization.
"We simply collect all morphologically derived word forms of the above five types and incorporate them into the lexicon, called morph lexicon."
The proce-dure involves three steps: (1) Candidate genera-tion.
It is done by applying a set of morphological rules to both the word lexicon and a large corpus.
"For example, the rule ‘noun + 们 ! plural noun’ would generate candidates like 朋友们 . (2) Statis-tical filtering."
"For each candidate, we obtain a set of statistical features such as frequency, mutual information, left/right context dependency from a large corpus."
"We then use an information gain-like metric described[REF_CITE]to estimate how likely a candidate is to form a morphologically derived word, and remove ‘bad’ candidates."
The basic idea behind the metric is that a Chinese word should appear as a stable sequence in the corpus.
"That is, the components within the word are strongly correlated, while the components at both ends should have low correlations with words outside the sequence. (3) Linguistic selec-tion."
"We finally manually check the remaining candidates, and construct the morph-lexicon, where each entry is tagged by its morphological pattern."
"We consider four types of named entities: person names (PN), location names (LN), organization names (ON), and transliterations of foreign names (FN)."
"Because any character strings can be in prin-ciple named entities of one or more types, to limit the number of candidates for a more effective search, we generate named entity candidates, given an input string, in two steps: First, for each type, we use a set of constraints (which are compiled by linguists and are represented as FSTs) to generate only those ‘most likely’ candidates."
"Second, each of the generated candidates is assigned a class model probability."
"These class models are defined as generative models which are respectively estimated on their corresponding named entity lists using maximum likelihood estimation (MLE), together with smoothing methods [Footnote_4] ."
"4 The detailed description of these models are[REF_CITE], which also describes the use of cache model and the way the abbreviations of LN and ON are handled."
We will describe briefly the constraints and the class models below.
"There are two main constraints. (1) PN patterns: We assume that a Chinese PN consists of a family name F and a given name G, and is of the pattern F+G. Both F and G are of one or two characters long. (2) Family name list: We only consider PN candidates that begin with an F stored in the family name list (which contains 373 entries in our system)."
"Given a PN candidate, which is a character string S’, the class model probability P(S’|PN) is computed by a character bigram model as follows: (1) Generate the family name sub-string S F , with the probability P(S F |F); (2) Generate the given name sub-string S G , with the probability P(S G |G) (or P(S G1 |G 1 )); and (3) Generate the second given name, with the probability P(S G2 |S G1 ,G 2 )."
"For example, the generative probability of the string 李俊生 given that it is a PN would be estimated as P( 李俊生 |PN) = P( 李 |F)P( 俊 |G 1 )P( 生 | 俊 ,G 2 )."
"Unlike PNs, there are no patterns for LNs."
"We assume that a LN candidate is generated given S’ (which is less than 10 characters long), if one of the following conditions is satisfied: (1) S’ is an entry in the LN list (which contains 30,000 LNs); (2) S’ ends in a keyword in a 120-entry LN keyword list such as 市 ‘city’ [Footnote_5] ."
"5 For a better understanding, the constraint is a simplified version of that used in our system."
The probability P(S’|LN) is computed by a character bigram model.
Consider a string 乌苏里江 ‘Wusuli river’.
It is a LN candidate because it ends in a LN keyword 江 ‘river’.
The generative probability of the string given it is a LN would be estimated as P( 乌苏里江 |LN) = P( 乌 |&lt;LN&gt;) P( 苏 | 乌 ) P( 里 | 苏 ) P( 江 | 里 )
"P(&lt;/LN&gt;| 江 ), where &lt;LN&gt; and &lt;/LN&gt; are symbols denoting the beginning and the end of a LN, re-spectively."
ONs are more difficult to identify than PNs and LNs because ONs are usually nested named entities.
Consider an ON 中国国际航空公司 ‘Air China Corporation’; it contains an LN 中国 ‘China’.
"Like the identification of LNs, an ON candidate is only generated given a character string S’ (less than 15 characters long), if it ends in a keyword in a 1,355-entry ON keyword list such as 公司 ‘corpo-ration’."
"To estimate the generative probability of a nested ON, we introduce word class segmentations of S’, C, as hidden variables."
"In principle, the ON class model recovers P(S’|ON) over all possible C: P(S’|ON) = ∑ C P(S’,C|ON) = ∑ C P(C|ON)P(S ’ |C, ON)."
"Since P(S ’ |C,ON) ="
"P(S ’ |C), we have P(S ’ |ON) = ∑ C P(C|ON) P(S ’ |C)."
"We then assume that the sum is approximated by a single pair of terms P(C * |ON)P(S ’ |C * ), where C * is the most probable word class segmentation discovered by Eq. 2."
"That is, we also use our system to find C * , but the source-channel models are estimated on the ON list."
Consider the earlier example.
"Assuming that C * = LN/ 国际 / 航空 / 公司 , where 中国 is tagged as a LN, the probability P(S’|ON) would be estimated using a word class bigram model as: P( 中国国际航空公司 |ON) ≈ P(LN/ 国际 / 航空 / 公司 |ON) P( 中国 |LN) ="
"P(LN|&lt;ON&gt;)P( 国际 |LN)P( 航空 | 国际 )P( 公司 | 航空 ) P(&lt;/ON&gt;| 公司 )P( 中国 |LN), where P( 中国 |LN) is the class model probability of 中国 given that it is a LN, &lt;ON&gt; and &lt;/ON&gt; are symbols denoting the beginning and the end of a ON, respectively."
As described[REF_CITE]: FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source lan-guage pronunciation of the name.
"Since FNs can be of any length and their original pronunciation is effectively unlimited, the recognition of such names is tricky."
"Fortunately, there are only a few hundred Chinese characters that are particularly common in transliterations."
"Therefore, an FN candidate would be generated given S’, if it contains only characters stored in a transliterated name character list (which contains 618 Chinese characters)."
The probability P(S’|FN) is estimated using a character bigram model.
"Notice that in our system a FN can be a PN, a LN, or an ON, depending on the context."
"Then, given a FN can-didate, three named entity candidates, each for one category, are generated in the lattice, with the class probabilities P(S ’ |PN)=P(S ’ |LN)=P(S ’ |ON)= P(S ’ |FN)."
"In other words, we delay the determina-tion of its type until decoding where the context model is used."
This section describes the way the class model probability P(C) (i.e. trigram probability) in Eq. 2 is estimated.
"Ideally, given an annotated corpus, where each sentence is segmented into words which are tagged by their classes, the trigram word class probabilities can be calculated using MLE, together with a backoff schema[REF_CITE]to deal with the sparse data problem."
"Unfortunately, building such annotated training corpora is very expensive."
Our basic solution is the bootstrapping approach described[REF_CITE].
"It consists of three steps: (1) Initially, we use a greedy word segmen-tor [Footnote_6] to annotate the corpus, and obtain an initial context model based on the initial annotated corpus; (2) we re-annotate the corpus using the obtained models; and (3) re-train the context model using the re-annotated corpus."
"6 The greedy word segmentor is based on a forward maximum matching (FMM) algorithm: It processes through the sentence from left to right, taking the longest match with the lexicon entry at each point."
Steps 2 and 3 are iterated until the performance of the system converges.
"In the above approach, the quality of the context model depends to a large degree upon the quality of the initial annotated corpus, which is however not satisfied due to two problems."
"First, the greedy segmentor cannot deal with the segmentation am-biguities, and even after iterations, these ambigui-ties can only be partially resolved."
"Second, many factoids and named entities cannot be identified using the greedy word segmentor which is based on the dictionary."
"To solve the first problem, we use two methods to resolve segmentation ambiguities in the initial segmented training data."
"We classify word seg-mentation ambiguities into two classes: overlap ambiguity (OA), and combination ambiguity (CA)."
"Consider a character string ABC, if it can be seg- mented into two words either as AB/C or A/BC depending on different context, ABC is called an overlap ambiguity string (OAS)."
"If a character string AB can be segmented either into two words, A/B, or as one word depending on different context."
AB is called a combination ambiguity string (CAS).
"To resolve OA, we identify all OASs in the training data and replace them with a single token &lt;OAS&gt;."
"By doing so, we actually remove the portion of training data that are likely to contain OA errors."
"To resolve CA, we select 70 high-frequent two-char-acter CAS (e.g. 才能 ‘talent’ and 才/能 ‘just able’)."
"For each CAS, we train a binary classifier (which is based on vector space models) using sentences that contains the CAS segmented manually."
"Then for each occurrence of a CAS in the initial segmented training data, the corresponding classifier is used to determine whether or not the CAS should be seg-mented."
"For the second problem, though we can simply use the finite-state machines described in Section 5 (extended by using the longest-matching constraint for disambiguation) to detect factoids in the initial segmented corpus, our method of NER in the initial step (i.e. step 1) is a little more complicated."
"First, we manually annotate named entities on a small subset (call seed set) of the training data."
"Then, we obtain a context model on the seed set (called seed model)."
We thus improve the context model which is trained on the initial annotated training corpus by interpolating it with the seed model.
"Finally, we use the improved context model in steps 2 and 3 of the bootstrapping."
"Our experiments show that a rela-tively small seed set (e.g., 10 million characters, which takes approximately three weeks for 4 per-sons to annotate the NE tags) is enough to get a good improved context model for initialization."
"To conduct a reliable evaluation, a manually anno-tated test set was developed."
"The text corpus con-tains approximately half million Chinese characters that have been proofread and balanced in terms of domain, styles, and times."
"Before we annotate the corpus, several questions have to be answered: (1) Does the segmentation depend on a particular lexicon? (2) Should we assume a single correct segmentation for a sentence? (3) What are the evaluation criteria? (4) How to perform a fair comparison across different systems?"
"As described earlier, it is more useful to define words depending on how the words are used in real applications."
"In our system, a lexicon (containing 98,668 lexicon words and 59,285 morphologically derived words) has been constructed for several applications, such as Asian language input and web search."
"Therefore, we annotate the text corpus based on the lexicon."
"That is, we segment each sentence as much as possible into words that are stored in our lexicon, and tag only the new words, which other-wise would be segmented into strings of one -character words."
"When there are multiple seg-mentations for a sentence, we keep only one that contains the least number of words."
"The annotated test set contains in total 247,039 tokens (including 205,162 lexicon/morph-lexicon words, 4,347 PNs, 5,311 LNs, 3,850 ONs, and 6,630 factoids, etc.)"
"Our system is measured through multiple preci-sion-recall (P/R) pairs, and F-measures (F β=1 , which is defined as 2PR/(P+R)) for each word class."
"Since the annotated test set is based on a particular lexicon, some of the evaluation measures are meaningless when we compare our system to other systems that use different lexicons."
"So in comparison with dif-ferent systems, we consider only the preci-sion-recall of NER and the number of OAS errors (i.e. crossing brackets) because these measures are lexicon independent and there is always a single unambiguous answer."
"The training corpus for context model contains approximately 80 million Chinese characters from various domains of text such as newspapers, novels, magazines etc."
The training corpora for class mod-els are described in Section 5.
"Our system is designed in the way that components such as factoid detector and NER can be ‘switched on or off’, so that we can investigate the relative contribution of each component to the overall word segmentation performance."
The main results are shown in Table 1.
"For comparison, we also include in the table (Row 1) the results of using the greedy segmentor (FMM) described in Section 6."
"Row 2 shows the baseline results of our system, where only the lexicon is used."
"It is interesting to find, in Rows 1 and 2, that the dictionary-based methods already achieve quite good recall, but the precisions are not very good because they cannot identify correctly unknown words that are not in the lexicon such factoids and named entities."
"We also find that even using the same lexicon, our approach that is based on the improved source-channel models outperforms the greedy approach (with a slight but statistically significant different i.e., P &lt; 0.01 according to the t test) because the use of context model resolves more ambiguities in segmentation."
The most promising property of our approach is that the source-channel models provide a flexible frame-work where a wide variety of linguistic knowledge and statistical models can be combined in a unified way.
"As shown in Rows 3 to 6, when components are switched on in turn by activating corresponding class models, the overall word segmentation per-formance increases consistently."
"We also conduct an error analysis, showing that 86.2% of errors come from NER and factoid detec-tion, although the tokens of these word types consist of only 8.7% of all that are in the test set."
"We compare our system – henceforth SCM, with other two Chinese word segmentation systems [Footnote_7] : 1."
"7 Although the two systems are widely accessible in mainland China, to our knowledge no standard evaluations on Chinese word segmentation of the two systems have been published by press time. More comprehensive comparisons (with other well-known systems) and detailed error analysis form one area of our future work."
The MSWS system is one of the best available products.
It is released by Microsoft ® (as a set of Windows APIs).
"MSWS first conducts the word breaking using MM (augmented by heu-ristic rules for disambiguation), then conducts factoid detection and NER using rules. 2."
The LCWS system is one of the best research systems in mainland China.
It is released by Beijing Language University.
"The system works similarly to MSWS, but has a larger dictionary containing more PNs and LNs."
"As mentioned above, to achieve a fair comparison, we compare the above three systems only in terms of NER precision-recall and the number of OAS errors."
"However, we find that due to the different annotation specifications used by these systems, it is still very difficult to compare their results auto-matically."
"For example, 北京市政府 ‘Beijing city government’ has been segmented inconsistently as 北京市/政府 ‘Beijing city’ + ‘government’ or 北京/ 市政府 ‘Beijing’ + ‘city government’ even in the same system."
"Even worse, some LNs tagged in one system are tagged as ONs in another system."
"Therefore, we have to manually check the results."
We also did not differentiate LNs and ONs in evaluation.
"That is, we only checked the word boundaries of LNs and ONs and treated both tags exchangeable."
The results are shown in Table 2.
We can see that in this small test set SCM achieves the best overall performance of NER and the best performance of resolving OAS.
The contributions of this paper are three-fold.
"First, we formulate the Chinese word segmentation problem as a set of correlated problems, which are better solved simultaneously, including word breaking, morphological analysis, factoid detection and NER."
"Second, we present a unified approach to these problems using the improved source-channel models."
The models provide a simple statistical framework to incorporate a wide variety of linguis-tic knowledge and statistical models in a unified way.
"Third, we evaluate the system’s performance on an annotated test set, showing very promising results."
"We also compare our system with several state-of-the-art systems, taking into account the fact that the definition of Chinese words varies from system to system."
"Given the comparison results, we can say with confidence that our system achieves at least the performance of state-of-the-art word seg-mentation systems."
We present a language-independent and unsupervised algorithm for the segmenta-tion of words into morphs.
"The algorithm is based on a new generative probabilis-tic model, which makes use of relevant prior information on the length and fre-quency distributions of morphs in a lan-guage."
"Our algorithm is shown to out-perform two competing algorithms, when evaluated on data from a language with agglutinative morphology (Finnish), and to perform well also on English data."
"In order to artificially “understand” or produce nat-ural language, a system presumably has to know the elementary building blocks, i.e., the lexicon, of the language."
"Additionally, the system needs to model the relations between these lexical units."
Many ex-isting NLP (natural language processing) applica-tions make use of words as such units.
"For in-stance, in statistical language modelling, probabil-ities of word sequences are typically estimated, and bag-of-word models are common in information re-trieval."
"However, for some languages it is infeasible to construct lexicons for NLP applications, if the lexi-cons contain entire words."
"In especially agglutina-tive languages, [Footnote_1] such as Finnish and Turkish, the number of possible different word forms is simply too high."
1 In agglutinative languages words are formed by the con-catenation of morphemes.
"For example, in Finnish, a single verb may appear in thousands of different forms[REF_CITE]."
"According to linguistic theory, words are built from smaller units, morphemes."
Morphemes are the smallest meaning-bearing elements of language and could be used as lexical units instead of entire words.
"However, the construction of a comprehensive mor-phological lexicon or analyzer based on linguistic theory requires a considerable amount of work by experts."
This is both time-consuming and expen-sive and hardly applicable to all languages.
"Further-more, as language evolves the lexicon must be up-dated continuously in order to remain up-to-date."
"Alternatively, an interesting field of research lies open: Minimally supervised algorithms can be de-signed that automatically discover morphemes or morpheme-like units from data."
"There exist a num-ber of such algorithms, some of which are entirely unsupervised and others that use some knowledge of the language."
"In the following, we discuss recent un-supervised algorithms and refer the reader[REF_CITE]for a comprehensive survey of previous research in the whole field."
"Many algorithms proceed by segmenting (i.e., splitting) words into smaller components."
Often the limiting assumption is made that words con-sist of only one stem followed by one (possibly empty) suffix[REF_CITE].
"This limitation is reduced[REF_CITE]by allowing a recursive struc-ture, where stems can have inner structure, so that they in turn consist of a substem and a suffix."
Also prefixes are possible.
"However, for languages with agglutinative morphology this may not be enough."
"In Finnish, a word can consist of lengthy sequences of alternating stems and affixes."
Some morphology discovery algorithms learn re-lationships between words by comparing the ortho-graphic or semantic similarity of the words[REF_CITE].
"Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages."
We previously presented two segmentation algo-rithms suitable for agglutinative languages[REF_CITE].
"The algorithms learn a set of segments, which we call morphs, from a corpus."
"Stems and affixes are not distinguished as sepa-rate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such[REF_CITE]."
"How-ever, we observed that for the corpus size studied (100 000 words), our two algorithms were somewhat prone to excessive segmentation of words."
"In this paper, we aim at overcoming the problem of excessive segmentation, particularly when small corpora (up to 200 000 words) are used for training."
"We present a new segmentation algorithm, which is language independent and works in an unsupervised fashion."
"Since the results obtained suggest that the algorithm performs rather well, it could possibly be suitable for languages for which only small amounts of written text are available."
The model is formulated in a probabilistic Bayesian framework.
It makes use of explicit prior information in the form of probability distributions for morph length and morph frequency.
The model is based on the same kind of reasoning as the proba-bilistic model[REF_CITE].
"While Brent’s model displays a prior probability that exponentially de-creases with word length (with one character as the most common length), our model uses a probabil-ity distribution that more accurately models the real length distribution."
"Also Brent’s frequency distribu-tion differs from ours, which we derive from Man-delbrot’s correction of Zipf’s law (cf. Section 2.5)."
"Our model requires that the values of two param-eters be set: (i) our prior belief of the most common morph length, and (ii) our prior belief of the pro- portion of morph types [Footnote_2] that occur only once in the corpus."
"2 We use standard terminology: Morph types are the set of different, distinct morphs. By contrast, morph tokens are the instances (or occurrences) of morphs in the corpus."
These morph types are called hapax legom-ena.
"While the former is a rather intuitive measure, the latter may not appear as intuitive."
"However, the proportion of hapax legomena may be interpreted as a measure of the richness of the text."
"Also note that since the most common morph length is calculated for morph types, not tokens, it is not independent of the corpus size."
"A larger corpus usually requires a higher average morph length, a fact that is stated for word lengths[REF_CITE]."
As an evaluation criterion for the performance of our method and two reference methods we use a measure that reflects the ability to recognize real morphemes of the language by examining the morphs found by the algorithm.
In this section we derive the new model.
"We fol-low a step-by-step process, during which a morph lexicon and a corpus are generated."
The morphs in the lexicon are strings that emerge as a result of a stochastic process.
The corpus is formed through another stochastic process that picks morphs from the lexicon and places them in a sequence.
"At two points of the process, prior knowledge is required in the form of two real numbers: the most common morph length and the proportion of hapax legomena morphs."
The model can be used for segmentation of words by requiring that the corpus created is exactly the input data.
"By selecting the most probable morph lexicon that can produce the input data, we obtain a segmentation of the words in the corpus, since we can rewrite every word as a sequence of morphs."
We start the generation process by deciding the num-ber of morphs in the morph lexicon (type count).
This number is denoted by n µ and its probability p(n µ ) follows the uniform distribution.
"This means that, a priori, no lexicon size is more probable than another. [Footnote_3]"
"3 This is an improper prior, but it is of little practical signif-icance for two reasons: (i) This stage of the generation process"
"For each morph in the lexicon, we independently choose its length in characters according to the gamma distribution: 1 p(l µ i ) ="
"Γ(α)β α l µ α−1 e i −l µi /β , (1) where l µ i is the length in characters of the ith morph, and α and β are constants."
Γ(α) is the gamma func-tion:
Γ(α) = Z ∞ z α−1 e dz. −z (2) 0
"The maximum value of the density occurs at l µ i = (α − 1)β, which corresponds to the most common morph length in the lexicon."
"When β is set to one, and α to one plus our prior belief of the most com-mon morph length, the pdf (probability density func-tion) is completely defined."
"We have chosen the gamma distribution for morph lengths, because it corresponds rather well to the real length distribution observed for word types in Finnish and English corpora that we have stud-ied."
The distribution also fits the length distribution of the morpheme labels used as a reference (cf.
"A Poisson distribution can be justified and has been used in order to model the length distri-bution of word and morph tokens [e.g.,[REF_CITE]], but for morph types we have chosen the gamma distribution, which has a thicker tail."
"For each morph µ i , we decide the character string it consists of: We independently choose l µ i characters at random from the alphabet in use."
"The probabil-ity of each character c j is the maximum likelihood estimate of the occurrence of this character in the corpus: [Footnote_4] p(c j ) = n c j , (3) P k n c k where n c j is the number of occurrences of the char-acter c j in the corpus, and P k n c k is the total num-ber of characters in the corpus."
"4 Alternatively, the maximum likelihood estimate of the oc-currence of the character in the lexicon could be used."
The lexicon consists of a set of n µ morphs and it makes no difference in which order these morphs have emerged.
"Regardless of their initial order, the morphs can be sorted into a uniquely defined (e.g., alphabetical) order."
"Since there are n µ ! ways to or-der n µ different elements, [Footnote_5] we multiply the proba-bility accumulated so far by n µ !: n µ l µi p(lexicon) = p(n µ ) Y hp(l µ ) Y p(c j )i · n µ ! (4) i i=1 j=1"
"5 Strictly speaking, our probabilistic model is not perfect, since we do not make sure that no morph can appear more than once in the lexicon."
The next step is to generate a corpus using the morph lexicon obtained in the previous steps.
"First, we in-dependently choose the number of times each morph occurs in the corpus."
We pursue the following line of thought:
"Zipf has studied the relationship between the fre-quency of a word, f, and its rank, z. [Footnote_6]"
"6 The rank of a word is the position of the word in a list, where the words have been sorted according to falling fre-quency."
He suggests that the frequency of a word is inversely proportional to its rank.
"Mandelbrot has refined Zipf’s formula, and suggests a more general relationship [see, e.g.,[REF_CITE]]: f ="
"C(z + b) −a , (5) where C, a and b are parameters of a text."
Let us derive a probability distribution from Man-delbrot’s formula.
The rank of a word as a func-tion of its frequency can be obtained by solving for z from (5): 1 − 1 z = C a f a − b. (6)
Suppose that one wants to know the number of words that have a frequency close to f rather than the rank of the word with frequency f.
"In order to obtain this information, we choose an arbitrary in-terval around f: [(1/γ)f . . . γf [, where γ &gt; 1, and compute the rank at the endpoints of the interval."
"The difference is an estimate of the number of words that fall within the interval, i.e., have a frequency close to f: 1 − 1 1 − 1 n f = z 1/γ − z γ = (γ a − γ a )C a f a . (7)"
"This can be transformed into an exponential pdf by (i) binning the frequency axis so that there are no overlapping intervals. (This means that the fre-quency axis is divided into non-overlapping inter-vals [(1/γ)fˆ. . . γfˆ [, which is equivalent to having fˆ values that are powers of γ 2 : fˆ 0 = γ 0 = 1, fˆ 1 = γ 2 ,fˆ 2 = γ 4 ,..."
All frequencies f are rounded to the closest fˆ.)
"Next (ii), we normalize the number of words with a frequency close to fˆ with the to- ˆ n ˆ ."
"Furthermore (iii), fˆ tal number of words P f f is written as e logfˆ , and (iv) C must be chosen so that the normalization coefficient equals 1/a, which yields a proper pdf that integrates to one."
Note also the factor log γ 2 .
"Like fˆ, log fˆis a discrete variable."
"We approximate the integral of the density function around each value log fˆby multiplying with the dif-ference between two successive log fˆ values, which equals log γ 2 : 1 1 p(f ∈ [(1/γ)fˆ. . . γfˆ[) = γ a − γ − a C 1 1 log fˆ a e − a 1 P fˆ n fˆ − 1 log fˆ · log γ 2 . (8) = ae a"
"Now, if we assume that Zipf’s and Madelbrot’s formulae apply to morphs as well as to words, we can use formula (8) for every morph frequency f µ i , which is the number of occurrences (or frequency) of the morph µ i in the corpus (token count)."
"How-ever, values for a and γ 2 must be chosen."
"We set γ 2 to 1.59, which is the lowest value for which no empty frequency bins will appear. [Footnote_7] For f µ i = 1, (8) reduces to log γ 2 /a."
"7 Empty bins can appear for small values of f µ i due to f µ i ’s being rounded to the closest fˆ µ , which is a power of γ 2 . i"
We set this value equal to our prior belief of the proportion of morph types that are to occur only once in the corpus (hapax legomena).
The morphs and their frequencies have been set.
The order of the morphs in the corpus remains to be de-cided.
The probability of one particular order is the inverse of the multinomial: n µ p(corpus) = (PQ i=1 f µ i )!
N! −1 −1 n µ = . n µ i=1 f µ i ! =1 f µ i !
Q i (9)
"The numerator of the multinomial is the factorial of the total number of morph tokens, N, which equals the sum of frequencies of every morph type."
The de-nominator is the product of the factorial of the fre-quency of each morph type.
The search for the optimal model given our input data corresponds closely to the recursive segmen-tation algorithm presented[REF_CITE].
"The search takes place in batch mode, but could as well be done incrementally."
"All words in the data are randomly shuffled, and for each word, every split into two parts is tested."
"The most proba-ble split location (or no split) is selected and in case of a split, the two parts are recursively split in two."
All words are iteratively reprocessed until the prob-ability of the model converges.
"From the point of view of linguistic theory, it is pos-sible to come up with different plausible sugges-tions for the correct location of morpheme bound-aries."
"Some of the solutions may be more elegant than others, [Footnote_8] but it is difficult to say if the most el-egant scheme will work best in practice, when real NLP applications are concerned."
8 Cf. “hop +ed” vs. “hope + d” (past tense of “to hope”).
We utilize an evaluation method for segmentation of words presented[REF_CITE].
"In this method, segments are not compared to one sin-gle “correct” segmentation."
The evaluation criterion can rather be interpreted from the point of view of language “understanding”.
"A morph discovered by the segmentation algorithm is considered to be “un-derstood”, if there is a low-ambiguity mapping from the morph to a corresponding morpheme."
"Alterna-tively, a morph may correspond to a sequence of morphemes, if these morphemes are very likely to occur together."
"The idea is that if an entirely new word form is encountered, the system will “under-stand” it by decomposing it into morphs that it “un-derstands”."
A segmentation algorithm that segments words into too small parts will perform poorly due to high ambiguity.
"At the other extreme, an algorithm that is reluctant at splitting words will have bad gen-eralization ability to new word forms."
Reference morpheme sequences for the words are obtained using existing software for automatic mor-phological analysis based on the two-level morphol-ogy[REF_CITE].
"For each word form, the analyzer outputs the base form of the word to-gether with grammatical tags."
"By filtering the out-put, we get a sequence of morpheme labels that ap-pear in the correct order and represent correct mor-phemes rather closely."
"Note, however, that the mor-pheme labels are not necessarily orthographically similar to the morphemes they represent."
The exact procedure for evaluating the segmenta-tion of a set of words consists of the following steps: (1) Segment the words in the corpus using the au-tomatic segmentation algorithm. (2) Divide the segmented data into two parts of equal size.
Collect all segmented word forms from the first part into a training vocabulary and collect all segmented word forms from the second part into a test vocabulary. (3) Align the segmentation of the words in the training vocabulary with the corresponding refer-ence morpheme label sequences.
"Each morph must be aligned with one or more consecutive morpheme labels and each morpheme label must be aligned with at least one morph; e.g., for a hypothetical seg-mentation of the English word winners’:"
Morpheme labels win -ER
PL GEN Morph sequence er s’w inn (4) Estimate conditional probabilities for the morph/morpheme mappings computed over the whole training vocabulary: p(morpheme | morph).
Re-align using the Viterbi algorithm and employ the Expectation-Maximization algorithm iteratively un-til convergence of the probabilities. (5) The quality of the segmentation is evaluated on the test vocabulary.
The segmented words in the test vocabulary are aligned against their reference morpheme label sequences according to the condi-tional probabilities learned from the training vocab-ulary.
"To measure the quality of the segmentation we compute the expectation of the proportion of correct mappings from morphs to morpheme labels,"
"E{p(morpheme | morph)}: 1 N N X p i (morpheme | morph), (10) i=1 where N is the number of morph/morpheme map-pings, and p i (·) is the probability associated with the ith mapping."
"Thus, we measure the proportion of morphemes in the test vocabulary that we can ex-pect to recognize correctly by examining the morph segments. [Footnote_9]"
"9 In[REF_CITE]the results are reported less intuitively as the “alignment distance”, i.e., the negative logprob"
"We have conducted experiments involving (i) three different segmentation algorithms, (ii) two corpora in different languages (Finnish and English), and (iii) data sizes ranging from 2000 words to 200 000 words."
The new probabilistic method is compared to two existing segmentation methods: the Recursive MDL method presented[REF_CITE][Footnote_10] and John Goldsmith’s algorithm called Linguistica[REF_CITE]. [Footnote_11] Both methods use MDL (Min-imum Description Length)[REF_CITE]as a cri-terion for model optimization.
10 Online demo[URL_CITE]of the entire test set: − log p i (morpheme | morph).
11 The software can be downloaded[URL_CITE]
"The effect of using prior information on the dis-tribution of morph length and frequency can be as-sessed by comparing the probabilistic method to Re-cursive MDL, since both methods utilize the same search algorithm, but Recursive MDL does not make use of explicit prior information."
"Furthermore, the possible benefit of using the two sources of prior information can be compared against the possible benefit of grouping stems and suffixes into signatures."
The latter technique is em-ployed by Linguistica.
The Finnish data consists of subsets of a news-paper text corpus[REF_CITE]from which non-words (numbers and punctuation marks) have been removed.
The reference morpheme labels have been filtered out from a morphosyntactic analysis of the text produced by the Connexor FDG parser. [URL_CITE]
The English corpus consists of mainly newspaper text (with non-words removed) from the Brown cor-pus. [Footnote_14]
14 The Brown corpus is available at the Linguistic Data Con-sortium[URL_CITE]
A morphological analysis of the words has been performed using the Lingsoft ENGTWOL an-alyzer. [URL_CITE]
"For both languages data sizes of 2000, 5000, 10000, 50000, 100000, and 200000 have been used."
"A notable difference between the morpholog-ical structure of the languages lies in the fact that whereas there are about 17 000 English word types in the largest data set, the corresponding number of Finnish word types is 58 000."
"In order to select good prior values for the prob-abilistic method, we have used separate develop-ment test sets that are independent of the final data sets."
Morph length and morph frequency distribu-tions have been computed for the reference mor-pheme representations of the development test sets.
The prior values for most common morph length and proportion of hapax legomena have been adjusted in order to produce distributions that fit the reference as well as possible.
We thus assume that we can make a good guess of the final morph length and frequency distributions.
"Note, however, that our reference is an approxima-tion of a morpheme representation."
"As the segmen-tation algorithms produce morphs, not morphemes, we can expect to obtain a larger number of morphs due to allomorphy."
Note also that we do not op-timize for segmentation performance on the devel-opment test set; we only choose the best fit for the morph length and frequency distributions.
"As for the two other segmentation algorithms, Re-cursive MDL has no parameters to adjust."
In Lin-guistica we have used Method A Suffixes + Find pre-fixes from stems with other parameters left at their default values.
We are unaware whether another configuration could be more advantageous for Lin-guistica.
The expected proportion of morphemes recognized by the three segmentation methods are plotted in Figures 1 and 2 for different sizes of the Finnish and English corpora.
The search algorithm used in the probabilistic method and Recursive MDL in-volve randomness and therefore every value shown for these two methods is the average obtained over ten runs with different random seeds.
"However, the fluctuations due to random behaviour are very small and paired t-tests show significant differences at the significance level of 0.01 for all pair-wise compar-isons of the methods at all corpus sizes."
"For Finnish, all methods show a curve that mainly increases as a function of the corpus size."
The prob-abilistic method is the best with morpheme recogni-tion percentages between 23.5% and 44.2%.
Lin-guistica performs worst with percentages between 16.5% and 29.1%.
"None of the methods are close to ideal performance, which, however, is lower than 100%."
"This is due to the fact that the test vocabu-lary contains a number of morphemes that are not present in the training vocabulary, and thus are im-possible to recognize."
The proportion of unrecog-nizable morphemes is highest for the smallest corpus size (32.5%) and decreases to 8.8% for the largest corpus size.
The evaluation measure used unfortunately scores a baseline of no segmentation fairly high.
"The no-segmentation baseline corresponds to a system that recognizes the training vocabulary fully, but has no ability to generalize to any other word form."
The results for English are different.
"Linguistica is the best method for corpus sizes below 50000 words, but its performance degrades from the max-imum of 39.6% at 10000 words to 29.8% for the largest data set."
The probabilistic method is con-stantly better than Recursive MDL and both methods outperform[REF_CITE]0 words.
The recognition percentages of the probabilistic method vary between 28.2% and 43.6%.
"However, for cor-pus sizes above 10000 words none of the three methods outperform the no-segmentation baseline."
"Overall, the results for English are closer to ideal performance than was the case for Finnish."
"This is partly due to the fact that the proportion of un-seen morphemes that are impossible to recognize is higher for English (44.5% at 2000 words, 19.0% at 200 000 words)."
"As far as the time consumption of the algorithms is concerned, the largest Finnish corpus took 20 min-utes to process for the probabilistic method and Re-cursive MDL, and 40 minutes for Linguistica."
The largest English corpus was processed in less than three minutes by all the algorithms.
The tests were run on a 900 MHz AMD Duron processor with 256 MB RAM.
"For small data sizes, Recursive MDL has a tendency to split words into too small segments, whereas Lin-guistica is much more reluctant at splitting words, due to its use of signatures."
The extent to which the probabilistic method splits words lies somewhere in between the two other methods.
Our evaluation measure favours low ambiguity as long as the ability to generalize to new word forms does not suffer.
This works against all segmentation methods for English at larger data sizes.
"The En-glish language has rather simple morphology, which means that the number of different possible word forms is limited."
"The larger the training vocabu-lary, the broader coverage of the test vocabulary, and therefore the no-segmentation approach works sur-prisingly well."
"Segmentation always increases am-biguity, which especially Linguistica suffers from as it discovers more and more signatures and short suf-fixes as the amount of data increases."
"For instance, a final ’s’ stripped off its stem can be either a noun or a verb ending, and a final ’e’ is very ambiguous, as it belongs to orthography rather than morphology and does not correspond to any morpheme."
Finnish morphology is more complex and there are endless possibilities to construct new word forms.
"As can be seen from Figure 1, the proba-bilistic method and Recursive MDL perform better than the no-segmentation baseline for all data sizes."
"The segmentations could be evaluated using other measures, but for language modelling purposes, we believe that the evaluation measure should not favour shattering of very common strings, even though they correspond to more than one morpheme."
These strings should rather work as individual vo-cabulary items in the model.
"It has been shown that increased performance of n-gram models can be ob-tained by adding larger units consisting of common word sequences to the vocabulary; see e.g.,[REF_CITE]."
"Nevertheless, in the near fu-ture we wish to explore possibilities of using com-plementary and more standard evaluation measures, such as precision, recall, and F-measure of the dis-covered morph boundaries."
"Concerning the length and frequency prior dis-tributions in the probabilistic model, one notes that they are very general and do not make far-reaching assumptions about the behaviour of natural lan-guage."
"In fact, Zipf’s law has been shown to ap-ply to randomly generated artificial texts[REF_CITE]."
"In our implementation, due to the independence as-sumptions made in the model and due to the search algorithm used, the choice of a prior value for the most common morph length is more important than the hapax legomena value."
"If a very bad prior value for the most common morph length is used perfor-mance drops by twelve percentage units, whereas extreme hapax legomena values only reduces per-formance by two percentage units."
But note that the two values are dependent: A greater average morph length means a greater number of hapax legomena and vice versa.
There is always room for improvement.
"Our cur-rent model does not represent contextual dependen-cies, such as phonological rules or morphotactic lim-itations on morph order."
"Nor does it identify which morphs are allomorphs of the same morpheme, e.g., “city” and “citi + es”."
"In the future, we expect to ad-dress these problems by using statistical language modelling techniques."
We will also study how the algorithms scale to considerably larger corpora.
"The results we have obtained suggest that the per-formance of a segmentation algorithm can indeed be increased by using prior information of general na-ture, when this information is expressed mathemati-cally as part of a probabilistic model."
"Furthermore, we have reasons to believe that the morph segments obtained can be useful as components of a statistical language model."
It is well known that occurrence counts of words in documents are often mod-eled poorly by standard distributions like the binomial or Poisson.
"Observed counts vary more than simple models predict, prompting the use of overdispersed mod-els like Gamma-Poisson or Beta-binomial mixtures as robust alternatives."
"Another deficiency of standard models is due to the fact that most words never occur in a given document, resulting in large amounts of zero counts."
"We propose using zero-inflated models for dealing with this, and evaluate competing models on a Naive Bayes text classification task."
"Simple zero-inflated models can account for prac-tically relevant variation, and can be easier to work with than overdispersed models."
Linguistic count data often violate the simplistic as-sumptions of standard probability models like the binomial or Poisson distribution.
"In particular, the inadequacy of the Poisson distribution for model-ing word (token) frequency is well known, and ro-bust alternatives have been proposed[REF_CITE]."
"In the case of the Poisson, a commonly used robust alternative is the negative binomial distribution ([REF_CITE]§4.5), which has the ability to capture extra-Poisson variation in the data, in other words, it is overdis-persed compared with the Poisson."
When a small set of parameters controls all properties of the dis-tribution it is important to have enough parameters to model the relevant aspects of one’s data.
"Sim-ple models like the Poisson or binomial do not have enough parameters for many realistic applications, and we suspect that the same might be true of log-linear models."
"When applying robust models like the negative binomial to linguistic count data like word occurrences in documents, it is natural to ask to what extent the extra-Poisson variation has been captured by the model."
"Answering that question is our main goal, and we begin by reviewing some of the classic results[REF_CITE]."
"In preparation of their authorship study of The Fed-eralist, Mosteller and Wallace (1984, §2.3) investi-gated the variation of word frequency across con-tiguous passages of similar length, drawn from pa-pers of known authorship."
"The occurrence frequen-cies of any in papers by Hamilton (op. cit., Ta-ble 2.3–3) are repeated here in Figure 1: out of a total of 247 passages there are 125 in which the word any does not occur; it occurs once in 88 pas-sages, twice in 26 passages, etc."
Figure 1 also shows the counts predicted by a Poisson distribution with mean 0.67.
"Visual inspection (“chi by eye”) indi-cates an acceptable fit between the model and the data, which is confirmed by a χ 2 goodness-of-fit test."
"This demonstrates that certain words seem to be adequately modeled by a Poisson distribution, whose probability mass function is shown in (1): λ x 1 Poisson(λ)(x) = (1) x! expλ"
For other words the Poisson distribution gives a much worse fit.
"Take the occurrences of were in pa-pers by Madison, as shown in Figure 2 (ibid.)."
"We calculate the χ 2 statistic for the counts expected un-der a Poisson model for three bins (0, 1, and 2–5, to ensure that the expected counts are greater than 5) and obtain 6.17 at one degree of freedom (number of bins minus number of parameters minus one), which is enough to reject the null hypothesis that the data arose from a Poisson(0.45) distribution."
"On the other hand, the χ 2 statistic for a negative bino-mial distribution NegBin(0.45,1.17) is only 0.013 for four bins (0, 1, 2, and 3–5), i. e., again 1 degree of freedom, as two parameters were estimated from the data."
Now we are very far from rejecting the null hypothesis.
"This provides some quantitative back-ing for Mosteller and Wallace’s statement that ‘even the most motherly eye can scarcely make twins of the [Poisson vs. empirical] distributions’ for certain words (op. cit., 31)."
"The probability mass function of the negative bi-nomial distribution, using Mosteller and Wallace’s parameterization, is shown in (2): λ x Γ(κ + x) κ κ NegBin(λ,κ)(x) = (2) x! (λ +κ) κ+x"
"If one recalls that the Gamma function is well be-haved and that κ λ (λ + κ) κ expλ = lim 1+ ,= lim κ→∞ κ κ κ κ→∞ it is easy to see that NegBin(λ,κ) converges to Poisson(λ) for λ constant and κ → ∞. On the other hand, small values of κ drag the mode of the nega-tive binomial distribution towards zero and increase its variance, compared with the Poisson."
"As more and more probability mass is concen-trated at 0, the negative binomial distribution starts to depart from the empirical distribution."
"One can already see this tendency in Mosteller and Wallace’s data, although they themselves never comment on it."
The problem with a huge chunk of the proba-bility mass at 0 is that one is forced to say that the outcome 1 is still fairly likely and that the probabil-ity should drop rapidly from 2 onwards as the term 1/x! starts to exert its influence.
This is often at odds with actual data.
"Take the word his in papers by Hamilton and Madison (ibid., pooled from individual sections of Table 2.3–3)."
"It is intuitively clear that his may not occur at all in texts that deal with certain as-pects of the US Constitution, since many aspects of constitutional law are not concerned with any sin-gle (male) person."
"For example, Federalist No. 23 (The Necessity of a Government as Energetic as the One Proposed to the Preservation of the Union, ap-prox. 1800 words, by Hamilton) does not contain a single occurrence of his, whereas Federalist No. 72 (approx. 2000 words, a continuation of No. 71"
"The Duration in Office of the Executive, also by Hamil-ton) contains 35 occurrences."
"The difference is that No. 23 is about the role of a federal government in the abstract, and Nos. 71/72 are about term limits for offices filled by (male) individuals."
"We might there-fore expect the occurrences of his to vary more, de- pending on topic, than any or were."
The overall distribution of his is summarized in Figure 3; full details can be found in Table 1.
"Ob-serve the huge number of passages with zero oc-currences of his, which is ten times the number of passages with exactly one occurrence."
"Also notice how the negative binomial distribution fitted using the Method of Maximum Likelihood ( MLE model, first line in Figure 3, third column in Table 1) over-shoots at 1, but underestimates the number of pas-sages with 2 and 3 occurrences."
The problem cannot be solved by trying to fit the two parameters of the negative binomial based on the observed counts of two points.
The second line in Figure 3 is from a distribution fitted to match the observed counts at 0 and 1.
"Although it fits those two points perfectly, the overall fit is worse than that of the MLE model, since it underestimates the observed counts at 2 and 3 more heavily."
The solution we propose is illustrated by the third line in Figure 3.
"It accounts for only about a third of the data, but covers all passages with one or more occurrences of his."
"Visual inspection suggests that it provides a much better fit than the other two models, if we ignore the outcome 0; a quantitative compari-son will follow below."
This last model has relaxed the relationship between the probability of the out-come 0 and the probabilities of the other outcomes.
"In particular, we obtain appropriate counts for the outcome 1 by pretending that the outcome 0 oc-curs only about 71 times, compared with an actual 405 observed occurrences."
Recall that the model accounts for only 34% of the data; the remaining counts for the outcome 0 are supplied entirely by a second component whose probability mass is con-centrated at zero.
The expected counts under the full model are found in the rightmost column of Table 1.
"The general recipe for models with large counts for the zero outcome is to construe them as two-component mixtures, where one component is a de-generate distribution whose entire probability mass is assigned to the outcome 0, and the other compo-nent is a standard distribution, call it F(θ)."
"Such a nonstandard mixture model is sometimes known as a ‘modified’ distribution ([REF_CITE]§8.4) or, more perspicuously, as a zero-inflated dis-tribution."
"The probability mass function of a zero-inflated F distribution is given by equation (3), where 0 ≤ z ≤ 1 (z &lt; 0 may be allowable subject to additional constraints) and x ≡ 0 is the Kronecker delta δ x,0 ."
"ZIF (z,θ)(x) = z (x ≡ 0) + (1 − z) F (θ)(x) (3) It corresponds to the following generative process: toss a z-biased coin; if it comes up heads, generate 0; if it comes up tails, generate according to F(θ)."
"If we apply this to word frequency in documents, what this is saying is, informally: whether a given word appears at all in a document is one thing; how often it appears, if it does, is another thing."
"This is reminiscent of Church’s statement that ‘[t]he first mention of a word obviously depends on frequency, but surprisingly, the second does not.’[REF_CITE]"
"However, Church was con-cerned with language modeling, and in particular cache-based models that overcome some of the limi-tations introduced by a Markov assumption."
"In such a setting it is natural to make a distinction between the first occurrence of a word and subsequent occur-rences, which according to Church are influenced by adaptati[REF_CITE], referring to an increase in a word’s chance of re-occurrence after it has been spotted for the first time."
"For empirically demonstrating the effects of adaptation,[REF_CITE]worked with nonparametric methods."
"By contrast, our focus is on parametric methods, and unlike in language modeling, we are also interested in words that fail to occur in a document, so it is nat-ural for us to distinguish between zero and nonzero occurrences."
"In Table 1, ZINB refers to the zero-inflated neg-ative binomial distribution, which takes a parame-ter z in addition to the two parameters of its nega-tive binomial component."
"Since the negative bino-mial itself can already accommodate large fractions of the probability mass at 0, we must ask whether the ZINB model fits the data better than a simple nega-tive binomial."
The bottom row of Table 1 shows the negative log likelihood of the maximum likelihood estimate θ̂ for each model.
Log odds of 2 in favor of ZINB are indeed sufficient (on Akaike’s likelihood-based information criterion; see e. g.[REF_CITE]§13.5) to justify the introduction of the additional parameter.
Also note that the cumulative χ 2 proba-bility of the χ 2 statistic at the appropriate degrees of freedom is lower for the zero-inflated distribution.
"It is clear that a large amount of the observed variation of word occurrences is due to zero infla-tion, because virtually all words are rare and many words are simply not “on topic” for a given docu-ment."
"Even a seemingly innocent word like his turns out to be “loaded” (and we are not referring to gen-der issues), since it is not on topic for certain dis-cussions of constitutional law."
"One can imagine that this effect is even more pronounced for taboo words, proper names, or technical jargon (cf.[REF_CITE])."
Our next question is whether the observed variation is best accounted for in terms of zero-inflation or overdispersion.
We phrase the discussion in terms of a practical task for which it matters whether a word is on topic for a document.
Word occurrence counts play an important role in document classification under an independent fea-ture model (commonly known as “Naive Bayes”).
"This is not entirely uncontroversial, as many ap-proaches to document classification use binary in-dicators for the presence and absence of each word, instead of full-fledged occurrence counts (see[REF_CITE]for an overview)."
"In fact,[REF_CITE]claim that for small vocabulary sizes one is generally better off using Bernoulli indicator variables; however, for a sufficiently large vocab-ulary, classification accuracy is higher if one takes word frequency into account."
"Comparing different probability models in terms of their effects on classification under a Naive Bayes assumption is likely to yield very conservative re-sults, since the Naive Bayes classifier can perform accurate classifications under many kinds of adverse conditions and even when highly inaccurate prob-ability estimates are used[REF_CITE]."
"On the other hand, an evaluation in terms of document classification has the advantages, compared with language modeling, of computational simplicity and the ability to benefit from information about non-occurrences of words."
"Making a direct comparison of overdispersed and zero-inflated models with those used[REF_CITE]is difficult, since McCal-lum and Nigam use multivariate models – for which the “naive” independence assumption is different[REF_CITE]– that are not as easily extended to the cases we are concerned about."
"For example, the natural overdispersed variant of the multinomial model is the Dirichlet-multinomial mixture, which adds just a single parameter that globally controls the overall variation of the entire vocabulary."
"How-ever, Church, Gale and other have demonstrated re-peatedly[REF_CITE]that adaptation or “burstiness” are clearly properties of individual words (word types)."
"Using joint inde-pendent models (one model per word) brings us back into the realm of standard independence assump-tions, makes it easy to add parameters that control overdispersion and/or zero-inflation for each word individually, and simplifies parameter estimation."
"So instead of a single multinomial distribution we use independent binomials, and instead of a multivariate Bernoulli model we use independent Bernoulli models for each word."
"The overall joint model is clearly wrong since it wastes probability mass on events that are known a priori to be impos-sible, like observing documents for which the sum of the occurrences of each word is greater than the doc-ument length."
"On the other hand, it allows us to take the true document length into account while using only a subset of the vocabulary, whereas on McCal-lum and Nigam’s approach one has to either com-pletely eliminate all out-of-vocabulary words and adjust the document length accordingly, or else map out-of-vocabulary words to an unknown-word token whose observed counts could then easily dominate."
"In practice, using joint independent models does not cause problems."
We replicated McCallum and Nigam’s Newsgroup experiment 1 and did not find any major discrepancies.
The reader is encour-aged to compare our Figure 4 with McCallum and Nigam’s Figure 3.
"Not only are the accuracy fig-ures comparable, we also obtained the same criti-cal vocabulary size of 200 words below which the Bernoulli model results in higher classification ac-curacy."
"The Newsgroup data set[REF_CITE]is a strati- fied sample of approximately 20,000 messages to-tal, drawn from 20 Usenet newsgroups."
"The fact that 20 newsgroups are represented in equal pro-portions makes this data set well suited for compar-ing different classifiers, as class priors are uniform and baseline accuracy is low at 5%."
Like[REF_CITE]we used (Rain)bow[REF_CITE]for tokenization and to obtain the word/ document count matrix.
"Even though we followed McCallum and Nigam’s tokenization recipe (skip-ping message headers, forming words from contigu-ous alphabetic characters, not using a stemmer), our total vocabulary size of 62,264 does not match Mc-Callum and Nigam’s figure of 62,258, but does come reasonably close."
Also following[REF_CITE]we performed a 4:[Footnote_1] random split into training and test data.
1 Many of the data sets used[REF_CITE]are available[URL_CITE]datasets.html.
The reported results were ob-tained by training classification models on the train-ing data and evaluating on the unseen test data.
We compared four models of token frequency.
"Each model is conditional on the document length n (but assumes that the parameters of the distribution do not depend on document length), and is derived from the binomial distribution n Binom(p)(x | n) = p x (1 − p) n−x , (4) x which we view as a one-parameter conditional model, our first model: x represents the token counts (0 ≤ x ≤ n); and n is the length of the document mea-sured as the total number of token counts, including out-of-vocabulary items."
"The second model is the Bernoulli model, which is derived from the binomial distribution by replac-ing all non-zero counts with 1:"
Bernoulli(p)(x | n)  x n  = Binom(p) | (5) x + 1 n + 1
"Our third model is an overdispersed binomial model, a “natural” continuous mixture of binomi-als with the integrated binomial likelihood – i. e. the Beta density (6), whose normalizing term involves the Beta function – as the mixing distribution. p α−1 (1 − p) β−[Footnote_1]"
1 Many of the data sets used[REF_CITE]are available[URL_CITE]datasets.html.
"Beta(α,β)(p) = (6) B(α,β)"
The resulting mixture model (7) is known as the Pólya–Eggenberger distributi[REF_CITE]or as the beta-binomial distribution.
It has been used for a comparatively small range of NLP applications[REF_CITE]and certainly deserves more widespread attention.
"BetaBin(α,β)(x | n) Z 1 = Binom(p)(x | n) Beta(α,β)(p) dp 0 n B(x + α,n − x + β) = (7) x B(α,β)"
"As was the case with the negative binomial (which is to the Poisson as the beta-binomial is to the bino-mial), it is convenient to reparameterize the distribu-tion."
"We choose a slightly different parameterization than[REF_CITE]; we follow[REF_CITE]and use the identities p = α/(α + β), γ = 1/(α + β + 1)."
"To avoid confusion, we will refer to the distribution parameterized in terms of p and γ as BB: 1 − γ 1 − γ BB(p,γ) = BetaBin p , (1 − p) (8) γ γ"
After reparameterization the expectation and vari-ance are
"E[x;BB(p,γ)(x | n)] = n p,"
"Var[x;BB(p,γ)(x | n)] = n p (1 − p) (1 + (n − 1) γ)."
"Comparing this with the expectation and variance of the standard binomial model, it is obvious that the beta-binomial has greater variance when γ &gt; 0, and for γ = 0 the beta-binomial distribution coincides with a binomial distribution."
Using the method of moments for estimation is particularly straightforward under this parameteri-zati[REF_CITE].
"Suppose one sample consists of observing x successes in n trials (x occur-rences of the target word in a document of length n), where the number of trials may vary across samples."
"Now we want to estimate parameters based on a se-quence of s samples hx 1 ,n 1 i,...,hx s ,n s i."
"We equate sample moments with distribution moments ∑ n i p̂ = ∑ x i , i i ∑ n i p̂ (1 − p̂) (1 + (n i − 1) γ̂) = ∑ (x i − n i p̂) 2 , i i and solve for the unknown parameters: p̂ = ∑ i x i , (9) ∑ i n i γ̂ = ∑ i (x i − n i p̂) 2 /(p̂ (1 − p̂)) − ∑ i n i . (10) ∑ i n [Footnote_2]i − ∑ i n i"
"2 Not that there is anything wrong with that. In fact, we cal-culated the MLE estimates for the negative binomial models us-ing a multidimensional quasi-Newton algorithm."
"In our experience, the resulting estimates are suf-ficiently close to the maximum likelihood esti-mates, while method-of-moment estimation is much faster than maximum likelihood estimation, which requires gradient-based numerical optimization 2 in this case."
"Since we estimate parameters for up to 400,000 models (for 20,000 words and 20 classes), we prefer the faster procedure."
"Note that the maximum likelihood estimates may be suboptimal[REF_CITE], but full-fledged Bayesian methods[REF_CITE]would require even more com-putational resources."
"The fourth and final model is a zero-inflated bino-mial distribution, which is derived straightforwardly via equation (3):"
"ZIBinom(z, p)(x | n) = z (x ≡ 0) + (1 − z) Binom(p)(x | n) z + (1 − z)(1 − p) n if x = 0  = n−x if x &gt; 0 (11) n x (1 − z) x p (1 − p)"
"Since the one parameter p of a single binomial model can be estimated directly using equation (9), maximum likelihood estimation for the zero-inflated binomial model is straightforward via the EM al-gorithm for finite mixture models."
Figure 5 shows pseudo-code for a single EM update.
Accuracy results of Naive Bayes document classi-fication using each of the four word frequency mod-els are shown in Table 2.
"One can observe that the differences between the binomial models are small, but even small effects can be significant on a test set of about 4,000 messages."
"More importantly, note that the beta-binomial and zero-inflated binomial models outperform both the simple binomial and the Bernoulli, except on unrealistically small vocabu-laries (intuitively, 20 words are hardly adequate for discriminating between 20 newsgroups, and those words would have to be selected much more care-fully)."
In light of this we can revise McCallum and Nigam’s[REF_CITE]recommen-dation to use the Bernoulli distribution for small vo-cabularies.
"Instead we recommend that neither the Bernoulli nor the binomial distributions should be used, since in all reasonable cases they are outper-formed by the more robust variants of the binomial distribution. (The case of a 20,000 word vocabulary is quickly declared unreasonable, since most of the words occur precisely once in the training data, and so any parameter estimate is bound to be unreliable.)"
We want to know whether the differences between the three binomial models could be dismissed as a chance occurrence.
"The McNemar test[REF_CITE]provides appropriate answers, which are sum-marized in Table 3."
"As we can see, the classifi-cation results under the zero-inflated binomial and beta-binomial models are never significantly differ- ent, in most cases not even approaching significance at the 5% level."
"A classifier based on the beta-binomial model is significantly different from one based on the binomial model; the difference for a vocabulary of 20,000 words is marginally significant (the χ 2 value of 3.8658 barely exceeds the critical value of 3.8416 required for significance at the 5% level)."
Classification based on the zero-inflated bi-nomial distribution differs most from using a stan-dard binomial model.
"We conclude that the zero-inflated binomial distribution captures the relevant extra-binomial variation just as well as the overdis-persed beta-binomial distribution, since their classi-fication results are never significantly different."
The differences between the four models can be seen more visually clearly on the WebKB data set ([REF_CITE]Figure 4).
Evaluation results for Naive Bayes text classification using the four models are displayed in Figure 6.
"The zero-inflated binomial model provides the overall high-est classification accuracy, and clearly dominates the beta-binomial model."
Either one should be preferred over the simple binomial model.
The early peak and rapid decline of the Bernoulli model had already been observed[REF_CITE].
"We recommend that the zero-inflated binomial distribution should always be tried first, unless there is substantial empirical or prior evidence against it: the zero-inflated binomial model is computation-ally attractive (maximum likelihood estimation us-ing EM is straightforward and numerically stable, most gradient-based methods are not), and its z pa-rameter is independently meaningful, as it can be in-terpreted as the degree to which a given word is “on topic” for a given class of documents."
We have presented theoretical and empirical evi-dence for zero-inflation among linguistic count data.
Zero-inflated models can account for increased vari-ation at least as well as overdispersed models on standard document classification tasks.
"Given the computational advantages of simple zero-inflated models, they can and should be used in place of stan-dard models."
"For document classification, an event model based on a zero-inflated binomial distribu-tion outperforms conventional Bernoulli and bino-mial models."
This paper presents a method to de-velop a class of variable memory Markov models that have higher memory capac-ity than traditional (uniform memory)
The structure of the vari-able memory models is induced from a manually annotated corpus through a de-cision tree learning algorithm.
A series of comparative experiments show the result-ing models outperform uniform memory Markov models in a part-of-speech tag-ging task.
Many major NLP tasks can be regarded as prob-lems of finding an optimal valuation for random processes.
"For example, for a given word se-quence, part-of-speech (POS) tagging involves find-ing an optimal sequence of syntactic classes, and NP chunking involves finding IOB tag sequences (each of which represents the inside, outside and begin-ning of noun phrases respectively)."
"Many machine learning techniques have been de-veloped to tackle such random process tasks, which include Hidden Markov Models (HMMs)[REF_CITE], Maximum Entropy Models (MEs)[REF_CITE], Support Vector Machines (SVMs)[REF_CITE], etc."
"Among them, SVMs have high memory capacity and show high performance, especially when the target classifica-tion requires the consideration of various features."
"On the other hand, HMMs have low memory capacity but they work very well, especially when the target task involves a series of classifications that are tightly related to each other and requires global optimization of them."
"As for POS tagging, recent comparisons[REF_CITE]show that HMMs work better than other models when they are combined with good smoothing techniques and with handling of unknown words."
"While global optimization is the strong point of HMMs, developers often complain that it is difficult to make HMMs incorporate various features and to improve them beyond given performances."
"For ex-ample, we often find that in some cases a certain lexical context can improve the performance of an HMM-based POS tagger, but incorporating such ad-ditional features is not easy and it may even degrade the overall performance."
"Because Markov models have the structure of tightly coupled states, an ar-bitrary change without elaborate consideration can spoil the overall structure."
This paper presents a way of utilizing statistical decision trees to systematically raise the memory capacity of Markov models and effectively to make Markov models be able to accommodate various fea-tures.
The tagging model is probabilistically defined as finding the most probable tag sequence when a word sequence is given (equation (1)).
"T (w 1,k ) = arg max P (t 1,k |w 1,k ) (1) t 1,k = arg max P (t 1,k )"
"P (w 1,k |t 1,k ) (2) t 1,k ≈ arg max Y k P (t i |t i−1 )"
"P (w i |t i ) (3) t 1,k i=1"
"By applying Bayes’ formula and eliminating a re-dundant term not affecting the argument maximiza-tion, we can obtain equation (2) which is a combi-nation of two separate models: the tag language model, P(t 1,k ) and the tag-to-word translation model, P(w 1,k |t 1,k )."
"Because the number of word sequences, w 1,k and tag sequences, t 1,k is infinite, the model of equation (2) is not computationally tractable."
"Introduction of Markov assumption re-duces the complexity of the tag language model and independent assumption between words makes the tag-to-word translation model simple, which result in equation (3) representing the well-known Hidden Markov Model."
Let’s focus on the Markov assumption which is made to reduce the complexity of the original tag-ging problem and to make the tagging problem tractable.
We can imagine the following process through which the Markov assumption can be intro-duced in terms of context classification:
"P (T = t 1,k ) ="
"Y P (t k i |t 1,i−1 ) (4) i=1 ≈ Y k P(t i |Φ(t 1,i−1 )) (5) i=1 ≈ Y k P(t i |t i−1 ) (6) i=1"
"In equation (5), a classification function Φ(t 1,i−1 ) is introduced, which is a mapping of infinite contextual patterns into a set of finite equivalence classes."
By defining the function as follows we can get equation (6) which represents a widely-used bi-gram model:
"Φ(t 1,i−1 ) ≡ t i−1 (7)"
"Equation (7) classifies all the contextual patterns ending in same tags into the same classes, and is equivalent to the Markov assumption."
The assumption or the definition of the above classification function is based on human intuition.
"Although this simple definition works well mostly, because it is not based on any intensive analysis of real data, there is room for improvement."
"Figure 1 and 2 illustrate the effect of context classification on the compiled distribution of syntactic classes, which we believe provides the clue to the improvement."
"Among the four distributions showed in Figure 1, the top one illustrates the distribution of syntactic classes in the Brown corpus that appear after all the conjunctions."
"In this case, we can say that we are considering the first order context (the immediately preceding words in terms of part-of-speech)."
The following three ones illustrates the distributions col-lected after taking the second order context into con-sideration.
"In these cases, we can say that we have extended the context into second order or we have classified the first order context classes again into second order context classes."
"It shows that distri-butions like P (∗|vb, conj) and P (∗|vbp, conj) are very different from the first order ones, while distri-butions like P (∗|fw, conj) are not."
"Figure 2 shows another way of context extension, so called lexicalization."
"Here, the initial first order context class (the top one) is classified again by re-ferring the lexical information (the following three ones)."
"We see that the distribution after the prepo-sition, out is quite different from distribution after other prepositions."
"From the above observations, we can see that by applying Markov assumptions we may miss much useful contextual information, or by getting a better context classification we can build a better context model."
One of the straightforward ways of context exten-sion is extending context uniformly.
Tri-gram tag-ging models can be thought of as a result of the uniform extension of context from bi-gram tagging models.
"TnT[REF_CITE]based on a second or-der HMM, is an example of this class of models and is accepted as one of the best part-of-speech taggers used around."
"The uniform extension can be achieved (rela-tively) easily, but due to the exponential growth of the model size, it can only be performed in restric-tive a way."
Another way of context extension is the selective extension of context.
"In the case of context exten-sion from lower context to higher like the examples in figure 1, the extension involves taking more infor-mation about the same type of contextual features."
"We call this kind of extension homogeneous con-text extension.[REF_CITE]presents this type of context extension method through model merging and splitting, and also prediction suffix tree learn-ing ([REF_CITE]; D. Ron et. al, 1996) is another well-known method that can perform ho-mogeneous context extension."
"On the other hand, figure 2 illustrates heteroge-neous context extension, in other words, this type of extension involves taking more information about other types of contextual features. (Kim et. al, 1999) and[REF_CITE]present this type of con-text extension method, so called selective lexicaliza-tion."
"The selective extension can be a good alternative to the uniform extension, because the growth rate of the model size is much smaller, and thus various contextual features can be exploited."
"In the follow- ing sections, we describe a novel method of selective extension of context which performs both homoge-neous and heterogeneous extension simultaneously."
Our approach to the selective context extension is making use of the statistical decision tree frame-work.
"The states of Markov models are represented in statistical decision trees, and by growing the trees the context can be extended (or the states can be split)."
We have named the resulting models Self-Organizing Markov Models to reflect their ability to automatically organize the structure.
The decision tree is a well known structure that is widely used for classification tasks.
"When there are several contextual features relating to the classifi-cation of a target feature, a decision tree organizes the features as the internal nodes in a manner where more informative features will take higher levels, so the most informative feature will be the root node."
Each path from the root node to a leaf node repre-sents a context class and the classification informa-tion for the target feature in the context class will be contained in the leaf node [Footnote_1] .
"1 While ordinary decision trees store deterministic classifi-cation information in their leaves, statistical decision trees store probabilistic distribution of possible decisions."
"In the case of part-of-speech tagging, a classifi-cation will be made at each position (or time) of a word sequence, where the target feature is the syn-tactic class of the word at current position (or time) and the contextual features may include the syntactic classes or the lexical form of preceding words."
"Fig-ure 3 shows an example of Markov model for a sim-ple language having nouns (N), conjunctions (C), prepositions (P) and verbs (V)."
The dollar sign ($) represents sentence initialization.
"On the left hand side is the graph representation of the Markov model and on the right hand side is the decision tree repre-sentation, where the test for the immediately preced-ing syntactic class (represented by P-1) is placed on the root, each branch represents a result of the test (which is labeled on the arc), and the correspond-ing leaf node contains the probabilistic distribution of the syntactic classes for the current position [Footnote_2] ."
2 The distribution doesn’t appear in the figure explicitly. Just imagine each leaf node has the distribution for the target feature in the corresponding context.
The example shown in figure 4 involves a further classification of context.
"On the left hand side, it is represented in terms of state splitting, while on the right hand side in terms of context extension (lexi-calization), where a context class representing con-textual patterns ending in P (a preposition) is ex-tended by referring the lexical form and is classi-fied again into the preposition, out and other prepo-sitions."
Figure 5 shows another further classification of context.
It involves a homogeneous extension of context while the previous one involves a hetero-geneous extension.
"Unlike prediction suffix trees which grow along an implicitly fixed order, decision trees don’t presume any implicit order between con-textual features and thus naturally can accommodate various features having no underlying order."
"In order for a statistical decision tree to be a Markov model, it must meet the following restric-tions: • There must exist at least one contextual feature that is homogeneous with the target feature. • When the target feature at a certain time is clas-sified, all the requiring context features must be visible"
"The first restriction states that in order to be a Markov model, there must be inter-relations be-tween the target features at different time."
"The sec-ond restriction explicitly states that in order for the decision tree to be able to classify contextual pat-terns, all the context features must be visible, and implicitly states that homogeneous context features that appear later than the current target feature can-not be contextual features."
"Due to the second re-striction, the Viterbi algorithm can be used with the self-organizing Markov models to find an optimal sequence of tags for a given word sequence."
Self-organizing Markov models can be induced from manually annotated corpora through the SDTL algorithm (algorithm 1) we have designed.
It is a variation of ID3 algorithm[REF_CITE].
"SDTL is a greedy algorithm where at each time of the node making phase the most informative feature is se-lected (line 2), and it is a recursive algorithm in the sense that the algorithm is called recursively to make child nodes (line 3),"
"Though theoretically any statistical decision tree growing algorithms can be used to train self-organizing Markov models, there are practical prob-lems we face when we try to apply the algorithms to language learning problems."
"One of the main obsta-cles is the fact that features used for language learn-ing often have huge sets of values, which cause in-tensive fragmentation of the training corpus along with the growing process and eventually raise the sparse data problem."
"To deal with this problem, the algorithm incor-porates a value selection mechanism (line 1) where only meaningful values are selected into a reduced value set."
"The meaningful values are statistically defined as follows: if the distribution of the target feature varies significantly by referring to the value v, v is accepted as a meaningful value."
We adopted the χ 2 -test to determine the difference between the distributions of the target feature before and after re-ferring to the value v. The use of χ 2 -test enables us to make a principled decision about the threshold based on a certain confidence level [Footnote_3] .
"3[REF_CITE]% of confidence level to extend context. In other words, only when there are enough evidences for improve-ment at 95% of confidence level, a context is extended."
"To evaluate the contribution of contextual features to the target classification (line 2), we adopted Lopez distance[REF_CITE]."
"While other measures in-cluding Information Gain or Gain Ratio[REF_CITE]also can be used for this purpose, the Lopez distance has been reported to yield slightly better re-sults[REF_CITE]."
The probabilistic distribution of the target fea-ture estimated on a node making phase (line 4) is smoothed by using Jelinek and Mercer’s interpola-tion method[REF_CITE]along the ancestor nodes.
The interpolation parameters are estimated by deleted interpolation algorithm intro-duced[REF_CITE].
We performed a series of experiments to compare the performance of self-organizing Markov models with traditional Markov models.
Wall Street Jour-nal as contained in Penn Treebank II is used as the reference material.
"As the experimental task is part-of-speech tagging, all other annotations like syntac-tic bracketing have been removed from the corpus."
Every figure (digit) in the corpus has been changed into a special symbol.
"From the whole corpus, every 10’th sentence from the first is selected into the test corpus, and the re-maining ones constitute the training corpus."
Table 6 shows some basic statistics of the corpora.
We implemented several tagging models based on equation (3).
"For the tag language model, we used the following 6 approximations:"
"P (t 1,k ) ≈ Y P (t k i |t i−1 ) (8) i=1 ≈ Y k P(t i |t i−2,i−1 ) (9) i=1 ≈ Y k P(t i |Φ(t i−2,i−1 )) (10) i=1 ≈ Y k P(t i |Φ(t i−1 , w i−1 )) (11) i=1 ≈ Y k P(t i |Φ(t i−2,i−1 , w i−1 )) (12) i=1 ≈ Y k P(t i |Φ(t i−2,i−1 , w i−2,i−1 ))(13) i=1"
Equation (8) and (9) represent first- and second-order Markov models respectively.
For the estimation of the tag-to-word translation model we used the following model:
P (w i |t i ) = k i × P (k i |t i ) × P̂ (w i |t i ) +(1 − k i ) × P (¬k i |t i ) × P̂ (e i |t i ) (14)
"If the word, w i is a known word, k i is set to 1 so the second model is ig-nored."
P̂ means the maximum likelihood probabil-ity.
P(k i |t i ) is the probability of knownness gener-ated from t i and is estimated by using Good-Turing estimati[REF_CITE].
"If the word, w i is an unknown word, k i is set to 0 and the first term is ignored. e i represents suffix of w i and we used the last two letters for it."
"With the 6 tag language models and the 1 tag-to-word translation model, we construct 6 HMM mod-els, among them 2 are traditional first- and second-hidden Markov models, and 4 are self-organizing hidden Markov models."
"Additionally, we used T3, a tri-gram-based POS tagger in ICOPOST release 1.8.3 for comparison."
The overall performances of the resulting models estimated from the test corpus are listed in figure 7.
"From the leftmost column, it shows the model name, the contextual features, the target features, the per-formance and the model size of our 6 implementa-tions of Markov models and additionally the perfor-mance of T3 is shown."
"Our implementation of the second-order hid-den Markov model (HMM-P2) achieved a slightly worse performance than T3, which, we are in-terpreting, is due to the relatively simple imple-mentation of our unknown word guessing module [Footnote_4] ."
"4 T3 uses a suffix trie for unknown word guessing, while our implementations use just last two letters."
"While HMM-P2 is a uniformly extended model from HMM-P1, SOHMM-P2 has been selectively extended using the same contextual feature."
"It is encouraging that the self-organizing model suppress the increase of the model size in half (2,099Kbyte vs 5,630Kbyte) without loss of performance (96.5%)."
"In a sense, the results of incorporating word features (SOHMM-P1W1, SOHMM-P2W1 and SOHMM-P2W2) are disappointing."
The improve-ments of performances are very small compared to the increase of the model size.
"Our interpretation for the results is that because the distribution of words is huge, no matter how many words the mod-els incorporate into context modeling, only a few of them may actually contribute during test phase."
"We are planning to use more general features like word class, suffix, etc."
"Another positive observation is that a homo-geneous context extension (SOHMM-P2) and a heterogeneous context extension (SOHMM-P1W1) yielded significant improvements respectively, and the combination (SOHMM-P2W1) yielded even more improvement."
This is a strong point of using decision trees rather than prediction suffix trees.
"Through this paper, we have presented a framework of self-organizing Markov model learning."
The experimental results showed some encouraging as-pects of the framework and at the same time showed the direction towards further improvements.
"Be-cause all the Markov models are represented as de-cision trees in the framework, the models are hu- man readable and we are planning to develop editing tools for self-organizing Markov models that help experts to put human knowledge about language into the models."
"By adopting χ 2 -test as the criterion for potential improvement, we can control the degree of context extension based on the confidence level."
This paper describes an alternative trans-lation model based on a text chunk un-der the framework of statistical machine translation.
The translation model sug-gested here first performs chunking.
"Then, each word in a chunk is translated."
"Fi-nally, translated chunks are reordered."
"Under this scenario of translation model-ing, we have experimented on a broad-coverage Japanese-English traveling cor-pus and achieved improved performance."
The framework of statistical machine translation for-mulates the problem of translating a source sentence in a language J into a target language E as the maximization problem of the conditional probability Ê = argmax E P(E|J).
The application of the Bayes Rule resulted in Ê = argmax E P(E)P(J|E).
"The for-mer term P(E) is called a language model, repre-senting the likelihood of E. The latter term P(J|E) is called a translation model, representing the gener-ation probability from E into J."
"As an implementation of P(J|E), the word align-ment based statistical translati[REF_CITE]has been successfully applied to similar lan-guage pairs, such as French–English and German– English, but not to drastically different ones, such as Japanese–English."
This failure has been due to the limited representation by word alignment and the weak model structure for handling complicated word correspondence.
This paper provides a chunk-based statistical translation as an alternative to the word alignment based statistical translation.
The translation process inside the translation model is structured as follows.
"A source sentence is first chunked, and then each chunk is translated into target language with local word alignments."
"Next, translated chunks are re-ordered to match the target language constraints."
"Based on this scenario, the chunk-based statis-tical translation model is structured with several components and trained by a variation of the EM-algorithm."
A translation experiment was carried out with a decoder based on the left-to-right beam search.
It was observed that the translation quality improved from 46.5% to 52.1% in BLEU score and from 59.2% to 65.1% in subjective evaluation.
The next section briefly reviews the word align-ment based statistical machine translati[REF_CITE].
"Section 3 discusses an alternative ap-proach, a chunk-based translation model, ranging from its structure to training procedure and decod-ing algorithm."
"Then, Section 4 provides experimen-tal results on Japanese-to-English translation in the traveling domain, followed by discussion."
"Word alignment based statistical translation rep-resents bilingual correspondence by the notion of word alignment A, allowing one-to-many generation from each source word."
"Figure 1 illustrates an exam-ple of English and Japanese sentences, E and J, with sample word alignments."
"In this example, “show 1 ” has generated two words, “mise 5 ” and “tekudasai 6 ”."
"Under this word alignment assumption, the transla-tion model P(J|E) can be further decomposed with-out approximation."
"During the generation process from E to J, P(J, A|E) is assumed to be structured with a couple of pro-cesses, such as insertion, deletion and reorder."
"A scenario for the word alignment based translation model defined[REF_CITE], for instance IBM Model 4, goes as follows (refer to Figure 2). 1. Choose the number of words to generate for each source word according to the Fertility Model."
"For example, “show” was increased to 2 words, while “me” was deleted. 2. Insert NULLs at appropriate positions by the NULL Generation Model."
Two NULLs were inserted after each “show” in Figure 2. 3. Translate word-by-word for each generated word by looking up the Lexicon Model.
One of the two “show” words was translated to “mise.” 4. Reorder the translated words by referring to the Distortion Model.
"The word “mise” was re-ordered to the 5th position, and “uindo” was reordered to the 1st position."
Positioning is de-termined by the previous word’s alignment to capture phrasal constraints.
"For the meanings of each symbol in each model, re-fer[REF_CITE]."
The strategy for the word alignment based transla-tion model is to translate each word by generating multiple single words (a bag of words) and to deter-mine the position of each translated word.
"Although this procedure is sufficient to capture the bilingual correspondence for similar language pairs, some is-sues remain for drastically different pairs: Insertion/Deletion Modeling Although deletion was modeled in the Fertility Model, it merely as-signs zero to each deleted word without considering context."
"Similarly, inserted words are selected by the Lexical Model parameter and inserted at the po-sitions determined by a binomial distribution."
"This insertion/deletion scheme contributed to the simplicity of this representation of the translation processes, allowing a sophisticated application to run on an enormous bilingual sentence collection."
"However, it is apparent that the weak modeling of those phenomena will lead to inferior performance for language pairs such as Japanese and English."
"Local Alignment Modeling The IBM Model 4 (and 5) simulates phrasal constraints, although there were implicitly implemented as its Distortion Model parameters."
"In addition, the entire reordering is determined by a collection of local reorderings in-sufficient to capture the long-distance phrasal con-straints."
"The next section introduces an alternative model-ing, chunk-based statistical translation, which was intended to resolve the above two issues."
"Chunk-based statistical translation models the pro-cess of chunking for both the source and target sen-tences, E and J,"
"P(J, J, E|E) J E where J and E are the chunked sentences for J and E, respectively, defined as two-dimentional ar- rays."
"For instance, J i,j represents the jth word of the ith chunk."
"The number of chunks for source and target is assumed to be equal, |J| = |E|, so that each chunk can convey a unit of meaning without added/subtracted information."
"The term P(J, J, E|E) is further decomposed by chunk align-ment A and word alignment for each chunk transla-tion A."
"P(J, J, E|E) ="
"P(J, J, A, A, E|E) A A"
"The notion of alignment A is the same as those found in the word alignment based translation model, which assigns a source chunk index for each target chunk."
A is a two-dimensional array which assigns a source word index for each target word per chunk.
"For example, Figure 3 shows two-level alignments taken from the example in Figure 1."
"The target chunk at position 3, J 3 , “mise tekudasai” is aligned to the first position (A 3 = 1), and both the words “mise” and “tekudasai” are aligned to the first posi-tion of the source sentence (A 3,1 = 1, A 3,2 = 1)."
"The term P(J, J, A, A, E|E) is further decomposed with approximation according to the scenario de-scribed below (refer to Figure 4). 1. Perform chunking for source sentence E by P(E|E)."
"For instance, chunks of “show me” and “the one” were derived."
The process is mod-eled by two steps: (a) Selection of chunk size (Head Model).
"For each word E i , assign the chunk size ϕ i using the head model (ϕ i |E i )."
"A word with chunk size more than 0 (ϕ i &gt; 0) is treated as a head word, otherwise a non-head (refer to the words in bold in Figure 4). (b) Associate each non-head word to a head word (Chunk Model)."
"Each non-head word E i is associated to a head word E h by the probability η(c(E h )|h − i, c(E i )), where h is the position of a head word and c(E) is a function to map a word E to its word class (i.e. POS)."
"For instance, “the 3 ” is associated with the head word “one 4 ” lo-cated at 4 − 3 = +1. 2."
Select words to be translated with Deletion and Fertility Model. (a) Select the number of head words.
"For each head word E h (ϕ h &gt; 0), choose fertility φ h according to the Fertility Model ν(φ h |E h )."
"We assume that the head word must be translated, therefore φ h &gt; 0."
"In addition, one of them is selected as a head word at target position using a uniform distribu-tion 1/φ h . (b) Delete some non-head words."
"For each non-head word E i (ϕ i = 0), delete it according to the Deletion Model δ(d i |c(E i ),c(E h )), where E h is the head word in the same chunk and d i is 1 if E i is deleted, otherwise 0. 3. Insert some words."
"In Figure 4, NULLs were inserted for two chunks."
"For each chunk E i , select the number of spurious words φ i by In-sertion Model ι(φ i |c(E h )), where E h is the head word of E i . 4. Translate word-by-word."
"Each source word E i , including spurious words, is translated to J j ac-cording to the Lexicon Model, τ(J j |E i ). 5."
"Each word in a chunk is reordered according to the Reorder Model P(A j |E A j ,J j )."
"The chunk reordering is taken after the Distortion Model of IBM Model 4, where the position is determined by the relative position from the head word, |A j | P(A j |E A j , J j ) = ρ(k − h|c(E A A ), c(J j,k )) j,k k=1 where h is the position of a head word for the chunk J j ."
"For example, “no” is positioned −1 of “uindo”. 6. Reorder chunks."
"All of the chunks are reordered according to the Chunk Reorder Model, P(A|E,J)."
"The chunk reordering is also similar to the Distortion Model, where the positioning is determined by the relative posi-tion from the previous alignment |J| P(A|E, J) = ( j − j |c(E A j −1,h ), c(J j,h )) j=1 where j is the chunk alignment of the the pre-vious chunk aE"
"A j −1 . h and h are the head word indices for J j and E A j −1 , respectively."
Note that the reordering is dependent on head words.
"To summarize, the chunk-based translation model can be formulated as"
"P(J|E) = (ϕ i |E i ) E,J,A,"
"A i × η(c(E h i ) |h i − i, c(E i )) i:ϕ i =0 × ν(φ i |E i )/φ i i:ϕ i &gt;0 × δ(d i |c(E i ), c(E h i )) i:ϕ i =0 × ι(φ i |c(E i )) × τ(J j,k |E A j,k ) i:ϕ i &gt;0 j k × P(A j |E A j , J j ) × P(A|E, J) j."
The main difference to the word alignment based translation model is the treatment of the bag of word translations.
"The word alignment based trans-lation model generates a bag of words for each source word, while the chunk-based model con-structs a set of target words from a set of source words."
The behavior is modeled as a chunking pro-cedure by first associating words to the head word of its chunk and then performing chunk-wise trans-lation/insertion/deletion.
The complicated word alignment is handled by the determination of word positions in two stages: translation of chunk and chunk reordering.
The for-mer structures local orderings while the latter con-stitutes global orderings.
"In addition, the concept of head associated with each chunk plays the central role in constraining different levels of the reordering by the relative positions from heads."
The parameter estimation for the chunk-based trans-lation model relies on the EM-algorithm[REF_CITE].
"Given a large bilingual corpus the conditional probability of P(J, A, A, E|J, E) = P(J, J, A, A, E|E)/ J,A,A,E P(J, J, A, A, E|E) is first estimated for each pair of J and E (E-step), then each model parameters is computed based on the estimated conditional probability (M-step)."
The above procedure is iterated until the set of parameters converge.
"However, this naive algorithm will suffer from se-vere computational problems."
The enumeration of all possible chunkings J and E together with word alignment A and chunk alignment A requires a sig-nificant amount of computation.
"Therefore, we have introduced a variation of the Inside-Outside algo-rithm as seen[REF_CITE]for E-step computation."
The details of the procedure are described in Appendix A.
"In addition to the computational problem, there exists a local-maximum problem, where the EM-Algorithm converges to a maximum solution but does not guarantee finding the global maximum."
"In order to solve this problem and to make the pa-rameters converge quickly, IBM Model 4 parame-ters were used as the initial parameters for training."
We directly applied the Lexicon Model and Fertility Model to the chunk-based translation model but set other parameters as uniform.
"The decoding algorithm employed for this chunk-based statistical translation is based on the beam search algorithm for word alignment statistical translation presented[REF_CITE], which generates outputs in left-to-right order by consuming input in an arbitrary order."
The decoder consists of two stages: [Footnote_1]. Generate possible output chunks for all possi-ble input chunks. 2. Generate hypothesized output by consuming input chunks in arbitrary order and combining possible output chunks in left-to-right order.
"1 For simplicity of notation, dependence on other variables are omitted, such as J."
The generation of possible output chunks is es-timated through an inverted lexicon model and sequences of inserted strings[REF_CITE].
"In addition, an example-based method is also introduced, which generates candidate chunks by looking up the viterbi chunking and alignment from a training corpus."
"Since the combination of all possible chunks is computationally very expensive, we have introduced the following pruning and scoring strategies. beampruning: Since the search space is enor-mous, we have set up a size threshold to main-tain partial hypotheses for both of the above two stages."
"We also incorporated a threshold for scoring, which allows partial hypotheses with a certain score to be processed. example-based scoring: Input/output chunk pairs that appeared in a training corpus are “re-warded” so that they are more likely kept in the beam."
"During the decoding process, when a pair of chunks appeared in the first stage, the score is boosted by using this formula in the log domain, log P tm (J|E) + log P lm (E)"
"The corpus for this experiment was extracted from the Basic Travel Expression Corpus (BTEC), a col-lection of conversational travel phrases for Japanese and English[REF_CITE]as seen in Ta-ble 1."
"The entire corpus was split into three parts: 152,169 sentences for training, 4,846 sentences for testing, and the remaining 10,148 sentences for pa-rameter tuning, such as the termination criteria for the training iteration and the parameter tuning for decoders."
Three translation systems were tested for compar-ison: model4:
"Word alignment based translation model, IBM Model 4 with a beam search decoder. chunk3:"
"Chunk-based translation model, limiting the maximum allowed chunk size to 3. model3+: chunk3 with example-based chunk can-didate generation."
Figure 5 shows some examples of viterbi chunking and chunk alignment for chunk3.
Translations were carried out on 510 sentences se-lected randomly from the test set and evaluated ac-cording to the following criteria with 16 reference sets.
"WER: Word-error-rate, which penalizes the edit distance against reference translations."
"Table 2 summarizes the evaluation of Japanese-to- English translations, and Figure 6 presents some of the results by model4 and chunk3+."
"As Table 2 indicates, chunk3 performs better than model4 in terms of the non-subjective evaluations, although it scores almost equally in subjective eval-uations."
"With the help of example-based decoding, chunk3+ was evaluated as the best among the three systems."
The chunk-based translation model was originally inspired by transfer-based machine translation but modeled by chunks in order to capture syntax-based correspondence.
"However, the structures evolved into complicated modeling: The translation model involves many stages, notably chunking and two kinds of reordering, word-based and chunk-based alignments."
"This is directly reflected in parameter estimation, where chunk3 took 20 days for 40 iter-ations, which is roughly the same amount of time required for training IBM Model 5 with pegging."
The unit of chunk in the statistical machine translation framework has been extensively dis-cussed in the literature.
All of these methods bias the training and/or decoding with phrase-level ex-amples obtained by preprocessing a corpus[REF_CITE]or by allowing a lexicon model to hold phrases[REF_CITE].
"On the other hand, the chunk-based transla-tion model holds the knowledge of how to construct a sequence of chunks from a sequence of words."
"The former approach is suitable for inputs with less de- viation from a training corpus, while the latter ap-proach will be able to perform well on unseen word sequences, although chunk-based examples are also useful for decoding to overcome the limited context of a n-gram based language model."
"Both assume that the source part of a translation model is structured either with a se-quence of chunks or with a parse tree, while our method directly models a string-to-string procedure."
It is clear that the string-to-string modeling with hi-den chunk-layers is computationally more expensive than those structure-to-string models.
"However, the structure-to-string approaches are already biased by a monolingual chunking or parsing, which, in turn, might not be able to uncover the bilingual phrasal or syntactical constraints often observed in a corpus."
"The main difference from our work is that their approach is basically deterministic, while the chunk-based translation model is non-deterministic."
"The former method, of course, performs more ef-ficient decoding but requires stronger heuristics to generate a set of transducers."
"Although the latter approach demands a large amount of decoding time and hypothesis space, it can operate on a very broad-coverage corpus with appropriate translation model-ing."
We define noun phrase translation as a subtask of machine translation.
This en-ables us to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical ma-chine translation methods by incorporat-ing special modeling and special features.
Recent research in machine translation challenges us with the exciting problem of combining statisti-cal methods with prior linguistic knowledge.
"The power of statistical methods lies in the quick acquisi-tion of knowledge from vast amounts of data, while linguistic analysis both provides a fitting framework for these methods and contributes additional knowl-edge sources useful for finding correct translations."
We present work that successfully defines a sub-task of machine translation: the translation of noun phrases.
We demonstrate through analysis and ex-periments that it is feasible and beneficial to treat noun phrase translation as a subtask.
"This opens the path to dedicated modeling of other types of syn-tactic constructs, e.g., verb clauses, where issues of subcategorization of the verb play a big role."
"Focusing on a narrower problem allows not only more dedicated modeling, but also the use of com-putationally more expensive methods."
We go on to tackle the task of noun phrase trans-lation in a maximum entropy reranking framework.
Treating translation as a reranking problem instead of as a search problem enables us to use features over the full translation pair.
We integrate both em-pirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation.
"Previous work on defining subtasks within sta-tistical machine translation has been performed on, e.g., noun-noun pair[REF_CITE]and named entity translati[REF_CITE]."
"In this work, we consider both noun phrases and prepositional phrases, which we will refer to as NP/PPs."
We include prepositional phrases for a number of reasons.
Both are attached at the clause level.
"Also, the translation of the preposition of-ten depends heavily on the noun phrase (in the morning)."
"Moreover, the distinction between noun phrases and prepositional phrases is not always clear (note the Japanese bunsetsu) or hard to separate (German joining of preposition and determiner into one lexical unit, e.g., ins in das in the)."
We define the NP/PPs in a sentence as follows:
"Given a sentence and its syntactic parse tree , the NP/PPs of the sentence are the subtrees  that contain at least one noun and no verb, and are not part of a larger subtree that contains no verb."
"The NP/PPs are the maximal noun phrases of the sentence, not just the base NPs."
This definition ex-cludes NP/PPs that consist of only a pronoun.
It also excludes noun phrases that contain relative clauses.
NP/PPs may have connectives such as and.
"For an illustration, see Figure 1."
"To understand the behavior of noun phrases in the translation process, we carried out a study to exam-ine how they are translated in a typical parallel cor-pus."
"Clearly, we cannot simply expect that certain syntactic types in one language translate to equiv-alent types in another language."
Equivalent types might not even exist.
This study answers the questions:
Do human translators translate noun phrases in foreign texts into noun phrases in English?
"If all noun phrases in a foreign text are trans-lated into noun phrases in English, is an accept-able sentence translation possible?"
What are the properties of noun phrases which cannot be translated as noun phrases without rendering the overall sentence translation unac-ceptable?
"Using the Europarl corpus [Footnote_1] , we consider a trans-lation task from German to English."
We marked the NP/PPs in the German side of a small 100 sentence parallel corpus manually.
We examined if these units are realized as noun phrases in the English side of the parallel corpus.
This is the case for 75% of the NP/PPs.
"Second, we tried to construct translations of these NP/PPs that take the form of NP/PPs in English in an overall acceptable translation of the sentence."
We could do this for 98% of the NP/PPs.
The four exceptions are: in Anspruch genommen; Gloss: take in demand Abschied nehmen; take good-bye meine Zustimmung geben; give my agreement in der Hauptsache; in the main-thing
The first three cases are noun phrases or preposi-tional phrases that merge with the verb.
"This is simi-lar to the English construction make an observation, which translates best into some languages as a verb equivalent to observe."
"The fourth example, literally translated as in the main thing, is best translated as mainly."
Why is there such a considerable discrepancy be-tween the number of noun phrases that can be trans-lated as noun phrases into English and noun phrases that are translated as noun phrases?
"The main reason is that translators generally try to translate the meaning of a sentence, and do not feel bound to preserve the same syntactic structure."
This leads them to sometimes arbitrarily restructure the sentence.
"Also, occasionally the translations are sloppy."
The conclusion of this study is: Most NP/PPs in German are translated to English as NP/PPs.
"Nearly all of them, 98%, can be translated as NP/PPs into English."
The exceptions to this rule should be treated as special cases and handled separately.
We carried out studies for Chinese-English and Portuguese-English NP/PPs with similar results.
One interesting question is if external context is nec-essary for the translation of noun phrases.
"While the sentence and document context may be available to the NP/PP subsystem, the English output is only as-sembled later and therefore harder to integrate."
"To address this issue, we carried out a manual ex-periment to check if humans can translate NP/PPs without any external context."
"Using the same corpus of 168 NP/PPs as in the previous section, a human translator translated 89% of the noun phrases cor-rectly, 9% had the wrong leading preposition, and only 2% were mistranslated with the wrong content word meaning."
"Picking the right phrase start (e.g., preposition or determiner) can sometimes only be resolved when the English verb is chosen and its subcategoriza-tion is known."
"Otherwise, sentence context does not play a big role: Word choice can almost always be resolved within the internal context of the noun phrase."
The findings of the previous section indicate that NP/PP translation can be conceived as a separate subsystem of a complete machine translation system – with due attention to special cases.
We will now estimate the importance of such a system.
"As a general observation, we note that NP/PPs cover roughly half of the words in news or similar texts."
All nouns are covered by NP/PPs.
"Nouns are the biggest group of open class words, in terms of the number of distinct words."
"Constantly, new nouns are added to the vocabulary of a language, be it by borrowing foreign words such as Fahrvergnügen or Zeitgeist, or by creating new words from acronyms such as AIDS, or by other means."
"In addition to new words, new phrases with distinct meanings are constantly formed: web server, home page, instant messaging, etc."
Learning new concepts from text sources when they become available is an elegant solution for this knowledge acquisition problem.
"In a preliminary study, we assess the impact of an NP/PP subsystem on the quality of an overall ma-chine translation system."
We try to answer the fol-lowing questions:
What is the impact on a machine translation system if noun phrases are translated in isola-tion?
What is the performance gain for a machine translation system if an NP/PP subsystem pro-vides perfect translations of the noun phrases?
"We built a subsystem for NP/PP translation that uses the same modeling as the overall system (IBM Model 4), but is trained on only NP/PPs."
"With this system, we translate the NP/PPs in isolation, with-out the assistance of sentence context."
"These trans-lations are fixed and provided to the general machine translation system, which does not change the fixed NP/PP translation."
"In a different experiment, we also provided cor-rect translations (motivated by the reference transla-tion) for the NP/PPs to the general machine trans-lation system."
We carried out these experiments on the same 100 sentence corpus as in the previous sec-tions.
The results are summarized in Table 1.
"Treating NP/PPs as isolated units, and translating them in iso- lation with the same methods as the overall system has little impact on overall translation quality."
"In fact, we achieved a slight improvement in results due to the fact that NP/PPs are consistently trans-lated as NP/PPs."
A perfect NP/PP subsystem would triple the number of correctly translated sentences.
"Performance is also measured by the BLEU score[REF_CITE], which measures similarity to the reference translation taken from the English side of the parallel corpus."
"These findings indicate that solving the NP/PP translation problem would be a significant step to-ward improving overall translation quality, even if the overall system is not changed in any way."
The findings also indicate that isolating the NP/PP trans-lation task as a subtask does not harm performance.
"When translating a foreign input sentence, we detect its NP/PPs and translate them with an NP/PP trans-lation subsystem."
The best translation (or multiple best translations) is then passed on to the full sen-tence translation system which in turn translates the remaining parts of the sentence and integrates the chosen NP/PP translations.
Our NP/PP translation subsystem is designed as follows: We train a translation system on a NP/PP parallel corpus.
We use this system to generate an n-best list of possible translations.
We then rescore this n-best list with the help of additional features.
This design is illustrated by Figure 2.
"To evaluate our methods, we automatically detected all of the 1362 NP/[REF_CITE]sentences from parts of the Europarl corpus which are not already used as training data."
Our evaluation metric is human as-sessment:
Can the translation provided by the sys-tem be part of an acceptable translation of the whole sentence?
"In other words, the noun phrase has to be translated correctly given the sentence context."
The NP/PPs are extracted in the same way that NP/PPs are initially detected for the acquisition of the NP/PP training corpus.
"This means that there are some problems with parse errors, leading to sen-tence fragments extracted as NP/PPs that cannot be translated correctly."
"Also, the test corpus contains all detected NP/PPs, even untranslatable ones, as discussed in Section 2.2."
"To train a statistical machine translation model, we need a training corpus of NP/PPs paired with their translation."
We create this corpus by extracting NP/PPs from a parallel corpus.
"First, we word-align the corpus with Giza++[REF_CITE]."
"Then, we parse both sides with syn-tactic parsers ([REF_CITE]; Schmidt and Schulte im[REF_CITE]) [Footnote_2] ."
2 English parser available[URL_CITE]German parser available[URL_CITE]
Our definition easily translates into an algorithm to detect NP/PPs in a sentence.
"Recall that in such a corpus, only part of the NP/PPs are translated as such into the foreign lan-guage."
"In addition, the word-alignment and syntac-tic parses may be faulty."
"As a consequence, initially only 43.4% of all NP/PPs could be aligned."
We raise this number to 67.2% with a number of automatic data cleaning steps:
NP/PPs that partially align are broken up Systematic parse errors are fixed
"Certain word types that are inconsistently tagged as nouns in the two languages are har-monized (e.g., the German wo and the English today)."
"Because adverb + NP/PP constructions (e.g., specifically this issue are inconsistently parsed, we always strip the adverb from these construc-tions."
"German verbal adjective constructions are bro-ken up if they involve arguments or adjuncts (e.g., der von mir gegessene Kuchen = the by me eaten cake), because this poses problems more related to verbal clauses."
Alignment points involving punctuation are stripped from the word alignment.
Punctuation is also stripped from the edges of NP/PPs.
"A total of 737,388 NP/PP pairs are collected from the German-English Europarl corpus as train-ing data."
Certain German NP/PPs consistently do not align to NP/PPs in English (see the example in Sec-tion 2.2).
These are detected at this point.
The obtained data of unaligned NP/PPs can be used for dealing with these special cases.
"Given the NP/PP corpus, we can use any general sta-tistical machine translation method to train a transla-tion system for noun phrases."
"As a baseline, we use an IBM Model 4[REF_CITE]system [Footnote_3] with a greedy decoder 4[REF_CITE]."
We found that phrase based models achieve better translation quality than IBM Model [Footnote_4].
"Such mod-els segment the input sequence into a number of (non-linguistic) phrases, translate each phrase using a phrase translation table, and allow for reordering of phrases in the output."
No phrases may be dropped or added.
We use a phrase translation model that extracts its phrase translation table from word alignments gen-erated by the Giza++ toolkit.
Details of this model are described[REF_CITE].
"To obtain an n-best list of candidate translations, we developed a beam search decoder."
This decoder employs hypothesis recombination and stores the search states in a search graph – similar to work[REF_CITE]– which can be mined with stan-dard finite state machine methods [Footnote_5] for n-best lists.
5 We use the Carmel toolkit available[URL_CITE]
One key question for our approach is how often an acceptable translation can be found in an n-best list.
"The answer to this is illustrated in Figure 3: While an acceptable translation comes out on top for only about 60% of the NP/PPs in our test corpus, one can be found in the 100-best list for over 90% of the NP/PPs [Footnote_6] ."
"6 Note that these numbers are obtained after compound split-ting, described in Section 4.1"
This means that rescoring has the potential to raise performance by 30%.
What are the problems with the remaining 10% for which no translation can be found?
"To investi-gate this, we carried out an error analysis of these NP/PPs."
Results are given in Table 2.
"The main sources of error are unknown words (34%) or words for which the correct translation does not occur in the training data (14%), and errors during tagging and parsing that lead to incorrectly detected NP/PPs (28%)."
"There are also problems with NP/PPs that require complex syntactic restructuring (7%), and NP/PPs that are too long, so an acceptable translation could not be found in the 100-best list, but only further down the list (6%)."
"There are also NP/PPs that can-not be translated as NP/PPs into English (2%), as discussed in Section 2.2."
"Given an n-best list of candidates and additional fea-tures, we transform the translation task from a search problem into a reranking problem, which we address using a maximum entropy approach."
"As training data for finding feature values, we col-lected a development corpus of 683 NP/PPs."
NP/PP comes with an n-best list of candidate trans-lations that are generated from our base model and are annotated with accuracy judgments.
"The initial features are the logarithm of the probability scores that the model assigns to each candidate transla-tion: the language model score, the phrase transla-tion score and the reordering (distortion) score."
The task for the learning method is to find a prob-ability distribution  that indicates if the can-didate translation is an accurate translation of the input .
The decision rule to pick the best translation is best argmax  .
The development corpus provides the empirical probability distribution by distributing the proba-bility mass over the acceptable translations  :   .
"If none of the candidate trans-lations for a given input is acceptable, we pick the candidates that are closest to reference translations measured by minimum edit distance."
We use a maximum entropy framework to parametrize this probability distribution as !&quot; exp # %$ &amp; ( where the &amp; ’s are the feature values and the $ ’s are the feature weights.
"Since we have only a sample of the possible trans-lations for the given input , we normalize the probability distribution, so that # !&quot;  our sample  + of candidate translations. *) for"
"Maximum entropy learning finds a set of fea-ture values $ so that ,./102&amp; 43 , &quot;5- 02&amp; 73 for 6 each feature &amp; ."
These expectations are com-puted as sums over all candidate translations for all inputs : #98;:=&lt; &gt; ? @! &amp; ( # 8B=: &lt; &gt;   &amp; ( .
A nice property of maximum entropy training is that it converges to a global optimum.
There are a number of methods and tools available to carry out this training of feature values.
We use the toolkit [Footnote_7] developed[REF_CITE].
7 Available[URL_CITE]mal ouf/pubs.html
"Note that any other machine learning, such as sup-port vector machines, could be used as well."
We chose maximum entropy for its ability to deal with both real-valued and binary features.
"This method is also similar to work[REF_CITE], who use maximum entropy to tune model parameters."
We will now discuss the properties of NP/PP trans-lation that we exploit in order to improve our NP/PP translation subsystem.
"The first of these (compound-ing of words) is addressed by preprocessing, while the others motivate features which are used in n-best list reranking."
"Compounding of words, especially nouns, is com-mon in a number of languages (German, Dutch, Finnish, Greek), and poses a serious problem for machine translation: The word Aktionsplan may not be known to the system, but if the word were bro-ken up into Aktion and Plan, the system could easily translate it into action plan, or plan for action."
"The issues for breaking up compounds are: Knowing the morphological rules for joining words, resolving ambiguities of breaking up a word (Haupt-sturm Haupt-Turm or Haupt-Sturm), and finding the right level of splitting granularity (Frei-Tag or Freitag)."
"Here, we follow an approach introduced[REF_CITE]: First, we collect fre-quency statistics over words in our training cor-pus."
Compounds may be broken up only into known words in the corpus.
For each potential compound we check if morphological splitting rules allow us to break it up into such known words.
"Finally, we pick a splitting option (perhaps not breaking up the compound at all)."
This decision is based on the frequency of the words involved.
"Specifically, we pick the splitting option with highest geometric mean of word frequencies of its parts : best argmax S  -  count"
The German side of both the training and testing corpus is broken up in this way.
"The base model is trained on a compound-split corpus, and input is broken up before being passed on to the system."
"This method works especially well with our phrase-based machine translation model, which can recover more easily from too eager or too timid splits than word-based models."
"After performing this type of compound splitting, hardly any errors occur with respect to compounded words."
"Generally speaking, the performance of statistical machine translation systems can be improved by better translation modeling (which ensures corre-spondence between input and output) and language modeling (which ensures fluent English output)."
"Language modeling can be improved by different types of language models (e.g., syntactic language models), or additional training data for the language model."
"Here, we investigate the use of the web as a lan-guage model."
"In preliminary studies we found that 30% of all 7-grams in new text can be also found on the web, as measured by consulting the search en-gine Google [URL_CITE] , which currently indexes 3 billion web pages."
This is only the case for 15% of 7-grams gen-erated by the base translation system.
There are various ways one may integrate this vast resource into a machine translation system:
"By building a traditional n-gram language model, by us-ing the web frequencies of the n-grams in a candi-date translation, or by checking if all n-grams in a candidate translation occur on the web."
We settled on using the following binary features: Does the candidate translation as a whole occur in the web?
Do all n-grams in the candidate translation occur on the web?
Do all n-grams in the candidate translation occur at least 10 times on the web?
We use both positive and negative features for n-grams of the size 2 to 7.
We were not successful in improving performance by building a web n-gram language model or using the actual frequencies as features.
The web may be too noisy to be used in such a straight-forward way without significant smoothing efforts.
"Unlike in decoding, for reranking we have the com-plete candidate translation available."
This means that we can define features that address any prop-erty of the full NP/PP translation pair.
One such set of features is syntactic features.
Syntactic features are computed over the syntac-tic parse trees of both input and candidate transla-tion.
"For the input NP/PPs, we keep the syntactic parse tree we inherit from the NP/PP detection pro-cess."
"For the candidate translation, we use a part-of-speech tagger and syntactic parser to annotate the candidate translation with its most likely syntactic parse tree."
We use the following three syntactic features:
"Preservation of the number of nouns: Plural nouns generally translate as plural nouns, while singular nouns generally translate as singular Preservation of prepositions: base preposi-tional phrases within NP/PPs generally trans-late as prepositional phrases, unless there is movement involved."
BaseNPs generally trans-late as baseNPs.
German genitive baseNP are treated as basePP.
"Within a baseNP/PP the determiner generally agree in number with the final noun (e.g., not: this nice green flowers)."
"The features are realized as integers, i.e., how many nouns did not preserve their number during translation?"
These features encode relevant general syntactic knowledge about the translation of noun phrases.
They constitute soft constraints that may be over-ruled by other components of the system.
"As described in Section 3.1, we evaluate the per-formance of our NP/PP translation subsystem on a blind test set of 1362 NP/PPs extracted from 534 sentences."
The contributions of different compo-nents of our system are displayed in Table 3.
"Starting from the IBM Model 4 baseline, we achieve gains using our phrase-based translation model (+5.5%), applying compound splitting to special modeling and additional features: Correct NP/PPs and BLEU score for overall sentence trans-lation training and test data (+2.8%), re-estimating the weights for the system components using the maximum entropy reranking frame-work (+1.5%), adding web count features (+1.7%) and syntactic features (+0.8%)."
Overall we achieve an improve-ment of 12.3% over the baseline.
Improvements of 2.5% are statistically significant given the size of our test corpus.
Table 3 also provides scores for overall sentence translation quality.
The chosen NP/PP translations are integrated into a general IBM Model 4 sys-tem that translates whole sentences.
"Performance is measured by the BLEU score, which measures sim-ilarity to a reference translation."
As reference trans-lation we used the English side of the parallel cor-pus.
"The BLEU scores track the improvements of our components, with an overall gain of 0.027."
We have shown that noun phrase translation can be separated out as a subtask.
"Our manual experiments show that NP/PPs can almost always be translated as NP/PPs across many languages, and that the transla-tion of NP/PPs usually does not require additional external context."
"We also demonstrated that the reduced complex-ity of noun phrase translation allows us to address the problem in a maximum entropy reranking frame-work, where we only consider the 100-best candi-dates of a base translation system."
"This enables us to introduce any features that can be computed over a full translation pair, instead of being limited to features that can be integrated into the search algo-rithm of the decoder, which only has access to partial translations."
"We improved performance of noun phrase trans-lation by 12.3% by using a phrase translation model, a maximum entropy reranking method and address-ing specific properties of noun phrase translation: compound splitting, using the web as a language model, and syntactic features."
"We showed not only improvement on NP/PP translation over best known methods, but also improved overall sentence trans-lation quality."
Our long term goal is to address additional syntac-tic constructs in a similarly dedicated fashion.
"The next step would be verb clauses, where modeling of the subcategorization of the verb is important."
Phrase level translation models are ef-fective in improving translation qual-ity by addressing the problem of local re-ordering across language boundaries.
Methods that attempt to fundamentally modify the traditional IBM translation model to incorporate phrases typically do so at a prohibitive computational cost.
We present a technique that begins with im-proved IBM models to create phrase level knowledge sources that effectively repre-sent local as well as global phrasal con-text.
"Our method is robust to noisy align-ments at both the sentence and corpus level, delivering high quality phrase level translation pairs that contribute to signif-icant improvements in translation quality (as measured by the BLEU metric) over word based lexica as well as a competing alignment based method."
Statistical Machine Translation defines the  task of  translating a source language sentence   into a target language sentence .
The traditional framework presented[REF_CITE]assumes a generative process where the source sentence is passed through a noisy stochastic process to produce the target sentence.
The task can be formally  !&quot; stated as finding the s.t =  where the search compo-nent is commonly referred to as the decoding step[REF_CITE].
"Within the generative model #! % , $ the  Bayes &amp;!   reformulation  is used to estimate where &amp;!  is considered the lan-guage model, and is the translation model; the IBM[REF_CITE]models being the de facto standard."
"Direct translation  approaches ! &quot;[REF_CITE]consider estimating directly, and work[REF_CITE]show that similar ! or improved results are achieved #!  by replacing in the optimization with , at the cost of devi-ating from the Bayesian framework."
"Regardless of the approach, the question of accurately estimating a model of translation from a large parallel or com-parable corpus is one of the defining components within statistical machine translation."
"Re-ordering effects across languages have been modeled in several ways, including word-based[REF_CITE], template-based[REF_CITE]and syntax-based (Yamada,[REF_CITE])."
"Analyzing these models from a generative mind-set, they all assume that the atomic unit of lexi-cal content is the word, and re-ordering effects are applied above that level. (Marcu,[REF_CITE]) il-lustrate the effects of assuming that lexical corre-spondence can only be modeled at the word level, and motivate a joint probability model that explic-itly generates phrase level lexical content across both languages.[REF_CITE]presents a bracketing method that models re-ordering at the sentence level."
"Both (Marcu,[REF_CITE]) model the re-ordering phenomenon effectively, but at significant computational expense, and tend to be difficult to scale to long sentences."
"Reasons to introduce phrase level translation knowledge sources have been ade- quately shown and confirmed by (Och,[REF_CITE]), and we focus on methods to build these sources from existing, mature components within the translation process."
This paper presents a method of phrase extraction from alignment data generated by IBM Models.
"By working directly from alignment data with appro-priate measures taken to extract accurate translation pairs, we try to avoid the computational complex-ity that can result from methods that try to create globally consistent alignment model phrase segmen-tations."
"We first describe the information available within alignment data, and go on to describe a method for extracting high quality phrase translation pairs from such data."
"We then discuss the implications of adding phrasal translation pairs to the decoding pro-cess, and present evaluation results that show sig-nificant improvements when applying the described extraction technique."
We end with a discussion of strengths and weaknesses of this method and the po-tential for future work.
Alignment models associate words and their transla-tions at the sentence level creating a translation lexi-con across the language pair.
"For each sentence pair, the model also presents the maximally likely associ-ation between each source and target word across the sentence pair, forming an alignment map for each sentence pair in the training corpus."
"The most likely alignment pattern between a source and target sen-tence under the trained alignment model will be re-ferred to as the maximum approximation, which un-der HMM alignment[REF_CITE]model cor-responds to the Viterbi path."
A set of words in the source sentence associated with a set of words in the target sentence is considered a phrasal pair and forms a partition within the alignment map.
Fig-ure ( . shows a source and target sentence pair with points indicating alignment points.
"A phrasal translation pair within a sentence pair can be represented as the 4-tuple , hypothesis +* &apos;, -&apos;. #/ .324) model (both the translation lexicon as well as the maximal approximation)."
"The maximal approxima-tion captures context at the sentence level, while the lexicon provides a corpus level translation esti-mate, motivating the alignment model as a starting point for phrasal extraction."
"The extraction tech-nique must be able to handle alignments that are only partially correct, as well as cases where the sen-tence pairs have been incorrectly matched as parallel translations within the corpus."
"Accommodating for the noisy corpus is an increasingly important com-ponent of the translation process, especially when considering languages where no manually aligned parallel corpus is available."
"Building a phrasal lexicon involves Generation, Scoring, and Pruning steps, corresponding to gen-erating a set of candidate translation pairs, scoring them based on the translation model, and pruning them to account for noise within the data as well as the extraction process."
The generation step refers to the process of identify-ing source phrases that require translations and then extracting translations from the alignment model data.
We begin by identifying all source language n-grams upto some &lt; within the training corpus.
"When the test sentences that require translation are known, we can simply extract those n-grams that appear in the test sentences."
"For each of these n-grams, we create a set of candidate translations extracted from the corpus."
"The primary motivation to restrict the identification step to the test sentence n-grams is savings in computational expense, and the result is a phrasal translation source that extracts translation pairs limited to the test sentences."
"For each source language n-gram within the pool, we have to find a set of candidate translations &gt;) . ="
"The generation task is formally defined as finding in Equation (1) ) = ?@:A )+* &apos;-&apos;, 6. / -10B-&apos;C2D. FE ) -   (1) where is the source ) n-gram for which we are ex-tracting translations, is the set of , all partitions, and  refers ) to = the word at position in the source sentence . is then the set of all translations for source n-gram , and M is a specific translation hypothesis within this set."
"When considering only those hypothesis translation extracted ) = from a partic-ular sentence pair , we use ."
"We extract these candidates from the alignment map by examining each sentence pair where the source n-gram occurs, and extracting all possible tar-get phrase translations using a sliding window ap-proach."
"We extract candidate translations of phrase length ( to N , starting at offset O to NQPR( ."
Figure 1. shows circular boxes indicating each potential parti-tion region.
One particular partition is indicated by the shading.
"Over all occurrences of the n-gram within the sen-tences as well as across sentences, a sizeable can-didate pool is generated that attempts the cover the translated usage of the source n-gram within the corpus."
"This set is large, and contains several spuri-ous translations, and does not consider other source side n-grams within each sentence."
The deliberate choice to avoid creating a consistent partitioning of the sentence pairs across n-grams reflects the abil-ity to model partially correct alignments within sen-tences.
"This sliding window can be 6. / restricted . to ex-clude word-word translations, ie ( , ( if other sources are available that are known to be more accurate."
"Now that the candidate pool has been gen-erated, it needs to be scored and pruned to reflect rel-ative confidence between candidate translations and to remove spurious translations due to the sliding window approach."
The candidate translations for the source n-gram now need to be scored and ranked according to some measure of confidence.
"Each candidate translation pair defines a partition within the sentence map, and this partitioning can be scored for confidence in translation quality."
"We estimate translation con-fidence by measures from three models; the estima-tion from the maximum approximation (alignment map), estimation from the word based translation lexicon, and language specific measures."
Each of the scoring methods discussed below contributes to the final score under (2)
"V , ."
E ) = ]_^ E ) = 7aGb  8YX[\ M&lt; 8YX[#\ 9 M 9 (2) where c 9ed 9 = ( and M refers to a translation hy-pothesis for a given source n-gram .
From now on we will refer to a 8YX[\ with regard to a particular implicitly.
"We define two kinds of scores, within sentence con-sistency and across sentence consistency from the alignment map, in order to represent local and global context effects."
The partition defined by each candidate translation pair imposes constraints over the maximum approx-imation hypothesis for sentences in which it occurs.
We evaluate the partition by examining its consis-tency with the maximum approximation hypothe-sis by considering the alignment hypothesis * points  within the sentence.
"An alignment point f (source, target) is said to be consistent ) * ,&apos;-&apos;. /  if . it 2 occurs within the partition defined by . fji&quot;k l is considered inconsistent in two cases. ,Tm ,:no6. /"
"LgqpF0 gsrF0tno.321 and or (3)  C2. px, ro,:n . / and or (4) ) * &apos;-&apos;, .6/ -10B-&apos;.C24+ , , . /) ="
Each ( + definesin ) determines a set of consistent and inconsistent points.
Figure 1. shows inconsistent points with re-spect to the shaded partition by drawing an X over the alignment point.
"The within sentence consis-tency scoring metric is defined in Equation (5). / +) * ,&apos;-&apos;. #/ 324.   z X[&lt; 8YX[\ a , z &lt;WX[&lt; z [X Z`&lt; (5)"
"This )+* ,&apos;-&apos;. /#-1 measure .324 represents consistency of within the maximal approxima-tion alignment for sentence pair ."
Several hypothesis ) within = 5{ are | similar { or iden-tical to those in where .
"We want to score hypothesis that are consistent across sentences higher than those that occur rarely, as the former are assumed to be the correct translations in context."
"We want to account for different contexts across sen-tences; therefore we want to highlight similar trans-lations, not simply exact matches."
We use a word level Levenstein distance ) = to compare the target side ) = hypotheses within .
"Each element M within (the complete candidate translation list for ) is as-signed the average Levenstein distance with all other elements as its across sentence consistence score; ef-fectively performing a single pass average link clus-tering to identify the correct translations. ` ~/ ( 8YX[\#} M (6) -G  c&quot; &quot; e M M where e calculates the Levenshein distance be-tween  the target phrases within two ) hypothesis = M and M ,  is the number of elements / in ."
"The higher the 8YX[\ } , the more likely the hy-pothesis pair is a correct translation."
"The clustering approach accounts for noise due to incorrect sen-tence alignment, as well as the different contexts in which a particular source n-gram can be used."
"As predicted by the formulation of this method, preference is given towards shorter target transla-tions."
This effect can be countered by introducing a phrase length model to approximate the difference in phrases lengths across the language boundary.
This will be discussed further as a language specific scor-ing method.
The methods presented above used the maximum approximation to score candidate translation hy-potheses.
The translation lexicon generated by the IBM models provides translation estimates at the word level built on the complete training corpus.
These corpus level estimates can be integrated into our scoring paradigm to balance the sentence level estimates from the alignment map methods.
The translation lexicon  ! provides a conditional *  probability estimate i l for each f ( i refers to the word at position in sentence ) within the maximum approximation.
"Depending on the direction in which the traditional IBM models are trained, we can either condition on the source or tar-get side, while joint probability models can give us a bidirectional estimate."
These translation *  probability estimates are used to weight the f within the methods described above.
"Instead of simply * count-   ing the number of consistent/inconsistent  ! f , we * sum  the probability estimates i l for each f ."
So far we have only considered the points within the partition where alignment points are pre-dicted by the maximal approximation.
"The transla-tion lexicon provides estimates at the word level, so we can construct +) * a scoring &apos;-&apos;, . /#.324 measure for the complete region within that models the com-plete probability of the partition."
"The lexical scoring equation below models this effect. ) * ,&apos;-&apos;. /#324. + ^   ! 8X \ I i i l  i LIKJ   l LI (7)"
This method prefers longer target side phrases due to the sum over the target words within the parti-tion.
"Although it would also prefer short source side phrases, we are only concerned with comparing hy-pothesis partitions for a given source n-gram ."
"The nature of the phrasal association between lan-guages varies depending on the level of inflexion, morphology as well as other factors."
The predomi-nant language specific correction to the scoring tech-niques discussed above models differences in phrase lengths across languages.
"For example, when com-paring English and Chinese translations, we see that on average, the English sentence is approximately 1.3 times longer (under our current segmentation in the small data track)."
"To model these language specific effects, we introduce a phrase length scor-ing component that is based on the ratio of sen-tence length between languages."
"We build a sen-tence length model ,7% based , on the [ DiffRatio statis-tic defined as  where I is the Z source sentence length and J is the 7, target % sentence , length."
"Let  be the average  Z over the sentences in the corpus, and % be the vari-ance; thereby defining a normal distribution over the DiffRatio statistic."
"Using the standard Z normaliza-tion technique under - a normal distribution param-eterized by     , we can estimate the proba-bility that a new DiffRatio calculated on the phrasal pair can be generated by the model, giving us the scoring estimate below. ) * -&apos;, 6. / -10B-&apos;.32D  ¡5. / -&apos;. ¢!+ - 8YX[\ I7   £ (8)"
To improve the model we might consider exam-ining known phrase translation pairs if this data is available.
We explore the language specific differ-ence further by noting that English phrases contain several function words that typically align to the empty Chinese word.
We accounted for this effect within the scoring process by treating all target lan-guage (English) phrases that only differed by the function words on the phrase boundary as the same translation.
The burden of selecting the appropriate hypothesis within the decoding process is moved to-wards the language model under this corrective strat-egy.
"The list of candidate translations for each source n-gram is large, and must be pruned to select the most likely set of translations."
This pruning is re-quired to ensure that the decoding process remains computationally tractable.
"Simple threshold meth-ods that rank hypotheses by their final score and only save the top  hypotheses will not work here, since phrases differ in the number of possible correct translations they could have when used in different contexts. ) ="
"Given the score ordered set of candidate phrases , we would like to label some subset as incorrect translations and remove them from the set."
We approach this task as a density estimation prob-lem where we need to separate the distribution of the incorrectly translated hypothesis from the dis-tribution of the likely translations.
"Instead of using the maximum likelihood criteria, we use the maxi-mal separation criteria ie. selecting a splitting point within the scores to maximize the difference of the mean score between distributions as shown below.. ,  &quot;¥ * P¦ B§ * * 8: (9) 8X \ where  &quot;¥ * is the mean score of those hypothesis with a score less than , and  &quot;§ * is the mean score of those hypothesis with a greater than or equal to ."
"Once pruning is completed, we convert the scores into a probability measure conditioned on the source n-gram and assign the probability estimate as the translation probability for the hypothesis M as shown below."
"V , .  & lt;!    8YX[\ M"
"M (10) c  «ª ¬ V , &lt; . 8YX[\ M (10 # ) ! calculates direct translation probabilities, ie ."
"As mentioned earlier,[REF_CITE], show that using direction translation estimates in the &amp;! decoding  process as compared with calculating as prescribed by the Bayesian framework does not reduce translation quality."
Our results corrob-orate these findings and we use (10) as the phrase level translation model estimate within our decoder.
"Phrase translation pairs that are generated by the method described in this paper are finally scored with estimates of translation probability, which can be conditioned on the target language if necessary."
"These estimates fit cleanly into the decoding pro-cess, except for the issue of phrase length."
"Tra-ditional word lexicons propose translations for one source word, while with phrase translations, a single hypothesis pair can span several words in the source or target language."
Comparing between a path that uses a phrase compared to one that uses multiple words (even if the constituent words are the same) is difficult.
"The word level pathway involves the product of several probabilities, whereas the phrasal path is represented by one probability score."
Po-tential solutions are to introduce translation length models or to learn scaling factors for phrases of dif-ferent lengths.
"Results in this paper have been gener-ated by empirically determining a scaling factor that was inversely proportional to the lenth of the phrase, causing each translation to have a score compara-ble to the product of the word to word translations within the phrase."
"In order to compare our method to a well under-stood phrase baseline, we present a method that ex- tracts phrases by harvesting the Viterbi path from an HMM alignment model[REF_CITE]."
"The HMM alignment model is computationally feasible even for very long sentences, and the phrase ex-traction method does not have limits on the length of extracted target side phrase. , For , each source phrase ranging from #0 ¯ positions ,  to  the 4, target 0 phrase ¯ is given  by 9°4, %9 , , #?´´? ? @, £ and 0 &lt; } i £ , where  and  refers to an index in the target sentence pair."
We cal-culate phrase translation probabilities (the scores for each extracted phrase) based on a statistical lexicon for the constituent words in the phrase.
"As the IBM1 alignment model gives the global optimum for the lexical probabilities, this is the natural choice."
"This leads to the phrase translation probability ! µ  ( ^  ! ¶ 9  (11)  9 ¶ where µ and N denotes  the length of the target phrase  , ! source phrase , and the word probabil-ities 9  are estimated using the IBM1 word alignment model."
The phrases extracted from this method can be used directly within our in-house decoder without the significant changes that other phrase based methods could require.
IBM alignment models were trained up to model 4 using GIZA[REF_CITE]from Chi-nese to English and Chinese to English on two tracks of data.
Figures describing the characteris-tics of each track as well as the test sentences are shown in Table (1).
All the data were extracted from a newswire source.
"We applied our in house segmentation toolkit on the Chinese data and per-formed basic preprocessing which included; lower-casing, tagging dates, times and numbers on both languages."
"Translation quality is evaluated by two metrics,[REF_CITE]and BLEU[REF_CITE], both of which measure n-gram matches between the translated text and the reference trans-lations."
NIST is more sensitive to unigram precision due to its emphasis toward high perplexity words.
Four reference translations were available for each test sentence.
"We first compare against a system built using word level lexica only to reiterate the im-pact of phrase translation, and then show gains by our method over a system that utilizes phrase ex-tracted from the HMM method."
The word level sys-tem consisted of a hand crafted (Linguistics Data Consortium) bilingual dictionary and a statistical lexicon derived from training IBM model 1.
"In our experiments we found that although training higher order IBM models does yield lower alignment error rates when measured against manually aligned sen-tences, the highest translation quality is achieved by using a lexicon extracted from the Model 1 align-ment."
Experiments were run with a language model (LM) built on a 20 million word news source corpus using our in house decoder which performs a mono-tone decoding without reordering.
"To implement our phrase extraction technique, the maximum approx-imation alignments were combined with the union operation as described[REF_CITE], result-ing in a dense but inaccurate alignment map as mea-sured against a human aligned gold standard."
"Since bi-directional translation models are available, scor-ing was performed in both directions, using IBM Model 1 lexica for the within sentence scoring."
The final phrase level scores computed in each direction were combined by a weighted average before the pruning step.
Source side phrases were restricted to be of length 2 or higher since word lexica were available.
Weights for each scoring metric were de-termined empirically against a validation set (align-ment map scores were assigned the highest weight-ing).
"Table (2) shows results on the small data track, while Table (3) shows results on the large data track. "
The technique described in this paper is la-belled  \ in the tables.
The results show that the phrase extraction method described in this paper contribute to statistically significant improvements over the baseline word and phrase level(HMM) sys-tems.
"When compared against the HMM phrases, our technique show statistically significant improve-ments."
Statistical significance is evaluated by con- sidering deviations in sentence level NIST scores over the 993 sentence test set with a NIST improve-ment of 0.05 being statistically significant at the 0.01 alpha level.
"In combination with the HMM method, our technique delivers further gains, providing evi-dence that different kinds of phrases have been learnt by each method."
The improvements caused by our methods is more apparent in the NIST score rather than the BLEU score.
We predict that this effect is due to the language specific correction that treats tar-get phrases with function words at the boundaries as the same phrase.
This correction cause the burden to be placed on the language model to select the cor-rect phrase instance from several possible transla-tions.
Correctly translating function words dramati-cally boosts the NIST measure as it places emphasis on high perplexity words ie. those with diverse con-texts.
We have presented a method to efficiently ex-tract phrase relationships from IBM word alignment models by leveraging the maximum approximation as well as the word lexicon.
"Our method is signifi-cantly less computationally expensive than methods that attempt to explicitly model phrase level inter-actions within alignment models, and recovers well from noisy alignments at the sentence and corpus level."
The significant improvements above the base-line carry through when this method is combined with other phrasal and word level methods.
"Further experimentation is required to fully appreciate the robustness of this technique, especially when con-sidering a comparable, but not parallel, corpus."
"The language specific scoring methods have a significant impact on translation quality, and further work to ex-tend these methods to represent specific characteris-tics of each language, promises to deliver further im-provements."
"Although the method performs well, it lacks an explanatory framework through the extrac-tion process; instead it leverages the well understood fundamentals of the traditional IBM models."
"Combining phrase level knowledge sources within a decoder in an effective manner is currently our primary research interest, specifically integrat-ing knowledge sources of varying reliability."
Our method has shown to be an effective contributing component within the translation framework and we expect to continue to improve the state of the art within machine translation by improving phrasal ex-traction and integration.
"This paper proposes the use of uncertainty reduction in machine learning methods such as co-training and bilingual boot-strapping, which are referred to, in a gen-eral term, as ‘collaborative bootstrapping’."
The paper indicates that uncertainty re-duction is an important factor for enhanc-ing the performance of collaborative bootstrapping.
It proposes a new measure for representing the degree of uncertainty correlation of the two classifiers in col-laborative bootstrapping and uses the measure in analysis of collaborative boot-strapping.
"Furthermore, it proposes a new algorithm of collaborative bootstrapping on the basis of uncertainty reduction."
Ex-perimental results have verified the cor-rectness of the analysis and have demonstrated the significance of the new algorithm.
We consider here the problem of collaborative bootstrapping.
It includes co-training[REF_CITE]and bilingual bootstrapping[REF_CITE].
Collaborative bootstrapping begins with a small number of labelled data and a large number of unlabelled data.
"It trains two (types of) classifiers from the labelled data, uses the two classifiers to label some unlabelled data, trains again two new classifiers from all the labelled data, and repeats the above process."
"During the process, the two classifiers help each other by exchanging the la-belled data."
"In co-training, the two classifiers have different feature structures, and in bilingual boot-strapping, the two classifiers have different class structures."
"Their analyses, however, cannot be directly used in studies of co-training in (Nigam &amp;[REF_CITE]) and bilingual bootstrapping."
"In this paper, we propose the use of uncertainty reduction in the study of collaborative bootstrap-ping (both co-training and bilingual bootstrapping)."
We point out that uncertainty reduction is an im-portant factor for enhancing the performances of the classifiers in collaborative bootstrapping.
"Here, the uncertainty of a classifier is defined as the por-tion of instances on which it cannot make classifi-cation decisions."
Exchanging labelled data in bootstrapping can help reduce the uncertainties of classifiers.
Uncertainty reduction was previously used in active learning.
We think that it is this paper which for the first time uses it for bootstrapping.
We propose a new measure for representing the uncertainty correlation between the two classifiers in collaborative bootstrapping and refer to it as ‘uncertainty correlation coefficient’ (UCC).
We use UCC for analysis of collaborative bootstrap-ping.
We also propose a new algorithm to improve the performance of existing collaborative boot-strapping algorithms.
"In the algorithm, one classi-fier always asks the other classifier to label the most uncertain instances for it."
Experimental results indicate that our theoreti-cal analysis is correct.
Experimental results also indicate that our new algorithm outperforms exist-ing algorithms.
"Co-training, proposed[REF_CITE], conducts two bootstrapping processes in parallel, and makes them collaborate with each other."
"More specifically, it repeatedly trains two classifiers from the labelled data, labels some unlabelled data with the two classifiers, and ex-changes the newly labelled data between the two classifiers."
Blum and Mitchell assume that the two classifiers are based on two subsets of the entire feature set and the two subsets are conditionally independent with one another given a class.
This assumption is called ‘view independence’.
"In their algorithm of co-training, one classifier always asks the other classifier to label the most certain in-stances for the collaborator."
The word sense dis-ambiguation method proposed[REF_CITE]can also be viewed as a kind of co-training.
"Since the assumption of view independence cannot always be met in practice,[REF_CITE]proposed a co-training algorithm based on ‘agreement’ between the classifiers."
"As for theoretical analysis,[REF_CITE]gave a bound on the generalization error of co-training within the framework of PAC learning."
The generalization error is a function of ‘dis-agreement’ between the two classifiers.
"Dasgupta et al’s result is based on the view independence assumption, which is strict in practice."
Dasgupta et al’s result by relaxing the view independence assumption with a new constraint.
He also proposed a new co-training algorithm on the basis of the constraint.
"For other work on co-training, see[REF_CITE]."
"Instead of making an assumption on the features, bilingual bootstrapping makes an assump-tion on the classes."
"Specifically, it assumes that the classes of the classifiers in bootstrapping do not overlap."
"Thus, bilingual bootstrapping is different from co-training."
"Because the notion of agreement is not involved in bootstrapping in (Nigam &amp;[REF_CITE]) and bilingual bootstrapping, Dasgupta et al and Abney’s analyses cannot be directly used on them."
Active leaning is a learning paradigm.
"Instead of passively using all the given labelled instances for training as in supervised learning, active learning repeatedly asks a supervisor to label what it con-siders as the most critical instances and performs training with the labelled instances."
"Thus, active learning can eventually create a reliable classifier with fewer labelled instances than supervised learning."
"One of the strategies to select critical in-stances is called ‘uncertain reduction’ (e.g.,[REF_CITE])."
"Under the strategy, the most un-certain instances to the current classifier are se-lected and asked to be labelled by a supervisor."
"The notion of uncertainty reduction was not used for bootstrapping, to the best of our knowl-edge."
We consider the collaborative bootstrapping prob-lem.
Let denote a set of instances (feature vectors) and let denote a set of labels (classes).
"Given a number of labelled instances, we are to construct a function h : → ."
We also refer to it as a classi-fier.
"In collaborative bootstrapping, we consider the use of two partial functions h 1 and h 2 , which either output a class label or a special symbol ⊥ denoting ‘no decision’."
Co-training and bilingual bootstrapping are two examples of collaborative bootstrapping.
"In co-training, the two collaborating classifiers are assumed to be based on two different views, namely two different subsets of the entire feature set."
"Formally, the two views are respectively inter-preted as two functions X 1 (x) and X 2 (x) , x ∈ ."
"Thus, the two collaborating classifiers h 1 and h 2 in co-training can be respectively represented as h 1 (X 1 (x)) and h 2 (X 2 (x)) ."
"In bilingual bootstrapping, a number of classifi-ers are created in the two languages."
"The classes of the classifiers correspond to word senses and do not overlap, as shown in Figure 1."
"For example, the classifier h 1 (x|E 1 ) in language 1 takes sense 2 and sense 3 as classes."
"The classifier h 2 ( x|C 1 ) in language 2 takes sense 1 and sense 2 as classes, and the classifier h 2 (x|C 2 ) takes sense 3 and sense 4 as classes."
"Here we use E 1 , C 1 , C 2 to de-note different words in the two languages."
Collabo-rative bootstrapping is performed between the classifiers h 1 (∗) in language 1 and the classifiers h 2 (∗) in language 2. ([REF_CITE]for de-tails).
"For the classifier h 1 ( x| E 1 ) in language 1, we assume that there is a pseudo classifier h 2 ( x|C 1 ,C 2 ) in language 2, which functions as a collaborator of h 1 (x|E 1 ) ."
"The pseudo classifier h 2 ( x |C 1 ,C 2 ) is based on h 2 ( x |C 1 ) and h 2 ( x |C 2 ) , and takes sense 2 and sense 3 as classes."
"Formally, the two collaborating classifiers (one real classifier and one pseudo classifier) in bilin-gual bootstrapping are respectively represented as h 1 (x | E) and h 2 (x | C ) , x ∈ ."
"Next, we introduce the notion of uncertainty re-duction in collaborative bootstrapping."
Definition 1 The uncertainty U (h) of a classi-fier h is defined as:
U (h) =
"P ({ x | h( x) =⊥ , x ∈ }) (1) In practice, we define U (h) as U(h) ="
"P({x | C(h(x) = y) &lt; θ , ∀y ∈ , x∈ }) (2) where θ denotes a predetermined threshold and C (∗) denotes the confidence score of the classifier h."
Definition The conditional uncer- 2 tainty U(h | y) of a classifier h given a class y is defined as:
U(h | y) =
"P({x | h(x) =⊥, x ∈ }| Y = y) (3)"
We note that the uncertainty (or conditional un-certainty) of a classifier (a partial function) is an indicator of the accuracy of the classifier.
Let us consider an ideal case in which the classifier achieves 100% accuracy when it can make a classi-fication decision and achieves 50% accuracy when it cannot (assume that there are only two classes).
"Thus, the total accuracy on the entire data space is 1 − 0.5 × U (h) ."
"Definition 3 Given the two classifiers h 1 and h 2 in collaborative bootstrapping, the uncertainty re-duction of h 1 with respect to h 2 (denoted as UR(h 1 \ h 2 ) ), is defined as"
UR(h 1 \ h 2 ) =
"P({x | h 1 (x) =⊥,h 2 (x) ≠⊥,x∈ }) (4) Similarly, we have UR(h 2 \ h 1 ) ="
"P({x | h 1 (x) ≠⊥,h 2 (x) =⊥,x∈ })"
Uncertainty reduction is an important factor for determining the performance of collaborative boot-strapping.
"In collaborative bootstrapping, the more the uncertainty of one classifier can be reduced by the other classifier, the higher the performance can be achieved by the classifier (the more effective the collaboration is)."
We introduce the measure of uncertainty correla-tion coefficient (UCC) to collaborative bootstrap-ping.
"Definition 4 Given the two classifiers h 1 and h 2 , the conditional uncertainty correlation coefficient (CUCC) between h 1 and h 2 given a class y (denoted as r h 1 h 2 y ), is defined as"
"P(h 1 (x) =⊥, h 2 (x) =⊥| Y = y) rh1h2y ="
P(h (x) =⊥| Y = y)P(h (x) =⊥| Y = y) (5) 1 2
"Definition 5 The uncertainty correlation coeffi-cient (UCC) between h 1 and h 2 (denoted as R h 1 h 2 ), is defined as"
R h 1 h 2 = P( y )r h 1 h 2 y (6) y
UCC represents the degree to which the uncer- tainties of the two classifiers are related.
"If UCC is high, then there are a large portion of instances which are uncertain for both of the classifiers."
"Note that UCC is a symmetric measure from both classi-fiers’ perspectives, while UR is an asymmetric measure from one classifier’s perspective (ei-ther UR ( h 1 \ h 2 ) or UR ( h 2 \ h 1 ) )."
Theorem 1 reveals the relationship between the CUCC (UCC) measure and uncertainty reduction.
Assume that the classifier h 1 can collaborate with either of the two classifiers h 2 and h&apos; 2 .
The two classifiers h 2 and h 2 ′ have equal conditional uncertainties.
The CUCC values between h 1 and h 2 ′ are smaller than the CUCC values between h 1 and h 2 .
"Then, according to Theorem 1, h 1 should collaborate with h 2 ′ , because h 2 ′ can help reduce its uncertainty more, thus, improve its accuracy more."
"Theorem 1 Given the two classifier pairs (h 1 , h 2 ) and (h 1 , h 2 ′ ) , if r h and 1 h 2 y ≥ r h 1 h 2 ′y , y ∈ U(h 2 | y) =U(h 2 ′ | y), y ∈ , then we have UR (h 1 \ h 2 ) ≤ UR (h 1 \ h′ 2 )"
We can decompose the uncertainty U(h 1 ) of h 1 as follows:
"U(h 1 ) = P({x| h 1 (x) =⊥,x∈ }|Y = y)P(Y = y) y = (P({x| h 1 (x) =⊥,h 2 (x) =⊥,x∈ }|Y = y) y + P({x| h 1 (x) =⊥,h 2 (x) ≠⊥,x∈ }|Y = y))P(Y = y) = (r h P({x | h 1 (x) =⊥,x∈ }|Y = y) 1 h 2 y y ⋅ P({x| h 2 (x) =⊥,x∈ }|Y = y) +"
"P({x| h 1 (x) =⊥,h 2 (x) ≠⊥,x∈ }|Y = y))P(Y = y) = (r h U(h 1 | y)U(h 2 | y) 1 h 2 y y + P({x| h 1 (x) =⊥,h 2 (x) ≠⊥,x∈ }|Y = y))P(Y = y) = (r h U(h 1 | y)U(h 2 | y)P(Y = y) 1 h 2 y y + P({x | h 1 (x) =⊥,h 2 (x) ≠⊥,x∈ }))"
UR(h 1 \ h 2 ) =
"P({x | h 1 (x) =⊥, h 2 (x) ≠⊥, x ∈ }) = U (h 1 ) − r h U (h 1 | y)U (h 2 | y)P(Y = y) 1 h 2 y y"
"Theorem 1 states that the lower the CUCC val-ues are, the higher the performances can be achieved in collaborative bootstrapping."
"Definition 6 The two classifiers in co-training are said to satisfy the view independence assump-ti[REF_CITE], if the following equations hold for any class y."
"P ( X 1 = x 1 | Y = y , X 2 = x 2 ) ="
P ( X 1 = x 1 | Y = y )
"P ( X 2 = x 2 | Y = y , X 1 = x 1 ) ="
P ( X 2 = x 2 | Y = y )
"Theorem 2 If the view independence assump-tion holds, then r h 1 h 2 y = 1.0 holds for any class y."
"According[REF_CITE], view independence implies classifier independence:"
"P(h 1 = u | Y = y,h 2 = v) ="
P(h 1 = u | Y = y)
"P(h 2 = v | Y = y,h 1 = u) ="
P(h 2 = v | Y = y)
We can rewrite them as
"P ( h 1 = u ,, h 2 = v | Y = y ) ="
P ( h 1 = u | Y = y )
P ( h 2 = v | Y = y )
"Thus, we have P ({ x | h 1 ( x ) =⊥ , h 2 ( x ) =⊥ , x ∈ }| Y = y ) ="
"P ({ x | h 1 ( x ) =⊥ , x ∈ }| Y = y ) P ({ x | h 2 ( x ) =⊥ x ∈ }| Y = y )"
"It means r h 1 h 2 y = 1.0, ∀y ∈"
"Theorem 2 indicates that in co-training with view independence, the CUCC values ( r h ,∀y ∈ ) are small, since by defini- 1 h 2 y tion 0 &lt; r h 1 h 2 y &lt; ∞ ."
"According to Theorem 1, it is easy to reduce the uncertainties of the classifiers."
"That is to say, co-training with view independence can perform well."
How to conduct theoretical evaluation on the CUCC measure in bilingual bootstrapping is still an open problem.
We conducted experiments to empirically evaluate the UCC values of collaborative bootstrapping.
We also investigated the relationship between UCC and accuracy.
The results indicate that the theoreti-cal analysis in Section 4.2 is correct.
"In the experiments, we define accuracy as the percentage of instances whose assigned labels agree with their ‘true’ labels."
"Moreover, when we refer to UCC, we mean that it is the UCC value on the test data."
We set the value of θ in Equation (2) to 0.8.
Co-Training for Artificial Data Classification We used the data[REF_CITE]to conduct co-training.
We utilized the articles from four newsgroups (see Table 1).
Each group had 1000 texts.
"By joining together randomly selected texts from each of the two newsgroups in the first row as positive instances and joining together randomly selected texts from each of the two newsgroups in the second row as negative instances, we created a two-class classification data with view independ-ence."
"The joining was performed under the condi-tion that the words in the two newsgroups in the first column came from one vocabulary, while the words in the newsgroups in the second column came from the other vocabulary."
We also created a set of classification data without view independence.
"To do so, we ran-domly split all the features of the pseudo texts into two subsets such that each of the subsets contained half of the features."
We next applied the co-training algorithm to the two data sets.
We conducted the same pre-processing in the two experiments.
"We discarded the header of each text, removed stop words from each text, and made each text have the same length, as did[REF_CITE]."
We randomly separated the data and performed co-training with random feature split and co-training with natural feature split in five times.
"The results obtained (cf., Table 2), thus, were averaged over five trials."
"In each trial, we used 3 texts for each class as labelled training instances, 976 texts as testing instances, and the remaining 1000 texts as unlabelled training instances."
"From Table 2, we see that the UCC value of the natural split (in which view independence holds) is lower than that of the random split (in which view independence does not hold)."
"That is to say, in natural split, there are fewer instances which are uncertain for both of the classifiers."
The accuracy of the natural split is higher than that of the random split.
"Theorem 1 states that the lower the CUCC values are, the higher the performances can be achieved."
"The results in Table 2 agree with the claim of Theorem 1. (Note that it is easier to use CUCC for theoretical analysis, but it is easier to use UCC for empirical analysis). split (view independence) is about 1.0."
The result agrees with Theorem 2.
We used the same data[REF_CITE]to perform co-training for web page classifi-cation.
The web page data consisted of 1051 web pages collected from the computer science departments of four universities.
The goal of classification was to determine whether a web page was concerned with an academic course. 22% of the pages were actually related to academic courses.
The features for each page were possible to be separated into two independent parts.
One part consisted of words occurring in the current page and the other part consisted of words occurring in the anchor texts pointed to the current page.
"We randomly split the data into three subsets: labelled training set, unlabeled training set, and test set."
The labelled training set had 3 course pages and 9 non-course pages.
The test set had 25% of the pages.
The unlabelled training set had the re-maining data. web page classification.
The setting for the experiment was almost the same as that of Nigam and Ghani’s.
"One exception was that we did not 5 Uncertainty Reduction Algorithm conduct feature selection, because we were not able to follow their method from their paper."
We repeated the experiment five times and evaluated the results in terms of UCC and accuracy.
Table 3 shows the average accuracy and UCC value over the five trials.
We conducted experiments to test the effectiveness of our new algorithm.
Experimental results indi-cate that the new algorithm performs better than the previous algorithm.
We refer to them as ‘new’ and ‘old’ respectively.
Co-Training for Artificial Data Classification
We used the artificial data in Section 4.3 and con-ducted co-training with both the old and new algo-rithms.
Table 5 shows the results.
"We see that in co-training the new algorithm performs as well as the old algorithm when UCC is low (view independence holds), and the new algo-rithm performs significantly better than the old al-gorithm when UCC is high (view independence does not hold)."
"This paper has theoretically and empirically dem-onstrated that uncertainty reduction is the essence of collaborative bootstrapping , which includes both co-training and bilingual bootstrapping ."
"The paper has conducted a new theoretical analysis of collaborative bootstrapping, and has proposed a new algorithm for collaborative boot-strapping, both on the basis of uncertainty reduc-tion."
Experimental results have verified the correctness of the analysis and have indicated that the new algorithm performs better than the existing algorithms.
This paper presents a new bootstrapping approach to named entity (NE) classification.
"This approach only requires a few common noun/pronoun seeds that correspond to the concept for the target NE type, e.g. he/she/man/woman for PERSON NE."
The entire bootstrapping procedure is implemented as training two successive learners: (i) a decision list is used to learn the parsing-based high precision NE rules; (ii) a Hidden Markov Model is then trained to learn string sequence-based NE patterns.
The second learner uses the training corpus automatically tagged by the first learner.
The resulting NE system approaches supervised NE performance for some NE types.
The system also demonstrates intuitive support for tagging user-defined NE types.
The differences of this approach from the co-training-based NE bootstrapping are also discussed.
Named Entity (NE) tagging is a fundamental task for natural language processing and information extraction.
"An NE tagger recognizes and classifies text chunks that represent various proper names, time, or numerical expressions."
"Seven types of named entities are defined in the Message Understanding Conference (MUC) standards, namely, PERSON (PER), ORGANIZATION (ORG), LOCATION (LOC), TIME, DATE, MONEY, and PERCENT [Footnote_1][REF_CITE]."
1 This paper only focuses on classifying proper names. Time and numerical NEs are not yet explored using this method.
There is considerable research on NE tagging using different techniques.
"These include systems based on handcrafted rules[REF_CITE], as well as systems using supervised machine learning, such as the Hidden Markov Model (HMM)[REF_CITE]and the Maximum Entropy Model[REF_CITE]."
The state-of-the-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain.
"However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult."
Such systems cannot effectively support user-defined named entities.
"That is the motivation for using unsupervised or weakly-supervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli &amp;[REF_CITE]) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. (Cucerzan &amp;[REF_CITE]), (Collins &amp;[REF_CITE]) and[REF_CITE]presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or handcrafted NE rules."
NE tagging has two tasks: (i) NE chunking; (ii) NE classification.
"Parsing-supported NE bootstrapping systems including ours only focus on NE classification, assuming NE chunks have been constructed by the parser."
The key idea of co-training is the separation of features into several orthogonal views.
"In case of NE classification, usually one view uses the context evidence and the other relies on the lexicon evidence."
Learners corresponding to different views learn from each other iteratively.
One issue of co-training is the error propagation problem in the process of the iterative learning.
The rule precision drops iteration-by-iteration.
"In the early stages, only few instances are available for learning."
This makes some powerful statistical models such as HMM difficult to use due to the extremely sparse data.
This paper presents a new bootstrapping approach using successive learning and concept-based seeds.
The successive learning is as follows.
"First, some parsing-based NE rules are learned with high precision but limited recall."
"Then, these rules are applied to a large raw corpus to automatically generate a tagged corpus."
"Finally, an HMM-based NE tagger is trained using this corpus."
"There is no iterative learning between the two learners, hence the process is free of the error propagation problem."
The resulting NE system approaches supervised NE performance for some NE types.
"To derive the parsing-based learner, instead of seeding the bootstrapping process with NE instances from a proper name list or handcrafted NE rules as (Cucerzan &amp;[REF_CITE]), (Collins &amp;[REF_CITE]) and[REF_CITE], the system only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE, e.g. he/she/man/woman for PERSON NE."
"Such concept-based seeds share grammatical structures with the corresponding NEs, hence a parser is utilized to support bootstrapping."
"Since pronouns and common nouns occur more often than NE instances, richer contextual evidence is available for effective learning."
"Using concept-based seeds, the parsing-based NE rules can be learned in one iteration so that the error propagation problem in the iterative learning can be avoided."
This method is also shown to be effective for supporting NE domain porting and is intuitive for configuring an NE system to tag user-defined NE types.
The remaining part of the paper is organized as follows.
The overall system design is presented in Section 2.
Section 3 describes the parsing-based NE learning.
Section 4 presents the automatic construction of annotated NE corpus by parsing-based NE classification.
Section 5 presents the string level HMM NE learning.
Benchmarks are shown in Section 6.
Section 7 is the Conclusion.
Figure 1 shows the overall system architecture.
"Before the bootstrapping is started, a large raw training corpus is parsed by the English parser from our InfoXtract system[REF_CITE]."
"The bootstrapping experiment reported in this paper is based on a corpus containing ~100,000 news articles and a total of ~88,000,000 words."
"The parsed corpus is saved into a repository, which supports fast retrieval by a keyword-based indexing scheme."
"Although the parsing-based NE learner is found to suffer from the recall problem, we can apply the learned rules to a huge parsed corpus."
"In other words, the availability of an almost unlimited raw corpus compensates for the modest recall."
"As a result, large quantities of NE instances are automatically acquired."
An automatically annotated NE corpus can then be constructed by extracting the tagged instances plus their neighboring words from the repository.
The bootstrapping is performed as follows: 1. Concept-based seeds are provided by the user. 2.
Parsing structures involving concept-based seeds are retrieved from the repository to train a decision list for NE classification. 3.
The learned rules are applied to the NE candidates stored in the repository. 4.
The proper names tagged in Step 3 and their neighboring words are put together as an NE annotated corpus. 5.
An HMM is trained based on the annotated corpus.
"The training of the first NE learner has three major properties: (i) the use of concept-based seeds, (ii) support from the parser, and (iii) representation as a decision list."
"This new bootstrapping approach is based on the observation that there is an underlying concept for any proper name type and this concept can be easily expressed by a set of common nouns or pronouns, similar to how concepts are defined by synsets in WordNet[REF_CITE]."
Concept-based seeds are conceptually equivalent to the proper name types that they represent.
These seeds can be provided by a user intuitively.
"For example, a user can use pill, drug, medicine, etc. as concept-based seeds to guide the system in learning rules to tag MEDICINE names."
"This process is fairly intuitive, creating a favorable environment for configuring the NE system to the types of names sought by the user."
"An important characteristic of concept-based seeds is that they occur much more often than proper name seeds, hence they are effective in guiding the non-iterative NE bootstrapping."
A parser is necessary for concept-based NE bootstrapping.
"This is due to the fact that concept-based seeds only share pattern similarity with the corresponding NEs at structural level, not at string sequence level."
"For example, at string sequence level, PERSON names are often preceded by a set of prefixing title words Mr./Mrs./Miss/Dr. etc., but the corresponding common noun seeds man/woman etc. cannot appear in such patterns."
"However, at structural level, the concept-based seeds share the same or similar linguistic patterns (e.g. Subject-Verb-Object patterns) with the corresponding types of proper names."
The rationale behind using concept-based seeds in NE bootstrapping is similar to that for parsing-based word clustering[REF_CITE]: conceptually similar words occur in structurally similar context.
"In fact, the anaphoric function of pronouns and common nouns to represent antecedent NEs indicates the substitutability of proper names by the corresponding common nouns or pronouns."
"For example, this man can be substituted for the proper name John Smith in almost all structural patterns."
"Following the same rationale, a bootstrapping approach is applied to the semantic lexicon acquisition task [Thelen &amp; Riloff. 2002]."
The InfoXtract parser supports dependency parsing based on the linguistic units constructed by our shallow parser[REF_CITE].
Five types of the decoded dependency relationships are used for parsing-based NE rule learning.
"These are all directional, binary dependency links between linguistic units: (1) Has_Predicate: from logical subject to verb e.g. He said she would want him to join."
Æ he: Has_Predicate(say) she: Has_Predicate(want) him: Has_Predicate(join) (2) Object_Of : from logical object to verb e.g. This company was founded to provide new telecommunication services.
"Æ company: Object_Of(found) service: Object_Of(provide) (3) Has_Amod: from noun to its adjective modifier e.g. He is a smart, handsome young man."
Æ man: Has_AMod(smart) man: Has_AMod(handsome) man: Has_AMod(young) (4) Possess: from the possessive noun-modifier to head noun e.g. His son was elected as mayor of the city.
Æ his: Possess(son) city: Possess(mayor) (5) IsA: equivalence relation from one NP to another NP e.g. Microsoft spokesman John Smith is a popular man.
Æ spokesman: IsA(John Smith) John Smith: IsA(man)
"The concept-based seeds used in the experiments are: 1. PER: he, she, his, her, him, man, woman 2."
"LOC: city, province, town, village 3."
"ORG: company, firm, organization, bank, airline, army, committee, government, school, university 4."
"PRO: car, truck, vehicle, product, plane, aircraft, computer, software, operating system, data-base, book, platform, network"
Note that the last target tag PRO (PRODUCT) is beyond the MUC NE standards: we added this NE type for the purpose of testing the system’s capability in supporting user-defined NE types.
"From the parsed corpus in the repository, all instances of the concept-based seeds associated with one or more of the five dependency relations are retrieved: 821,267 instances in total in our experiment."
Each seed instance was assigned a concept tag corresponding to NE.
"For example, each instance of he is marked as PER."
"The marked instances plus their associated parsing relationships form an annotated NE corpus, as shown below: he/PER: Has_Predicate(say) she/PER: Has_Predicate(get) company/ORG: Object_Of(compel) city/LOC: Possess(mayor) car/PRO: Object_Of(manufacture)"
This training corpus supports the Decision List Learning which learns homogeneous rules (Segal &amp;[REF_CITE]).
The accuracy of each rule was evaluated using Laplace smoothing: positive +1 accuracy = positive + negative + NE category No.
It is noteworthy that the PER tag dominates the corpus due to the fact that the pronouns he and she occur much more often than the seeded common nouns.
So the proportion of NE types in the instances of concept-based seeds is not the same as the proportion of NE types in the proper name instances.
"For example, considering a running text containing one instance of John Smith and one instance of a city name Rochester, it is more likely that John Smith will be referred to by he/him than Rochester by (the) city."
Learning based on such a corpus is biased towards PER as the answer.
"To correct this bias, we employ the following modification scheme for instance count."
"Suppose there are a total of N PER PER instances, N LOC LOC instances, N ORG ORG instances, N PRO PRO instances, then in the process of rule accuracy evaluation, the involved instance count for any NE type will be adjusted by the coefficient min(NPER,NLOC,NORG,NPRO) ."
"For example, if"
"NNE the number of the training instances of PER is ten times that of PRO, then when evaluating a rule accuracy, any positive/negative count associated with PER will be discounted by 0.1 to correct the bias."
"A total of 1,290 parsing-based NE rules are learned, with accuracy higher than 0.9."
The following are sample rules of the learned decision list:
Æ PER Possess(daughter)
Æ PER Possess(bravery)
Æ PER Possess(father)
Æ PER Has_Predicate(divorce)
Æ PER Has_Predicate(remarry) Æ PER Possess(brother)
Æ PER Possess(son)
Æ PER Possess(mother)
Æ PER Object_Of(deport)
Æ PER Possess(sister) Æ PER Possess(colleague )
Æ PER Possess(career)
Æ PER Possess(forehead)
Æ PER Has_Predicate(smile) Æ PER Possess(respiratory system)
"Æ PER {Has_Predicate(threaten), Has_Predicate(kill)} ÆPER ………… Possess(concert hall)"
Æ LOC Has_AMod(coastal)
Æ LOC Has_AMod(northern)
Æ LOC Has_AMod(eastern)
Æ LOC Has_AMod(northeastern)
Æ LOC Possess(undersecretary)
Æ LOC Possess(mayor)
Æ LOC Has_AMod(southern) Æ LOC Has_AMod(northwestern)
Æ LOC Has_AMod(populous)
Æ LOC Has_AMod(rogue)
Æ LOC Has_AMod(southwestern)
Æ LOC Possess(medical examiner)
Æ LOC Has_AMod(edgy)
Æ LOC ………… Has_AMod(broad-base) Æ ORG Has_AMod(advisory) Æ ORG Has_AMod(non-profit) Æ ORG Possess(ceo)
Æ ORG Possess(operate loss) Æ ORG Has_AMod(multinational) Æ ORG Has_AMod(non-governmental) Æ ORG Possess(filings)
Æ ORG Has_AMod(for-profit) Æ ORG Has_AMod(not-for-profit) Æ ORG Has_AMod(nongovernmental)
Æ ORG Object_Of(undervalue)
Æ ORG ………… Has_AMod(handheld) Æ PRO Has_AMod(unman)
Æ PRO Has_AMod(well-sell)
Æ PRO Has_AMod(value-add) Æ PRO Object_Of(refuel)
Æ PRO Has_AMod(fuel-efficient)
Æ PRO Object_Of(vend)
Æ PRO Has_Predicate(accelerate) Æ PRO Has_Predicate(collide) Æ PRO Object_Of(crash)
Has_AMod(scalable) Æ PRO Possess(patch)
Æ PRO Object_Of(commercialize)ÆPRO Has_AMod(custom-design) Æ PRO Possess(rollout) Æ PRO Object_Of(redesign)
Æ PRO …………
"Due to the unique equivalence nature of the IsA relation, the above bootstrapping procedure can hardly learn IsA-based rules."
"Therefore, we add the following IsA-based rules to the top of the decision list: IsA(seed)Æ tag of the seed, for example:"
IsA(man) Æ PER IsA(city)
Æ LOC IsA(company)
Æ ORG IsA(software)
"In this step, we use the parsing-based first learner to tag a raw corpus in order to train the second NE learner."
One issue with the parsing-based NE rules is modest recall.
"For incoming documents, approximately 35%-40% of the proper names are associated with at least one of the five parsing relations."
"Among these proper names associated with parsing relations, only ~5% are recognized by the parsing-based NE rules."
"So we adopted the strategy of applying the parsing-based rules to a large corpus (88 million words), and let the quantity compensate for the sparseness of tagged instances."
A repository level consolidation scheme is also used to improve the recall.
The NE classification procedure is as follows.
"From the repository, all the named entity candidates associated with at least one of the five parsing relationships are retrieved."
An NE candidate is defined as any chunk in the parsed corpus that is marked with a proper name Part-Of-Speech (POS) tag (i.e. NNP or NNPS).
"A total of 1,607,709 NE candidates were retrieved in our experiment."
A small sample of the retrieved NE candidates with the associated parsing relationships are shown below:
Deep South : Possess(project)
Ramada : Possess(president)
Argentina : Possess(first lady) …………
"After applying the decision list to the above the[REF_CITE]104[REF_CITE]426[REF_CITE]908 ORG names and 6,280 PRO names were extracted."
It is a common practice in the bootstrapping research to make use of heuristics that suggest conditions under which instances should share the same answer.
"For example, the one sense per discourse principle is often used for word sense disambiguati[REF_CITE]."
"In this research, we used the heuristic one tag per domain for multi-word NE in addition to the one sense per discourse principle."
These heuristics were found to be very helpful in improving the performance of the bootstrapping algorithm for the purpose of both increasing positive instances (i.e. tag propagation) and decreasing the spurious instances (i.e. tag elimination).
The following are two examples to show how the tag propagation and elimination scheme works.
"Tyco[REF_CITE]times in the corpus, and 11 instances are recognized as ORG, only one instance is recognized as PER."
"Based on the heuristic one tag per domain for multi-word NE, the minority tag of PER is removed, and all the 67 instances of Tyco Toys are tagged as ORG."
"Three instances of Postal Service are recognized as ORG, and two instances are recognized as PER."
"These tags are regarded as noise, hence are removed by the tag elimination scheme."
The tag propagation/elimination scheme is adopted[REF_CITE].
"After this step, a total of 386,614 proper names were recognized, including 134,722[REF_CITE]488[REF_CITE]231 ORG names and 19,173 PRO names."
The overall precision was ~90%.
The benchmark details will be shown in Section 6.
The extracted proper name instances then led to the construction of a fairly large training corpus sufficient for training the second NE learner.
"Unlike manually annotated running text corpus, this corpus consists of only sample string sequences containing the automatically tagged NE instances and their left and right neighboring words within the same sentence."
The two neighboring words are always regarded as common words while constructing the corpus.
This is based on the observation that the proper names usually do not occur continuously without any punctuation in between.
"A small sample of the automatically constructed corpus is shown below: in &lt;LOC&gt; Argentina &lt;/LOC&gt; . &lt;LOC&gt; Argentina &lt;/LOC&gt; &apos;s and &lt;PER&gt; Troy Glaus &lt;/PER&gt; walk call &lt;ORG&gt; Prudential Associates &lt;/ORG&gt; . , &lt;PRO&gt; Photoshop &lt;/PRO&gt; has not &lt;PER&gt; David Bonderman &lt;/PER&gt; , …………"
"This corpus is used for training the second NE learner based on evidence from string sequences, to be described in Section 5 below."
String sequence-based HMM learning is set as our final goal for NE bootstrapping because of the demonstrated high performance of this type of NE taggers.
"In this research, a bi-gram HMM is trained based on the sample strings in the annotated corpus constructed in section 4."
"During the training, each sample string sequence is regarded as an independent sentence."
The training process is similar[REF_CITE].
The HMM is defined as follows:
"Given a word sequence Wsequence= w f w n f n (where 0 0 f j denotes a single token feature which will be defined below), the goal for the NE tagging task is to find the optimal NE tag sequence T sequence = t 0 t 1 t 2 t n , which maximizes the conditional probability Pr(T sequence | W sequence)[REF_CITE]."
"By Bayesian equality, this is equivalent to maximizing the joint probability Pr(W sequence,T sequence) ."
This joint probability can be computed by bi-gram HMM as follows:
"Pr(W sequence, T sequence) = ∏ Pr( w ,f ,t | w ,f ,t ) i i i i-1 i-1 i−1 i"
"The back-off model is as follows,"
"Pr( w i ,f i ,t i | w i-1 ,f i-1 ,t i−1 ) = λ 1 P 0 ( w i ,f i ,t i | w i-1 ,f i-1 ,t i−1 ) + (1- λ 1 )Pr( w i ,f i | t i , t i−1 )"
"Pr(t i | w i−1 , t i−1 )"
"Pr( w i ,f i | t i , t i − 1 ) = λ 2 P 0 ( w i ,f i | t i , t i − 1 ) + (1- λ 2 )Pr( w i ,f i | t i )"
"Pr(t i | w i-1 , t i − 1 ) = λ 3 P 0 (t i | w i-1 , t i − 1 ) + (1- λ 3 )Pr(t i | w i-1 )"
"Pr( w i ,f i | t i ) = λ 4 P 0 ( w i ,f i | t i ) + (1- λ 4 ) Pr(w i | t i )P 0 (f i | t i )"
Pr(t i | w i-1 ) = λ 5 P 0 (t i | w i-1 ) + (1- λ 5 )P 0 (t i )
"Pr(w i | t i ) = λ 6 P 0 (w i | t i ) + (1- λ 6 ) 1 V where V denotes the size of the vocabulary, the back-off coefficients λ’s are determined using the Witten-Bell smoothing algorithm."
"The quantities P 0 ( w i ,f i ,t i | w i − 1 , f i− 1 ,t i − 1 ) ,"
"P 0 ( w i ,f i | t i , t i − 1 ) , P 0 (t i | w i-1 , t i − 1 ) , P 0 ( w i ,f i | t i ) , P 0 (f i | t i ) , P 0 (t i | w i-1 ) , P 0 (t i ) , and P 0 (w i | t i ) are computed by the maximum likelihood estimation."
We use the following single token feature set for HMM training.
"The definitions of these features are the same as[REF_CITE]. twoDigitNum, fourDigitNum, containsDigitAndAlpha, containsDigitAndDash, containsDigitAndSlash, containsDigitAndComma, containsDigitAndPeriod, otherNum, allCaps, capPeriod, initCap, lowerCase, other."
"Two types of benchmarks were measured: (i) the quality of the automatically constructed NE corpus, and (ii) the performance of the HMM NE tagger."
The HMM NE tagger is considered to be the resulting system for application.
"The benchmarking shows that this system approaches the performance of supervised NE tagger for two of the three proper name NE types in MUC, namely, PER NE and LOC NE."
"We used the same blind testing corpus of 300,000 words containing 20,000 PER, LOC and ORG instances that were truthed in-house originally for benchmarking the existing supervised NE tagger (Srihari, Niu &amp;[REF_CITE])."
This has the benefit of precisely measuring performance degradation from the supervised learning to unsupervised learning.
The performance of our supervised NE tagger using the MUC scorer is shown in Table 1.
"To benchmark the quality of the automatically constructed corpus (Table 2), the testing corpus is first processed by our parser and then saved into the repository."
"The repository level NE classification scheme, as discussed in section 4, is applied."
"From the recognized NE instances, the instances occurring in the testing corpus are compared with the answer key."
"To benchmark the performance of the HMM tagger, the testing corpus is parsed."
The noun chunks with proper name POS tags (NNP and NNPS) are extracted as NE candidates.
The preceding word and the succeeding word of the NE candidates are also extracted.
Then we apply the HMM to the NE candidates with their neighboring context.
The NE classification results are shown in Table 3.
"Compared with our existing supervised NE tagger, the degradation using the presented bootstrapping method for PER NE, LOC NE, and ORG NE are 5%, 6%, and 34% respectively."
"The performance for PER and LOC are above 80%, approaching the performance of supervised learning."
The reason for the low recall of ORG (~50%) is not difficult to understand.
"For PERSON and LOCATION, a few concept-based seeds seem to be sufficient in covering their sub-types (e.g. the sub-types COUNTRY, CITY, etc for LOCATION)."
"But there are hundreds of sub-types of ORG that cannot be covered by less than a dozen concept-based seeds, which we used."
"As a result, the recall of ORG is significantly affected."
"Due to the same fact that ORG contains many more sub-types, the results are also noisier, leading to lower precision than that of the other two NE types."
"Some threshold can be introduced, e.g. perplexity per word, to remove spurious ORG tags in improving the precision."
"As for the recall issue, fortunately, in a real-life application, the organization type that a user is interested in usually is in a fairly narrow spectrum."
We believe that the performance will be better if only company names or military organization names are targeted.
"In addition to the key NE types in MUC, our system is able to recognize another NE type, namely, PRODUCT (PRO) NE."
"We instructed our truthing team to add this NE type into the testing corpus which contains ~2,000 PRO instances."
Table 4 shows the performance of the HMM on the PRO tag.
"Similar to the case of ORG NEs, the number of concept-based seeds is found to be insufficient to cover the variations of PRO subtypes."
So the performance is not as good as PER and LOC NEs.
"Nevertheless, the benchmark shows the system works fairly effectively in extracting the user-specified NEs."
It is noteworthy that domain knowledge such as knowing the major sub-types of the user-specified NE type is valuable in assisting the selection of appropriate concept-based seeds for performance enhancement.
The performance of our HMM tagger is comparable with the reported performance in (Collins &amp;[REF_CITE]).
"But our benchmarking is more extensive as we used a much larger data set (20,000 NE instances in the testing corpus) than theirs (1,000 NE instances)."
A novel bootstrapping approach to NE classification is presented.
This approach does not require iterative learning which may suffer from error propagation.
"With minimal human supervision in providing a handful of concept-based seeds, the resulting NE tagger approaches supervised NE performance in NE types for PERSON and LOCATION."
The system also demonstrates effective support for user-defined NE classification.
This paper presents a method for unsu-pervised discovery of semantic patterns.
"Semantic patterns are useful for a vari-ety of text understanding tasks, in par-ticular for locating events in text for in-formation extraction."
The method builds upon previously described approaches to iterative unsupervised pattern acquisition.
"One common characteristic of prior ap-proaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision."
Our method differs from the previous pat-tern acquisition algorithms in that it intro-duces competition among several scenar-ios simultaneously.
"This provides natu-ral stopping criteria for the unsupervised learners, while maintaining good preci-sion levels at termination."
"We discuss the results of experiments with several scenar-ios, and examine different aspects of the new procedure."
The work described in this paper is motivated by research into automatic pattern acquisition.
"Pat-tern acquisition is considered important for a variety of “text understanding” tasks, though our particular reference will be to Information Extraction (IE)."
"In IE, the objective is to search through text for enti-ties and events of a particular kind—corresponding to the user’s interest."
Many current systems achieve this by pattern matching.
"The problem of recall, or coverage, in IE can then be restated to a large ex-tent as a problem of acquiring a comprehensive set of good patterns which are relevant to the scenario of interest, i.e., which describe events occurring in this scenario."
"Among the approaches to pattern acquisition recently proposed, unsupervised methods [Footnote_1] have gained some popularity, due to the substantial re-duction in amount of manual labor they require."
"1 As described in, e.g.,[REF_CITE]."
We build upon these approaches for learning IE patterns.
The focus of this paper is on the problem of con-vergence in unsupervised methods.
"As with a variety of related iterative, unsupervised methods, the out-put of the system is a stream of patterns, in which the quality is high initially, but then gradually de-grades."
"This degradation is inherent in the trade-off, or tension, in the scoring metrics: between trying to achieve higher recall vs. higher precision."
"Thus, when the learning algorithm is applied against a ref-erence corpus, the result is a ranked list of patterns, and going down the list produces a curve which trades off precision for recall."
"Simply put, the unsupervised algorithm does not know when to stop learning."
"In the absence of a good stopping criterion, the resulting list of patterns must be manually reviewed by a human; otherwise one can set ad-hoc thresholds, e.g., on the number of allowed iterations, as[REF_CITE], or else to resort to supervised training to determine such thresholds—which is unsatisfactory when our goal from the outset is to try to limit supervision."
"Thus, the lack of natural stopping criteria renders these algorithms less unsupervised than one would hope."
"More importantly, this lack makes the al-gorithms difficult to use in settings where training must be completely automatic, such as in a general-purpose information extraction system, where the topic may not be known in advance."
"At the same time, certain unsupervised learning algorithms in other domains exhibit inherently natu-ral stopping criteria."
One example is the algorithm for word sense disambiguation[REF_CITE].
Of particular relevance to our method are the algo-rithms for semantic classification of names or NPs described[REF_CITE].
"Inspired in part by these algorithms, we introduce the counter-training technique for unsupervised pat-tern acquisition."
The main idea behind counter-training is that several identical simple learners run simultaneously to compete with one another in dif-ferent domains.
"This yields an improvement in pre-cision, and most crucially, it provides a natural indi-cation to the learner when to stop learning—namely, once it attempts to wander into territory already claimed by other learners."
We review the main features of the underlying un-supervised pattern learner and related work in Sec-tion 2.
"In Section 3 we describe the algorithm; 3.2 gives the details of the basic learner, and 3.3 in-troduces the counter-training framework which is super-imposed on it."
"We present the results with and without counter-training on several domains, Sec-tion 4, followed by discussion in Section 5."
We outline those aspects of the prior work that are relevant to the algorithm developed in our presenta-tion.
"We are given an IE scenario , e.g., “Man-agement Succession” (as in MUC-6)."
"We have a raw general news corpus for training, i.e., an un-classified and un-tagged set of documents  ."
"The problem is to find a good set of patterns in , which cover events relevant to ."
"We presuppose the existence of two general- purpose, lower-level language tools—a name recog-nizer and a parser."
These tools are used to extract all potential patterns from the corpus.
The user provides a small number of seed pat-terns for .
The algorithm uses the corpus to itera-tively bootstrap a larger set of good patterns for .
"The algorithm/learner achieves this bootstrap-ping by utilizing the duality between the space of documents and the space of patterns: good extrac-tion patterns select documents relevant to the chosen scenario; conversely, relevant documents typically contain more than one good pattern."
This duality drives the bootstrapping process.
The primary aim of the learning is to train a strong recognizer for ; is embodied in the set of good patterns.
"However, as a result of training , the procedure also produces the set  of doc- uments that it deems relevant to —the documents selected by ."
"Evaluation: to evaluate the quality of discov-ered patterns,[REF_CITE]describes a direct eval-uation strategy, where precision of the patterns re-sulting from a given run is established by manual re-view.[REF_CITE]uses an automatic but indirect evaluation  of the recognizerfrom the training: they retrievecorpusa test sub-set and manually judge the relevance of every document in  ; one can then obtain standard IR-style recall and precision scores for relative to ."
"In presenting our results, we will discuss both kinds of evaluation."
The recall/precision curves produced by the indi-rect evaluation generally reach some level of recall at which precision begins to drop.
"This happens be-cause at some point in the learning process the al-gorithm picks up patterns that are common in , but are not sufficiently specific to alone."
"These pat-terns then pick up irrelevant documents, and preci-sion drops."
"Our goal is to prevent this kind of degradation, by helping the learner stop when precision is still high, while achieving maximal recall."
"We briefly mention some of the unsupervised meth-ods for acquiring knowledge for NL understanding, in particular in the context of IE."
"A typical archi-tecture for an IE system includes knowledge bases (KBs), which must be customized when the system is ported to new domains."
"The KBs cover different levels, viz. a lexicon, a semantic conceptual hierar-chy, a set of patterns, a set of inference rules, a set of logical representations for objects in the domain."
"Each KB can be expected to be domain-specific, to a greater or lesser degree."
"Among the research that deals with automatic ac-quisition of knowledge from text, the following are particularly relevant to us.[REF_CITE]proposed a method for learning concepts be-longing to a given semantic class.[REF_CITE]present different combinations of learners of patterns and concept classes specifically for IE."
"In[REF_CITE]the system AutoSlog-TS learns patterns for filling an individual slot in an event tem-plate, while simultaneously acquiring a set of lexical elements/concepts eligible to fill the slot."
"AutoSlog-TS, does not require a pre-annotated corpus, but does require one that has been split into subsets that are relevant vs. non-relevant subsets to the scenario.[REF_CITE]attempts to find extrac-tion patterns, without a pre-classified corpus, start-ing from a set of seed patterns."
This is the ba-sic unsupervised learner on which our approach is founded; it is described in the next section.
"We first present the basic algorithm for pattern ac-quisition, similar to that presented[REF_CITE]."
Section 3.3 places the algorithm in the framework of counter-training.
"Prior to learning, the training corpus undergoes sev-eral steps of pre-processing."
"The learning algorithm depends on the fundamental redundancy in natural language, and the pre-processing the text is designed to reduce the sparseness of data, by reducing the ef-fects of phenomena which mask redundancy."
"Name Factorization: We use a name classifier to tag all proper names in the corpus as belonging to one of several categories—person, location, and or-ganization, or as an unidentified name."
"Each name is replaced with its category label, a single token."
"The name classifier also factors out other out-of- vocabulary (OOV) classes of items: dates, times, numeric and monetary expressions."
"Name classifi-cation is a well-studied subject, e.g.,[REF_CITE]."
"The name recognizer we use is based on lists of common name markers—such as personal titles (Dr., Ms.) and corporate designators (Ltd., GmbH)—and hand-crafted rules."
"Parsing: After name classification, we apply a gen-eral English parser, from Conexor Oy,[REF_CITE]."
"The parser recognizes the name tags generated in the preceding step, and treats them as atomic."
The parser’s output is a set of syn-tactic dependency trees for each document.
"Syntactic Normalization: To reduce variation in the corpus further, we apply a tree-transforming pro-gram to the parse trees."
"For every (non-auxiliary) verb heading its own clause, the transformer pro-duces a corresponding active tree, where possi-ble."
"This converts for passive, relative, subordinate clauses, etc. into active clauses."
"Pattern Generalization: A “primary” tuple is ex-tracted from each clause: the verb and its main ar-guments, subject and object."
"The tuple consists of three literals [s,v,o]; if the direct object is missing the tuple contains in its place the subject complement; if the object is a sub-ordinate clause, the tuple contains in its place the head verb of that clause."
"Each primary tuple produces three generalized tu-ples, with one of the literals replaced by a wildcard."
A pattern is simply a primary or generalized tuple.
The pre-processed corpus is thus a many-many map-ping between the patterns and the document set.
"We now outline the main steps of the algorithm, fol-lowed by the formulas used in these steps. 1."
"Given: a seed set of patterns, expressed as pri-mary or generalized tuples. 2."
Partition: divide the corpus into relevant vs. non-relevant documents.
"A document is relevant—receives a weight of 1—if some seed matches , and non-relevant otherwise, receiving weight 0."
"After the first iteration, documents are assigned relevance weights between and ."
"So at each iteration, there is a distribution of relevance weights on the corpus, rather than a binary partition. 3."
Pattern Ranking: Every pattern appearing in a relevant document is a candidate pattern.
"Assign a score to each candidate; the score depends on how accurately the candidate predicts the relevance of a document, with respect to the current weight distri-bution, and on how much support it has—the total wight of the relevant documents it matches in the corpus (in Equation 2)."
Rank the candidates accord-ing to their score.
"On the -th iteration, we select the pattern most correlated with the documents that have high relevance."
"Add  to the growing set of seeds  , and record its accuracy. 4."
"For each document  , re- covered by any of the accepted patterns in compute the relevance of to the target scenario , !&quot;$# ."
Relevance of is based on the cumulative accuracy of patterns from %&amp; which match . 5.
Repeat: Back to Partition in step 2.
The ex-panded pattern set induces a new relevance distribu-tion on the corpus.
Repeat the procedure as long as learning is possible.
"The formula used for scoring candidate patterns in step 3 is similar to that  4 56.%48# ;=: &lt;  ./#[REF_CITE]: (&apos;*),+ ./10 # (1) 5 0 5 .%&gt;# 2 .% # where are documents where matched, and the support is computed as the sum of their relevance:  .%10# @BADCFE?"
GIH  &quot;$# (2)
"Document relevance is computed as[REF_CITE]&quot ;# GDA=OQP ERSH@ T  + &apos; .%# W (3) where X6&quot;# is the set of accepted patterns that match ; this is a rough estimate of the likelihood of relevance of , based on the pattern accuracy mea-sure."
"Pattern accuracy, or precision, is given by the average relevance  4 5Z&gt;.4%# 0 4 564 BADCFE@ ?"
GIH  &quot;$# of the documents matched by :
U + &apos; .%#Y0 (4)
Equation 1 can therefore be written simply as: &gt;&apos;B[) + \.%#10]U + &apos; ./# ;:=&lt; &gt;2 3.%# (5)
The two terms in Equation 5 capture the trade-off between precision and recall.
"As mentioned in Sec-tion 2.1, the learner running in isolation will even-tually acquire patterns that are too general for the scenario, which will cause it to assign positive rel-evance to non-relevant documents, and learn more irrelevant patterns."
From that point onward pattern accuracy will decline.
"To deal with this problem, we arrange ^ different learners, for ^ different scenarios _ ;bc^ to train simultaneously on each iteration."
"Each learner stores its own bag of good patterns, and each as-signs its own relevance,  [d &quot;# , to the documents."
Documents that are “ambiguous” will have high rel-evance in more than one scenario.
"Now, given multiple learners, we can refine the  measure of pattern precision in Eq. 4 for scenario , to take into account the negative evidence—i.e., how much weight the documents matched by the pattern received in other scenarios: + &apos ; .%#[REF_CITE]Z4 e@ ADCFE?"
GIH T   &quot;&gt;# M ?fDgh   &quot;# W U
If U + &apos; .%# the candidate is not considered for(6) acceptance.
Equations 6 and 5 imply that the learner will disfavor a pattern if it has too much opposition from other scenarios.
The algorithm proceeds as long as two or more scenarios are still learning patterns.
"When the num-ber of surviving scenarios drops to one, learning terminates, since, running unopposed, the surviving scenario is may start learning non-relevant patterns which will degrade its precision."
"Scenarios may be represented with different den-sity within the corpus, and may be learned at dif-ferent rates."
"To account for this, we introduce a pa-rameter, n : rather than acquiring a single pattern on each iteration, each learner may acquire up to n patterns (3 in this paper), as long as their scores are near (within 5% of) the top-scoring pattern."
We tested the algorithm on documents from the Wall Street Journal (WSJ).
"The training corpus consisted of 15,000 articles from 3 months between 1992 and 1994."
This included the MUC-6 training corpus of 100 tagged WSJ articles (from 1993).
We used the scenarios shown in Table 1 to com-pete with each other in different combinations.
"The seed patterns for the scenarios, and the number of documents initially picked up by the seeds are shown in the table. [Footnote_2]"
"2 Capitalized entries refer to Named Entity classes, and ital-about 3 words each; e.g., appoint appoint, name, promote q . icized entries refer to small classes of synonyms, containing"
"The seeds were kept small, and they yielded high precision; it is evident that these scenarios are represented to a varying degree within the corpus."
"We also introduced an additional “negative” sce-nario (the row labeled “Don’t care”), seeded with patterns for earnings reports and interest rate fluctu-ations."
The last column shows the number of iterations before learning stopped.
A sample of the discovered patterns [Footnote_3] appears in Table 2.
3 The algorithm learns hundreds of patterns; we present a sample to give the reader a sense of their shape and content.
"For an indirect evaluation of the quality of the learned patterns, we employ the text-filtering eval-uation strategy, as[REF_CITE]."
"As a by-product of pattern acquisition, the algorithm ac-quires a set of relevant documents (more precisely, a distribution of document relevance weights)."
"Rather than inspecting patterns  on the -th iteration by hand, we can judge the quality of this pattern set based on the quality of the documents that the pat-  match."
"Viewed as a categorization task terns on a set of documents, this is similar to the text- filtering task in the MUC competitions."
We use the  as a quantitative text-filtering power of the set measure of the goodness of the patterns.
To conduct the text-filtering evaluation we need a binary relevance judgement for each document.
This is obtained as follows.
"We introduce a cutoff threshold  s on document relevance; if the system has internal confidence of more than r  that a doc-ument is relevant, it labels as relevant externally for the purpose of scoring recall and precision."
Oth-erwise it labels as non-relevant. [Footnote_4]
"4 The relevance cut-off parameter, tjuwv&quot;x was set to 0.3 for mono-trained experiments, and to 0.2 for counter-training. These numbers were obtained from empirical trials, which sug-ative evidence. Internal relevance measures, &gt;y z|{~};I , are main-gest that a lower confidence is acceptable in the presence of neg-tained by the algorithm, and the external, binary measures are used only for evaluation of performance."
"The results of the pattern learner for the “Man-agement Succession” scenario, with and without counter-training, are shown in Figure 1."
The test sub-corpus consists of the 100 MUC-6 documents.
The initial seed yields about 15% recall at 86% precision.
The curve labeled Mono shows the perfor-mance of the baseline algorithm up to 150 iterations.
"It stops learning good patterns after 60 iterations, at 73% recall, from which point precision drops."
"The reason the recall appears to continue improv-ing is that, after this point, the learner begins to ac-quire patterns describing secondary events, deriva-tive of or commonly co-occurring with the focal topic."
"Examples of such events are fluctuations in stock prices, revenue estimates, and other common business news elements."
The performance of the Management Succes-sion learner counter-trained against other learners is traced by the curve labeled Counter.
"It is impor-tant to recall that the counter-trained algorithm ter-minates at the final point on the curve, whereas the mono-trained case it does not."
We checked the quality of the discovered patterns by hand.
Termination occurs at 142 iterations.
"We observed that after iteration 103 only 10% of the pat-terns are “good”, the rest are secondary."
"However, in the first 103 iterations, over 90% of the patterns are good Management Succession patterns."
In the same experiment the behaviour of the learner of the “Legal Action” scenario is shown in Figure 2.
The test corpus for this learner consists of 250 documents: the 100 MUC-6 training docu-ments and 150 WSJ documents which we retrieved using a set of keywords and categorized manually.
"The curves labeled Mono, Counter and Baseline are as in the preceding figure."
"We observe that the counter-training termination point is near the mono-trained curve, and has a good recall-precision trade-off."
"However, the improve-ment from counter-training is less pronounced here than for the Succession scenario."
"This is due to a subtle interplay between the combination of scenar-ios, their distribution in the corpus, and the choice of seeds."
We return to this in the next section.
"Although the results we presented here are encour-aging, there remains much research, experimenta-tion and theoretical work to be done."
"In this paper we have presented counter-training, a method for strengthening unsupervised strategies for knowledge acquisition."
"It is a simple way to com-bine unsupervised learners for a kind of “mutual supervision”, where they prevent each other from degradation of accuracy."
Our experiments in acquisition of semantic pat-terns show that counter-training is an effective way to combat the otherwise unlimited expansion in un-supervised search.
Counter-training is applicable in settings where a set of data points has to be catego-rized as belonging to one or more target categories.
The main features of counter-training are:
Training several simple learners in parallel;
Competition among learners;
Convergence of the overall learning process;
"Termination with good recall-precision trade-off, compared to the single-trained learner."
This paper is concerned with learning cat-egorial grammars in Gold’s model.
"In contrast to k-valued classical categorial grammars, k-valued Lambek grammars are not learnable from strings."
"This re-sult was shown for several variants but the question was left open for the weak-est one, the non-associative variant NL."
Categorial grammars[REF_CITE]and Lam-bek grammars[REF_CITE]have been studied in the field of natural language process-ing.
They are well adapted to learning perspectives since they are completely lexicalized and an actual way of research is to determine the sub-classes of such grammars that remain learnable in the sense of Gold[REF_CITE].
We recall that learning here consists to define an algorithm on a finite set of sentences that converge to obtain a grammar in the class that generates the examples.
"Let G be a class of grammars, that we wish to learn from positive examples."
"Formally, let L(G) denote the language associated with grammar G, and let V be a given alphabet, a learning algorith-m is a function φ from finite sets of words in V ∗ to G, such that for all G ∈ G with L(G) =&lt; e i &gt; i∈N there exists a grammar G 0 ∈ G and there exists n 0 ∈ N such that: ∀n &gt; n 0 φ({e 1 , . . . , e n }) = G 0 ∈ G with L(G 0 ) = L(G)."
"After pessimistic unlearnability results[REF_CITE], learnability of non trivial classes has been proved[REF_CITE]and[REF_CITE]."
Recent works[REF_CITE]and[REF_CITE]following[REF_CITE]have answered the problem for different sub-classes of classical categorial grammars (we recall that the w-hole class of classical categorial grammars is equiv-alent to context free grammars; the same holds for the class of Lambek grammars[REF_CITE]that is thus not learnable in Gold’s model).
"The extension of such results for Lambek gram-mars is an interesting challenge that is addressed by works on logic types[REF_CITE](these grammars enjoy a direct link with Mon-tague semantics), learning from structures in (Re-tor and Bonato, september 2001), complexity results[REF_CITE]or unlearnability results[REF_CITE]; this result was shown for several variants but the question was left open for the basic variant, the non-associative variant NL."
"In this paper, we consider the following question: is the non-associative variant NL of k-valued Lam-bek grammars learnable from strings; we answer by constructing a limit point for this class."
Our con-struction is in some sense more complex than those for the other systems since they do not directly trans-late as limit point in the more restricted system NL.
The paper is organized as follows.
Section 2 gives some background knowledge on three main aspects: Lambek categorial grammars ; learning in Gold’s model ; Lambek pregroup grammars that we use later as models in some proofs.
"Section 3 then presents our main result on NL (NL denotes non-associative Lambek grammars not allowing empty sequence): after a construction overview, we dis-cuss some corollaries and then provide the details of proof."
Section 4 concludes.
The reader not familiar with Lambek Calculus and its non-associative version will find nice presenta-tion in the first ones written by Lambek[REF_CITE]or more recently in ([REF_CITE]; de[REF_CITE]; de[REF_CITE]).
"The types T p, or formulas, are generated from a set of primitive types P r, or atom-ic formulas by three binary connectives “ / ” (over), “ \ ” (under) and “•” (product): Tp ::= P r | T p \"
"T p | T p / T p | T p • T p. As a logical sys-tem, we use a Gentzen-style sequent presentation."
A sequent Γ ` A is composed of a sequence of for-mulas Γ which is the antecedent configuration and a succedent formula A.
Let Σ be a fixed alphabet.
A categorial grammar over Σ is a finite relation G between Σ and Tp.
"If &lt; c, A &gt;∈ G, we say that G assigns A to c, and we write G : c 7→ A."
"The relation ` L is the smallest relation ` between T p + and T p, such that for all Γ, Γ 0 ∈ T p + , ∆, ∆ 0 ∈ T p ∗ and for all A, B, C ∈ T p : ∆, A, ∆ 0 ` C Γ ` A (Cut) A ` A (Id) ∆, Γ, ∆ 0 ` C"
"Γ ` A ∆, B, ∆ 0 ` C Γ, A ` B /L /R ∆, B / A, Γ, ∆ 0 ` C Γ ` B / A"
"Γ ` A ∆, B, ∆ 0 ` C A, Γ ` B \L \R ∆, Γ, A \ B, ∆ 0 ` C Γ ` A \ B ∆, A, B, ∆ 0 ` C Γ ` A Γ 0 ` B •L •R ∆, A • B, ∆ 0 ` C Γ, Γ 0 ` A • B"
We write L ∅ for the Lambek calculus with empty antecedents (left part of the sequent).
"In the Gentzen presentation, the derivability rela-tion of NL holds between a term in S and a formula in T p, where the term language is S ::= T p|(S, S)."
Terms in S are also called G-terms.
"A sequent is a pair (Γ,A) ∈ S × Tp."
The notation Γ[∆] repre-sents a G-term with a distinguished occurrence of ∆ (with the same position in premise and conclusion of a rule).
"The relation ` NL is the smallest relation ` between S and T p, such that for all Γ, ∆ ∈ S and for all A, B, C ∈ T p :"
Γ[A] ` C ∆ ` A (Cut) A ` A (Id)
Γ[∆] ` C
"Γ ` A ∆[B] ` C (Γ, A) ` B /L /R ∆[(B / A, Γ)] ` C Γ ` B / A"
"Γ ` A ∆[B] ` C (A, Γ) ` B \L \R ∆[(Γ, A \ B)] ` C Γ ` A \ B ∆[(A, B)] `"
"C Γ ` A ∆ ` B •L •R ∆[A • B] ` C (Γ, ∆) ` (A • B)"
We write NL ∅ for the non-associative Lambek calculus with empty antecedents (left part of the se-quent).
We recall that cut rule can be e-liminated in ` L and ` NL : every derivable sequent has a cut-free derivation.
"The order ord(A) of a type A of L or NL is defined by: ord(A) = 0 if A is a primitive type ord(C 1 / C 2 ) = max(ord(C 1 ), ord(C 2 ) + 1) ord(C 1 \ C 2 ) = max(ord(C 1 ) + 1, ord(C 2 )) ord(C 1 • C 2 ) = max(ord(C 1 ), ord(C 2 ))"
"Let G be a categorial grammar over Σ. G gen-erates a string c 1 ...c n ∈ Σ + iff there are types A 1 , . . . , A n ∈ T p such that:"
G : c i 7→
"A i (1 ≤ i ≤ n) and A 1 , . . . , A n ` L S."
"The language of G, written L L (G) is the set of strings generated by G. We define similarly L L ∅ (G), L NL (G) and L NL ∅ (G) replacing ` L by ` L ∅ , ` NL and ` NL ∅ in the sequent where the types are parenthesized in some way."
"In some sections, we may write simply ` instead of ` L , ` L ∅ , ` NL or ` NL ∅ ."
We may simply write L(G) accordingly.
Categorial grammars that assign at most k types to each symbol in the alphabet are called k-valued grammars; [Footnote_1]-valued grammars are also called rigid grammars.
"1 This implies that the class has infinite elasticity. A class CL of languages has infinite elasticity iff ∃ &lt; e i &gt; i∈N sentences ∃ &lt; L i &gt; i∈N languages in CL ∀i ∈ N : e i 6∈ L i and {e 1 , . . . , e n } ⊆ L n+1 ."
"Example 1 Let Σ 1 = {John, Mary, likes} and let P r = {S, N} for sentences and nouns respectively."
"Let G 1 = {John 7→ N, Mary 7→ N, likes 7→ N \ (S / N)}."
"We get (John likes Mary) ∈ L NL (G 1 ) since ((N, N \ (S / N)), N) ` NL S. G 1 is a rigid (or 1-valued) grammar."
We now recall some useful definitions and known properties on learning.
A class CL of languages has a limit point iff there exists an infinite sequence &lt; L n &gt; n∈N of lan-guages in CL and a language L ∈ CL such that:
L 0 ( L 1 . . . ( ... ( L n ( . . . and L = S n∈N L n (L is a limit point of CL).
The following property is important for our pur-pose.
If the languages of the grammars in a class G have a limit point then the class G is unlearnable. 1
"For ease of proof, in next section we use two kinds of models that we now recall: free groups and pre-groups introduced recently[REF_CITE]as an alternative of existing type grammars."
"Let F G denote the free group with generators P r, operation · and with neutral element 1."
"We associate with each formula C of L or NL, an element in F G written [[C]] as follows: [[A]] ="
A if A is a primitive type [[C 1 \ C 2 ]] = [[C 1 ]] −1 · [[C 2 ]] [[C 1 / C 2 ]] = [[C 1 ]] · [[C 2 ]] −1 [[C 1 • C 2 ]] = [[C 1 ]] · [[C 2 ]]
"We extend the notation to sequents by: [[C 1 , C 2 , . . . , C n ]] = [[C 1 ]] · [[C 2 ]] · · · · · [[C n ]]"
The following property states that F G is a model for
L (hence for NL): if Γ ` L C then [[Γ]] = FG [[C]]
"A pregroup is a structure (P, ≤ , ·, l, r, 1) such that (P, ≤, ·, 1) is a partially ordered monoid [Footnote_2] and l,r are two unary operations on P that satisfy for all a ∈ P a l a ≤ 1 ≤ aa l and aa r ≤ 1 ≤ a r a."
"2 We briefly recall that a monoid is a structure &lt; M, ·, 1 &gt;, such that · is associative and has a neutral element 1 (∀x ∈ M : 1 · x = x · 1 = x). A partially ordered monoid is a monoid M, ·, 1) with a partial order ≤ that satisfies ∀a, b, c: a ≤ b ⇒ c · a ≤ c · b and a · c ≤ b · c."
"Let (P,≤) be an ordered set of primitive types, P ( ) = {p (i) | p ∈ P,i ∈ Z} is the set of atomic types and T (P,≤) ="
"P ( ) ∗ = {p (1i 1 ) ··· p (ni | 0 ≤ k ≤ n,p k ∈ P and i k ∈ Z} n ) is the set of types."
"For X and Y ∈ T (P,≤) , X ≤ Y iif this relation is deductible in the following system where p, q ∈ P , n, k ∈ Z and X, Y, Z ∈ T (P,≤) :"
"This construction, proposed by Buskowski, de-fines a pregroup that extends ≤ on primitive types P to T (P,≤)[Footnote_3] ."
"3 Left and right adjoints are defined by (p (n) ) l = p (n−1) , (p (n) ) r = p (n+1) , (XY ) l = Y l X l and (XY ) r = Y r X r . We write p for p (0) ."
"As for L and NL, cut rule can be eliminated: every derivable inequality has a cut-free derivation."
Simple free pregroup.
A simple free pregroup is a free pregroup where the order on primitive type is equality.
Free pregroup interpretation.
Let FP denotes the simple free pregroup with P r as primitive types.
"We associate with each formula C of L or NL, an element in FP written [C] as follows: [A] ="
A if A is a primitive type [C 1 \ C 2 ] = [C 1 ] r [C 2 ] [C 1 / C 2 ] = [C 1 ] [C 2 ] l [C 1 • C 2 ] = [C 1 ] [C 2 ]
"We extend the notation to sequents by: [A 1 , . . . , A n ] = [A 1 ] · · · [A n ]"
The following property states that FP is a model for L (hence for NL): if Γ ` L C then [Γ] ≤ FP [C].
Form of grammars.
"We define grammars G n where A, B, D n and E n are complex types and S is the main type of each grammar:"
G n = {a 7→ A / B; b 7→ D n ; c 7→ E n \ S}
"Some key points. • We prove that {a k bc | 0 ≤ k ≤ n} ⊆ L(G n ) using the following properties: bc ∈ L(G n ) since D n ` E n if w ∈ L(G n ) then aw ∈ L(G n+1 ) since (A / B, D n+1 ) ` D n ` E n ` E n+1 • The condition A 6` B is crucial for strict-ness of language inclusion."
"In particular: (A / B, A) 6` A, where A = D 0 • This construction is in some sense more com-plex than those for the other systems[REF_CITE]since they do not directly translate as limit points in the more restricted system NL."
"Definitions of Rigid grammars G n and G ∗ Definition 1 Let p, q, S, three primitive types."
A = D 0 = E 0 = q / (p \ q) B = p D n+1 = (A / B) \
D n E n+1 = (A / A) \ E n Let G n =  a 7→ A / B = (q / (p \ q)) / p  b 7→ D n Let G ∗ = {a 7→c 7→(pE/ p) b 7→ p c 7→ (p \ S)} n \ S
Main Properties Proposition 1 (language description) • L(G n ) = {a k bc | 0 ≤ k ≤ n} • L(G ∗ ) = {a k bc | 0 ≤ k}.
From this construction we get a limit point and the following result.
Proposition 2 (NL-non-learnability)
The class of languages of rigid (or k-valued for an arbitrary k) non-associative Lambek grammars (not allowing empty sequence and without product) admits a limit point ; the class of rigid (or k-valued for an arbitrary k) non-associative Lambek grammars (not allowing empty sequence and without product) is not learn-able from strings.
Lemma {a k bc | 0 ≤ k ≤ n} ⊆ L(G n )
"Proof: It is relatively easy to see that for 0 ≤ k ≤ n, a k bc ∈ L(G n )."
We have to consider ((a · · · (a(a b)) · · · )c) and prove the following se-quent| {inz NL} : z(|
NL S {z n
Lemma {a k bc | 0 ≤ k} ⊆ L(G ∗ )
"Proof: As with G n , it is relatively easy to see that for k ≥ 0, a k bc ∈ L(G ∗ )."
"We have to consider ((a · · · (a(a b)) · · · )c) and prove the following se-quent| {inz NL} : k (((p / p), . .. , ((p / p), p) · · · ), (p \ S)) ` NL S | {z } k"
L(G ∗ ) ⊆ {a k bc | 0 ≤ k}
"Proof: Like for w ∈ G n , due to free groups, a word of L(G ∗ ) has exactly one occurrence of c and one occurrence of b on the left of c (since [[τ ∗ (c)]] = p −1 S, [[τ ∗ (b)]] = p and [[τ ∗ (a)]] = 1). 0 00"
"Suppose w = a k ba k ca k a similar discussion as for G n in pregroups, gives k 0 = k 00 = 0, hence the result"
"An interest point of this construction: It provides a limit point for the whole hierarchy of Lambek gram-mars, and pregroup grammars."
"We have shown that with-out empty sequence, non-associative Lambek rigid grammars are not learnable from strings."
"With this result, the whole landscape of Lambek-like rigid grammars (or k-valued for an arbitrary k) is now de-scribed as for the learnability question (from strings, in Gold’s model)."
Non-learnability for subclasses.
Our construct is of order [Footnote_5] and does not use the product operator.
"5 The order of a type p 1i 1 · · · p ki is the maximum of the ab- k solute value of the exponents: max(|i 1 |, . . . , |i k |)."
"Thus, we have the following corollaries: • Restricted connectives: k-valued NL, NL ∅ , L and L ∅ grammars without product are not learnable from strings. • Restricted type order: - k-valued NL, NL ∅ , L and L ∅ grammars (with-out product) with types not greater than or-der [Footnote_5] are not learnable from strings [Footnote_4] . - k-valued free pregroup grammars with type-s not greater than order 1 are not learnable from strings 5 ."
"5 The order of a type p 1i 1 · · · p ki is the maximum of the ab- k solute value of the exponents: max(|i 1 |, . . . , |i k |)."
"4 Even less for some systems. For example in L ∅ , all E n collapse to A"
The learnability question may still be raised for NL grammars of order lower than 5.
Special learnable subclasses.
"Note that howev-er, we get specific learnable subclasses of k-valued grammars when we consider NL, NL ∅ , L or L ∅ without product and we bind the order of types in grammars to be not greater than 1."
This holds for all variants of Lambek grammars as a corollary of the equivalence between generation in classical catego-rial grammars and in Lambek systems for grammars with such product-free types[REF_CITE].
Restriction on types.
An interesting perspective for learnability results might be to introduce reason-able restrictions on types.
"From what we have seen, the order of type alone (order 1 excepted) does not seem to be an appropriate measure in that context."
These results also indicate the necessity of using structured examples as input of learning algorithms.
What intermediate structure should then be taken as a good alternative between insufficient structures (strings) and linguistic unreal-istic structures (full proof tree structures) remains an interesting challenge.
"The model used by the CCG parser[REF_CITE]would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch."
"This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model[REF_CITE], and defines a generative model for CCG derivations that captures these dependencies, includ-ing bounded and unbounded long-range dependencies."
"State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars[REF_CITE],[REF_CITE], but also for Categorial Grammar[REF_CITE], include models of bilexical dependencies defined in terms of local trees."
"However, this paper demonstrates that such models would be inadequate for languages with freer word order."
"We use the example of Dutch ditransitives, but our argument equally applies to other languages such as Czech (see[REF_CITE])."
"We argue that this problem can be avoided if instead the bilexical dependencies in the predicate-argument structure are captured, and propose a generative model for these dependencies."
"The focus of this paper is on models for Combina-tory Categorial Grammar (CCG,[REF_CITE])."
"Due to CCG’s transparent syntax-semantics inter-face, the parser has direct and immediate access to the predicate-argument structure, which includes not only local, but also long-range dependencies arising through coordination, extraction and con-trol."
"These dependencies can be captured by our model in a sound manner, and our experimental re-sults for English demonstrate that their inclusion im-proves parsing performance."
"However, since the predicate-argument structure itself depends only to a degree on the grammar formalism, it is likely that parsers that are based on other grammar for-malisms could equally benefit from such a model."
"The conditional model used by the CCG parser[REF_CITE]also captures dependencies in the predicate-argument structure; however, their model is inconsistent."
"First, we review the dependency model proposed[REF_CITE]."
We then use the example of Dutch ditransitives to demon-strate its inadequacy for languages with a freer word order.
"This leads us to define a new generative model of CCG derivations, which captures word-word de-pendencies in the underlying predicate-argument structure."
"We show how this model can capture long-range dependencies, and deal with the pres-ence of multiple dependencies that arise through the presence of long-range dependencies."
"In our current implementation, the probabilities of derivations are computed during parsing, and we discuss the dif-ficulties of integrating the model into a probabilis-tic chart parsing regime."
"Since there is no CCG treebank for other languages available, experimen-tal results are presented for English, using CCGbank[REF_CITE], a translation of the Penn Treebank to CCG."
These results demon-strate that this model benefits greatly from the inclu-sion of long-range dependencies.
"It as-sumes that binary trees (with parent category P ) have one head child (with category H ) and one non-head child (with category D ), and that each node has one lexical head h = h c; w i ."
"In the following tree, P = S [ dcl ] n NP , H =( S [ dcl ] n NP ) ="
"NP , D = NP , h H = h ( S [ dcl ] n NP ) ="
"NP ; opened i , and h D = h N ; doors i ."
S [ dcl ] n NP ( S [ dcl ] n NP ) =
NP NP opened its doors
"The model conditions w D on its own lexical cate-gory c D , on h H = h c H ; w H i and on the local tree in which the D is generated (represented in terms of the categories h P; H; D i ):"
P ( w D j c D ; = h P; H; D i ; h H = h c H ; w H i )
"We assume that a lexical head is a pair h c; w i , consisting of a word w and its lexical category c ."
Each constituent has at least one lexical head (more if it is a coordinate construction).
"The arguments of functor categories are numbered from 1 to n , starting at the innermost argument, where n is the arity of the functor, eg. ( S [ dcl ] n NP 1) ="
"NP 2 , ( NP n NP 1) = ( S [ dcl ] = NP )2 ."
De-pendencies hold between lexical heads whose cat-egory is a functor category and the lexical heads of their arguments.
"Such dependencies can be ex-pressed as 3-tuples hh c; w i ; i; h c ; w ii , where c is a"
"The predicate-argument structure that corre-sponds to a derivation contains not only local, but also long-range dependencies that are projected from the lexicon or through some rules such as the coordination of functor categories."
"For details, see[REF_CITE]."
Dutch has a much freer word order than English.
The analyses given[REF_CITE]assume that this can be accounted for by an extended use of composition.
"As indicated by the indices (which are only included to improve readability), in the following examples, hij is the subject ( NP 3 ) of geeft, de politieman the indirect object ( NP 2 ), and een bloem the direct object ( NP 1 ). [Footnote_1]"
1 The variables T are uninstantiated for reasons of space.
Hij geeft de politieman een bloem (He gives the policeman a flower)
S = ( S = NP 3 ) (( S = NP 1 ) =
NP 2 ) =
NP 3 T n ( T = NP 2 ) T n ( T = NP &lt; 1 B ) T n (( T = NP 1 ) = NP 2 ) &lt; B S = NP 3 &gt; S Een bloem geeft hij de politieman S = ( S = NP 1 ) (( S = NP 1 ) =
NP 2 ) =
NP 3 T n ( T = NP 3 &lt; ) T n ( T = NP 2 ) ( S = NP 1 ) =
NP 2 &lt; S = NP [Footnote_1] &gt; S De politieman geeft hij een bloem S = ( S = NP 2 ) (( S = NP 1 ) =
1 The variables T are uninstantiated for reasons of space.
NP 2 ) =
NP 3 T n ( T = NP 3 &lt; ) T n ( T = NP 1 ) ( S = NP 1 ) =
NP 2 &lt; B S = NP 2 &gt; S
A SD model estimated from a corpus containing these three sentences would not be able to capture the correct dependencies.
"Unless we assume that the above indices are given as a feature on the NP categories, the model could not distinguish between the dependency relations of Hij and geeft in the first sentence, bloem and geeft in the second sen-tence and politieman and geeft in the third sentence."
"Even with the indices, either the dependency be-tween politieman and geeft or between bloem and geeft in the first sentence could not be captured by a model that assumes that each local tree has exactly one head."
"Furthermore, if one of these sentences oc-curred in the training data, all of the dependencies in the other variants of this sentence would be unseen to the model."
"However, in terms of the predicate-argument structure, all three examples express the same relations."
The model we propose here would therefore be able to generalize from one example to the word-word dependencies in the other examples.
The cross-serial dependencies of Dutch are one of the syntactic constructions that led people to believe that more than context-free power is re-quired for natural language analysis.
Here is an example together with the CCG derivation[REF_CITE]: dat ik Cecilia de paarden zag voeren (that I Cecilia the horses saw feed)
NP 1 NP 2 NP 3 (( S n NP 1 ) n NP 2 ) =
VP VP n &gt; NP B 3 (( S n NP 1 ) n NP 2 ) n NP 3 &lt; ( S n NP 1 ) n NP 2 &lt; S n NP 1 &lt; S
"Again, a local dependency model would systemat-ically model the wrong dependencies in this case, since it would assume that all noun phrases are ar-guments of the same verb."
"However, since there is no Dutch corpus that is annotated with CCG derivations, we restrict our at-tention to English in the remainder of this paper."
"We first explain how word-word dependencies in the predicate-argument structure can be captured in a generative model, and then describe how these prob-abilities are estimated in the current implementation."
We first define the probabilities for purely local de-pendencies without coordination.
"By excluding non-local dependencies and coordination, at most one dependency relation holds for each word."
Consider the following sentence:
S [ dcl ] NP S [ dcl ] n NP N S [ dcl ] n NP ( S n NP ) n ( S n NP ) Smith resigned yesterday
This derivation expresses the following depen-dencies: hh S [ dcl ] n NP ; resigned i ; 1 ; h N ; Smith ii hh ( S n NP ) n ( S n NP ) ; yesterday i ; 2 ; h S [ dcl ] n NP ; resigned ii
"We assume again that heads are generated before their modifiers or arguments, and that word-word dependencies are expressed by conditioning modi-fiers or arguments on heads."
"Therefore, the head words of arguments (such as Smith) are generated in the following manner:"
P ( w a j c a ; hh c h ;w h i ; i; h c a ; w a ii )
The head word of modifiers (such as yesterday) are generated differently:
P ( w m j c m ; hh c m ;w m i ; i; h c h ;w h i )
"However, as the Dutch examples show, the argument slot i can only be determined if the head constituent is fully expanded."
"For instance, if S [ dcl ] expands to a non-head S = ( S = NP ) and to a head S [ dcl ] ="
"NP , it is necessary to know how the S [ dcl ] ="
"NP expands to determine which argument is filled by the non-head, even if we already know that the lexical cate-gory of the head word of S [ dcl ] ="
NP is a ditransitive (( S [ dcl ] = NP ) =
NP ) =
"Therefore, we assume that the non-head child of a node is only expanded after the head child has been fully expanded."
"The predicate-argument structure that corresponds to a derivation contains not only local, but also long-range dependencies that are projected from the lex-icon or through some rules such as the coordination of functor categories."
"In the following derivation, Smith is the subject of resigned and of left:"
S [ dcl ] NP S [ dcl ] n NP N S [ dcl ] n NP S [ dcl ] n NP [ conj ] Smith resigned conj S [ dcl ] n NP and left
"In order to express both dependencies, Smith has to be conditioned on resigned and on left:"
P ( w = Smith j N ; hhhh SS [ dcl ] n NP ; resigned i ; 1 ; h N ; w ii ; [ dcl ] n NP ; left i ; 1 ; h N ; w ii )
"In terms of the predicate-argument structure, resigned and left are both lexical heads of this sentence."
"Since neither fills an argument slot of the other, we assume that they are generated inde-pendently."
"This is different from the SD model, which conditions the head word of the second and subsequent conjuncts on the head word of the first conjunct."
"Similarly, in a sentence such as Miller and Smith resigned, the current model as-sumes that the two heads of the subject noun phrase are conditioned on the verb, but not on each other."
Argument-cluster coordination constructions such as give a dog a bone and a policeman a flower are another example where the dependencies in the predicate-argument structure cannot be expressed at the level of the local trees that combine the individual arguments.
"Instead, these dependencies are projected down through the category of the argument cluster:"
S n NP 1 (( S n NP 1 ) =
NP 2 ) =
NP 3 ( S n NP 1 ) n ((( S n NP 1 ) =
NP 2 ) =
NP 3 ) give
"Lexical categories that project long-range depen-dencies include cases such as relative pronouns, con-trol verbs, auxiliaries, modals and raising verbs."
"This can be expressed by co-indexing their argu-ments, eg. ( NP n NP ) = ( S [ dcl ] n NP ) for relative pro- i i nouns."
"Here, Smith is also the subject of resign:"
S [ dcl ] NP S [ dcl ] n NP N ( S [ dcl ] n NP ) = ( S [ b ] n NP ) S [ b ] n NP Smith will resign
"Again, in order to capture this dependency, we as-sume that the entire verb phrase is generated before the subject."
"In relative clauses, there is a dependency between the verbs in the relative clause and the head of the noun phrase that is modified by the relative clause:"
NP NP NP n NP N ( NP n NP ) = ( S [ dcl ] n NP ) S [ dcl ] n
NP Smith who resigned
"Since the entire relative clause is an adjunct, it is generated after the noun phrase Smith."
"Therefore, we cannot capture the dependency between Smith and resigned by conditioning Smith on resigned."
"In-stead, resigned needs to be conditioned on the fact that its subject is Smith."
This is similar to the way in which head words of adjuncts such as yesterday are generated.
"In addition to this dependency, we also assume that there is a dependency between who and resigned."
"It follows that if we want to capture unbounded long-range dependencies such as object extraction, words cannot be generated at the max-imal projection of constituents anymore."
Consider the following examples:
"In both cases, there is a S [ dcl ] = NP with lexical head ( S [ dcl ] n NP ) ="
"NP ; however, in the second case, the NP argument is not the object of the transitive verb."
This problem can be solved by generating words at the leaf nodes instead of at the maxi-mal projection of constituents.
After expanding the ( S [ dcl ] n NP ) =
NP node to ( S [ dcl ] n NP ) =
"NP and NP = NP , the NP that is co-indexed with woman can-not be unified with the object of saw anymore."
These examples have shown that two changes to the generative process are necessary if word-word dependencies in the predicate-argument structure are to be captured.
"First, head constituents have to be fully expanded before non-head constituents are generated."
"Second, words have to be generated at the leaves of the tree, not at the maximal projection of constituents."
Not all words have functor categories or fill argu-ment slots of other functors.
"For instance, punctu-ation marks, conjunctions, and the heads of entire sentences are not conditioned on any other words."
"Therefore, they are only conditioned on their lexical categories."
"Therefore, this model contains the fol-lowing three kinds of word probabilities: 1."
Argument probabilities: P ( w j c; hh c ; w i ; i; h c; w ii ) 0 0
"The probability of generating word w , given that its lexical category is c and that h c;w i is head of the i th argument of h c ; w i . 2."
P ( w j c; hh c; w i ; i; h c ; w ii ) 0 0
"The probability of generating word w , given that its lexical category is c and that h c ; w i is 0 0 head of the i th argument of h c; w i . 3."
Other word probabilities: P ( w j c )
"If a word does not fill any dependency relation, it is only conditioned on its lexical category."
"Like the SD model, we assume an underlying pro-cess which generates CCG derivation trees starting from the root node."
"Each node in a derivation tree has a category, a list of lexical heads and a (possi-bly empty) list of dependency relations to be filled by its lexical heads."
"As discussed in the previous section, head words cannot in general be generated at the maximal projection if unbounded long-range dependencies are to be captured."
This is not the case for lexical categories.
"We therefore assume that a node’s lexical head category is generated at its max-imal projection, whereas head words are generated at the leaf nodes."
"Since lexical categories are gen-erated at the maximal projection, our model has the same structural probabilities as the LexCat model[REF_CITE]."
"This model generates words in three different ways—as arguments of functors that are already generated, as functors which have already one (or more) arguments instantiated, or independent of the surrounding context."
"The last case is simple, as this probability can be estimated directly, by counting the number of times c is the lexical category of w in the training corpus, and dividing this by the number of times c occurs as a lexical category in the training corpus:"
P ^( w j c ) = C ( w; c ) C ( c )
"In order to estimate the probability of an argument w , we count the number of times it occurs with lex-ical category c and is the i th argument of the lexical functor h c ; w i in question, divided by the number with a constituent whose lexical head category is c :"
P ^( w j c; hh c ; w i ; i; h c; w ii ) = 0 0 C ( hh c ; w i ; i; h c; w ii ) P 0 0 w 00 C ( hh c 0 ; w 0 i ; i; h c; w ii 00 )
"The probability of a functor w , given that its i th ar-gument is instantiated by a constituent whose lexical head is h c ; w i can be estimated in a similar manner: 0 0"
P ^( w j c; hh c; w i ; i; h c 0 ; w 0 ii ) =
P C ( hh c; w i ; i; h c ; w ii ) 0 0 w 00 C ( hh c;w i h 00 ; i; c 0 ; w 0 ii )
"Here we count the number of times the i th argu-ment of h c; w i is instantiated with h c ; w i , and di- 0 0 vide this by the number of times that h c ; w i is the 0 0 i th argument of any lexical head with category c ."
"For instance, in order to compute the probability of yesterday modifying resigned as in the previous section, we count the number of times the transitive verb resigned was modified by the adverb yesterday and divide this by the number of times resigned was modified by any adverb of the same category."
"We have seen that functor probabilities are not only necessary for adjuncts, but also for certain types of long-range dependencies such as the rela-tion between the noun modified by a relative clause and the verb in the relative clause."
"In the case of zero or reduced relative clauses, some of these dependen-cies are also captured by the SD model."
"However, in that model, only counts from the same type of con-struction could be used, whereas in our model, the functor probability for a verb in a zero or reduced relative clause can be estimated from all occurrences of the head noun."
"In particular, all instances of the noun and verb occurring together in the training data (with the same predicate-argument relation between them, but not necessarily with the same surface con-figuration) are taken into account by the new model."
"To obtain the model probabilities, the relative fre-quency estimates of the functor and argument prob-abilities are both interpolated with the word proba-bilities P ^ ( w j c ) ."
"In the presence of long-range dependencies and co-ordination, the new model requires the conditioning of certain events on multiple heads."
"Since it is un-likely that such probabilities can be estimated di-rectly from data, they have to be approximated in some manner."
"If we assume that all dependencies dep i that hold for a word are equally likely, we can approximate P ( w j c; dep 1 ; :::; dep n ) as the average of the individ-ual dependency probabilities:"
P ( w j c; dep 1 ; :::; dep n ) 1 X n P ( w j c; dep i ) n i =1
"This approximation is has the advantage that it is easy to compute, but might not give a good estimate, since it averages over all individual distributions."
This section describes how this model is integrated into a CKY chart parser.
Dynamic programming and effective beam search strategies are essential to guar-antee efficient parsing in the face of the high ambi-guity of wide-coverage grammars.
Both use the in-side probability of constituents.
"In lexicalized mod-els where each constituent has exactly one lexical head, and where this lexical head can only depend on the lexical head of one other constituent, the in-side probability of a constituent is the probability that a node with the label and lexical head of this constituent expands to the tree below this node."
The probability of generating a node with this label and lexical head is given by the outside probability of the constituent.
"In the model defined here, the lexical head of a constituent can depend on more than one other word."
"As explained in section 5.2, there are in-stances where the categorial functor is conditioned on its arguments – the example given above showed that verbs in relative clauses are conditioned on the lexical head of the noun which is modified by the relative clause."
"Therefore, the inside probability of a constituent cannot include the probability of any lexical head whose argument slots are not all filled."
"This means that the equivalence relation defined by the probability model needs to take into account not only the head of the constituent itself, but also all other lexical heads within this constituent which have at least one unfilled argument slot."
"As a conse-quence, dynamic programming becomes less effec-tive."
"There is a related problem for the beam search: in our model, the inside probabilities of constituents within the same cell cannot be directly compared anymore."
"Instead, the number of unfilled lexical heads needs to be taken into account."
"If a lexical head h c; w i is unfilled, the evaluation of the proba-bility of w is delayed."
This creates a problem for the beam search strategy.
The fact that constituents can have more than one lexical head causes similar problems for dynamic programming and the beam search.
"In order to be able to parse efficiently with our model, we use the following approximations for dy-namic programming and the beam search: Two con-stituents with the same span and the same category are considered equivalent if they delay the evalua-tion of the probabilities of the same words and if they have the same number of lexical heads, and if the first two elements of their lists of lexical heads are identical (the same words and lexical categories)."
"This is only an approximation to true equivalence, since we do not check the entire list of lexical heads."
"Furthermore, if a cell contains more than 100 con-stituents, we iteratively narrow the beam (by halv-ing it in size) [Footnote_2] until the beam search has no further effect or the cell contains less than 100 constituents."
2 Beam search is as[REF_CITE].
"This is a very aggressive strategy, and it is likely to adversely affect parsing accuracy."
"However, more lenient strategies were found to require too much space for the chart to be held in memory."
"A better way of dealing with the space requirements of our model would be to implement a packed shared parse forest, but we leave this to future work."
"We use sections 02-21 of CCGbank for training, sec-tion 00 for development, and section 23 for test-ing."
The input is POS-tagged using the tagger[REF_CITE].
"However, since parsing with the new model is less efficient, only sentences 40 tokens only are used to test the model."
"A fre-quency cutoff of 20 was used to determine rare words in the training data, which are replaced with their POS-tags."
Unknown words in the test data are also replaced by their POS-tags.
The models are evaluated according to their Parseval scores and to the recovery of dependencies in the predicate-argument structure.
Undirectional recovery (UdirP/UdirR) evalu-ates only whether there is a dependency between w and w 0 .
"Unlike unlabelled recovery, this does not pe- nalize the parser if it mistakes a complement for an adjunct or vice versa."
"In order to determine the impact of capturing dif-ferent kinds of long-range dependencies, four differ-ent models were investigated: The baseline model is like the LexCat model of (2002b), since the struc-tural probabilities of our model are like those of that model."
Local only takes local dependencies into account.
LeftArgs only takes long-range de-pendencies that are projected through left arguments ( n X ) into account.
"This includes for instance long-range dependencies projected by subjects, subject and object control verbs, subject extraction and left-node raising."
"All takes all long-range dependen-cies into account, in particular it extends LeftArgs by capturing also the unbounded dependencies aris-ing through right-node-raising and object extraction."
"Local, LeftArgs and All are all tested with the ag-gressive beam strategy described above."
"In all cases, the CCG derivation includes all long-range dependencies."
"However, with the models that exclude certain kinds of dependencies, it is possible that a word is conditioned on no dependencies."
"In these cases, the word is generated with P ( w j c ) ."
"Table 1 gives the performance of all four mod-els on section 23 in terms of the accuracy of lexical categories, Parseval scores, and in terms of the re-covery of word-word dependencies in the predicate-argument structure."
"Here, results are further bro-ken up into the recovery of local, all long-range, bounded long-range and unbounded long-range de-pendencies."
LexCat does not capture any word-word de-pendencies.
Its performance on the recovery of predicate-argument structure can be improved by 3% by capturing only local word-word dependen-cies (Local).
This excludes certain kinds of depen-dencies that were captured by the SD model.
"For in-stance, the dependency between the head of a noun phrase and the head of a reduced relative clause (the shares bought by John) is captured by the SD model, since shares and bought are both heads of the local trees that are combined to form the complex noun phrase."
"However, in the SD model the probability of this dependency can only be estimated from occur-rences of the same construction, since dependency relations are defined in terms of local trees and not in terms of the underlying predicate-argument struc- ture."
"By including long-range dependencies on left arguments (such as subjects) (LeftArgs), a further improvement of 0.7% on the recovery of predicate-argument structure is obtained."
This model captures the dependency between shares and bought.
"In con-trast to the SD model, it can use all instances of shares as the subject of a passive verb in the train-ing data to estimate this probability."
"Therefore, even if shares and bought do not co-occur in this partic-ular construction in the training data, the event that is modelled by our dependency model might not be unseen, since it could have occurred in another syn-tactic context."
"Our results indicate that in order to perform well on long-range dependencies, they have to be in-cluded in the model, since Local, the model that captures only local dependencies performs worse on long-range dependencies than LexCat, the model that captures no word-word dependencies."
"How-ever, with more than 5% difference on labelled pre- cision and recall on long-range dependencies, the model which captures long-range dependencies on left arguments performs significantly better on re-covering long-range dependencies than Local."
The greatest difference in performance between the mod-els which do capture long-range dependencies and the models which do not is on long-range dependen-cies.
"This indicates that, at least in the kind of model considered here, it is very important to model not just local, but also long-range dependencies."
"It is not clear why All, the model that includes all dependen-cies, performs slightly worse than the model which includes only long-range dependencies on subjects."
"On the Wall Street Journal task, the overall per-formance of this model is lower than that of the SD model[REF_CITE]."
"In that model, words are generated at the maxi-mal projection of constituents; therefore, the struc-tural probabilities can also be conditioned on words, which improves the scores by about 2%."
It is also very likely that the performance of the new models is harmed by the very aggressive beam search.
"This paper has defined a new generative model for CCG derivations which captures the word-word de-pendencies in the corresponding predicate-argument structure, including bounded and unbounded long-range dependencies."
"In contrast to the conditional model[REF_CITE], our model captures these dependencies in a sound and consistent man-ner."
The experiments presented here demonstrate that the performance of a simple baseline model can be improved significantly if long-range depen-dencies are also captured.
"In particular, our re-sults indicate that it is important not to restrict the model to local dependencies."
"Future work will ad-dress the question whether these models can be run with a less aggressive beam search strategy, or whether a different parsing algorithm is more suit-able."
The problems that arise due to the overly aggressive beam search strategy might be over-come if we used an n-best parser with a simpler probability model (eg. of the kind proposed[REF_CITE]) and used the new model as a re-ranker.
"The current implemen-tation uses a very simple method of estimating the probabilities of multiple dependencies, and more so-phisticated techniques should be investigated."
"We have argued that a model of the kind proposed in this paper is essential for parsing languages with freer word order, such as Dutch or Czech, where the model[REF_CITE](and other models of surface dependencies) would sys-tematically capture the wrong dependencies, even if only local dependencies are taken into account."
"For English, our experimental results demonstrate that our model benefits greatly from modelling not only local, but also long-range dependencies, which are beyond the scope of surface dependency models."
Minimal Recursion Semantics (MRS) is the standard formalism used in large-scale HPSG grammars to model underspecified semantics.
"We present the first provably efficient algorithm to enumerate the read-ings of MRS structures, by translating them into normal dominance constraints."
In the past few years there has been considerable activity in the development of formalisms for un-derspecified semantics[REF_CITE].
The common idea is to delay the enu-meration of all readings for as long as possible.
"In-stead, they work with a compact underspecified rep-resentation; readings are enumerated from this rep-resentation by need."
"Minimal Recursion Semantics (MRS)[REF_CITE]is the standard formalism for se-mantic underspecification used in large-scale HPSG grammars ([REF_CITE]; Copestake and Flickinger, )."
"Despite this clear relevance, the most obvious questions about MRS are still open: 1. Is it possible to enumerate the readings of MRS structures efficiently?"
No algorithm has been published so far.
"Existing implementa-tions seem to be practical, even though the problem whether an MRS has a reading is NP-complete ([REF_CITE].1). 2."
What is the precise relationship to other un-derspecification formalism?
"Are all of them the same, or else, what are the differences?"
"We distinguish the sublanguages of MRS nets and normal dominance nets, and show that they can be intertranslated."
This translation answers the first question: existing constraint solvers for normal dominance constraints can be used to enumerate the readings of MRS nets in low polynomial time.
The translation also answers the second ques-tion restricted to pure scope underspecification.
"It shows the equivalence of a large fragment of MRSs and a corresponding fragment of normal dominance constraints, which in turn is equivalent to a large fragment of Hole Semantics[REF_CITE]as proven[REF_CITE]."
"Additional underspecified treatments of ellipsis or reinterpretation, however, are available for extensions of dominance constraint only (CLLS, the constraint language for lambda structures[REF_CITE])."
Our results are subject to a new proof tech-nique which reduces reasoning about MRS struc-tures to reasoning about weakly normal dominance constraints[REF_CITE].
The previous proof techniques for normal dominance constraints[REF_CITE]do not apply.
We define a simplified version of Minimal Recur-sion Semantics and discuss differences to the origi-nal definitions presented[REF_CITE].
MRS is a description language for formulas of first order object languages with generalized quanti-fiers.
Underspecified representations in MRS consist of elementary predications and handle constraints.
"Roughly, elementary predications are object lan-guage formulas with “holes” into which other for-mulas can be plugged; handle constraints restrict the way these formulas can be plugged into each other."
"More formally, MRSs are formulas over the follow-ing vocabulary: 1."
An infinite set of variables ranged over by h. Variables are also called handles. 2.
"An infinite set of constants ranged over by x,y,z. Constants are the individual vari-ables of the object language. 3. Function symbols. (a) A set of function symbols written as P. (b) A set of quantifier symbols ranged over by Q (such as every and some)."
Pairs Q x are further function symbols (the variable binders of x in the object language). 4.
The symbol ≤ for the outscopes relation.
"Formulas of MRS have three kinds of literals, the first two are called elementary predications (EPs) and the third handle constraints: 1. h:P(x 1 ,...,x n ,h 1 ,...,h m ) where n,m ≥ 0 2. h:Q x (h 1 ,h 2 ) 3. h 1 ≤ h 2"
Label positions are to the left of colons ‘:’ and argu-ment positions to the right.
Let M be a set of literals.
The label set lab(M) contains those handles of M that occur in label but not in argument position.
The argument handle set arg(M) contains the handles of M that occur in argument but not in label position.
Definition 1 (MRS).
An MRS is finite set M of MRS-literals such that:
M1 Every handle occurs at most once in label and at most once in argument position in M.
M2 Handle constraints h 1 ≤ h 2 in M always relate argument handles h 1 to labels h 2 of M.
"M3 For every constant (individual variable) x in ar-gument position in M there is a unique literal of the form h:Q x (h 1 ,h 2 ) in M."
We call an MRS compact if it additionally satisfies:
M4 Every handle of M occurs exactly once in an elementary predication of M.
"We say that a handle h immediately outscopes a handle h 0 in an MRS M iff there is an EP E in M such that h occurs in label and h 0 in argument position of E. The outscopes relation is the reflexive, transitive closure of the immediate outscopes relation. every x some y student x book y read x,y {h 1 :every x (h 2 ,h 4 ),h 3 :student(x),h 5 :some y (h 6 ,h 8 ), h 7 :book(y),h 9 :read(x,y),h 2 ≤ h 3 ,h 6 ≤ h 7 }"
An example MRS for the scopally ambiguous sentence “Every student reads a book” is given in Fig. 1.
We often represent MRSs by directed graphs whose nodes are the handles of the MRS.
Elemen-tary predications are represented by solid edges and handle constraints by dotted lines.
"Note that we make the relation between bound variables and their binders explicit by dotted lines (as from every x to read x,y ); redundant “binding-edges” that are sub-sumed by sequences of other edges are omitted how-ever (from every x to student x for instance)."
"A solution for an underspecified MRS is called a configuration, or scope-resolved MRS."
Definition 2 (Configuration).
An MRS M is a con-figuration if it satisfies the following conditions.
C1 The graph of M is a tree of solid edges: handles don’t properly outscope themselves or occur in different argument positions and all handles are pairwise connected by elementary predications.
"C2 If two EPs h:P(...,x,...) and h 0 :"
"Q x (h 1 ,h 2 ) belong to M, then h 0 outscopes h in M (so that the binding edge from h 0 to h is redundant)."
"We call M a configuration for another MRS M 0 if there exists some substitution σ : arg(M 0 ) 7→ lab(M 0 ) which states how to identify argument handles of M 0 with labels of M 0 , so that:"
"C3 M = {σ(E) | E is EP in M 0 }, and C4 σ(h 1 ) outscopes h 2 in M, for all h 1 ≤ h 2 ∈ M 0 ."
"The value σ(E) is obtained by substituting all ar-gument handles in E, leaving all others unchanged."
The MRS in Fig. 1 has precisely two configura-tions displayed in Fig. 2 which correspond to the two readings of the sentence.
"In this paper, we present an algorithm that enumerates the configurations of MRSs efficiently."
Differences to Standard MRS.
Our version de-parts from standard MRS in some respects.
"First, we assume that different EPs must be labeled with different handles, and that labels cannot be identi-fied."
"In standard MRS, however, conjunctions are encoded by labeling different EPs with the same handle."
These EP-conjunctions can be replaced in a preprocessing step introducing additional EPs that make conjunctions explicit.
"Second, our outscope constraints are slightly less restrictive than the original “qeq-constraints.”"
"A handle h is qeq to a handle h 0 in an MRS M, h = q h 0 , if either h = h 0 or a quantifier h:Q x (h 1 ,h 2 ) occurs in M and h 2 is qeq to h 0 in M. Thus, h = q h 0 im-plies h ≤ h 0 , but not the other way round."
We believe that the additional strength of qeq-constraints is not needed in practice for modeling scope.
Recent work in semantic construction for HPSG[REF_CITE]supports our conjecture: the examples dis-cussed there are compatible with our simplification.
"Third, we depart in some minor details: we use sets instead of multi-sets and omit top-handles which are useful only during semantics construction."
"Dominance constraints are a general framework for describing trees, and thus syntax trees of logical for-mulas."
Dominance constraints are the core language underlying CLLS[REF_CITE]which adds par-allelism and binding constraints.
"We assume a possibly infinite signature Σ of func-tion symbols with fixed arities and an infinite set Var of variables ranged over by X,Y,Z. We write f,g for function symbols and ar( f ) for the arity of f ."
"A dominance constraint ϕ is a conjunction of dominance, inequality, and labeling literals of the following forms where ar( f ) = n: ϕ ::= X/ ∗ Y | X 6= Y | X : f (X 1 ,...,X n ) | ϕ ∧ ϕ 0"
"Dominance constraints are interpreted over finite constructor trees, i.e. ground terms constructed from the function symbols in Σ."
"We identify ground terms with trees that are rooted, ranked, edge-ordered and labeled."
"A solution for a dominance constraint con-sists of a tree τ and a variable assignment α that maps variables to nodes of τ such that all constraints are satisfied: a labeling literal X : f (X 1 ,...,X n ) is sat-isfied iff the node α(X) is labeled with f and has daughters α(X 1 ),...,α(X n ) in this order; a domi-nance literal X/ ∗ Y is satisfied iff α(X) is an ancestor of α(Y ) in τ; and an inequality literal X =6 Y is satis-fied iff α(X) and α(Y ) are distinct nodes."
Note that solutions may contain additional mate-rial.
"The tree f(a,b), for instance, satisfies the con-straint Y :a ∧ Z :b."
The satisfiability problem of arbitrary dominance constraints is NP-complete[REF_CITE]in general.
"However,[REF_CITE]identify a natural fragment of so called normal dominance constraints, which have a polynomial time satisfia-bility problem."
We call a variable a hole of ϕ if it occurs in argu-ment position in ϕ and a root of ϕ otherwise.
A dominance constraint ϕ is normal (and compact) if it satisfies the following conditions.
N1 (a) each variable of ϕ occurs at most once in the labeling literals of ϕ. (b) each variable of ϕ occurs at least once in the labeling literals of ϕ.
"N2 for distinct roots X and Y of ϕ, X =6 Y is in ϕ."
"N3 (a) if X C ∗ Y occurs in ϕ, Y is a root in ϕ. (b) if X C ∗ Y occurs in ϕ, X is a hole in ϕ. A dominance constraint is weakly normal if it satis-fies all above properties except for N1 (b) and N3 (b)."
The idea behind (weak) normality is that the con-straint graph (see below) of a dominance constraint consists of solid fragments which are connected by dominance constraints; these fragments may not properly overlap in solutions.
"Note that Definition 3 always imposes compact-ness, meaning that the heigth of solid fragments is at most one."
"As for MRS, this is not a serious restric-tion, since more general weakly normal dominance constraints can be compactified, provided that dom-inance links relate either roots or holes with roots."
We often represent domi-nance constraints as graphs.
"A dominance graph is the directed graph (V,/ ∗ ]/)."
"The graph of a weakly normal constraint ϕ is defined as follows: The nodes of the graph of ϕ are the variables of ϕ. A labeling literal X : f(X 1 ,...,X n ) of ϕ contributes tree edges (X,X i ) ∈ / for 1 ≤ i ≤ n that we draw as X X i ; we freely omit the label f and the edge order in the graph."
"A dominance literal X/ ∗ Y contributes a dom-inance edge (X,Y) ∈ / ∗ that we draw as X Y. Inequality literals in ϕ are also omitted in the graph."
"For example, the constraint graph f g on the right represents the dominance constraint X : f (X 0 ) ∧Y :g(Y 0 ) ∧ X 0 / ∗ Z ∧ a Y 0 / ∗ Z ∧ Z :a ∧ X6=Y ∧ X=6 Z ∧Y6=Z."
A dominance graph is weakly normal or a wnd-graph if it does not contain any forbidden subgraphs: Dominance graphs of a weakly normal dominance constraints are clearly weakly normal.
Solved Forms and Configurations.
The main dif-ference between MRS and dominance constraints lies in their notion of interpretation: solutions versus configurations.
Every satisfiable dominance constraint has in-finitely many solutions.
Algorithms for dominance constraints therefore do not enumerate solutions but solved forms.
We say that a dominance constraint is in solved form iff its graph is in solved form.
A wnd-graph Φ is in solved form iff Φ is a forest.
"The solved forms of Φ are solved forms Φ 0 that are more spe-cific than Φ, i.e. Φ and Φ 0 differ only in their dom-inance edges and the reachability relation of Φ ex- tends the reachability of Φ 0 ."
A minimal solved form of Φ is a solved form of Φ that is minimal with re-spect to specificity.
The notion of configurations from MRS applies to dominance constraints as well.
"Here, a configu-ration is a dominance constraint whose graph is a tree without dominance edges."
A configuration of a constraint ϕ is a configuration that solves ϕ in the obvious sense.
Simple solved forms are tree-shaped solved forms where every hole has exactly one out-going dominance edge.
"Simple solved forms and configurations correspond: Every simple solved form has exactly one configuration, and for every configuration there is exactly one solved form that it configures."
"Unfortunately, Lemma 1 does not extend to min-imal as opposed to simple solved forms: there are minimal solved forms without configurations."
"The constraint on the right of Fig. 3, for instance, has no configuration: the hole of L1 would have to be filled twice while the right hole of L2 cannot be filled."
We next map (compact) MRSs to weakly normal dominance constraints so that configurations are preserved.
"Note that this translation is based on a non-standard semantics for dominance constraints, namely configurations."
We address this problem in the following sections.
The translation of an MRS M to a dominance con-straint ϕ M is quite trivial.
"The variables of ϕ M are the handles of M and its literal set is: {h:P x 1 ,...,x n (h 1 ,...) | h:P(x 1 ,...,x n ,h 1 ,...) ∈ M} ∪{h:Q x (h 1 ,h 2 ) | h:Q x (h 1 ,h 2 ) ∈ M} ∪{h 1 / ∗ h 2 | h 1 ≤ h 2 ∈ M} ∪{h/ ∗ h 0 | h:Q x (h 1 ,h 2 ),h 0 :P(...,x,...) ∈ M} ∪{h6=h 0 | h,h 0 in distinct label positions of M}"
Compact MRSs M are clearly translated into (com-pact) weakly normal dominance constraints.
Labels of M become roots in ϕ M while argument handles become holes.
Weak root-to-root dominance literals are needed to encode variable binding condition C2 of MRS.
It could be formulated equivalently through lambda binding constraints of CLLS (but this is not necessary here in the absence of parallelism).
The translation of a compact MRS M into a weakly normal dominance constraint ϕ M preserves configurations.
This weak correctness property follows straight-forwardly from the analogy in the definitions.
We recall an algorithm[REF_CITE]that efficiently enumerates all minimal solved forms of wnd-graphs or constraints.
All results of this sec-tion are proved there.
"The algorithm can be used to enumerate config-urations for a large subclass of MRSs, as we will see in Section 6."
"But equally importantly, this algo-rithm provides a powerful proof method for reason-ing about solved forms and configurations on which all our results rely."
"Two nodes X and Y of a wnd-graph Φ = (V,E) are weakly connected if there is an undirected path from X to Y in (V,E)."
We call Φ weakly connected if all its nodes are weakly connected.
A weakly connected component (wcc) of Φ is a maximal weakly con-nected subgraph of Φ.
"The wccs of Φ = (V,E) form proper partitions of V and E."
The graph of a solved form of a weakly connected wnd-graph is a tree.
The enumeration algorithm is based on the notion of freeness.
A node X of a wnd-graph Φ is called free in Φ if there exists a solved form of Φ whose graph is a tree with root X.
A weakly connected wnd-graph without free nodes is unsolvable.
"Otherwise, it has a solved form whose graph is a tree (Prop. 2) and the root of this tree is free in Φ."
"Given a set of nodes V 0 ⊆ V , we write Φ| V 0 for the restriction of Φ to nodes in V 0 and edges in V 0 ×V 0 ."
The following lemma characterizes freeness: Lemma 2.
A wnd-graph Φ with free node X satis-fies the freeness conditions:
"F1 node X has indegree zero in graph Φ, and"
F2 no distinct children Y and Y 0 of X in Φ that are linked to X by immediate dominance edges are weakly connected in the remainder Φ| V\{X} .
The algorithm for enumerating the minimal solved forms of a wnd-graph (or equivalently constraint) is given in Fig. 4.
We illustrate the algorithm for the problematic wnd-graph Φ in Fig. 3.
"The graph of Φ is weakly connected, so that we can call solve(Φ)."
This procedure guesses topmost fragments in solved forms of Φ (which always exist by Prop. 2).
"The only candidates are L1 or L2 since L3 and L4 have incoming dominance edges, which violates F1 ."
Let us choose the fragment L2 to be topmost.
The graph which remains when removing L2 is still weakly connected.
"It has a single minimal solved form computed by a recursive call of the solver, where L1 dominates L3 and L4 ."
"The solved form of the restricted graph is then put below the left hole of L2 , since it is connected to this hole."
"As a result, we obtain the solved form on the right of Fig. 3."
The function solved-form(Φ) com-putes all minimal solved forms of a weakly normal dominance graph Φ; it runs in quadratic time per solved form.
"Next, we explain how to encode a large class of MRSs into wnd-constraints such that configurations correspond precisely to minimal solved forms."
The result of the translation will indeed be normal.
The naive representation of MRSs as weakly nor-mal dominance constraints is only correct in a weak sense.
The encoding fails in that some MRSs which have no configurations are mapped to solvable wnd-constraints.
"For instance, this holds for the MRS on the right in Fig 3."
"We cannot even hope to translate arbitrary MRSs correctly into wnd-constraints: the configurability problem of MRSs is NP-complete, while satisfia-bility of wnd-constraints can be solved in polyno-mial time."
"Instead, we introduce the sublanguages of MRS-nets and equivalent wnd-nets, and show that they can be intertranslated in quadratic time."
"A hypernormal path[REF_CITE]in a wnd-graph is a sequence of adjacent edges that does not traverse two outgoing dominance edges of some hole X in sequence, i.e. a wnd-graph without situa-tions Y 1 X Y 2 ."
A dominance net Φ is a weakly normal domi-nance constraint whose fragments all satisfy one of the three schemas in Fig. 5.
MRS-nets can be de-fined analogously.
"This means that all roots of Φ are labeled in Φ, and that all fragments X : f (X 1 ,...,X n ) of Φ satisfy one of the following three conditions: strong. n ≥ 0 and for all Y ∈ {X 1 ,...,X n } there ex-ists a unique Z such that Y C ∗ Z in Φ, and there exists no Z such that X C ∗ Z in Φ. weak. n ≥ 1 and for all Y ∈ {X 1 ,...,X n−1 ,X} there exists a unique Z such that Y C ∗ Z in Φ, and there exists no Z such that X n C ∗ Z in Φ. island. n = 1 and all variables in {Y | X 1 C ∗ Y} are connected by a hypernormal path in the graph of the restricted constraint Φ |V−{X 1 } , and there exists no Z such that X C ∗ Z in Φ."
"The requirement of hypernormal connections in islands replaces the notion of chain-connectedness[REF_CITE], which fails to apply to dom-inance constraints with weak dominance edges."
"For ease of presentation, we restrict ourselves to a simple version of island fragments."
"In general, we should allow for island fragments with n &gt; 1."
Dominance nets are wnd-constraints.
We next trans-late dominance nets Φ to normal dominance con-straints Φ 0 so that Φ has a configuration iff Φ 0 is sat-isfiable.
The trick is to normalize weak dominance edges.
The normalization norm(Φ) of a weakly nor-mal dominance constraint Φ is obtained by convert-ing all root-to-root dominance literals X C ∗ Y as fol-lows:
X C ∗ Y ⇒ X n C ∗ Y if X roots a fragment of Φ that satisfies schema weak of net fragments.
If Φ is a dominance net then norm(Φ) is indeed a normal dominance net.
The configurations of a weakly con-nected dominance net Φ correspond bijectively to the minimal solved forms of its normalization norm(Φ).
"For illustration, consider the problematic wnd-constraint Φ on the left of Fig. 3."
Φ has two minimal solved forms with top-most fragments L1 and L2 re-spectively.
"The former can be configured, in contrast to the later which is drawn on the right of Fig. 3."
Normalizing Φ has an interesting consequence: norm(Φ) has (in contrast to Φ) a single minimal solved form with L1 on top.
"Indeed, norm(Φ) cannot be satisfied while placing L2 topmost."
Our algorithm detects this correctly: the normalization of fragment L2 is not free in norm(Φ) since it violates property F2 .
The proof of Theorem 2 captures the rest of this section.
We show in a first step (Prop. 3) that the con-figurations are preserved when normalizing weakly connected and satisfiable nets.
"In the second step, we show that minimal solved forms of normalized nets, and thus of norm(Φ), can always be configured (Prop. 4)."
Configurability of weakly connected MRS-nets can be decided in polynomial time; con-figurations of weakly connected MRS-nets can be enumerated in quadratic time per configuration.
"Most importantly, nets can be recursively decom-posed into nets as long as they have configurations:"
"If a dominance net Φ has a configuration whose top-most fragment is X : f(X 1 ,...,X n ), then the restriction Φ |V−{X,X 1 ,...,X n } is a dominance net."
Note that the restriction of the problematic net Φ by L2 on the left in Fig. 3 is not a net.
"This does not contradict the lemma, as Φ does not have a configu-ration with top-most fragment L2 ."
First note that as X is free in Φ it cannot have incoming edges (condition F1 ).
"This means that the restriction deletes only dominance edges that depart from nodes in {X,X 1 ,...,X n }."
Other fragments thus only lose ingoing dominance edges by normality condition N3 .
Such deletions preserve the validity of the schemas weak and strong .
The island schema is more problematic.
We have to show that the hypernormal connections in this schema can never be cut.
"So suppose that Y : f (Y 1 ) is an island fragment with outgoing dominance edges Y 1 C ∗ Z 1 and Y 1 C ∗ Z 2 , so that Z 1 and Z 2 are con-nected by some hypernormal path traversing the deleted fragment X : f(X 1 ,...,X n )."
"We distinguish the three possible schemata for this fragment: weak: there is one other way of traversing weak fragments, shown in Fig. 6(b)."
Let X C ∗ Y be the weak dominance edge.
"The traversal proves that Y belongs to the weakly connected components of one of the X i , so the Φ ∧ X n C ∗ Y is unsatisfiable."
"This shows that the hole X n cannot be identified with any root, i.e. Φ does not have any configuration in con-trast to our assumption. island: free island fragments permit one single non-trivial form of traversals, depicted in Fig. 6(c)."
But such traversals are not hypernormal.
"A configuration of a weakly con-nected dominance net Φ configures its normalization norm(Φ), and vice versa of course."
Let C be a configuration of Φ.
We show that it also configures norm(Φ).
"Let S be the simple solved form of Φ that is configured by C (Lemma 1), and S 0 be a minimal solved form of Φ which is more general than S."
"Let X : f(Y 1 ,...,Y n ) be the top-most fragment of the tree S."
"This fragment must also be the top-most fragment of S 0 , which is a tree since Φ is assumed to be weakly connected (Prop. 2)."
"S 0 is constructed by our algorithm (Theorem 1), so that the evaluation of solve(Φ) must choose X as free root in Φ."
"Since Φ is a net, some literal X : f (Y 1 ,...,Y n ) must belong to Φ. Let Φ 0 = Φ |{X,Y 1 ,...,Y n } be the restriction of Φ to the lower fragments."
"The weakly connected components of all Y 1 , ..., Y n−1 must be pairwise dis-joint by F2 (which holds by Lemma 2 since X is free in Φ)."
The X-fragment of net Φ must satisfy one of three possible schemata of net fragments: weak fragments: there exists a unique weak domi-nance edge X C ∗ Z in Φ and a unique hole Y n without outgoing dominance edges.
The variable Z must be a root in Φ and thus be labeled.
"If Z is equal to X then Φ is unsatisfiable by normality condition N2 , which is impossible."
"Hence, Z occurs in the restriction Φ 0 but not in the weakly connected components of any Y 1 , ..., Y n−1 ."
"Otherwise, the minimal solved form S 0 could not be configured since the hole Y n could not be identified with any root."
"Furthermore, the root of the Z-component must be identified with Y n in any configuration of Φ with root X. Hence, C satisfies Y n C ∗ Z which is add by normalization."
"The restriction Φ 0 must be a dominance net by Lemma 3, and hence, all its weakly connected com-ponents are nets."
"For all 1 ≤ i ≤ n − 1, the compo-nent of Y i in Φ 0 is configured by the subtree of C at node Y i , while the subtree of C at node Y n configures the component of Z in Φ 0 ."
"The induction hypothesis yields that the normalizations of all these compo-nents are configured by the respective subconfigura-tions of C. Hence, norm(Φ) is configured by C. strong or island fragments are not altered by nor-malization, so we can recurse to the lower fragments (if there exist any)."
"Minimal solved forms of normal, weakly connected dominance nets have configura-tions."
"By induction over the construction of min-imal solved forms, we can show that all holes of minimal solved forms have a unique outgoing dom-inance edge at each hole."
"Furthermore, all minimal solved forms are trees since we assumed connect-edness (Prop.2)."
"Thus, all minimal solved forms are simple, so they have configurations (Lemma 1)."
"We have related two underspecification formalism, MRS and normal dominance constraints."
"We have distinguished the sublanguages of MRS-nets and normal dominance nets that are sufficient to model scope underspecification, and proved their equiva-lence."
"Thereby, we have obtained the first provably efficient algorithm to enumerate the readings of un-derspecified semantic representations in MRS."
Our encoding has the advantage that researchers interested in dominance constraints can benefit from the large grammar resources of MRS.
This requires further work in order to deal with unrestricted ver-sions of MRS used in practice.
"Conversely, one can now lift the additional modeling power of CLLS to MRS."
We present a large-scale meta evaluation of eight evaluation measures for both single-document and multi-document summarizers.
"To this end we built a corpus consisting of (a) 100 Million auto-matic summaries using six summarizers and baselines at ten summary lengths in both English and Chinese, (b) more than 10,000 manual abstracts and extracts, and (c) 200 Million automatic document and summary retrievals using 20 queries."
We present both qualitative and quantitative results showing the strengths and draw-backs of all evaluation methods and how they rank the different summarizers.
Automatic document summarization is a field that has seen increasing attention from the NLP commu-nity in recent years.
"In part, this is because sum-marization incorporates many important aspects of both natural language understanding and natural lan-guage generation."
In part it is because effective auto-matic summarization would be useful in a variety of areas.
"Unfortunately, evaluating automatic summa-rization in a standard and inexpensive way is a diffi-cult task[REF_CITE]."
"Traditional large-scale evaluations are either too simplistic (using measures like precision, recall, and percent agreement which (1) don’t take chance agreement into account and (2) don’t account for the fact that human judges don’t agree which sentences should be in a summary) or too expensive (an approach using manual judge-ments can scale up to a few hundred summaries but not to tens or hundreds of thousands)."
"In this paper, we present a comparison of six summarizers as well as a meta-evaluation including eight measures: Precision/Recall, Percent Agree-ment, Kappa, Relative Utility, Relevance Correla-tion, and three types of Content-Based measures (cosine, longest common subsequence, and word overlap)."
"We found that while all measures tend to rank summarizers in different orders, measures like Kappa, Relative Utility, Relevance Correlation and Content-Based each offer significant advantages over the more simplistic methods."
We performed our experiments on the Hong Kong News corpus provided by the Hong Kong SAR of the People’s Republic of China (LDC catalog num-ber[REF_CITE]).
The texts are not typical news articles.
"The Hong Kong News-paper mainly publishes announcements of the local administration and descriptions of municipal events, such as an anniversary of the fire department, or sea-sonal festivals."
We tokenized the corpus to iden-tify headlines and sentence boundaries.
"For the En-glish text, we used a lemmatizer for nouns and verbs."
We also segmented the Chinese documents using the tool provided[URL_CITE]
Several steps of the meta evaluation that we per-formed involved human annotator support.
"First, we asked LDC to build a set of queries (Figure 1)."
Each of these queries produced a cluster of relevant doc-uments.
Twenty of these clusters were used in the experiments in this paper.
"Additionally, we needed manual summaries or ex-tracts for reference."
The LDC annotators produced summaries for each document in all clusters.
"In or-der to produce human extracts, our judges also la-beled sentences with “relevance judgements”, which indicate the relevance of sentence to the topic of the document."
The relevance judgements for sentences range from 0 (irrelevant) to 10 (essential).
"As[REF_CITE], in order to create an extract of a certain length, we simply extract the top scoring sentences that add up to that length."
"For each target summary length, we produce an extract using a summarizer or baseline."
Then we compare the output of the summarizer or baseline with the extract produced from the human relevance judgements.
Both the summarizers and the evalua-tion measures are described in greater detail in the next two sections.
This section briefly describes the summarizers we used in the evaluation.
All summarizers take as input a target length (n%) and a document (or cluster) split into sentences.
Their output is an n% extract of the document (or cluster). • MEAD[REF_CITE]:
MEAD is a centroid-based extractive summarizer that scores sentences based on sentence-level and inter-sentence features which indicate the qual-ity of the sentence as a summary sentence.
It then chooses the top-ranked sentences for in-clusion in the output summary.
MEAD runs on both English documents and on BIG5-encoded Chinese.
We tested the summarizer in both lan-guages. • WEBS (Websumm[REF_CITE]): can be used to produce generic and query-based summaries.
Websumm uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. • SUMM (Summarist[REF_CITE]): an extractive summarizer based on topic signa-tures. • ALGN (alignment-based): We ran a sentence alignment algorithm[REF_CITE]for each pair of English and Chinese stories.
We used it to automatically generate Chinese “manual” extracts from the English manual ex-tracts we received from LDC. • LEAD (lead-based): n% sentences are chosen from the beginning of the text. • RAND (random): n% sentences are chosen at random.
The six summarizers were run at ten different tar-get lengths to produce more than 100 million sum-maries (Figure 2).
"For the purpose of this paper, we only focus on a small portion of the possible experi-ments that our corpus can facilitate."
"We used three general types of evaluation measures: co-selection, content-based similarity, and relevance correlation."
"Co-selection measures include preci-sion and recall of co-selected sentences, relative util-ity[REF_CITE], and Kappa[REF_CITE]."
Co-selection meth-ods have some restrictions: they only work for ex-tractive summarizers.
Two manual summaries of the same input do not in general share many identical sentences.
We address this weakness of co-selection measures with several content-based similarity mea-sures.
"The similarity measures we use are word overlap, longest common subsequence, and cosine."
One advantage of similarity measures is that they can compare manual and automatic extracts with manual abstracts.
"To our knowledge, no system-atic experiments about agreement on the task of summary writing have been performed before."
We use similarity measures to measure interjudge agree-ment among three judges per topic.
"We also ap-ply the measures between human extracts and sum-maries, which answers the question if human ex-tracts are more similar to automatic extracts or to human summaries."
The third group of evaluation measures includes relevance correlation.
It shows the relative perfor-mance of a summary: how much the performance of document retrieval decreases when indexing sum-maries rather than full texts.
"Task-based evaluations (e.g., SUMMAC[REF_CITE], DUC[REF_CITE], or[REF_CITE]measure human performance using the summaries for a certain task (after the summaries are created)."
"Although they can be a very effective way of measuring summary quality, task-based evaluations are prohibitively expensive at large scales."
"In this project, we didn’t perform any task-based evaluations as they would not be appro-priate at the scale of millions of summaries."
"For each document and target length we produce three extracts from the three different judges, which we label throughout as J1, J2, and J3."
"We used the rates 5%, 10%, 20%, 30%, 40% for most experiments."
"For some experiments, we also consider summaries of 50%, 60%, 70%, 80% and 90% of the original length of the documents."
Figure 3 shows some abbreviations for co-selection that we will use throughout this section.
Precision and recall are defined as:
A A P J2 (J 1 ) =
"A + C ,R J2 (J 1 ) ="
A + B
"In our case, each set of documents which is com-pared has the same number of sentences and also the same number of sentences are extracted; thus P = R."
The average precision P avg (SY ST EM) and re-call R avg (SY STEM) are calculated by summing over individual judges and normalizing.
The aver-age interjudge precision and recall is computed by averaging over all judge pairs.
"However, precision and recall do not take chance agreement into account."
The amount of agreement one would expect two judges to reach by chance de-pends on the number and relative proportions of the categories used by the coders.
The next section on Kappa shows that chance agreement is very high in extractive summarization.
Kappa[REF_CITE]is an evalua-tion measure which is increasingly used in NLP an-notation work[REF_CITE].
Kappa has the following advantages over P and R: • It factors out random agreement.
"Random agreement is defined as the level of agreement which would be reached by random annotation using the same distribution of categories as the real annotators. • It allows for comparisons between arbitrary numbers of annotators and items. • It treats less frequent categories as more im-portant (in our case: selected sentences), simi-larly to precision and recall but it also consid-ers (with a smaller weight) more frequent cate-gories as well."
The Kappa coefficient controls agreement P(A) by taking into account agreement by chance P (E) :
P(A) − P(E) K = 1 − P(E)
"No matter how many items or annotators, or how the categories are distributed, K = 0 when there is no agreement other than what would be expected by chance, and K = 1 when agreement is perfect."
"If two annotators agree less than expected by chance, Kappa can also be negative."
"We report Kappa between three annotators in the case of human agreement, and between three hu-mans and a system (i.e. four judges) in the next sec-tion."
Relative Utility (RU)[REF_CITE]is tested on a large corpus for the first time in this project.
RU takes into account chance agreement as a lower bound and interjudge agreement as an upper bound of performance.
RU allows judges and summarizers to pick different sentences with similar content in their summaries without penalizing them for doing so.
Each judge is asked to indicate the importance of each sentence in a cluster on a scale from 0 to 10.
Judges also specify which sentences subsume or paraphrase each other.
"In relative utility, the score of an automatic summary increases with the impor-tance of the sentences that it includes but goes down with the inclusion of redundant sentences."
Content-based similarity measures compute the sim-ilarity between two summaries at a more fine-grained level than just sentences.
For each automatic extract S and similarity measure M we compute the following number:
"M(S,J3) sim(M,S,{J1,J2,J3}) = 3"
We used several content-based similarity mea-sures that take into account different properties of the text:
Cosine similarity is computed using the follow-ing formula[REF_CITE]:
"P x i ∗ y i cos(X,Y ) = pP (x i ) 2 ∗ pP (y i ) 2 where X and Y are text representations based on the vector space model."
"Longest Common Subsequence is computed as follows: lcs(X,Y ) = (length(X) + length(Y ) − d(X,Y ))/2 where X and Y are representations based on sequences and where lcs(X,Y ) is the length of the longest common subsequence between X and Y , length(X) is the length of the string X, and d(X, Y ) is the minimum number of deletion and in-sertions needed to transform X into Y[REF_CITE]."
Relevance correlation (RC) is a new measure for as-sessing the relative decrease in retrieval performance when indexing summaries instead of full documents.
The idea behind it is similar[REF_CITE].
"In that experiment, Sparck-Jones and Sakai determine that short summaries are good sub-stitutes for full documents at the high precision end."
With RC we attempt to rank all documents given a query.
"Suppose that given a query Q and a corpus of doc-uments D i , a search engine ranks all documents in D i according to their relevance to the query Q."
"If instead of the corpus D i , the respective summaries of all documents are substituted for the full docu-ments and the resulting corpus of summaries S i is ranked by the same retrieval engine for relevance to the query, a different ranking will be obtained."
"If the summaries are good surrogates for the full docu-ments, then it can be expected that rankings will be similar."
There exist several methods for measuring the similarity of rankings.
One such method is Kendall’s tau and another is Spearman’s rank correlation.
"Both methods are quite appropriate for the task that we want to perform; however, since search engines pro-duce relevance scores in addition to rankings, we can use a stronger similarity test, linear correlation between retrieval scores."
"When two identical rank-ings are compared, their correlation is 1."
Two com-pletely independent rankings result in a score of 0 while two rankings that are reverse versions of one another have a score of -1.
"Although rank correla-tion seems to be another valid measure, given the large number of irrelevant documents per query re-sulting in a large number of tied ranks, we opted for linear correlation."
"Interestingly enough, linear cor-relation and rank correlation agreed with each other."
Relevance correlation r is defined as the linear correlation of the relevance scores (x and y) as-signed by two different IR algorithms on the same set of documents or by the same IR algorithm on different data sets:
P (x r = i i − x)(y i − y) pP (x i i − x) 2 pP (y i i − y) 2
Here x and y are the means of the relevance scores for the document sequence.
We preprocess the documents and use Smart to index and retrieve them.
"After the retrieval process, each summary is associated with a score indicating the relevance of the summary to the query."
The relevance score is actually calculated as the inner product of the summary vector and the query vec-tor.
"Based on the relevance score, we can produce a full ranking of all the summaries in the corpus."
"In contrast[REF_CITE]who run 12 Boolean queries on a corpus of 21,000 documents and compare three types of documents (full docu-ments, lead extracts, and ANES extracts), we mea-sure retrieval performance under more than 300 con-ditions (by language, summary length, retrieval pol-icy for 8 summarizers or baselines)."
This section reports results for the summarizers and baselines described above.
We relied directly on the relevance judgements to create “manual extracts” to use as gold standards for evaluating the English sys-tems.
"To evaluate Chinese, we made use of a ta-ble of automatically produced alignments."
"While the accuracy of the alignments is quite high, we have not thoroughly measured the errors produced when mapping target English summaries into Chi-nese."
This will be done in future work.
"Co-selection agreement (Section 3.1) is reported in Figures 4, and 5)."
"The tables assume human perfor-mance is the upper bound, the next rows compare the different summarizers."
Figure 4 shows results for precision and recall.
"We observe the effect of a dependence of the nu-merical results on the length of the summary, which is a well-known fact from information retrieval eval-uations."
Websumm has an advantage over MEAD for longer summaries but not for 20% or less.
"Lead summaries perform better than all the automatic summarizers, and better than the human judges."
"This result usually occurs when the judges choose different, but early sentences."
Human judgements overtake the lead baseline for summaries of length 50% or more.
Figure 5 shows results using Kappa.
Random agreement is 0 by definition between a random pro-cess and a non-random process.
"While the results are overall rather low, the num-bers still show the following trends: • MEAD outperforms Websumm for all but the 5% target length. • Lead summaries perform best below 20%, whereas human agreement is higher after that. • There is a rather large difference between the two summarizers and the humans (except for the 5% case for Websumm)."
This numerical difference is relatively higher than for any other co-selection measure treated here. • Random is overall the worst performer. • Agreement improves with summary length.
Figures 6 and 7 summarize the results obtained through Relative Utility.
"As the figures indicate, random performance is quite high although all non-random methods outperform it significantly."
"Fur-ther, and in contrast with other co-selection evalua-tion criteria, in both the single- and multi-document case MEAD outperforms LEAD for shorter sum-maries (5-30%)."
The lower bound (R) represents the average performance of all extracts at the given sum-mary length while the upper bound (J) is the inter-judge agreement among the three judges.
The results obtained for a subset of target lengths using content-based evaluation can be seen in Fig-ures 8 and 9.
"In all our experiments with tf ∗ idf-weighted cosine, the lead-based summarizer ob-tained results close to the judges in most of the target lengths while MEAD is ranked in second position."
"In all our experiments using longest common sub-sequence, no system obtained better results in the majority of the cases."
The numbers obtained in the evaluation of Chi-nese summaries for cosine and longest common sub-sequence can be seen[REF_CITE]and 11.
Both measures identify MEAD as the summarizer that produced results closer to the ideal summaries (these results also were observed across measures and text representations).
"We have based this evaluation on target sum-maries produced by LDC assessors, although other alternatives exist."
"Content-based similarity mea-sures do not require the target summary to be a sub-set of sentences from the source document, thus, content evaluation based on similarity measures can be done using summaries published with the source documents which are in many cases available[REF_CITE]."
We present several results using Relevance Correla-tion.
RC is as high as 1.0 when full documents (FD) are compared to themselves.
One can notice that even random extracts get a relatively high RC score.
It is also worth observing that Chinese summaries score lower than their corresponding English summaries.
"As one might expect, longer summaries carry more of the content of the full doc-ument than shorter ones."
"At the same time, the rel-ative performance of the different summarizers re-mains the same across compression rates."
This paper describes several contributions to text summarization:
"First, we observed that different measures rank summaries differently, although most of them showed that “intelligent” summarizers outperform lead-based summaries which is encouraging given that previous results had cast doubt on the ability of summarizers to do better than simple baselines."
"Second, we found that measures like Kappa, Rel-ative Utility, Relevance Correlation and Content-Based, each offer significant advantages over more simplistic methods like Precision, Recall, and Per-cent Agreement with respect to scalability, applica-bility to multidocument summaries, and ability to include human and chance agreement."
"Third, we performed extensive experiments using a new evaluation measure, Relevance Correlation, which measures how well a summary can be used to replace a document for retrieval purposes."
"Finally, we have packaged the code used for this project into a summarization evaluation toolkit and produced what we believe is the largest and most complete annotated corpus for further research in text summarization."
The corpus and related software is slated for release by the LDC in mid 2003.
"¥:  r/Cn¡¢£o¤ML\¥l¦/ ( « ¸ ²&quot;\®­a¤¯¤\L |±°³²/|°®´\º¤\¤La|©¶|a´¦Tl»|L¥T|¤·¤R¨¢±µ¼C ½ \| \®|©a¤¾¿l¡¢ ´? o ©§Àªn©LlT´ ¢¤|?¬Á¥nÂ­Z¤\ln°Ã¤l ©Lª¾|¹L|±§À¤\®l |aÄ®¹L©|\°Ã®lµÅo©§Æ¤½ a§Ê¤\L®©¾o©n´\®­a¤Ê°Ã¥&quot;´¡´²ZÇÈ±°³¤R |®¦®Rn´\¥ lr¤l°Ã¥&quot;¢(¥ r¥&quot;|¬ |\±°³²L/°®´¡|¤Ro¤R©LªÌ|¤R¡°®n´ZÄR¹o©±¤l.n´ |¥Í¢±o¤R|¹L\a§Å¬Á¥n: ±n°E6°Ã¥&quot;´¡´ ±°³¤R ¥n©» |arl°eÏ¤\¥n ¡°ÃCo\, Î:©a¤\©n´¡´| ¥G§i¹²nµ°Ãa§Å¤|±´¨r¤\¥nªn¤\\L®|É «1§L |¤|±°³¤R|¥&quot;!§L l°Ã¹L|l &quot;¬L¬Á¹L¤\¹L|||arl°e"
"; &lt;&gt;K °p 39 = =&gt;H  rH35 ùó Õ;[REF_CITE]? &lt;; ! y5 ú m =&gt;5 ùó |=&gt;K Õ  ½ \| ¤­Z/l¡· \L¢§L |¤\l ¸ ¹LÉ ¤R¥T©&lt;&quot;¥ ¬G¤\Lq°Ã¥&quot;© °¨§i®©°³·l°³¥T|·¥&quot;¬/¤\®lè|aÄ®¹L© °Ã® \¤  o¤«d|Ú|L| ² «1 ¤|:|±¬ |É l \¥¯¤\L ¥nR ª7 © ¢®©±¤l» \ãn ©¨°e  o©§¤ LÏ¤ÎT¥n·¤^«d¥Æ\  ||/ ¥Ta¡µo©§|ë  í ½ \ë |° ¤ã|í |©a¤R°³É·T§£Õ û ·.­Z¤o©§·©a¤R°³É\¹Lãµ|M¥ë ¸&quot;í¬  |&amp;å ç ªn©Llr¤d» |±§!­Z¤RµT«d· ­Z¤\¹L|l»Ú´¡:\L ||¤|¤||l©Lª¢¤\ o¤Ú°Ã¥n©LÉ\\L ç  ºå å   Ð  ¤Rn´ a°Ã¤|±§Ûo©§á¤¢®©±¤\®l|¿á¬Á||¥aÄ®¹L©&quot; ¸ ¥T¤°Ã®¯«1|È°Ã¥&quot;|¤´¡´à¤±°³¤R\LÌ|8¹(|®É  °Ã¥&quot;T ©| |¯o\|¢«d||¥$\¤  o¤·¸ ¥T¤|C°Ã¥&quot;¥T¤´¡´ |±°³¤R\ o¤Ú¤\¸ LÉ |´¤\¥nl,¤\¥Û¤\$Ln°n§ia&lt;ln^²Z/r/ ë|¨»¸ |¤|\¤Rn°Ã¤L(| ©L«8í ¸ | r/ªTÉ §L © r¤l|©Lª©a¤,¬Á|©L«8&quot;¥ À§L| r/M°Ã¥|©a¤$&quot;L¹¢ ¸ r©´¡¡°®r¤l!¥n©Ì¥nMn°n§ia|·¡°âë ¨»|a»¡µ¥nÉ ä ±° ©à¤¤l\¤ L\L&lt;í¸ °o»Ú²Å°eL¥X¥Tl:|¥&quot;\¥n |&quot;´¡´r/±°³¤Rqo\¤l¡°´&quot;¬:§L|¥ TL¤|¸ ²H¬Á¥G°Ã¹Ll·T´¨´¡µ\|lµÉ ä  ä  ©Lªá¥n©Ï§Lr/| ¸ |¤||Rn°Ã¤©a¤Ön°n§ia|l» ·¡° ½ ±´¡§lµ8 ©à¤\L&lt;°o|¥&quot;¬ ä ë ¨» Î: ªT¹L¢\  l¡°®T´|L¥R«8»¡§L»¡¬,¤\» LÏ©L¥Tlí ¥&quot;¬:\¤ L·T´¨&lt;°Ã¥&quot;Ça§ © ;°¨§i®©°³ßl°³¥T  |¤\¥nªTln¢| ¥ ¬d¤\Lq¤\L|Í§L |©a¤á· ­Z±§&lt; T | : É&quot; ¤8¹ Ël°³¥T|&lt; | ©È ¸ ´  ü µ:¤\¥nªn¤\ ¤|"
Ý  ^«d®© ì  ¿ü¤P|±o|¥T©  ¸ ´o¤|$\¤ ®©a¤¬Á¥nn´¡´G¤¤\L&lt;8¹\L|¯ T| lç¸ ±´¸¥l«É |aÄ®¹L©¬Á¥nÚ¤\°Ã®L ¸   ²T¥T©¤\L|\|   ¹L ¹La»¸ ±°³¥7¢M°o©©LlT´¨´ ²µ7¤\®l\® ( ( ¬Á¥n!¤\Lá ©L|¤Ro©a¤.´ ®­/¨°Ã¥n©µÚ © §L¡° o¤R (|¥&quot;\¥n ¡°n´ ( Ý ß ¤(| L¥T¹ ´¡§ ¸ &lt; ©L¥T¤|±§à¤\ o¤(¤\LÅ¤\L|| |±´¨r¤\| ¸ ¤×«%\¢® ©±¤|l»ÈÙ%¥R«d®¦®Rµ ¸ q¹L|±§Ø¬Á¥nã§L l°Ãl¡· ©o¤R |±´  ||±´ É ¢®©±¤| ¸ ±°®r¹L|¢®©±¤|M«1 ¤| |aÄ®¹L© ·r¤l°e°o © | ¤R¨´¡´ ¸ :  l¡·¨´¡r ¤\!¥n¤\Ll»  ¸ ´  ü ¬Á|¥&quot;Ü§L |©a¤1°Ã¥n || |¤ ¥nl|êo|||±§Ï«1 ¤|Ëo\¤l¡°´ 
X  aÄ®¹L©Õ  °Ã®.| ¥.¢¥T¡¢|/¤l´¥T&lt;²|¤lr ©a¤| L:|¤\!¥ |®©±¤1©L¥T¤||a\ \L8­Z¤²Ø\ln°Ã¤¢±o©LÉ|±§ | ©Lª7¬ ¹´l  ±°³Ú¥&quot;¬L © ¬Á¥nl ¥T© »d:\¥n$¤\L|¢É oLT´ \·µ|²Tµ!¸ ²ã|®lT© §²Tµ|µ
O  N |Mo©§ë l°³¥T|&quot;´¡olµì 4Tü P » P T§i®¤|\â«d¤|±µ ||l¡°o©Lá©û  ®¤\ë ®Rµ í ML To¤¢|\±µaµ r  |&amp;å ç ì ñ  6 ç ´¡ë ¨¡| ¤ í¸ Q í µ| L ²\ë¤ ¨Lí QL »lT¡§||¤%|¥T©/¦±¨§ªTa¤\lµ ¥7«8¨°e» ON ë l°³¥T Õ || lo±´ìnü%&quot;°oL¤» Tí·\|¹L¤Mo¤\a§
M å ß ãñ Û  Û Þ ¼C¬Á|&quot;¥ ! ­Z²al¡Å/\ ©Ø¤\L\L®|ì   \¤ ®lã¡§L§L´&gt;|aÄ®¹L© SN ë °Ã®l°³¥T!|o \1¹Lìnì |±¬Á¹» Tí P »´ ß (Ý ç   &gt; © °oL¤\¹LR \LÖ¤\¥n |±´¨r¤R ¥T©L ¸ ¤×«%®©H¤\L &quot;¬L¤\L  |¤|1 P´ a¬Á¤!¬Á¥nP¬Á¹L¤\¹L|â|¤\¹ §²n»¥¢®©±¤|M ©C¤\L$ln ||¤ lµ ¸  ¤\  Mo| a°Ã¤ 0 òôónä   ; j5¹9|= aö o 5 qp &gt;J&lt;39; &lt;Øj= °÷ :5
"Õ ;| a°Ã¥n©§ ­Z/l¡¢®©±¤lµ,«dÜªn\/\L p /÷ &lt;= &lt;1L? 5 m &gt;= 5 ùó |=&gt;K ­Z¤\ln°Ã¤|±§  |¤|| ©a¤\¥Ü¤\LÜ¬Á¥&quot;´¡´ ¥l«1 ©LªÜ¤^«d¥ ªn\ë  íMòXó¸ ²·¤|\L¢ln\$ ! °Ã¥lÿ&quot;·¢¥T©Ì¬Á¥n| oë\¨»$!a»¡µ°Ã¥n´¡´X¤&quot;¢o¤¢´\/LMo¥Tar|a§¢¥|\¤l¡°´&quot;¬/o\&quot;í¤l¡°´µ¬a¤\L  r òZù s:u ë ¨ íßð¸¤\L¥nRù9ô\P P°Ã¥|&quot;·lÿ ¢¥T©Å¬Á¥n|\o! \q°Ã¥n´¡´X¤ë ¨» a»¡µÚ©L¥T©L&quot;\LMo¢/¥T|\¤l¡°´a§·¥(¥&quot;&quot;¬d¤í \» \¤l¡°´   :u s ä ©a¤R°³É:Î  ªT¹LÕ \û  ±»% Ú¤: \|a¬Á¤|.¹ o©§ß¤´ ¤ ¬Á¥n \LMl|ªn±¤|lµ.°Ã¥ãn&quot;´ ¹ ©¨°e¡µ/|L¥R«o©§"
"Û ÿç \¤ Lq|±´¨r¤R ¥T©L|   ¸ ¤×«%®©,\¤ Lq¤\®lÊ§L |\¤ l ¸ ¹L¤R ¥n© ºå l¡·¨´¡rl¤×²qo©§¢¤\L8¤\®lØ|aÄ®¹L© °Ã1°Ã¥&quot;© °¨§i®©°³1¬Á¥n \¤ Lá· ­Z±§Åo©§q¹L©¨Ä®¹L  |¤||!||/ ±°³¤R ¦a´ ²µ °´ ||¤|l»&lt;(±n°Eê: /¥7 ©a¤Í°Ã¥nÖ°Ã¥|\&quot;®´|¹ /¢©¥T©&lt;§i¤|L¥R«81¤\¥¶à§L\L·lr¤l|¤R ©°³¤R(¥ ¦T&quot;¥ ¬ \L8¤^«d¥1¤^²Z/&quot;¬a¤\L  |¤|| ©L|!§L |É¤ &quot;© °¨§i®©°³l°³¥T||±¬ |© °Ãaµi¤\L1|¦®RoªT  ä l°³¥T|&quot;¬/ ©Lª7´ |®©±¤|©  ân´ |¥P|L¥R«8©áo¢P§¥T¤|É \¤ a§·¦T|¤R¨°n´G´¡ ¢¥T¤l |±§ ¸ &lt;² â© T ¦T$L¹LR | ¤R¡° \²Å°eo©°³±»·ÎT¥n¢¤&quot;´ â|®©±¤\|© °Ã¯|/¥T|P¥a´&quot;/¬ ².| \¤ ¥ ¸ ¸ ¡´¡|¤×²lµ¥n ©´/±o¤\a§² ¤ ¸·¢¥T©a¤||\ å ë§¹L£¤|¤Ro¤R\|¥Ø¤¤R¡°³lµ\±o©±¹|ªTÍlr|² Ç®ì í  » µÕ +©|L¥R«8©Ì¬Á¥n¤R ¥T© µ å (ÝÝ §Lr¤|­Z±§|. ||¤¹§a§|â¥&quot;¸ ±°®r¹L|¢®©±¤| «1¤|ã¤§ß¤\\L·ln ß°o|  «1°Ã¥n©a¤Rn·r©a²¤|.©L£||®R|arl°eL®·r©a²d¥&quot;&gt;¬Gn°n§ia\·l» | |  ½r/°o¤R|¢°Ã¥nÉ9o¹L¤¥T©Llµ8|/\L¥T±°®|½ ±§°n´¡´¸ ²² ¤\Lé \¤oL|±§M¥n©Ö¤|¯°Ã¥&quot;´¡´\LqÎ±°³¤R: ªT¹L¸\  \µ:¦T.¤ ¸§L±°³¥7|©a¤R´ ²C©¤\®l\ ¢o¤"
Û &quot;||lµZ¤\LM§L | ¤R ©°³¤R ¥n© ¸ ¤×«%®©q¤\LM· ­Z±§qo©§&gt;¼  ¤| ¥ ¬·¤\||\¤ | ¹ °Ã¤||Ï¥&quot;¬·¤\LÏ¤\®­a¤|l» ä ¹L©¨Ä®¹L  |¤||â  ¸    ©a¹ ¸ |&quot;¬:o\¤l¡°´ ¢| r|²¤\L1ln¥T©Ll¡§l|¸ ´    \®© |©a¤R» ¥T|| |/ \¥T©La§L! ©Lª7´§lªn©\LÅ°er¤|±§$|/¥T|¤|&quot;| \oL\¤§L ¤ «d¥Mlr¤l\LÖ¥n¤(¥ °Ã¹L\|L¦®.¸ ±°³¥7r© ¢¢l´|¤ ¥l«È«1 ãn|¤ ©¨°e||l» ^ ä 
"Õ û \L®\ | ¤ ¥n©´ ,² (¬Á®« · ­Z±§&lt;°´ ||¤ |  ç ºå   $     ¥a¦ £ ´aµa¶ 0"
Î  ªT¹L\ Í : ²Zl &quot;¬  ||¤ |! o|| Í |±§$«1 ¤|£o\¤l¡°´  ¸ &lt;² 8¹ ´ ¤R ´  Í  |\ 10 10 100    100 S T U  T  1000 [ [ \ ][ ]^ 10000 [ T 10 100 1000 10000   1000   10000  § ¨ © ª¬« ¨   ² ¨ :  ~É °§  4 ~ Î ~É ¸1Î ~¸ ~  ~É ¸~~ °~~ © ¯
"Û X ¦ © ¯ «1 ¤|Í  ªTH°Ã¥&quot; © °¨§i®©°³Ml°³¥T|l»( ©Lª7´ ²iµ%\¤ L lr¤l ¥P°Ã¹L|\L®|¢°Ã¥&quot;´¡´ ±°³¤R  ¸ ±°³¥7|¤\®/®R» Õ  ¤R ¥T©  |¤ £¤\L ãn ©¨°e |\L LÉ ¹L©¨Ä®¹Lã°´ |¤||o\á§L ¦/¨§ia§Í ©a¤\¥Ö¤^«d¥áªn\ ç \®| \/©;|Éel¬Á¹L|ªna¤\¤ L|.ª&quot; ¥n©·«d· ©o¤R|! ¥T©µ¤\§¢¤L\,¥¯°Ã¥n©LÉ°´ |É ¤\ » ¤  ¤Rn  |©a¤M´ ¥G°n´ ±§L ¤l  ë  a²T¥Ío©§ ¥&quot;¬n¤\\| ard|\¤ ¥nl ®%|®©±¤ ¸ $² ¤\L8lnl |Éí ä (    ¥T|¤\®|l» \L®|ß r|¤R¨°Ã¹ ´¡oß°o|®lµ¤\L /°Ã¥&quot; © °¨§i®©°³1l°³¥T|1¥&quot;¬&quot;¤\L   |¤| |¤  jÐ ¤  r|±§C«1 ¤|Û¤\LÖ¥n¤\L¯¤^«d¥Ï°Ã¥&quot;´¡´ ±°³¤R °Ã¥&quot;\¢ÉL \L ãn ©¨°e $ °Ã¥&quot;´¡´ ± °³¤R  C|±´¨r¤R ¦a´ ²+´ ¥l« ç ©a¤R° ³É Õ û ¥n© T´°o|| |ß  ®R ¥&quot;¬X r/|d«d|âL\®|©a¤\a§1¥n©M¤\L¢ln\²| ºå ¸ ©C¤\L$© T¢P¥&quot; ||\%¥R«d®¦®Rµ \L®|1°o|\¹§a§$¬Á|&quot;¥ H¤\L ½ ªT¹L\ \ ¦T¤ ä ¸ ®©Øn´ |  ¸  ¸ ·|®©È © \¤ L ½ ªT¹L\a» © ¯ °´ :|¤||  \¤ L  |»|àë&quot;í ¬Á¥n¹L&lt;&quot;¥ ¬8¤|\|lµ ·ì   ­Z±§» üãë\LÉü ní Ý ½¬Á¥nªT¹Lãn\®ß©¨°e©£¤\LM¡µ/o©§r|©a¤ìnì \Li» ü!\ë oì  ní \·¤¬Á¥n%\Lá´©a¤R°³ÉÕ û \£¥&quot;¬ \¤ L| (Ý  Û  ¸Û ( Û ôç ºå  ¤ ®lÂ|aÄ®¹L© °Ã®â¥n©&lt;\¤ L ¸ ¥Tl§i®l»\ \LÌL\®¦/ ¥T¹LÌ°o|Í¸|L¥R«8©Ü´ ²¶n°Ã|¥T|q¤\L¸,´¥7 ¢§L ü µr||¤\©a¤a§M«1&lt;°Ã¥&quot;|¤ ´ ÉÉ ¤  ´n´a°Ã¤l| |²;·°Ã¥n©Ll¡§l\|¤\LÍ°´¨rl¡¬© ²Û¤\&quot;L¬8¤,\L§L&lt;¤R¨|©É ä ä |o¤R \L&lt;¤\L|| ¹L®l»Ü: . ¤R¨ ÐÏ  a´ ±°³¤\ ² · ­Z±§Ö°´ ||¤ ||1¤R¨¢.§É  ¥n©ã %ªn\ar¤|d¤\ o© |  a§ ¸ ¤×«%®©  ¬Á¥n!±n°Eê°Ã¥§L\²Zlµ7o©§&quot;´¡´ ±°³¤R$¥n©»°n´¡°³¹´¨r¤é o||±§q¥n©Å¤±§â¤\L¤\LM\L||||¹ ´ ¤1|L¥R«8©¹L® ÒÑ ÓÑ  ( «1 ©CÎ¤|:  ªT¹L\ ||µ/· ¤ ¸±°³¥7¢¥T||±´¨r¤\a§·¤\\ ¥¤o¤%\Lß§Lr¤\¤ L¢¤\®­a¤Ú|P¥&quot;|/¬¹L¤\L| |\¤ ¥nl ®l» «d|%|L|©´ ²Ø´¡¨·n¬Á¤\¤\a§Å´®do©M©a¤\\Í¥®|¦Xn´®¥&quot;¬ ¤\&quot;®l¬®|®¦®Rn´n§L|aÄ®¹L© \°Ã®²Zl» w"
"X ÐÏ ¤\®­a¤Õ |©1|¹ ·|²µT¤|\¥TLd|M§L|¹L|d|©a¤·r¤|¤\¢±§L¡|©1¥&quot;.¬l¤\o©§ML¹|¸ ´¡¡°®rÉ\a§ ¤Ro\·ªn©LlT´¨´|¤^ ®l»,²Û&gt;¼ ¢¥T |¤ |ã´¡a´|²á¤|\lµÚ¤¥ \¸ Öo|| |\|a§C¤±§ß«1\®­a¤|¤ |  ä l¯¥G§â¥|/ ±°®&quot;¬X¤R¨½ !° ¢±»¦T®©±¤·¤©M¤\\ \LÚ|1«1r© |¤ ©È¯¤| |ãnL¥T|©¨°e¤$/É¡µ ¤ L8¤\®­a¤|¢o\1 ©L|¤\|±§â¤\.¥  © §L ¦/¨§i¹ T´Lo\É\ "
Âç ¤R¨°´ â¥&quot;¬G¤\LÖ§L\  |¤ C©a¤R° ³É Õ û \L®²Ío\ l¡¢ ´ ²ßo|| | |¤ ( © §L ¦/¨§i¹ |\ ºå \\¤
L = ;3?
Õ  ½ ©n´T­Z/l¡¢®©±¤lµ«d1¢±o|¹L\a§¢¤\L1§ªn\® &quot;¥ ¬8ä |  &quot;å«1 ¤| ||lµ ãn ©¨°e  Õ û |±§!¤\¥P¥&quot;¬a¤\®lÈ|aÄ®¹L© °Ã® vç ¤  /±o|±§à¬Á¥n(¢¥T|.\¤  o©È.| a°Ã¥n©§Ö¤R¨¢£©\ ºå ¤\L °Ã¥&quot;´¡´ ±°³¤R ¥n©µi«1 |¤ !  \¤ La &quot; © °¨§i®©°³l°³¥T| ¸ ± ©Lª ªn\ar¤|:\¤  o©(Úª&quot;¦®©$¤\L||L¥7´¨§ » ë :L± ½ \ | /±oRo© °Ã!$°Ã¥n¹L©±¤a§ |±§Öì (© ¤\L1©a¹ P ¸ l»:í ªT¹L: \  Ö  ¹Lá¥&quot;¬ 
"P |L¥R«8·¤|$lr¤l\¥lrLß||¡§L´¹ ´²Ì§±°³¤l»à:\ar||®M¬Á¥n|¹ ´ ªn\ar¤|l·T´¨´:\¤  ®â¤o© \ì o© » ( Ü Ö $¤ | L¥R«8·¤\ o¤P¤\L Ö Ü ( |¹L\L®© ¸ ±°³¥7¢® r¤8¬Á¥n Ö ¼C·r¤!|©L¥T¤±´ ²·¤|±§P¤\¥P¤\\L¸ \¥Tl§i® LM°e|% ¥&quot;¬a¤\L·$°Ã¥n©L||\¤Ro©a¤â´®|/ ¥T© ®­/¨°Ã¥n©ão©§\ É ( ( Ï Ö \|\a§!\1o¤ ®­a¤\|M´ a¬Á¤.© !¤\¬Á¥nP¬Á¹L¤\®¦\/¹L ¥T¹L­Z|¯ ©a¦®/|¤Rl¡ª7o¤R¢®©±¤¥T©|lµ» ¸   ¤ LM§¤lT¡´Î: ©n´¡´  \¤ L¥T¹Lªnà¤\|/ ¥T|¥&quot;\Lq­Z/l É r|¤R¨°Ã¹ ´¡o \¥ã°Ã¥¢® ©±¤&quot;¢|lµZ¤r|P¤\L\L1½ ªT¹L|¹L\®8|1lr¤l|L¥R«ê¤.¥ \¥&quot; &quot;¬ ¤\L®\L| lr¤l:¥Ø ¯©L¥T¤·©Lª¹L¢°Ã¥n¹ ´¡§ &quot;´¡¸ ª&quot;¢ ¸8¹´ £¬Á¥n¯°eP  ªTL®8|¤Ro© &quot;| ´¡´¥T©±°³¤R¢©a¤\ |¹ °eÍo8¤\L ¼C ¸ » &quot;! 7# 0.4 %$ 7 %&amp; 0. 7 35 )(&apos; 0.3 * 7 25 +.-   ( 0. 7 0.2 7 0.15 7 +,.- /  0.1 7    6 0.05 7 0 7 8 9 : 0 400100 200 300 500 600     Î  ªT¹L\ P  &quot;¬L¤\|¹L|: X & lt;  Ò ;  ¤|\L¢o©n´ ²Zl  &quot;¥ ¬l|¹L|\®­a¤Ú§L l°Ã¹L||a§P © \ 1 r/!¥n©´\¹L\®. r|¤R¨°Ã¹ ´¡oßo| a°Ã¤ ¥&quot;¬¤ ¤  | |±· ©o¤R ¥T© µ7|±´¨r¤\a§·|\¤ ¹ §L ·o\ §á ©\ |\ë ìní ||| Å¡§©a¤R ½ °®r¤l ¥n©  ®¦®Rn´G§L |©a¤ ½ &quot;¬ © ¬Á¥nl ¥T©(|¤|R  ä ¤ü\L&lt;:Tí¡§©a¤R;» |:½ °®r¤l|| Ö¥a©Í¥|^¤ &quot;¥&quot;\¢¤|R¨°·|&quot;¬·\| ¤\|¹ ¤\ë §L¹§LáÄR¹o©±¤l¡¬Á²;°Ã¥n©ã¡°Eo±´¨µ°Ã®,\© \¤ ©LªL Û ç &quot;ä |¤×²/´ ±å¥&quot;¬X r|¤R¨°Ã¹ | &quot; ¸  ©o¤R ¥T© (( ¥ &quot;¬ |¤Ro¤R|¤R¡°®n´¢±o|¹L\|¹ °eào¢¤\L1| ¤ ®© °Ã(´ \ÉEªTlnÂ¸ ©¹´¨r¬Á¥nl|¥T© ë |l» Õ &lt;© | êì±°³©a¤Tí \  ¤\¹ §LlµZ«d¥Tl§ |¢¥T (© §L¡°o¤R°Ã¥|&quot;¤l´·²Å¬Á¥G°Ã¹L¢¥T©´ ²Í¹L|||±§àn´\·¥&quot;¬\|¥7»ÛÙ&quot;ln°Ã¤l¥ ´¡§i®lµ8§L%©LªC§L¥R«d®¦®RµÚ| l°Ãl¡|±§Í´¡·r©o¤|\¤ \®Ro¤|¥n\¤ |¹ \§L¹L| R = ] ( oR°E|¢â¦®l»\ ërÕ¬ ¤¢¸ Ú¥n ©´²·¢´|±°³©a¤R´aµá´¡ a§1¤:\|¹ \¸  ¥&quot;o¤Ú¤ \ão¤|¤^\| ¥&quot;¹ ¢¤¢¥T¤¢¥T¹L|\R¨°¥&quot;µ ü nüTí \,» ¥C°Ã¥na²al ¤|ªn±¤·\|LqL| ¹Llµ7¤\¥n\L¥n$|¥ ¸ a°Ã¤l ¦T¢Í. |±´\É¥ Û ç ( ¸§¤²Ì¤|±°Ã¤ã¡´¨´\ª&quot;|T´%\|l»LlÕ ©L©±¤l|\¤ ©LªHaT§Lµd¤©a¤\\®©±¤l ¥n©n´¡´¢£°Ã¥n¹_²Ï§L | ªT¹´¡§  |a§¸  ¸°Ã¥na²al¤|\¤ ªn±¤|±§â¤¸ ²&lt;\¥âL ©L­Z\®¦®©±¤Ú¹L©/l ©±¤|©a¤R|¥n©:\l» ¥7´¨r¤l ¥n©·¥&quot;¬ ë \üníæÍ|¹L±´¨r¤´¡¡°®r¤\a§|||arl°eP¤¢®©±¤\¥n !¡°%§¤ |±°Ã¤l ¥n©´¡¡°®r¤|¢®©±¤"
"M /§¤|±°Ã¤l¥n©»Ú:\¥n ¡° r ¸ ±°³¥7¢|/ ±°®½ °n´¡´²M¡¢É °Ã|¥Tar|¤Ro©±¤·|Ö¥&quot;©ã|¢®©±¤|M¥n©\,\¤ L \¥.Õ ©a¤\¤\L®$|­Z ´ ¥nl¦ ©LÉ ¤ ®­a¤|É ¸ r| ´¡¡°®r¤ní|Û§¤°o¤\®ªT¥nl|±°Ã¤lÇ®ß¤¥n©È¤\LÅ°Ã¥n©a¦®©±¤l\a°eL© ¡Ä®¹L®Í¥n©n´©a¤\¥ |²Í¤ãn´¡» \ë ü  \ (( \LÌ¬Á¥&quot;´¡´ ¥l«1 ©Lª&lt;^¤ «d¥Ì¤^²Z/ Í: ½ \|¤ã   ó"
Rÿ ó ¤þeù l÷nó ~ @ r òXùRõ ||®¤\¥&quot;¬Lä |   ©Lª7´ lå¨µi¤^²Z ¡°n´¡´ ²M°Ã¥n©LÉ  (    ¤RªT¹L¥T¹LM¤\®l¢lµâo\Ì°Ã¥&quot;¢ r| ´¡¡°®r¤|Í§É \a°Ã¤l¥n© ë é \¥G§â¤l»¡n´¡»¡µ ì  ç ¤üRòXÿnüTíönþ »ºï±ó :||M¤\Lß¤\®l 6§Lõ ð | ¤\ÿ önøl ¸¹L¤Rþ
"Øð¥n ©Íl¡$\²ZµG¤l»¡n´¡»¡µ·¨´¡rl9ö±ÿ¤×² ¤ ®|lµX|\¤ a·· ©LªÖo©§·|®©±¤|© °Ã.´ ®¦a´L|®ª7¢©a¤Ro¤R ¥T© «d| ´¡ a§Ìo¤$\¤ LßL\®ÉeL||l ©Lªã|\®l  aÄ®¹L© °Ã®ã´\ o©Ûl­Ì«d|ã/©¸ rT´¨|°´Ça§ã«1||¤ ||¤ lå | ±ÄR¹o°o¹L\á¬Á¹L|T´L«d±|\¤ LªTa¤\©a¤\®ªTlo¤¯¥n©Ì¤\\a§ÌL1­Z¤\©a¤\,\¥ ln°Ã¤´¡o||ªT®ß°´±§Öä$\¤ ®lË||¤ |aÄ®¹L©|l» é ®É°Ã ­Z¤·r¤l°e\ln°Ã¤|±§q¬Á|¥&quot;;¤\²ZÇÛ§LL ©a¤l| ±°³¤R´| \¤ ®­a¤P°Ã¥\&quot;´¡´¸±°³¤Rr|Û°´|¤||lå \L¥n\|  £¥&quot;\®­a¤|1|L¥T¹ ´¡§ ¸ á¬Á¹L|\¤ L¤´ ¥l«8l»Î: ©n´¡´:Î  \ | ¤lµâ¤\¹L\LÏ|¯|||| arl°eÈ§L¹LÌ¥&quot;| ±°³¤R ©LªØ¤\áoß¬Á¥\&quot;´ É ­Z ´¥n\ ¸ ±°®r¹L|Û¤\©a¤\®||¤lr¤l_¥n© ·\²Ü§É /©   n°Ã¤|¥T|P © °´ ¹§L ©Lª¯¤\ rªn±µ ¤\LÏ¢±§L¡nµ1¤\LÌ±§L ¤l ©LªÏ/¥7´¡¨°Ã²TµP¥ná¤\LÌ|¹ ¸ ±°³¤ ½ ±´¡§L»&apos;:\¥n ¥n|²Z¤R¡°®T´$¢¤| ´¡§ ¸ Ì(L\&quot;¥ · l ©LªÌ¤\¥X¥&quot;\¥à­Z ´ ¥n\à§L |©a¤ß¤^²Z/ &quot;¬&quot;¤\®­a¤|¹ T´7|| © °´ ¹§L ©Lª.¼C ¸ ¢®©±¤|lµ¥ ä :|a°Ã¥n©§¸ o|±§1§Lr¤l$/ ¥T¤|©a¤R¡n´±¸ r|®lµi¥n||arl°eL\¥nªTln(\¤ ¥nÛ|\L °Ã¥G§l»lr ¡§ ï çèñ §¤|±°Ã¤l¥n©ê¥&quot; r|¤R¨T´¡´ ²&gt; ´¡¡°®r¤|±§Í¤\®­a¤|Ìoq«d±´¨´ o·¤\L&lt;|¥7·o¤R¡°Pªn©Llr¤l ¥n©,¥&quot;¬:± ¸ ±§L§±§C¤\®­a¤ |P¹Ll ©LªÖ¤\LßL\¥n ¥n|a§&lt;°´ ||¤ l ©Lª,¢¤| :/ ±°®\½    ||\®¹L|l°Ã¥n©°Ã®\\o¤P°o©\L­Z¤¸ \ln°Ã¤l¹L¤R¡´¨Ça§á¬Á¹L&quot;¬¦T®©±¤|\¤ L|É | ©.| ¹ · Ça \L$°Ã¥n©a¤\®©±¤|&quot;¬a¤\L·°´ |¤|: |´¡o|¤Í¢©a¤Ro¤R|| ¥T©èo©§| ¥à|±ÄR¹; ©a¤\ \®Ö®| |&quot;¥¹ ´¡o¤R°eÈ¤¥n©¶¥\a°eL©&quot;¡Ä®¹L®Íoq\®l¢lµ¯o©§|É ®ª7|¥7·o¤R¡°ê§¤Õ |±°Ã¤l¤R ¥T© ¥n©è¥µâ ¤C&quot;¬ÖC¡¢±§L¡oÉe¢/¥T||¤lr©a¤M¤/ ±°® \½ ¥ê§¦T±´°Å­ZL\®¥n|É lo©§Ö°´¨r (© |¤\ L½ \¤ |®­a¤&lt;|l» rªnÉ ¸\a§ã§l°³Ro|±§Å¢¤|&quot;\ /±o ©Lª"
This paper presents an unsupervised learn-ing approach to building a non-English (Arabic) stemmer.
The stemming model is based on statistical machine translation and it uses an English stemmer and a small (10K sentences) parallel corpus as its sole training resources.
No parallel text is needed after the training phase.
"Mono-lingual, unannotated text can be used to further improve the stemmer by allow-ing it to adapt to a desired domain or genre."
"Examples and results will be given for Arabic , but the approach is applica-ble to any language that needs affix re-moval."
"Our resource-frugal approach re-sults in 87.5% agreement with a state of the art, proprietary Arabic stemmer built using rules, affix lists, and human anno-tated text, in addition to an unsupervised component."
"Task-based evaluation using Arabic information retrieval indicates an improvement of 22-38% in average pre-cision over unstemmed text, and 96% of the performance of the proprietary stem-mer above."
Stemming is the process of normalizing word vari-ations by removing prefixes and suffixes.
"From an information retrieval point of view, prefixes and suf-fixes add little or no additional meaning; in most cases, both the efficiency and effectiveness of text processing applications such as information retrieval and machine translation are improved."
"Building a rule-based stemmer for a new, arbitrary language is time consuming and requires experts with linguistic knowledge in that particular lan-guage."
"Supervised learning also requires large quan-tities of labeled data in the target language, and qual-ity declines when using completely unsupervised methods."
We would like to reach a compromise by using a few inexpensive and readily available re-sources in conjunction with unsupervised learning.
"Our goal is to develop a stemmer generator that is relatively language independent (to the extent that the language accepts stemming) and is trainable us-ing little, inexpensive data."
This paper presents an unsupervised learning approach to non-English stemming.
The stemming model is based on statisti-cal machine translation and it uses an English stem-mer and a small (10K sentences) parallel corpus as its sole training resources.
"A parallel corpus is a collection of sentence pairs with the same meaning but in different languages (i.e. United Nations proceedings, bilingual newspa-pers, the Bible)."
Table 1 shows an example that uses the Buckwalter transliterati[REF_CITE].
"Usually, entire documents are translated by humans, and the sentence pairs are subsequently aligned by automatic means."
"A small parallel corpus can be available when native speakers and translators are not, which makes building a stemmer out of such corpus a preferable direction."
We describe our approach towards reaching this goal in section 2.
"Although we are using resources other than monolingual data, the unsupervised na-ture of our approach is preserved by the fact that no direct information about non-English stemming is present in the training data."
"Monolingual, unannotated text in the target lan-guage is readily available and can be used to further improve the stemmer by allowing it to adapt to a de-sired domain or genre."
"This optional step is closer to the traditional unsupervised learning paradigm and is described in section 2.4, and its impact on stem-mer quality is described in 3.1.4."
Our approach (denoted by UNSUP in the rest of the paper) is evaluated in section 3.1 by compar-ing it to a proprietary Arabic stemmer (denoted by GOLD).
"The latter is a state of the art Arabic stem-mer, and was built using rules, suffix and prefix lists, and human annotated text."
"GOLD is an earlier ver-sion of the stemmer described in (Lee et al., )."
The task-based evaluation section 3.2 compares the two stemmers by using them as a preprocessing step in the TREC Arabic retrieval task.
This section also presents the improvement obtained over using unstemmed text.
"In this paper, Arabic was the target language but the approach is applicable to any language that needs affix removal."
"In Arabic, unlike English, both pre-fixes and suffixes need to be removed for effective stemming."
"Although Arabic provides the additional challenge of infixes, we did not tackle them because they often substantially change the meaning."
Irregu-lar morphology is also beyond the scope of this pa-per.
"As a side note for readers with linguistic back-ground (Arabic in particular), we do not claim that the resulting stems are units representing the entire paradigm of a lexical item."
The main purpose of stemming as seen in this paper is to conflate the to-ken space used in statistical methods in order to im-prove their effectiveness.
"The quality of the result-ing tokens as perceived by humans is not as impor-tant, since the stemmed output is intended for com-puter consumption."
The problem of unsupervised stemming or morphol-ogy has been studied using several different ap-proaches.
"For Arabic, good results have been ob-tained for plural detecti[REF_CITE].[REF_CITE]used a minimum description length paradigm to build Linguistica, a system for which the reported accuracy for European languages is cca. 83%."
"Note that the results in this section are not di-rectly comparable to ours, since we are focusing on Arabic."
"A notable contribution was published by Sn[REF_CITE], who defines an objective function to be optimized and performs a search for the stemmed configuration that optimizes the function over all stemming possibilities of a given text."
"Rule-based stemming for Arabic is a problem studied by many researchers; an excellent overview is provided by (Larkey et al., )."
Morphology is not limited to prefix and suffix re-moval; it can also be seen as mapping from a word to an arbitrary meaning carrying token.
"Using an LSI approach, (Schone and Jurafsky, ) obtained 88% ac-curacy for English."
"This approach also deals with irregular morphology, which we have not addressed."
"A parallel corpus has been successfully used be-fore[REF_CITE]to project part of speech tags, named entity tags, and morphology in-formation from one language to the other."
"For a par-allel corpus of comparable size with the one used in our results, the reported accuracy was 93% for French (when the English portion was also avail-able); however, this result only covers 90% of the tokens."
"Accuracy was later improved using suffix trees.[REF_CITE]used a parallel corpus for word sense disambiguation, exploiting the fact that different meanings of the same word tend to be translated into distinct words."
Our approach is based on the availability of the following three resources: • a small parallel corpus • an English stemmer • an optional unannotated Arabic corpus
Our goal is to train an Arabic stemmer using these resources.
The resulting stemmer will simply stem Arabic without needing its English equivalent.
We divide the training into two logical steps: • Step 1: Use the small parallel corpus • Step 2: (optional) Use the monolingual corpus
"The two steps are described in detail in the fol-lowing subsections. 



"
"In Step 1, we are trying to exploit the English stemmer by stemming the English half of the paral-lel corpus and building a translation model that will establish a correspondence between meaning carry-ing substrings (the stem) in Arabic and the English stems."
"For our purposes, a translation model is a matrix of translation probabilities p(Arabic stem| English stem) that can be constructed based on the small parallel corpus (see subsection 2.2 for more details)."
The Arabic portion is stemmed with an initial guess (discussed in subsection 2.1.1)
"Conceptually, once the translation model is built, we can stem the Arabic portion of the parallel corpus by scoring all possible stems that an Arabic word can have, and choosing the best one."
"Once the Ara-bic portion of the parallel corpus is stemmed, we can build a more accurate translation model and repeat the process (see figure 2)."
"However, in practice, in-stead of using a harsh cutoff and only keeping the best stem, we impose a probability distribution over the candidate stems."
The distribution starts out uni-form and then converges towards concentrating most of the probability mass in one stem candidate.
The starting point is an inherent problem for un-supervised learning.
We would like our stemmer to give good results starting from a very general initial guess (i.e. random).
"In our case, the starting point is the initial choice of the stem for each individual word."
We distinguish several solutions:
• No stemming.
"This is not a desirable starting point, since affix probabilities used by our model would be zero. • Random stemming As mentioned above, this is equivalent to im-posing a uniform prior distribution over the candidate stems."
"This is the most general start-ing point. • A simple language specific rule - if available If a simple rule is available, it would provide a better than random starting point, at the cost of reduced generality."
"For Arabic, this simple rule was to use Al as a prefix and p as a suffix."
This rule (or at least the first half) is obvious even to non-native speakers looking at transliterated text.
It also constitutes a surprisingly high base-line.
We adapted Model 1[REF_CITE]to our purposes.
Model 1 uses the concept of alignment between two sentences e and f in a parallel corpus; the alignment is defined as an object indicating for each word e i which word f j generated it.
"To ob-tain the probability of an foreign sentence f given the English sentence e, Model 1 sums the products of the translation probabilities over all possible align-ments:"
P r(f|e) ∼ X Y m t(f j |e a j ) {a} j=1
The alignment variable a i controls which English word the foreign word f i is aligned with. t(f|e) is simply the translation probability which is refined iteratively using EM.
"For our purposes, the transla-tion probabilities (in a translation matrix) are the fi-nal product of using the parallel corpus to train the translation model."
"To take into account the weight contributed by each stem, the model’s iterative phase was adapted to use the sum of the weights of a word in a sentence instead of the count."
"As previously mentioned, each word has a list of substrings that are possible stems."
We reduced the problem to that of placing two separators inside each Arabic word; the “candidate stems” are simply the substrings inside the separators.
"While this may seem inefficient, in practice words tend to be short, and one or two letter stems can be disallowed."
"An initial, naive approach when scoring the stem would be to simply look up its translation probabil-ity, given the English stem that is most likely to be its translation in the parallel sentence (i.e. the En-glish stem aligned with the Arabic stem candidate)."
Figure 3 presents scoring examples before normal-ization.
"However, this approach has several drawbacks that prevent us from using it on a corpus other than the training corpus."
Both of the drawbacks below are brought about by the small size of the parallel corpus: • Out-of-vocabulary words: many Arabic stems will not be seen in the small corpus • Unreliable translation probabilities for low-frequency stems.
"We can avoid these issues if we adopt an alternate view of stemming a word, by looking at the prefix and the suffix instead."
"Given the word, the choice of prefix and suffix uniquely determines the stem."
"Since the number of unique affixes is much smaller by definition, they will not have the two problems above, even when using a small corpus."
These prob-abilities will be considerably more reliable and are a very important part of the information extracted from the parallel corpus.
"Therefore, the score of a candidate stem should be based on the score of the corresponding prefix and the suffix, in addition to the score of the stem string itself: score(“pas 00 ) = f(p) × f(a) × f(s) where a = Arabic stem, p = prefix, s=suffix"
"When scoring the prefix and the suffix, we could simply use their probabilities from the previous stemming iteration."
"However, there is additional in-formation available that can be successfully used to condition and refine these probabilities (such as the length of the word, the part of speech tag if given etc.)."
"We explored several stem scoring models, using different levels of available information."
"Examples include: • Use the stem translation probability alone score = t(a|e) where a = Arabic stem, e = corresponding word in the English sentence • Also use prefix (p) and suffix (s) conditional probabilities; several examples are given in ta-ble 2."
"Probability con- Scoring Formula models successfully solve the problem of the empty prefix and suffix accumulating excessive probability, which would yield to a stemmer that never removed any affixes."
The results presented in the rest of the paper use the last scoring model.
This optional second step can adapt the trained stem-mer to the problem at hand.
"Here, we are moving away from providing the English equivalent, and we are relying on learned prefix, suffix and (to a lesser degree) stem probabilities."
"In a new domain or cor-pus, the second step allows the stemmer to learn new stems and update its statistical profile of the previ-ously seen stems."
"This step can be performed using monolingual Arabic data, with no annotation needed."
"Even though it is optional, this step is recommended since its sole resource can be the data we would need to stem anyway (see Figure 5)."
Step 1 produced a functional stemming model. ditioned on the candidate stem the length of the unstemmed Arabic word (len) the possible pre-fixes and/or suf-fixes the first and last letter
"We can use the corpus statistics gathered in Step 1 t(a|e) × p(p,s|a)+p(2s|a)×p(p|a) to stem the new, monolingual corpus."
"However, the t(a|e) × scoring model needs to be modified, since t(a|e) is p(p,s|len)+p(s|len)×p(p|len) no longer available."
"By removing the conditioning, 2 the first/last letter scoring model we used becomes t(a|e) × p(s|S possible ) × score = p(a) × p(s|last) × p(p|first) p(p|P possible )"
"The model can be updated if the stem candidate t(a|e)×p(s|last)×p(p|first) score/probability distribution is sufficiently skewed,"
"The first two examples use the joint probability of the prefix and suffix, with a smoothing back-off (the product of the individual probabilities)."
"Scor-ing models of this form proved to be poor perform-ers from the beginning, and they were abandoned in favor of the last model, which is a fast, good approx-imation to the third model in Table 2."
The last two and the monolingual text can be stemmed iteratively using the new model.
"The model is thus adapted to the particular needs of the new corpus; in practice, convergence is quick (less than 10 iterations)."
"For unsupervised training in Step 1, we used a small parallel corpus: 10,000 Arabic-English sentences from the United Nations(UN) corpus, where the En-glish part has been stemmed and the Arabic translit-erated."
"For unsupervised training in Step 2, we used a larger, Arabic only corpus: 80,000 different sen-tences in the same dataset."
"The test set consisted of 10,000 different sen-tences in the UN dataset; this is the testing set used below unless specified."
"We also used a larger corpus ( a year of Agence France Press (AFP) data, 237K sentences) for Step 2 training and testing, in order to gauge the robustness and adaptation capability of the stemmer."
"Since the UN corpus contains legal proceedings, and the AFP corpus contains news stories, the two can be seen as coming from different domains."
In this subsection the accuracy is defined as agree-ment with GOLD.
"GOLD is a state of the art, pro-prietary Arabic stemmer built using rules, suffix and prefix lists, and human annotated text, in addition to an unsupervised component."
"GOLD is an ear-lier version of the stemmer described in (Lee et al., )."
Freely available (but less accurate) Arabic light stemmers are also used in practice.
"When measuring accuracy, all tokens are consid-ered, including those that cannot be stemmed by simple affix removal (irregulars, infixes)."
"Note that our baseline (removing Al and p, leaving everything unchanged) is higher that simply leaving all tokens unchanged."
"For a more relevant task-based evaluation, please refer to Subsection 3.2."
We begin by examining the effect that the size of the parallel corpus has on the results after the first step.
"Here, we trained our stemmer on three dif-ferent corpus sizes: 50K, 10K, and 2K sentences."
The high baseline is obtained by treating Al and p as affixes.
The 2K corpus had acceptable results (if this is all the data available).
Note that different languages might have different corpus size needs.
All other results
"Although severely handicapped at the beginning, the knowledge-free starting point manages to narrow the performance gap after a few iterations."
Knowing the Al+p rule still helps at this stage.
"However, the performance gap is narrowed further in Step 2 (see figure 8), where the knowledge free starting point benefitted from the monolingual training."
Figure 8 shows the results obtained when aug-menting the stemmer trained in Step 1.
"Two dif-ferent monolingual corpora are used: one from the same domain as the test set (80K UN), and one from a different domain/corpus, but three times larger (237K AFP)."
"The larger dataset seems to be more useful in improving the stemmer, even though the domain was different."
The baseline and the accuracy after Step 1 are pre-sented for reference.
"We used an additional test set that consisted of 10K sentences taken from AFP, instead of UN as in previous experiments shown in figure 8 ."
Its pur-pose was to test the cross-domain robustness of the stemmer and to further examine the importance of applying the second step to the data needing to be stemmed.
"Figure 9 shows that, even though in Step 1 the stemmer was trained on UN proceedings, the re-sults on the cross-domain (AFP) test set are compa-rable to those from the same domain (UN, figure 8)."
"However, for this particular test set the baseline was much higher; thus the relative improvement with re-spect to the baseline is not as high as when the unsu-pervised training and testing set came from the same collection."
This paper presents an unsupervised learning ap-proach to building a non-English (Arabic) stemmer using a small sentence-aligned parallel corpus in which the English part has been stemmed.
No paral-lel text is needed to use the stemmer.
"Monolingual, unannotated text can be used to further improve the stemmer by allowing it to adapt to a desired domain or genre."
"The approach is applicable to any language that needs affix removal; for Arabic, our approach results in 87.5% agreement with a proprietary Ara-bic stemmer built using rules, affix lists, and hu-man annotated text, in addition to an unsupervised component."
"Task-based evaluation using Arabic in-formation retrieval indicates an improvement of 22- 38% in average precision over unstemmed text, and 93-96% of the performance of the state of the art, language specific stemmer above."
"We can speculate that, because of the statistical nature of the unsupervised stemmer, it tends to fo-cus on the same kind of meaning units that are sig-nificant for IR, whether or not they are linguistically correct."
This could explain why the gap betheen GOLD and UNSUP is narrowed with task-based evaluation and is a desirable effect when the stem-mer is to be used for IR tasks.
"We are planning to experiment with different lan-guages, translation model alternatives, and to extend task-based evaluation to different tasks such as ma-chine translation and cross-lingual topic detection and tracking."
We would like to thank the reviewers for their help-ful observations and for identifying Arabic mis-spellings.
This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract
This research is also spon- sored in part by the National Science Foundation (NSF) under grants[REF_CITE]and in part by the DoD under award 114008-[REF_CITE].
"However, any opinions, views, conclusions and findings in this paper are those of the authors and do not necessarily reflect the posi-tion of policy of the Government and no official en-dorsement should be inferred."
We approximate Arabic’s rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme).
Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus.
The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input.
"The language model is initially estimated from a small manually segmented corpus of about 110,000 words."
"To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus."
"The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens."
We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.
"Morphologically rich languages like Arabic present significant challenges to many natural language processing applications because a word often conveys complex meanings decomposable into several morphemes (i.e. prefix, stem, suffix)."
"By segmenting words into morphemes, we can improve the performance of natural language systems including machine translati[REF_CITE]and information retrieval (Franz, M. and McCarley, S. 2002)."
"In this paper, we present a general word segmentation algorithm for handling inflectional morphology capable of segmenting a word into a prefix*-stem-suffix* sequence, using a small manually segmented corpus and a table of prefixes/suffixes of the language."
We do not address Arabic infix morphology where many stems correspond to the same root with various infix variations; we treat all the stems of a common root as separate atomic units.
The use of a stem as a morpheme (unit of meaning) is better suited than the use of a root for the applications we are considering in information retrieval and machine translation (e.g. different stems of the same root translate into different English words.)
"Examples of Arabic words and their segmentation into prefix*-stem-suffix* are given in Table 1, where &apos;#&apos; indicates a morpheme being a prefix, and &apos;+&apos; a suffix. [Footnote_1] As shown in Table 1, a word may include multiple prefixes, as in ﻞ ﻟ (l: for, Al: the), or multiple suffixes, as in ﻪ ﺗ (t: feminine singular, h: his)."
"1 Arabic is presented in both native and Buckwalter transliterated Arabic whenever possible. All native Arabic is to be read from right-to-left, and transliterated Arabic is to be read from left-to-right. The convention of"
"A word may also consist only of a stem, as in ﻰ ﻟا (AlY, to/towards)."
"The algorithm implementation involves (i) language model training on a morpheme-segmented corpus, (ii) segmentation of input text into a sequence of morphemes using the language model parameters, and (iii) unsupervised acquisition of new stems from a large unsegmented corpus."
"The only linguistic resources required include a small manually segmented corpus ranging from 20,000 words to 100,000 words, a table of prefixes and suffixes of the language and a large unsegmented corpus."
"In Section 2, we discuss related work."
"In Section 3, we describe the segmentation algorithm."
"In Section 4, we discuss the unsupervised algorithm for new stem acquisition."
"In Section 5, we present experimental results."
"In Section 6, we summarize the paper."
Our work adopts major components of the algorithm from (Luo &amp;[REF_CITE]): language model (LM) parameter estimation from a segmented corpus and input segmentation on the basis of LM probabilities.
"However, our work diverges from their work in two crucial respects: (i) new technique of computing all possible segmentations of a word into prefix*-stem-suffix* for decoding, and (ii) unsupervised algorithm for new stem acquisition based on a stem candidate&apos;s similarity to stems occurring in the training corpus.[REF_CITE]presents a supervised technique which identifies the root of an Arabic word by stripping away the prefix and the suffix of the word on the basis of manually acquired dictionary of word-root pairs and the likelihood that a prefix and a suffix would occur with the template from which the root is derived."
"His technique pre-supposes at most one prefix and one suffix per stem regardless of the actual number and meanings of prefixes/suffixes associated with the stem.[REF_CITE]presents a finite-state morphological analyzer for Arabic, which displays the root, pattern, and prefixes/suffixes."
The analyses are based on manually acquired lexicons and rules.
"Although his analyzer is comprehensive in the types of knowledge it presents, it has been criticized for their extensive development time and lack of robustness, cf.[REF_CITE].[REF_CITE]presents a minimally supervised morphological analysis with a performance of over 99.2% accuracy for the 3,888 past-tense test cases in English."
The core algorithm lies in the estimation of a probabilistic alignment between inflected forms and root forms.
"The probability estimation is based on the lemma alignment by frequency ratio similarity among different inflectional forms derived from the same lemma, given a table of inflectional parts-of-speech, a list of the canonical suffixes for each part of speech, and a list of the candidate noun, verb and adjective roots of the language."
"Their algorithm does not handle multiple affixes per word.[REF_CITE]presents an unsupervised technique based on the expectation-maximization algorithm and minimum description length to segment exactly one suffix per word, resulting in an F-score of 81.8 for suffix identification in English according[REF_CITE].[REF_CITE]proposes an unsupervised algorithm capable of automatically inducing the morphology of inflectional languages using only text corpora."
"Their algorithm combines cues from orthography, semantics, and contextual information to induce morphological relationships in German, Dutch, and English, among others."
They report F-scores between 85 and 93 for suffix analyses and between 78 and 85 for circumfix analyses in these languages.
"Although their algorithm captures orprefix-suffix combinations circumfixes, it does not handle the multiple affixes per word we observe in Arabic."
"Given an Arabic sentence, we use a trigram language model on morphemes to segment it into a sequence of morphemes {m 1 , m [Footnote_2], …, m n }."
2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC ([URL_CITE]We divided this corpus into training and the development test sets as described in Section 5.
"The input to the morpheme segmenter is a sequence of Arabic tokens – we use a tokenizer that looks only at white space and other punctuation, e.g. quotation marks, parentheses, period, comma, etc."
A sample of a manually segmented corpus is given below 2 .
Here multiple occurrences of prefixes and suffixes per word are marked with an underline. ﺰآﺮﻣ #لا ﰲ ﻞﺣ يﺬﻟا ﻦﻳﺎﻓﺮﻳا نﺎآ #و مﺎﻋ #لا ﺎﺴﳕ #لا ة+ ﺰﺋﺎﺟ ﰲ لوا #لا #ب ﺮﻌﺷ يراﲑﻓ ة+ رﺎﻴﺳ ﻲﻠﻋ ﻲﺿﺎﻣ #لا #لا ﱄا ﻩ+ ت+ ﺮﻄﺿا ﻩ+ ﻦﻄﺑ ﰲ مﻻا دﻮﻋ #ي #س ﻮه # و برﺎﲡ #لا ﻦﻣ بﺎﺤﺴﻧا #لا تا+ صﻮﺤﻓ #لا ءاﺮﺟا #ل نﺪﻨﻟ ﱄا . راﻮﻏﺎﺟ ﻖﻳﺮﻓ رﺎﺷا ﺎﻣ ﺐﺴﺣ ة+ يروﺮﺿ راﻮﻏﺎﺟ ﰲ برﺎﲡ #لا ﻖﺋﺎﺳ ﻞﺣ #ي #س #و نﺎﻜﻣ ﻲﺗرﻮﺑ ﻮﻧﺎﻴﺳﻮﻟ ﻲﻠﻳزاﺮﺑ #لا ﺪﺣا #لا اﺪﻏ قﺎﺒﺳ #لا ﰲ ﻦﻳﺎﻓﺮﻳا ﰲ ﻩ+ تا+ ﻮﻄﺧ ﱄوا نﻮآ #ي #س يﺬﻟا ﻻﻮﻣرﻮﻔﻟا تا+ قﺎﺒﺳ ﱂﺎﻋ w# kAn
AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA
Al# EAm Al# mADy
Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al # AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA&apos; Al# fHwS +At Al# Drwry +p Hsb mA
A$Ar fryq jAgwAr. w# s# y #
Hl sA}q Al # tjArb fy jAgwAr Al# brAzyly lwsyAnw bwrty mkAn AyrfAyn fy Al # sbAq gdA Al# AHd Al*y s# y# kwn
Awly xTw +
At +h fy
EAlm sbAq +At AlfwrmwlA Many instances of prefixes and suffixes in Arabic are meaning bearing and correspond to a word in English such as pronouns and prepositions.
"Therefore, we choose a segmentation into multiple prefixes and suffixes."
"Segmentation into one prefix and one suffix per word, cf.[REF_CITE], is not very useful for applications like statistical machine translation,[REF_CITE], for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations."
"The trigram language model probabilities of morpheme sequences, p(m i |m i-1, m i-2 ), are estimated from the morpheme-segmented corpus."
"At token boundaries, the morphemes from previous tokens constitute the histories of the current morpheme in the trigram language model."
"The trigram model is smoothed using deleted interpolation with the bigram and unigram models,[REF_CITE], as in (1): (1) p(m 3 | m 1 ,m 2 ) = λ 3 p(m 3 |m 1 ,m 2 ) + λ 2 p(m 3 |m 2 ) + λ 3 p(m 3 ), where λ 1 + λ 2 + λ 3 = 1."
A small morpheme-segmented corpus results in a relatively high out of vocabulary rate for the stems.
We describe below an unsupervised acquisition of new stems from a large unsegmented Arabic corpus.
"However, we first describe the segmentation algorithm."
We take the unit of decoding to be a sentence that has been tokenized using white space and punctuation.
"The task of a decoder is to find the morpheme sequence which maximizes the trigram probability of the input sentence, as in (2): (2) SEGMENTATION best = Argmax II i=1 , N p(m i |m i-1 m i-2 ), N = number of morphemes in the input."
Search algorithm for (2) is informally described for each word token as follows:
Step 1: Compute all possible segmentations of the token (to be elaborated in 3.2.1).
Step 2: Compute the trigram language model score of each segmentation.
"For some segmentations of a token, the stem may be an out of vocabulary item."
"In that case, we use an “ UNKNOWN ” class in the trigram language model with the model probability given by p( UNKNOWN |m i-1, m i-2 ) * UNK_Fraction , where UNK_Fraction is 1e-9 determined on empirical grounds."
"This allows us to segment new words with a high accuracy even with a relatively high number of unknown stems in the language model vocabulary, cf. experimental results in Tables 5 &amp; 6."
Step 3: Keep the top N highest scored segmentations.
Possible segmentations of a word token are restricted to those derivable from a table of prefixes and suffixes of the language for decoder speed-up and improved accuracy.
"Table 2 shows examples of atomic (e.g. لا, تا) and multi-component (e.g. لﺎ ﺑو, ﺎ ﻬﺗا ) prefixes and suffixes, along with their component morphemes in native Arabic. 3"
"Each token is assumed to have the structure prefix*-stem-suffix*, and is compared against the prefix/suffix table for segmentation."
"Given a word token, (i) identify all of the matching prefixes and suffixes from the table, (ii) further segment each matching prefix/suffix at each character position, and (iii) enumerate all prefix*-stem-suffix* sequences derivable from (i) and (ii)."
"Table 3 shows all of its possible segmentations of the token ﺎهرﺮآاو (wAkrrhA; &apos;and I repeat it&apos;), [Footnote_4] where ∅ indicates the null prefix/suffix and the Seg Score is the language model probabilities of each segmentation S1 ..."
4 A sentence in which the token occurs is as follows: ﺎﻬﺘﻠﻗ ﺔﻴﻄﻔﻨﻟا تﺎﻘﺘﺸﻤﻟا ﻲﻓ ﺎﻤﻧاو مﺎﺨﻟا ﻂﻔﻨﻟا ﻲﻓ ﺖﺴﻴﻟ ﺔﻠﻜﺸﻤﻟﺎﻓ ﺎهرﺮآاو (qlthA wAkrrhA fAlm$klp lyst fy AlfnT AlxAm wAnmA fy Alm$tqAt AlnfTyp.)
"For this token, there are two matching prefixes #و (w#) and #او( wA#) from the prefix table, and two matching suffixes ا+( +A) and ﺎه+( +hA) from the suffix table."
"S1, S2, &amp; S[Footnote_3] are the segmentations given the null prefix ∅ and suffixes ∅, +A, +hA. S4, S5, &amp; S6 are the segmentations given the prefix w# and suffixes ∅, +A, +hA. S7, S8, &amp;"
"3 We have acquired the prefix/suffix table from a 110K word manually segmented LDC corpus (51 prefixes &amp; 72 suffixes) and from IBM-Egypt (additional 14 prefixes &amp; 122 suffixes). The performance improvement by the additional prefix/suffix list ranges from 0.07% to 0.54% according to the manually segmented training corpus size. The smaller the manually segmented corpus size is, the bigger the performance improvement by adding additional prefix/suffix list is."
"S9 are the segmentations given the prefix wA# and suffixes ∅, +A, +hA. S10, S11, &amp; S12 are the segmentations given the prefix sequence w# A# derived from the prefix wA# and suffixes ∅, +A, +hA. As illustrated by S12, derivation of sub-segmentations of the matching prefixes/suffixes enables the system to identify possible segmentations which would have been missed otherwise."
"In this case, segmentation including the derived prefix sequence ﺎه+ رﺮآ #ا #و (w# A# krr +hA) happens to be the correct one."
"While the number of possible segmentations is maximized by sub-segmenting matching prefixes and suffixes, some of illegitimate sub-segmentations are filtered out on the basis of the knowledge specific to the manually segmented corpus."
"For instance, sub-segmentation of the suffix hA into +h +A is ruled out because there is no suffix sequence +h +A in the training corpus."
"Likewise, sub-segmentation of the prefix Al into A# l# is filtered out. improbableFiltering out prefix/suffix sequences improves the segmentation accuracy, as shown in Table 5."
"Once the seed segmenter is developed on the basis of a manually segmented corpus, the performance may be improved by iteratively expanding the stem vocabulary and retraining the language model on a large automatically segmented Arabic corpus."
"Given a small manually segmented corpus and a large unsegmented corpus, segmenter development proceeds as follows."
"Initialization: Develop the seed segmenter Segmenter 0 trained on the manually segmented corpus Corpus 0 , using the language model vocabulary, Vocab 0, acquired from Corpus 0 ."
"For i = 1 to N, N = the number of partitions of the unsegmented corpus i. Use Segmenter i-1 to segment Corpus i . ii."
Acquire new stems from the newly segmented Corpus i .
Add the new stems to
"Vocab i-1 , creating an expanded vocabulary Vocab i . iii."
Develop Segmenter i trained on Corpus 0 through Corpus i with Vocab i .
"Optimal Performance Identification: Identify the Corpus i and Vocab i , which result in the best performance, i.e. system training with Corpus i+1 and Vocab i+1 does not improve the performance any more."
"Unsupervised acquisition of new stems from an automatically segmented new corpus is a three-step process: (i) select new stem candidates on the basis of a frequency threshold, (ii) filter out new stem candidates containing a sub-string with a high likelihood of being a prefix, suffix, or prefix-suffix."
"The likelihood of a sub-string being a prefix, suffix, and prefix-suffix of a token is computed as in (5) to (7), (iii) further filter out new stem candidates on the basis of contextual information, as in (8). (5) P score = number of tokens with prefix P / number of tokens starting with sub-string P (6) S score = number of tokens with suffix S / number of tokens ending with sub-string S (7) PS score = number of tokens with prefix P and suffix S / number of tokens starting with sub-string P and ending with sub-string S"
"Stem candidates containing a sub-string with a high prefix, suffix, or prefix-suffix likelihood are filtered out."
"Example sub-strings with the prefix, suffix, prefix-suffix likelihood 0.85 or higher in a 110K word manually segmented corpus are given in Table 4."
"If a token starts with the sub-string ـﻨﺱ (sn), and end with ﺎﻬـ (hA), the sub-string&apos;s likelihood of being the prefix-suffix of the token is 1."
"If a token starts with the sub-string ﻞ ﻟ (ll), the sub-string&apos;s likelihood of being the prefix of the token is 0.945, etc. (8) Contextual Filter: (i)"
Filter out stems co-occurring with prefixes/suffixes not present in the training corpus. (ii) Filter out stems whose prefix/suffix distributions are highly disproportionate to those seen in the training corpus.
"According to (8), if a stem is followed by a potential suffix +m, not present in the training corpus, then it is filtered out as an illegitimate stem."
"In addition, if a stem is preceded by a prefix and/or followed by a suffix with a significantly higher proportion than that observed in the training corpus, it is filtered out."
"For instance, the probability for the suffix +A to follow a stem is less than 50% in the training corpus regardless of the stem properties, and therefore, if a candidate stem is followed by +A with the probability of over 70%, e.g. mAnyl +A, then it is filtered out as an illegitimate stem."
"We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size."
Segmentation error rate is defined in (9). (9) (number of incorrectly segmented tokens / total number of tokens) x 100
"Evaluations have been performed on a development test corpus containing 28,449 word tokens."
The test set is extracted from 20001115_AFP_ARB.0060.xml.txt through 20001115_AFP_ARB.0236.xml.txt of the LDC Arabic Treebank: Part 1 v 2.0 Corpus.
"Impact of the core algorithm and the unsupervised stem acquisition has been measured on segmenters developed from 4 different sizes of manually segmented seed corpora: 10K, 20K, 40K, and 110K words."
The experimental results are shown in Table 5.
The baseline performances are obtained by assigning each token the most frequently occurring segmentation in the manually segmented training corpus.
The column headed by &apos;3-gram LM&apos; indicates the impact of the segmenter using only trigram language model probabilities for decoding.
"Regardless of the manually segmented training corpus size, use of trigram language model probabilities reduces the word error rate of the corresponding baseline by approximately 50%."
The column headed by &apos;3-gram LM + PS Filter&apos; indicates the impact of the core algorithm plus Prefix-Suffix Filter discussed in Section 3.2.2.
Prefix-Suffix Filter reduces the word error rate ranging from 7.4% for the smallest (10K word) manually segmented corpus to 21.8% for the largest (110K word) manually segmented corpus - around 1% absolute reduction for all segmenters.
The column headed by &apos;3-gram LM + PS Filter + New Stems&apos; shows the impact of unsupervised stem acquisition from a 155 million word Arabic corpus.
Word error rate reduction due to the unsupervised stem acquisition is 38% for the segmenter developed from the 10K word manually segmented corpus and 32% for the segmenter developed from 110K word manually segmented corpus.
Language model vocabulary size (LM VOC Size) and the unknown stem ratio (OOV ratio) of various segmenters is given in Table 6.
"For unsupervised stem acquisition, we have set the frequency threshold at 10 for every 10-15 million word corpus, i.e. any new morphemes occurring more than 10 times in a 10-15 million word corpus are considered to be new stem candidates."
"Prefix, suffix, prefix-suffix likelihood score to further filter out illegitimate stem candidates was set at 0.[Footnote_5] for the segmenters developed from 10K, 20K, and 40K manually segmented corpora, whereas it was set at 0.85 for the segmenters developed from a 110K manually segmented corpus."
"5 Without the Contextual Filter, the error rate of the same segmenter is 3.1%."
"Both the frequency threshold and the optimal prefix, suffix, prefix-suffix likelihood scores were determined on empirical grounds."
Contextual Filter stated in (8) has been applied only to the segmenter developed from 110K manually corpus. 5 segmented training Comparison of Tables 5 and 6 indicates a high correlation between the segmentation error rate and the unknown stem ratio.
"Table 7 gives the error analyses of four segmenters according to three factors: (i) errors due to unknown stems, (ii) errors involving مﻮﻴ ﻟا (Alywm), and (iii) errors due to other factors."
"Interestingly, the segmenter developed from a 110K manually segmented corpus has the lowest percentage of “unknown stem” errors at 39.6% indicating that our unsupervised acquisition of new stems is working well, as well as suggesting to use a larger unsegmented corpus for unsupervised stem acquisition. مﻮﻴ ﻟا (Alywm) should be segmented differently depending on its part-of-speech to capture the semantic ambiguities."
"If it is an adverb or a proper noun, it is segmented as مﻮﻴ ﻟا &apos;today/Al-Youm&apos;, whereas if it is a noun, it is segmented as مﻮ ﻳ #لا &apos;the day.&apos; Proper segmentation of مﻮﻴ ﻟا primarily requires its part-of-speech information, and cannot be easily handled by morpheme trigram models alone."
Other errors include over-segmentation of foreign words such as ﻦ ﻴ ﺗﻮ ﺑ (bwtyn) as ب# ﻦ ﻴ ﺗو and ﺮ ﺘ ﻴ ﻟ (lytr) &apos;litre&apos; as ﺮ ﺗ #ي #ل.
These errors are attributed to the segmentation ambiguities of these tokens: ﻦ ﻴ ﺗﻮ ﺑ is ambiguous between &apos;ﻦ ﻴ ﺗﻮ ﺑ (Putin)&apos; and &apos;ب# ﻦ ﻴ ﺗو (by aorta)&apos;. ﺮ ﺘ ﻴ ﻟ is ambiguous between &apos;ﺮ ﺘ ﻴ ﻟ (litre)&apos; and &apos;ﺮ ﺗ #ي #ل (for him to harm)&apos;.
These errors may also be corrected by incorporating part-of-speech information for disambiguation.
"To address the segmentation ambiguity problem, as illustrated by &apos;ﻦ ﻴ ﺗﻮ ﺑ (Putin)&apos; vs. &apos;ﻦ ﻴ ﺗو #ب (by aorta)&apos;, we have developed a joint model for segmentation and part-of-speech tagging for which the best segmentation of an input sentence is obtained according to the formula (10), where t i is the part-of-speech of morpheme m i , and N is the number of morphemes in the input sentence. (10) SEGMENTATION best ="
"Argmax Π i=1,N p(m i |m i-1 m i-2 ) p(t i |t i-1 t i-2 ) p(m i |t i )"
"By using the joint model, the segmentation word error rate of the best performing segmenter has been reduced by about 10% from 2.9% (cf. the last column of Table 5) to 2.6%."
"We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results."
"Our Arabic word segmentation system implementing the algorithm achieves around 97% segmentation accuracy on a development test corpus containing 28,449 word tokens."
"Since the algorithm can identify any number of prefixes and suffixes of a given token, it is generally applicable to various language families including agglutinative languages (Korean, Turkish, Finnish), highly inflected languages (Russian, Czech) as well as semitic languages (Arabic, Hebrew)."
"Our future work includes (i) application of the current technique to other highly inflected languages, (ii) application of the unsupervised stem acquisition technique on about 1 billion word unsegmented Arabic corpus, and (iii) adoption of a novel morphological analysis technique to handle irregular morphology, as realized in Arabic broken plurals بﺎﺘ آ (ktAb) &apos;book&apos; vs. ﺐ ﺘ آ (ktb) &apos;books&apos;."
]ÊdºË¶º[[¼ µ´&amp;ºp½µ½¾m¸ÁÃjÌ[ [º Å·»¼[¿&apos;·»Êjº []Àd¿ÏÌ ÐÇ³&apos;Ðm³&apos;¿&apos;º]´Ë³+[[¼ µ´&amp; ¼]ÌÑº]_Ã ³&apos;·ÁÀmÂj¸»Ð&quot;·Á´Ç½µÌÑ¿ÏÃjÀd¿Ç¿&apos;º[[¼  !&amp;´ [º ¿ÏÉmµmÅ³@[[º ÅÕÌ º ÖnÀmº]´Ëº]Àj¿]Í ¿    [º ³Ï½º[¼[]Ì ³&apos;µÀÃ_¸»·»Ù] [º ³Ï½º[¼[º ÈÜÌ [º Âj·»³&apos;¿&apos;º_!× ]
ÌÑº¿&apos;µ[.@Å _Ã ·ÁÀ[¼@Ú¾m·ÁÌ¿ÏÉmº&amp;ºÆÊjµ¼³&apos;Ðm³&apos;¿&apos;º]Ãd¶dÈ]´ Å·»¼[¿&apos;·»µÀÃdÌ ÐjÒgÝaÀÞ¿ÏÉm·»³V½Ãd½º][Ì ] ½µÌ ¿ µmÀpÃjÀÆÃj¾m¿&apos;µm´ÇÃÃu¸¹¸»ÐÇµm¶m¿ÏÃ_·ÁÀm³%_¿&apos;·»¼0´]&amp;)Ì [º ³ ¿ÏÉmµmÅ¿ÏÉÃ½º[¼[·ßÖ3¼0Êjµm¼_¿%]ÅÐÀÃj´Ãj¶¾m¸ÁÃjÌ&amp;·ßÈÐ àaÌ]¼ã Émº]À@]]Ì0´ÇÃjäÌ[áâ³%¾ÀÃjÀÃ_[º _¸»ÐÙ[º][Àd¿ÏÌ ÐjÍ5¿ÏÉmº]¾´&amp;&amp;]º Àd¿&apos;³³&apos;Ð³ÑÈ]Ò ³ ]´ ÅÐÀÃj´&amp;[Ém¾Àäj³LàaÌ·»¼]_Ã ¸»¸»Ðåº[Îm¿ÏÌ&apos;Ãµ´ _ [¼ ¿ÏÉmºO¾m³&apos;º&quot;¿ÏÉmºO¼]ÌL¿&apos;º[µÌ&apos;ÌÑº[Îm¿È ÃdÀmÅæ³Ï¾mÂdÂjº[³&apos;¿&apos;³¿ÏÉmº]´çÃ_
Å³ ³ ¾mÂjÂjº[³&apos;¿&apos;º[.Å [ ÐjÒ ã ·»¿ÏÉµ¾Ì ´Ëº[¿ÏÉmµÅ Íé¿&apos;º[&quot;ÃY½ÃjÌ ¿&apos;·»¼]¾m¸ÁÃjÌ&amp;³&apos;¿ Ðm¸»ºµÌ [µmÀm¼[]º [[¼ ]%º]ÀdÈ¼ ]Ì [º &amp;½Ì [º Å·»¼[¿&apos;·»Êjº0¿&apos;º[]Àj¿ Ì &amp;Ð ³&apos;Ð³ÑÈ ]´Ò ã &amp;º Êjº]Ì ·ßÖ3º[ÅL¿ÏÉÃ_
ÂjºÆÃj´&amp;µ¾Àd¿  Å³pÀmµj¿pÌÑº[Âj·»³&apos;¿&apos;º]Ì º[ÅF·ÁÀè¿ÏÉmº.Å·»¼[¿&apos;·»µ_È ÀÃdÌ Ð&amp;¼]ÃjÀp¶º+]º
Àd¿&apos;º]Ì [º Å¾m³&apos;·ÁÀmÂ&amp;µ¾Ì%´Ëº[¿ÏÉmµÅ Ò
An investment of effort over the last two years has begun to produce a wealth of data concerning computational psycholin-guistic models of syntax acquisition.
"The data is generated by running simulations on a recently completed database of word order patterns from over 3,000 abstract languages."
"This article presents the design of the database which contains sentence patterns, grammars and derivations that can be used to test acquisition models from widely divergent paradigms."
The domain is generated from grammars that are lin-guistically motivated by current syntactic theory and the sentence patterns have been validated as psychologically/developmen-tally plausible by checking their frequency of occurrence in corpora of child-directed speech.
A small case-study simulation is also presented.
The exact process by which a child acquires the grammar of his or her native language is one of the most beguiling open problems of cognitive science.
There has been recent interest in computer simulation of the acquisition process and the interrelationship between such models and linguis-tic and psycholinguistic theory.
"The hope is that through computational study, certain bounds can be established which may be brought to bear on pivotal issues in developmental psycholinguistics."
"Simulation research is a significant departure from standard learnability models that provide results through formal proof (e.g.,[REF_CITE];"
"Although research in learnability theory is valuable and ongoing, there are several disadvantages to formal modeling of language acquisition: • Certain proofs may involve impractically many steps for large language domains (e.g. those involving Markov methods). • Certain paradigms are too complex to readily lend themselves to deductive study (e.g. con-nectionist models). [Footnote_1] • Simulations provide data on intermediate stages whereas formal proofs typically prove whether a domain is (or more often is not) learnable a priori to specific trials. • Proofs generally require simplifying assump-tions which are often distant from natural lan-guage."
1 Although see[REF_CITE]for some insight.
"However, simulation studies are not without disadvantages and limitations."
"Most notable perhaps, is that out of practicality, simulations are typically carried out on small, severely circum-scribed domains – usually just large enough to allow the researcher to hone in on how a particular model (e.g. a connectionist network or a principles &amp; parameters learner) handles a few grammatical features (e.g. long-distance agreement and/or topicalization) often, though not always, in a single language."
"So although there have been many successful studies that demonstrate how one algorithm or another is able to acquire some aspect of grammatical structure, there is little doubt that the question of what mechanism children actually employ during the acquisition process is still open."
"This paper reports the development of a large, multilingual database of sentence patterns, gram- mars and derivations that may be used to test computational models of syntax acquisition from widely divergent paradigms."
The domain is generated from grammars that are linguistically motivated by current syntactic theory and the sentence patterns have been validated as psycho-logically/developmentally plausible by checking their frequency of occurrence in corpora of child-directed speech.
"We report here the structure of the domain, its interface and a case-study that demon-strates how the domain has been used to test the feasibility of several different acquisition strate-gies."
The domain is currently publicly available on the web via[URL_CITE]and it is our hope that it will prove to be a valuable resource for investigators interested in computational models of natural language acquisition.
"The focus of the language domain database, (hereafter LDD), is to make readily available the different word order patterns that children are typically exposed to, together with all possible syntactic derivations of each pattern."
The patterns and their derivations are generated from a large battery of grammars that incorporate many features from the domain of natural language.
"At this point the multilingual language domain contains sentence patterns and their derivations generated from 3,072 abstract grammars."
"The patterns encode sentences in terms of tokens denoting the grammatical roles of words and complex phrases, e.g., subject (S), direct object (O1), indirect object (O2), main verb (V), auxiliary verb (Aux), adverb (Adv), preposition (P), etc."
An example pattern is S Aux V O1 which corresponds to the English sentence: The little girl can make a paper airplane.
There are also tokens for topic and question markers for use when a grammar specifies overt topicalization or question marking.
"Declarative sentences, imperative sentences, negations and questions are represented within the LDD, as is prepositional movement/stranding (pied-piping), null subjects, null topics, topicaliza-tion and several types of movement."
"Although more work needs to be done, a first round study of actual child-directed sentences from the CHILDES corpus[REF_CITE]indicates that our patterns capture many sentential word orders that children typically encounter in the period from 1-1/2 to 2-1/2 years; the period generally accepted by psycholinguists to be when children establish the correct word order of their native language."
"For example, although the LDD is currently limited to degree-0 (i.e. no embedding) and does not contain DP-internal structure, after examining by hand, several thousand sentences from corpora in the CHILDES database in five languages (English, German, Italian, Japanese and Russian), we found that approximately 85% are degree-0 and an approximate 10 out of 11 have no internal DP structure."
"Adopting the principles and parameters (P&amp;P) hypothesis[REF_CITE]as the underlying framework, we implemented an application that generated patterns and derivations given the following points of variation between languages: 1. Affix Hopping 2."
Comp Initial/Final 3.
I to C Movement 4.
Null Subject 5.
Null Topic 6.
Obligatory Topic 7. Object Final/Initial 8.
Pied Piping 9.
V to I[REF_CITE].
Obligatory Wh movement
"The patterns have fully specified X-bar struc-ture, and movement is implemented as HPSG local dependencies."
Pattern production is generated top-down via rules applied at each subtree level.
"Subtree levels include: CP, C&apos;, IP, I&apos;, NegP, Neg&apos;, VP, V&apos; and PP."
"After the rules are applied, the subtrees are fully specified in terms of node categories, syntactic feature values and constituent order."
The subtrees are then combined by a simple unification process and syntactic features are percolated down.
"In particular, movement chains are represented as traditional “slash” features which are passed (locally) from parent to daughter; when unification is complete, there is a trace at the bottom of each slash-feature path."
"Other features include +/-NULL for non-audible tokens (e.g. S[+NULL] represents a null subject pro), +TOPIC to represent a topicalized token, +WH to represent “who”, “what”, etc. (or “qui”, “que” if one pre-fers), +/-FIN to mark if a verb is tensed or not and the illocutionary (ILLOC) features Q, DEC, IMP for questions, declaratives and imperatives respec-tively."
"Although further detail is beyond the scope of this paper, those interested may refer[REF_CITE]which resides on the LDD website."
It is important to note that the domain is suit-able for many paradigms beyond the P&amp;P frame-work.
For example the context-free rules (with local dependencies) could be easily extracted and used to test probabilistic CFG learning in a multilingual domain.
"Likewise the patterns, without their derivations, could be used as input to statistical/connectionist models which eschew traditional (generative) structure altogether and search for regularity in the left-to-right strings of tokens that makeup the learner&apos;s input stream."
"Or, the patterns could help bootstrap the creation of a domain that might be used to test particular types of lexical learning by using the patterns as tem-plates where tokens may be instantiated with actual words from a lexicon of interest to the investigator."
"The point is that although a particular grammar formalism was used to generate the patterns, the patterns are valid independently of the formalism that was in play during generation. [Footnote_2]"
"2 If this is the case, one might ask: Why bother with a grammar formalism at all; why not use actual child-directed speech as input instead of artificially generated patterns? Although this approach has proved workable for several types of non-generative acquisition models, a generative (or hybrid) learner is faced with the task of selecting the rules or parameter values that generate the linguistic environment being encountered by the learner. In order to simulate this, there must be some grammatical structure incorporated into the experimental design that serves as the target the learner must acquire. Constructing a viable grammar and a parser with coverage over a multilingual domain of real child-directed speech is a daunting proposition. Even building a parser to parse a single language of child-directed speech turns out to be extremely difficult. See, for example, Sagae, Lavie, &amp;[REF_CITE], which discusses an impressive number of practical difficulties encountered while attempting to build a parser that could cope with the EVE corpus; one the cleanest transcriptions in the CHILDES database. By abstracting away from actual child-directed speech, we were able to build a pattern generator and include the pattern derivations in the database for retrieval during simulation runs, effectively sidestepping the need to build an online multilingual parser."
"To be sure, similar domains have been con-structed."
The relationship between the LDD and other artificial domains is summarized in Table 1.
"In designing the LDD, we chose to include syntactic phenomena which: i) occur in a relatively high proportion of the known natural languages; ii) are frequently exemplified in speech di-rected to 2-year-olds; iii) pose potential learning problems (e.g. cross-language ambiguity) for which theoretical solutions are needed; iv) have been a focus of linguistic and/or psy-cholinguistic research; v) have a syntactic analysis that is broadly agreed on."
"As a result the following have been included: • By criteria (i) and (ii): negation, non-declarative sentences (questions, impera-tives). • By criterion (iv): null subject parameter ([REF_CITE]and since). • By criterion (iv): affix-hopping (though not widespread in natural languages). • By criterion (v): no scrambling yet."
"There are several phenomena that the LDD does not yet include: • No verb subcategorization. • No interface with LF (cf.[REF_CITE]). • No discourse contexts to license sentence fragments (e.g., DP or PP fragments). • No XP-internal structure yet (except PP = P + O3, with piping or stranding). • No Linear Correspondence Axiom[REF_CITE]. • No feature checking as implementation of movement parameters[REF_CITE]."
The LDD on the web: The two primary purposes of the web-interface are to allow the user to interactively peruse the patterns and the derivations that the LDD contains and to download raw data for the user to work with locally.
Users are asked to register before using the LDD online.
"The user ID is typically an email address, although no validity checking is carried out."
"The benefit of entering a valid email address is simply to have the ability to recover a forgotten password, otherwise a user can have full access anonymously."
"The interface has three primary areas: Gram-mar Selection, Sentence Selection and Data Download."
"First a user has to specify, on the Grammar Selection page, which settings of the 13 parameters are of interest and save those settings as an available grammar."
A user may specify multiple grammars.
Then in the sentence selection page a user may peruse sentences and their derivations.
On this page a user may annotate the patterns and derivations however he or she wishes.
All grammar settings and annotations are saved and available the next time the user logs on.
"Finally on the Data Download page, users may download data so that they can use the patterns and derivations offline."
The derivations are stored as bracketed strings representing tree structure.
These are practically indecipherable by human users.
"To be readable, the derivations are displayed graphically as tree structures."
Towards this end we have utilized a set of publicly available LaTex macros:
"QTree (Siskind &amp; Dimitriadis, [online])."
A server-side script parses the bracketed structures into the proper QTree/LaTex format from which a pdf file is generated and subsequently sent to the user&apos;s client application.
"Even with the graphical display, a simple sen-tence-by-sentence presentation is untenable given the large amount of linguistic data contained in the database."
"The Sentence Selection area allows users to access the data filtered by sentence type and/or by grammar features (e.g. all sentences that have obligatory-wh movement and contain a preposi-tional phrase), as well as by the user’s defined grammar(s) (all sentences that are &quot;Italian-like&quot;)."
"On the Data Download page, users may filter sentences as on the Sentence Selection page and download sentences in a tab-delimited format."
The entire LDD may also be downloaded – approxi-mately 17[REF_CITE]MB as a raw ascii file.
We have recently run experiments of seven parameter-setting (P&amp;P) models of acquisition on the domain.
What follows is a brief discussion of the algorithms and the results of the experiments.
We note in particular where results stemming from work with the LDD lead to conclusions that differ from those previously reported.
We stress that this is not intended as a comprehensive study of parameter-setting algorithms or acquisition algorithms in general.
There is a large number of models that are omitted; some of which are targets of current investigation.
"Rather, we present the study as an example of how the LDD could be effectively utilized."
"In the discussion that follows we will use the terms “pattern”, “sentence” and “input” inter-changeably to mean a left-to-right string of tokens drawn from the LDD without its derivation."
"As a simple example of a learning strategy and of our simulation approach, consider a domain of 4 binary parameters and a memoryless learner [Footnote_3] which blindly guesses how all 4 parameters should be set upon encountering an input sentence."
3 By “memoryless” we mean that the learner processes inputs one at a time without keeping a history of encountered inputs or past learning events.
"Since there are 4 parameters, there are 16 possible combinations of parameter settings. i.e., 16 different grammars."
"Assuming that each of the 16 grammars is equally likely to be guessed, the learner will consume, on average, 16 sentences before achieving the target grammar."
This is one measure of a model’s efficiency or feasibility.
"However, when modeling natural language acquisition, since practically all human learners attain the target grammar, the average number of expected inputs is a less informative statistic than the expected number of inputs required for, say, 99% of all simulation trials to succeed."
"For our blind-guess learner, this number is 72. [Footnote_4]"
4 The average and 99-percentile figures (16 and 72) in this section are easily derived from the fact that input consumption follows a hypergeometric distribution.
"We will use this 99-percentile feasibility measure for most discussion that follows, but also include the average number of inputs for completeness."
"In all experiments: • The learners are memoryless. • The language input sample presented to the learner consists of only grammatical sentences generated by the target grammar. • For each learner, 1000 trials were run for each of the 3,072 target languages in the LDD. • At any point during the acquisition process, each sentence of the target grammar is equally likely to be presented to the learner."
Subset Avoidance and Other Local Maxima:
"Depending on the algorithm, it may be the case that a learner will never be motivated to change its current hypothesis (G curr ), and hence be unable to ultimately achieve the target grammar (G targ )."
"For example, most error-driven learners will be trapped if G curr generates a language that is a superset of the language generated by G targ ."
There is a wealth of learnability literature that addresses local maxima and their ramifications. [Footnote_5]
"5 Discussion of the problem of subset relationships among languages starts with Gold’s (1967) seminal paper and is discussed[REF_CITE]and Wexler &amp;[REF_CITE]. Detailed accounts of the types of local maxima that the learner might encounter in a domain similar to the one we employ are given in Frank &amp;[REF_CITE], Gibson &amp;[REF_CITE], and Niyogi &amp;[REF_CITE]."
"However, since our study’s focus is on feasibility (rather than on whether a domain is learnable given a particular algorithm), we posit a built-in avoidance mecha-nism, such as the subset principle and/or default values that preclude local maxima; hence, we set aside trials where a local maximum ensues."
"In all cases the learner is error-driven: if G curr can parse the current input pattern, retain it. [Footnote_6]"
"6 We intend for a “can-parse/can’t-parse outcome” to be equivalent to the result from a language membership test. If the current input sentence is one of the set of sentences generated by G curr , can-parse is engendered; if not, can’t-parse."
"The following refers to what the learner does when G curr fails on the current input. • Error-driven, blind-guess (EDBG): adopt any grammar from the domain chosen at random – not psychologically plausible, it serves as our baseline. • TLA (Gibson &amp;[REF_CITE]): change any one parameter value of those that make up G curr ."
Call this new grammar G new .
"If G new can parse the current input, adopt it."
"Otherwise, retain G curr . • Non-Greedy TLA (Niyogi &amp;[REF_CITE]): change any one parameter value of those that make up G curr ."
Adopt it. (I.e. there is no testing of the new grammar against the current input). • Non-SVC TLA (Niyogi &amp;[REF_CITE]): try any grammar in the domain.
Adopt it only in the event that it can parse the current input. • Guessing STL[REF_CITE]: Perform a structural parse of the current input.
"If a choice point is encountered, chose an alternative based on one of the following and then set parameter values based on the final parse tree: • STL Random Choice (RC) – randomly pick a parsing alternative. • Minimal Chain (MC) – pick the choice that obeys the Minimal Chain Principle (De[REF_CITE]), i.e., avoid positing movement transformations if possible. • Local Attachment/Late Closure (LAC) –pick the choice that attaches the new word to the current constituent[REF_CITE]."
The EDBG learner is our first learner of inter-est.
It is easy to show that the average and 99% scores increase exponentially in the number of parameters and syntactic research has proposed more than 100 (e.g.[REF_CITE]).
"Clearly, human learners do not employ a strategy that performs as poorly as this."
Results will serve as a baseline to compare against other models.
The TLA incorporates two search heuristics: the Single Value Constraint (SVC) and Greediness.
"In the event that G curr cannot parse the current input sentence s, the TLA attempts a second parse with a randomly chosen new gram-mar, G new , that differs from G curr by exactly one parameter value (SVC)."
"If G new can parse s, G new becomes the new G curr otherwise G new is rejected as a hypothesis (Greediness)."
The TLA has become a seminal model and has been extensively studied (cf.
The results from the TLA variants operating in the LDD are presented in Table 3.
"Particularly interesting is that contrary to results reported by Niyogi &amp;[REF_CITE]and Sakas &amp;[REF_CITE], the SVC and Greediness constraints do help the learner achieve the target in the LDD."
The previous research was based on simulations run on much smaller 9 and 16 lan-guage domains (see Table 1).
It would seem that the local hill-climbing search strategies employed by the TLA do improve learning efficiency in the LDD.
"However, even at best, the TLA performs less well than the blind guess learner."
We conjec-ture that this fact probably rules out the TLA as a viable model of human language acquisition.
Fodor’s Structural Triggers Learner (STL) makes greater use of the parser than the TLA.
"A key feature of the model is that parameter values are not simply the standardly presumed 0 or 1, but rather bits of tree structure or treelets."
"Thus, a grammar, in the STL sense, is a collection of treelets rather than a collection of 1&apos;s and 0&apos;s."
The STL is error-driven.
"If G curr cannot license s, new treelets will be utilized to achieve a successful parse. [Footnote_7] Treelets are applied in the same way as any “normal” grammar rule, so no unusual parsing activity is necessary."
"7 In addition to the treelets, UG principles are also available for parsing, as they are in the other models discussed above."
The STL hypothesizes grammars by adding parameter value treelets to G curr when they contribute to a successful parse.
The basic algorithm for all STL variants is: 1.
"If G curr can parse the current input sentence, retain the treelets that make up G curr . 2."
"Otherwise, parse the sentence making use of any or all parametric treelets available and adopt those treelets that contribute to a suc-cessful parse."
We call this parametric de-coding.
"Because the STL can decode inputs into their parametric signatures, it stands apart from other acquisition models in that it can detect when an input sentence is parametrically ambiguous."
"During a parse of s, if more than one treelet could be used by the parser (i.e., a choice point is encountered), then s is parametrically ambiguous."
The TLA variants do not have this capacity because they rely only on a can-parse/can’t-parse outcome and do not have access to the on-line operations of the parser.
"Originally, the ability to detect ambiguity was employed in two variations of the STL: the strong STL (SSTL) and the weak STL."
The SSTL executes a full parallel parse of each input sentence and adopts only those treelets (parameter values) that are present in all the generated parse trees.
"This would seem to make the SSTL an extremely powerful, albeit psycho-logically implausible, learner. [Footnote_8]"
"8 It is important to note th[REF_CITE]does not put forth the strong STL as a psychologically plausible model. Rather, it is intended to demonstrate the potential effectiveness of parametric decoding."
"However, this is not necessarily the case."
The SSTL needs some unambiguity to be present in the structures derived from the sentences of the target language.
"For example, there may not be a single input generated by G targ that when parsed yields an unambiguous treelet for a particular parameter."
"Unlike the SSTL, the weak STL executes a psychologically plausible left-to-right serial (deterministic) parse."
"One variant of the weak STL, the waiting STL (WSTL), deals with ambigu-ous inputs abiding by the heuristic: Don’t learn from sentences that contain a choice point."
These sentences are simply discarded for the purposes of learning.
"This is not to imply that children do not parse ambiguous sentences they hear, but only that they set no parameters if the current evidence is ambiguous."
"As with the TLA, these STL variants have been studied from a mathematical perspective[REF_CITE]."
Mathematical analyses point to the fact that the strong and weak STL are extremely efficient learners in conducive domains with some unambiguous inputs but may become paralyzed in domains with high degrees of ambigu-ity.
These mathematical analyses among other considerations spurred a new class of weak STL variants which we informally call the guessing STL family.
"The basic idea behind the guessing STL models is that there is some information available even in sentences that are ambiguous, and some strategy that can exploit that information."
"We incorporate three different heuristics into the original STL paradigm, the RC, MC and LAC heuristics described above."
"Although the MC and LAC heuristics are not stochastic, we regard them as “guessing” heuristics because, unlike the WSTL, a learner cannot be certain that the parametric treelets obtained from a parse guided by MC and LAC are correct for the target."
These heuristics are based on well-established human parsing strategies.
"Interestingly, the difference in performance between the three variants is slight."
"Although we have just begun to look at this data in detail, one reason may be that the typical types of problems these parsing strategies address are not included in the LDD (e.g. relative clause attachment ambiguity)."
"Still, the STL variants perform the most efficiently of the strategies presented in this small study (approxi-mately a 100-fold improvement over the TLA)."
Certainly this is due to the STL&apos;s ability to perform parametric decoding.
"The thrust of our current research is directed at collecting data for a comprehensive, comparative study of psycho-computational models of syntax acquisition."
"To support this endeavor, we have developed the Language Domain Database – a publicly available test-bed for studying acquisition models from diverse paradigms."
Mathematical analysis has shown that learners are extremely sensitive to various distributions in the input stream (Niyogi &amp;[REF_CITE]2003).
Approaches that thrive in one domain may dramatically flounder in others.
"So, whether a particular computational model is successful as a model of natural language acquisition is ultimately an empirical issue and depends on the exact conditions under which the model performs well and the extent to which those favorable conditions are in line with the facts of human language."
The LDD is a useful tool that can be used within such an empirical research program.
"Though the LDD has been vali-dated against CHILDES data in certain respects, we intend to extend this work by adding distribu-tions to the LDD that correspond to actual distribu-tions of child-directed speech."
"For example, what percentage of utterances, in child-directed Japa-nese, contain pro-drop? object-drop?"
How often in English does the pattern: S[+WH] aux Verb O1 occur and at what periods of a child&apos;s develop-ment?
We believe that these distributions will shed some light on many of the complex subtleties involved in ambiguity disambiguation and the role of nondeterminism and statistics in the language acquisition process.
"This is proving to be a formidable, yet surmountable task; one that we are just beginning to tackle."
"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar."
"Indeed, its performance of 86.36% (LP/LR F [Footnote_1] ) is better than that of early lexicalized PCFG models, and surprisingly close to the current state-of-the-art."
"1 There are minor differences, but all the current best-known lexicalized PCFG s employ both monolexical statistics, which describe the phrasal categories of arguments and adjuncts that appear around a head lexical item, and bilexical statistics, or de-pendencies, which describe the likelihood of a head word taking as a dependent a phrase headed by a certain other word."
"This result has potential uses beyond establish-ing a strong lower bound on the maximum possi-ble accuracy of unlexicalized models: an unlexical-ized PCFG is much more compact, easier to repli-cate, and easier to interpret than more complex lex-ical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic com-plexity, and easier to optimize."
"To facilitate comparison with previous work, we trained our models on sections 2–21 of the WSJ sec-tion of the Penn treebank."
We used the first 20 files (393 sentences) of section 22 as a development set (devset).
"This set is small enough that there is no-ticeable variance in individual results, but it allowed rapid search for good features via continually repars-ing the devset in a partially manual hill-climb."
All of section 23 was used as a test set for the final model.
"For each model, input trees were annotated or trans-formed in some way, as[REF_CITE]."
"Given a set of transformed trees, we viewed the local trees as grammar rewrite rules in the standard way, and used (unsmoothed) maximum-likelihood estimates for rule probabilities. [Footnote_5] To parse the grammar, we used a simple array-based Java implementation of a generalized CKY parser, which, for our final best model, was able to exhaustively parse all sentences in section 23 in 1 GB of memory, taking approxi-mately 3 sec for average length sentences. [Footnote_6]"
"5 The tagging probabilities were smoothed to accommodate unknown words. The quantity P(tag|word) was estimated as follows: words were split into one of several categories wordclass, based on capitalization, suffix, digit, and other character features. For each of these categories, we took the maximum-likelihood estimate of P(tag|wordclass). This dis-tribution was used as a prior against which observed taggings, if any, were taken, giving P(tag|word) = [c(tag,word) + κ P(tag|wordclass)]/[c(word)+κ]. This was then inverted to give P(word|tag). The quality of this tagging model impacts all numbers; for example the raw treebank grammar’s devset F 1 is 72.62 with it and 72.09 without it."
6 The parser is available for download as open source at[URL_CITE]
The traditional starting point for unlexicalized pars-ing is the raw n-ary treebank grammar read from training trees (after removing functional tags and null elements).
This basic grammar is imperfect in two well-known ways.
"First, the category symbols are too coarse to adequately render the expansions independent of the contexts."
"For example, subject NP expansions are very different from object NP ex-pansions: a subject NP is 8.7 times more likely than an object NP to expand as just a pronoun."
Having separate symbols for subject and object NP s allows this variation to be captured and used to improve parse scoring.
"One way of capturing this kind of external context is to use parent annotation, as pre-sented[REF_CITE]."
"For example, NP s with S parents (like subjects) will be marked NP ˆ S , while NP s with VP parents (like objects) will be NP ˆ VP ."
"The second basic deficiency is that many rule types have been seen only once (and therefore have their probabilities overestimated), and many rules which occur in test sentences will never have been seen in training (and therefore have their probabili-ties underestimated – see[REF_CITE]for analy-sis)."
"Note that in parsing with the unsplit grammar, not having seen a rule doesn’t mean one gets a parse failure, but rather a possibly very weird parse[REF_CITE]."
One successful method of combating sparsity is to markovize the rules[REF_CITE].
"In particular, we follow that work in markovizing out from the head child, despite the grammar being un-lexicalized, because this seems the best way to cap-ture the traditional linguistic insight that phrases are organized around a head[REF_CITE]."
Both parent annotation (adding context) and RHS markovization (removing it) can be seen as two in-stances of the same idea.
"In parsing, every node has a vertical history, including the node itself, parent, grandparent, and so on."
A reasonable assumption is that only the past v vertical ancestors matter to the current expansion.
"Similarly, only the previous h horizontal ancestors matter (we assume that the head child always matters)."
It is a historical accident that the default notion of a treebank PCFG grammar takes v = 1 (only the current node matters vertically) and h = ∞ (rule right hand sides do not decompose at all).
"On this view, it is unsurprising that increasing v and decreasing h have historically helped."
"As an example, consider the case of v = 1, h = 1."
"If we start with the rule VP → VBZ NP PP PP , it will be broken into several stages, each a binary or unary rule, which conceptually represent a head-outward generation of the right hand size, as shown in figure 1."
The bottom layer will be a unary over the head declaring the goal: h VP : [ VBZ ]i → VBZ .
"The square brackets indicate that the VBZ is the head, while the angle brackets h X i indicates that the symbol h X i is an intermediate symbol (equiv-alently, an active or incomplete state)."
The next layer up will generate the first rightward sibling of the head child: h VP : [ VBZ ]. . .
NP i → h VP : [ VBZ ]i NP .
"Next, the PP is generated: h VP : [ VBZ ]. . ."
PP i → h VP : [ VBZ ]. . .
NP i PP .
"We would then branch off left siblings if there were any. [Footnote_7] Finally, we have another unary to finish the VP ."
"7 In our system, the last few right children carry over as pre-ceding context for the left children, distinct from common prac-tice. We found this wrapped horizon to be beneficial, and it also unifies the infinite order model with the unmarkovized raw rules."
"Note that while it is con-venient to think of this as a head-outward process, these are just PCFG rewrites, and so the actual scores attached to each rule will correspond to a downward generation order."
Figure 2 presents a grid of horizontal and verti-cal markovizations of the grammar.
"The raw tree-bank grammar corresponds to v = 1,h = ∞ (the upper right corner), while the parent annotation[REF_CITE]corresponds to v = 2, h = ∞, and the second-order model[REF_CITE], is broadly a smoothed version of v = 2,h = 2."
"In addi-tion to exact nth-order models, we tried variable- history models similar in intent to those described[REF_CITE]."
"For variable horizontal his-tories, we did not split intermediate states below 10 occurrences of a symbol."
"For example, if the symbol h VP : [ VBZ ]. . ."
"PP PP i were too rare, we would col-lapse it to h VP : [ VBZ ]. . ."
"For vertical histories, we used a cutoff which included both frequency and mutual information between the history and the ex-pansions (this was not appropriate for the horizontal case because MI is unreliable at such low counts)."
Figure 2 shows parsing accuracies as well as the number of symbols in each markovization.
These symbol counts include all the intermediate states which represent partially completed constituents.
"The general trend is that, in the absence of further annotation, more vertical annotation is better – even exhaustive grandparent annotation."
"This is not true for horizontal markovization, where the variable-order second-order model was superior."
"The best entry, v = 3, h ≤ 2, has an F 1 of 79.74, already a substantial improvement over the baseline."
"In the remaining sections, we discuss other an-notations which increasingly split the symbol space."
"Since we expressly do not smooth the grammar, not all splits are guaranteed to be beneficial, and not all sets of useful splits are guaranteed to co-exist well."
"In particular, while v = 3, h ≤ 2 markovization is good on its own, it has a large number of states and does not tolerate further splitting well."
"Therefore, we base all further exploration on the v ≤ 2, h ≤ 2 grammar."
"Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories."
"The two major previous annotation strategies, par-ent annotation and head lexicalization, can be seen as instances of external and internal annotation, re-spectively."
Parent annotation lets us indicate an important feature of the external environment of a node which influences the internal expansion of that node.
"On the other hand, lexicalization is a (radi-cal) method of marking a distinctive aspect of the otherwise hidden internal contents of a node which influence the external distribution."
Both kinds of an-notation can be useful.
"To identify split states, we add suffixes of the form - X to mark internal content features, and ˆ X to mark external features."
"To illustrate the difference, consider unary pro-ductions."
"In the raw grammar, there are many unar-ies, and once any major category is constructed over a span, most others become constructible as well us-ing unary chains (see[REF_CITE]for discussion)."
"Such chains are rare in real treebank trees: unary rewrites only appear in very specific contexts, for example S complements of verbs where the S has an empty, controlled subject."
"Figure 4 shows an erroneous output of the parser, using the baseline markovized grammar."
"Intuitively, there are several reasons this parse should be ruled out, but one is that the lower S slot, which is intended pri-marily for S complements of communication verbs, is not a unary rewrite position (such complements usually have subjects)."
It would therefore be natural to annotate the trees so as to confine unary produc-tions to the contexts in which they are actually ap-propriate.
We tried two annotations.
"First, UNARY -"
INTERNAL marks (with a - U ) any nonterminal node which has only one child.
"In isolation, this resulted in an absolute gain of 0.55% (see figure 3)."
"The same sentence, parsed using only the baseline and UNARY - INTERNAL , is parsed correctly, because the VP rewrite in the incorrect parse ends with an S ˆ VP - U with very low probability. [Footnote_8]"
"8 Note that when we show such trees, we generally only show one annotation on top of the baseline at a time. More-over, we do not explicitly show the binarization implicit by the horizontal markovization."
"Alternately, UNARY - EXTERNAL , marked nodes which had no siblings with ˆ U ."
"It was similar to UNARY - INTERNAL in solo benefit (0.01% worse), but provided far less marginal benefit on top of other later features (none at all on top of UNARY - INTERNAL for our top models), and was discarded. [Footnote_9] One restricted place where external unary annota-tion was very useful, however, was at the pretermi-nal level, where internal annotation was meaning-less."
9 These two are not equivalent even given infinite data.
"One distributionally salient tag conflation in the Penn treebank is the identification of demonstra-tives (that, those) and regular determiners (the, a)."
Splitting DT tags based on whether they were only children ( UNARY - DT ) captured this distinction.
"The same external unary annotation was even more ef-fective when applied to adverbs ( UNARY - RB ), dis-tinguishing, for example, as well from also)."
"Be-yond these cases, unary tag marking was detrimen-tal."
"The F 1 after UNARY - INTERNAL , UNARY - DT , and UNARY -[REF_CITE].86%."
The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word be-haviour is a cornerstone of lexicalization.
"The UNARY - DT annotation, for example, showed that the determiners which occur alone are usefully distin-guished from those which occur with other nomi-nal material."
This marks the DT nodes with a single bit about their immediate external context: whether there are sisters.
"Given the success of parent anno-tation for nonterminals, it makes sense to parent an-notate tags, as well ( TAG - PA )."
"In fact, as figure 3 shows, exhaustively marking all preterminals with their parent category was the most effective single annotation we tried."
Why should this be useful?
Most tags have a canonical category.
"For example, NNS tags occur under NP nodes (only 234 of 70855 do not, mostly mistakes)."
"However, when a tag somewhat regularly occurs in a non-canonical posi-tion, its distribution is usually distinct."
"For example, the most common adverbs directly under ADVP are also (1599) and now (544)."
"Under VP , they are n’t (3779) and not (922)."
"Under NP , only (215) and just (132), and so on."
"TAG - PA brought F 1 up substan-tially, to 80.62%."
"In addition to the adverb case, the Penn tag set conflates various grammatical distinctions that are commonly made in traditional and generative gram-mar, and from which a parser could hope to get use-ful information."
"For example, subordinating con-junctions (while, as, if ), complementizers (that, for), and prepositions (of, in, from) all get the tag IN ."
"Many of these distinctions are captured by TAG - PA (subordinating conjunctions occur under S and prepositions under PP ), but are not (both subor-dinating conjunctions and complementizers appear under SBAR )."
"Also, there are exclusively noun-modifying prepositions (of), predominantly verb-modifying ones (as), and so on."
"The annotation SPLIT - IN does a linguistically motivated 6-way split of the IN tag, and brought the total to 81.19%."
Figure 5 shows an example error in the baseline which is equally well fixed by either TAG - PA or SPLIT - IN .
"In this case, the more common nominal use of works is preferred unless the IN tag is anno-tated to allow if to prefer S complements."
We also got value from three other annotations which subcategorized tags for specific lexemes.
"First we split off auxiliary verbs with the SPLIT - AUX annotation, which appends ˆ BE to all forms of be and ˆ HAVE to all forms of have. [Footnote_10] More mi-norly, SPLIT - CC marked conjunction tags to indicate whether or not they were the strings [Bb]ut or &amp;, each of which have distinctly different distributions from other conjunctions."
"10 This is an extended uniform version of the partial auxil-iary annotation[REF_CITE], wherein all auxiliaries are marked as AUX and a - G is added to gerund auxiliaries and gerund VP s."
"Finally, we gave the per-cent sign (%) its own tag, in line with the dollar sign ($) already having its own."
Together these three an-notations brought the F 1 to 81.81%.
"Around this point, we must address exactly what we mean by an unlexicalized PCFG ."
"To the extent that we go about subcategorizing POS categories, many of them might come to represent a single word."
"One might thus feel that the approach of this paper is to walk down a slippery slope, and that we are merely arguing degrees."
"However, we believe that there is a fundamental qualitative distinction, grounded in lin-guistic practice, between what we see as permitted in an unlexicalized PCFG as against what one finds and hopes to exploit in lexicalized PCFG s."
The di-vision rests on the traditional distinction between function words (or closed-class words) and content words (or open class or lexical words).
"It is stan-dard practice in linguistics, dating back decades, to annotate phrasal nodes with important function-word distinctions, for example to have a CP [for] or a PP [to], whereas content words are not part of grammatical structure, and one would not have spe-cial rules or constraints for an NP [stocks], for exam-ple."
"We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features com-monly expressed by function words are annotated onto phrasal nodes (such as whether a VP is finite, or a participle, or an infinitive clause)."
"However, no use is made of lexical class words, to provide either monolexical or bilexical probabilities. [Footnote_11]"
"11 It should be noted that we started with four tags in the Penn treebank tagset that rewrite as a single word: EX (there), WP $ (whose), # (the pound sign), and TO ), and some others such as WP , POS , and some of the punctuation tags, which rewrite as barely more. To the extent that we subcategorize tags, there will be more such cases, but many of them already exist in other tag sets. For instance, many tag sets, such as the Brown and CLAWS (c5) tagsets give a separate sets of tags to each form of the verbal auxiliaries be, do, and have, most of which rewrite as only a single word (and any corresponding contractions)."
"At any rate, we have kept ourselves honest by es-timating our models exclusively by maximum like-lihood estimation over our subcategorized gram-mar, without any form of interpolation or shrink-age to unsubcategorized categories (although we do markovize rules, as explained above)."
This effec- tively means that the subcategories that we break off must themselves be very frequent in the language.
"In such a framework, if we try to annotate cate-gories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses."
The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact.
"Even with the individual-lexeme annotations in this section, the grammar still has only 9255 states com-pared to the 7619 of the baseline model."
"At this point, one might wonder as to the wisdom of stripping off all treebank functional tags, only to heuristically add other such markings back in to the grammar."
"By and large, the treebank out-of-the package tags, such as PP - LOC or ADVP - TMP , have negative utility."
"Recall that the raw treebank gram-mar, with no annotation or markovization, had an F 1 of 72.62% on our development set."
"With the func-tional annotation left in, this drops to 71.49%."
"The h ≤ 2,v ≤ 1 markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included."
"Nonetheless, some distinctions present in the raw treebank trees were valuable."
"For example, an NP with an S parent could be either a temporal NP or a subject."
"For the annotation TMP - NP , we retained the original - TMP tags on NP s, and, furthermore, propa-gated the tag down to the tag of the head of the NP ."
"This is illustrated in figure 6, which also shows an example of its utility, clarifying that CNN last night is not a plausible compound and facilitating the oth-erwise unusual high attachment of the smaller NP ."
TMP - NP brought the cumulative F 1 to 82.25%.
"Note that this technique of pushing the functional tags down to preterminals might be useful more gener-ally; for example, locative PP s expand roughly the same way as all other PP s (usually as IN NP ), but they do tend to have different prepositions below IN ."
A second kind of information in the original trees is the presence of empty elements.
This brought F 1 to 82.28%.
The notion that the head word of a constituent can affect its behavior is a useful one.
"However, often the head tag is as good (or better) an indicator of how a constituent will behave. [Footnote_12]"
12 This is part of the explanation of why[REF_CITE]finds that early generation of head tags as[REF_CITE]is so beneficial. The rest of the benefit is presumably in the availability of the tags for smoothing purposes.
We found several head annotations to be particularly effective.
"First, pos-sessive NP s have a very different distribution than other NP s – in particular, NP → NP α rules are only used in the treebank when the leftmost child is pos-sessive (as opposed to other imaginable uses like for New York lawyers, which is left flat)."
"To address this, POSS - NP marked all possessive NP s."
This brought the total F 1 to 83.06%.
"Second, the VP symbol is very overloaded in the Penn treebank, most severely in that there is no distinction between finite and in-finitival VP s."
"An example of the damage this con-flation can do is given in figure 7, where one needs to capture the fact that present-tense verbs do not generally take bare infinitive VP complements."
"To allow the finite/non-finite distinction, and other verb type distinctions, SPLIT - VP annotated all VP nodes with their head tag, merging all finite forms to a sin-gle tag VBF ."
"In particular, this also accomplished Charniak’s gerund-"
"This was extremely useful, bringing the cumulative F 1 to 85.72%, 2.66% absolute improvement (more than its solo improve-ment over the baseline)."
Error analysis at this point suggested that many re-maining errors were attachment level and conjunc-tion scope.
"While these kinds of errors are undoubt-edly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured."
"Indeed, this ten-dency is a difficult trend to capture in a PCFG be-cause often the high and low attachments involve the very same rules."
"Even if not, attachment height is not modeled by a PCFG unless it is somehow ex-plicitly encoded into category labels."
More com-plex parsing models have indirectly overcome this by modeling distance (rather than height).
Linear distance is difficult to encode in a PCFG – marking nodes with the size of their yields mas-sively multiplies the state space. [Footnote_13]
"13 The inability to encode distance naturally in a naive PCFG is somewhat ironic. In the heart of any PCFG parser, the funda-mental table entry or chart item is a label over a span, for ex-ample an NP from position 0 to position 5. The concrete use of a grammar rule is to take two adjacent span-marked labels and combine them (for example NP [0,5] and VP [5,12] into S [0,12]). Yet, only the labels are used to score the combination."
"Therefore, we wish to find indirect indicators that distinguish high attachments from low ones."
"In the case of two PP s following a NP , with the question of whether the second PP is a second modifier of the leftmost NP or should attach lower, inside the first PP , the im-portant distinction is usually that the lower site is a non-recursive base NP ."
"Further, if an NP - B does not have a non-base NP parent, it is given one with a unary production."
"This was helpful, but substantially less effective than marking base NP s without introducing the unary, whose presence actually erased a useful internal indicator – base NP s are more frequent in subject position than object position, for example."
"In isolation, the Collins method actually hurt the base-line (absolute cost to F 1 of 0.37%), while skipping the unary insertion added an absolute 0.73% to the baseline, and brought the cumulative F 1 to 86.04%."
"In the case of attachment of a PP to an NP ei-ther above or inside a relative clause, the high NP is distinct from the low one in that the already mod-ified one contains a verb (and the low one may be a base NP as well)."
This is a partial explanation of the utility of verbal distance[REF_CITE].
"To capture this, DOMINATES - V marks all nodes which dominate any verbal node ( V *, MD ) with a - V ."
This brought the cumulative F 1 to 86.91%.
"We also tried marking nodes which dominated prepositions and/or conjunctions, but these features did not help the cu-mulative hill-climb."
"The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy."
"With RIGHT - REC - NP , we marked all NP s which contained another NP on their right periphery (i.e., as a right-most descendant)."
"This captured some further at-tachment trends, and brought us to a final develop-ment F 1 of 87.04%."
We took the final model and used it to parse sec-tion 23 of the treebank.
Figure 8 shows the re-sults.
"The test set F 1 is 86.32% for ≤ 40 words, already higher than early lexicalized models, though of course lower than the state-of-the-art parsers."
"The advantages of unlexicalized grammars are clear enough – easy to estimate, easy to parse with, and time- and space-efficient."
"However, the dismal per-formance of basic unannotated unlexicalized gram-mars has generally rendered those advantages irrel-evant."
"Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexi-calized PCFG can parse on par with early lexicalized parsers."
"We do not want to argue that lexical se-lection is not a worthwhile component of a state-of-the-art parser – certain attachments, at least, require it – though perhaps its necessity has been overstated."
"Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well."
We present a novel approach for find-ing discontinuities that outperforms pre-viously published results on this task.
"Rather than using a deeper grammar for-malism, our system combines a simple un-lexicalized PCFG parser with a shallow pre-processor."
"This pre-processor, which we call a trace tagger, does surprisingly well on detecting where discontinuities can occur without using phase structure information."
"In this paper, we explore a novel approach for find-ing long-distance dependencies."
"In particular, we detect such dependencies, or discontinuities, in a two-step process: (i) a conceptually simple shal-low tagger looks for sites of discontinuties as a pre-processing step, before parsing; (ii) the parser then finds the dependent constituent (antecedent)."
"Clearly, information about long-distance relation-ships is vital for semantic interpretation."
"However, such constructions prove to be difficult for stochas-tic parsers[REF_CITE]and they either avoid tackling the problem[REF_CITE]or only deal with a subset of the problematic cases[REF_CITE]."
"Although this algo-rithm fares well, it faces the problem that stochastic parsers not designed to capture non-local dependen-cies may get confused when parsing a sentence with discontinuities."
"However, the approach presented here is not susceptible to this shortcoming as it finds discontinuties before parsing."
"Overall, we present three primary contributions."
"First, we extend the mechanism of adding gap vari-ables for nodes dominating a site of discontinu-ity[REF_CITE]."
"This approach allows even a context-free parser to reliably recover antecedents, given prior information about where discontinuities occur."
"Second, we introduce a simple yet novel finite-state tagger that gives exactly this information to the parser."
"Finally, we show that the combina-tion of the finite-state mechanism, the parser, and our new method for antecedent recovery can com-petently analyze discontinuities."
The overall organization of the paper is as fol-lows.
"First, Section 2 sketches the material we use for the experiments in the paper."
"In Section 3, we propose a modification to a simple PCFG parser that allows it to reliably find antecedents if it knows the sites of long-distance dependencies."
"Then, in Sec-tion 4, we develop a finite-state system that gives the parser exactly that information with fairly high accu-racy."
We combine the models in Section 5 to recover antecedents.
Section 6 discusses related work.
"Different linguistic theories offer various treatments of non-local head–dependent relations (referred to by several other terms such as extraction, discon-tinuity, movement or long-distance dependencies)."
"The underlying idea, however, is the same: extrac-tion sites are marked in the syntactic structure and this mark is connected (co-indexed) to the control- ling constituent."
The experiments reported here rely on a train-ing corpus annotated with non-local dependencies as well as phrase-structure information.
"We used the Wall Street Journal (WSJ) part of the Penn Tree-bank[REF_CITE], where extraction is rep-resented by co-indexing an empty terminal element (henceforth EE ) to its antecedent."
"Without commit-ting ourselves to any syntactic theory, we adopt this representation."
"Following the annotation guidelines[REF_CITE], we distinguish seven basic types of EE s: controlled NP -traces ( NP ), PRO s ( PRO ), traces of A -movement (mostly wh-movement: WH ), empty complementizers ( COMP ), empty units ( UNIT ), and traces representing pseudo-attachments (shared constituents, discontinuous dependencies, etc.: PSEUDO ) and ellipsis ( ELLIPSIS )."
"These la-bels, however, do not identify the EE s uniquely: for instance, the label WH may represent an extracted NP object as well as an adverb moved out of the verb phrase."
"In order to facilitate antecedent re-covery and to disambiguate the EE s, we also anno-tate them with their parent nodes."
"Furthermore, to ease straightforward comparison with previous work[REF_CITE], a new label CLAUSE is introduced for COMP - SBAR whenever it is followed by a moved clause WH – S ."
"Table 1 summarizes the most frequent types occurring in the development data, Section 0 of the WSJ corpus, and gives an example for each, following[REF_CITE]."
"For the parsing and antecedent recovery exper-iments, in the case of WH -traces ( WH –  ) and controlled NP -traces ( NP – NP ), we follow the stan-dard technique of marking nodes dominating the empty element up to but not including the par-ent of the antecedent as defective (missing an ar-gument) with a gap feature[REF_CITE]. [Footnote_1]"
1 This technique fails for 82 sentences of the treebank where the antecedent does not c-command the corresponding EE .
"Furthermore, to make antecedent co-indexation possible with many types of EE s, we generalize Collins’ approach by enriching the anno-tation of non-terminals with the type of the EE in question (eg."
WH – NP ) by using different gap+ fea-tures (gap+WH-NP; cf.
The original non-terminals augmented with gap+ features serve as new non-terminal labels.
"In the experiments, Sections 2–21 were used to train the models, Section 0 served as a develop-ment set for testing and improving models, whereas we present the results on the standard test set,[REF_CITE]."
"The present section explores whether an unlexical-ized PCFG parser can handle non-local dependen-cies: first, is it able to detect EE s and, second, can it find their antecedents?"
"The answer to the first question turns out to be negative: due to efficiency reasons and the inappropriateness of the model, de-tecting all types of EE s is not feasible within the parser."
"Antecedents, however, can be reliably recov-ered provided a parser has perfect knowledge about EE s occurring in the input."
This shows that the main bottleneck is detecting the EE s and not finding their antecedents.
"In the following section, therefore, we explore how we can provide the parser with infor-mation about EE sites in the current sentence without relying on phrase structure information."
There are three modifications required to allow a parser to detect EE s and resolve antecedents.
"First, it should be able to insert empty nodes."
"Second, it must thread the gap+ variables to the parent node of the antecedent."
"Knowing this node is not enough, though."
"Since the Penn Treebank grammar is not binary-branching, the final task is to decide which child of this node is the actual antecedent."
The first two modifications are not diffi-cult conceptually.
A bottom-up parser can be easily modified to insert empty elements (c.f.[REF_CITE]).
"Likewise, the changes required to include gap+ categories are not compli-cated: we simply add the gap+ features to the non-terminal category labels."
The final and perhaps most important concern with developing a gap-threading parser is to ensure it is possible to choose the correct child as the an-tecedent of an EE .
"To achieve this task, we em-ploy the algorithm presented in Figure 2."
"At any node in the tree where the children, all together, have more gap+ features activated than the par-ent, the algorithm deduces that a gap+ must have an antecedent."
It then picks a child as the an-tecedent and recursively removes the gap+ feature corresponding to its EE from the non-terminal la-bels.
"The algorithm has a shortcoming, though: it cannot reliably handle cases when the antecedent does not c-command its EE ."
"This mostly happens with PSEUDO s (pseudo-attachments), where the al-gorithm gives up and (wrongly) assumes they have no antecedent."
"Given the perfect trees of the development set, the antecedent recovery algorithm finds the correct antecedent with 95% accuracy, rising to 98% if PSEUDO s are excluded."
"Most of the remaining mis-takes are caused either by annotation errors, or by binding NP -traces ( NP – NP ) to adjunct NP s, as op-posed to subject NP s."
The parsing experiments are carried out with an unlexicalized PCFG augmented with the antecedent recovery algorithm.
We use an unlexicalized model to emphasize the point that even a simple model de-tects long distance dependencies successfully.
"The parser uses beam thresholding[REF_CITE]to for a tree T, iterate over nodes bottom-up ensure efficient parsing."
PCFG probabilities are cal-culated in the standard way[REF_CITE].
"In order to keep the number of independently tunable parameters low, no smoothing is used."
The parser is tested under two different condi-tions.
"First, to assess the upper bound an EE -detecting unlexicalized PCFG can achieve, the input of the parser contains the empty elements as sepa-rate words ( PERFECT )."
"Second, we let the parser introduce the EE s itself ( INSERT )."
We evaluate on all sentences in the test section of the treebank.
"As our interest lies in trace detection and antecedent recovery, we adopt the evaluation mea-sures introduced[REF_CITE]."
An EE is cor-rectly detected if our model gives it the correct la-bel as well as the correct position (the words before and after it).
"When evaluating antecedent recovery, the EE s are regarded as four-tuples, consisting of the type of the EE , its location, the type of its antecedent and the location(s) (beginning and end) of the an-tecedent."
An antecedent is correctly recovered if all four values match the gold standard.
"The preci-sion, recall, and the combined F-score is presented for each experiment."
Missed parses are ignored for evaluation purposes.
The main results for the two conditions are summa-rized in Table 2.
"In the INSERT case, the parser de-tects empty elements with precision 64.7%, recall 40.3% and F-[REF_CITE].7%."
"It recovers antecedents with overall precision 55.7%, recall 35.0% and F-score 43.0%."
"With a beam width of 1000, about half of the parses were missed, and successful parses take, on average, 21 seconds per sentence and enu-merate 1.7 million edges."
"Increasing the beam size to 40000 decreases the number of missed parses marginally, while parsing time increases to nearly two minutes per sentence, with 2.9 million edges enumerated."
"In the PERFECT case, when the sites of the empty elements are known before parsing, only about 1.6% of the parses are missed and average parsing time goes down to 2 5 seconds per sentence."
"More impor-tantly, the overall precision and recall of antecedent recovery is 91.4%."
The result of the experiment where the parser is to detect long-distance dependencies is negative.
"The parser misses too many parses, regardless of the beam size."
This cannot be due to the lack of smooth-ing: the model with perfect information about the EE -sites does not run into the same problem.
"Hence, the edges necessary to construct the required parse are available but, in the INSERT case, the beam search loses them due to unwanted local edges hav-ing a higher probability."
"Doing an exhaustive search might help in principle, but it is infeasible in prac-tice."
"Clearly, the problem is with the parsing model: an unlexicalized PCFG parser is not able to detect where EE s can occur, hence necessary edges get low probability and are, thus, filtered out."
"The most interesting result, though, is the dif-ference in speed and in antecedent recovery accu-racy between the parser that inserts traces, and the parser which uses perfect information from the tree-bank about the sites of EE s. Thus, the question naturally arises: could EE s be detected before pars-ing?"
"The benefit would be two-fold: EE s might be found more reliably with a different module, and the parser would be fast and accurate in recovering an-tecedents."
"In the next section we show that it is in-deed possible to detect EE s without explicit knowl-edge of phrase structure, using a simple finite-state tagger."
"This section shows that EE s can be detected fairly reliably before parsing, i.e. without using phrase structure information."
"Specifically, we develop a finite-state tagger which inserts EE s at the appro-priate sites."
"It is, however, unable to find the an-tecedents for the EE s; therefore, in the next section, we combine the tagger with the PCFG parser to re-cover the antecedents."
Detecting empty elements can be regarded as a sim-ple tagging task: we tag words according to the ex-istence and type of empty elements preceding them.
"For example, the word Sasha in the sentence"
"Sam said COMP – SBAR Sasha snores. will get the tag EE = COMP – SBAR , whereas the word Sam is tagged with EE =* expressing the lack of an EE immediately preceding it."
"If a word is preceded by more than one EE , such as to in the following example, it is tagged with the concatenation of the two EE s, i.e., EE = COMP – WHNP PRO – NP ."
It would have been too late COMP – WHNP PRO – NP to think about on Friday.
"Although this approach is closely related to POS-tagging, there are certain differences which make this task more difficult."
"Despite the smaller tagset, the data exhibits extreme sparseness: even though more than 50% of the sentences in the Penn Tree-bank contain some EE s, the actual number of EE s is very small."
"In Section 0 of the WSJ corpus, out of the 46451 tokens only 3056 are preceded by one or more EE s, that is, approximately 93.5% of the words are tagged with the EE =* tag."
"The other main difference is the apparently non-local nature of the problem, which motivates our choice of a Maximum Entropy (ME) model for the tagging task[REF_CITE]."
"ME allows the flexible combination of different sources of informa-tion, i.e., local and long-distance cues characterizing possible sites for EE s. In the ME framework, linguis-tic cues are represented by (binary-valued) features ( f i ), the relative importance (weight, λ i ) of which is determined by an iterative training algorithm."
The weighted linear combination of the features amount to the log-probability of the label (l) given the con-text (c): 1 exp ∑ i λ i f i l c p l c (1) Z c where Z c is a context-dependent normalizing fac-tor to ensure that p l c be a proper probability dis-tribution.
We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm[REF_CITE].
"Templates for local features are similar to the ones employed[REF_CITE]for POS-tagging (Table 3), though as our input already includes POS-tags, we can make use of part-of-speech information as well."
Long-distance features are simple hand- written regular expressions matching possible sites for EE s (Table 4).
Features and labels occurring less than 10 times in the training corpus are ignored.
"Since our main aim is to show that finding empty elements can be done fairly accurately without us-ing a parser, the input to the tagger is a POS-tagged corpus, containing no syntactic information."
"The best label-sequence is approximated by a bigram Viterbi-search algorithm, augmented with variable width beam-search."
The results of the EE -detection experiment are sum-marized in Table 5.
"The overall unlabeled F-score is 85 3%, whereas the labeled F-score is 79 1%, which amounts to 97 9% word-level tagging accuracy."
"For straightforward comparison with Johnson’s results, we must conflate the categories PRO – NP and NP – NP ."
"If the trace detector does not need to differ-entiate between these two categories, a distinction that is indeed important for semantic analysis, the overall labeled F-score increases to 83 0%, which outperforms Johnson’s approach by 4%."
"The success of the trace detector is surprising, es-pecially if compared to Johnson’s algorithm which uses the output of a parser."
The tagger can reliably detect extraction sites without explicit knowledge of the phrase structure.
"This shows that, in English, ex-traction can only occur at well-defined sites, where local cues are generally strong."
"Indeed, the strength of the model lies in detecting such sites (empty units, UNIT ; NP traces, NP – NP ) or where clear-cut long-distance cues exist ( WH – S , COMP – SBAR )."
"The accuracy of detecting uncon- trolled PRO s ( PRO – NP ) is rather low, since it is a dif-ficult task to tell them apart from NP traces: they are confused in 10 15% of the cases."
"Furthermore, the model is unable to capture for. . . to+ INF construc-tions if the noun-phrase is long."
"The precision of detecting long-distance NP ex-traction ( WH – NP ) is also high, but recall is lower: in general, the model finds extracted NP s with overt complementizers."
"Detection of null WH -complementizers ( COMP – WHNP ), however, is fairly inaccurate (48 8% F-score), since finding it and the corresponding WH – NP requires information about the transitivity of the verb."
"The performance of the model is also low (59 5%) in detecting movement sites for extracted WH -adverbs ( WH – ADVP ) despite the presence of unambiguous cues ( where , how , etc. starting the subordinate clause)."
The difficulty of the task lies in finding the correct verb-phrase as well as the end of the verb-phrase the constituent is ex-tracted from without knowing phrase boundaries.
"One important limitation of the shallow approach described here is its inability to find the antecedents of the EE s, which clearly requires knowledge of phrase structure."
"In the next section, we show that the shallow trace detector and the unlexicalized PCFG parser can be coupled to efficiently and suc-cessfully tackle antecedent recovery."
"In Section 3, we found that parsing with EE s is only feasible if the parser knows the location of EE s be-fore parsing."
"In Section 4, we presented a finite-state tagger which detects these sites before parsing takes place."
"In this section, we validate the two-step ap-proach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy[REF_CITE]."
"Theoretically, the ‘best’ way to combine the trace tagger and the parsing algorithm would be to build a unified probabilistic model."
"However, the nature of the models are quite different: the finite-state model is conditional, taking the words as given."
"The pars-ing model, on the other hand, is generative, treat-ing the words as an unlikely event."
There is a rea-sonable basis for building the probability models in different ways.
"Most of the tags emitted by the EE tagger are just EE =*, which would defeat genera-tive models by making the ‘hidden’ state uninfor-mative."
"Conditional parsing algorithms do exist, but they are difficult to train using large corpora[REF_CITE]."
"However, we show that it is quite ef-fective if the parser simply treats the output of the tagger as a certainty."
"Given this combination method, there still are two interesting variations: we may use only the EE s proposed by the tagger (henceforth the NOINSERT model), or we may allow the parser to insert even more EE s (henceforth the INSERT model)."
"In both cases, EE s outputted by the tagger are treated as sep-arate words, as in the PERFECT model of Section 3."
The NOINSERT model did better at antecedent de-tection (see Table 6) than the INSERT model.
"NOINSERT model was also faster, taking on aver-age 2.7 seconds per sentence and enumerating about 160,000 edges whereas the INSERT model took 25 seconds on average and enumerated 2 million edges."
"The coverage of the NOINSERT model was higher than that of the INSERT model, missing 2.4% of all parses versus 5.3% for the INSERT model."
"Comparing our results[REF_CITE], we find that the NOINSERT model outperforms that of John-son by 4.6% (see Table 7)."
The strength of this sys-tem lies in its ability to tell unbound PRO s and bound NP – NP traces apart.
Combining the finite-state tagger with the parser seems to be invaluable for EE detection and an-tecedent recovery.
"Paradoxically, taking the com-bination to the extreme by allowing both the parser and the tagger to insert EE s performed worse."
"While the INSERT model here did have wider coverage than the parser in Section 3, it seems the real benefit of using the combined approach is to let the simple model reduce the search space of the more complicated parsing model."
"This search space reduction works because the shallow finite-state method takes information about adjacent words into account, whereas the context-free parser does not, since a phrase boundary might separate them."
"While these systems should, in theory, be able to handle discontinuities accurately, there has not yet been a study on how these systems handle such phenomena overall."
The tagger presented here is not the first one proposed to recover syntactic information deeper than part-of-speech tags.
"For example, supertag-ging[REF_CITE]also aims to do more meaningful syntactic pre-processing."
"Unlike supertagging, our approach only focuses on detect-ing EE s."
"The idea of threading EE s to their antecedents in a stochastic parser was proposed[REF_CITE], following the GPSG traditi[REF_CITE]."
"However, we extend it to capture all types of EE s."
This paper has three main contributions.
"First, we show that gap+ features, encoding necessary infor-mation for antecedent recovery, do not incur any substantial computational overhead."
"Second, the paper demonstrates that a shallow finite-state model can be successful in detecting sites for discontinuity, a task which is generally under-stood to require deep syntactic and lexical-semantic knowledge."
"The results show that, at least in En-glish, local clues for discontinuity are abundant."
This opens up the possibility of employing shal-low finite-state methods in novel situations to exploit non-apparent local information.
"Our final contribution, but the one we wish to em-phasize the most, is that the combination of two or-thogonal shallow models can be successful at solv-ing tasks which are well beyond their individual power."
The accent here is on orthogonality – the two models take different sources of information into ac-count.
"The tagger makes good use of adjacency at the word level, but is unable to handle deeper re-cursive structures."
"A context-free grammar is better at finding vertical phrase structure, but cannot ex-ploit linear information when words are separated by phrase boundaries."
"As a consequence, the finite-state method helps the parser by efficiently and re-liably pruning the search-space of the more compli-cated PCFG model."
The benefits are immediate: the parser is not only faster but more accurate in recov-ering antecedents.
The real power of the finite-state model is that it uses information the parser cannot.
"When rules of transfer-based machine translation (MT) are automatically ac-quired from bilingual corpora, incor-rect/redundant rules are generated due to acquisition errors or translation variety in the corpora."
"As a new countermeasure to this problem, we propose a feedback cleaning method using automatic evalua-tion of MT quality, which removes incor-rect/redundant rules as a way to increase the evaluation score."
BLEU is utilized for the automatic evaluation.
"The hill-climbing algorithm, which involves fea-tures of this task, is applied to searching for the optimal combination of rules."
Our experiments show that the MT quality im-proves by 10% in test sentences according to a subjective evaluation.
This is consid-erable improvement over previous meth-ods.
"Along with the efforts made in accumulating bilin-gual corpora for many language pairs, quite a few machine translation (MT) systems that automati-cally acquire their knowledge from corpora have been proposed."
"However, knowledge for transfer-based MT acquired from corpora contains many in-correct/redundant rules due to acquisition errors or translation variety in the corpora."
Such rules con-flict with other existing rules and cause implausible
MT results or increase ambiguity.
"If incorrect rules could be avoided, MT quality would necessarily im-prove."
"There are two approaches to overcoming incor-rect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line process-ing,[REF_CITE]). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing,[REF_CITE])."
We employ the second approach in this paper.
The cutoff by frequency[REF_CITE]and the hypothesis test[REF_CITE]have been applied to clean the rules.
"The cutoff by fre-quency can slightly improve MT quality, but the im-provement is still insufficient from the viewpoint of the large number of redundant rules."
The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confi-dent.
Another current topic of machine translation is automatic evaluation of MT quality[REF_CITE].
These methods aim to replace subjective evaluation in or-der to speed up the development cycle of MT sys-tems.
"However, they can be utilized not only as de-velopers’ aids but also for automatic tuning of MT systems[REF_CITE]."
We propose feedback cleaning that utilizes an automatic evaluation for removing incor-rect/redundant translation rules as a tuning method (Figure 1).
Our method evaluates the contribution of each rule to the MT results and removes inap-propriate rules as a way to increase the evaluation scores.
"Since the automatic evaluation correlates with a subjective evaluation, MT quality will im-prove after cleaning."
"Our method only evaluates MT results and does not consider various conditions of the MT engine, such as parameters, interference in dictionaries, dis-ambiguation methods, and so on."
"Even if an MT engine avoids incorrect/redundant rules by on-line processing, errors inevitably remain."
Our method cleans the rules in advance by only focusing on the remaining errors.
"Thus, our method complements on-line processing and adapts translation rules to the given conditions of the MT engine."
We use the Hierarchical Phrase Alignment-based Translator (HPAT)[REF_CITE]as a transfer-based MT system.
"The most important knowledge in HPAT is transfer rules, which define the correspon-dences between source and target language expres-sions."
An example of English-to-Japanese transfer rules is shown in Figure 2.
The transfer rules are regarded as a synchronized context-free grammar.
"When the system translates an input sentence, the sentence is first parsed by using source patterns of the transfer rules."
"Next, a tree structure of the tar-get language is generated by mapping the source patterns to the corresponding target patterns."
"When non-terminal symbols remain in the target tree, tar- get words are inserted by referring to a translation dictionary."
"Ambiguities, which occur during parsing or map-ping, are resolved by selecting the rules that mini-mize the semantic distance between the input words and source examples (real examples in the training corpus) of the transfer rules[REF_CITE]."
"For instance, when the input phrase “leave at 11 a.m.” is translated into Japanese, Rule 2 in Figure 2 is selected because the semantic distance from the source example (arrive, p.m.) is the shortest to the head words of the input phrase (leave, a.m.)."
HPAT automatically acquires its transfer rules from parallel corpora by using Hierarchical Phrase Align-ment[REF_CITE].
"However, the rule set con-tains many incorrect/redundant rules."
The reasons for this problem are roughly classified as follows. • Errors in automatic rule acquisition • Translation variety in corpora – The acquisition process cannot generalize the rules because bilingual sentences de-pend on the context or the situation. – Corpora contain multiple (paraphrasable) translations of the same source expres-sion.
"In the experiment[REF_CITE], about 92,000 transfer rules were acquired from about 120,000 bilingual sentences [Footnote_1] ."
"1 In this paper, the number of rules denotes the number of unique pairs of source patterns and target patterns."
Most of these rules are low-frequency.
"They reported that MT quality slightly improved, even though the low-frequency rules were removed to a level of about 1/9 the pre-vious number."
"However, since some of them, such as idiomatic rules, are necessary for translation, MT quality cannot be dramatically improved by only re-moving low-frequency rules."
We utilize BLEU[REF_CITE]for the au-tomatic evaluation of MT quality in this paper.
BLEU measures the similarity between MT re-sults and translation results made by humans (called references).
This similarity is measured by N-gram precision scores.
Several kinds of N-grams can be used in BLEU.
"We use from 1-gram to 4-gram in this paper, where a 1-gram precision score indicates the adequacy of word translation and longer N-gram (e.g., 4-gram) precision scores indicate fluency of sentence translation."
"The BLEU score is calculated from the product of N-gram precision scores, so this measure combines adequacy and fluency."
Note that a sizeable set of MT results is necessary in order to calculate an accurate BLEU score.
"Al-though it is possible to calculate the BLEU score of a single MT result, it contains errors from the subjec-tive evaluation."
BLEU cancels out individual errors by summing the similarities of MT results.
"There-fore, we need all of the MT results from the evalua-tion corpus in order to calculate an accurate BLEU score."
One feature of BLEU is its use of multiple ref-erences for a single source sentence.
"However, one reference per sentence is used in this paper because an already existing bilingual corpus is applied to the cleaning."
"In this section, we introduce the proposed method, called feedback cleaning."
This method is carried out by selecting or removing translation rules to increase the BLEU score of the evaluation corpus (Figure 1).
"Thus, this task is regarded as a combinatorial op-timization problem of translation rules."
"The hill-climbing algorithm, which involves the features of this task, is applied to the optimization."
The fol-lowing sections describe the reasons for using this method and its procedure.
The hill-climbing al-gorithm often falls into locally optimal solutions.
"However, we believe that a locally optimal solution is more effective in improving MT quality than the previous methods."
Most combinatorial optimization methods iterate changes in the combination and the evaluation.
"In the machine translation task, the evaluation process requires the longest time."
"For example, in order to calculate the BLEU score of a combination (solu-tion), we have to translate C times, where C denotes the size of the evaluation corpus."
"Furthermore, in order to find the nearest neighbor solution, we have to calculate all BLEU scores of the neighborhood."
"If the number of rules is R and the neighborhood is regarded as consisting of combinations made by changing only one rule, we have to translate C × R times to find the nearest neighbor solution."
"Assume that C = 10,000 and R = 100,000, the number of sentence translations (sentences to be translated) becomes one billion."
It is infeasible to search for the optimal solution without reducing the number of sentence translations.
A feature of this task is that removing rules is eas-ier than adding rules.
The rules used for translating a sentence can be identified during the translation.
"Conversely, the source sentence set S[r], where a rule r is used for the translation, is determined once the evaluation corpus is translated."
"When r is re-moved, only the MT results of S[r] will change, so we do not need to re-translate other sentences."
"Assuming that five rules on average are applied to translate a sentence, the number of sentence trans-lations becomes 5 × C + C = 60,000 for testing all rules."
"On the contrary, to add a rule, the entire corpus must be re-translated because it is unknown which MT results will change by adding a rule."
"Based on the above discussion, we utilize the hill-climbing algorithm, in which the initial solution contains all rules (called the base rule set) and the search for a combination is done by only removing rules."
The algorithm is shown in Figure 3.
"This al-gorithm can be summarized as follows. • Translate the evaluation corpus first and then obtain the rules used for the translation and the BLEU score before removing rules. • For each rule one-by-one, calculate the BLEU score after removing the rule and obtain the dif-ference between this score and the score before the rule was removed."
"This difference is called the rule contribution. • If the rule contribution is negative (i.e., the BLUE score increases after removing the rule), remove the rule."
"In order to achieve faster convergence, this algo-rithm removes all rules whose rule contribution is negative in one iteration."
This assumes that the re-moved rules are independent from one another.
"In general, most evaluation corpora are smaller than training corpora."
"Therefore, omissions of cleaning will remain because not all rules can be tested by the evaluation corpus."
"In order to avoid this problem, we propose an advanced method called cross-cleaning (Figure 4), which is similar to cross-validation."
The procedure of cross-cleaning is as follows. 1.
"First, create the base rule set from the entire training corpus. 2."
"Next, divide the training corpus into N pieces uniformly. 3. Leave one piece for the evaluation, acquire rules from the rest (N − 1) of the pieces, and repeat them N times."
"Thus, we obtain N pairs of rule set and evaluation sub-corpus."
Each rule set is a subset of the base rule set. 4. Apply the feedback cleaning algorithm to each of the N pairs and record the rule contributions even if the rules are removed.
The purpose of this step is to obtain the rule contributions. 5.
"For each rule in the base rule set, sum up the rule contributions obtained from the rule sub-sets."
"If the sum is negative, remove the rule from the base rule set."
The major difference of this method from cross-validation is Step 5.
"In the case of cross-cleaning, the rule subsets cannot be directly merged because some rules have already been removed in Step 4."
"Therefore, we only obtain the rule contributions from the rule subsets and sum them up."
The summed contribution is an approximate value of the rule contribution to the entire training corpus.
Cross-cleaning removes the rules from the base rule set based on this approximate contribution.
"Cross-cleaning uses all sentences in the training corpus, so it is nearly equivalent to applying a large evaluation corpus to feedback cleaning, even though it does not require specific evaluation corpora."
"In this section, the effects of feedback cleaning are evaluated by using English-to-Japanese translation."
Bilingual Corpora The corpus used in the fol-lowing experiments is the Basic Travel Expression Corpus[REF_CITE].
This is a collec-tion of Japanese sentences and their English trans-lations based on expressions that are usually found in phrasebooks for foreign tourists.
"We divided it into sub-corpora for training, evaluation, and test as shown in Table 1."
"The number of rules acquired from the training corpus (the base rule set size) was 105,588."
Evaluation Methods of MT Quality We used the following two methods to evaluate MT quality. 1. Test Corpus BLEU Score
The BLUE score was calculated with the test corpus.
"The number of references was one for each sentence, in the same way used for the feedback cleaning. 2."
Subjective Quality A total of 510 sentences from the test corpus were evaluated by paired comparison.
"Specif-ically, the source sentences were translated us-ing the base rule set, and the same sources were translated using the rules after the cleaning."
"One-by-one, a Japanese native speaker judged which MT result was better or that they were of the same quality."
"Subjective quality is repre-sented by the following equation, where I de-notes the number of improved sentences and D denotes the number of degraded sentences."
I − D Subj.
Quality = (1) # of test sentences
"In order to observe the characteristics of feedback cleaning, cleaning of the base rule set was carried out by using the evaluation corpus."
The results are shown in Figure 5.
"This graph shows changes in the test corpus BLEU score, the evaluation corpus BLEU score, and the number of rules along with the number of iterations."
"Consequently, the removed rules converged at nine iterations, and 6,220 rules were removed."
"The evaluation corpus BLEU score was improved by in-creasing the number of iterations, demonstrating that the combinatorial optimization by the hill-climbing algorithm worked effectively."
The test corpus BLEU score reached a peak score of 0.245 at the second iteration and slightly decreased after the third itera-tion due to overfitting.
"However, the final score was 0.244, which is almost the same as the peak score."
The test corpus BLEU score was lower than the evaluation corpus BLEU score because the rules used in the test corpus were not exhaustively checked by the evaluation corpus.
"If the evaluation corpus size could be expanded, the test corpus score would improve."
This means that the time for an iteration is estimated at about ten hours if trans-lation speed is one second per sentence.
This is a short enough time for us because our method does not require real-time processing. [Footnote_2]
"2 In this experiment, it took about 80 hours until convergence using a Pentium 4 2-GHz computer."
"Next, in order to compare the proposed methods with the previous methods, the MT quality achieved by each of the following five methods was measured. 1."
Baseline The MT results using the base rule set. 2.
Cutoff by Frequency Low-frequency rules that appeared in the train-ing corpus less often than twice were removed from the base rule set.
This threshold was experimentally determined by the test corpus BLEU score. 3. χ 2 Test The χ [Footnote_2] test was performed in the same manner as[REF_CITE]’s experiment.
"2 In this experiment, it took about 80 hours until convergence using a Pentium 4 2-GHz computer."
We intro-duced rules with more than 95 percent confi-dence (χ 2 ≥ 3.841). 4.
Simple Feedback Cleaning Feedback cleaning was carried out using the evaluation corpus in Table 1. 5.
Cross-cleaning N-fold cross-cleaning was carried out.
We ap-plied five-fold cross-cleaning in this experi-ment.
The results are shown in Table 2.
This table shows that the test corpus BLEU score and the subjective quality of the proposed methods (simple feedback cleaning and cross-cleaning) are considerably im-proved over those of the previous methods.
"Focusing on the subjective quality of the proposed methods, some MT results were degraded from the baseline due to the removal of rules."
"However, the subjective quality levels were relatively improved because our methods aim to increase the portion of correct MT results."
"Focusing on the number of the rules, the rule set of the simple feedback cleaning is clearly a lo-cally optimal solution, since the number of rules is more than that of cross-cleaning, although the BLEU score is lower."
"In comparing the number of rules in cross-cleaning with that in the cutoff by fre-quency, the former is three times higher than the lat-ter."
We assume that the solution of cross-cleaning is also the locally optimal solution.
"If we could find the globally optimal solution, the MT quality would certainly improve further."
The idea of feedback cleaning is independent of BLEU.
Some automatic evaluation methods of MT quality other than BLEU have been proposed.
"For example,[REF_CITE],[REF_CITE], and[REF_CITE]measure similarity between MT results and the references by DP matching (edit dis-tances) and then output the evaluation scores."
These automatic evaluation methods that output scores are applicable to feedback cleaning.
"The characteristics common to these methods, in-cluding BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities."
"Therefore, MT results of the eval-uation corpus are necessary to evaluate the system, and reducing the number of sentence translations is an important technique for all of these methods."
The effects of feedback cleaning depend on the characteristics of objective measures.
DP-based measures and BLEU have different characteristics[REF_CITE].
The exploration of several measures for feedback cleaning remains an interest-ing future work.
"When applying corpus-based machine translation to a different domain, bilingual corpora of the new do-main are necessary."
"However, the sizes of the new corpora are generally smaller than that of the orig-inal corpus because the collection of bilingual sen-tences requires a high cost."
The feedback cleaning proposed in this paper can be interpreted as adapting the translation rules so that the MT results become similar to the evaluation corpus.
"Therefore, if we regard the bilingual corpus of the new domain as the evaluation corpus and carry out feedback cleaning, the rule set will be adapted to the new domain."
"In other words, our method can be applied to adaptation of an MT system by using a smaller corpus of the new domain."
"In this paper, we proposed a feedback cleaning method that utilizes automatic evaluation to remove incorrect/redundant translation rules."
"BLEU was utilized for the automatic evaluation of MT qual-ity, and the hill-climbing algorithm was applied to searching for the combinatorial optimization."
"Uti-lizing features of this task, incorrect/redundant rules were removed from the initial solution, which con-tains all rules acquired from the training corpus."
"In addition, we proposed N-fold cross-cleaning to re-duce the influence of the evaluation corpus size."
Our experiments show that the MT quality was improved by 10% in paired comparison and by 0.045 in the BLEU score.
This is considerable improvement over the previous methods.
A central problem of word sense disam-biguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
"In this paper, we evaluate an ap-proach to automatically acquire sense-tagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task."
Our investigation reveals that this method of acquiring sense-tagged data is promising.
"On a subset of the most diffi-cult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could nar-row further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage."
Our analysis also highlights the importance of the issue of domain dependence in evalu-ating WSD programs.
"The task of word sense disambiguation (WSD) is to determine the correct meaning, or sense of a word in context."
"It is a fundamental problem in natural language processing (NLP), and the ability to disambiguate word sense accurately is important for applications like machine translation, informa-tion retrieval, etc."
"Corpus-based, supervised machine learning methods have been used to tackle the WSD task, just like the other NLP tasks."
"Among the various approaches to WSD, the supervised learning ap-proach is the most successful to date."
"In this ap-proach, we first collect a corpus in which each occurrence of an ambiguous word w has been manually annotated with the correct sense, accord-ing to some existing sense inventory in a diction-ary."
This annotated corpus then serves as the training material for a learning algorithm.
"After training, a model is automatically learned and it is used to assign the correct sense to any previously unseen occurrence of w in a new context."
"While the supervised learning approach has been successful, it has the drawback of requiring manually sense-tagged data."
"This problem is par-ticular severe for WSD, since sense-tagged data must be collected separately for each word in a language."
"One source to look for potential training data for WSD is parallel texts, as proposed[REF_CITE]."
"Given a word-aligned paral-lel corpus, the different translations in a target lan-guage serve as the “sense-tags” of an ambiguous word in the source language."
"For example, some possible Chinese translations of the English noun channel are listed in Table 1."
"To illustrate, if the sense of an occurrence of the noun channel is “a path over which electrical signals can pass”, then this occurrence can be translated as “频道” in Chi-nese."
This approach of getting sense-tagged corpus also addresses two related issues in WSD.
"Firstly, what constitutes a valid sense distinction carries much subjectivity."
Different dictionaries define a different sense inventory.
"By tying sense distinc-tion to the different translations in a target lan-guage, this introduces a “data-oriented” view to sense distinction and serves to add an element of objectivity to sense definition."
"Secondly, WSD has been criticized as addressing an isolated problem without being grounded to any real application."
"By defining sense distinction in terms of different tar-get translations, the outcome of word sense disam-biguation of a source language word is the selection of a target word, which directly corre-sponds to word selection in machine translation."
"While this use of parallel corpus for word sense disambiguation seems appealing, several practical issues arise in its implementation: (i) What is the size of the parallel corpus needed in order for this approach to be able to dis-ambiguate a source language word accurately? (ii) While we can obtain large parallel corpora in the long run, to have them manually word-aligned would be too time-consuming and would defeat the original purpose of getting a sense-tagged corpus without manual annotation."
"How-ever, are current word alignment algorithms accu-rate enough for our purpose? (iii) Ultimately, using a state-of-the-art super-vised WSD program, what is its disambiguation accuracy when it is trained on a “sense-tagged” corpus obtained via parallel text alignment, com-pared with training on a manually sense-tagged corpus?"
Much research remains to be done to investi-gate all of the above issues.
"The lack of large-scale parallel corpora no doubt has impeded progress in this direction, although attempts have been made to mine parallel corpora from the Web[REF_CITE]."
"However, large-scale, good-quality parallel corpora have recently become available."
"For ex-ample, six English-Chinese parallel corpora are now available from Linguistic Data Consortium."
"These parallel corpora are listed in Table 2, with a combined size of 280 MB."
"In this paper, we ad-dress the above issues and report our findings, ex-ploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation."
"We evalu-ated our approach on all the nouns in the English lexical sample task of SENSEVAL-2[REF_CITE], which used the WordNet 1.7 sense inventory[REF_CITE]."
"While our approach has only been tested on English and Chinese, it is completely general and applicable to other language pairs."
Our approach of exploiting parallel texts for word sense disambiguation consists of four steps: (1) parallel text alignment (2) manual selection of tar-get translations (3) training of WSD classifier (4) WSD of words in new contexts.
"In this step, parallel texts are first sentence-aligned and then word-aligned."
Various alignment algo-rithms[REF_CITE]have been developed in the past.
"For the six bilingual corpora that we used, they already come with sen-tences pre-aligned, either manually when the cor-pora were prepared or automatically by sentence-alignment programs."
"After sentence alignment, the English texts are tokenized so that a punctuation symbol is separated from its preceding word."
"For the Chinese texts, we performed word segmenta-tion, so that Chinese characters are segmented into words."
The resulting parallel texts are then input to the GIZA++ software[REF_CITE]for word alignment.
"In the output of GIZA++, each English word token is aligned to some Chinese word token."
"The alignment result contains much noise, especially for words with low frequency counts."
"In this step, we will decide on the sense classes of an English word w that are relevant to translating w into Chinese."
"We will illustrate with the noun channel, which is one of the nouns evaluated in the English lexical sample task of SENSEVAL-2."
We rely on two sources to decide on the sense classes of w: (i)
"The sense definitions in WordNet 1.7, which lists seven senses for the noun channel."
Two senses are lumped together if they are translated in the same way in Chinese.
"For example, sense 1 and 7 of channel are both translated as “频道” in Chi-nese, so these two senses are lumped together. (ii) From the word alignment output of GIZA++, we select those occurrences of the noun channel which have been aligned to one of the Chinese translations chosen (as listed in Table 1)."
These occurrences of the noun channel in the Eng-lish side of the parallel texts are considered to have been disambiguated and “sense-tagged” by the ap-propriate Chinese translations.
Each such occur-rence of channel together with the 3-sentence context in English surrounding channel then forms a training example for a supervised WSD program in the next step.
The average time taken to perform manual se-lection of target translations for one SENSEVAL-2 English noun is less than 15 minutes.
"This is a rela-tively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training exam-ples."
This step could also be potentially automated if we have a suitable bilingual translation lexicon.
Much research has been done on the best super-vised learning approach for WSD[REF_CITE].
"In this paper, we used the WSD program reported[REF_CITE]."
"In particular, our method made use of the knowledge sources of part-of-speech, sur-rounding words, and local collocations."
We used naïve Bayes as the learning algorithm.
Our previ-ous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance.
"Given an occurrence of w in a new context, we then used the naïve Bayes classifier to determine the most probable sense of w."
We evaluated our approach to word sense disam-biguation on all the 29 nouns in the English lexical sample task of SENSEVAL-2[REF_CITE].
The list of 29 nouns is given in Table 3.
The second column of Table 3 lists the number of senses of each noun as given in the WordNet 1.7 sense inventory[REF_CITE].
We first lump together two senses s 1 and s 2 of a noun if s 1 and s 2 are translated into the same Chi-nese word.
The number of senses of each noun after sense lumping is given in column 3 of Table 3.
"For the 7 nouns that are lumped into one sense (i.e., they are all translated into one Chinese word), we do not perform WSD on these words."
The aver- age number of senses before and after sense lump-ing is 5.07 and 3.52 respectively.
"After sense lumping, we trained a WSD classi-fier for each noun w, by using the lumped senses in the manually sense-tagged training data for w pro-vided by the SENSEVAL-2 organizers."
We then tested the WSD classifier on the official SENSEVAL-2 test data (but with lumped senses) for w. The test accuracy (based on fine-grained scoring of SENSEVAL-2) of each noun obtained is listed in the column labeled M1 in Table 3.
We then used our approach of parallel text alignment described in the last section to obtain the training examples from the English side of the par-allel texts.
"Due to the memory size limitation of our machine, we were not able to align all six par-allel corpora of 280MB in one alignment run of"
"For two of the corpora, Hong Kong Han-sards and Xinhua News, we gathered all English sentences containing the 29 SENSEVAL-2 noun occurrences (and their sentence-aligned Chinese sentence counterparts)."
"This subset, together with the complete corpora of Hong Kong News, Hong Kong Laws, English translation of Chinese Tree-bank, and Sinorama, is then given to GIZA++ to perform one word alignment run."
It took about 40 hours on our 2.4 GHz machine with 2 GB memory to perform this alignment.
"After word alignment, each 3-sentence context in English containing an occurrence of the noun w that is aligned to a selected Chinese translation then forms a training example."
"For each SENSEVAL-2 noun w, we then collected training examples from the English side of the parallel texts using the same number of training examples for each sense of w that are present in the manually sense-tagged SENSEVAL-2 official training cor-pus (lumped-sense version)."
"If there are insuffi-cient training examples for some sense of w from the parallel texts, then we just used as many paral-lel text training examples as we could find for that sense."
We chose the same number of training ex-amples for each sense as the official training data so that we can do a fair comparison between the accuracy of the parallel text alignment approach versus the manual sense-tagging approach.
"After training a WSD classifier for w with such parallel text examples, we then evaluated the WSD classifier on the same official SENSEVAL-2 test set (with lumped senses)."
The test accuracy of each noun obtained by training on such parallel text training examples (averaged over 10 trials) is listed in the column labeled P1 in Table 3.
The baseline accuracy for each noun is also listed in the column labeled “P1-Baseline” in Table 3.
The baseline accuracy corresponds to always picking the most frequently occurring sense in the training data.
"Ideally, we would hope M1 and P1 to be close in value, since this would imply that WSD based on training examples collected from the parallel text alignment approach performs as well as manu-ally sense-tagged training examples."
"Comparing the M1 and P1 figures, we observed that there is a set of nouns for which they are relatively close."
"These nouns are: bar, bum, chair, day, dyke, fa-tigue, hearth, mouth, nation, nature, post, re-straint, sense, stress."
"This set of nouns is relatively easy to disambiguate, since using the most-frequently-occurring-sense baseline would have done well for most of these nouns."
"The parallel text alignment approach works well for nature and sense, among these nouns."
"For nature, the parallel text alignment approach gives better accuracy, and for sense the accuracy differ-ence is only 0.014 (while there is a relatively large difference of 0.231 between P1 and P1-Baseline of sense)."
This demonstrates that the parallel text alignment approach to acquiring training examples can yield good results.
"For the remaining nouns (art, authority, chan-nel, church, circuit, facility, grip, spade), the accuracy difference between M1 and P1 is at least 0.10."
"Henceforth, we shall refer to this set of 8 nouns as “difficult” nouns."
We will give an analy-sis of the reason for the accuracy difference be-tween M1 and P1 in the next section.
"To see why there is still a difference between the accuracy of the two approaches, we first examined the quality of the training examples obtained through parallel text alignment."
"If the automati-cally acquired training examples are noisy, then this could account for the lower P1 score."
The word alignment output of GIZA++ con-tains much noise in general (especially for the low frequency words).
"However, note that in our ap-proach, we only select the English word occur-rences that align to our manually selected Chinese translations."
"Hence, while the complete set of word alignment output contains much noise, the subset of word occurrences chosen may still have high quality sense tags."
"Our manual inspection reveals that the annota-tion errors introduced by parallel text alignment can be attributed to the following sources: (i) Wrong sentence alignment: Due to errone-ous sentence segmentation or sentence alignment, the correct Chinese word that an English word w should align to is not present in its Chinese sen-tence counterpart."
"In this case, word alignment will align the wrong Chinese word to w. (ii) Presence of multiple Chinese translation candidates:"
"Sometimes, multiple and distinct Chi- nese translations appear in the aligned Chinese sentence."
"For example, for an English occurrence channel, both “频道” (sense 1 translation) and “途 径” (sense 5 translation) happen to appear in the aligned Chinese sentence."
"In this case, word alignment may erroneously align the wrong Chi-nese translation to channel. (iii) Truly ambiguous word: Sometimes, a word is truly ambiguous in a particular context, and dif-ferent translators may translate it differently."
"For example, in the phrase “the church meeting”, church could be the physical building sense (教 堂), or the institution sense (教会)."
"In manual sense tagging done in SENSEVAL-2, it is possible to assign two sense tags to church in this case, but in the parallel text setting, a particular translator will translate it in one of the two ways (教堂 or 教 会), and hence the sense tag found by parallel text alignment is only one of the two sense tags."
"By manually examining a subset of about 1,000 examples, we estimate that the sense-tag error rate of training examples (tagged with lumped senses) obtained by our parallel text alignment approach is less than 1%, which compares favorably with the quality of manually sense tagged corpus prepared in SENSEVAL-2[REF_CITE]."
"While it is encouraging to find out that the par-allel text sense tags are of high quality, we are still left with the task of explaining the difference be-tween M1 and P1 for the set of difficult nouns."
Our further investigation reveals that the accuracy dif-ference between M1 and P1 is due to the following two reasons: domain dependence and insufficient sense coverage.
"Domain Dependence The accuracy figure of M1 for each noun is obtained by training a WSD classifier on the manually sense-tagged training data (with lumped senses) provided by SENSEVAL-2 organizers, and testing on the cor-responding official test data (also with lumped senses), both of which come from similar domains."
"In contrast, the P1 score of each noun is obtained by training the WSD classifier on a mixture of six parallel corpora, and tested on the official SENSEVAL-2 test set, and hence the training and test data come from dissimilar domains in this case."
"Moreover, from the “docsrc” field (which re-cords the document id that each training or test example originates) of the official SENSEVAL-2 training and test examples, we realized that there are many cases when some of the examples from a document are used as training examples, while the rest of the examples from the same document are used as test examples."
"In general, such a practice results in higher test accuracy, since the test exam-ples would look a lot closer to the training exam-ples in this case."
"To address this issue, we took the official SENSEVAL-2 training and test examples of each noun w and combined them together."
We then ran-domly split the data into a new training and a new test set such that no training and test examples come from the same document.
The number of training examples in each sense in such a new training set is the same as that in the official train-ing data set of w.
"A WSD classifier was then trained on this new training set, and tested on this new test set."
The accuracy figures for the set of difficult nouns thus obtained are listed in the col-umn labeled M2 in Table 3.
We observed that M2 is always lower in value compared to M1 for all difficult nouns.
This sug-gests that the effect of training and test examples coming from the same document has inflated the accuracy figures of SENSEVAL-2 nouns.
"Next, we randomly selected 10 sets of training examples from the parallel corpora, such that the number of training examples in each sense fol-lowed the official training set of w. (When there were insufficient training examples for a sense, we just used as many as we could find from the paral-lel corpora.)"
"In each trial, after training a WSD classifier on the selected parallel text examples, we tested the classifier on the same test set (from SENSEVAL-2 provided data) used in that trial that generated the M2 score."
The accuracy figures thus obtained for all the difficult nouns are listed in the column labeled P2 in Table 3.
Insufficient Sense Coverage We observed that there are situations when we have insufficient training examples in the parallel corpora for some of the senses of some nouns.
"For instance, no oc-currences of sense 5 of the noun circuit (racing circuit, a racetrack for automobile races) could be found in the parallel corpora."
"To ensure a fairer comparison, for each of the 10-trial manually sense-tagged training data that gave rise to the ac-curacy figure M2 of a noun w, we extracted a new subset of 10-trial (manually sense-tagged) training data by ensuring adherence to the number of train-ing examples found for each sense of w in the cor-responding parallel text training set that gave rise to the accuracy figure P2 for w. The accuracy fig-ures thus obtained for the difficult nouns are listed in the column labeled M3 in Table 3."
M3 thus gave the accuracy of training on manually sense-tagged data but restricted to the number of training exam-ples found in each sense from parallel corpora.
The difference between the accuracy figures of M2 and P2 averaged over the set of all difficult nouns is 0.140.
This is smaller than the difference of 0.189 between the accuracy figures of M1 and P1 averaged over the set of all difficult nouns.
This confirms our hypothesis that eliminating the possi-bility that training and test examples come from the same document would result in a fairer com-parison.
"In addition, the difference between the accuracy figures of M3 and P2 averaged over the set of all difficult nouns is 0.065."
"That is, eliminating the advantage that manually sense-tagged data have in their sense coverage would reduce the performance gap between the two approaches from 0.140 to 0.065."
Notice that this reduction is particularly sig-nificant for the noun circuit.
"For this noun, the par-allel corpora do not have enough training examples for sense 4 and sense 5 of circuit, and these two senses constitute approximately 23% in each of the 10-trial test set."
"We believe that the remaining difference of 0.065 between the two approaches could be attrib-uted to the fact that the training and test examples of the manually sense-tagged corpus, while not coming from the same document, are however still drawn from the same general domain."
"To illustrate, we consider the noun channel where the difference between M3 and P2 is the largest."
"For channel, it turns out that a substantial number of the training and test examples contain the collocation “Channel tunnel” or “Channel Tunnel”."
"On average, about 9.8 training examples and 6.2 test examples con-tain this collocation."
This alone would have ac-counted for 0.088 of the accuracy difference between the two approaches.
That domain dependence is an important issue affecting the performance of WSD programs has been pointed out[REF_CITE].
Our work confirms the importance of domain depend-ence in WSD.
"As to the problem of insufficient sense cover-age, with the steady increase and availability of parallel corpora, we believe that getting sufficient sense coverage from larger parallel corpora should not be a problem in the near future for most of the commonly occurring words in a language."
"However, they only looked at assigning at most two senses to a word, and their method only asked a single ques-tion about a single word of context."
Their method also does not require a parallel corpus.
The research[REF_CITE]dealt with sense distinctions across multiple languages.
Our present work can be similarly extended beyond bilingual corpora to multilingual corpora.
The research most similar to ours is the work[REF_CITE].
"However, they used ma-chine translated parallel corpus instead of human translated parallel corpus."
"In addition, they used an unsupervised method of noun group disambigua-tion, and evaluated on the English all-words task."
"In this paper, we reported an empirical study to evaluate an approach of automatically acquiring sense-tagged training data from English-Chinese parallel corpora, which were then used for disam-biguating the nouns in the SENSEVAL-2 English lexical sample task."
Our investigation reveals that this method of acquiring sense-tagged data is pro-mising and provides an alternative to manual sense tagging.
This paper describes a method for learn-ing the countability preferences of English nouns from raw text corpora.
"The method maps the corpus-attested lexico-syntactic properties of each noun onto a feature vector, and uses a suite of memory-based classifiers to predict membership in 4 countability classes."
We were able to as-sign countability to English nouns with a precision of 94.6%.
"This paper is concerned with the task of knowledge-rich lexical acquisition from unannotated corpora, focusing on the case of countability in English."
Knowledge-rich lexical acquisition takes unstruc-tured text and extracts out linguistically-precise cat-egorisations of word and expression types.
"By combining this with a grammar, we can build broad-coverage deep-processing tools with a min-imum of human effort."
"This research is close in spirit to the work[REF_CITE]on classi-fying the semantics of derivational affixes, and[REF_CITE]on learning verb as-pect."
"In English, nouns heading noun phrases are typ-ically either countable or uncountable (also called count and mass)."
"Countable nouns can be modi-fied by denumerators, prototypically numbers, and have a morphologically marked plural form: one dog, two dogs."
"Uncountable nouns cannot be modi-fied by denumerators, but can be modified by unspe-cific quantifiers such as much, and do not show any number distinction (prototypically being singular): *one equipment, some equipment, *two equipments."
"Many nouns can be used in countable or uncountable environments, with differences in interpretation."
We call the lexical property that determines which uses a noun can have the noun’s countability prefer- ence.
Knowledge of countability preferences is im-portant both for the analysis and generation of En-glish.
"In analysis, it helps to constrain the inter-pretations of parses."
"In generation, the countabil-ity preference determines whether a noun can be-come plural, and the range of possible determin-ers."
"Knowledge of countability is particularly im-portant in machine translation, because the closest translation equivalent may have different countabil-ity from the source noun."
"Many languages, such as Chinese and Japanese, do not mark countability, which means that the choice of countability will be largely the responsibility of the generation compo-nent[REF_CITE]."
"In addition, knowledge of count-ability obtained from examples of use is an impor-tant resource for dictionary construction."
"In this paper, we learn the countability prefer-ences of English nouns from unannotated corpora."
"We first annotate them automatically, and then train classifiers using a set of gold standard data, taken from COMLEX[REF_CITE]and the trans-fer dictionaries used by the machine translation sys-tem ALT-J/E[REF_CITE]."
The classifiers and their training are described in more detail[REF_CITE].
These are then run over the corpus to extract nouns as members of four classes — countable : dog; uncountable : furniture; bi-partite : [pair of] scissors and plural only : clothes.
We first discuss countability in more detail (§ 2).
Then we present the lexical resources used in our ex-periment (§ 3).
"Next, we describe the learning pro-cess (§ 4)."
We then present our results and evalu-ation (§ 5).
"Finally, we discuss the theoretical and practical implications (§ 6)."
Grammatical countability is motivated by the se-mantic distinction between object and substance reference (also known as bounded/non-bounded or individuated/non-individuated).
It is a subject of contention among linguists as to how far grammat-ical countability is semantically motivated and how much it is arbitrary[REF_CITE].
The prevailing position in the natural language processing community is effectively to treat count-ability as though it were arbitrary and encode it as a lexical property of nouns.
The study of countabil-ity is complicated by the fact that most nouns can have their countability changed: either converted by a lexical rule or embedded in another noun phrase.
"An example of conversion is the so-called universal packager, a rule which takes an uncountable noun with an interpretation as a substance, and returns a countable noun interpreted as a portion of the sub-stance: I would like two beers."
"An example of em-bedding is the use of a classifier, e.g. uncountable nouns can be embedded in countable noun phrases as complements of classifiers: one piece of equip-ment."
"Nouns which rarely undergo conversion are marked as either fully countable , uncountable or plural only ."
"Fully countable nouns have both singular and plural forms, and can-not be used with determiners such as much, little, a little, less and overmuch."
"Uncountable nouns, such as furniture, have no plural form, and can be used with much."
"Plural only nouns never head a singular noun phrase: goods, scissors."
"Nouns that are readily converted are marked as ei-ther strongly countable (for countable nouns that can be converted to uncountable, such as cake) or weakly countable (for uncountable nouns that are readily convertible to countable, such as beer)."
"NLP systems must list countability for at least some nouns, because full knowledge of the refer-ent of a noun phrase is not enough to predict count-ability."
There is also a language-specific knowl-edge requirement.
This can be shown most sim-ply by comparing languages: different languages en-code the countability of the same referent in dif-ferent ways.
"There is nothing about the concept denoted by lightning, e.g., that rules out *a light-ning being interpreted as a flash of lightning."
"In-deed, the German and French translation equivalents are fully countable (ein Blitz and un éclair respec-tively)."
"Even within the same language, the same referent can be encoded countably or uncountably: clothes/clothing, things/stuff , jobs/work."
"Therefore, we must learn countability classes from usage examples in corpora."
There are several impediments to this approach.
"The first is that words are frequently converted to different countabilities, sometimes in such a way that other native speak- ers will dispute the validity of the new usage."
"We do not necessarily wish to learn such rare examples, and may not need to learn more common conver-sions either, as they can be handled by regular lexi-cal rules[REF_CITE]."
"The second problem is that some constructions affect the appar-ent countability of their head: for example, nouns denoting a role, which are typically countable, can appear without an article in some constructions (e.g. We elected him treasurer)."
"The third is that different senses of a word may have different countabilities: interest “a sense of concern with and curiosity” is normally countable, whereas interest “fixed charge for borrowing money” is uncountable."
There have been at several earlier approaches to the automatic determination of countabil-ity.
"O’[REF_CITE]get better results (89.5%) using the much larger Cyc ontology, although they only distinguish between countable and uncountable."
"ACT looks at deter-miner co-occurrence in singular noun chunks, and classifies the noun if and only if it occurs with a de-terminer which can modify only countable or un-countable nouns."
"The method has a coverage of around 50%, and agrees[REF_CITE]% of the nouns marked countable and with the ALT-J/E lexicon for 88%."
Agreement was worse for uncount-able nouns (6% and 44% respectively).
Information about noun countability was obtained from two sources.
"One was COMLEX 3.0[REF_CITE], which has around 22,000 noun entries."
The remainder are unmarked for countability.
The other was the common noun part of ALT-J/E ’s Japanese-to-English semantic transfer dictio-nary[REF_CITE].
"Considering only unique English entries with different countability and ignoring all other informa-tion gave 56,245 entries."
Nouns in the ALT-J/E dic-tionary are marked with one of the five major count- ability preference classes described in Section 2.
"In addition to countability, default values for number and classifier (e.g. blade for grass: blade of grass) are also part of the lexicon."
"We classify words into four possible classes, with some words belonging to multiple classes."
"The first class is countable : COMLEX ’s COUNTABLE and ALT-J/E ’s fully, strongly and weakly countable ."
The sec-ond class is uncountable : COMLEX ’s NCOLLECTIVE or :PLURAL *NONE* and ALT-J/E ’s strongly and weakly countable and uncountable .
The third class is bipartite nouns.
"These can only be plural when they head a noun phrase (trousers), but singular when used as a modifier (trouser leg)."
When they are denumerated they use pair: a pair of scissors.
"COMLEX does not have a feature to mark bipartite nouns; trouser, for example, is listed as countable."
Nouns in ALT-J/E marked plural only with a default classifier of pair are classified as bipartite .
"The last class is plural only nouns: those that only have a plural form, such as goods."
They can nei-ther be denumerated nor modified by much.
"Many of these nouns, such as clothes, use the plural form even as modifiers (a clothes horse)."
The word clothes cannot be denumerated at all.
Nouns marked :SINGULAR *NONE* in COMLEX and nouns in ALT-J/E marked plural only without the default classifier pair are classified as plural only .
"There was some noise in the ALT-J/E data, so this class was hand-checked, giving a total of 104 entries; 84 of these were attested in the training data."
"Our classification of countability is a subset of ALT-J/E ’s, in that we use only the three basic ALT-J/E classes of countable , uncountable and plural only , (although we treat bipartite as a separate class, not a subclass)."
"As we derive our countability classifica-tions from corpus evidence, it is possible to recon-struct countability preferences (i.e. fully , strongly , or weakly countable ) from the relative token occurrence of the different countabilities for that noun."
"In order to get an idea of the intrinsic difficulty of the countability learning task, we tested the agree-ment between the two resources in the form of clas-sification accuracy."
"That is, we calculate the average proportion of (both positive and negative) countabil-ity classifications over which the two methods agree."
"E.g., COMLEX lists tomato as being only countable where ALT-J/E lists it as being both countable and un-countable ."
"Agreement for this one noun, therefore, is 3 4 , as there is agreement for the classes of countable , plural only and bipartite (with implicit agreement as to negative membership for the latter two classes), but not for uncountable ."
"Averaging over the total set of nouns countability-classified in both lexicons, the mean was 93.8%."
Almost half of the disagreements came from words with two countabilities in ALT-J/E but only one in COMLEX .
"The basic methodology employed in this research is to identify lexical and/or constructional features as-sociated with the countability classes, and determine the relative corpus occurrence of those features for each noun."
We then feed the noun feature vectors into a classifier and make a judgement on the mem-bership of the given noun in each countability class.
"In order to extract the feature values from corpus data, we need the basic phrase structure, and partic-ularly noun phrase structure, of the source text."
"We use three different sources for this phrase structure: part-of-speech tagged data, chunked data and fully-parsed data, as detailed below."
"The corpus of choice throughout this paper is the written component of the British National Corpus (BNC version 2,[REF_CITE]), totalling around 90m w-units (POS-tagged items)."
"We chose this be-cause of its good coverage of different usages of En-glish, and thus of different countabilities."
The only component of the original annotation we make use of is the sentence tokenisation.
"Below, we outline the features used in this re-search and methods of describing feature interac-tion, along with the pre-processing tools and ex-traction techniques, and the classifier architecture."
"The full range of different classifier architectures tested as part of this research, and the experi-ments to choose between them are described[REF_CITE]."
"For each target noun, we compute a fixed-length feature vector based on a variety of features intended to capture linguistic constraints and/or preferences associated with particular countability classes."
"The feature space is partitioned up into feature clusters, each of which is conditioned on the occurrence of the target noun in a given construction."
"Feature clusters take the form of one- or two-dimensional feature matrices, with each dimension describing a lexical or syntactic property of the construction in question."
"In the case of a one-dimensional feature cluster (e.g. noun occurring in singular or plural form), each component feature feat s in the cluster is translated into the 3-tuple:"
"In the case of a two-dimensional feature cluster (e.g. subject-position noun number vs. verb number agreement), each component feature feat s,t is trans-lated into the 5-tuple: freq(feat s,t |word) , P freq(feat s,t |word) hfreq(feat s,t |word), , freq(word) i,j freq(feat i,j |word) P freq(feat , P freq(feat s,t |word) i s,t |word) i freq(feat i,t |word) j freq(feat s,j |word)"
"In order to extract the features described above, we need some mechanism for detecting NP and PP boundaries, determining subject–verb agreement and deconstructing NPs in order to recover con-juncts and noun-modifier data."
We adopt three ap-proaches.
"First, we use part-of-speech (POS) tagged data and POS-based templates to extract out the nec-essary information."
"Second, we use chunk data to determine NP and PP boundaries, and medium-recall chunk adjacency templates to recover inter-phrasal dependency."
"Third, we fully parse the data and simply read off all necessary data from the de-pendency output."
"With the POS extraction method, we first Penn-tagged the BNC using an fnTBL-based tagger[REF_CITE], training over the Brown and WSJ corpora with some spelling, number and hy-phenation normalisation."
We then lemmatised this data using a version of morph[REF_CITE]customised to the Penn POS tagset.
"Finally, we implemented a range of high-precision, low-recall POS-based templates to extract out the features from the processed data."
"For example, NPs are in many cases recoverable with the following Perl-style reg-ular expression over Penn POS tags: (PDT)* DT (RB|JJ[RS]?|NNS?) * NNS? [ˆN] ."
"For the chunker, we ran fnTBL over the lem-matised tagged data, training[REF_CITE]-style (Tjong[REF_CITE]) chunk-converted versions of the full Brown and WSJ cor-pora."
"For the NP-internal features (e.g. determin-ers, head number), we used the noun chunks directly, or applied POS-based templates locally within noun chunks."
"For inter-chunk features (e.g. subject–verb agreement) , we looked at only adjacent chunk pairs so as to maintain a high level of precision."
"As the full parser, we used RASP[REF_CITE], a robust tag sequence grammar-based parser."
"RASP’s grammatical relation output function provides the phrase structure in the form of lemmatised dependency tuples, from which it is possible to read off the feature information."
"RASP has the advantage that recall is high, although pre-cision is potentially lower than chunking or tagging as the parser is forced into resolving phrase attach-ment ambiguities and committing to a single phrase structure analysis."
"Although all three systems map onto an identi-cal feature space, the feature vectors generated for a given target noun diverge in content due to the dif-ferent feature extraction methodologies."
"In addition, we only consider nouns that occur at least 10 times as head of an NP, causing slight disparities in the target noun type space for the three systems."
"There were sufficient instances found by all three systems for 20,530 common nouns (out of 33,050 for which at least one system found sufficient instances)."
"The classifier design employed in this research is four parallel supervised classifiers, one for each countability class."
"This allows us to classify a sin-gle noun into multiple countability classes, e.g. de-mand is both countable and uncountable."
"Thus, rather than classifying a given target noun accord-ing to the unique most plausible countability class, we attempt to capture its full range of countabilities."
"Note that the proposed classifier design is that which was found[REF_CITE]to be opti-mal for the task, out of a wide range of classifier architectures."
"In order to discourage the classifiers from over-training on negative evidence, we constructed the gold-standard training data from unambiguously negative exemplars and potentially ambiguous pos-itive exemplars."
"That is, we would like classifiers to judge a target noun as not belonging to a given countability class only in the absence of positive ev-idence for that class."
"This was achieved in the case of countable nouns, for instance, by extracting all countable nouns from each of the ALT-J/E and COM-LEX lexicons."
"As positive training exemplars, we then took the intersection of those nouns listed as countable in both lexicons (irrespective of member-ship in alternate countability classes); negative train-ing exemplars, on the other hand, were those con-tained in both lexicons but not classified as count- able in either. 1"
The uncountable gold-standard data was constructed in a similar fashion.
"We used the ALT-J/E lexicon as our source of plural only and bi-partite nouns, using all the instances listed as our positive exemplars."
"The set of negative exemplars was constructed in each case by taking the intersec-tion of nouns not contained in the given countability class in ALT-J/E , with all annotated nouns with non-identical singular and plural forms in COMLEX ."
"Having extracted the positive and negative exem-plar noun lists for each countability class, we filtered out all noun lemmata not occurring in the BNC."
"The final make-up of the gold-standard data for each of the countability classes is listed in Table 2, along with a baseline classification accuracy for each class (“Baseline”), based on the relative fre-quency of the majority class (positive or negative)."
"That is, for bipartite nouns, we achieve a 99.4% clas-sification accuracy by arbitrarily classifying every training instance as negative."
"The supervised classifiers were built using TiMBL version 4.2[REF_CITE], a memory-based classification system based on the k-nearest neighbour algorithm."
"As a result of exten-sive parameter optimisation, we settled on the de-fault configuration for TiMBL with k set to 9. [Footnote_2]"
"2 We additionally experimented with the kernel-based TinySVM system, but found TiMBL to be superior in all cases."
Evaluation is broken down into two components.
"First, we determine the optimal classifier configura-tion for each countability class by way of stratified cross-validation over the gold-standard data."
We then run each classifier in optimised configuration over the remaining target nouns for which we have feature vectors.
"First, we ran the classifiers over the full feature set for the three feature extraction methods."
"In each case, we quantify the classifier performance by way of 10-fold stratified cross-validation over the gold-standard data for each countability class."
The fi-nal classification accuracy and F-score 3 are averaged over the 10 iterations.
"The cross-validated results for each classifier are presented in Table 3, broken down into the differ-ent feature extraction methods."
"For each, in addi-tion to the F-score and classification accuracy, we present the relative error reduction (e.r.) in classifi-cation accuracy over the majority-class baseline for that gold-standard set (see Table 2)."
"For each count-ability class, we additionally ran the classifier over the concatenated feature vectors for the three basic feature extraction methods, producing a [Footnote_3],852-value feature space (“Combined”)."
3 Calculated according to: 2·precisionprecision+·recallrecall
"Given the high baseline classification accuracies for each gold-standard dataset, the most revealing statistics in Table 3 are the error reduction and F-score values."
"In all cases other than bipartite, the combined system outperformed the individual sys-tems."
"The difference in F-score is statistically sig-nificant (based on the two-tailed t-test, p &lt; .05) for the asterisked systems in Table 3."
"For the bipartite class, the difference in F-score is not statistically sig-nificant between any system pairing."
"There is surprisingly little separating the tagger-, chunker- and RASP-based feature extraction meth-ods."
This is largely due to the precision/recall trade-off noted above for the different systems.
"We next turn to the task of classifying all unseen common nouns using the gold-standard data and the best-performing classifier configurations for each countability class (indicated in bold in Table 3). 4 Here, the baseline method is to classify every noun as being uniquely countable."
"Of these, the classifiers were able to classify 10,355 (90.0%): 7,974 (77.0%) as count-able (e.g. alchemist), 2,588 (25.0%) as uncountable (e.g. ingenuity), 9 (0.1%) as bipartite (e.g. head-phones), and 80 (0.8%) as plural only (e.g. dam-ages)."
We evaluated the classifier outputs in two ways.
"In the first, we compared the classifier output to the combined COMLEX and ALT-J/E lexicons: a lexicon with countability information for 63,581 nouns."
"The classifiers found a match for 4,982 of the nouns."
The predicted countability was judged correct 94.6% of the time.
This is marginally above the level of match between ALT-J/E and COMLEX (93.8%) and substan-tially above the baseline of all-countable at 89.7% (error reduction = 47.6%).
"To gain a better understanding of the classifier performance, we analysed the correlation between corpus frequency of a given target noun and its pre-cision/recall for the countable class. [Footnote_5] To do this, we listed the 11,499 unannotated nouns in increas-ing order of corpus occurrence, and worked through the ranking calculating the mean precision and re-call over each partition of 500 nouns."
5 We similarly analysed the uncountable class and found the same basic trend.
"This resulted in the precision–recall graph given in Figure 1, from which it is evident that mean recall is proportional and precision inversely proportional to corpus fre- quency."
"That is, for lower-frequency nouns, the clas-sifier tends to rampantly classify nouns as count-able, while for higher-frequency nouns, the classi-fier tends to be extremely conservative in positively classifying nouns."
"One possible explanation for this is that, based on the training data, the frequency of a noun is proportional to the number of count-ability classes it belongs to."
"Thus, for the more frequent nouns, evidence for alternate countability classes can cloud the judgement of a given classifier."
"In secondary evaluation, the authors used BNC corpus evidence to blind-annotate 100 randomly-selected nouns from the test data, and tested the cor-relation with the system output."
"This is intended to test the ability of the system to capture corpus-attested usages of nouns, rather than independent lexicographic intuitions as are described in the COM-LEX and ALT-J/E lexicons."
"On this set, the baseline of all-countable was 87.8%, and the classifiers gave an agreement of 92.4% (37.7% e.r.), agreement with the dictionaries was also 92.[Footnote_4]%."
"4 In each case, the classifier is run over the best- 500 features as selected by the method described[REF_CITE]rather than the full feature set, purely in the interests of reducing processing time. Based on cross-validated results over the training data, the resultant difference in performance is not statistically significant."
"Again, the main source of errors was the classi-fier only returning a single countability for each noun."
"To put this figure in proper perspective, we also hand-annotated 100 randomly-selected nouns from the training data (that is words in our com-bined lexicon) according to BNC corpus evidence."
"Here, we tested the correlation between the manual judgements and the combined ALT-J/E and COMLEX dictionaries."
"For this dataset, the baseline of all-countable was 80.5%, and agreement with the dic-tionaries was a modest 86.8% (32.3% e.r.)."
"Based on this limited evaluation, therefore, our automated method is able to capture corpus-attested count-abilities with greater precision than a manually-generated static repository of countability data."
The above results demonstrate the utility of the proposed method in learning noun countability from corpus data.
"In the final system configu-ration, the system accuracy was 94.6%, compar-ing favourably with the 78% accuracy reported[REF_CITE], 89.5% of O’[REF_CITE], and also the noun token-based results[REF_CITE]."
At the moment we are merely classifying nouns into the four classes.
The next step is to store the distribution of countability for each target noun and build a representation of each noun’s countability preferences.
"We have made initial steps in this direc-tion, by isolating token instances strongly support-ing a given countability class analysis for that target noun."
We plan to estimate the overall frequency of the different countabilities based on this evidence.
"This would represent a continuous equivalent of the discrete 5-way scale employed in ALT-J/E , tunable to different corpora/domains."
"For future work we intend to: investigate further the relation between meaning and countability, and the possibility of using countability information to prune the search space in word sense disambigua-tion; describe and extract countability-idiosyncratic constructions, such as determinerless PPs and role-nouns; investigate the use of a grammar that distin-guishes between countable and uncountable uses of nouns; and in combination with such a grammar, in-vestigate the effect of lexical rules on countability."
We have proposed a knowledge-rich lexical acqui-sition technique for multi-classifying a given noun according to four countability classes.
"The tech-nique operates over a range of feature clusters draw-ing on pre-processed corpus data, which are then fed into independent classifiers for each of the count-ability classes."
The classifiers were able to selec-tively classify the countability preference of English nouns with a precision of 94.6%.
"Noun extraction is very important for many NLP applications such as informa-tion retrieval, automatic text classification, and information extraction."
Most of the previous Korean noun extraction systems use a morphological analyzer or a Part-of-Speech (POS) tagger.
"Therefore, they require much of the linguistic knowledge such as morpheme dictionaries and rules (e.g. morphosyntactic rules and morpho-logical rules)."
This paper proposes a new noun extrac-tion method that uses the syllable based word recognition model.
It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries.
"Furthermore, it does not require any labor for construct-ing and maintaining linguistic knowledge."
We have performed various experiments with a wide range of variables influenc-ing the performance.
"The experimental results show that without morphological analysis or POS tagging, the proposed method achieves comparable performance with the previous methods."
Noun extraction is a process to find every noun in a document[REF_CITE].
"In Korean, Nouns are used as the most important terms (features) that express the document in NLP applications such as information retrieval, document categorization, text summarization, information extraction, and etc."
Korean is a highly agglutinative language and nouns are included in Eojeols.
An Eojeol is a sur-face level form consisting of more than one com-bined morpheme.
"Therefore, morphological anal-ysis or POS tagging is required to extract Korean nouns."
The previous Korean noun extraction methods are classified into two categories: morphological analy-sis based method[REF_CITE]and POS tagging based method[REF_CITE].
The mor-phological analysis based method tries to generate all possible interpretations for a given Eojeol by implementing a morphological analyzer or a sim-pler method using lexical dictionaries.
It may over-generate or extract inaccurate nouns due to lexical ambiguity and shows a low precision rate.
"Although several studies have been proposed to reduce the over-generated results of the morphological analy-sis by using exclusive informati[REF_CITE], they cannot completely resolve the ambiguity."
The POS tagging based method chooses the most probable analysis among the results produced by the morphological analyzer.
"Due to the resolution of the ambiguities, it can obtain relatively accurate results."
But it also suffers from errors not only produced by a POS tagger but also triggered by the preceding mor-phological analyzer.
"Furthermore, both methods have serious deficien-   (Cheol-Su saw the persons)” cies in that they require considerable manual la-bor to construct and maintain linguistic knowledge and suffer from the unknown word problem."
"If a morphological analyzer fails to recognize an un-known noun in an unknown Eojeol, the POS tagger would never extract the unknown noun."
"Although the morphological analyzer properly recognizes the unknown noun, it would not be extracted due to the sparse data problem."
This paper proposes a new noun extraction method that uses a syllable based word recognition model.
The proposed method does not require labor for constructing and maintaining linguistic knowl-edge and it can also alleviate the unknown word problem or the sparse data problem.
It finds the most probable syllable-tag sequence of the input sentence by using statistical information and extracts nouns by detecting the word boundaries.
The statistical in-formation is automatically acquired from a POS an-notated corpus and the word boundary can be de-tected by using an additional tag to represent the boundary of a word.
This paper is organized as follows.
"In Section 2, the notion of word is defined."
Section 3 presents the syllable based word recognition model.
Section 4 describes the method of constructing the training data from existing POS tagged corpora.
Section 5 discusses experimental results.
"Finally, Section 6 concludes the paper."
"Korean spacing unit is an Eojeol, which is delimited by whitespace, as with word in English."
"In Korean, an Eojeol is made up of one or more words, and a word is made up of one or more morphemes."
"Figure 1 represents the relationships among morphemes, words, and Eojeols with an example sentence."
Syl-lables are delimited by a hyphen in the figure.
All of the previous noun extraction methods re-gard a morpheme as a processing unit.
"In order to extract nouns, nouns in a given Eojeol should be segmented."
"To do this, the morphological analysis has been used, but it requires complicated processes because of the surface forms caused by various mor-phological phenomena such as irregular conjugation of verbs, contraction, and elision."
"Most of the mor-phological phenomena occur at the inside of a mor-pheme or the boundaries between morphemes, not a word."
We have also observed that a noun belongs to a morpheme as well as a word.
"Thus, we do not have to do morphological analysis in the noun extraction point of view."
"In Korean linguistics, a word is defined as a mor-pheme or a sequence of morphemes that can be used independently."
"Even though a postposition is not used independently, it is regarded as a word because it is easily segmented from the preceding word."
This definition is rather vague for computational process-ing.
"If we follow the definition of the word in lin-guistics, it would be difficult to analyze a word like the morphological analysis."
"For this reason, we de-fine a different notion of a word."
"According to our definition of a word, each un-inflected morpheme or a sequence of successive inflected morphemes is regarded as an individual word. 1"
"By virtue of the new definition of a word, we need not consider mismatches between the sur-face level form and the lexical level one in recogniz-ing words."
"The example sentence “ (Cheol-Su saw the persons)” represented in Fig-ure 1 includes six words such as “ “ (neun)”, “  (sa-lam)”, “ (deul)”(Cheol-Su, “ (eul))””,, and “  (bwass-da)”."
"Unlike the Korean linguis-tics, a noun suffix such as “ (nim)”, “ (deul)”, or “ (jeog)” is also regarded as a word because it is an uninflected morpheme."
"A Korean syllable consists of an obligatory onset (initial-grapheme, consonant), an obligatory peak (nuclear grapheme, vowel), and an optional coda (final-grapheme, consonant)."
"In theory, the number of syllables that can be used in Korean is the same as the number of every combination of the graphemes. 2"
"Fortunately, only a fixed number of syllables is frequently used in practice. 3 The amount of in-formation that a Korean syllable has is larger than that of an alphabet in English."
"In addition, there are particular characteristics in Korean syllables."
The fact that words do not start with certain syllables is one of such examples.
Several attempts have been made to use characteristics of Korean sylla-bles.
Syllable statistics have been also used for automatic word spacing[REF_CITE].
The syllable based word recognition model is rep-resented as a function like the following equations.
"It is to find the most probable syllable-tag sequence   , for a given sentence consist-ing of a sequence of syllables   .     ([Footnote_1])   ½   ([Footnote_2]) ½  "
"1 Korean morphemes can be classified into two types: un-inflected morphemes having fixed word forms (such as noun, unconjugated adjective, postposition, adverb, interjection, etc.) and inflected morphemes having conjugated word forms (such as a morpheme with declined or conjugated endings, predicative postposition, etc.)"
2 ( ) of pure Korean syllables are pos-sible
Two Markov assumptions are applied in Equation 2.
One is that the probability of a current syllable tag conditionally depends on only the previous sylla-ble tag.
The other is that the probability of a cur-rent syllable conditionally depends on the current tag.
"In order to reflect word spacing information in Equation 2, which is very useful in Korean POS tag-ging, Equation 2 is changed to Equation 3 which can consider the word spacing information by calculat-ing the transition probabilities like the equation used[REF_CITE].     ([Footnote_3]) ½  "
"3 Actually, of syllables are used in the training data, including Korean characters and non-Korean characters (e.g. al-phabets, digits, Chinese characters, symbols)."
"In the equation, becomes zero if the transition oc-curs in the inside of an Eojeol; otherwise is one."
Word boundaries can be detected by an additional tag.
This method has been used in some tasks such as text chunking and named entity recognition to represent a boundary of an element (e.g. individual phrase or named entity).
There are several possi-ble representation schemes to do this.
"The simplest one is the BIO representation scheme[REF_CITE], where a “B” denotes the first item of an element and an “I” any non-initial item, and a syllable with tag “O” is not a part of any element."
"Because every syllable corresponds to one syllable tag, “O” is not used in our task."
The representation schemes used in this paper are described in detail in Section 4.
The probabilities in Equation 3 are estimated by the maximum likelihood estimator (MLE) using rel-ative frequencies in the training data. [Footnote_4]
"4 Since the MLE suffers from zero probability, to avoid zero probability, we just assign a very low value such as ½¼¼ for an unseen event in the training data."
The most probable sequence of syllable tags in a sentence (a sequence of syllables) can be efficiently computed by using the Viterbi algorithm.
"Given a sequence of syllables and syllable tags, it is straightforward to obtain the corresponding se-quence of words and word tags."
"Among the words recognized through this process, we can extract nouns by just selecting words tagged as nouns. [Footnote_5]"
"5 For the purpose of noun extraction, we only select com-mon nouns here (tagged as “nc” or “NC”) among other kinds of nouns."
"Our model is a supervised learning approach, so it requires a training data."
"Because the existing Korean POS tagged corpora are annotated by a morpheme level, we cannot use them as a training data without converting the data suitable for the word recognition model."
The corpus can be modified through the fol-lowing steps:
"Step 1 For a given Eojeol, segment word bound-aries and assign word tags to each word."
"Step 2 For each separated word, assign the word tag to each syllable in the word according to one of the representations."
"In step 1, word boundaries are identified by using the information of an uninflected morpheme and a sequence of successive inflected morphemes."
An uninflected morpheme becomes one word and its tag is assigned to the morpheme’s tag.
Successive inflected morphemes form a word and the combined form of the first and the last morpheme’s tag repre-tagged form of the Eojeol “  (gass-eoss-da)” sents its tag.
"For example, the morpheme-unit POS is “ (ga)/pv+  (ass)/ep+  (eoss)/ep+  (da)/ef”, the Eojeol “  (gass-eoss-da)” becomes one and all of them are inflected morphemes."
"Hence, word and its tag is represented as “pv ef” by using the first morpheme’s tag (“pv”) and the last one’s (“ef”)."
"In step 2, a syllable tag is assigned to each of syl-lables forming a word."
The syllable tag should ex-press not only POS tag but also the boundary of the word.
"In order to detect the word boundaries, we use the following four representation schemes:"
"BI representation scheme Assign “B” tag to the first syllable of a word, and “I” tag to the others."
"BIS representation scheme Assign “S” tag to a syllable which forms a word, and other tags (“B” and “I”) are the same as “BI” represen-tation scheme."
"IE representation scheme Assign “E” tag to the last syllable of a word, and “I” tag to the others."
"IES representation scheme Assign “S” tag to a syllable which forms a word, and other tags (“I” and “E”) are the same as “IE” represen-tation scheme."
Table 1 shows an example of assigning word tag by syllable unit to the morpheme unit POS tagged corpus.
"We used ETRI POS tagged corpus of 288,269 Eojoels for testing and the 21st Century Sejong Project’s POS tagged corpus (Sejong corpus, for short) for training."
The Sejong corpus consists of three different corpora acquired from 1999 to 2001.
The Sejong corpus of 1999 consists of 1.5 million Eojeols and other two corpora have 2 million Eo-jeols respectively.
"The evaluation measures for the noun extraction task are recall, precision, and F-measure."
They measure the performance by docu-ment and are averaged over all the test documents.
This is because noun extractors are usually used in the fields of applications such as information re-trieval (IR) and document categorization.
"We also consider the frequency of nouns; that is, if the noun frequency is not considered, a noun occurring twice or more in a document is treated as other nouns oc-curring once."
"From IR point of view, this takes into account of the fact that even if a noun is extracted just once as an index term, the document including the term can also be retrieved."
"The performance considerably depends on the following factors: the representation schemes for word boundary detection, the tagset, the amount of training data, and the difference between training data and test data."
"First, we compare four different representation schemes (BI, BIS, IE, IES) in word boundary de-tection as explained in Section 4."
We try to use the following three kinds of tagsets in order to select the most optimal tagset through the experiments:
Tagset 1 Simply use two tags (e.g. noun and non-noun).
"This is intended to examine the syllable characteristics; that is, which syllables tend to belong to nouns or not."
Tagset 2 Use the tagset used in the training data without modification.
ETRI tagset used for training is relatively smaller than that of other tagsets.
This tagset is changeable according to the POS tagged corpus used in training.
Tagset 3 Use a simplified tagset for the purpose of noun extraction.
"This tagset is simplified by combining postpositions, adverbs, and verbal suffixes into one tag, respectively."
This tagset is always fixed even in a different training corpus.
Tagset 2 used in Section 5.2 and Tagset 3 are rep-resented in Table 2.
We divided the test data into ten parts.
The perfor-mances of the model are measured by averaging over the ten test sets in the 10-fold cross-validation exper-iment.
Table 3 shows experimental results according to each representation scheme and tagset.
"In the first column, each number denotes the tagset used."
"When it comes to the issue of frequency, the cases of con-sidering frequency are better for precision but worse for recall, and better for F-measure."
"The representa-tion schemes using single syllable information (e.g. “BIS”, “IES”) are better than other representation schemes (e.g. “BI”, “IE”)."
"Contrary to our expec-tation, the results of Tagset 2 consistently outper-form other tagsets."
The results of Tagset 1 are not as good as other tagsets because of the lack of the syntactic context.
"Nevertheless, the results reflect the usefulness of the syllable based processing."
The changes of the F-measure according to the tagsets and the representation schemes reflecting frequency are shown in Figure 2.
"To show the influence of the difference between the training data and the test data, we have performed the experiments on the Sejong corpus as a training data and the entire ETRI corpus as a test data."
Table 4 shows the experimental results on all of the three training data.
"Although more training data are used in this experiment, the results of Table 3 shows bet-ter outcomes."
"Like other POS tagging models, this indicates that our model is dependent on the text do-main."
Figure 3 shows the changes of the F-measure ac-cording to the size of the training data.
"In this fig-ure, “99-2000” means 1999 corpus and 2000 cor-pus are used, and “99-2001” means all corpora are used as the training data."
"The more training data are used, the better performance we obtained."
"However, the improvement is insignificant in considering the amount of increase of the training data."
Results reported[REF_CITE]are pre-sented in Table 5.
The experiments were performed on the same condition as that of our experiments.
KOMA[REF_CITE]is a general-purpose morphological an-alyzer.
"HanTag[REF_CITE]is a POS tagger, which takes the result of KOMA as input."
"Accord-ing to Table 5, HanTag, which is a POS tagger, is an optimal tool in performing noun extraction in terms of the precision and the F-measure."
"Although the best performance of our proposed model (BIS-2) is worse than HanTag, it is better than[REF_CITE]and KOMA."
"As mentioned earlier, we assume that morphologi-cal variations do not occur at any inflected words."
"However, some exceptions might occur in a col-of two Eojeols “  (ddai)+ (neun)” and “ loquial text."
"For example, the lexical level forms (go-gai)+ (leul)” are changed into the surface level forms by contractions such as “  (ddain)” and “ (go-gail)”, respectively."
Our models alone cannot deal with these cases.
"Such exceptions, however, are very rare. [Footnote_6] In these experiments, we do not perform any post-processing step to deal with such excep-tions."
"6 Actually, about 0.145% of nouns in the test data belong to these cases."
We have presented a word recognition model for ex-tracting nouns.
"While the previous noun extraction methods require morphological analysis or POS tag-ging, our noun extraction method only uses the syl-lable information without using any additional mor-phological analyzer."
This means that our method does not require any dictionary or linguistic knowl-edge.
"Therefore, without manual labor to construct and maintain those resources, our method can ex-tract nouns by using only the statistics, which can be automatically extracted from a POS tagged corpus."
"The previous noun extraction methods take a mor-pheme as a processing unit, but we take a new notion of word as a processing unit by considering the fact that nouns belong to uninflected morphemes in Ko-rean."
"By virtue of the new definition of a word, we need not consider mismatches between the surface level form and the lexical level one in recognizing words."
"We have performed various experiments with a wide range of variables influencing the performance such as the representation schemes for the word boundary detection, the tag set, the amount of train-ing data, and the difference between the training data and the test data."
"Without morphological analysis or POS tagging, the proposed method achieves compa-rable performance compared with the previous ones."
"In the future, we plan to extend the context to im-prove the performance."
"Although the word recognition model is designed to extract nouns in this paper, the model itself is meaningful and it can be applied to other fields such as language modeling and automatic word spacing."
"Furthermore, our study make some contributions in the area of POS tagging research."
"This paper describes two methods for de-tecting word segments and their morpho-logical information in a Japanese sponta-neous speech corpus, and describes how to tag a large spontaneous speech corpus accurately by using the two methods."
The first method is used to detect any type of word segments.
"The second method is used when there are several definitions for word segments and their POS categories, and when one type of word segments in-cludes another type of word segments."
"In this paper, we show that by using semi-automatic analysis we achieve a precision of better than 99% for detecting and tag-ging short words and 97% for long words; the two types of words that comprise the corpus."
We also show that better accuracy is achieved by using both methods than by using only the first.
"The “Spontaneous Speech: Corpus and Process-ing Technology” project is sponsoring the construc-tion of a large spontaneous Japanese speech corpus, Corpus of Spontaneous Japanese (CSJ)[REF_CITE]."
"The CSJ is a collection of mono-logues and dialogues, the majority being mono-logues such as academic presentations and simu-lated public speeches."
Simulated public speeches are short speeches presented specifically for the cor-pus by paid non-professional speakers.
The CSJ in- cludes transcriptions of the speeches as well as audio recordings of them.
One of the goals of the project is to detect two types of word segments and cor-responding morphological information in the tran-scriptions.
The two types of word segments were defined by the members of The National Institute for Japanese Language and are called short word and long word.
"The term short word approximates a dic-tionary item found in an ordinary Japanese dictio-nary, and long word represents various compounds."
"The length and part-of-speech (POS) of each are dif-ferent, and every short word is included in a long word, which is shorter than a Japanese phrasal unit, a bunsetsu."
"If all of the short words in the CSJ were detected, the number of the words would be approximately seven million."
That would be the largest spontaneous speech corpus in the world.
"So far, approximately one tenth of the words have been manually detected, and morphological information such as POS category and inflection type have been assigned to them."
"Human annotators tagged every morpheme in the one tenth of the CSJ that has been tagged, and other annotators checked them."
The hu-man annotators discussed their disagreements and resolved them.
"The accuracies of the manual tagging of short and long words in the one tenth of the CSJ were greater than 99.8% and 97%, respectively."
The accuracies were evaluated by random sampling.
"As it took over two years to tag one tenth of the CSJ ac-curately, tagging the remainder with morphological information would take about twenty years."
"There-fore, the remaining nine tenths of the CSJ must be tagged automatically or semi-automatically."
"In this paper, we describe methods for detecting the two types of word segments and corresponding morphological information."
We also describe how to tag a large spontaneous speech corpus accurately.
"Henceforth, we call the two types of word segments short word and long word respectively, or merely morphemes."
We use the term morphological anal-ysis for the process of segmenting a given sentence into a row of morphemes and assigning to each mor-pheme grammatical attributes such as a POS cate-gory.
"As we mentioned in Section 1, tagging the whole of the CSJ manually would be difficult."
"Therefore, we are taking a semi-automatic approach."
"This section describes major problems in tagging a large sponta-neous speech corpus with high precision in a semi-automatic way, and our solutions to those problems."
"One of the most important problems in morpho-logical analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus."
Two statistical approaches have been applied to this problem.
"One is to find un-known words from corpora and put them into a dictionary (e.g.,[REF_CITE]), and the other is to estimate a model that can identify un-known words correctly (e.g.,[REF_CITE])."
Uchimoto et al. used both ap-proaches.
They proposed a morphological analysis method based on a maximum entropy (ME) model[REF_CITE].
"Their method uses a model that estimates how likely a string is to be a mor-pheme as its probability, and thus it has a potential to overcome the unknown word problem."
"Therefore, we use their method for morphological analysis of the CSJ."
"However, Uchimoto et al. reported that the accuracy of automatic word segmentation and POS tagging was 94 points in F-measure[REF_CITE]."
That is much lower than the accuracy ob-tained by manual tagging.
Several problems led to this inaccuracy.
"In the following, we describe these problems and our solutions to them. • Fillers and disfluencies Fillers and disfluencies are characteristic ex-pressions often used in spoken language, but they are randomly inserted into text, so detect-ing their segmentation is difficult."
"In the CSJ, they are tagged manually."
"Therefore, we first delete fillers and disfluencies and then put them back in their original place after analyzing a text. • Accuracy for unknown words The morpheme model that will be described in Section 3.1 can detect word segments and their POS categories even for unknown words."
"However, the accuracy for unknown words is lower than that for known words."
"One of the solutions is to use dictionaries developed for a corpus on another domain to reduce the num-ber of unknown words, but the improvement achieved is slight[REF_CITE]."
"We believe that the reason for this is that defini-tions of a word segment and its POS category depend on a particular corpus, and the defi-nitions from corpus to corpus differ word by word."
"Therefore, we need to put only words extracted from the same corpus into a dictio-nary."
We are manually examining words that are detected by the morpheme model but that are not found in a dictionary.
We are also manually examining those words that the mor-pheme model estimated as having low proba-bility.
"During the process of manual exami-nation, if we find words that are not found in a dictionary, those words are then put into a dictionary."
Section 4.2.1 will describe the ac-curacy of detecting unknown words and show how much those words contribute to improving the morphological analysis accuracy when they are detected and put into a dictionary. • Insufficiency of features The model currently used for morphological analysis considers the information of a target morpheme and that of an adjacent morpheme on the left.
"To improve the model, we need to consider the information of two or more mor-phemes on the left of the target morpheme."
"However, too much information often leads to overtraining the model."
Using all the informa-tion makes training the model difficult when there is too much of it.
"Therefore, the best way to improve the accuracy of the morpholog-ical information in the CSJ within the limited time available to us is to examine and revise the errors of automatic morphological analysis and to improve the model."
"We assume that the smaller the probability estimated by a model for an output morpheme is, then the greater the likelihood is that the output morpheme is wrong."
"Therefore, we examine output mor-phemes in ascending order of their probabili-ties."
The expected improvement of the accu-racy of the morphological information in the whole of the CSJ will be described in Sec-tion 4.2.1
Another problem concerning unknown words is that the cost of manual examination is high when there are several definitions for word seg-ments and their POS categories.
"Since there are two types of word definitions in the CSJ, the cost would double."
"Therefore, to reduce the cost, we propose another method for detecting word segments and their POS categories."
"The method will be described in Section 3.2, and the advantages of the method will be described in Section 4.2.2"
The next problem described here is one that we have to solve to make a language model for auto-matic speech recognition. logical analysis is the basic form of the CSJ and it does not have information on actual pro- nunciation.
"The result of morphological anal-ysis, therefore, is a row of morphemes that do not have information on actual pronuncia-tion."
To estimate actual pronunciation by using only the basic form and a dictionary is impossi-ble.
"Therefore, actual pronunciation is assigned to results of morphological analysis by align-ing the basic form and pronunciation in the CSJ."
"First, the results of morphological anal-ysis, namely, the morphemes, are transliterated into katakana characters by using a dictionary, and then they are aligned with pronunciation in the CSJ by using a dynamic programming method."
"In this paper, we will mainly discuss methods for detecting word segments and their POS categories in the whole of the CSJ."
This section describes two methods for detecting word segments and their POS categories.
The first method uses morpheme models and is used to detect any type of word segment.
The second method uses a chunking model and is only used to detect long word segments.
"Given a tokenized test corpus, namely a set of strings, the problem of Japanese morphological analysis can be reduced to the problem of assign-ing one of two tags to each string in a sentence."
A string is tagged with a 1 or a 0 to indicate whether it is a morpheme.
"When a string is a morpheme, a grammatical attribute is assigned to it."
"A tag desig-nated as a 1 is thus assigned one of a number, n, of grammatical attributes assigned to morphemes, and the problem becomes to assign an attribute (from 0 to n) to every string in a given sentence."
We define a model that estimates the likelihood that a given string is a morpheme and has a gram-matical attribute i(1 ≤ i ≤ n) as a morpheme model.
We implemented this model within an ME modeling framework[REF_CITE].
"The model is represented by Eq. (1):  exp i,j λ i,j g i,j (a,b) p λ (a|b) = (1) Z λ (b)"
"Z λ (b) = exp λ i,j g i,j (a, b) , (2) a i,j where a is one of the categories for classification, and it can be one of (n + 1) tags from 0 to n (This is called a “future.”), b is the contextual or condition-ing information that enables us to make a decision among the space of futures (This is called a “his-tory.”), and Z λ (b) is a normalizing constant deter-mined by the requirement that a p λ (a|b) = 1 for all b."
The computation of p λ (a|b) in any ME model is dependent on a set of “features” which are binary functions of the history and future.
"For instance, one of our features is 1 : if has(b, f j ) = 1 &amp; a = a i g i,j (a, b) = f j = “POS(−1)(Major) : verb,  (3) 0 : otherwise."
"Here “has(b,f j )” is a binary function that returns 1 if the history b has feature f j ."
The features used in our experiments are described in detail in Sec-tion 4.1.1.
"Given a sentence, probabilities of n tags from 1 to n are estimated for each length of string in that sentence by using the morpheme model."
"From all possible division of morphemes in the sentence, an optimal one is found by using the Viterbi algorithm."
"Each division is represented as a particular division of morphemes with grammatical attributes in a sen-tence, and the optimal division is defined as a di-vision that maximizes the product of the probabil-ities estimated for each morpheme in the division."
"For example, the sentence “ 形態素解析についてお 話いたします ” in basic form as shown in Fig. 1 is analyzed as shown in Fig. 2. “ 形態素解析 ” is ana-lyzed as three morphemes, “ 形態 (noun)”, “ 素 (suf-fix)”, and “ 解析 (noun)”, for short words, and as one morpheme, “ 形態素解析 (noun)” for long words."
"In conventional models (e.g.,[REF_CITE]), probabilities were estimated for candidate morphemes that were found in a dic-tionary or a corpus and for the remaining strings obtained by eliminating the candidate morphemes from a given sentence."
"Therefore, unknown words were apt to be either concatenated as one word or di-vided into both a combination of known words and a single word that consisted of more than one char-acter."
"However, this model has the potential to cor-rectly detect any length of unknown words."
The model described in this section can be applied when several types of words are defined in a cor-pus and one type of words consists of compounds of other types of words.
"In the CSJ, every long word consists of one or more short words."
"Our method uses two models, a morpheme model for short words and a chunking model for long words."
"After detecting short word segments and their POS categories by using the former model, long word segments and their POS categories are de-tected by using the latter model."
"We define four la-bels, as explained below, and extract long word seg-ments by estimating the appropriate labels for each short word according to an ME model."
The four la-bels are listed below:
"Ba: Beginning of a long word, and the POS cat-egory of the long word agrees with the short word."
"Ia: Middle or end of a long word, and the POS cat-egory of the long word agrees with the short word."
"B: Beginning of a long word, and the POS category of the long word does not agree with the short word."
"I: Middle or end of a long word, and the POS cat-egory of the long word does not agree with the short word."
A label assigned to the leftmost constituent of a long word is “Ba” or “B”.
"Labels assigned to other con-stituents of a long word are “Ia”, or “I”."
"For exam-ple, the short words shown in Fig. 2 are labeled as shown in Fig. 3."
The labeling is done deterministi-cally from the beginning of a given sentence to its end.
The label that has the highest probability as es-timated by an ME model is assigned to each short word.
The model is represented by Eq. (1).
"In Eq. (1), a can be one of four labels."
The features used in our experiments are described in Section 4.1.2.
"When a long word that does not include a short word that has been assigned the label “Ba” or “Ia”, this indicates that the word’s POS category differs from all of the short words that constitute the long word."
Such a word must be estimated individually.
"In this case, we estimate the POS category by us-ing transformation rules."
"The transformation rules are automatically acquired from the training corpus by extracting long words with constituents, namely short words, that are labeled only “B” or “I”."
A rule is constructed by using the extracted long word and the adjacent short words on its left and right.
"For example, the rule shown in Fig. 4 was acquired in our experiments."
"The middle division of the con-sequent part represents a long word “ てみ ” (auxil-iary verb), and it consists of two short words “ て ” (post-positional particle) and “ み ” (verb)."
"If several different rules have the same antecedent part, only the rule with the highest frequency is chosen."
"If no rules can be applied to a long word segment, rules are generalized in the following steps. 1."
Delete posterior context 2.
Delete anterior and posterior contexts 3.
Delete anterior and posterior contexts and lexi-cal entries.
"If no rules can be applied to a long word segment in any step, the POS category noun is assigned to the long word."
"In our experiments, we used 744,204 short words and 618,538 long words for training, and 63,037 short words and 51,796 long words for testing."
Those words were extracted from one tenth of the CSJ that already had been manually tagged.
The training corpus consisted of 319 speeches and the test corpus consisted of 19 speeches.
"Transcription consisted of basic form and pronun-ciation, as shown in Fig. 1. Speech sounds were faithfully transcribed as pronunciation, and also rep-resented as basic forms by using kanji and hiragana characters."
Lines beginning with numerical digits are time stamps and represent the time it took to produce the lines between that time stamp and the next time stamp.
Each line other than time stamps represents a bunsetsu.
"In our experiments, we used only the basic forms."
"Basic forms were tagged with several types of labels such as fillers, as shown in Table 1."
Strings tagged with those labels were han-dled according to rules as shown in the rightmost columns in Table 1.
"Since there are no boundaries between sentences in the corpus, we selected the places in the CSJ that 



"
"In the CSJ, bunsetsu boundaries, which are phrase boundaries in Japanese, were manually detected."
Fillers and disfluencies were marked with the labels (F) and (D).
"In the experiments, we eliminated fillers and disfluencies but we did use their positional infor-mation as features."
"We also used as features, bun-setsu boundaries and the labels (M), (O), (R), and (A), which were assigned to particular morphemes such as personal names and foreign words."
"Thus, the input sentences for training and testing were charac-ter strings without fillers and disfluencies, and both boundary information and various labels were at-tached to them."
"Given a sentence, for every string within a bunsetsu and every string appearing in a dictionary, the probabilities of a in Eq. (1) were es- timated by using the morpheme model."
"The output was a sequence of morphemes with grammatical at-tributes, as shown in Fig. 2."
We used the POS cate-gories in the CSJ as grammatical attributes.
We ob-tained 14 major POS categories for short words and 15 major POS categories for long words.
"Therefore, a in Eq. (1) can be one of 15 tags from 0 to 14 for short words, and it can be one of 16 tags from 0 to 15 for long words. our experiments are listed in Table 2."
"Each feature consists of a type and a value, which are given in the rows of the table, and it corresponds to j in the func-tion g i,j (a,b) in Eq. (1)."
The notations “(0)” and “(-1)” used in the feature-type column in Table 2 re-spectively indicate a target string and the morpheme to the left of it.
The terms used in the table are ba-sically as same as those that Uchimoto et al. used[REF_CITE].
The main difference is the following one:
Boundary: Bunsetsu boundaries and positional in-formation of labels such as fillers. “(Begin-ning)” and “(End)” in Table 2 respectively indi-cate whether the left and right side of the target strings are boundaries.
We used only those features that were found three or more times in the training corpus.
"We used the following information as features on the target word: a word and its POS cate-gory, and the same information for the four clos-est words, the two on the left and the two on the right of the target word."
Bigram and tri-gram words that included a target word plus bigram and trigram POS categories that included the tar-get word’s POS category were used as features.
"In addition, bunsetsu boundaries as described in Sec-tion 4.1.1 were used."
"For example, when a target word was “ に ” in Fig. 3, “ 素 ”, “ 解析 ”, “ に ”, “ つ い ”, “ て ”, “Suffix”, “Noun”, “PPP”, “Verb”, “PPP”, “ 解析 &amp; に ”, “ に &amp; つい ”, “ 素 &amp; 解析 &amp; に ”, “ に &amp; つい &amp; て ”, “Noun&amp;PPP”, “PPP&amp;Verb”, “Suf-fix&amp;Noun&amp;PPP”, “PPP&amp;Verb&amp;PPP”, and “Bun-setsu(Beginning)” were used as features."
Results of the morphological analysis obtained by using morpheme models are shown in Table 3 and 4.
"In these tables, OOV indicates Out-of-Vocabulary rates."
"Shown in Table 3, OOV was calculated as the proportion of words not found in a dictionary to all words in the test corpus."
"In Table 4, OOV was cal-culated as the proportion of word and POS category pairs that were not found in a dictionary to all pairs in the test corpus."
Recall is the percentage of mor-phemes in the test corpus for which the segmentation and major POS category were identified correctly.
Precision is the percentage of all morphemes identi-fied by the system that were identified correctly.
The F-measure is defined by the following equation.
Tables 3 and 4 show that accuracies would im-prove significantly if no words were unknown.
This indicates that all morphemes of the CSJ could be an-alyzed accurately if there were no unknown words.
The improvements that we can expect by detecting unknown words and putting them into dictionaries are about 1.5 in F-measure for detecting word seg-ments of short words and 2.5 for long words.
"For de-tecting the word segments and their POS categories, for short words we expect an improvement of about 2 in F-measure and for long words 3."
"Next, we discuss accuracies obtained when un-known words existed."
The OOV for long words was 4% higher than that for short words.
"In gen-eral, the higher the OOV is, the more difficult de-tecting word segments and their POS categories is."
"However, the difference between accuracies for short and long words was about 1% in recall and 2% in precision, which is not significant when we consider that the difference between OOVs for short and long words was 4%."
"This result indi-cates that our morpheme models could detect both known and unknown words accurately, especially long words."
"Therefore, we investigated the recall of unknown words in the test corpus, and found that 55.7% (928/1,667) of short word segments and 74.1% (2,660/3,590) of long word segments were detected correctly."
"In addition, regarding unknown words, we also found that 47.5% (791/1,667) of short word segments plus their POS categories and 67.3% (2,415/3,590) of long word segments plus their POS categories were detected correctly."
The recall of unknown words was about 20% higher for long words than for short words.
We believe that this result mainly depended on the difference be-tween short words and long words in terms of the definitions of compound words.
A compound word is defined as one word when it is based on the def-inition of long words; however it is defined as two or more words when it is based on the definition of short words.
"Furthermore, based on the definition of short words, a division of compound words depends on its context."
More information is needed to pre-cisely detect short words than is required for long words.
"Next, we extracted words that were detected by the morpheme model but were not found in a dic-tionary, and investigated the percentage of unknown words that were completely or partially matched to the extracted words by their context."
"This percent-age was 77.6% (1,293/1,667) for short words, and 80.6% (2,892/3,590) for long words."
Most of the re-maining unknown words that could not be detected by this method are compound words.
"We expect that these compounds can be detected during the manual examination of those words for which the morpheme model estimated a low probability, as will be shown later."
"The recall of unknown words was lower than that of known words, and the accuracy of automatic mor-phological analysis was lower than that of manual morphological analysis."
"As previously stated, to improve the accuracy of the whole corpus we take a semi-automatic approach."
"We assume that the smaller the probability is for an output morpheme estimated by a model, the more likely the output morpheme is wrong, and we examine output mor-phemes in ascending order of their probabilities."
We investigated how much the accuracy of the whole corpus would increase.
Fig. 5 shows the relation-ship between the percentage of output morphemes whose probabilities exceed a threshold and their precision.
"In this figure, “short without UKW”, “long without UKW 」 ”, “short with UKW”, and “long with UKW” represent the precision for short words detected assuming there were no unknown words, precision for long words detected assuming there were no unknown words, precision of short words including unknown words, and precision of long words including unknown words, respectively."
"When the output rate in the horizontal axis in-creases, the number of low-probability morphemes increases."
"In all graphs, precisions monotonously decrease as output rates increase."
This means that tagging errors can be revised effectively when mor-phemes are examined in ascending order of their probabilities.
"Next, we investigated the relationship between the percentage of morphemes examined manually and the precision obtained after detected errors were re-vised."
The result is shown in Fig. 6.
Precision represents the precision of word segmentation and POS tagging.
"If unknown words were detected and put into a dictionary by the method described in the fourth paragraph of this section, the graph line for short words would be drawn between the graph lines “short without UKW” and “short with UKW”, and the graph line for long words would be drawn be-tween the graph lines “long without UKW” and “long with UKW”."
"Based on test results, we can expect better than 99% precision for short words and better than 97% precision for long words in the whole corpus when we examine 10% of output mor-"
Figure 6: Relationship between the percentage of morphemes examined manually and precision ob-tained after revising detected errors (when mor-phemes with probabilities under threshold and their adjacent morphemes are examined). phemes in ascending order of their probabilities.
"Finally, we investigated the relationship between percentage of morphemes examined manually and the error rate for all of the examined morphemes."
The result is shown in Fig. 7.
We found that about 50% of examined morphemes would be found as er-rors at the beginning of the examination and about 20% of examined morphemes would be found as errors when examination of 10% of the whole cor-pus was completed.
"When unknown words were de-tected and put into a dictionary, the error rate de-creased; even so, over 10% of examined morphemes would be found as errors."
Results of the morphological analysis of long words obtained by using a chunking model are shown in Table 5 and 6.
The first and second lines Table 5: Accuracies of long word segmentation. show the respective accuracies obtained when OOVs were 5.81% and 6.93%.
The third lines show the ac-curacies obtained when we assumed that the OOV for short words was 0% and there were no errors in detecting short word segments and their POS cate-gories.
The fourth line in Table 6 shows the accuracy obtained when a chunking model without transfor-mation rules was used.
"The accuracy obtained by using the chunking model was one point higher in F-measure than that obtained by using the morpheme model, and it was very close to the accuracy achieved for short words."
"This result indicates that errors newly produced by applying a chunking model to the results obtained for short words were slight, or errors in the results obtained for short words were amended by apply-ing the chunking model."
This result also shows that we can achieve good accuracy for long words by ap-plying a chunking model even if we do not detect unknown long words and do not put them into a dic-tionary.
"If we could improve the accuracy for short words, the accuracy for long words would be im-proved also."
The third lines in Tables 5 and 6 show that the accuracy would improve to over 98 points in F-measure.
The fourth line in Tables 6 shows that transformation rules significantly contributed to im-proving the accuracy.
"Considering the results obtained in this section and in Section 4.2.1, we are now detecting short and long word segments and their POS categories in the whole corpus by using the following steps: 1. Automatically detect and manually examine unknown words for short words. 2. Improve the accuracy for short words in the whole corpus by manually examining short words in ascending order of their probabilities estimated by a morpheme model. 3. Apply a chunking model to the short words to detect long word segments and their POS cate-gories."
"As future work, we are planning to use an active learning method such as that proposed by Argamon-Engelson and Dagan[REF_CITE]to more effectively improve the accuracy of the whole corpus."
"This paper described two methods for detecting word segments and their POS categories in a Japanese spontaneous speech corpus, and describes how to tag a large spontaneous speech corpus accu-rately by using the two methods."
The first method is used to detect any type of word segments.
We found that about 80% of unknown words could be semi-automatically detected by using this method.
"The second method is used when there are several defi-nitions for word segments and their POS categories, and when one type of word segments includes an-other type of word segments."
We found that better accuracy could be achieved by using both methods than by using only the first method alone.
"Two types of word segments, short words and long words, are found in a large spontaneous speech corpus, CSJ."
"We found that the accuracy of auto-matic morphological analysis for the short words was 95.79 in F-measure and for long words, 95.49."
"Although the OOV for long words was much higher than that for short words, almost the same accuracy was achieved for both types of words by using our proposed methods."
"We also found that we can ex-pect more than 99% of precision for short words, and 97% for long words found in the whole corpus when we examined 10% of output morphemes in as-cending order of their probabilities as estimated by the proposed models."
"In our experiments, only the information con-tained in the corpus was used; however, more appro-priate linguistic knowledge than that could be used, such as morphemic and syntactic rules."
We would like to investigate whether such linguistic knowl-edge contributes to improved accuracy.
"We train a decision tree inducer (CART) and a memory-based classifier (MBL) on predicting prosodic pitch accents and breaks in Dutch text, on the basis of shal-low, easy-to-compute features."
We train the algorithms on both tasks individu-ally and on the two tasks simultaneously.
"The parameters of both algorithms and the selection of features are optimized per task with iterative deepening, an efficient wrapper procedure that uses progressive sampling of training data."
"Results show a consistent significant advantage of MBL over CART, and also indicate that task combination can be done at the cost of little generalization score loss."
"Tests on cross-validated data and on held-out data yield F-scores of MBL on accent place-ment of 84 and 87, respectively, and on breaks of 88 and 91, respectively."
"Accent placement is shown to outperform an in-formed baseline rule; reliably predicting breaks other than those already indicated by intra-sentential punctuation, however, appears to be more challenging."
Any text-to-speech (TTS) system that aims at pro-ducing understandable and natural-sounding out-put needs to have on-board methods for predict-ing prosody.
"Most systems start with generating a prosodic representation at the linguistic or sym-bolic level, followed by the actual phonetic real-ization in terms of (primarily) pitch, pauses, and segmental durations."
The first step involves plac-ing pitch accents and inserting prosodic boundaries at the right locations (and may involve tune choice as well).
Pitch accents correspond roughly to pitch movements that lend emphasis to certain words in an utterance.
"Prosodic breaks are audible interrup-tions in the flow of speech, typically realized by a combination of a pause, a boundary-marking pitch movement, and lengthening of the phrase-final seg-ments."
Errors at this level may impede the listener in the correct understanding of the spoken utterance[REF_CITE].
"Predicting prosody is known to be a hard problem that is thought to require informa-tion on syntactic boundaries, syntactic and seman-tic relations between constituents, discourse-level knowledge, and phonological well-formedness con-straints[REF_CITE]."
"However, producing all this information – using full parsing, including es-tablishing semanto-syntactic relations, and full dis-course analysis – is currently infeasible for a real-time system."
Resolving this dilemma has been the topic of several studies in pitch accent placement[REF_CITE]and in prosodic boundary placement[REF_CITE].
"The commonly adopted solution is to use shallow infor-mation sources that approximate full syntactic, se-mantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) infor-mation content or load tend to receive pitch accents[REF_CITE]."
"Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an annotated corpus of newspaper text, and machine learning algorithms to produce classi-fiers for both tasks."
"We address two questions that have been left open thus far in previous work: 1. Is there an advantage in inducing decision trees for both tasks, or is it better to not abstract from individual instances and use a memory-based k-nearest neighbour classifier? 2. Is there an advantage in inducing classifiers for both tasks individually, or can both tasks be learned together."
The first question deals with a key difference be-tween standard decision tree induction and memory-based classification: how to deal with exceptional instances.
"Decision trees, CART (Classification and Regression Tree) in particular[REF_CITE], have been among the first successful machine learning algorithms applied to predicting pitch ac-cents and prosodic boundaries for TTS[REF_CITE]."
"Decision tree induction finds, through heuristics, a minimally-sized decision tree that is estimated to generalize well to unseen data."
"Its minimality strategy makes the algorithm reluctant to remember individual out-lier instances that would take long paths in the tree: typically, these are discarded."
"This may work well when outliers do not reoccur, but as demonstrated[REF_CITE], exceptions do typically reoccur in language data."
"Hence, machine learn-ing algorithms that retain a memory trace of indi-vidual instances, like memory-based learning algo-rithms based on the k-nearest neighbour classifier, outperform decision tree or rule inducers precisely for this reason."
"Comparing the performance of machine learning algorithms is not straightforward, and deserves care-ful methodological consideration."
"For a fair com-parison, both algorithms should be objectively and automatically optimized for the task to be learned."
"This point is made[REF_CITE], who show that, for tasks such as word-sense dis-ambiguation and part-of-speech tagging, tuning al- gorithms in terms of feature selection and classifier parameters gives rise to significant improvements in performance."
"In this paper, therefore, we optimize both CART and MBL individually and per task, us-ing a heuristic optimization method called iterative deepening."
"The second issue, that of task combination, stems from the intuition that the two tasks have a lot in common."
"For instance,[REF_CITE]re-ports that knowledge of the location of breaks facil-itates accent placement."
"Although pitch accents and breaks do not consistently occur at the same posi-tions, they are to some extent analogous to phrase chunks and head words in parsing: breaks mark boundaries of intonational phrases, in which typi-cally at least one accent is placed."
A learner may thus be able to learn both tasks at the same time.
"Apart from the two issues raised, our work is also practically motivated."
Our goal is a good algorithm for real-time TTS.
This is reflected in the type of features that we use as input.
"These can be com-puted in real-time, and are language independent."
"We intend to show that this approach goes a long way towards generating high-quality prosody, cast-ing doubt on the need for more expensive sentence and discourse analysis."
The remainder of this paper has the following structure.
"In Section 2 we define the task, describe the data, and the feature generation process which involves POS tagging, syntactic chunking, and com-puting several information-theoretic metrics."
"Fur-thermore, a brief overview is given of the algorithms we used (CART and MBL)."
Section 3 describes the experimental procedure (ten-fold iterative deepen-ing) and the evaluation metrics (F-scores).
Section 4 reports the results for predicting accents and major prosodic boundaries with both classifiers.
It also re-ports their performance on held-out data and on two fully independent test sets.
The final section offers some discussion and concluding remarks.
"To explore the generalization abilities of machine learning algorithms trained on placing pitch accents and breaks in Dutch text, we define three classifica-tion tasks:"
"Pitch accent placement – given a word form in its sentential context, decide whether it should be accented."
This is a binary classification task.
"Break insertion – given a word form in its senten-tial context, decide whether it should be fol-lowed by a boundary."
This is a binary classi-fication task.
"Combined accent placement and break insertion – given a word form in its sentential context, decide whether it should be accented and whether it should be followed by a break."
This is a four-class task: no accent and no break; an accent and no break; no accent and a break; an accent and a break.
"Finer-grained classifications could be envisioned, e.g. predicting the type of pitch accent, but we assert that finer classification, apart from being arguably harder to annotate, could be deferred to later pro-cessing given an adequate level of precision and re-call on the present task."
In the next subsections we describe which data we selected for annotation and how we annotated it with respect to pitch accents and prosodic breaks.
We then describe the implementation of memory-based learning applied to the task.
"The data used in our experiments consists of 201 articles from the ILK corpus (a large collection of Dutch newspaper text), totalling 4,493 sentences and 58,097 tokens (excluding punctuation)."
"We set apart 10 articles, containing 2,905 tokens (excluding punctuation) as held-out data for testing purposes."
"As a preprocessing step, the data was tokenised by a rule-based Dutch tokeniser, splitting punctuation from words, and marking sentence endings."
"The articles were then prosodically annotated, without overlap, by four different annotators, and were corrected in a second stage, again without over-lap, by two corrector-annotators."
The annotators’ task was to indicate the locations of accents and/or breaks that they preferred.
They used a custom an-notation tool which provided feedback in the form of synthesized speech.
An excerpt of the annotated data with all generated symbolic and numeric [Footnote_1] features is presented in Table 1.
"1 Numeric features were rounded off to two decimal points, where appropriate."
Word forms (Wrd) – The word form tokens form the central unit to which other features are added.
"Pre- and post-punctuation – All punctuation marks in the data are transferred to two separate fea-tures: a pre-punctuation feature (PreP) for punctua-tion marks such as quotation marks appearing before the token, and a post-punctuation feature (PostP) for punctuation marks such as periods, commas, and question marks following the token."
"Part-of-speech (POS) tagging – We used MBT version 1.0[REF_CITE]to develop a memory-based POS tagger trained on the Eindhoven corpus of written Dutch, which does not overlap with our base data."
"We split up the full POS tags into two features, the first (PosC) containing the main POS category, the second (PosF) the POS subfea-tures."
Diacritical accent – Some tokens bear an ortho-graphical diacritical accent put there by the author to particularly emphasize the token in question.
"These accents were stripped off the accented letter, and transferred to a binary feature (DiA)."
NP and VP chunking (NpC &amp; VpC) –
"An ap-proximation of the syntactic structure is provided by simple noun phrase and verb phrase chunkers, which take word and POS information as input and are based on a small number of manually written reg-ular expressions."
"Phrase boundaries are encoded per word using three tags: ‘B’ for chunk-initial words, ‘I’ for chunk-internal words, and ‘O’ for words out-side chunks."
"The NPs are identified according to the base principle of one semantic head per chunk (non-recursive, base NPs)."
"VPs include only verbs, not the verbal complements."
"IC – Information content (IC) of a word w is given by IC(w) = −log(P (w)), where P(w) is esti- mated by the observed frequency of w in a large dis-joint corpus of about 1.7 GB of unannotated Dutch text garnered from various sources."
"Word forms not in this corpus were given the highest IC score, i.e. the value for hapax legomenae (words that occur once)."
"Bigram IC – IC on bigrams (BIC) was calculated for the bigrams (pairs of words) in the data, accord-ing to the same formula and corpus material as for unigram IC."
TF*IDF – The TF*IDF metric[REF_CITE]es-timates the relevance of a word in a document.
Doc-ument frequency counts for all token types were ob-tained from a subset of the same corpus as used for IC calculations.
"TF*IDF and IC (previous two features) have been succesfully tested as features for accent prediction[REF_CITE], who assert that IC is a more powerful predictor than TF*IDF."
"Phrasometer – The phrasometer feature (PM) is the summed log-likelihood of all n-grams the word form occurs in, with n ranging from 1 to 25, and computed in an iterative growth procedure: log-likelihoods of n + 1-grams were computed by ex-panding all stored n-grams one word to the left and to the right; only the n + 1-grams with higher log-likelihood than that of the original n-gram are stored."
Computations are based on the complete ILK Corpus.
"Distance to previous occurrence – The distance, counted in the number of tokens, to previous occur-rence of a token within the same article (D2P)."
Un-seen words were assigned the arbitrary high default distance of 9999.
"Distance to sentence boundaries – Distance of the current token to the start of the sentence (D2S) and to the end of the sentence (D2E), both measured as a proportion of the total sentence length measured in tokens."
CART[REF_CITE]is a statistical method to induce a classification or regression tree from a given set of instances.
"An instance consists of a fixed-length vector of n feature-value pairs, and an information field containing the classification of that particular feature-value vector."
Each node in the CART tree contains a binary test on some categor- ical or numerical feature in the input vector.
"In the case of classification, the leaves contain the most likely class."
The tree building algorithm starts by selecting the feature test that splits the data in such a way that the mean impurity (entropy times the num-ber of instances) of the two partitions is minimal.
The algorithm continues to split each partition recur-sively until some stop criterion is met (e.g. a mini-mal number of instances in the partition).
"Alterna-tively, a small stop value can be used to build a tree that is probably overfitted, but is then pruned back to where it best matches some amount of held-out data."
"In our experiments, we used the CART imple-mentation that is part of the Edinburgh Speech Tools[REF_CITE]."
"Memory-based learning (MBL), also known as instance-based, example-based, or lazy learning[REF_CITE], is a supervised inductive learning algorithm for learning classification tasks."
"Memory-based learning treats a set of training instances as points in a multi-dimensional feature space, and stores them as such in an instance base in memory (rather than perform-ing some abstraction over them)."
"After the instance base is stored, new (test) instances are classified by matching them to all instances in memory, and by calculating with each match the distance, given by a distance function between the new instance X and the memory instance Y ."
Classification in memory-based learning is performed by the k-NN algorithm ([REF_CITE];
The majority class of the k nearest neighbours then determines the class of the new case.
"In our k-NN implementation [Footnote_2] , equi-distant neighbours are taken as belonging to the same k, so this implementation is effectively a k-nearest distance classifier."
"2 All experiments with memory-based learning were per-formed with TiMBL, version 4.3[REF_CITE]."
"Iterative deepening (ID) is a heuristic search algo-rithm for the optimization of algorithmic parameter and feature selection, that combines classifier wrap-ping (using the training material internally to test ex-perimental variants)[REF_CITE]with progressive sampling of training material[REF_CITE]."
"We start with a large pool of experiments, each with a unique combination of input features and algorithmic parameter settings."
"In the first step, each attempted setting is applied to a small amount of training material and tested on a fixed amount of held-out data (which is a part of the full train-ing set)."
Only the best settings are kept; all others are removed from the pool of competing settings.
"In subsequent iterations, this step is repeated, ex-ponentially decreasing the number of settings in the pool, while at the same time exponentially increas-ing the amount of training material."
"The idea is that the increasing amount of time required for training is compensated by running fewer experiments, in ef-fect keeping processing time approximately constant across iterations."
"This process terminates when only the single best experiment is left (or, the n best ex-periments)."
This ID procedure can in fact be embedded in a standard 10-fold cross-validation procedure.
"In such a 10-fold CV ID experiment, the ID procedure is car-ried out on the 90% training partition, and the result-ing optimal setting is tested on the remaining 10% test partition."
"The average score of the 10 optimized folds can then be considered, as that of a normal 10-fold CV experiment, to be a good estimation of the performance of a classifier optimized on the full data set."
"For current purposes, our specific realization of this general procedure was as follows."
We used folds of approximately equal size.
"Within each ID ex-periment, the amount of held-out data was approx-imately 5%; the initial amount of training data was 5% as well."
"Eight iterations were performed, dur-ing which the number of experiments was decreased, and the amount of training data was increased, so that in the end only the 3 best experiments used all available training data (i.e. the remaining 95%)."
Increasing the training data set was accomplished by random sampling from the total of training data available.
Selection of the best experiments was based on their F-score (van[REF_CITE]) on the target class (accent or break).
"F-score, the har-monic mean of precision and recall, is chosen since it directly evaluates the tasks (placement of accents or breaks), in contrast with classification accuracy (the percentage of correctly classified test instances) which is biased to the majority class (to place no ac-cent or break)."
"Moreover, accuracy masks relevant differences between certain inappropriate classifiers that do not place accents or breaks, and better clas-sifiers that do place them, but partly erroneously."
The initial pool of experiments was created by systematically varying feature selection (the input features to the classifier) and the classifier set-tings (the parameters of the classifiers).
We re-stricted these selections and settings within reason-able bounds to keep our experiments computation-ally feasible.
"In particular, feature selection was lim-ited to varying the size of the window that was used to model the local context of an instance."
"A uni-form window (i.e. the same size for all features) was applied to all features except DiA, D2P, D2S, and D2E. Its size (win) could be 1, 3, 5, 7, or 9, where win = 1 implies no modeling of context, whereas win = 9 means that during classification not only the features of the current instance are taken into ac-count, but also those of the preceding and following four instances."
"For CART, we varied the following parameter val-ues, resulting in a first ID step with 480 experiments: • the minimum number of examples for leaf nodes (stop): 1, 10, 25, 50, and 100 • the number of partitions to split a float feature range into (frs): 2, 5, 10, and 25 • the percentage of training material held out for pruning (held-out): 0, 5, 10, 15, 20, and 25 (0 implies no pruning)"
"For MBL, we varied the following parameter val-ues, which led to 1184 experiments in the first ID step: • the number of nearest neighbours (k): 1, 4, 7, 10, 13, 16, 19, 22, 25, and 28 • the type of feature weighting: Gain Ratio (GR), and Shared Variance (SV) • the feature value similarity metric: Overlap, or Modified Value Difference Metric (MVDM) with back-off to Overlap at value frequency tresholds 1 (L=1, no back-off), 2, and 10 • the type of distance weighting: None, Inverse Distance, Inverse Linear Distance, and Expo-nential Decay with α = 1.0 (ED1) and α = 4.0 (ED4)"
"We first determined two sharp, informed baselines; see Table 2."
"The informed baseline for accent place-ment is based on the content versus function word distinction, commonly employed in TTS systems[REF_CITE]."
We refer to this baseline as CF-rule.
"It is constructed by accenting all content words, while leaving all function words (determin-ers, prepositions, conjunctions/complementisers and auxiliaries) unaccented."
The required word class in-formation is obtained from the POS tags.
"The base-line for break placement, henceforth PUNC-rule, re-lies solely on punctuation."
"A break is inserted after any sequence of punctuation symbols containing one or more characters from the set {,!?:;()}."
It should be noted that both baselines are simple rule-based algorithms that have been manually optimized for the current training set.
"They perform well above chance level, and pose a serious challenge to any ML approach."
"From the results displayed in Table 2, the follow-ing can be concluded."
"First, MBL attains the highest F-scores on accent placement, 83.6, and break place-ment, 88.0."
It does so when trained on the ACCENT and BREAK tasks individually.
"On these tasks, MBL performs significantly better than CART (paired t-tests yield p &lt; 0.01 for both differences)."
"Second, the performances of MBL and CART on the combined task, when split in F-scores on accent and break placement, are rather close to those on the accent and break tasks."
"For both MBL and CART, the scores on accent placement as part of the com-bined task versus accent placement in isolation are not significantly different."
"For break insertion, how-ever, a small but significant drop in performance can be seen with MBL (p &lt; 0.05) and CART (p &lt; 0.01) when it is performed as part of the COMBINED task."
"As is to be expected, the optimal feature selec-tions and classifier settings obtained by iterative deepening turned out to vary over the ten folds for both MBL and CART."
Table 3 lists the settings pro-ducing the best F-score on accents or breaks.
"A win-dow of 7 (i.e. the features of the three preceding and following word form tokens) is used by CART and MBL for accent placement, and also for break in-sertion by CART, whereas MBL uses a window of just 3."
"Both algorithms (stop in CART, and k in Break MBL) base classifications on minimally around 25 instances."
"Furthermore, MBL uses the Gain Ratio feature weighting and Exponential Decay distance weighting."
"Although no pruning was part of the Iter-ative Deepening experiment, CART prefers to hold out 5% of its training material to prune the decision tree resulting from the remaining 95%."
"We tested our optimized approach on our held-out data of 10 articles (2,905 tokens), and on an indepen-dent test corpus (van[REF_CITE])."
"The latter contains two types of text: 2 newspaper texts (55 sentences, 786 words excluding punctua-tion), and 17 email messages (70 sentences, 1133 words excluding punctuation)."
"This material was an-notated by 10 experts, who were asked to indicate the preferred accents and breaks."
"For the purpose of evaluation, words were assumed to be accented if they received an accent by at least 7 of the annota-tors."
"Furthermore, of the original four break levels annotated (i.e. no break, light, medium, or heavy ), only medium and heavy level breaks were consid-ered to be a break in our evaluation."
"Table 4 lists the precision, recall, and F-scores obtained on the two tasks using the single-best scoring setting from the 10-fold CV ID experiment per task."
"It can be seen that both CART and MBL outperformed the CF-rule baseline on our own held-out data and on the news and email texts, with similar margins as observed in our 10-fold CV ID experiment."
"MBL attains an F-score of 86.6 on accents, and 91.0 on breaks; both are improvements over the cross-validation estima-tions."
"On breaks, however, both CART and MBL failed to improve on the PUNC-rule baseline; on the news and email texts they perform even worse."
"In-specting MBLs output on these text, it turned out that MBL does emulate the PUNC-rule baseline, but that it places additional breaks at positions not marked by punctuation."
A considerable portion of these non-punctuation breaks is placed incorrectly – or at least different from what the annotators pre-ferred – resulting in a lower precision that does not outweigh the higher recall.
"With shallow features as input, we trained machine learning algorithms on predicting the placement of pitch accents and prosodic breaks in Dutch text, a desirable function for a TTS system to produce synthetic speech with good prosody."
"Both algo-rithms, the memory-based classifier MBL and de-cision tree inducer CART, were automatically opti-mized by an Iterative Deepening procedure, a classi-fier wrapper technique with progressive sampling of training data."
"It was shown that MBL significantly outperforms CART on both tasks, as well as on the combined task (predicting accents and breaks simul-taneously)."
This again provides an indication that it is advantageous to retain individual instances in memory (MBL) rather than to discard outlier cases as noise (CART).
"Training on both tasks simultaneously, in one model rather than divided over two, results in generalization accuracies similar to that of the individually-learned models (identical on accent placement, and slightly lower for break placement)."
This shows that learning one task does not seriously hinder learning the other.
"From a practical point of view, it means that a TTS developer can resort to one system for both tasks instead of two."
Pitch accent placement can be learned from shal-low input features with fair accuracy.
"Break in-sertion seems a harder task, certainly in view of the informed punctuation baseline PUNC-rule."
Es-pecially the precision of the insertion of breaks at other points than those already indicated by com-mas and other ‘pseudo-prosodic’ orthographic mark up is hard.
"This may be due to the lack of crucial information in the shallow features, to inherent lim-itations of the ML algorithms, but may as well point to a certain amount of optionality or personal pref-erence, which puts an upper bound on what can be achieved in break predicti[REF_CITE]."
"We plan to integrate the placement of pitch ac-cents and breaks in a TTS system for Dutch, which will enable the closed-loop annotation of more data using the TTS itself and on-line (active) learning."
"Moreover, we plan to investigate the perceptual cost of false insertions and deletions of accents and breaks in experiments with human listeners."
This paper proposes a hybrid of hand-crafted rules and a machine learning method for chunking Korean.
"In the par-tially free word-order languages such as Korean and Japanese, a small number of rules dominate the performance due to their well-developed postpositions and endings."
"Thus, the proposed method is primarily based on the rules, and then the residual errors are corrected by adopting a memory-based machine learning method."
"Since the memory-based learning is an efficient method to handle exceptions in natural language processing, it is good at checking whether the estimates are excep-tional cases of the rules and revising them."
An evaluation of the method yields the im-provement in F-score over the rules or var-ious machine learning methods alone.
Text chunking has been one of the most interest-ing problems in natural language learning commu-nity since the first work[REF_CITE]using a machine learning method.
"The main purpose of the machine learning methods applied to this task is to capture the hypothesis that best deter-mine the chunk type of a word, and such methods have shown relatively high performance in English ([REF_CITE];"
"Zhang et. al, 2001)."
"In order to do it, various kinds of information, such as lexical information, part-of-speech and grammat-ical relation, of the neighboring words is used."
"Since the position of a word plays an important role as a syntactic constraint in English, the methods are suc-cessful even with local information."
"However, these methods are not appropriate for chunking Korean and Japanese, because such lan-guages have a characteristic of partially free word-order."
"That is, there is a very weak positional con-straint in these languages."
"Instead of positional con-straints, they have overt postpositions that restrict the syntactic relation and composition of phrases."
"Thus, unless we concentrate on the postpositions, we must enlarge the neighboring window to get a good hypothesis."
"However, enlarging the win-dow size will cause the curse of dimensionality[REF_CITE], which results in the deficiency in the generalization performance."
"Especially in Korean, the postpositions and the endings provide important information for noun phrase and verb phrase chunking respectively."
"With only a few simple rules using such information, the performance of chunking Korean is as good as the rivaling other inference models such as ma-chine learning algorithms and statistics-based meth-ods[REF_CITE]."
"Though the rules are approxi-mately correct for most cases drawn from the do-main on which the rules are based, the knowledge in the rules is not necessarily well-represented for any given set of cases."
"Since chunking is usually processed in the earlier step of natural language pro-cessing, the errors made in this step have a fatal in-fluence on the following steps."
"Therefore, the ex-ceptions that are ignored by the rules must be com- pensated for by some special treatments of them for higher performance."
"To solve this problem, we have proposed a com-bining method of the rules and the k-nearest neigh-bor (k-NN) algorithm[REF_CITE]."
The problem in this method is that it has redundant k- NNs because it maintains a separate k-NN for each kind of errors made by the rules.
"In addition, be-cause it applies a k-NN and the rules to each exam-ples, it requires more computations than other infer-ence methods."
The goal of this paper is to provide a new method for chunking Korean by combining the hand-crafted rules and a machine learning method.
"The chunk type of a word in question is determined by the rules, and then verified by the machine learning method."
The role of the machine learning method is to de-termine whether the current context is an exception of the rules.
"Therefore, a memory-based learning (MBL) is used as a machine learning method that can handle exceptions efficiently (Daelemans et. al, 1999)."
The rest of the paper is organized as follows.
Sec-tion 2 explains how the proposed method works.
Section 3 describes the rule-based method for chunking Korean and Section 4 explains chunking by memory-based learning.
Section 5 presents the experimental results.
Section 6 introduces the issues for applying the proposed method to other problems.
"Finally, Section 7 draws conclusions."
Figure 1 shows the structure of the chunking model for Korean.
"The main idea of this model is to apply rules to determine the chunk type of a word w i in a sentence, and then to refer to a memory based clas-sifier in order to check whether it is an exceptional case of the rules."
"In the training phase, each sentence is analyzed by the rules and the predicted chunk type is compared with the true chunk type."
"In case of mis-prediction, the error type is determined according to the true chunk type and the predicted chunk type."
The mispredicted chunks are stored in the error case library with their true chunk types.
"Since the error case library accumulates only the exceptions of the rules, the number of cases in the library is small if the rules are general enough to represent the instance space well."
The classification phase in Figure 1 is expressed as a procedure in Figure 2.
It determines the chunk type of a word w i given with the context C i .
"First of all, the rules are applied to determine the chunk type."
"Then, it is checked whether C i is an exceptional case of the rules."
"If it is, the chunk type determined by the rules is discarded and is determined again by the memory based reasoning."
The condition to make a decision of exceptional case is whether the similar-ity between C i and the nearest instance in the error
"Input : a word w i , a context C i , and the threshold t"
Output : a chunk type c
"Since determiners, nouns and pronouns play the similar syntactic role in Korean, they form a noun case library is larger than the threshold t."
"Since the library contains only the exceptional cases, the more similar is C i to the nearest instance, the more prob-able is it an exception of the rules."
"There are four basic phrases in Korean: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP)."
"Thus, chunking by rules is divided into largely four components."
"When the part-of-speech of w i is one of determiner, noun, and pronoun, there are only seven rules to determine the chunk type of w i due to the well-developed postpositions of Korean. 1."
If POS(w i−1 ) = determiner and w i−1 does not have a postposition
Then y i =
I-NP. 2. Else If POS(w i−1 ) = pronoun and w i−1 does not have a postposition
Then y i =
I-NP. 3. Else If POS(w i−1 ) = noun and w i−1 does not have a postposition
Then y i =
I-NP. 4. Else If POS(w i−1 ) = noun and w i−1 has a possessive postposition
Then y i =
I-NP. 5. Else If POS(w i−1 ) = noun and w i−1 has a relative post-fix Then y i =
I-NP. 6. Else If POS(w i−1 ) = adjective and w i−1 has a relative ending
Then y i =
I-NP. 7. Else y i = B-NP.
"Here, POS(w i−1 ) is the part-of-speech of w i−1 ."
"B-NP represents the first word of a noun phrase, while I-NP is given to other words in the noun phrase. tive clause with no sub-constituent also constitutes a noun phrase."
"Since the adjectives of Korean have no definitive usage, this rule corresponds to the defini-tive usage of the adjectives in English."
The verb phrase chunking has been studied for a long time under the name of compound verb pro-cessing in Korean and shows relatively high accu-racy.
"Shin used a finite state automaton for verb phrase chunking[REF_CITE], while K.-C. Kim used knowledge-based rules (Kim et. al, 1995)."
"For the consistency with noun phrase chunking, we use the rules in this paper."
"The rules used are the ones pro-posed by (Kim et. al, 1995) and the further explana-tion on the rules is skipped."
The number of the rules used is 29.
"When the adverbs appear in succession, they have a great tendency to form an adverb phrase."
"Though an adverb sequence is not always one adverb phrase, it usually forms one phrase."
Table 1 shows this empiri-cally.
The usage of the successive adverbs is investi-gated[REF_CITE]dataset [Footnote_1] where 270 cases are observed.
1 This dataset will be explained in Section 5.1.
"Thus, it can be said that the possibility that an adverb sequence forms a phrase is far higher than the possibility that it forms two phrases."
"When the part-of-speech of w i is an adjective, its chunk type is determined by the following rule. 1."
If POS(w i−1 ) = adverb
Then y i =
I-ADVP. 2. Else y i = B-ADVP.
There is no special rule for independent phrase chunking.
It can be done only through knowledge base that stores the cases where independent phrases take place.
Memory-based learning is a direct descent of the k-Nearest Neighbor (k-NN) algorithm[REF_CITE].
"Since many natural language process-ing (NLP) problems have constraints of a large num-ber of examples and many attributes with different relevance, memory-based learning uses more com-plex data structure and different speedup optimiza-tion from the k-NN."
It can be viewed with two components: a learning component and a similarity-based performance com-ponent.
"The learning component involves adding training examples to memory, where all examples are assumed to be fixed-length vectors of n at-tributes."
"The similarity between an instance x and all examples y in memory is computed using a dis-tance metric, ∆(x, y)."
The chunk type of x is then determined by assigning the most frequent category within the k most similar examples of x.
"The distance from x and y, ∆(x, y) is defined to be n ∆(x, y) ≡ α i δ(x i , y i ), i=1 where α i is the weight of i-th attribute and 0 if x i = y i , δ(x i , y i ) = 1 if x i = y i ."
"When α i is determined by information ga[REF_CITE], the k-NN algorithm with this metric is called IB1-IG (Daelemans et. al, 2001)."
All the ex-periments performed by memory-based learning in this paper are done with IB1-IG.
Table 2 shows the attributes of IB1-IG for chunk-ing Korean.
"To determine the chunk type of a word w i , the lexicons, POS tags, and chunk types of surrounding words are used."
"For the surrounding words, three words of left context and three words of right context are used for lexicons and POS tags, while two words of left context are used for chunk types."
"Since chunking is performed sequentially, the chunk types of the words in right context are not known in determining the chunk type of w i ."
"For the evaluation of the proposed method, all exper-iments are performed[REF_CITE]Korean Chunk-ing dataset ([REF_CITE]dataset) [Footnote_2] ."
2[REF_CITE]Korean Chunking dataset is available[URL_CITE]
"This dataset is derived from the parsed corpus, which is a product[REF_CITE]project supported by Korean govern-ment."
"The corpus consists of 12,092 sentences with 111,658 phrases and 321,328 words, and the vocab-ulary size is 16,808."
Table 3 summarizes the infor-mation on the dataset.
The format of the dataset follows that[REF_CITE]dataset[REF_CITE].
Figure 3 shows an ex-ample sentence in the dataset [Footnote_3] .
"3 The last column of this figure, the English annotation, does"
"Each word in the dataset has two additional tags, which are a part-of-speech tag and a chunk tag."
The part-of-speech tags are based on KAIST tagset[REF_CITE].
Each phrase can have two kinds of chunk types: B-XP and I-XP.
"In addition to them, there is O chunk type that is used for words which are not part of any chunk."
"Since there are four types of phrases and one additional chunk type O, there exist nine chunk types."
Table 4 shows the chunking performance when only the rules are applied.
Using only the rules gives 97.99% of accuracy and 91.87 of F-score.
"In spite of relatively high accuracy, F-score is somewhat low."
"Because the important unit of the work in the appli-cations of text chunking is a phrase, F-score is far more important than accuracy."
"Thus, we have much room to improve in F-score."
Table 5 shows the error types by the rules and their distribution.
"For example, the error type ‘B-ADVP I-ADVP’ contains the errors whose true la-bel is B-ADVP and that are mislabeled by I-ADVP."
"There are eight error types, but most errors are re-lated with noun phrases."
We found two reasons for this: 1.
It is difficult to find the beginning of noun phrases.
All nouns appearing successively without postpositions are not a single noun phrase.
"But, they are always predicted to be single noun phrase by the rules, though they “I-NP I-NP”."
"But, when it is just an adverbial postposition that implies ‘with’ in English, the chunk types should be “I-NP B-NP”."
Table 6 gives the 10-fold cross validation result of three machine learning algorithms.
"In each fold, the corpus is divided into three parts: training (80%), held-out (10%), test (10%)."
"Since held-out set is used only to find the best value for the threshold t in the combined model, it is not used in measuring the performance of machine learning algorithms."
"The machine learning algorithms tested are (i) memory-based learning (MBL), (ii) decision tree, and (iii) support vector machines (SVM)."
"We use C4.5 release 8[REF_CITE]for decision tree in-duction and SV M light[REF_CITE]for support vector machines, while TiMBL (Daelemans et. al, 2001) is adopted for memory-based learning."
De-cision trees and SVMs use the same attributes with memory-based learning (see Table 2).
"Two of the al-gorithms, memory-based learning and decision tree, show worse performance than the rules."
"The F-scores of memory-based learning and decision tree are 91.38 and 91.36 respectively, while that of the rules is 91.87 (see Table 4)."
"On the other hand, sup-port vector machines present a slightly better perfor-mance than the rules."
"The F-score of support vector machine is 92.54, so the improvement over the rules is just 0.67."
Table 7 shows the weight of attributes when only memory-based learning is used.
"Each value in this table corresponds to α i in calculating ∆(x,y)."
"The more important is an attribute, the larger is the weight of it."
"Thus, the most im-portant attribute among 17 attributes is C i−1 , the chunk type of the previous word."
"On the other hand, the least important attributes are W i−3 and C i−3 ."
Because the words make less influence on determining the chunk type of w i in ques-tion as they become more distant from w i .
That not exist in the dataset.
"It is given for the explanation. is, the order of important lexical attributes is  i , W i−1 , W i+1 , W i−2 , W i+2 , W i+3 , W i−3 ."
The same phenomenon is found in part-of-speech (POS) and chunk type (C).
"In comparing the part-of-speech information with the lexical information, we find out that the part-of-speech is more impor-tant."
One possible explanation for this is that the lexical information is too sparse.
The best performance on English reported is 94.13 in F-score (
"Zhang et. al, 2001)."
The reason why the performance on Korean is lower than that on English is the curse of dimensionality.
"That is, the wider context is required to compensate for the free order of Korean, but it hurts the performance[REF_CITE]."
Table 8 shows the final result of the proposed method.
"The F-score is 94.21 on the average which is improvement of 2.34 over the rules only, 1.67 over support vector machines, and 2.83 over memory-based learning."
"In addition, this result is as high as the performance on English (Zhang et. al, 2001)."
The threshold t is set to the value which produces the best performance on the held-out set.
The total sum of all weights in Table 7 is 2.48.
"This implies that when we set t &gt; 2.48, only the rules are ap-plied since there is no exception with this threshold."
"When t = 0.00, only the memory-based learning is used."
Since the memory-based learning determines the chunk type of w i based on the exceptional cases of the rules in this case. the performance is poor with t = 0.00.
The best performance is obtained when t is near 1.94.
Figure 4 shows how much F-score is improved for each kind of phrases.
The average F-score of noun phrase is 94.54 which is far improved over that of the rules only.
This implies that the exceptional cases of the rules for noun phrase are well handled by the memory-based learning.
"The performance is much improved for noun phrase and verb phrase, while it remains same for adverb phrases and independent phrases."
This result can be attributed to the fact that there are too small number of exceptions for adverb phrases and independent phrases.
"Because the ac-curacy of the rules for these phrases is already high enough, most cases are covered by the rules."
"Mem-ory based learning treats only the exceptions of the rules, so the improvement by the proposed method is low for the phrases."
"In order to make the proposed method practical and applicable to other NLP problems, the following is-sues are to be discussed:"
"In the proposed method, memory-based learn-ing is used not to find a hypothesis for inter-preting whole data space but to handle the ex-ceptions of the rules."
"If we use all data for both the rules and memory-based learning, we have to weight the methods to combine them."
"But, it is difficult to know the weights of the methods. 3. Why don’t we convert the memory-based learning to the rules?"
Converting between the rules and the cases in the memory-based learning tends to yield inef-ficient or unreliable representation of rules.
The proposed method can be directly applied to the problems other than chunking Korean if the proper rules are prepared.
The proposed method will show better performance than the rules or machine learning methods alone.
In this paper we have proposed a new method to learn chunking Korean by combining the hand-crafted rules and a memory-based learning.
"Our method is based on the rules, and the estimates on chunks by the rules are verified by a memory-based learning."
"Since the memory-based learning is an efficient method to handle exceptional cases of the rules, it supports the rules by making decisions only for the exceptions of the rules."
"That is, the memory-based learning enhances the rules by efficiently han-dling the exceptional cases of the rules."
The experiments[REF_CITE]dataset showed that the proposed method improves the F-score of the rules by 2.34 and of the memory-based learn-ing by 2.83.
"Even compared with support vector machines, the best machine learning algorithm in text chunking, it achieved the improvement of 1.67."
The improvement was made mainly in noun phrases among four kinds of phrases in Korean.
This is because the errors of the rules are mostly related with noun phrases.
"With relatively many instances for noun phrases, the memory-based learning could compensate for the errors of the rules."
We also em-pirically found the threshold value t used to deter-mine when to apply the rules and when to apply memory-based learning.
We also discussed some issues in combining a rule-based method and a memory-based learning.
These issues will help to understand how the method works and to apply the proposed method to other problems in natural language processing.
"Since the method is general enough, it can be applied to other problems such as POS tagging and PP attachment."
"The memory-based learning showed good perfor-mance in these problems, but did not reach the state-of-the-art."
We expect that the performance will be improved by the proposed method.
"Supertagging is the tagging process of assigning the correct elementary tree of LTAG, or the correct supertag, to each word of an input sentence [Footnote_1] ."
1 By the correct supertag we mean the supertag that an LTAG parser would assign to a word in a sentence.
In this pa-per we propose to use supertags to expose syntactic dependencies which are unavail-able with POS tags.
We first propose a novel method of applying Sparse Network of Winnow (SNoW) to sequential models.
"Then we use it to construct a supertagger that uses long distance syntactical depen-dencies, and the supertagger achieves an accuracy of  ."
We apply the su-pertagger to NP chunking.
The use of su-pertags in NP chunking gives rise to al-most absolute increase (from  to  ) in F-score under Transforma-tion Based Learning(TBL) frame.
The surpertagger described here provides an effective and efficient way to exploit syn-tactic information.
"In Lexicalized Tree-Adjoining Grammar (LTAG)[REF_CITE], each word in a sentence is associated with an el-ementary tree, or a supertag[REF_CITE]."
Supertagging is the process of assigning the correct supertag to each word of an input sentence.
The following two facts make supertagging attrac-tive.
"Firstly supertags encode much more syntac-tical information than POS tags, which makes su-pertagging a useful pre-parsing tool, so-called, al-most parsing[REF_CITE]."
"On the other hand, as the term ’supertagging’ suggests, the time complexity of supertagging is similar to that of POS tagging, which is linear in the length of the in-put sentence."
"In this paper, we will focus on the NP chunk-ing task, and use it as an application of supertag-ging.[REF_CITE]proposed a two-phase pars-ing model which includes chunking and attaching.[REF_CITE]approached chuck-ing by using Transformation Based Learning(TBL)."
"Many machine learning techniques have been suc-cessfully applied to chunking tasks, such as Regular-ized Winnow[REF_CITE], SVMs[REF_CITE], CRFs[REF_CITE], Maximum Entropy Model[REF_CITE], Memory Based Learning[REF_CITE]and SNoW[REF_CITE]."
"The previous best result on chunking in literature was achieved by Regularized Winnow[REF_CITE], which took some of the parsing results given by an English Slot Grammar-based parser as input to the chunker."
The use of parsing results contributed  absolute increase in F-score.
"However, this approach conflicts with the purpose of chunking."
"Ideally, a chunker geneates n-best results, and an at-tacher uses chunking results to construct a parse."
"The dilemma is that syntactic constraints are use-ful in the chunking phase, but they are unavail-able until the attaching phase."
The reason is that POS tags are not a good labeling system to encode enough linguistic knowledge for chunking.
"How-ever another labeling system, supertagging, can pro-vide a great deal of syntactic information."
"In an LTAG, each word is associated with a set of possible elementary trees."
"An LTAG parser assigns the correct elementary tree to each word of a sen-tence, and uses the elementary trees of all the words to build a parse tree for the sentence."
"Elementary trees, which we call supertags, contain more infor-mation than POS tags, and they help to improve the chunking accuracy."
"Although supertags are able to encode long dis-tance dependence, supertaggers trained with local information in fact do not take full advantage of complex information available in supertags."
"In order to exploit syntactic dependencies in a larger context, we propose a new model of supertag-ging based on Sparse Network of Winnow (SNoW)[REF_CITE]."
We also propose a novel method of applying SNoW to sequential models in a way anal-ogous to the Projection-base Markov Model (PMM) used[REF_CITE].
"In contrast to PMM, we construct a SNoW classifier for each POS tag."
"For each word of an input sentence, its POS tag, instead of the supertag of the previous word, is used to select the corresponding SNoW classifier."
This method helps to avoid the sparse data problem and forces SNoW to focus on difficult cases in the con-text of supertagging task.
"Since PMM suffers from the label bias problem[REF_CITE], we have used two methods to cope with this problem."
"One method is to skip the local normalization step, and the other is to combine the results of left-to-right scan and right-to-left scan."
We test our supertagger on both the hand-coded supertags used[REF_CITE]as well as the supertags extracted from Penn Treebank(PTB)[REF_CITE].
"On the dataset used[REF_CITE], our supertagger achieves an accuracy of  ."
We then apply our supertagger to NP chunking.
"The purpose of this paper is to find a better way to exploit syntactic information which is useful in NP chunking, but not the machine learning part."
"So we just use TBL, a well-known algorithm in the com-munity of text chunking, as the machine learning tool in our research."
"Using TBL also allows us to easily evaluate the contribution of supertags with re-spect to Ramshaw and Marcus’s original work, the de facto baseline of NP chunking."
The use of su-pertags with TBL can be easily extended to other machine learning algorithms.
We repeat Ramshaw and Marcus’ Transformation Based NP chunking[REF_CITE]algorithm by substituting supertags for POS tags in the dataset.
The use of supertags gives rise to almost absolute increase (from  to  ) in F-score under Transformation Based Learning(TBL) frame.
This confirms our claim that using supertag-ging as a labeling system helps to increase the over-all performance of NP Chunking.
The supertag-ger presented in this paper provides an opportunity for advanced machine learning techniques to im-prove their performance on chunking tasks by ex-ploiting more syntactic information encoded in the supertags.
"In[REF_CITE]trigram models were used for su-pertagging, in which Good-Turing discounting tech-nique and Katz’s back-off model were employed."
"The supertag for a word was determined by the lexi-cal preference of the word, as well as by the contex-tual preference of the previous two supertags."
"The model was tested[REF_CITE]of PTB, and trained on section 0 through 24 except section 20."
The accuracy on the test data is  [Footnote_2] .
2 This number is based on footnote 1[REF_CITE]. A few supertags were grouped into equivalence classes for eval-uation
"In[REF_CITE], supertagging was used for NP chunking and it achieved an F-score of  .[REF_CITE]reported a similar result with a tri-gram supertagger."
"In their approaches, they first su-pertagged the test data and then uesd heuristic rules to detect NP chunks."
But it is hard to say whether it is the use of supertags or the heuristic rules that makes their system achieve the good results.
"As a first attempt, we use fast TBL[REF_CITE], a TBL program, to repeat Ramshaw and Marcus’ experiment on the standard dataset."
Then we use Srinivas’ supertagger[REF_CITE]to su-pertag both the training and test data.
We run the fast TBL for the second round by using supertags in-stead of POS tags in the dataset.
"With POS tags we achieve an F-score of  , but with supertags we only achieve an F-score of  ."
This is not sur-prising becuase Srinivas’ supertag was only trained with a trigram model.
"Although supertags are able to encode long distance dependence, supertaggers trained with local information in fact do not take full advantage of their strong capability."
So we must use long distance dependencies to train supertaggers to take full advantage of the information in supertags.
The trigram model often fails in capturing the co-occurrence dependence between a head word and its dependents.
Consider the phrase ”will join the board as a nonexecutive director”.
The occurrence of join has influence on the lexical selection of as.
"But join is outside the window of trigram.[REF_CITE]proposed a head trigram model in which the lexical selection of a word depended on the su-pertags of the previous two head words , instead of the supertags of the two words immediately leading the word of interest."
But the performance of this model was worse than the traditional trigram model because it discarded local information.[REF_CITE]combined the traditional tri-gram model and head trigram model in their trigram mixed model.
"In their model, context for the current word was determined by the supertag of the previ-ous word and context for the previous word accord-ing to 6 manually defined rules."
The mixed model achieved an accuracy of  on the same dataset as that[REF_CITE].
"In[REF_CITE], three other models were proposed, but the mixed model achieved the highest accuracy."
"In addition, they combined all their models with pairwise voting, yielding an accuracy of  ."
The mixed trigram model achieves better results on supertagging because it can capture both lo-cal and long distance dependencies to some extent.
"However, we think that a better way to find useful context is to use machine learning techniques but not define the rules manually."
"One approach is to switch to models like PMM, which can not only take advan-tage of generative models with the Viterbi algorithm, but also utilize the information in a larger contexts through flexible feature sets."
This is the basic idea guiding the design of our supertagger.
Sparse Network of Winnow (SNoW)[REF_CITE]is a learning architecture that is specially tailored for learning in the presence of a very large number of features where the decision for a single sample de-pends on only a small number of features.
"Further-more, SNoW can also be used as a general purpose multi-class classifier."
It is noted[REF_CITE]that one of the important properites of the sparse architecture of
"SNoW is that the complexity of processing an exam-ple depends only on the number of features active in it,  , and is independent of the total number of fea-tures, , observed over the life time of the system and this is important in domains in which the total number of features in very large, but only a small number of them is active in each example."
"As far as supertagging is concerned, word context forms a very large space."
"However, for each word in a given sentence, only a small part of features in the space are related to the decision on supertag."
"Specif-ically the supertag of a word is determined by the ap-pearances of certain words, POS tags, or supertags in its context."
Therefore SNoW is suitable for the supertagging task.
"Supertagging can be viewed in term of the se-quential model, which means that the selection of the supertag for a word is influenced by the decisions made on the previous few words.[REF_CITE]proposed three methods of using classi-fiers in sequential inference, which are HMM, PMM and CSCL."
"Among these three models, PMM is the most suitable for our task."
The basic idea of PMM is as follows.
"Given an observation sequence ! , we find the most likely state sequence &quot; given ! by maximiz-ing %$ &apos;&quot; &amp; !) (+* , . - %# 43$ &amp;   753 :9 ;6 7 !&lt;&gt;( = #?;6 $ &amp; @ 6 ( # 21/- , . % * # 43$ &amp; 3 :9 6 7 A@ (&gt;= C6A# $ &amp;@ 6 ( (1) /21"
"In this #%$43 model &amp;  @ ( , theandoutput C6# ;$43 &amp; @ ( of, whereSNoW 3 isisusedthe currentto es-timate  is the #% previous $43 &amp; 3 D 7 @ ( isstateseparated, and @ isto themanycurrent state, observation GF# . & gt;HB$43 3 sub- D practice,functions IF# H 43$ &amp;&amp;@@ (( accordingis estimatedto previousin a widerstatewindow."
"In of the observed sequence, instead of @ only."
Then the problem is how to map the SNoW results into sigmoid J ?KML probabilities $ .
"In ( 9 Punyakanok : ( is definedand Rothas,confidence2000), the, where S is the threshold for SNoW,  is the dot product of the weight vector and the example vec-and used as the distribution masstor."
The confidence is normalized #[REF_CITE]$ summing &amp; @ ( . to 1
Firstly we have to decide how to treat POS tags.
One approach is to assign POS tags at the same time that we do supertagging.
"The other approach is to as-sign POS tags with a traditional POS tagger first, and then use them as input to the supertagger."
"Su-pertagging an unknown word becomes a problem for supertagging due to the huge size of the supertag set, Hence we use the second approach in our paper."
"We first run the Brill POS tagger[REF_CITE]on both the training and the test data, and use POS tags as part of the input. 6 ] 6 ] Let  ]"
"Z- be * the [ POS [ tags [ - ,beandthe ^S sentence _*"
"V 6  , \ - be * the supertags respectively."
"Given Z 7 \ , we can find the most likely supertag sequence S given Z 7 \ by maximizing - # $ Sa&amp; 7 \&lt;(b*c, d. /21 #%$ V d &amp; V d 9 6g7 Z 7 &lt;\ &gt;( = ?# 6A$ V 6 &amp; [  ] 6 ( % % V d &amp; V# $ Analogous d 9 6h7 Z 7 \) to ( intoPMMsub-classifiers, we decompose."
"How-ever, in our model, we divide it with respect to POS tags as follows i$ V d &amp; V d 9 6g7 Z 7 &lt;\ (?j #lkBmn$ V d &amp; V d 9 6g7 Z 7 \&lt;( # (2) % V d &amp; V# $ There  d 9 are 6h7 Z several 7 \) ( withreasonsrespect toforthedecomposingPOS tag of the current word, instead of the supertag of the pre-vious word. o To avoid sparse-data problem."
"Thus by defining a clas-sifier on the POS tag of the current word but not the POS tag of the previous word forces the learning algorithm to focus on difficult cases. o Decomposition of the probability estimation can decrease the complexity of the learning al-gorithm and allows the use of different param-eters for different POS tags. pqk For each POS ] , we construct #$ a : SNoW D 7 7 classifier to estimate distribution D &amp; V Z \ ( accord- &lt; ing to the previous supertags V ."
"Following the esti-mation of distribution function[REF_CITE], we define confidence with a sigmoid bKtsuL 9 { HE|~} | :"
"R 9 F R 7 r $ Vh&amp; V D7 Z 7 \)Cj( (3) 3 pqk where is the threshold of , and s is set to 1."
The distribution mass is then defined with normal-ized confidence
GkA$ &amp; V D 7 Z 7 &lt;\ Cj(  r kr $ &amp;$ V DE7 Z 7 \&lt;( # Vh&amp; V D 7 Z 7 \)( (4)
"In[REF_CITE], it is shown that PMM and other non-generative finite-state models based on next-state classifiers share a weakness which they called the label bias problem: the transitions leaving a given state compete only against each other, rather than against all other transitions in the model."
They proposed Conditional Random Fields (CRFs) as so-lution to this problem.[REF_CITE]proposed a new algorithm for pa-rameter estimation as an alternate to CRF.
The new algorithm was similar to maximum-entropy model except that it skipped the local normalization step.
"Intuitively, it is the local normalization that makes distribution mass of the transitions leaving a given state incomparable with all other transitions."
"It is noted[REF_CITE]that SNoW’s output provides, in addition to the prediction, a ro-bust confidence level in the prediction, which en-ables its use in an inference algorithm that combines predictors to produce a coherent inference."
"In that paper, SNoW’s output is used to estimate the proba-bility of open and close tags."
"In general, the proba-bility of a tag can be estimated as follows # k $ &amp; V D7 Z 7 &lt;\ (?j  $$ Vh$&amp;"
V D 7 Z 7 )\ ( 3 Vh&amp;V D 7 Z 7 )\ (I 3 ( 7 (5) as one of the anonymous reviewers has suggested.
"However, this makes probabilities comparable D only within the transitions of the same history V ."
"An alternative to this approach is to use the SNoW’s output directly in the prediction combination, which makes transitions of different history comparable, since the SNoW’s output provides a robust confi-dence level in the prediction."
"Furthermore, in order to make sure that the confidences are not too sharp, we use the confidence defined in (3)."
"In addition, we use two supertaggers, one scans from left to right and the other scans from right to left."
Then we combine the results via pairwise vot-ing as in (van[REF_CITE]) as the final supertag.
This approach of vot-ing also helps to cope with the label bias problem.
GkA$ &amp; V D7 Z 7 \&lt;( is estimated within a 5-word window # plus two head supertags before the current word.
"For each | e e e |  1 word, \ *[ d , ] the d 91 | e e basic e |  1 features D d are | d Z 6 * [ d 91   91 | 9 6 , the two head supertags, V before * V the 91 current 9 and word."
Gk # &gt;mn$ V d &amp; V d 9 6h7 Z 7 \)( Gk * # &gt;mn$ V d &amp;V d 91 |d 9  [ d 91 d8 1 7 ] d 91  1 7   91 | 9 6 (
A basic feature is called active for word [ d if and only if the corresponding word/POS-tag/supertag appears at a specified place around [ d .
For our SNoW classifiers we use unigram and bigram of ba-sic features as our feature set.
A feature defined as a bigram of two basic features is active if and only if the two basic features are both active.
"The value of a feature of [ d is set to 1 if this feature is active for [ d , or 0 otherwise."
The feature sets used in the MEMM model were similar to ours.
"In addi-tion, prefix and suffix features were used to handle rare words."
Several MEMM supertaggers were im-plemented based on distinct feature sets.
"In[REF_CITE], SNoW was used for text chunking."
"The IOB tagging model in that pa-per was similar to our model for supertagging, but there are some differences."
They did not decom-pose the SNoW classifier with respect to POS tags.
"They used two-level deterministic ( beam-width=1 ) search, in which the second level IOB classifier takes the IOB output of the first classifier as input features."
"In our experiments, we use the default settings of the SNoW promotion parameter, demotion parame-ter and the threshold value given by the SNoW sys-tem."
"We train our model on the training data for 2 rounds, only counting the features that appear for at least 5 times."
"We skip the normalization step in test, and we use beam search with the width of 5."
"In our first experiment, we use the same dataset as that[REF_CITE]for our experiments."
"We use[REF_CITE]through 24 expect section 20 as training data, and use section 20 as test data."
Both training and test data are first tagged by Brill’s POS tagger[REF_CITE].
We use the same pair-wise voting algorithm as[REF_CITE].
We run supertagging on the training data and use the su-pertagging result to generate the mapping table used in pairwise voting.
"The SNoW supertagger scanning from left to right achieves an accuracy of  , and the one scanning from right to left achieves an accuracy of  ."
"By combining the results of these two su-pertaggers with pairwise voting, we achieve an ac-curacy of  , an error reduction of  com-pared to  , the best supertagging result to date[REF_CITE]."
Table 1 shows the comparison with previous work.
"Our algorithm, which is coded in Java, takes about 10 minutes to supertag the test data with a P3 1.13GHz processor."
"However,[REF_CITE], the accuracy of  was achieved by a Viterbi search program that took about 5 days to supertag the test data."
"The counterpart of our algorithm[REF_CITE]is the beam search on Model 8 with width of 5, which is the same as the beam width in our algorithm."
"Compared with this program, our al-gorithm achieves an error reduction of  .[REF_CITE]achieved an accuracy of  by combination of 5 distinct supertaggers."
"However, our result is achieved by combining out-puts of two homogeneous supertaggers, which only differ in scan direction."
Our next experiment is with the set of supertags abstracted from PTB with Fei Xia’s LexTract[REF_CITE].
"Xia extracted an LTAG-style grammar from PTB, and repeated Srinivas’ experiment[REF_CITE]on her supertag set."
We have experimented with our model on 1 and her dataset.
"We train our left-to-right model[REF_CITE]through 21 of PTB, and test on section 22 and 23."
We achieve an average error reduction of  .
The reason why the accuracy is rather low is that systems using 1 have to cope with much more ambiguities due the large size of the supertag set.
The results are shown in Table 2.
We test on both normalized and unnormalized models with both hand coded supertag set and auto-extracted supertag set.
We use the left-to-right SNoW model in these experiments.
The results in Table 3 show that skipping the local normalization improves performance in all the systems.
The ef-fect of skipping normalization is more significant on auto-extracted tags.
We think this is because sparse data is more vulnerable to the label bias problem.
Now we come back to the NP chunking problem.
The standard dataset of NP chunking consists[REF_CITE]-18 as train data and section 20 as test data.
"In our approach, we substitute the supertags for the POS tags in the dataset."
The new data look as follows.
For B Pnxs O the B
I nine B Dnx I months
A NXN I
"The first field is the word, the second is the su-pertag of the word, and the last is the IOB tag."
"We first use the fast TBL[REF_CITE], a Transformation Based Learning algorithm, to re-peat Ramshaw and Marcus’ experiment, and then apply the same program to our new dataset."
"We have trained another supertagger that is trained on 776K words[REF_CITE]-14 and 21-24, and it is tuned with 44K words[REF_CITE]."
We use this supertagger to supertag section 15-18 and sec-tion 20.
"We train an NP Chunker on section 15-18 with fast TBL, and test it on section 20."
"There is a small problem with the supertag set that we have been using, as far as NP chunking is con-cerned."
Two words with different POS tags may be tagged with the same supertag.
For example both de-terminer (DT) and number (CD) can be tagged with B Dnx.
However this will be harmful in the case
Test data is[REF_CITE].
A = Accuracy of IOB tagging.
P = NP chunk Precision.
R = NP chunk Recall.
F = F-score.
Brill-POS = fast TBL with Brill’s POS tags.
Tri-STAG = fast TBL with supertags given by Srinivas’ trigram-based supertagger.
SNoW-STAG = fast TBL with supertags given by our SNoW supertagger.
SNoW-STAG2 = fast TBL with augmented supertags given by our SNoW supertagger.
GOLD-POS = fast TBL with gold standard POS tags.
GOLD-STAG = fast TBL with gold standard supertags. of NP Chunking.
"As a solution, we use augmented supertags that have the POS tag of the lexical item specified."
An augmented supertag can also be re-garded as concatenation of a supertag and a POS tag.
For B Pnxs(IN)
O the B Dnx(DT)
I nine B Dnx(CD)
I months A NXN(NNS)
The results are shown in Table 4.
"The system using augmented supertags achieves an F-score of  , or an error reduction of  below the baseline of using Brill POS tags."
"Although these two systems are both trained with the same TBL algo-rithm, we implicitly employ more linguistic knowl-edge as the learning bias when we train the learn-ing machine with supertags."
"Supertags encode more syntactical information than POS tag do. companies ..., the POS tag of 4L T For example, in the sentence Three   leadingis VBGdrug, or present   participle, Three can. beBasedthe subjecton theoflocalleadingcontext."
"How-of  ever, the supertag of leading is B An, which repre-sents a modifier of a noun."
"With this extra informa-tion, the chunker can easily solve the ambiguity."
We find many instances like this in the test data.
It is important to note that the accuracy of su-pertag itself is much lower than that of POS tag while the use of supertags helps to improve the over-all performance.
"On the other hand, since the accu-racy of supertagging is rather lower, there is more room left for improving."
"If we use gold standard POS tags in the previ-ous experiment, we can only achieve an F-score of  ."
"However, if we use gold standard supertags in our previous experiment, the F-score is as high as  ."
This tells us how much room there is for further improvements.
Improvements in su-pertagging may give rise to further improvements in chunking.
We have proposed the use of supertags in the NP chunking task in order to use more syntactical de-pendencies which are unavailable with POS tags.
"In order to train a supertagger with a larger context, we have proposed a novel method of applying SNoW to the sequential model and have applied it to supertag-ging."
"Our algorithm takes advantage of rich feature sets, avoids the sparse-data problem, and forces the learning algorithm to focus on the difficult cases."
"Being aware of the fact that our algorithm may suf-fer from the label bias problem, we have used two methods to cope with this problem, and achieved de-sirable results."
We have tested our algorithms on both the hand-coded tag set used[REF_CITE]and su-pertags extracted for Penn Treebank(PTB).
"On the same dataset as that[REF_CITE], our new supertagger achieves an accuracy of  ."
"Com-pared with the supertaggers with the same decoding complexity[REF_CITE], our algorithm achieves an error reduction of  ."
We repeat Ramshaw and Marcus’ Transforma-tion Based NP chunking[REF_CITE]test by substituting supertags for POS tags in the dataset.
"The use of supertags in NP chunk-ing gives rise to almost absolute increase (from  to  ) in F-score under Transformation Based Learning(TBL) frame, or an error reduction of  ."
"The accuracy of  with our individual TBL chunker is close to results of POS-tag-based systems using advanced machine learning algorithms, such as  by voted MBL chunkers[REF_CITE],  by SNoW chunker[REF_CITE]."
The benefit of using a supertagger is obvious.
The su-pertagger provides an opportunity for advanced ma-chine learning techniques to improve their perfor-mance on chunking tasks by exploiting more syn-tactic information encoded in the supertags.
"To sum up, the supertagging algorithm presented here provides an effective and efficient way to em-ploy syntactic information."
Phrasal Verbs are an important feature of the English language.
Properly identifying them provides the basis for an English parser to decode the related structures.
Phrasal verbs have been a challenge to Natural Language Processing (NLP) because they sit at the borderline between lexicon and syntax.
Traditional NLP frameworks that separate the lexicon module from the parser make it difficult to handle this problem properly.
This paper presents a finite state approach that integrates a phrasal verb expert lexicon between shallow parsing and deep parsing to handle morpho-syntactic interaction.
"With precision/recall combined performance benchmarked consistently at 95.8%-97.5%, the Phrasal Verb identification problem has basically been solved with the presented method."
"Any natural language processing (NLP) system needs to address the issue of handling multiword expressions, including Phrasal Verbs (PV) [[REF_CITE]]."
This paper presents a proven approach to identifying English PVs based on pattern matching using a formalism called Expert Lexicon.
Phrasal Verbs are an important feature of the English language since they form about one third of the English verb vocabulary. [Footnote_1] Properly recognizing PVs is an important condition for English parsing.
"1 For the verb vocabulary of our system based on machine-readable dictionaries and two Phrasal Verb dictionaries, phrasal verb entries constitute 33.8% of the entries."
"Like single-word verbs, each PV has its own lexical features including subcategorization features that determine its structural patterns [[REF_CITE]], e.g., look for has syntactic subcategorization and semantic features similar to those of search; carry…on shares lexical features with continue."
"Such lexical features can be represented in the PV lexicon in the same way as those for single-word verbs, but a parser can only use them when the PV is identified."
Problems like PVs are regarded as ‘a pain in the neck for NLP’ [[REF_CITE]].
A proper solution to this problem requires tighter interaction between syntax and lexicon than traditionally available [[REF_CITE]].
"Simple lexical lookup leads to severe degradation in both precision and recall, as our benchmarks show (Section 4)."
"The recall problem is mainly due to separable PVs such as turn…off which allow for syntactic units to be inserted inside the PV compound, e.g., turn it off, turn the radio off."
The precision problem is caused by the ambiguous function of the particle.
"For example, a simple lexical lookup will mistag looked for as a phrasal verb in sentences such as He looked for quite a while but saw nothing."
"In short, the traditional NLP framework that separates the lexicon module from a parser makes it difficult to handle this problem properly."
This paper presents an expert lexicon approach that integrates the lexical module with contextual checking based on shallow parsing results.
"Extensive blind benchmarking shows that this approach is very effective for identifying phrasal verbs, resulting in the precision/recall combined F-score of about 96%."
The remaining text is structured as follows.
Section 2 presents the problem and defines the task.
Section 3 presents the Expert Lexicon formalism and illustrates the use of this formalism in solving this problem.
"Section 4 shows the benchmarking and analysis, followed by conclusions in Section 5."
"This section defines the problems we intend to solve, with a checklist of tasks to accomplish."
"First, we define the task as the identification of PVs in support of deep parsing, not as the parsing of the structures headed by a PV."
"These two are separated as two tasks not only because of modularity considerations, but more importantly based on a natural labor division between NLP modules."
Essential to the second argument is that these two tasks are of a different linguistic nature: the identification task belongs to (compounding) morphology (although it involves a syntactic interface) while the parsing task belongs to syntax.
"The naturalness of this division is reflected in the fact that there is no need for a specialized, PV-oriented parser."
"The same parser, mainly driven by lexical subcategorization features, can handle the structural problems for both phrasal verbs and other verbs."
The following active and passive structures involving the PVs look after (corresponding to watch) and carry…on (corresponding to continue) are decoded by our deep parser after PV identification: she is being carefully ‘looked after’ (watched); we should ‘carry on’ (continue) the business for a while.
There has been no unified definition of PVs among linguists.
Semantic compositionality is often used as a criterion to distinguish a PV from a syntactic combination between a verb and its associated adverb or prepositional phrase [[REF_CITE]].
"In reality, however, PVs reside in a continuum from opaque to transparent in terms of semantic compositionality [[REF_CITE]]."
There exist fuzzy cases such as take something away [Footnote_2] that may be included either as a PV or as a regular syntactic sequence.
2 Single-word verbs like ‘take’ are often over-burdened with dozens of senses/uses. Treating marginal cases like ‘take…away’ as independent phrasal verb entries has practical benefits in relieving the burden and the associated noise involving ‘take’.
"There is agreement on the vocabulary scope for the majority of PVs, as reflected in the overlapping of PV entries from major English dictionaries."
English PVs are generally classified into three major types.
Type I usually takes the form of an intransitive verb plus a particle word that originates from a preposition.
"Hence the resulting compound verb has become transitive, e.g., look for, look after, look forward to, look into, etc."
"Type II typically takes the form of a transitive verb plus a particle from the set {on, off, up, down}, e.g., turn…on, take…off, wake…up, let…down."
"Marginal cases of particles may also include {out, in, away} such as take…away, kick …in, pull…out. [Footnote_3]"
"3 These three are arguably in the gray area. Since they do not fundamentally affect the meaning of the leading verb, we do not have to treat them as phrasal verbs. In principle, they can also be treated as adverb complements of verbs."
"Type III takes the form of an intransitive verb plus an adverb particle, e.g., get by, blow up, burn up, get off, etc."
"Note that Type II and Type III PVs have considerable overlapping in vocabulary, e.g., The bomb blew up vs. The clown blew up the balloon."
The overlapping phenomenon can be handled by assigning both a transitive feature and an intransitive feature to the identified PVs in the same way that we treat the overlapping of single-word verbs.
The first issue in handling PVs is inflection.
"A system for identifying PVs should match the inflected forms, both regular and irregular, of the leading verb."
The second is the representation of the lexical identity of recognized PVs.
This is to establish a PV (a compound word) as a syntactic atomic unit with all its lexical properties determined by the lexicon [[REF_CITE]].
The output of the identification module based on a PV lexicon should support syntactic analysis and further processing.
"This translates into two sub-tasks: (i) lexical feature assignment, and (ii) canonical form representation."
"After a PV is identified, its lexical features encoded in the PV lexicon should be assigned for a parser to use."
The representation of a canonical form for an identified PV is necessary to allow for individual rules to be associated with identified PVs in further processing and to facilitate verb retrieval in applications.
"For example, if we use turn_off as the canonical form for the PV turn…off, identified in both he turned off the radio and he turned the radio off, a search for turn_off will match all and only the mentions of this PV."
The fact that PVs are separable hurts recall.
"In particular, for Type II, a Noun Phrase (NP) object can be inserted inside the compound verb."
NP insertion is an intriguing linguistic phenomenon involving the morpho-syntactic interface: a morphological compounding process needs to interact with the formation of a syntactic unit.
"Type I PVs also have the separability problem, albeit to a lesser degree."
"The possible inserted units are adverbs in this case, e.g., look everywhere for, look carefully after."
What hurts precision is spurious matches of PV negative instances.
"In a sentence with the structure V+[P+NP], [V+P] may be mistagged as a PV, as seen in the following pairs of examples for Type I and Type II: (1a)"
She [looked for] you yesterday. (1b)
She looked [for quite a while] (but saw nothing). (2a)
She [put on] the coat. (2b)
She put [on the table] the book she borrowed yesterday.
"To summarize, the following is a checklist of problems that a PV identification system should handle: (i) verb inflection, (ii) lexical identity representation, (iii) separability, and (iv) negative instances."
"Two lines of research are reported in addressing the PV problem: (i) the use of a high-level grammar formalism that integrates the identification with parsing, and (ii) the use of a finite state device in identifying PVs as a lexical support for the subsequent parser."
Both approaches have their own ways of handling the morpho-syntactic interface. [[REF_CITE]] and [[REF_CITE]] present their project LinGO-ERG that handles PV identification and parsing together.
"LingGO-ERG is based on Head-driven Phrase Structure Grammar (HPSG), a unification-based grammar formalism."
HPSG provides a mono-stratal lexicalist framework that facilitates handling intricate morpho-syntactic interaction.
PV-related morphological and syntactic structures are accounted for by means of a lexical selection mechanism where the verb morpheme subcategorizes for its syntactic object in addition to its particle morpheme.
The LingGO-ERG lexicalist approach is believed to be effective.
"However, their coverage and testing of the PVs seem preliminary."
"The LinGO-ERG lexicon contains 295 PV entries, with no report on benchmarks."
"In terms of the restricted flexibility and modifiability of a system, the use of high-level grammar formalisms such as HPSG to integrate identification in deep parsing cannot be compared with the alternative finite state approach [[REF_CITE]]. [[REF_CITE]]’s approach is similar to our work."
"Multiword expressions including idioms, collocations, and compounds as well as PVs are accounted for by using local grammar rules formulated as regular expressions."
"There is no detailed description for English PV treatment since their work focuses on multilingual, multi-word expressions in general."
"The authors believe that the local grammar implementation of multiword expressions can work with general syntax either implemented in a high-level grammar formalism or implemented as a local grammar for the required morpho-syntactic interaction, but this interaction is not implemented into an integrated system and hence it is impossible to properly measure performance benchmarks."
"There is no report on implemented solutions covering the entire English PVs that are fully integrated into an NLP system and are well tested on sizable real life corpora, as is presented in this paper."
"This section illustrates the system architecture and presents the underlying Expert Lexicon (EL) formalism, followed by the description of the implementation details."
Figure 1 shows the system architecture that contains the PV Identification Module based on the PV Expert Lexicon.
This is a pipeline system mainly based on pattern matching implemented in local grammars and/or expert lexicons [[REF_CITE]]. [Footnote_4]
4 POS and NE tagging are hybrid systems involving both hand-crafted rules and statistical learning.
English parsing is divided into two tasks: shallow parsing and deep parsing.
"The shallow parser constructs Verb Groups (VGs) and basic Noun Phrases (NPs), also called BaseNPs [[REF_CITE]]."
"The deep parser utilizes syntactic subcategorization features and semantic features of a head (e.g., VG) to decode both syntactic and logical dependency relationships such as Verb-Subject, Verb-Object, Head-Modifier, etc."
The general lexicon lookup component involves stemming that transforms regular or irregular inflected verbs into the base forms to facilitate the later phrasal verb matching.
This component also performs indexing of the word occurrences in the processed document for subsequent expert lexicons.
The PV Identification Module is placed between the Shallow Parser and the Deep Parser.
It requires shallow parsing support for the required syntactic interaction and the PV output provides lexical support for deep parsing.
Results after shallow parsing form a proper basis for PV identification.
"First, the inserted NPs and adverbial time NEs are already constructed by the shallow parser and NE tagger."
This makes it easy to write pattern matching rules for identifying separable PVs.
"Second, the constructed basic units NE, NP and VG provide conditions for constraint-checking in PV identification."
"For example, to prevent spurious matches in sentences like she put the coat on the table, it is necessary to check that the post-particle unit should NOT be an NP."
"The VG chunking also decodes the voice, tense and aspect features that can be used as additional constraints for PV identification."
"A sample macro rule active_V_Pin that checks the ‘NOT passive’ constraint and the ‘NOT time’, ‘NOT location’ constraints is shown in 3.3."
The Expert Lexicon used in our system is an index-based formalism that can associate pattern matching rules with lexical entries.
"It is organized like a lexicon, but has the power of a lexicalized local grammar."
"All Expert Lexicon entries are indexed, similar to the case for the finite state tool in INTEX [[REF_CITE]]."
The pattern matching time is therefore reduced dramatically compared to a sequential finite state device [[REF_CITE]]. [Footnote_5]
"5 Some other unique features of our EL formalism include: (i) providing the capability of proximity checking as rule constraints in addition to pattern matching using regular expressions so that the rule writer or lexicographer can exploit the combined advantages of both, and (ii) the propagation functionality of semantic tagging results, to accommodate principles like one sense per discourse."
"The expert lexicon formalism is designed to enhance the lexicalization of our system, in accordance with the general trend of lexicalist approaches to NLP."
It is especially beneficial in handling problems like PVs and many individual or idiosyncratic linguistic phenomena that can not be covered by non-lexical approaches.
"Unlike the extreme lexicalized word expert system in [[REF_CITE]] and similar to the IDAREX local grammar formalism [[REF_CITE]], our EL formalism supports a parameterized macro mechanism that can be used to capture the general rules shared by a set of individual entries."
"This is a particular useful mechanism that will save time for computational lexicographers in developing expert lexicons, especially for phrasal verbs, as shall be shown in Section 3.3 below."
"The Expert Lexicon tool provides a flexible interface for coordinating lexicons and syntax: any number of expert lexicons can be placed at any levels, hand-in-hand with other non-lexicalized modules in the pipeline architecture of our system."
"To cover the three major types of PVs, we use the macro mechanism to capture the shared patterns."
"For example, the NP insertion for Type II PV is handled through a macro called V_NP_P, formulated in pseudo code as follows."
"V_NP_P($V,$P,$V_P,$F1, $F2,…) :="
"This macro represents cases like Take the coat off, please; put it back on, it’s raining now."
"It consists of two parts: ‘Pattern’ in regular expression form (with parentheses for optionality, a bar for logical OR, a quoted string for checking a word or head word) and ‘Action’ (signified by the prefix %)."
"The parameters used in the macro (marked by the prefix $) include the leading verb $V, particle $P, the canonical form $V_P, and features $F n. After the defined pattern is matched, a Type II separable verb is identified."
"The Action part ensures that the lexical identity be represented properly, i.e. the assignment of the lexical features and the canonical form."
The deactivate action flags the particle as being part of the phrasal verb.
"In addition, to prevent a spurious case in (3b), the macro V_NP_P checks the contextual constraints that no NP (i.e. NOT NP) should follow a PV particle."
"In our shallow parsing, NP chunking does not include identified time NEs, so it will not block the PV identification in (3c). (3a)"
She [put the coat on]. (3b)
She put the coat [on the table]. (3c)
She [put the coat on] yesterday.
"All three types of PVs when used without NP insertion are handled by the same set of macros, due to the formal patterns they share."
"We use a set of macros instead of one single macro, depending on the type of particle and the voice of the verb, e.g., look for calls the macro [active_V_Pfor | passive_V_Pfor], fly in calls the macro [active_V_Pin | passive_V_Pin], etc."
The distinction between active rules and passive rules lies in the need for different constraints.
"For example, a passive rule needs to check the post-particle constraint [NOT NP] to block the spurious case in (4b). (4a)"
He [turned on] the radio. (4b)
The world [had been turned] [on its head] again.
"As for particles, they also require different constraints in order to block spurious matches."
"For example, active_V_Pin (formulated below) requires the constraints ‘NOT location NOT time’ after the particle while active_V_Pfor only needs to check ‘NOT time’, shown in (5) and (6). (5a)"
Howard [had flown in] from Atlanta. (5b)
The rocket [would fly] [in 1999]. (6a)
She was [looking for] California on the map. (6b)
"She looked [for quite a while]. active_V_Pin($V, in, $V_P,$F1, $F2,…) :="
Pattern: $V NOT passive (Adv|time) $P
NOT location NOT time
"Action: $V: %assign_feature($F1, $F2, …) %assign_canonical_form($V_P) $P: %deactivate"
The coding of the few PV macros requires skilled computational grammarians and a representative development corpus for rule debugging.
"In our case, it was approximately 15 person-days of skilled labor including data analysis, macro formulation and five iterations of debugging against the development corpus."
"But after the PV macros are defined, lexicographers can quickly develop the PV entries: it only cost one person-day to enter the entire PV vocabulary using the EL formalism and the implemented macros."
We used the Cambridge International Dictionary of Phrasal Verbs and Collins Cobuild Dictionary of Phrasal Verbs as the major reference for developing our PV Expert
"6 Some entries that are listed in these dictionaries do not seem to belong to phrasal verb categories, e.g., relieve…of (as used in relieve somebody of something), remind…of (as used in remind somebody of something), etc. It is generally agreed that such cases belong to syntactic patterns in the form of V+NP+P+NP that can be captured by subcategorization. We have excluded these cases."
"This expert lexicon contains 2,590 entries."
The EL-rules are ordered with specific rules placed before more general rules.
"A sample of the developed PV Expert Lexicon is shown below (the prefix @ denotes a macro call): abide: @V_P_by(abide, by, abide_by, V6A,"
"APPROVING_AGREEING) accede: @V_P_to(accede, to, accede_to, V6A,"
"APPROVING_AGREEING) add: @V_P(add, up, add_up, V2A, MATH_REASONING); @V_NP_P(add, up, add_up, V6A, MATH_REASONING) …………"
"In the above entries, V6A and V2A are subcategorization features for transitive and intransitive verb respectively, while APPROVING_AGREEING and MATH_REASONING are semantic features."
These features provide the lexical basis for the subsequent parser.
The PV identification method as described above resolves all the problems in the checklist.
The following sample output shows the identification result:
NP[That] VG[could slow: slow_down/V6A/MOVING] NP[him] down/deactivated .
Blind benchmarking was done by two non-developer testers manually checking the results.
"In cases of disagreement, a third tester was involved in examining the case to help resolve it."
We ran benchmarking on both the formal style and informal style of English text.
Our development corpus (around 500 KB) consists of the MUC-7 (Message Understanding
Conference-7) dryrun corpus and an additional collection of news domain articles from TREC (Text Retrieval Conference) data.
"The PV expert lexicon rules, mainly the macros, were developed and debugged using the development corpus."
The first testing corpus (called English-zone corpus) was downloaded from a website that is designed to teach PV usage in Colloquial English[URL_CITE]als.html).
It consists of 357 lines of sample sentences containing 347 PVs.
This addresses the sparseness problem for the less frequently used PVs that rarely get benchmarked in running text testing.
"This is a concentrated corpus involving varieties of PVs from text sources of an informal style, as shown below. [Footnote_7] &quot;Would you care for some dessert?"
"7 Proper treatment of PVs is most important in parsing text sources involving Colloquial English, e.g., interviews, speech transcripts, chat room archives. There is an increasing demand for NLP applications in handling this type of data."
"We have ice cream, cookies, or cake.&quot;"
Why are you wrapped up in that blanket?
"After John&apos;s wife died, he had to get through his sadness."
"After my sister cut her hair by herself, we had to take her to a hairdresser to even her hair out!"
"After the fire, the family had to get by without a house."
"We have prepared two collections from the running text data to test written English of a more formal style in the general news domain: (i) the MUC-7 formal run corpus (342 KB) consisting of 99 news articles, and (ii) a collection of 23,557 news articles (105MB) from the TREC data."
There is no available system known to the NLP community that claims a capability for PV treatment and could thus be used for a reasonable performance comparison.
"Hence, we have devised a bottom-line system and a baseline system for comparison with our EL-driven system."
The bottom-line system is defined as a simple lexical lookup procedure enhanced with the ability to match inflected verb forms but with no capability of checking contextual constraints.
There is no discussion in the literature on what constitutes a reasonable baseline system for PV.
"We believe that a baseline system should have the additional, easy-to-implement ability to jump over inserted object case pronouns (e.g., turn it on) and adverbs (e.g., look everywhere for) in PV identification."
Both the MUC-7 formal run corpus and the English-zone corpus were fed into the bottom-line and the baseline systems as well as our EL-driven system described in Section 3.3.
The benchmarking results are shown in Table 1 and Table 2.
"The F-score is a combined measure of precision and recall, reflecting the overall performance of a system."
"Compared with the bottom-line performance and the baseline performance, the F-score for the presented method has surged 9-20 percentage points and 4-14 percentage points, respectively."
"The high precision (100%) in Table 2 is due to the fact that, unlike running text, the sampling corpus contains only positive instances of PV."
"This weakness, often associated with sampling corpora, is overcome by benchmarking running text corpora (Table 1 and Table 3)."
"To compensate for the limited size of the MUC formal run corpus, we used the testing corpus from the TREC data."
"For such a large testing corpus (23,557 articles, 105MB), it is impractical for testers to read every article to count mentions of all PVs in benchmarking."
"Therefore, we selected three representative PVs look for, turn…on and blow…up and used the head verbs (look, turn, blow), including their inflected forms, to retrieve all sentences that contain those verbs."
We then ran the retrieved sentences through our system for benchmarking (Table 3).
"All three of the blind tests show fairly consistent benchmarking results (F-score 95.8%-97.5%), indicating that these benchmarks reflect the true capability of the presented system, which targets the entire PV vocabulary instead of a selected subset."
"Although there is still some room for further enhancement (to be discussed shortly), the PV identification problem is basically solved."
"There are two major factors that cause errors: (i) the impact of errors from the preceding modules (POS and Shallow Parsing), and (ii) the mistakes caused by the PV Expert Lexicon itself."
"The POS errors caused more problems than the NP grouping errors because the inserted NP tends to be very short, posing little challenge to the BaseNP shallow parsing."
Some verbs mis-tagged as nouns by POS were missed in PV identification.
There are two problems that require the fine-tuning of the PV Identification Module.
"First, the macros need further adjustment in their constraints."
Some constraints seem to be too strong or too weak.
"For example, in the Type I macro, although we expected the possible insertion of an adverb, however, the constraint on allowing for only one optional adverb and not allowing for a time adverbial is still too strong."
"As a result, the system failed to identify listening…to and meet…with in the following cases: …was not listening very closely on Thursday to American concerns about human tights… and ... meet on Friday with his Chinese..."
The second type of problems cannot be solved at the macro level.
These are individual problems that should be handled by writing specific rules for the related PV.
An example is the possible spurious match of the PV have…out in the sentence ...still have our budget analysts out working the numbers.
"Since have is a verb with numerous usages, we should impose more individual constraints for NP insertion to prevent spurious matches, rather than calling a common macro shared by all Type II verbs."
"To test the efficiency of the index-based PV Expert Lexicon in comparison with a sequential Finite State Automaton (FSA) in the PV identification task, we conducted the following experiment."
"The PV Expert Lexicon was compiled as a regular local grammar into a large automaton that contains 97,801 states and 237,302 transitions."
"For a file of 104 KB (the MUC-7 dryrun corpus of 16,878 words), our sequential FSA runner takes over 10 seconds for processing on the Windows NT platform with a Pentium PC."
This processing only requires 0.36 second using the indexed PV Expert Lexicon module.
This is about 30 times faster.
An effective and efficient approach to phrasal verb identification is presented.
This approach handles both separable and inseparable phrasal verbs in English.
An Expert Lexicon formalism is used to develop the entire phrasal verb lexicon and its associated pattern matching rules and macros.
This formalism allows the phrasal verb lexicon to be called between two levels of parsing for the required morpho-syntactic interaction in phrasal verb identification.
Benchmarking using both the running text corpus and sampling corpus shows that the presented approach provides a satisfactory solution to this problem.
"In future research, we plan to extend the successful experiment on phrasal verbs to other types of multi-word expressions and idioms using the same expert lexicon formalism."
"This paper presents a dependency language model (DLM) that captures linguistic con-straints via a dependency structure, i.e., a set of probabilistic dependencies that express the relations between headwords of each phrase in a sentence by an acyclic, planar, undirected graph."
Our contributions are three-fold.
"First, we incorporate the de-pendency structure into an n-gram language model to capture long distance word de-pendency."
"Second, we present an unsuper-vised learning method that discovers the dependency structure of a sentence using a bootstrapping procedure."
"Finally, we evaluate the proposed models on a realistic application (Japanese Kana-Kanji conver-sion)."
Experiments show that the best DLM achieves an 11.3% error rate reduction over the word trigram model.
"In recent years, many efforts have been made to utilize linguistic structure in language modeling, which for practical reasons is still dominated by trigram-based language models."
"There are two major obstacles to successfully incorporating lin-guistic structure into a language model: (1) captur-ing longer distance word dependencies leads to higher-order n-gram models, where the number of parameters is usually too large to estimate; (2) capturing deeper linguistic relations in a language model requires a large annotated training corpus and a decoder that assigns linguistic structure, which are not always available."
"This paper presents a new dependency language model (DLM) that captures long distance linguistic constraints between words via a dependency structure, i.e., a set of probabilistic dependencies that capture linguistic relations between headwords of each phrase in a sentence."
"To deal with the first obstacle mentioned above, we approximate long-distance linguistic dependency by a model that is similar to a skipping bigram model in which the prediction of a word is conditioned on exactly one other linguistically related word that lies arbitrarily far in the past."
"This dependency model is then in-terpolated with a headword bigram model and a word trigram model, keeping the number of pa-rameters of the combined model manageable."
"To overcome the second obstacle, we used an unsu-pervised learning method that discovers the de-pendency structure of a given sentence using an Expectation-Maximization (EM)-like procedure."
"In this method, no manual syntactic annotation is required, thereby opening up the possibility for building a language model that performs well on a wide variety of data and languages."
"The proposed model is evaluated using Japanese Kana-Kanji conversion, achieving significant error rate reduc-tion over the word trigram model."
"A trigram language model predicts the next word based only on two preceding words, blindly dis-carding any other relevant word that may lie three or more positions to the left."
"Such a model is likely to be linguistically implausible: consider the Eng-lish sentence in Figure 1(a), where a trigram model would predict cried from next seat, which does not agree with our intuition."
"In this paper, we define a dependency structure of a sentence as a set of probabilistic dependencies that express linguistic relations between words in a sentence by an acyclic, planar graph, where two related words are con-nected by an undirected graph edge (i.e., we do not differentiate the modifier and the head in a de- pendency)."
"The dependency structure for the sen-tence in Figure 1(a) is as shown; a model that uses this dependency structure would predict cried from baby, in agreement with our intuition. (a) [A baby] [in the next seat] cried [throughout the flight] (b) [ ⿒ߜ߾ࠎ / ߇ ] [ 㘧ⴕᯏ / ߩ ] [ 㓞 / ߩ ] [ Ꮸ / ߢ ] [ ߕߞߣ ] [ ᵅ޿ߡ / ޿ߚ ] Figure 1."
Examples of dependency structure. (a) A dependency structure of an English sentence.
Square brackets indicate base NPs; underlined words are the headwords. (b) A Japanese equivalent of (a).
Slashes demarcate morpheme boundaries; square brackets indicate phrases (bunsetsu).
A Japanese sentence is typically divided into non-overlapping phrases called bunsetsu.
"As shown in Figure 1(b), each bunsetsu consists of one con-tent word, referred to here as the headword H, and several function words F. Words (more precisely, morphemes) within a bunsetsu are tightly bound with each other, which can be adequately captured by a word trigram model."
"However, headwords across bunsetsu boundaries also have dependency relations with each other, as the diagrams in Figure 1 show."
Such long distance dependency relations are expected to provide useful and complementary information with the word trigram model in the task of next word prediction.
"In constructing language models for realistic applications such as speech recognition and Asian language input, we are faced with two constraints that we would like to satisfy: First, the model must operate in a left-to-right manner, because (1) the search procedures for predicting words that corre-spond to the input acoustic signal or phonetic string work left to right, and (2) it can be easily combined with a word trigram model in decoding."
"Second, the model should be computationally feasible both in training and decoding."
"In the next section, we offer a DLM that satisfies both of these constraints."
The DLM attempts to generate the dependency structure incrementally while traversing the sen-tence left to right.
It will assign a probability to every word sequence W and its dependency struc- ture D.
"The probability assignment is based on an encoding of the (W, D) pair described below."
"Let W be a sentence of length n words to which we have prepended &lt;s&gt; and appended &lt;/s&gt; so that w 0 = &lt;s&gt; , and w n+[Footnote_1] = &lt;/s&gt; ."
"1 The model in Equation (3) is not strictly probabilistic because it drops the probabilities of illegal dependencies (e.g., crossing dependencies)."
"In principle, a language model recovers the probability of a sentence P(W) over all possible D given W by estimating the joint probability P(W, D): P(W) = ∑ D P(W, D)."
"In prac-tice, we used the so-called maximum approximation where the sum is approximated by a single term P(W, D * ):"
"P(W ) = ∑ P(W, D) ≈ P(W, D ∗ ) . (1) D"
"Here, D * is the most probable dependency structure of the sentence, which is generally discovered by maximizing P(W, D):"
"D ∗ = arg max P(W, D) . (2) D"
"Below we restrict the discussion to the most prob-able dependency structure of a given sentence, and simply use D to represent D * ."
"In the remainder of this section, we first present a statistical dependency parser, which estimates the parsing probability at the word level, and generates D incrementally while traversing W left to right."
"Next, we describe the elements of the DLM that assign probability to each possible W and its most probable D, P(W, D)."
"Fi-nally, we present an EM-like iterative method for unsupervised learning of dependency structure."
The aim of dependency parsing is to find the most probable D of a given W by maximizing the prob-ability P(D|W).
"Let D be a set of probabilistic de-pendencies d, i.e. d ∈ D."
"Assuming that the de-pendencies are independent of each other, we have"
P(D |W ) = ∏ P(d |W ) (3) d∈D where P(d|W) is the dependency probability condi-tioned by a particular sentence. 1
It is impossible to estimate P(d|W) directly because the same sentence is very unlikely to appear in both training and test data.
"We thus approximated P(d|W) by P(d), and estimated the dependency probability from the training corpus."
"Let d ij = (w i , w j ) be the dependency between w i and w j ."
The maximum likelihood esti-mation (MLE) of P(d ij ) is given by (4) P(d ij ) =
"C(w i ,w j ,R) C(w i ,w j ) where C(w i , w j , R) is the number of times w i and w j have a dependency relation in a sentence in training data, and"
"C(w i , w j ) is the number of times w i and w j are seen in the same sentence."
"To deal with the data sparseness problem of MLE, we used the backoff estimation strategy similar to the one proposed[REF_CITE], which backs off to estimates that use less conditioning context."
"More specifically, we used the following three estimates:"
"E 1 = δη E = η E 4 = ηδ , 1 2 + η [Footnote_3] (5) 4 1 23 δ 2 + δ [Footnote_3] 4 Where η 1 = C(w i , w j , R) , δ 1 = C(w i , w j ) , η 2 = C(w i ,*,R) , δ 2 = C(w i ,*) , η [Footnote_3] = C(*, w j , R) , δ [Footnote_3] = C(*,w j ) , η 4 = C(*,*, R) , δ 4 = C(*,*) . in which * indicates a wild-card matching any word."
"3 This operation leaves some headwords disconnected; in such a case, we assumed that each disconnected head-word has a dependency relation with its preceding headword."
"3 This operation leaves some headwords disconnected; in such a case, we assumed that each disconnected head-word has a dependency relation with its preceding headword."
"3 This operation leaves some headwords disconnected; in such a case, we assumed that each disconnected head-word has a dependency relation with its preceding headword."
"3 This operation leaves some headwords disconnected; in such a case, we assumed that each disconnected head-word has a dependency relation with its preceding headword."
The final estimate E is given by linearly interpolating these estimates:
E = λ 1 E 1 + (1− λ 1 )( λ 2 E 23 + (1− λ 2 )E 4 ) (6) where λ 1 and λ [Footnote_2] are smoothing parameters.
"2 For parsers that use bigram lexical dependencies,[REF_CITE]presents parsing algorithms that are O(n 4 ) or O(n 3 ). We thank Joshua Goodman for pointing this out."
"Given the above parsing model, we used an ap-proximation parsing algorithm that is O(n 2 )."
"Tradi-tional techniques use an optimal Viterbi-style algo-rithm (e.g., bottom-up chart parser) that is O(n 5 ). [Footnote_2]"
"2 For parsers that use bigram lexical dependencies,[REF_CITE]presents parsing algorithms that are O(n 4 ) or O(n 3 ). We thank Joshua Goodman for pointing this out."
"Although the approximation algorithm is not guaranteed to find the most probable D, we opted for it because it works in a left-to-right manner, and is very efficient and simple to implement."
"In our experiments, we found that the algorithm performs reasonably well on average, and its speed and sim-plicity make it a better choice in DLM training where we need to parse a large amount of training data iteratively, as described in Section 3.3."
The parsing algorithm is a slightly modified version of that proposed[REF_CITE].
"It reads a sentence left to right; after reading each new word w j , it tries to link w j to each of its previous words w i , and push the generated dependency d ij into a stack."
"When a dependency crossing or a cycle is detected in the stack, the dependency with the lowest de-pendency probability in conflict is eliminated."
"The algorithm is outlined in Figures 2 and 3. that P(d 23 ) is smaller than P(d 12 ) and P(d 13 ), d 23 is removed (represented as dotted line). (b) An example of a dependency crossing: given that P(d 13 ) is smaller than P(d 24 ), d 13 is removed."
"Let the dependency probability be the measure of the strength of a dependency, i.e., higher probabili-ties mean stronger dependencies."
"Note that when a strong new dependency crosses multiple weak dependencies, the weak dependencies are removed even if the new dependency is weaker than the sum of the old dependencies. [Footnote_3] Although this action results in lower total probability, it was imple-mented because multiple weak dependencies con-nected to the beginning of the sentence often pre- vented a strong meaningful dependency from being created."
"3 This operation leaves some headwords disconnected; in such a case, we assumed that each disconnected head-word has a dependency relation with its preceding headword."
"In this manner, the directional bias of the approximation algorithm was partially compen-sated for. [Footnote_4]"
"4 Theoretically, we should arrive at the same dependency structure no matter whether we parse the sentence left to right or right to left. However, this is not the case with the approximation algorithm. This problem is called direc-tional bias."
"The DLM together with the dependency parser provides an encoding of the (W, D) pair into a se-quence of elementary model actions."
Each action conceptually consists of two stages.
The first stage assigns a probability to the next word given the left context.
The second stage updates the dependency structure given the new word using the parsing algorithm in Figure 2.
"The probability P(W, D) is calculated as:"
"P(W, D) = (7) n ∏"
"P(w j | Φ(W j−1 ,D j−1 ))"
"P(D jj−1 | Φ(W j−1 ,D j−1 ),w j ) j=1"
"P(D jj−1 | Φ(W j−1 , D j−1 ), w j ) = (8) j ∏ P( p ij |W j−1 , D j−1 , p 1j ,..., p ij−1 ) . i=1"
"Here (W j-1 , D j-1 ) is the word-parse (j-1)-prefix that D j-1 is a dependency structure containing only those dependencies whose two related words are included in the word (j-1)-prefix, W j-1 . w j is the word to be predicted."
D j-1j is the incremental dependency structure that generates D j = D j-1 ||
"D j-1j (|| stands for concatenation) when attached to D j-1 ; it is the de-pendency structure built on top of D j-1 and the newly predicted word w j (see the for-loop of line 2 in Figure 2). p ij denotes the ith action of the parser at position j in the word string: to generate a new dependency d ij , and eliminate dependencies with the lowest dependency probability in conflict (see lines 4 – 7 in Figure 2)."
"Φ is a function that maps the history (W j-1 , D j-1 ) onto equivalence classes."
The model in Equation (8) is unfortunately in-feasible because it is extremely difficult to estimate the probability of p ij due to the large number of parameters in the conditional part.
"According to the parsing algorithm in Figure 2, the probability of each action p ij depends on the entire history (e.g. for detecting a dependency crossing or cycle), so any mapping Φ that limits the equivalence classi-fication to less context suitable for model estima-tion would be very likely to drop critical conditional information for predicting p ij ."
"In practice, we ap-proximated P(D j-1j | Φ (W , D ), w ) by P(D |W ) of Equation (3), yielding P(W j , D j ) ≈ P(W j | Φ (W , j-1 j-1 j j j j-1"
D j-1 ))
P(D j |W j ).
"This approximation is probabilisti-cally deficient, but our goal is to apply the DLM to a decoder in a realistic application, and the perform-ance gain achieved by this approximation justifies the modeling decision."
"Now, we describe the way P(w j | Φ (W ,D )) is j-1 j-1 estimated."
"As described in Section 2, headwords and function words play different syntactic and semantic roles capturing different types of de-pendency relations, so the prediction of them can better be done separately."
"Assuming that each word token can be uniquely classified as a headword or a function word in Japanese, the DLM can be con-ceived of as a cluster-based language model with two clusters, headword H and function word F. We can then define the conditional probability of w j based on its history as the product of two factors: the probability of the category given its history, and the probability of w j given its category."
"Let h j or f j be the actual headword or function word in a sentence, and let H j or F j be the category of the word w j ."
"P(w j | Φ (W ,D )) can then be formulated as: j-1 j-1"
"P(w j |Φ(W j−1 , D j−1 )) = (9) P(H j |Φ(W j−1 , D j−1 )) ×"
"P(w j |Φ(W j−1 ,D j−1 ),H j ) +"
"P(F j | Φ(W j−1 , D j−1 )) ×"
"P(w j | Φ(W j−1 , D j−1 ), F j ) ."
"We first describe the estimation of headword probability P(w j | Φ (W , D ), H )."
"Let HW be the j-1 j-1 j j-1 headwords in (j-1)-prefix, i.e., containing only those headwords that are included in W j-1 ."
"Because HW j-1 is determined by W j-1 , the headword prob-ability can be rewritten as P(w j | Φ (W , HW , D ), j-1 j-1 j-1 H j )."
The problem is to determine the mapping Φ so as to identify the related words in the left context that we would like to condition on.
"Based on the discussion in Section 2, we chose a mapping func-tion that retains (1) two preceding words w j-1 and w j-2 in W j-1 , (2) one preceding headword h j-1 in HW j-1 , and (3) one linguistically related word w i according to D j-1 . w i is determined in two stages:"
"First, the parser updates the dependency structure"
D j-1 incrementally to D j assuming that the next word is w j .
"Second, when there are multiple words that have dependency relations with w j in D j , w i is se-lected using the following decision rule: w i = arg max"
"P(w j | w i , R) , (10) w i :( w i ,w j )∈D j where the probability P(w j | w i , R) of the word w j given its linguistic related word w i is computed using MLE[REF_CITE]: (11) C(w i , w j , R) ."
"P(w j | w i , R) = ∑ C(w i , w j , R) w j"
"We thus have the mapping function Φ (W , HW , j-1 j-1"
"D j-1 ) = (w j-2, w j-1 , h j-1 , w i )."
The estimate of headword probability is an interpolation of three probabilities:
"P(w j |Φ(W j−1 , D j−1 ),H j ) = (12) λ 1 ( λ 2"
"P(w j | h j−1 , H j ) + (1− λ 2 )"
"P(w j | w i ,R)) + (1− λ 1 )"
"P(w j | w j−2 , w j−1 , H j ) ."
"Here P(w j |w j-2 , w j-1 , H j ) is the word trigram prob-ability given that w j is a headword, P(w j |h j-1 , H j ) is the headword bigram probability, and λ 1 , λ 2 ∈ [0,1] are the interpolation weights optimized on held-out data."
We now come back to the estimate of the other three probabilities in Equation (9).
"Following the work[REF_CITE], we used the unigram estimate for word category probabilities, (i.e.,"
"P(H j | Φ (W , D )) ≈ P(H ) and P(F | Φ (W , D )) ≈ j-1 j-1 j j j-1 j-1 P(F j )), and the standard trigram estimate for func-tion word probability (i.e., P(w j | Φ (W ,D ),F ) ≈ j-1 j-1 j"
"P(w j | w j-2, w j-1, F j ))."
Let C j be the category of w j ; we approximated P(C j )×
"P(w j |w j-2 , w j-1, C j ) by P(w j | w j-2, w j-1 )."
"By separating the estimates for the probabili-ties of headwords and function words, the final estimate is given below:"
"P(w j | Φ (W , D ))= (13) j-1 j-1  λ 1 (P(H j )( λ 2"
P(w j | h j−1 )  + (1− λ )
"P(w | w ,R))  2 j i   + (1 − λ 1 )"
"P(w j | w j − 2 ,w j − 1 )  w j : headword  "
"P(w j | w j−2 , w j−1 ) w j : function word"
All conditional probabilities[REF_CITE]are obtained using MLE on training data.
"In order to deal with the data sparseness problem, we used a backoff scheme[REF_CITE]for parameter estima-tion."
This backoff scheme recursively estimates the probability of an unseen n-gram by utilizing (n–1)-gram estimates.
"In particular, the probability[REF_CITE]backs off to the estimate of P(w j |R), which is computed as:"
P(w j | R) =
"C(w j , R) , (14) N where N is the total number of dependencies in training data, and C(w j , R) is the number of de-pendencies that contains w j ."
"To keep the model size manageable, we removed all n-grams of count less than 2 from the headword bigram model and the word trigram model, but kept all long-distance dependency bigrams that occurred in the training data."
"This section describes two methods that were used to tag raw text corpus for DLM training: (1) a method for headword detection, and (2) an unsu-pervised learning method for dependency structure acquisition."
"In order to classify a word uniquely as H or F, we used a mapping table created in the following way."
"We first assumed that the mapping from part-of-speech (POS) to word category is unique and fixed; [Footnote_5] we then used a POS-tagger to generate a POS-tagged corpus, which are then turned into a category-tagged corpus. [Footnote_6] Based on this corpus, we created a mapping table which maps each word to a unique category: when a word can be mapped to either H or F, we chose the more frequent category in the corpus."
"5 The tag set we used included 1,187 POS tags, of which 102 counted as headwords in our experiments."
"6 Since the POS-tagger does not identify phrases (bun-setsu), our implementation identifies multiple headwords in phrases headed by compounds."
This method achieved a 98.5% ac-curacy of headword detection on the test data we used.
"Given a headword-tagged corpus, we then used an EM-like iterative method for joint optimization of the parsing model and the dependency structure of training data."
"This method uses the maximum likelihood principle, which is consistent with lan- guage model training."
"There are three steps in the algorithm: (1) initialize, (2) (re-)parse the training corpus, and (3) re-estimate the parameters of the parsing model."
Steps (2) and (3) are iterated until the improvement in the probability of training data is less than a threshold.
Initialize: We set a window of size N and assumed that each headword pair within a headword N-gram constitutes an initial dependency.
The optimal value of N is 3 in our experiments.
"That is, given a headword trigram (h 1 , h 2 , h 3 ), there are 3 initial dependencies: d 12 , d 13 , and d 23 ."
"From the initial dependencies, we computed an initial dependency parsing model by Equation (4). (Re-)parse the corpus: Given the parsing model, we used the parsing algorithm in Figure 2 to select the most probable dependency structure for each sentence in the training data."
This provides an up-dated set of dependencies.
Re-estimate the parameters of parsing model: We then re-estimated the parsing model parameters based on the updated dependency set.
"In this study, we evaluated language models on the application of Japanese Kana-Kanji conversion, which is the standard method of inputting Japanese text by converting the text of a syllabary-based Kana string into the appropriate combination of Kanji and Kana."
"This is a similar problem to speech recognition, except that it does not include acoustic ambiguity."
"Performance on this task is measured in terms of the character error rate (CER), given by the number of characters wrongly converted from the phonetic string divided by the number of characters in the correct transcript."
"For our experiments, we used two newspaper corpora, Nikkei and Yomiuri Newspapers, both of which have been pre-word-segmented."
"We built language models from a 36-million-word subset of the Nikkei Newspaper corpus, performed parameter optimization on a 100,000-word subset of the Yo-miuri Newspaper (held-out data), and tested our models on another 100,000-word subset of the Yomiuri Newspaper corpus."
"The lexicon we used contains 167,107 entries."
"Our evaluation was done within a framework of so-called “N-best rescoring” method, in which a list of hypotheses is generated by the baseline language model (a word trigram model in this study), which is then rescored using a more sophisticated lan-guage model."
"We use the N-best list of N=100, whose “oracle” CER (i.e., the CER of the hy-potheses with the minimum number of errors) is presented in Table 1, indicating the upper bound on performance."
"We also note in Table 1 that the per-formance of the conversion using the baseline tri-gram model is much better than the state-of-the-art performance currently available in the marketplace, presumably due to the large amount of training data we used, and to the similarity between the training and the test data."
The results of applying our models to the task of Japanese Kana-Kanji conversion are shown in Table 2.
"The baseline result was obtained by using a conventional word trigram model (WTM). [Footnote_7] HBM stands for headword bigram model, which does not use any dependency structure (i.e. λ 2 = 1[REF_CITE])."
"7 For a detailed description of the baseline trigram model, see[REF_CITE]."
DLM_1 is the DLM that does not use head-word bigram (i.e. λ 2 = 0[REF_CITE]).
"DLM_2 is the model where the headword probability is estimated by interpolating the word trigram prob-ability, the headword bigram probability, and the probability given one previous linguistically related word in the dependency structure."
"Although Equation (7) suggests that the word probability P(w j | Φ (W ,D )) and the parsing model j-1 j-1 probability can be combined through simple multi-plication, some weighting is desirable in practice, especially when our parsing model is estimated using an approximation by the parsing score P(D|W)."
We therefore introduced a parsing model weight PW: both DLM_1 and DLM_2 models were built with and without PW.
"In Table 2, the PW-prefix refers to the DLMs with PW = 0.5, and the DLMs without PW- prefix refers to DLMs with PW = 0."
"For both DLM_1 and DLM_2, models with the parsing weight achieve better performance; we therefore discuss only DLMs with the parsing weight for the rest of this section."
"By comparing both HBM and PW-LDM_1 models with the baseline model, we can see that the use of headword dependency contributes greatly to the CER reduction: HBM outperformed the baseline model by 8.8% in CER reduction, and PW-LDM_1 by 7.8%."
"By combining headword bigram and dependency structure, we obtained the best model PW-DLM_2 that achieves 11.3% CER reduction over the baseline."
The improvement achieved by PW-DLM_2 over the HBM is statistically signifi-cant according to the t test (P&lt;0.01).
These results demonstrate the effectiveness of our parsing tech-nique and the use of dependency structure for lan-guage modeling.
"In this section, we relate our model to previous research and discuss several factors that we believe to have the most significant impact on the per-formance of DLM."
"The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function Φ , and (3) the method of unsu-pervised dependency structure acquisition."
"One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T * = arg-max T P(W, T)."
"Many recent studies (e.g.,[REF_CITE]) adopt this approach."
"Similarly, dependency-based models (e.g.,[REF_CITE]) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees."
"Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are esti-mated from syntactically annotated corpora such as the Penn Treebank."
"DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency struc-ture used in language modeling was learned directly from data in an unsupervised manner, subject to two weak syntactic constraints (i.e., dependency struc-ture is acyclic and planar). [Footnote_8]"
"8 In this sense, our model is an extension of a depend-ency-based model proposed[REF_CITE]. However, this work has not been evaluated as a language model with error rate reduction."
This resulted in cap-turing the dependency relations that are not pre-cisely syntactic in nature within our model.
"For example, in the conversion of the string below, the word ᥅ ban &apos;evening&apos; was correctly predicted in DLM by using the long-distance bigram ᦺ ~ ᥅ asa~ban &apos;morning~evening&apos;, even though these two words are not in any direct syntactic dependency relationship: ਥᏨߩ⡿௝ߩ೨ߢᦺޔᜰ␜ࠍઔ߉ޔ᥅ߦႎ๔ࠍⴕ߁ &apos;asks for instructions in the morning and submits daily reports in the evening&apos;"
"Though there is no doubt that syntactic dependency relations provide useful information for language modeling, the most linguistically related word in the previous context may come in various linguistic relations with the word being predicted, not limited to syntactic dependency."
This opens up new possi-bilities for exploring the combination of different knowledge sources in language modeling.
"Regarding the function Φ that maps the left context onto equivalence classes, we used a simple approximation that takes into account only one linguistically related word in left context."
An al-ternative is to use the maximum entropy (ME) approach[REF_CITE].
"Although ME models provide a nice framework for incorporating arbitrary knowledge sources that can be encoded as a large set of constraints, training and using ME models is extremely computationally expensive."
"Our working hypothesis is that the in-formation for predicting the new word is dominated by a very limited set of words which can be selected heuristically: in this paper, Φ is defined as a heu-ristic function that maps D to one word in D that has the strongest linguistic relation with the word being predicted, as in (8)."
"This hypothesis is borne out by an additional experiment we conducted, where we used two words from D that had the strongest rela-tion with the word being predicted; this resulted in a very limited gain in CER reduction of 0.62%, which is not statistically significant (P&gt;0.05 according to the t test)."
"The EM-like method for learning dependency relations described in Section 3.3 has also been applied to other tasks such as hidden Markov model training[REF_CITE], syntactic relation learning[REF_CITE], and Chinese word segmentati[REF_CITE]."
"In applying this method, two factors need to be considered: (1) how to initialize the model (i.e. the value of the window size N), and (2) the number of iterations."
We investigated the impact of these two factors empirically on the CER of Japanese Kana-Kanji conversion.
We built a series of DLMs using different window size N and different number of iterations.
Some sample results are shown in Table 3: the improvement in CER begins to saturate at the second iteration.
We also find that a larger N results in a better initial model but makes the following iterations less effective.
"The possible reason is that a larger N generates more initial dependencies and would lead to a better initial model, but it also introduces noise that pre-vents the initial model from being improved."
All DLMs in Table 2 are initialized with N = 3 and are run for two iterations.
"We have presented a dependency language model that captures linguistic constraints via a dependency structure – a set of probabilistic dependencies that express the relations between headwords of each phrase in a sentence by an acyclic, planar, undi-rected graph."
Promising results of our experiments suggest that long-distance dependency relations can indeed be successfully exploited for the purpose of language modeling.
There are many possibilities for future im-provements.
"In particular, as discussed in Section 6, syntactic dependency structure is believed to cap-ture useful information for informed language modeling, yet further improvements may be possi-ble by incorporating non-syntax-based dependen-cies."
Correlating the accuracy of the dependency parser as a parser vs. its utility in CER reduction may suggest a useful direction for further research.
This paper describes an extension of the se-mantic grammars used in conventional sta-tistical spoken language interfaces to allow the probabilities of derived analyses to be conditioned on the meanings or denotations of input utterances in the context of an interface&apos;s underlying application environ-ment or world model .
"Since these denota-tions will be used to guide disambiguation in interactive applications, they must be ef-ciently shared among the many possible analyses that may be assigned to an input utterance."
"This paper therefore presents a formal restriction on the scope of variables in a semantic grammar which guarantees that the denotations of all possible analy-ses of an input utterance can be calculated in polynomial time, without undue con-straints on the expressivity of the derived semantics."
Empirical tests show that this model-theoretic interpretation yields a sta-tistically signi cant improvement on stan-dard measures of parsing accuracy over a baseline grammar not conditioned on deno-tations.
"The development of speaker-independent mixed-initiative speech interfaces, in which users not only answer questions but also ask questions and give instructions, is currently limited by the perfor-mance of language models based largely on word co-occurrences."
"Even under ideal circumstances, with large application-speci c corpora on which to train, conventional language models are not suÆciently predictive to correctly analyze a wide variety of in-puts from a wide variety of speakers, such as might be encountered in a general-purpose interface for di-recting robots, oÆce assistants, or other agents with complex capabilities."
"Such tasks may involve unla-beled objects that must be precisely described, and a wider range of actions than a standard database in-terface would require (which also must be precisely described), introducing a great deal of ambiguity into input processing."
This paper therefore explores the use of a statis-tical model of language conditioned on the mean-ings or denotations of input utterances in the context of an interface&apos;s underlying application environment or world model .
"This use of model-theoretic inter-pretation represents an important extension to the `semantic grammars&apos; used in existing statistical spo-ken language interfaces, which rely on co-occurrences among lexically-determined semantic classes and slot llers[REF_CITE], in that the probability of an analysis is now also conditioned on the exis-tence of denoted entities and relations in the world model."
"The advantage of the interpretation-based disambiguation advanced here is that the probabil-ity of generating, for example, the noun phrase `the lemon next to the safe&apos; can be more reliably esti-mated from the frequency with which noun phrases have non-empty denotations { given the fact that `the lemon next to the safe&apos; does indeed denote some-thing in the world model { than it can from the rel-atively sparse co-occurrences of frame labels such as lemon and next-to , or of next-to and safe ."
"Since there are exponentially many word strings attributable to any utterance, and an exponential (Catalan-order) number of possible parse tree anal-yses attributable to any string of words, this use of model-theoretic interpretation for disambiguation must involve some kind of sharing of partial results between competing analyses if interpretation is to be performed on large numbers of possible analyses in a practical interactive application."
This paper there-fore also presents a formal restriction on the scope of variables in a semantic grammar (without untoward constraints on the expressivity of the derived seman-tics) which guarantees that the denotations of all possible analyses of an input utterance can be calcu-lated in polynomial time.
Empirical tests show that this use of model-theoretic interpretation in disam-biguation yields a statistically signi cant improve-ment on standard measures of parsing accuracy over a baseline grammar not conditioned on denotations.
"In order to determine whether a user&apos;s directions de-note entities and relations that exist in the world model { and of course, in order to execute those directions once they are disambiguated { it is nec-essary to precisely represent the meanings of input utterances."
Semantic grammars of the sort employed in cur-rent spoken language interfaces for ight reservation tasks[REF_CITE]asso-ciate fragments of logical { typically relational alge-bra { expressions with recursive transition networks encoding lexicalized rules in a context-free grammar (the independent probabilities of these rules can then be estimated from a training corpus and multiplied together to give a probability for any given analysis).
"In ight reservation systems, these associated seman-tic expressions usually designate entities through a xed set of constant symbols used as proper names (e.g. for cities and numbered ights); but in applica-tions with unlabeled (perhaps visually-represented) environments, entities must be described by pred-icating one or more modi ers over some variable, narrowing the set of potential referents by specify-ing colors, spatial locations, etc., until only the de-sired entity or entities remain."
"A semantic grammar for interacting with this kind of unlabeled environ-ment might contain the following rules, using vari-ables x 1 :::x n (over entities in the world model) in the associated logical expressions:"
VP PP :  1 ::: x n : $1( x 1 ::: x m ) ^
VP ! hold NP :  1 : Hold ($ Agent 2( x 1 ; x; x m [Footnote_1]+ ) 1 ^ ::: $ x 2 n ( x ) 1 )
"1 This use of pseudo-variables is intended to resemble that of the unix program `yacc,&apos; which has a similar pur-pose (associating syntax with semantics in constructing compilers for programming languages)."
NP ! a glass
PP ! under NP ::  11 :x Glass 2 : Under ( x 1 )( x 1 ; x 2 ) ^ $2( x 2 )
NP ! the faucet :  1 :
Faucet ( x 1 ) in which m and n are integers and 0 m n .
"Each lambda expression  1 :::x n : indicates a function from a tuple of entities h e 1 :::e n i to a truth value de ned by the remainder of the expression (sub- stituting e 1 ::: e n for x 1 ::: x n ), which denotes a set of tuples satisfying , drawn from E n (where E is the set of entities in the world model)."
"The pseudo-variables $1 ; $2 ; : : : in this notation in-dicate the sites at which the semantic expressions associated with each rule&apos;s nonterminal symbols are to compose (the numbers correspond to the relative positions of the symbols on the right hand side of each rule, numbered from left to right)."
Semantic ex-pressions for complete sentences are then formed by composing the sub-expressions associated with each rule at the appropriate sites. 1
Figure 1 shows the above rules assembled in a derivation of the sentence `hold a glass under the faucet.&apos; The denotation annotated beneath each con-stituent is simply the set of variable assignments (for each free variable) that satisfy the constituent&apos;s semantics.
"These denotations exactly capture the meaning (in a given world model) of the assem-bled semantic expressions dominated by each con-stituent, regardless of how many sub-expressions are subsumed by that constituent, and can therefore be shared among competing analyses in lieu of the se-mantic expression itself, as a partial result in model-theoretic interpretation."
"Note, however, that the adjunction of the preposi-tional phrase modi er `under the faucet&apos; adds an-other free variable ( x 2 ) to the semantics of the verb phrase, and therefore another factor of jEj to the car-dinality of its denotation."
"Moreover, under this kind of global scoping, if additional prepositional phrases are adjoined, they would each contribute yet an-other free variable, increasing the complexity of the denotation by an additional factor of jEj , making the shared interpretation of such structures poten-tially exponential on the length of the input."
"This proliferation of free variables means that the vari-ables introduced by the noun phrases in an utter-ance, such as `hold a glass under the faucet,&apos; can-not all be given global scope, as in Figure 1."
"On the other hand, the variables introduced by quanti-ed noun phrases cannot be bound as soon as the noun phrases are composed, as in Figure 2, because these variables may need to be used in modi ers composed in subsequent (higher) rule applications."
"Fortunately, if these non-immediate variable scop-ing arrangements are expressed structurally, as dom-inance relationships in the elementary tree struc-tures of some grammar, then a structural restriction on this grammar can be enforced that preserves as many non-immediate scoping arrangements as possi-ble while still preventing an unbounded proliferation of free variables."
"The correct scoping arrangements (e.g. for the sen-tence `hold a glass under the faucet,&apos; shown Fig-ure 3) can be expressed using ordered sets of parse rules grouped together in such a way as to allow other structural material to intervene."
"In this case, a group would include a rule for composing a verb and a noun phrase with some associated predicate, and one or more rules for binding each of the pred-icate&apos;s variables in a quanti er somewhere above it (thereby ensuring that these rules always occur to-gether with the quanti er rules dominating the pred-icate rule), while still allowing rules adjoining prepo-sitional phrase modi ers to apply in between them (so that variables in their associated predicates can be bound by the same quanti ers)."
"These `grouped rules&apos; can be formalized using a tree-rewriting system whose elementary trees can subsume several ordered CFG rule applications (or steps in a context-free derivation), as shown in Fig-ure 4."
Each such elementary tree contains a rule (node) associated with a logical predicate and rules (nodes) associated with quanti ers binding each of the predicate&apos;s variables.
"These trees are then com-posed by rewriting operations (dotted lines), which split them up and either insert them between or iden-tify them with (if demarcated with dashed lines) the rules in another elementary tree { in this case, the elementary tree anchored by the word `under.&apos; These trees are considered elementary in order to exclude the possibility of generating derivations that contain unbound variables or quanti ers over unused vari-ables, which would have no intuitive meaning."
The composition operations will be presented in further detail in Section 2.2.
"A general class of rewriting systems can be de ned using sets of allowable expansions of some type of object to incorporate zero or more other instances of the same type of object, each of which is simi-larly expandable."
"Such a system can generate ar-bitrarily complex structure by recursively expand-ing or `rewriting&apos; each new object, concluding with a set of zero-expansions at the frontier."
"For example, a context-free grammar may be cast as a rewriting system whose objects are strings, and whose allow-able expansions are its grammar productions, each of which expands or rewrites a certain string as a set of zero or more sub-strings arranged around certain `elementary&apos; strings contributing terminal symbols."
"A class of tree -rewriting systems can similarly be de ned as rewriting systems whose objects are trees, and whose allowable expansions are produc-tions (similar to context-free productions), each of which rewrite a tree A as some function f applied to zero or more sub-trees A 1 ; : : : A s ; s 0 arranged around some `elementary&apos; tree structure de ned by f[REF_CITE]:"
A ! f ( A 1 ; : : : A s ) (1)
"This elementary tree structure can be used to ex-press the dominance relationship between a logical predicate and the quanti ers that bind its variables (which must be preserved in any meaningful derived structure); but in order to allow the same instance of a quanti er to bind variables in more than one predicate, the rewriting productions of such a se-mantic tree-rewriting system must allow expanded subtrees to identify parts of their structure (speci -cally, the parts containing quanti ers) with parts of each other&apos;s structure, and with that of their host elementary tree."
"In particular, a rewriting production in such a sys-tem would rewrite a tree A as an elementary tree 0 with a set of sub-trees A 1 ; : : : A s inserted into it, each of which is rst partitioned into a set of contiguous components (in order to isolate particular quanti er nodes and other kinds of sub-structure) using a tree partition function g at some sequence of split points h # i 1 ,... # ic i i , which are node addresses in A i (the rst of which simply speci es the root). 2"
"The resulting se-quence of partitioned components of each expanded sub-tree are then inserted into 0 at a correspond-ing sequence of insertion site addresses h i 1 ,... ic i i de ned by the rewriting function f : f ( A 1 ; : : : A s ) = 0 [ h 11 ,... 1 c 1 i ; g # 11 ,... # 1 c 1 ( A 1 )] : : : [ h s 1 ,... sc s i ; g # s 1 ,... # scs ( A s )] ([Footnote_2])"
2 The node addresses encode a path from the root of
"Since each address can only host a single inserted component, any components from di erent sub-tree arguments of f that are assigned to the same inser-tion site address are constrained to be identical in or-der for the production to apply."
"Additionally, some addresses may be `pre- lled&apos; as part of the elemen-tary structure de ned in f , and therefore may also be identi ed with components of sub-tree arguments of f that are inserted at the same address."
"Figure 4 shows the set of insertion sites (designated with boxed indices) for each argument of an elemen-tary tree anchored by `under.&apos; The sites labeled 1 , associated with the rst argument sub-tree (in this case, the tree anchored by `hold&apos;), indicate that it is composed by partitioning it into three components, each dominating or dominated by the others, the low-est of which is inserted at the terminal node labeled `VP,&apos; the middle of which is identi ed with a pre-lled component (delimited by dashed lines), con-taining the quanti er node labeled `VP ! VP,&apos; and the uppermost of which (empty in the gure) is in-serted at the root, while preserving the relative dom-inance relationships among the nodes in both trees."
"Similarly, sites labeled 2 , associated with the sec-ond argument sub-tree (for the noun phrase comple-the tree in which every address i speci es the i th child of the node at the end of path . ment to the preposition), indicate that it is composed by partitioning it into two components { again, one dominating the other { the lowest of which is inserted at the terminal node labeled `NP,&apos; and the uppermost of which is identi ed with another pre- lled compo-nent containing the quanti er node labeled `PP !"
"PP,&apos; again preserving the relative dominance rela-tionships among the nodes in both trees."
Recall the problem of unbounded variable prolif-eration described in Section 2.1.
"The advantage of using a tree-rewriting system to model semantic composition is that such systems allow the appli-cation of well-studied restrictions to limit their re-cursive capacity to generate structural descriptions (in this case, to limit the unbounded overlapping of quanti er-variable dependencies that can produce unlimited numbers of free variables at certain steps in a derivation), without limiting the multi-level struc-ture of their elementary trees, used here for captur-ing the well-formedness constraint that a predicate be dominated by its variables&apos; quanti ers."
"One such restriction, based on the regular form restriction de ned for tree adjoining grammars[REF_CITE], prohibits a grammar from allowing any cycle of elementary trees, each intervening inside a spine (a path connecting the insertion sites of any argument) of the next."
This restriction is de ned below: De nition 2.1 Let a spine in an elementary tree be the path of nodes (or object-level rule applications) connecting all insertion site addresses of the same argument.
"De nition 2.2 A grammar G is in regular form if a directed acyclic graph h V;E i can be drawn with ver-tices v H ;v A 2 V corresponding to partitioned ele-mentary trees of G (partitioned as described above), and directed edges h v H ;v A i 2 E V V from each vertex v H , corresponding to a partitioned elementary tree that can host an argument, to each vertex v A , corresponding to a partitioned elementary tree that can function as its argument, whose partition inter-sects its spine at any place other than the top node in the spine."
"This restriction ensures that there will be no un-bounded `pumping&apos; of intervening tree structure in any derivation, so there will never be an unbounded amount of unrecognized tree structure to keep track of at any step in a bottom-up parse, so the number of possible descriptions of each sub-span of the in-put will be bounded by some constant."
It is called a `regular form&apos; restriction because it ensures that the set of root-to-leaf paths in any derived structure will form a regular language.
"A CKY-style parser can now be built that rec-ognizes each context-free rule in an elementary tree from the bottom up, storing in order the unrecog-nized rules that lie above it in the elementary tree (as well as any remaining rules from any composed sub-trees) as a kind of promissory note."
"The fact that any regular-form grammar has a regular path set means that only a nite number of states will be required to keep track of this promised, unrecognized structure in a bottom-up traversal, so the parser will have the usual O ( n 3 ) complexity (times a constant equal to the nite number of possible unrecognized structures)."
"Moreover, since the parser can recognize any string derivable by such a grammar, it can create a shared forest representation of every possible analysis of a given input by annotating every possible applica-tion of parse rules that could be used in the deriva-tion of each constituent[REF_CITE]."
"This polynomial-sized shared forest representation can then be interpreted determine which constituents denote entities and relations in the world model, in order to allow model-theoretic semantic information to guide disambiguation decisions in parsing."
"Finally, the regular form restriction also has the important e ect of ensuring that the number of un-recognized quanti er nodes at any step in a bottom-up analysis { and therefore the number of free vari-ables in any word or phrase constituent of a parse { is also bounded by some constant, which limits the size of any constituent&apos;s denotation to a polynomial or-der of E , the number of entities in the environment."
The interpretation of any shared forest derived by this kind of regular-form tree-rewriting system can therefore be calculated in worst-case polynomial time on E .
"A denotation-annotated shared forest for the noun phrase `the girl with the hat behind the counter&apos; is shown in Figure 5, using the noun and preposition trees from Figure 4, with alternative applications of parse rules represented as circles below each derived constituent."
"This shared structure subsumes two competing analyses: one containing the noun phrase `the girl with the hat&apos;, denoting the entity g 1 , and the other containing the noun phrase `the hat be-hind the counter&apos;, which does not denote anything in the world model."
"Assuming that noun phrases rarely occur with empty denotations in the training data, the parse containing the phrase `the girl with the hat&apos; will be preferred, because there is indeed a girl with a hat in the world model."
"This formalism has similarities with two ex- tensions of tree-adjoining grammar[REF_CITE], namely multi-component tree adjoining grammar[REF_CITE]and description tree substitu-tion grammar[REF_CITE], and indeed represents something of a combination of the two: 1."
"Like description tree substitution grammars, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2."
"Like multi-component TAGs, but unlike descrip-tion tree substitution grammars, it allows the speci cation of particular insertion sites within elementary trees, and 3."
"Unlike both, it allow duplication of structure (which is used for merging quanti ers from dif-ferent elementary trees)."
"The use of lambda calculus functions to de ne de-composable meanings for input sentences draws on traditions[REF_CITE]and[REF_CITE], but this approach di ers from the Montagovian sys-tem by introducing explicit limits on computational complexity (in order to allow tractable disambigua-tion)."
"This approach to semantics is very similar to that described[REF_CITE], in which syntactic and semantic expressions are assembled synchronously using paired tree-adjoining grammars with isomor-phic derivations, except that in this approach the derived structures are isomorphic as well, hence the reduction of synchronous tree pairs to semantically-annotated syntax trees."
"This isomorphism restric-tion on derived trees reduces the number of quanti er scoping con gurations that can be assigned to any given input (most of which are unlikely to be used in a practical application), but its relative parsimony allows syntactically ambiguous inputs to be seman-tically interpreted in a shared forest representation in worst-case polynomial time."
"The interleaving of semantic evaluation and parsing for the purpose of disambiguation also has much in common with that[REF_CITE], except that in this case, constituents are not only semantically type-checked, but are also fully interpreted each time they are pro-posed."
There are also commonalities between the un- derspeci ed semantic representation of structurally-ambiguous elementary tree constituents in a shared forest and the underspeci ed semantic representa-tion of (e.g. quanti er) scope ambiguity described[REF_CITE]. 3
The contribution of this model-theoretic semantic in-formation toward disambiguation was evaluated on a set of directions to animated agents collected in a controlled but spatially complex [Footnote_3]-D simulated en-vironment (of children running a lemonade stand).
"3 Denotation of competing applications of parse rules can be unioned (though this e ectively treats ambiguity as a form of disjunction), or stored separately to some nitie beam (though some globally preferable but locally dispreferred 4 Here it wasstructuresassumedwouldthatbethelostintention). of the user was to direct the agent to perform the actions shown in the `desired behavior&apos; animation."
"In order to avoid priming them towards particu-lar linguistic constructions, subjects were shown un-narrated animations of computer-simulated agents performing di erent tasks in this environment (pick-ing fruit, operating a juicer, and exchanging lemon-ade for money), which were described only as the `de-sired behavior&apos; of each agent."
"The subjects were then asked to direct the agents, using their own words, to perform the desired behaviors as shown. 340 utterances were collected and annotated with brackets and elementary tree node addresses as de-scribed in Section 2.2, for use as training data and as gold standard data in testing."
Some sample direc-tions are shown in Figure 6.
"Most elementary trees were extracted, with some simpli cations for parsing eÆciency, from an existing broad-coverage grammar resource (XTAG[REF_CITE]), but some elementary trees for multi-word expressions had to be created anew."
"In all, a complete annotation of this corpus required a grammar of 68 elementary trees and a lexicon of 288 lexicalizations (that is, words or sets of words with indivisible semantics, forming the anchors of a given elementary tree)."
Each lex-icalization was then assigned a semantic expression describing the intended geometric relation or class of objects in the simulated [Footnote_3]-D environment. 4
"3 Denotation of competing applications of parse rules can be unioned (though this e ectively treats ambiguity as a form of disjunction), or stored separately to some nitie beam (though some globally preferable but locally dispreferred 4 Here it wasstructuresassumedwouldthatbethelostintention). of the user was to direct the agent to perform the actions shown in the `desired behavior&apos; animation."
"The interface was tested on the rst 100 collected utterances, and the parsing model was trained on the remaining utterances."
"The presence or absence of a denotation of each constituent was added to the label of each constituent in the denotation-sensitive parsing model (for example, statistics were collected for the frequency of `NP: !"
"NP:+ PP:+&apos; events, meaning a noun phrase that does not denote any- thing in the environment expands to a noun phrase and a prepositional phrase that do have a denota-tion in the environment), whereas the baseline sys-tem used a parsing model conditioned on only con-stituent labels (for example, `NP !"
NP PP&apos; events).
"The entire word lattice output of the speech recog-nizer was fed directly into the parser, so as to al-low the model-theoretic semantic information to be brought to bear on word recognition ambiguity as well as on structural ambiguity in parsing."
"Since any derivation of elementary trees uniquely de nes a semantic expression at each node, the task of evaluating this kind of semantic analysis is reduced to the familiar task of evaluating a the accuracy of a labeled bracketing (labeled with elementary tree names and node addresses)."
"Here, the standard mea-sures of labeled precision and recall are used."
Note that there may be multiple possible bracketings for each gold standard tree in a given word lattice that di er only in the start and end frames of the com-ponent words.
"Since neither the baseline nor test parsing models are sensitive to the start and end frames of the component words, the gold standard bracketing is simply assumed to use the most likely frame segmentation in the word lattice that yields the correct word sequence."
The results of the experiment are summarized below.
"The environment-based model shows a statistically signi cant (p &lt; .05) improvement of 3 points in labeled recall, a 12% reduction in error."
"Most of the improvement can be attributed to the denotation-sensitive parser dispreferring noun phrase constituents with mis-attached modi ers, which do not denote anything in the world model."
Model LPLR baseline model 82% 78% baseline + denotation bit 85% 81%
This paper has described an extension of the seman-tic grammars used in conventional spoken language interfaces to allow the probabilities of derived anal-yses to be conditioned on the results of a model-theoretic interpretation.
"In particular, a formal re-striction was presented on the scope of variables in a semantic grammar which guarantees that the deno-tations of all possible analyses of an input utterance can be calculated in polynomial time, without un-due constraints on the expressivity of the derived semantics."
Empirical tests show that this model-theoretic interpretation yields a statistically signif-icant improvement on standard measures of parsing accuracy over a baseline grammar not conditioned on denotations.
"We describe the ongoing construction of a large, semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic infor-mation, e.g. the construction of domain-independent lexica."
The backbone of the annotation are semantic roles in the frame semantics paradigm.
We report expe-riences and evaluate the annotated data from the first project stage.
"On this ba-sis, we discuss the problems of vagueness and ambiguity in semantic annotation."
Corpus-based methods for syntactic learning and processing are well-established in computational linguistics.
"There are comprehensive and carefully worked-out corpus resources available for a num-ber of languages, e.g. the Penn Treebank[REF_CITE]for English or the NEGRA corpus[REF_CITE]for German."
"In semantics, the sit-uation is different: Semantic corpus annotation is only in its initial stages, and currently only a few, mostly small, corpora are available."
"Semantic an-notation has predominantly concentrated on word senses, e.g. in the SENSEVAL initiative[REF_CITE], a notable exception being the Prague Tree-bank[REF_CITE]."
"As a consequence, most recent work in corpus-based semantics has taken an unsupervised approach, relying on statistical meth-ods to extract semantic regularities from raw cor-pora, often using information from ontologies like WordNet[REF_CITE]."
"Meanwhile, the lack of large, domain-independent lexica providing word-semantic information is one of the most serious bottlenecks for language technology."
"To train tools for the acquisition of semantic information for such lexica, large, extensively annotated resources are necessary."
"In this paper, we present current work of the SALSA (SAarbrücken Lexical Semantics Annota-tion and analysis) project, whose aim is to provide such a resource and to investigate efficient methods for its utilisation."
"In the current project phase, the focus of our research and the backbone of the an-notation are semantic role relations."
"More specif-ically, our role annotation is based on the Berke-ley FrameNet project[REF_CITE]."
"In addition, we selectively annotate word senses and anaphoric links."
"The TIGER corpus[REF_CITE], a 1.5M word German newspa-per corpus, serves as sound syntactic basis."
"Besides the sparse data problem, the most seri-ous problem for corpus-based lexical semantics is the lack of specificity of the data: Word meaning is notoriously ambiguous, vague, and subject to con-textual variance."
The problem has been recognised and discussed in connection with the SENSEVAL task[REF_CITE].
"Annotation of frame semantic roles compounds the problem as it combines word sense assignment with the assign-ment of semantic roles, a task that introduces vague-ness and ambiguity problems of its own."
The problem can be alleviated by choosing a suit-able resource as annotation basis.
"FrameNet roles, which are local to particular frames (abstract sit-uations), may be better suited for the annotation task than the “classical” thematic roles concept with a small, universal and exhaustive set of roles like agent, patient, theme: The exact extension of the role concepts has never been agreed[REF_CITE]."
"Furthermore, the more concrete frame se- mantic roles may make the annotators’ task easier."
"The FrameNet database itself, however, cannot be taken as evidence that reliable annotation is pos-sible: The aim of the FrameNet project is essen-tially lexicographic and its annotation not exhaus-tive; it comprises representative examples for the use of each frame and its frame elements in the BNC."
"While the vagueness and ambiguity problem may be mitigated by the using of a “good” resource, it will not disappear entirely, and an annotation format is needed that can cope with the inherent vagueness of word sense and semantic role assignment."
Plan of the paper.
In Section 2 we briefly intro-duce FrameNet and the TIGER corpus that we use as a basis for semantic annotation.
"Section 3 gives an overview of the aims of the SALSA project, and Section 4 describes the annotation with frame se-mantic roles."
"Section 5 evaluates the first annotation results and the suitability of FrameNet as an anno-tation resource, and Section 6 discusses the effects of vagueness and ambiguity on frame semantic role annotation."
"Although the current amount of anno-tated data does not allow for definitive judgements, we can discuss tendencies."
"SALSA currently extends the TIGER corpus by se-mantic role annotation, using FrameNet as a re-source."
"In the following, we will give a short overview of both resources."
The FrameNet project[REF_CITE]is based on Fillmore’s Frame Semantics.
A frame is a conceptual structure that describes a situ-ation.
It is introduced by a target or frame-evoking element (FEE).
"The roles, called frame elements (FEs), are local to particular frames and are the par-ticipants and props of the described situations."
The aim of FrameNet is to provide a comprehen-sive frame-semantic description of the core lexicon of English.
"A database of frames contains the frames’ basic conceptual structure, and names and descriptions for the available frame elements."
"A lexicon database associates lemmas with the frames they evoke, lists possible syntactic realizations of FEs and provides annotated examples from the BNC."
"The current on-line version of the frame database[REF_CITE]consists of almost 400 frames, and covers about 6,900 lexical entries."
Figure 1 shows two frames.
"The frame REQUEST involves a FE SPEAKER who voices the request, an ADDRESSEE who is asked to do something, the MESSAGE , the request that is made, the TOPIC that the request is about, and the MEDIUM that is used to convey the request."
Among the FEEs for this frame are the verb ask and the noun request.
"In the frame COMMERCIAL TRANSACTION (henceforth C T ), a BUYER gives MONEY to a SELLER and receives GOODS in exchange."
This frame is evoked e.g. by the verb pay and the noun money.
The TIGER Corpus.
"We are using the TIGER Corpus[REF_CITE], a manually syntacti-cally annotated German corpus, as a basis for our annotation."
"It is the largest available such cor-pus (80,000 sentences in its final release compared to 20,000 sentences in its predecessor NEGRA) and uses a rich annotation format."
The annotation scheme is surface oriented and comparably theory-neutral.
Individual words are labelled with POS information.
"The syntactic structures of sentences are described by relatively flat trees providing in-formation about grammatical functions (on edge la-bels), syntactic categories (on node labels), and ar-gument structure of syntactic heads (through the use of dependency-oriented constituent structures, which are close to the syntactic surface)."
An exam-ple for a syntactic structure is given in Figure 2.
The aim of the SALSA project is to construct a large semantically annotated corpus and to provide meth-ods for its utilisation.
"In the first phase of the project, we annotate the TIGER corpus in part man- ually, in part semi-automatically, having tools pro-pose tags which are verified by human annotators."
"In the second phase, we will extend these tools for the weakly supervised annotation of a much larger corpus, using the TIGER corpus as training data."
"The SALSA corpus is designed to be utilisable for many purposes, like improving sta-tistical parsers, and extending methods for informa-tion extraction and access."
"The focus in the SALSA project itself is on lexical semantics, and our first use of the corpus will be to extract selectional pref-erences for frame elements."
The SALSA corpus will be tagged with the fol-lowing types of semantic information:
"We tag all FEEs that oc-cur in the corpus with their appropriate frames, and specify their frame elements."
"Thus, our focus is different from the lexicographic orientation of the FrameNet project mentioned above."
"As we tag all corpus instances of each FEE, we expect to en-counter a wider range of phenomena. which Cur-rently, FrameNet only exists for English and is still under development."
"We will produce a “light ver-sion” of a FrameNet for German as a by-product of the annotation, reusing as many as possible of the semantic frame descriptions from the English FrameNet database."
"Our first results indicate that the frame structure assumed for the description of the English lexicon can be reused for German, with minor changes and extensions."
The additional value of word sense disambiguation in a corpus is obvious.
"However, exhaustive word sense annotation is a highly time-consuming task."
"Therefore we decided for a selec-tive annotation policy, annotating only the heads of frame elements."
"GermaNet, the German WordNet version, will be used as a basis for the annotation."
"Similarly, we will selectively anno-tate coreference."
"If a lexical head of a frame element is an anaphor, we specify the antecedent to make the meaning of the frame element accessible."
"To give a first impression of frame annotation, we turn to the sentence in Fig. 2: (1) SPD fordert Koalition zu Gespräch über Re-form auf. (SPD requests that coalition talk about reform.)"
Fig. 3 shows the frame annotation associated with (1).
Frames are drawn as flat trees.
The root node is labelled with the frame name.
"The edges are labelled with abbreviated FE names, like SPKR for SPEAKER , plus the tag FEE for the frame-evoking element."
The terminal nodes of the frame trees are always nodes of the syntactic tree.
"Cases where a semantic unit (FE or FEE) does not form one syntactic constituent, like fordert ...auf in the example, are represented by assignment of the same label to several edges."
"Sentence (1), a newspaper headline, contains at least two FEEs: auffordern and Gespräch. auf-fordern belongs to the frame REQUEST (see Fig. 1)."
"In our example the SPEAKER is the subject NP SPD, the ADDRESSEE is the direct object NP Koalition, and the MESSAGE is the complex PP zu Gespräch über Reform."
"So far, the frame structure follows the syntactic structure, except for that fact that the FEE, as a separable prefix verb, is realized by two syntac-tic nodes."
"However, it is not always the case that frame structure parallels syntactic structure."
The second FEE Gespräch introduces the frame CON - VERSATION .
In this frame two (or more) groups talk to one another and no participant is construed as only a SPEAKER or only an ADDRESSEE .
In our example the only NP-internal frame element is the TOPIC (“what the message is about”) über
"Re-form, whereas the INTERLOCUTOR -1 (“the promi-nent participant in the conversation”) is realized by the direct object of auffordern."
"As shown in Fig. 3, frames are annotated as trees of depth one."
"Although it might seem semantically more adequate to admit deeper frame trees, e.g. to allow the MSG edge of the REQUEST frame in Fig. 3 to be the root node of the CONVERSATION tree, as its “real” semantic argument, the representation of frame structure in terms of flat and independent semantic trees seems to be preferable for a number of practical reasons: It makes the annotation process more modular and flexible – this way, no frame an-notation relies on previous frame annotation."
The closeness to the syntactic structure makes the an-notators’ task easier.
"Finally, it facilitates statistical evaluation by providing small units of semantic in-formation that are locally related to syntax."
"Because frame elements may span more than one sentence, like in the case of direct speech, we cannot restrict ourselves to an-notation at sentence level."
"Also, compound nouns require annotation below word level."
"For ex-ample, the word “Gagenforderung” (demand for wages) consists of “-forderung” (demand), which in-vokes the frame REQUEST , and a MESSAGE element “Gagen-”."
Another interesting point is that one word may introduce more than one frame in cases of co-ordination and ellipsis.
An example is shown in (2).
"In the elliptical clause only one fifth for daughters, the elided bought introduces a C T frame."
"So we let the bought in the antecedent introduce two frames, one for the antecedent and one for the ellipsis. (2) Ein Viertel aller Spielwaren würden für Söhne erworben, nur ein Fünftel für Töchter. (One quarter of all toys are bought for sons, only one fifth for daughters.)"
"Frame annotation proceeds one frame-evoking lemma at a time, using subcor-pora containing all instances of the lemma with some surrounding context."
"Since most FEEs are polysemous, there will usually be several frames rel-evant to a subcorpus."
Annotators first select a frame for an instance of the target lemma.
Then they assign frame elements.
At the moment the annotation uses XML tags on bare text.
The syntactic structure of the TIGER-sentences can be accessed in a separate viewer.
An annotation tool is being implemented that will pro-vide a graphical interface for the annotation.
"It will display the syntactic structure and allow for a graph-ical manipulation of semantic frame trees, in a simi-lar way as shown in Fig. 3."
"Since FrameNet is far from being complete, there are many word senses not yet covered."
"For example the verb fordern, which belongs to the REQUEST frame, additionally has the reading challenge, for which the current ver-sion of FrameNet does not supply a frame."
"Compared to the pilot study we previ-ously reported[REF_CITE], in which 3 annota-tors tagged 440 corpus instances of a single frame, resulting in 1,320 annotation instances, we now dis-pose of a considerably larger body of data."
"It con-sists of 703 corpus instances for the two frames shown in Figure 1, making up a total of 4,653 an-notation instances."
"For the frame REQUEST , we obtained 421 instances with 8-fold and 114 with 7-fold annotation."
"The annotated lemmas com-prise auffordern (to request), fordern, verlangen (to demand), zurückfordern (demand back), the noun Forderung (demand), and compound nouns ending with -forderung."
"For the frame C T we have 30, 40 and 98 instances with 5-, 3-, and 2-fold annotation respectively."
"The annotated lemmas are kaufen (to buy), erwerben (to acquire), verbrauchen (to con-sume), and verkaufen (to sell)."
"Note that the corpora we are evaluating do not constitute a random sample: At the moment, we cover only two frames, and REQUEST seems to be relatively easy to annotate."
"Also, the annotation re-sults may not be entirely predictive for larger sam-ple sizes: While the annotation guidelines were be-ing developed, we used REQUEST as a “calibration” frame to be annotated by everybody."
"As a result, in some cases reliability may be too low because de-tailed guidelines were not available, and in others it may be too high because controversial instances were discussed in project meetings."
The results in this section refer solely to the assignment of fully specified frames and frame elements.
Underspecification is discussed at length in Section 6.
"Due to the limited space in this pa-per, we only address the question of inter-annotator agreement or annotation reliability , since a reliable annotation is necessary for all further corpus uses."
"Table 1 shows the inter-annotator agreement on frame assignment and on frame element assignment, computed for pairs of annotators."
"The “average” column shows the total agreement for all annotation instances, while “best” and “worst” show the fig-ures for the (lemma-specific) subcorpora with high-est and lowest agreement, respectively."
"The upper half of the table shows agreement on the assignment of frames to FEEs, for which we performed 14,410 pairwise comparisons, and the lower half shows agreement on assigned frame elements (29,889 pair-wise comparisons)."
Agreement on frame elements is “exact match”: both annotators have to tag exactly the same sequence of words.
"In sum, we found that annotators agreed very well on frames."
"Disagree-ment on frame elements was higher, in the range of 12-25%."
"Generally, the numbers indicated consider-able differences between the subcorpora."
"To investigate this matter further, we computed the Alpha statistic[REF_CITE]for our an-notation."
"Like the widely used Kappa, α is a chance-corrected measure of reliability."
It is defined as observed disagreement α = 1 − expected disagreement
We chose Alpha over Kappa because it also indi-cates unreliabilities due to unequal coder preference for categories.
"With an α value of 1 signifying total agreement and 0 chance agreement, α values above 0.8 are usually interpreted as reliable annotation."
Figure 4 shows single category reliabilities for the assignment of frame elements.
"The graphs shows that not only did target lemmas vary in their difficulty, but that reliability of frame ele-ment assignment was also a matter of high varia- tion."
"Firstly, frames introduced by nouns ( Forderung and -forderung ) were more difficult to annotate than verbs."
"Secondly, frame elements could be assigned to three groups: frame elements which were al-ways annotated reliably, those whose reliability was highly dependent on the FEE, and the third group whose members were impossible to annotate reli-ably (these are not shown in the graphs)."
"In the REQUEST frames, SPEAKER , MESSAGE and AD - DRESSEE belong to the first group, at least for verbal FEEs."
"MEDIUM is a member of the second group, and TOPIC was annotated at chance level (α ≈ 0)."
"In the COMMERCE frame, only BUYER and GOODS always show high reliability."
SELLER can only be re-liably annotated for the target verkaufen .
PURPOSE and REASON fall into the third group.
Interpretation of the data.
Inter-annotator agree-ment on the frames shown in Table 1 is very high.
"However, the lemmas we considered so far were only moderately ambiguous, and we might see lower figures for frame agreement for highly polysemous FEEs like laufen (to run)."
"For frame elements, inter-annotator agreement is not that high."
Can we expect improvement?
The Prague Treebank reported a disagreement of about 10% for manual thematic role assignment[REF_CITE].
"However, in contrast to our study, they also annotated temporal and local modi-fiers, which are easier to mark than other roles."
One factor that may improve frame element agreement in the future is the display of syntactic structure directly in the annotation tool.
"Annotators were instructed to assign each frame element to a single syntactic constituent whenever possible, but could only access syntactic structure in a separate viewer."
"We found that in 35% of pairwise frame ele-ment disagreements, one annotator assigned a single syntactic constituent and the other did not."
"Since a total of 95.6% of frame elements were assigned to single constituents, we expect an increase in agree-ment when a dedicated annotation tool is available."
"As to the pronounced differences in reliability be-tween frame elements, we found that while most central frame elements like SPEAKER or BUYER were easy to identify, annotators found it harder to agree on less frequent frame elements like MEDIUM , PURPOSE and REASON ."
The latter two with their particularly low agreement (α &lt; 0.8) contribute to-wards the low overall inter-annotator agreement of the C T frame.
We suspect that annotators saw too few instances of these elements to build up a reli-able intuition.
"However, the elements may also be inherently difficult to distinguish."
"How can we interpret the differences in frame el-ement agreement across target lemmas, especially between verb and noun targets?"
"While frame ele-ments for verbal targets are usually easy to identify based on syntactic factors, this is not the case for nouns."
Figure 3 shows an example: Should SPD be tagged as INTERLOCUTOR -2 in the CONVERSA - TION frame?
This appears to be a question of prag-matics.
Here it seems that clearer annotation guide-lines would be desirable.
FrameNet as a resource for semantic role an-notation.
"Above, we have asked about the suitabil-ity of FrameNet for semantic role annotation, and our data allow a first, though tentative, assessment."
"Concerning the portability of FrameNet to other languages than English, the English frames worked well for the German lemmas we have seen so far."
"For C T a number of frame elements seem to be missing, but these are not language-specific, like CREDIT (for on commission and in installments)."
The FrameNet frame database is not yet complete.
How often do annotators encounter missing frames?
"The frame UNKNOWN was assigned in 6.3% of the instances of REQUEST , and in 17.6% of the C T in-stances."
"The last figure is due to the overwhelm-ing number of UNKNOWN cases in verbrauchen, for which the main sense we encountered is “to use up a resource”, which FrameNet does not offer."
Is the choice of frame always clear?
And can frame elements always be assigned unambiguously?
Above we have already seen that frame element as-signment is problematic for nouns.
In the next sec-tion we will discuss problematic cases of frame as-signment as well as frame element assignment.
It is a well-known prob-lem from word sense annotation that it is often im-possible to make a safe choice among the set of pos-sible semantic correlates for a linguistic item.
"In frame annotation, this problem appears on two lev-els: The choice of a frame for a target is a choice of word sense."
The assignment of frame elements to phrases poses a second disambiguation problem.
"An example of the first problem is the Ger-man verb verlangen, which associates with both the frame REQUEST and the frame C T ."
"We found sev-eral cases where both readings seem to be equally present, e.g. sentence (3)."
Sentences (4) and (5) ex-emplify the second problem.
The italicised phrase in (4) may be either a SPEAKER or a MEDIUM and the one in (5) either a MEDIUM or not a frame element at all.
"In our exhaustive annotation, these problems are much more virulent than in the FrameNet corpus, which consists mostly of prototypical examples. (3) Gleichwohl versuchen offenbar Assekuranzen, [das Gesetz] zu umgehen, indem sie von Nicht-deutschen mehr Geld verlangen. (Nonetheless insurance companies evidently try to cir-cumvent [the law] by asking/demanding more money from non-Germans.) (4) Die nachhaltigste Korrektur der Programmatik fordert ein Antrag. . . (The most fundamental policy correction is requested by a motion...) (5) Der Parteitag billigte ein Wirtschaftskonzept, in dem der Umbau gefordert wird. (The party congress approved of an economic concept in which a change is demanded.)"
"In SALSA, we use the concept of underspecifica-tion to handle all three cases: Annotators may assign underspecified frame and frame element tags."
"While the cases have different semantic-pragmatic status, we tag all three of them as underspecified."
This is in accordance with the general view on underspecifica-tion in semantic theory[REF_CITE].
"Furthermore,[REF_CITE]argue that it is im-possible to distinguish those cases"
Allowing underspecified tags has several advan-tages.
"First, it avoids (sometimes dubious) decisions for a unique tag during annotation."
"Second, it is use-ful to know if annotators systematically found it hard to distinguish between two frames or two frame ele-ments."
This diagnostic information can be used for improving the annotation scheme (e.g. by removing vague distinctions).
"Third, underspecified tags may indicate frame relations beyond an inheritance hier-archy, horizontal rather than vertical connections."
"In (3), the use of underspecification can indicate that the frames REQUEST and C T are used in the same situation, which in turn can serve to infer relations between their respective frame elements."
Evaluating underspecified annotation.
"In the previous section, we disregarded annotation cases involving underspecification."
"In order to evalu-ate underspecified tags, we present a method of computing inter-annotator agreement in the pres-ence of underspecified annotations."
"Represent-ing frames and frame elements as predicates that each take a sequence of word indices as their argument, a frame annotation can be seen as a pair (CF,CE) of two formulae, describing the frame and the frame elements, respectively."
"With-out underspecification, CF is a single predicate and CE is a conjunction of predicates."
"For the CONVERSATION frame of sentence (1), CF has the form CONVERSATION (Gespräch) 1 , and CE is INTLC [Footnote_1](Koalition) ∧ TOPIC (über Reform)."
1 We use words instead of indices for readability.
Un-derspecification is expressed by conjuncts that are disjunctions instead of single predicates.
Table 2 shows the admissible cases.
"For example, the CE of (4) contains the conjunct SPKR (ein Antrag) ∨ MEDIUM (ein Antrag)."
Our annotation scheme guar-antees that every FE name appears in at most one conjunct of CE.
"Exact agreement means that ev-ery conjunct of annotator A must correspond to a conjunct by annotator B, and vice versa."
"For partial agreement, it suffices that for each conjunct of A, one disjunct matches a disjunct in a conjunct of B, and conversely."
"Using this measure of partial agreement, we now evaluate underspecified annotation."
The most strik-ing result is that annotators made little use of under-specification.
"Frame underspecification was used in 0.4% of all frames, and frame element underspecifi-cation for 0.9% of all frame elements."
"The frame el-ement MEDIUM , which was rarely assigned outside underspecification, accounted for roughly half of all underspecification in the REQUEST frame. 63% of the frame element underspecifications are cases of optional elements, the third class in the lower half of Table 2. (Partial) agreement on underspecified tags was considerably lower than on non-underspecified tags, both in the case of frames (86%) and in the case of frame elements (54%)."
"This was to be ex-pected, since the cases with underspecified tags are the more difficult and controversial ones."
"Since un-derspecified annotation is so rare, overall frame and frame element agreement including underspecified annotation is virtually the same as in Table 1."
"It is unfortunate that annotators use underspecifi-cation only infrequently, since it can indicate inter-esting cases of relatedness between different frames and frame elements."
"However, underspecification may well find its main use during the merging of independent annotations of the same corpus."
"Not only underspecified annotation, also disagreement between annotators can point out vague and ambigu-ous cases."
"If, for example, one annotator has as-signed SPEAKER and the other MEDIUM in sentence (4), the best course is probably to use an underspec-ified tag in the merged corpus."
"We presented the SALSA project, the aim of which is to construct and utilize a large corpus reliably annotated with semantic information."
"While the SALSA corpus is designed to be utilizable for many purposes, our focus is on lexical semantics, in or-der to address one of the most serious bottlenecks for language technology today: the lack of large, domain-independent lexica."
In this paper we have focused on the annotation with frame semantic roles.
"We have presented the annotation scheme, and we have evaluated first an-notation results, which show encouraging figures for inter-annotator agreement."
"We have discussed the problem of vagueness and ambiguity of the data and proposed a representation for underspecified tags, which are to be used both for the annotation and the merging of individual annotations."
"Important next steps are: the design of a tool for semi-automatic annotation, and the extraction of se-lectional preferences from the annotated data."
"We would like to thank the following people, who helped us with their sugges- tions and discussions: Sue Atkins, Collin Baker, Ulrike Baldewein, Hans Boas, Daniel Bobbert, Sabine Brants, Paul Buitelaar, Ann Copestake, Christiane Fellbaum, Charles Fillmore, Gerd Flied-ner, Silvia Hansen, Ulrich Heid, Katja Markert and Oliver Plaehn."
"We are especially indebted to Maria Lapata, whose suggestions have contributed to the current shape of the project in an essential way."
"Any errors are, of course, entirely our own."
Ordering information is a critical task for natural language generation applications.
In this paper we propose an approach to information ordering that is particularly suited for text-to-text generation.
We de-scribe a model that learns constraints on sentence order from a corpus of domain-specific texts and an algorithm that yields the most likely order among several al-ternatives.
We evaluate the automatically generated orderings against authored texts from our corpus and against human sub-jects that are asked to mimic the model’s task.
We also assess the appropriateness of such a model for multidocument summa-rization.
Structuring a set of facts into a coherent text is a non-trivial task which has received much attention in the area of concept-to-text generation (see[REF_CITE]for an overview).
"The structured text is typically assumed to be a tree (i.e., to have a hier-archical structure) whose leaves express the content being communicated and whose nodes specify how this content is grouped via rhetorical or discourse re-lations (e.g., contrast, sequence, elaboration)."
"For domains with large numbers of facts and rhetorical relations, there can be more than one pos-sible tree representing the intended content."
These different trees will be realized as texts with different sentence orders or even paragraph orders and differ-ent levels of coherence.
Finding the tree that yields the best possible text is effectively a search prob-lem.
One way to address it is by narrowing down the search space either exhaustively or heuristically.
The latter are operationalized as weights on the ordering and adjacency of facts and are derived from a corpus of naturally occurring texts.
A con-straint satisfaction algorithm is used to find the tree with maximal weights from the space of all possi-ble trees.
"Rather than requiring a global op-timum to be found, they use a genetic algorithm to select a tree that is coherent enough for people to understand (local optimum)."
The problem of finding an acceptable order-ing does not arise solely in concept-to-text gener-ation but also in the emerging field of text-to-text generati[REF_CITE].
"Examples of applica-tions that require some form of text structuring, are single- and multidocument summarization as well as question answering."
Note that these applications do not typically assume rich semantic knowledge orga-nized in tree-like structures or communicative goals as is often the case in concept-to-text generation.
"Al-though in single document summarization the posi-tion of a sentence in a document can provide cues with respect to its ordering in the summary, this is not the case in multidocument summarization where sentences are selected from different documents and must be somehow ordered so as to produce a coher-ent summary[REF_CITE]."
"Answering a question may also involve the extraction, potentially summarization, and ordering of information across multiple information sources."
Barzilay et al. further conduct a study where subjects are asked to produce a coherent text from the output of a multidocument summarizer.
"Their re- sults reveal that although the generated orders differ from subject to subject, topically related sentences always appear together."
Based on the human study they propose an algorithm that first identifies top-ically related groups of sentences and then orders them according to chronological information.
In this paper we introduce an unsupervised probabilistic model for text structuring that learns ordering constraints from a large corpus.
The model operates on sentences rather than facts in a knowl-edge base and is potentially useful for text-to-text generation applications.
"For example, it can be used to order the sentences obtained from a multidocu-ment summarizer or a question answering system."
"Sentences are represented by a set of informative features (e.g., a verb and its subject, a noun and its modifier) that can be automatically extracted from the corpus without recourse to manual annotation."
The model learns which sequences of features are likely to co-occur and makes predictions con-cerning preferred orderings.
Local coherence is thus operationalized by sentence proximity in the train-ing corpus.
Global coherence is obtained by greedily searching through the space of possible orders.
As in the case[REF_CITE]we construct an ac-ceptable ordering rather than the best possible one.
"We propose an automatic method of evaluating the orders generated by our model by measuring close-ness or distance from the gold standard, a collection of orders produced by humans."
The remainder of this paper is organized as fol-lows.
Section 2 introduces our model and an algo-rithm for producing a possible order.
Section 3 de-scribes our corpus and the estimation of the model parameters.
Our experiments are detailed in Sec-tion 4.
We conclude with a discussion in Section 5.
"Given a collection of texts from a particular domain, our task is to learn constraints on the ordering of their sentences."
In the training phase our model will learn these constraints from adjacent sentences rep-resented by a set of informative features.
"In the test-ing phase, given a set of unseen sentences, we will rely on our prior experience of how sentences are usually ordered for choosing the most likely order-ing."
We express the probability of a text made up of sen-tences S 1 ...S n as shown in (1).
"According to (1), the task of predicting the next sentence is dependent on its n − i previous sentences. (1) P(T) ="
P(S 1 ...S n ) =
"P(S 1 )P(S 2 |S 1 )P(S 3 |S 1 ,S 2 )..."
P(S n |S 1 ...
S n−1 ) n = ∏ P(S n |S 1 ...S n−i ) i=1
We will simplify (1) by assuming that the prob-ability of any given sentence is determined only by its previous sentence: (2) P(T) =
P(S 1 )P(S 2 |S 1 )P(S 3 |S 2 )...
P(S n |S n−1 ) n = ∏
P(S i |S i−1 ) i=1
This is a somewhat simplistic attempt at cap-turing Marcu’s (1997) local coherence constraints as well as Barzilay et al.’s (2002) observations about topical relatedness.
"While this is clearly a naive view of text coherence, our model has some notion of the types of sentences that typically go together, even though it is agnostic about the specific rhetorical re-lations that glue sentences into a coherent text."
Also note that the simplification in (2) will make the es-timation of the probabilities
P(S i |S i−1 ) more reli-able in the face of sparse data.
Of course estimat-ing
P(S i |S i−1 ) would be impossible if S i and S i−1 were actual sentences.
It is unlikely to find the ex-act same sentence repeated several times in a corpus.
What we can find and count is the number of times a given structure or word appears in the corpus.
We will therefore estimate P(S i |S i−1 ) from features that express its structure and content (these features are described in detail in Section 3): (3) P(S i |S i−1 ) =
"P(ha hi,1i ,a hi,2i ...a hi,ni i|ha hi−1,1i ,a hi−1,2i ...a hi−1,mi i) where ha hi,1i ,a hi,2i ...a hi,ni i are features relevant for sentence S i and ha hi−1,1i ,a hi−1,2i ...a hi−1,mi i for sen-tence S i−1 ."
"We will assume that these features are independent and that P(S i |S i−1 ) can be estimated from the pairs in the Cartesian product defined over the features expressing sentences S i and S i−1 : (a hi,ji ,a hi−1,ki ) ∈ S i × S i−1 ."
Under these assumptions P(S i |S i−1 ) can be written as follows: (4) P(S i |S i−1 ) =
"P(a hi,1i |a hi−1,1i )..."
"P(a hi,ni |a hi−1,mi ) ="
"P(a hi,ji |a hi−1,ki ) ∏ (a hi,ji ,a hi−1,ki ) ∈S i ×S i−1"
Assuming that the features are independent again makes parameter estimation easier.
The Carte-sian product over the features in S i and S i−1 is an at-tempt to capture inter-sentential dependencies.
"Since we don’t know a priori what the important feature combinations are, we are considering all possible combinations over two sentences."
"This will admit-tedly introduce some noise, given that some depen-dencies will be spurious, but the model can be easily retrained for different domains for which different feature combinations will be important."
"The proba-bility P(a hi,ji |a hi−1,ki ) is estimated as: f(a hi,ji ,a hi−1,ki ) (5)"
"P(a hi,ji |a hi−1,ki ) = ∑ f(a hi,ji ,a hi−1,ki ) ahi,ji where f(a hi,ji ,a hi−1,ki ) is the number of times fea-ture a hi,ji is preceded by feature a hi−1,ki in the corpus."
"The denominator expresses the number of times a hi−1,ki is attested in the corpus (preceded by any feature)."
"The probabilities P(a hi,ji |a hi−1,ki ) will be unreliable when the frequency estimates for f(a hi,ji ,a hi−1,ki ) are small, and undefined in cases where the feature combinations are unattested in the corpus."
We therefore smooth the observed frequen-cies using back-off smoothing[REF_CITE].
"To illustrate with an example consider the text in Figure 1 which has three sentences S 1 , S 2 , S 3 , each represented by their respective features denoted by letters."
"The probability P(S 3 |S 2 ) will be calcu-lated by taking the product of P(h|e), P(h| f ), P(h|g), P(i|e), P(i| f ), and P(i|g)."
"To obtain P(h|e), we need f (h,e) and f (e) which can be estimated in Figure 1 by counting the number of edges connecting e and h and the number of edges starting from e, respec-tively."
"So, P(h|e) will be 0.16 given that f(h,e) is one and f (e) is six (see the normalization in (5))."
"Once we have collected the counts for our features we can determine the order for a new text that we haven’t encountered before, since some of the features representing its sentences will be familiar."
Given a text with N sentences there are N! possi-ble orders.
"The set of orders can be represented as a complete graph, where the set of vertices V is equal to the set of sentences S and each edge u → v has a weight, the probability P(u|v)."
"Fortunately, they propose a simple greedy algorithm that provides an approximate solution which can be easily modified for our task (see also[REF_CITE])."
The algorithm starts by assigning each vertex v ∈ V a probability.
Recall that in our case vertices are sentences and their probabilities can be calcu-lated by taking the product of the probabilities of their features.
The greedy algorithm then picks the node with the highest probability and orders it ahead of the other nodes.
The selected node and its incident edges are deleted from the graph.
Each remaining node is now assigned the conditional probability of seeing this node given the previously selected node (see (4)).
The node which yields the highest condi-tional probability is selected and ordered ahead.
The process is repeated until the graph is empty.
As an example consider again a three sentence text.
We illustrate the search for a path through the graph in Figure 2.
"First we calculate which of the three sentences S 1 , S 2 , and S 3 is most likely to start the text (during training we record which sentences appear in the beginning of each text)."
"Assuming that P(S 2 | START ) is the highest, we will order S 2 first, and ignore the nodes headed by S 1 and S 3 ."
We next compare the probabilities P(S 1 |S 2 ) and P(S 3 |S 2 ).
"Since P(S 3 |S 2 ) is more likely than P(S 1 |S 2 ), we or-der S 3 after S 2 and stop, returning the order S 2 , S 3 , and S 1 ."
"As can be seen in Figure 2 for each vertex we keep track of the most probable edge that ends in that vertex, thus setting th beam search width to one."
"Note, that equation (4) would assign lower and lower probabilities to sentences with large numbers of features."
"Since we need to compare sentence pairs with varied numbers of features, we will normalize the conditional probabilities"
P(S i |S i−1 ) by the num-ber feature of pairs that form the Cartesian product over S i and S i−1 .
"The model in Section 2.1 was trained on the B LLIP corpus (30 M words), a collection of texts from the Wall Street Journal (years 1987-89)."
"The corpus con-tains 98,732 stories."
The average story length is 19.2 sentences. 71.30% of the texts in the corpus are less than 50 sentences long.
An example of the texts in this newswire corpus is shown in Figure 3.
The corpus is distributed in a Treebank-style machine-parsed version which was produced with Charniak’s (2000) parser.
The parser is a “maximum-entropy inspired” probabilistic gener-ative model.
"We also obtained a dependency-style version of the corpus using MINIPAR[REF_CITE]a broad coverage parser for English which employs a manu-ally constructed grammar and a lexicon derived from WordNet with an additional dictionary of proper names (130,000 entries in total)."
"The grammar is represented as a network of 35 nodes (i.e., grammat-ical categories) and 59 edges (i.e., types of syntactic (dependency) relations)."
The output of MINIPAR is a dependency graph which represents the dependency relations between words in a sentence (see Table 1 for an example).
From the two different parsed versions of the B LLIP corpus the following features were extracted: Verbs.
"Investigations into the interpretation of nar-rative discourse[REF_CITE]have shown that specific lexical information (e.g., verbs, adjectives) plays an important role in determining the discourse relations between propositions."
"Al-though we don’t have an explicit model of rhetorical relations and their effects on sentence ordering, we capture the lexical inter-dependencies between sen- tences by focusing on verbs and their precedence re-lationships in the corpus."
From the Treebank parses we extracted the verbs contained in each sentence.
"We obtained two versions of this feature: (a) a lemmatized ver-sion where verbs were reduced to their base forms and (b) a non-lemmatized version which preserved tense-related information; more specifically, verbal complexes (e.g., I will have been going ) were iden-tified from the parse trees heuristically by devis-ing a set of 30 patterns that search for sequences of modals, auxiliaries and verbs."
This is an attempt at capturing temporal coherence by encoding se-quences of events and their morphology which in-directly indicates their tense.
To give an example consider the text in Fig-ure 3.
"For the lemmatized version, sentence (1) will be represented by say , will , be , ask , and approve ; for the tensed version, the relevant features will be said , will be asked , and to approve ."
"Centering Theory (CT,[REF_CITE]) is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker’s use of certain referring ex-pressions."
"The principles underlying CT (e.g., conti-nuity, salience) are of interest to concept-to-text gen-eration as they offer an entity-based model of text and sentence planning which is particularly suited for descriptional genres[REF_CITE]."
We operationalize entity-based coherence for text-to-text generation by simply keeping track of the nouns attested in a sentence without however taking personal pronouns into account.
This simpli-fication is reasonable if one has text-to-text genera-tion mind.
"In multidocument summarization for ex-ample, sentences are extracted from different docu-ments; the referents of the pronouns attested in these sentences are typically not known and in some cases identical pronouns may refer to different entities."
So making use of noun-pronoun or pronoun-pronoun co-occurrences will be uninformative or in fact mis-leading.
We extracted nouns from a lemmatized version of the Treebank-style parsed corpus.
"In cases of noun compounds, only the compound head (i.e., rightmost noun) was taken into account."
"A small set of rules was used to identify organizations (e.g., United Lab-oratories Inc. ), person names (e.g., Jose Y. Cam-pos ), and locations (e.g., New England ) spanning more than one word."
"These were grouped together and were also given the general categories person, organization, and location."
"The model backs off to these categories when unknown person names, lo-cations, and organizations are encountered."
"Dates, years, months and numbers were substituted by the categories date, year, month, and number."
"In sentence (1) (see Figure 3) we identify the nouns Laidlaw Transportation Ltd. , shareholder , Dec 7 , meeting , change , name and Laidlaw Inc ."
"In sentence (2) the relevant nouns are company , name , business , 1984 , sale , and operation ."
Note that the noun and verb fea-tures do not capture the structure of the sentences to be ordered.
"This is important for our domain, as texts seem to be rather formulaic and similar syn-tactic structures are often used (e.g., direct and in-direct speech, restrictive relative clauses, predicative structures)."
"In this domain companies typically say things, and texts often begin with a statement of what a company or an individual has said (see sentence (1) in Figure 3)."
"Furthermore, companies and individu-als are described with certain attributes (persons can be presidents or governors, companies are bankrupt or manufacturers, etc.) that can give clues for infer-ring coherence."
The dependencies were obtained from the out-put of MINIPAR .
Some of the dependencies for sen-tence (2) from Figure 3 are shown in Table 1.
The dependencies capture structural as well lexical infor-mation.
"They are represented as triples, consisting of a head (leftmost element, e.g., say , name ), a modi-fier (rightmost element, e.g., company , its ) and a re-lation (e.g., subject (V:subj:N), object (V:obj:N), modifier (N:mod:A))."
"For efficiency reasons we focused on triples whose dependency relations (e.g., V:subj:N) were attested in the corpus with frequency larger than one per million."
We further looked at how individ-ual types of relations contribute to the ordering task.
"More specifically we experimented with dependen-cies relating to verbs (49 types), nouns (52 types), verbs and nouns (101 types) (see Table 1 for exam-ples)."
"We also ran a version of our model with all types of relations, including adjectives, adverbs and"
In this section we describe our experiments with the model and the features introduced in the previous sections.
"We first evaluate the model by attempting to reproduce the structure of unseen texts from the B LLIP corpus, i.e., the corpus on which the model is trained on."
We next obtain an upper bound for the task by conducting a sentence ordering experiment with humans and comparing the model against the human data.
"Finally, we assess whether this model can be used for multi-document summarization us-ing data[REF_CITE]."
But before we outline the details of our experiments we discuss our choice of metric for comparing different orders.
Our task is to produce an ordering for the sentences of a given text.
We can think of the sentences as objects for which a ranking must be produced.
"Ta-ble 2 gives an example of a text containing 10 sen-tences (A–J) and the orders (i.e., rankings) produced by three hypothetical models."
"A number of metrics can be used to measure the distance between two rankings such as Spear-man’s correlation coefficient for ranked data, Cayley distance, or Kendall’s τ (see[REF_CITE]for details)."
"Kendall’s τ is based on the number of inversions in the rankings and is defined in (6): (6) τ = 1− 2(number of inversions) N(N −1)/2 where N is the number of objects (i.e., sentences) being ranked and inversions are the number of in-terchanges of consecutive elements necessary to ar-range them in their natural order."
"If we think in terms of permutations, then τ can be interpreted as the min-imum number of adjacent transpositions needed to bring one order to the other."
In Table 2 the number of inversions can be calculated by counting the num-ber of intersections of the lines.
The metric ranges from −1 (inverse ranks) to 1 (identical ranks).
The τ for Model 1 and Model 2 in Table 2 is .822.
Kendall’s τ seems particularly appropriate for the tasks considered in this paper.
The metric is sen-sitive to the fact that some sentences may be always ordered next to each other even though their absolute orders might differ.
It also penalizes inverse rank-ings.
Comparison between Model 1 and Model 3 would give a τ of 0.244 even though the orders be-tween the two models are identical modulo the be-ginning and the end.
This seems appropriate given that flipping the introduction in a document with the conclusions seriously disrupts coherence.
The model from Section 2.1 was trained on the B LLIP corpus and tested on 20 held-out randomly selected unseen texts (average length 15.3).
We also used 20 randomly chosen texts (disjoint from the test data) for development purposes (average length 16.2).
All our results are reported on the test set.
The input to the the greedy algorithm (see Sec-tion 2.2) was a text with a randomized sentence or-dering.
The ordered output was compared against the original authored text using τ.
"Table 3 gives the average τ (T) for all 20 test texts when the fol-lowing features are used: lemmatized verbs (V L ), tensed verbs (V T ), lemmatized nouns (N L ), lem-matized verbs and nouns (V L N L ), tensed verbs and lemmatized nouns (V T N L ), verb-related dependen-cies (V D ), noun-related dependencies (N D ), verb and noun dependencies (V D N D ), and all available de-pendencies (A D )."
For comparison we also report the naive baseline of generating a random oder (B R ).
As can be seen from Table 3 the best performing fea-tures are N L and V D N D .
"This is not surprising given that N L encapsulates notions of entity-based coher-ence, which is relatively important for our domain."
A lot of texts are about a particular entity (company or individual) and their properties.
"The feature V D N D subsumes several other features and does expectedly better: it captures entity-based coherence, the inter-relations among verbs, the structure of sentences and also preserves information about argument structure (who is doing what to whom)."
The distance between the orders produced by the model and the original texts increases when all types of dependencies are taken into account.
"The feature space becomes too big, there are too many spurious feature pairs, and the model can’t distinguish informative from non-informative features."
We carried out a one-way Analysis of Vari-ance (A NOVA ) to examine the effect of different fea-ture types.
"The A NOVA revealed a reliable effect of feature type (F(9,171) = 3.31; p &lt; 0.01)."
We performed Post-hoc Tukey tests to further examine whether there are any significant differences among the different features and between our model and the baseline.
"We found out that N L , V T N L , V D , and V D N D are significantly better than B R (α = 0.01), whereas N L and V D N D are not significantly differ-ent from each other."
"However, they are significantly better than all other features (α = 0.05)."
In this experiment we compare our model’s perfor-mance against human judges.
Twelve texts were ran-domly selected from the 20 texts in our test data.
The texts were presented to subjects with the order of their sentences scrambled.
Participants were asked to reorder the sentences so as to produce a coherent text.
Each participant saw three texts randomly cho-sen from the pool of 12 texts.
A random order of sen-tences was generated for every text the participants saw.
"Sentences were presented verbatim, pronouns and connectives were retained in order to make or-dering feasible."
Notice that this information is absent from the features the model takes into account.
The study was conducted remotely over the In-ternet using a variant of Barzilay et al.’s (2002) soft-ware.
"Subjects first saw a set of instructions that ex-plained the task, and had to fill in a short question-naire including basic demographic information."
"The experiment was completed by 137 volunteers (ap-proximately 33 per text), all native speakers of En-glish."
Subjects were recruited via postings to local humans and the model on multidocument summaries Usenet newsgroups.
Table 4 reports pairwise τ averaged over 12 texts for all participants (H H ) and the average τ between the model and each of the subjects for all features used in Experiment [Footnote_1].
1 The summaries as well as the human data are available[URL_CITE]
The average distance in the orderings produced by our subjects is .58.
The distance between the humans and the best features is .51 for N L and .55 for V D N D .
"An A NOVA yielded a significant effect of feature type (F(9,99) = 5.213; p &lt; 0.01)."
"Post-hoc Tukey tests revealed that V L , V T , V D , N D , A D , V L N L , and V T N L perform sig-nificantly worse than H H (α = 0.01), whereas N L and V D N D are not significantly different from H H (α = 0.01)."
This is in agreement with Experiment 1 and points to the importance of lexical and structural information for the ordering task.
Their goal was to improve the ordering strat-egy of M ULTI G EN[REF_CITE]a mul-tidocument summarization system that operates on news articles describing the same event.
M ULTI G EN identifies text units that convey similar information across documents and clusters them into themes.
Each theme is next syntactically analysed into pred-icate argument structures; the structures that are re-peated often enough are chosen to be included into the summary.
A language generation system outputs a sentence (per theme) from the selected predicate argument structures.
This way they ensured that order-ings were not influenced by mistakes their system could have made.
Explicit references and connec-tives were removed from the sentences so as not to reveal clues about the sentence ordering.
Ten sub-jects provided orders for each summary which had an average length of 8.8.
We simulated the participants’ task by using the model from Section 2.1 to produce an order for each candidate summary 1 .
"We then compared the differ-ences in the orderings generated by the model and participants using the best performing features from Experiment 2 (i.e., N L and V D N D )."
"Note that the model was trained on the B LLIP corpus, whereas the sentences to be ordered were taken from news arti-cles describing the same event."
Not only were the news articles unseen but also their syntactic struc-ture was unfamiliar to the model.
"The results are shown in table 5, again average pairwise τ is re-ported."
We also give the naive baseline of choosing a random order (B R ).
The average distance in the orderings produced by Barzilay et al.’s (2002) par-ticipants is .60.
The distance between the humans and N L is .48 whereas the average distance between V D N D and the humans is .56.
"An A NOVA yielded a significant effect of feature type (F(3,27) = 15.25; p &lt; 0.01)."
"Post-hoc Tukey tests showed that V D N D was significantly better than B R , but N L wasn’t."
The difference between V D N D and H H was not signifi-cant.
"Although N L performed adequately in Experi-ments 1 and 2, it failed to outperform the baseline in the summarization task."
This may be due to the fact that entity-based coherence is not as important as temporal coherence for the news articles summaries.
Recall that the summaries describe events across documents.
This information is captured more ad-equately by V D N D and not by N L that only keeps a record of the entities in the sentence.
In this paper we proposed a data intensive approach to text coherence where constraints on sentence or-dering are learned from a corpus of domain-specific texts.
We experimented with different feature encod-ings and showed that lexical and syntactic informa-tion is important for the ordering task.
Our results indicate that the model can successfully generate or-ders for texts taken from the corpus on which it is trained.
The model also compares favorably with hu-man performance on a single- and multiple docu-ment ordering task.
"Our model operates on the surface level rather than the logical form and is therefore suitable for text-to-text generation systems; it acquires ordering constraints automatically, and can be easily ported to different domains and text genres."
"The model is par-ticularly relevant for multidocument summarization since it could provide an alternative to chronolog-ical ordering especially for documents where pub-lication date information is unavailable or uninfor-mative (e.g., all documents have the same date)."
We proposed Kendall’s τ as an automated method for evaluating the generated orders.
There are a number of issues that must be ad-dressed in future work.
So far our evaluation metric measures order similarities or dissimilarities.
This enables us to assess the importance of particular feature combinations automatically and to evaluate whether the model and the search algorithm gener-ate potentially acceptable orders without having to run comprehension experiments each time.
Such ex-periments however are crucial for determining how coherent the generated texts are and whether they convey the same semantic content as the originally authored texts.
For multidocument summarization comparisons between our model and alternative or-dering strategies are important if we want to pursue this approach further.
Several improvements can take place with re-spect to the model.
An obvious question is whether a trigram model performs better than the model presented here.
The greedy algorithm implements a search procedure with a beam of width one.
"In the future we plan to experiment with larger widths (e.g., two or three) and also take into account fea-tures that express semantic similarities across docu-ments either by relying on WordNet or on automatic clustering methods."
"We investigate the verbal and nonverbal means for grounding, and propose a design for embodied conversational agents that re-lies on both kinds of signals to establish common ground in human-computer inter-action."
"We analyzed eye gaze, head nods and attentional focus in the context of a di-rection-giving task."
"The distribution of nonverbal behaviors differed depending on the type of dialogue move being grounded, and the overall pattern reflected a monitor-ing of lack of negative feedback."
"Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to up-date dialogue state."
"An essential part of conversation is to ensure that the other participants share an understanding of what has been said, and what is meant."
The proc-ess of ensuring that understanding – adding what has been said to the common ground – is called grounding [1].
"In face-to-face interaction, nonver-bal signals as well as verbal participate in the grounding process, to indicate that an utterance is grounded, or that further work is needed to ground."
Figure 1 shows an example of human face-to-face conversation.
"Even though no verbal feedback is provided, the speaker (S) continues to add to the directions."
"Intriguingly, the listener gives no ex-plicit nonverbal feedback – no nods or gaze to-wards S. S, however, is clearly monitoring the listener’s behavior, as we see by the fact that S looks at her twice (continuous lines above the words)."
"In fact, our analyses show that maintaining focus of attention on the task (dash-dot lines un-derneath the words) is the listener’s public signal of understanding S’s utterance sufficiently for the task at hand."
"Because S is manifestly attending to this signal, the signal allows the two jointly to rec-ognize S’s contribution as grounded."
"This paper provides empirical support for an essential role for nonverbal behaviors in grounding, motivating an architecture for an embodied conversational agent that can establish common ground using eye gaze, head nods, and attentional focus."
"Although grounding has received significant at-tention in the literature, previous work has not ad-dressed the following questions: (1) what predictive factors account for how people use non-verbal signals to ground information, (2) how can a model of the face-to-face grounding process be used to adapt dialogue management to face-to-face conversation with an embodied conversational agent."
"This paper addresses these issues, with the goal of contributing to the literature on discourse phenomena, and of building more advanced con-versational humanoids that can engage in human conversational protocols."
"In the next section, we discuss relevant previous work, report results from our own empirical study and, based on our analysis of conversational data, propose a model of grounding using both verbal and nonverbal information, and present our im-plementation of that model into an embodied con-versational agent."
"As a preliminary evaluation, we compare a user interacting with the embodied con-versational agent with and without grounding."
Conversation can be seen as a collaborative activ-ity to accomplish information-sharing and to pur-sue joint goals and tasks.
"Under this view, agreeing on what has been said, and what is meant, is crucial to conversation."
"The part of what has been said that the interlocutors understand to be mutually shared is called the common ground, and the process of establishing parts of the conversa-tion as shared is called grounding [1]."
"As [2] point out, participants in a conversation attempt to minimize the effort expended in grounding."
"Thus, interlocutors do not always convey all the informa-tion at their disposal; sometimes it takes less effort to produce an incomplete utterance that can be re-paired if needs be. [3] has proposed a computational approach to grounding where the status of contributions as provisional or shared is part of the dialogue system’s representation of the “information state” of the conversation."
Conversational actions can trigger updates that register provisional information as shared.
These actions achieve grounding.
"Acknowledgment acts are directly as-sociated with grounding updates while other utter-ances effect grounding updates indirectly, because they proceed with the task in a way that presup-poses that prior utterances are uncontroversial. [4], on the other hand, suggest that actions in conversation give probabilistic evidence of under-standing, which is represented on a par with other uncertainties in the dialogue system (e.g., speech recognizer unreliability)."
"The dialogue manager assumes that content is grounded as long as it judges the risk of misunderstanding as acceptable. [1, 5] mention that eye gaze is the most basic form of positive evidence that the addressee is at-tending to the speaker, and that head nods have a similar function to verbal acknowledgements."
"They suggest that nonverbal behaviors mainly contribute to lower levels of grounding, to signify that inter-locutors have access to each other’s communica-tive actions, and are attending."
"With a similar goal of broadening the notion of communicative action beyond the spoken word, [6] examine other kinds of multimodal grounding behaviors, such as post-ing information on a whiteboard."
"Although these and other researchers have suggested that nonver-bal behaviors undoubtedly play a role in grounding, previous literature does not characterize their pre-cise role with respect to dialogue state."
"On the other hand, a number of studies on these particular nonverbal behaviors do exist."
"An early study, [7], reported that conversation involves eye gaze about 60% of the time."
"Speakers look up at grammatical pauses for feedback on how utter-ances are being received, and also look at the task."
Listeners look at speakers to follow their direction of gaze.
"In fact, [8] claimed speakers will pause and restart until they obtain the listener’s gaze. [9] found that during conversational difficulties, mu-tual gaze was held longer at turn boundaries."
"Previous work on embodied conversational agents (ECAs) has demonstrated that it is possible to implement face-to-face conversational protocols in human-computer interaction, and that correct relationships among verbal and nonverbal signals enhances the naturalness and effectiveness of em-bodied dialogue systems [10], [11]. [12] reported that users felt the agent to be more helpful, lifelike, and smooth in its interaction style when it demon-strated nonverbal conversational behaviors."
"In order to get an empirical basis for modeling face-to-face grounding, and implementing an ECA, we analyzed conversational data in two conditions."
"Based on previous direction-giving tasks, students from two different universities gave directions to campus locations to one another."
"Each pair had a conversation in a (1) Face-to-face condition (F2F): where two subjects sat with a map drawn by the direction-giver sitting between them, and in a (2) Shared Reference condition (SR): where an L-shaped screen between the subjects let them share a map drawn by the direction-giver, but not to see the other’s face or body."
"Interactions between the subjects were video-recorded from four different angles, and combined by a video mixer into synchronized video clips."
"10 experiment sessions resulted in 10 dialogues per condition (20 in total), transcribed as follows."
Coding verbal behaviors:
"As grounding oc-curs within a turn, which consists of consecutive utterances by a speaker, following [13] we token-ized a turn into utterance units (UU), correspond-ing to a single intonational phrase [14]."
Each UU was categorized using the DAMSL coding scheme [15].
"In the statistical analysis, we concentrated on the following four categories with regular occur-rence in our data: Acknowledgement, Answer, In-formation request (Info-req), and Assertion."
"Coding nonverbal behaviors: Based on previ-ous studies, four types of behaviors were coded:"
"Gaze At Partner (gP): Looking at the partner’s eyes, eye region, or face."
Gaze At Map (gM): Looking at the map
Gaze Elsewhere (gE): Looking away elsewhere
"Head nod (Nod): Head moves up and down in a single continuous movement on a vertical axis, but eyes do not go above the horizontal axis."
"By combining Gaze and Nod, six complex catego-ries (ex. gP with nod, gP without nod, etc) are gen-erated."
"In what follows, however, we analyze only categories with more than 10 instances."
"In order to analyze dyadic behavior, 16 combinations of the nonverbal behaviors are defined, as shown in Table 1."
"Thus, gP/gM stands for a combination of speaker gaze at partner and listener gaze at map."
"Analyzing spoken dialogues, [18] reported that grounding behavior is more likely to occur at an intonational boundary, which we use to identify UUs."
This implies that multiple grounding behav-iors can occur within a turn if it consists of multi-ple UUs.
"However, in previous models, information is grounded only when a listener re-turns verbal feedback, and acknowledgement marks the smallest scope of grounding."
"If we ap-ply this model to the example in Figure 1, none of the UU have been grounded because the listener has not returned any spoken grounding clues."
"In contrast, our results suggest that considering the role of nonverbal behavior, especially eye-gaze, allows a more fine-grained model of grounding, employing the UU as a unit of grounding."
"Our results also suggest that speakers are ac-tively monitoring positive evidence of understand-ing, and also the absence of negative evidence of understanding (that is, signs of miscommunication)."
"When listeners continue to gaze at the task, speak-ers continue on to the next leg of directions."
"Because of the incremental nature of grounding, we implement nonverbal grounding functionality into an embodied conversational agent using a process model that describes steps for a system to judge whether a user understands system contribu-tion: (1) Preparing for the next UU: according to the speech act type of the next UU, nonverbal posi-tive or negative evidence that the agent expects to receive are specified. (2) Monitoring: monitors and checks the user’s nonverbal status and signals dur-ing the UU."
"After speaking, the agent continues monitoring until s/he gets enough evidence of un-derstanding or not-understanding represented by user’s nonverbal status and signals.(3)"
"Judging: once the agent gets enough evidence, s/he tries to judge groundedness as soon as possible."
"According to some previous studies, length of pause between UUs is in between 0.4 to 1 sec [18, 19]."
"Thus, time out for judgment is 1 sec after the end of the UU."
"If the agent does not have evidence then, the UU re-mains ungrounded."
"This model is based on the information state approach [3], with update rules that revise the state of the conversation based on the inputs the system receives."
"In our case, however, the inputs are sam-pled continuously, include the nonverbal state, and only some require updates."
"Other inputs indicate that the last utterance is still pending, and allow the agent to wait further."
"In particular, task attention over an interval following the utterance triggers grounding."
"Gaze in the interval means that the contribution stays provisional, and triggers an ob-ligation to elaborate."
"Likewise, if the system times-out without recognizing any user feedback, the segment remains ungrounded."
This process allows the system to keep talking across multiple utterance units without getting verbal feedback from the user.
"From the user’s perspective, explicit acknowledgement is not necessary, and minimal cost is involved in eliciting elaboration."
"Based on our empirical results, we propose a dia-logue manager that can handle nonverbal input to the grounding process, and we implement the mechanism in an embodied conversational agent."
MACK is an interactive public information ECA kiosk.
"His current knowledgebase concerns the activities of the MIT Media Lab; he can answer questions about the lab’s research groups, projects, and demos, and give directions to each."
"On the input side, MACK recognizes three mo-dalities: (1) speech, using IBM’s ViaVoice, (2) pen gesture via a paper map atop a table with an em-bedded Wacom tablet, and (3) head nod and eye gaze via a stereo-camera-based 6-degree-of-freedom head-pose tracker (based on [20])."
"These inputs operate as parallel threads, allowing the Un-derstanding Module (UM) to interpret the multiple modalities both individually and in combination."
"MACK produces multimodal output as well: (1) speech synthesis using the Microsoft Whistler Text-to-Speech (TTS) API, (2) a graphical figure with synchronized hand and arm gestures, and head and eye movements, and (3) LCD projector highlighting on the paper map, allowing MACK to reference it."
The system architecture is shown in Figure 3.
The UM interprets the input modalities and con-verts them to dialogue moves which it then passes on to the Dialogue Manager (DM).
"The DM con-sists of two primary sub-modules, the Response Planner, which determines MACK’s next action(s) and creates a sequence of utterance units, and the Grounding Module (GrM), which updates the Dis-course Model and decides when the Response Planner’s next UU should be passed on to the Gen-eration module (GM)."
"The GM converts the UU into speech, gesture, and projector output, sending these synchronized modalities to the TTS engine, Animation Module (AM), and Projector Module."
The Discourse Model maintains information about the state and history of the discourse.
"This includes a list of grounded beliefs and ungrounded UUs; a history of previous UUs with timing infor-mation; a history of nonverbal information (di-vided into gaze states and head nods) organized by timestamp; and information about the state of the dialogue, such as the current UU under considera-tion, and when it started and ended."
"Eye gaze and head nod inputs are recognized by a head tracker, which calculates rotations and trans-lations in three dimensions based on visual and depth information taken from two cameras [20]."
"The calculated head pose is translated into “look at MACK,” “look at map,” or “look elsewhere.”"
"The rotation of the head is translated into head nods, using a modified version of [21]."
Head nod and eye gaze events are timestamped and logged within the nonverbal component of the Discourse History.
The Grounding Module can thus look up the ap-propriate nonverbal information to judge a UU.
"In a kiosk ECA, the system needs to ensure that the user understands the information provided by the agent."
"For this reason, we concentrated on imple-menting a grounding mechanism for Assertion, when the agent gives the user directions, and An swer, when the agent answers the user’s questions"
Figure 4 shows an example of a user&apos;s interaction with MACK.
"The user asks MACK for directions, and MACK replies using speech and pointing (us-ing a projector) to the shared map."
"When the GrM sends the first segment in the Agenda to the GM,the starting time of the UU is noted and it is sent to the AM to be spoken and animated."
"During this time, the user’s nonverbal signals are logged in the Discourse Model."
"When the UU has finished, the GrM evaluates the log of the UU and of the very beginning of the pause (by waiting a tenth of a second and then checking the nonverbal history)."
"In this case, MACK noted that the user looked at the map during the UU, and con-tinued to do so just afterwards."
This pattern matches the model for Assertion.
"The UU is judged as grounded, and the grounded belief is added to the Discourse Model."
"MACK then utters the second segment as be-fore, but this time the GrM, finds that the user was looking up at MACK during most of the UU as well as after it, which signals that the UU is not grounded."
"Therefore, the RP generates an elabora-tion (line 4)."
"This utterance is judged to be grounded both because the user continues looking at the map, and because the user nods, and so the final stage of the directions is spoken."
"This is also grounded, leaving MACK ready for a new inquiry."
"Although we have shown an empirical basis for our implementation, it is important to ensure both that human users interact with MACK as we ex-pect, and that their interaction is more effective than without nonverbal grounding."
The issue of effectiveness merits a full-scale study and thus we have chosen to concentrate here on whether MACK elicits the same behaviors from users as does interaction with other humans.
"Two subjects were therefore assigned to one of the following two conditions, both of which were run as Wizard of Oz (that is, “speech recognition” was carried out by an experimenter): (a) MACK-with-grounding: MACK recognized user’s nonverbal signals for grounding, and dis-played his nonverbal signals as a speaker. (b) MACK-without-grounding:"
"MACK paid no attention to the user’s nonverbal behavior, and did not display nonverbal signals as a speaker."
He gave the directions in one single turn.
"Subjects were instructed to ask for directions to two places, and were told that they would have to lead the experimenters to those locations to test their comprehension."
"We analyzed the second di-rection-giving interaction, after subjects became accustomed to the system."
"Results: In neither condition, did users return ver-bal feedback during MACK’s direction giving."
"As shown in Table 4, in MACK-with-grounding 7 nonverbal status transitions were observed during his direction giving, which consisted of 5 Assertion UUs, one of them an elaboration."
The transition patterns between MACK and the user when
MACK used nonverbal grounding are strikingly similar to those in our empirical study of human-to-human communication.
"There were three transi-tions to gM/gM (both look at the map), which is a normal status in map task conversation, and two transitions to gP/gM (MACK looks at the user, and the user looks at the map), which is the most fre-quent transition in Assertion as reported in Section 3."
"Moreover, in MACK’s third UU, the user began looking at MACK at the middle of the UU and kept looking at him after the UU ended."
This be-havior successfully elicited MACK’s elaboration in the next UU.
"On the other hand, in the MACK-without-grounding condition, the user never looked at MACK, and nodded only once, early on."
"As shown in Table 4, only three transitions were observed (shift to gMgM at the beginning of the interaction, shift to gMgMwN, then back to gMgM)."
"While a larger scale evaluation with quantita-tive data is one of the most important issues for future work, the results of this preliminary study strongly support our model, and show MACK’s potential for interacting with a human user using human-human conversational protocols."
We have reported how people use nonverbal sig-nals in the process of grounding.
We found that nonverbal signals that are recognized as positive evidence of understanding are different depending on the type of speech act.
"We also found that main-taining gaze on the speaker is interpreted as evi-dence of not-understanding, evoking an additional explanation from the speaker."
"Based on these em-pirical results, we proposed a model of nonverbal grounding and implemented it in an embodied conversational agent."
One of the most important future directions is to establish a more comprehensive model of face-to-face grounding.
"Our study focused on eye gaze and head nods, which directly contribute to grounding."
"It is also important to analyze other types of nonverbal behaviors and investigate how they interact with eye gaze and head nods to achieve common ground, as well as contradictions between verbal and nonverbal evidence (eg. an interlocutor says, “OK”, but looks at the partner)."
"Finally, the implementation proposed here is a simple one, and it is clear that a more sophisticated dialogue management strategy is warranted, and will allow us to deal with back-grounding, and other aspects of miscommunication."
"For example, it would be useful to distinguish different levels of miscommunication: a sound that may or may not be speech, an out-of-grammar utterance, or an ut-terance whose meaning is ambiguous."
"In order to deal with such uncertainty in grounding, incorpo-rating a probabilistic approach [4] into our model of face-to-face grounding is an elegant possibility."
We present a domain-independent topic segmentation algorithm for multi-party speech.
Our feature-based algorithm com-bines knowledge about content using a text-based algorithm as a feature and about form using linguistic and acous-tic cues about topic shifts extracted from speech.
This segmentation algorithm uses automatically induced decision rules to combine the different features.
The em-bedded text-based algorithm builds on lex-ical cohesion and has performance compa-rable to state-of-the-art algorithms based on lexical information.
A significant er-ror reduction is obtained by combining the two knowledge sources.
"Topic segmentation aims to automatically divide text documents, audio recordings, or video segments, into topically related units."
"While extensive research has targeted the problem of topic segmentation of written texts and spoken monologues, few have stud-ied the problem of segmenting conversations with many participants (e.g., meetings)."
"In this paper, we present an algorithm for segmenting meeting tran-scripts."
"This study uses recorded meetings of typi-cally six to eight participants, in which the informal style includes ungrammatical sentences and overlap-ping speakers."
"These meetings generally do not have pre-set agendas, and the topics discussed in the same meeting may or may not related."
"The meeting segmenter comprises two compo-nents: one that capitalizes on word distribution to identify homogeneous units that are topically cohe-sive, and a second component that analyzes conver-sational features of meeting transcripts that are in-dicative of topic shifts, like silences, overlaps, and speaker changes."
We show that integrating features from both components with a probabilistic classifier (induced with c4.5rules) is very effective in improv-ing performance.
"In Section 2, we review previous approaches to the segmentation problem applied to spoken and written documents."
"In Section 3, we describe the corpus of recorded meetings intended to be seg-mented, and the annotation of its discourse structure."
"In Section 4, we present our text-based segmenta-tion component."
"This component mainly relies on lexical cohesion, particularly term repetition, to de-tect topic boundaries."
We evaluated this segmenta-tion against other lexical cohesion segmentation pro-grams and show that the performance is state-of-the-art.
"In the subsequent section, we describe conver-sational features, such as silences, speaker change, and other features like cue phrases."
We present a machine learning approach for integrating these con-versational features with the text-based segmenta-tion module.
Experimental results show a marked improvement in meeting segmentation with the in-corporation of both sets of features.
We close with discussions and conclusions.
Existing approaches to textual segmentation can be broadly divided into two categories.
"On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive."
"Embodi-ments of this idea include semantic similarity[REF_CITE], cosine similarity in word vector space[REF_CITE], inter-sentence similarity matrix[REF_CITE], en-tity repetiti[REF_CITE], word frequency models[REF_CITE], or adaptive language models[REF_CITE]."
"Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases[REF_CITE]."
"In work on segmentation of spoken docu-ments, intonational, prosodic, and acoustic indica-tors are used to detect topic boundaries[REF_CITE]."
"Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak."
"These approaches use different learning mecha-nisms to combine features, including decision trees[REF_CITE]exponential models[REF_CITE]or other probabilistic mod-els[REF_CITE]."
We have evaluated our segmenter on the ICSI Meet-ing corpus[REF_CITE].
This corpus is one of a growing number of corpora with human-to-human multi-party conversations.
"In this corpus, record-ings of meetings ranged primarily over three differ-ent recurring meeting types, all of which concerned speech or language research. [Footnote_1]"
"1 While it would be desirable to have a broader variety of meetings, we hope that experiments on this corpus will still carry some generality."
"The average duration is 60 minutes, with an average of 6.5 participants."
"They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content."
"From the corpus, we selected 25 meetings to be segmented, each by at least three subjects."
"We opted for a linear representation of discourse, since finer-grained discourse structures (e.g.[REF_CITE]) are generally considered to be diffi-cult to mark reliably."
Subjects were asked to mark each speaker change (potential boundary) as either boundary or non-boundary.
"In the resulting anno-tation, the agreed segmentation based on majority opinion contained 7.5 segments per meeting on av-erage, while the average number of potential bound-aries is 770."
We used Cochran’s Q (1950) to eval-uate the agreement among annotators.
Cochran’s test evaluates the null hypothesis that the number of subjects assigning a boundary at any position is randomly distributed.
"The test shows that the inter-judge reliability is significant to the 0.05 level for 19 of the meetings, which seems to indicate that seg-ment identification is a feasible task. [Footnote_2]"
"2 Four other meetings failed short the significance test, while there was little agreement on the two last ones (p &gt; 0.1)."
Previous work on discourse segmentation of written texts indicates that lexical cohesion is a strong in-dicator of discourse structure.
"Lexical cohesion is a linguistic property that pertains to speech as well, and is a linguistic phenomenon that can also be ex-ploited in our case: while our data does not have the same kind of syntactic and rhetorical structure as written text, we nonetheless expect that informa-tion from the written transcription alone should pro-vide indications about topic boundaries."
"In this sec-tion, we describe our work on LCseg, a topic seg-menter based on lexical cohesion that can handle both speech and text, but that is especially designed to generate the lexical cohesion feature used in the feature-based segmentation described in Section 5."
"LCseg computes lexical chains, which are thought to mirror the discourse structure of the underly-ing text[REF_CITE]."
"We ignore syn-onymy and other semantic relations, building a re-stricted model of lexical chains consisting of sim-ple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repeti-tions start and end."
"While other relations between lexical items also work as cohesive factors (e.g. be-tween a term and its super-ordinate), the work on linear topic segmentation reporting the most promis-ing results account for term repetitions alone[REF_CITE]."
The preprocessing steps of LCseg are common to many segmentation algorithms.
"The input document is first tokenized, non-content words are removed, and remaining words are stemmed using an exten-sion of Porter’s stemming algorithm[REF_CITE]that conflates stems using corpus statistics."
Stemming will allow our algorithm to more accu-rately relate terms that are semantically close.
"The core algorithm of LCseg has two main parts: a method to identify and weight strong term repeti-tions using lexical chains, and a method to hypothe-size topic boundaries given the knowledge of multi-ple, simultaneous chains of term repetitions."
A term is any stemmed content word within the text.
A lexical chain is constructed to consist of all repetitions ranging from the first to the last appear-ance of the term in the text.
"The chain is divided into subchains when there is a long hiatus of h consecu-tive sentences with no occurrence of the term, where h is determined experimentally."
"For each hiatus, a new division is made and thus, we avoid creating weakly linked chains."
"For all chains that have been identified, we use a weighting scheme that we believe is appropriate to the task of inducing the topical or sub-topical struc-ture of text."
The weighting scheme depends on two factors:
Frequency: chains containing more repeated terms receive a higher score.
Compactness: shorter chains receive a higher weight than longer ones.
"If two chains of different lengths contain the same number of terms, we assign a higher score to the shortest one."
"Our assumption is that the shorter one, being more compact, seems to be a better indicator of lexical cohesion. [Footnote_3]"
"3 The latter parameter might seem controversial at first, and one might assume that longer chains should receive a higher score. However we point out that in a linear model of dis-course, chains that almost span the entire text are barely indica-tive of any structure (assuming boundaries are only hypothe-sized where chains start and end)."
"We apply a variant of a metric commonly used in information retrieval, TF.IDF[REF_CITE], to score term repetitions."
If R 1 . . .
"R n is the set of all term repetitions collected in the text, t 1 . . . t n the corresponding terms, L 1 . . ."
"L n their re-spective lengths, [Footnote_4] and L the length of the text, the adapted metric is expressed as follows, combining frequency (freq(t i )) of a term t i and the compact-ness of its underlying chain: score(R i ) = freq(t i ) · log( LL ) i"
4 All lengths are expressed in number of sentences.
"In the second part of the algorithm, we combine information from all term repetitions to compute a lexical cohesion score at each sentence break (or, in the case of spoken conversations, speaker turn break)."
This step of our algorithm is very similar in spirit to TextTiling[REF_CITE].
"The idea is to work with two adjacent analysis windows, each of fixed size k."
"For each sentence break, we determine a lexical cohesion function by computing the cosine similarity at the transition between the two windows."
"Instead of using word counts to compute similarity, we analyze lexical chains that overlap with the two windows."
The similarity between windows (A and B) is computed with: [Footnote_5]
"5 Normalizing anything in these windows has little ef-fect, since the cosine similarity is scale invariant, that is cosine(αx a , x b ) = cosine(x a , x b ) for α &gt; 0."
"P w i,A ·w i,B cosine(A, B) = qP i i w i2,A"
"P i w i2,B where ( score(R i ) if R i overlaps Γ ∈ {A, B} w i,Γ = 0 otherwise"
The similarity computed at each sentence break produces a plot that shows how lexical cohesion changes over time; an example is shown in Figure 1.
"The lexical cohesion function is then smoothed us-ing a moving average filter, and minima become po-tential segment boundaries."
"Then, in a manner quite similar[REF_CITE], the algorithm determines for every local minimum m i how sharp of a change there is in the lexical cohesion function."
"The algo-rithm looks on each side of m i for maxima of cohe-sion, and once it eventually finds one on each side (l and r), it computes the hypothesized segmentation probability: p(m i ) = 12 [LCF(l) + LCF(r) − 2 · LCF(m)] where LCF(x) is the value of the lexical cohesion function at x."
"This score is supposed to capture the sharpness of the change in lexical cohesion, and give probabilities close to 1 for breaks like sentence 179 in Figure 1."
"Finally, the algorithm selects the hypothesized boundaries with the highest computed probabilities."
"If the number of reference boundaries is unknown, the algorithm has to make a guess."
It computes the mean and the variance of the hypothesized probabil-ities of all potential boundaries (local minima).
"As we can see in Figure 1, there are many local minima that do not correspond to actual boundaries."
"Thus, we ignore all potential boundaries with a probability lower than p limit ."
"For the remaining points, we com-pute the threshold using the average (µ) and standard deviation (σ) of the p(m i ) values, and each potential boundary m i above the threshold µ−α·σ is hypoth-esized as a real boundary."
We evaluate LCseg against two state-of-the-art seg-mentation algorithms based on lexical cohesi[REF_CITE].
We use the error metric P k proposed[REF_CITE]to evaluate segmentation accuracy.
It com-putes the probability that sentences k units (e.g. sen-tences) apart are incorrectly determined as being ei-ther in different segments or in the same one.
"Since it has been argued[REF_CITE]that P k has some weaknesses, we also include results ac-cording to the WindowDiff (WD) metric (which is described in the same work)."
A test corpus of concatenated [Footnote_6] texts extracted from the Brown corpus was built[REF_CITE]to evaluate several domain-independent segmenta-tion algorithms.
6 Concatenated documents correspond to reference seg-ments.
"We reuse the same test corpus for our evaluation, in addition to two other test corpora we constructed to test how segmenters scale across genres and how they perform with texts with various number of segments. [Footnote_7] We designed two test corpora, each of 500 documents, using concatenated texts extracted from the TDT and WSJ corpora, ranging from 4 to 22 in number of segments."
7 All texts in Choi’s test corpus have exactly 10 segments. We are concerned that the adjustments of any algorithm param-eters might overfit this predefined number of segments.
LCseg depends on several parameters.
Parameter tuning was performed on three tuning corpora of one thousand texts each. [Footnote_8]
8 These texts are different from the ones used for evaluation.
"We performed searches for the optimal settings of the four tunable parameters in-troduced above; the best performance was achieved with h = 11 (hiatus length for dividing a chain into parts), k = 2 (analysis window size), p limit = 0.1 and α = 12 (thresholding limits for the hypothesized boundaries)."
"As shown in Table 1, our algorithm is signifi-cantly better than[REF_CITE](labeled C99) on all three test corpora, according to a one-sided t-test of the null hypothesis of equal mean at the 0.01 level."
It is not clear whether our algorithm is better than[REF_CITE](U00).
"When the number of segments is provided to the algorithms, our algorithm is significantly better than Utiyama’s on WSJ, better on Brown (but not significant), and significantly worse on TDT."
"When the number of boundaries is unknown, our algorithm is insignifi-cantly worse on Brown, but significantly better on WSJ and TDT – the two corpora designed to have a varying number of segments per document."
"In the case of the Meeting corpus, none of the algorithms are significantly different than the others, due to the small test set size."
"In conclusion, LCseg has a performance compara-ble to state-of-the-art text segmentation algorithms, with the added advantage of computing a segmen-tation probability at each potential boundary."
"This information can be effectively used in the feature-based segmenter to account for lexical cohesion, as described in the next section."
"In the previous section, we have concentrated exclu-sively on the consideration of content (through lexi-cal cohesion) to determine the structure of texts, ne-glecting any influence of form."
"In this section, we explore formal devices that are indicative of topic shifts, and explain how we use these cues to build a segmenter targeting conversational speech."
"Topic segmentation is reduced here to a classifica-tion problem, where each utterance break B i is ei-ther considered a topic boundary or not."
"We use statistical modeling techniques to build a classifier that uses local features (e.g. cue phrases, pauses) to determine if an utterance break corresponds to a topic boundary."
"We chose C4.5 and C4.5rules[REF_CITE], two programs to induce classifi-cation rules in the form of decision trees and pro-duction rules (respectively)."
"C4.5 generates an un-pruned decision tree, which is then analyzed by C4.5rules to generate a set of pruned production rules (it tries to find the most useful subset of them)."
"The advantage of pruned rules over decision trees is that they are easier to analyze, and allow combina-tion of features in the same rule (feature interactions are explicit)."
The greedy nature of decision rule learning algo-rithms implies that a large set of features can lead to bad performance and generalization capability.
"It is desirable to remove redundant and irrelevant fea-tures, especially in our case since we have little data labeled with topic shifts; with a large set of fea-tures, we would risk overfitting the data."
"We tried to restrict ourselves to features whose inclusion is motivated by previous work (pauses, speech rate) and added features that are specific to multi-speaker speech (overlap, changes in speaker activity)."
"Cue phrases: previous work on segmentation has found that discourse particles like now, well pro-vide valuable information about the structure of texts[REF_CITE]."
"We analyzed the correlation between words in the meeting cor-pus and labeled topic boundaries, and automatically extracted utterance-initial cue phrases [Footnote_9] that are sta-tistically correlated with boundaries."
"9 As[REF_CITE], we restrict ourselves to the first lexical item of any utterance, plus the second one if the first item is also a cue word."
"For every word in the meeting corpus, we counted the number of its occurrences near any topic boundary, and its num-ber of appearances overall."
"Then, we performed χ 2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists."
We se-lected terms whose χ 2 value rejected the hypothesis under a 0.01-level confidence (the rejection criterion is χ 2 ≥ 6.635).
"Finally, induced cue phrases whose usage has never been described in other work were removed (marked with ∗ in Table 3)."
"Indeed, there is a risk that the automatically derived list of cue phrases could be too specific to the word usage in these meetings."
Silences: previous work has found that ma-jor shifts in topic typically show longer silences[REF_CITE].
"We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinc-tion between pauses and gaps[REF_CITE]."
"A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech."
"Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion."
"As an approximation of this distinc-tion, we classified a silence that follows a question or in the middle of somebody’s speech as a pause, and any other silences as a gap."
"While the correlation be-tween long silences and discourse boundaries seem to be less pervasive in meetings than in other speech corpora, we have noticed that some topic boundaries are preceded (within some window) by numerous gaps."
"However, we found little correlation between pauses and topic boundaries."
Overlaps: we also analyzed the distribution of overlapping speech by counting the average overlap rate within some window.
"We noticed that, many times, the beginning of segments are characterized by having little overlapping speech."
Speaker change: we sometimes noticed a corre-lation between topic boundaries and sudden changes in speaker activity.
"For example, in Figure 2, it is clear that the contribution of individual speakers to the discussion can greatly change from one dis-course unit to the next."
We try to capture significant changes in speakership by measuring the dissimilar-ity between two analysis windows.
"For each poten-tial boundary, we count for each speaker i the num-ber of words that are uttered before (L i ) and after (R i ) the potential boundary (we limit our analysis to a window of fixed size)."
"The two distributions are normalized to form two probability distributions l and r, and significant changes of speakership are detected by computing their Jensen-Shannon diver-gence:"
"JS(l, r) = 12 [D(l||avg l,r ) +"
"D(r||avg l,r )] where D(l||r) is the KL-divergence between the two distributions."
Lexical cohesion: we also incorporated the lexi-cal cohesion function computed by LCseg as a fea-ture of the multi-source segmenter in a manner simi-lar to the knowledge source combination performed[REF_CITE]and[REF_CITE].
Note that we use both the posterior estimate com-puted by LCseg and the raw lexical cohesion func-tion as features of the system.
"For every potential boundary B i , the classifier ana-lyzes features in a window surrounding B i to decide whether it is a topic boundary or not."
It is generally unclear what is the optimal window size and how features should be analyzed.
"Windows of various sizes can lead to different levels of prediction, and in some cases, it might be more appropriate to only extract features preceding or following B i ."
"We avoided making arbitrary choices of parame-ters; instead, for any feature F and a set F 1 , . . . , F n of possible ways to measure the feature (different window sizes, different directions), we picked the F i that is in isolation the best predictor of topic bound-aries (among F 1 , . . . , F n )."
Table 4 presents for each feature the analysis mode that is the most useful on the training data.
"Feature selection and decision rule learning is always performed on sets of 24 meetings, while the held-out data is used for testing."
Table 5 gives some examples of the type of rules that are learned.
"The first rule states that if the value for the lexical cohesion (LC) function is low at the current sen-tence break, there is at least one CUE phrase, there is less than three seconds of silence to the left of the break, [Footnote_10] and a single speaker holds the floor for a longer period of time than usual to the right of the break, then we have a topic break."
10 Note that rules are not always meaningful in isolation and it is likely that a subordinate rule in the tree to this one would do further tests on silence to determine if a topic boundary exists.
"In general, we found that the derived rules show that lexical cohe-sion plays a stronger role than most other features in determining topic breaks."
"Nonetheless, the quan-titative results summarized in table 6, which corre-spond to the average performance on the held-out sets, show that the integration of conversational fea-tures with the text-based segmenter outperforms ei-ther alone."
We presented a domain-independent segmentation algorithm for multi-party conversation that inte-grates features based on content with features based on form.
The learned combination of features results in a significant increase in accuracy over previous approaches to segmentation when applied to meet-ings.
Features based on form that are likely to in-dicate topic shifts are automatically extracted from speech.
Content based features are computed by a segmentation algorithm that utilizes a metric of lex-ical cohesion and that performs as well as state-of-the-art text-based segmentation techniques.
It works both with written and spoken texts.
"The text-based segmentation approach alone, when applied to meet-ings, outperforms all other segmenters, although the difference is not statistically significant."
"In future work, we would like to investigate the effects of adding prosodic features, such as pitch ranges, to our segmenter, as well as the effect of using errorful speech recognition transcripts as op- posed to manually transcribed utterances."
An implementation of our lexical cohesion seg-menter is freely available for educational or research purposes. 11
"In this paper, we present a method for the semantic tagging of word chunks ex-tracted from a written transcription of con-versations."
This work is part of an ongo-ing project for an information extraction system in the field of maritime Search And Rescue (SAR).
Our purpose is to auto-matically annotate parts of texts with con-cepts from a SAR ontology.
"Our approach combines two knowledge sources a SAR ontology and the Wordsmyth dictionary-thesaurus, and it uses a similarity measure for the classification."
Evaluation is carried out by comparing the output of the system with key answers of predefined extraction templates.
This work is a part of a project aiming to imple-ment an information extraction (IE) system in the field of maritime Search And Rescue (SAR).
It was originally conducted by the Defense Research Es-tablishment Valcartier (DREV) to develop a deci-sion support tool to help in producing SAR plans given the information extracted by the SAR IE sys-tem from a collection of transcribed dialogs.
The goal of our project is to develop a robust approach to extract relevant words for small-scale corpora and transcribed speech dialogs.
"To achieve this task, we developed a semantic tagger which annotates words with domain-specific informations and a selection process to extract or reject a word according to the semantic tag and the context."
"The rationale behind our approach, is that the relevance of a word depends strongly on how close it is to the SAR domain and its context of use."
We believe that reasoning on se-mantic tags instead of the word is a way of getting around some of the problems of small-scale corpora.
"In this paper, we focus on semantic tagging based on a domain-specific ontology, a dictionary-thesaurus and the overlapping coefficient similarity measure[REF_CITE]to semanti-cally annotate words."
"We first describe the corpus (section 2), then the overall IE system (section 3)."
Next we explain the different components of the semantic tagger (section 4) and we present the preliminary results of our ex-periments (section 5).
Finally we give some direc-tions for future work (section 6).
"The corpus is a collection of 95 manually tran-scribed telephone conversations (about 39,000 words)."
"They are mostly informative dialogs, where two speakers (a caller C and an operator O) dis-cuss the conditions and circumstances related to a SAR mission."
"The conversations are either (1) incident reports, such as reporting missing per-sons or overdue boats, (2) SAR mission plans, such as requesting an SAR airplane or coast guard ships for a mission, or (3) debriefings, in which case the results of the SAR mission are com-municated."
They can also be a combination of the three kinds.
Figure 1 is an excerpt of such conversations.
"We can notice many disfluencies 1-O:Hi, it’s Mr. Joe Blue.[REF_CITE]such as repetitions (13-O: Ha, do, is there, is there ...) , omissions and interruptions (3-O: we’ve been, actu-ally had a ...)."
"And, there is about 3% of transcription errors such as flowing instead of blowing (11-O Figure 1)."
The underlined words are the relevant informa-tions that will be extracted to fill in the IE tem-plates.
"They are, for example, the incident, its lo-cation, SAR resources needed for the mission, the result of the SAR mission and weather conditions."
The information extraction system is a four stage process (Figure 2).
It begins with the extraction of words that could be candidates to the extraction (stage I).
"Then, the semantic tagger annotates the extracted words (stage II)."
"Next, given the context and the semantic tag a word is extracted or rejected (stage III)."
"Finally, the extracted words are used for the coreference resolution and to fill in IE tem-plates (stage IV)."
The knowledge sources used for the IE task are the SAR ontology and the Wordsmyth dictionary-thesaurus [Footnote_1] .
"In this section we describe the extraction of can-didates, the SAR ontology design and the topic seg-mentation which have already been implemented."
"We leave the description of the topic labeling, the selection of relevant words and the template genera-tion to future work."
"The semantic tagger, is detailed in section 4."
"Candidates considered in the semantic tagging pro-cess are noun phrases NP , proposition phrases PP , verb phrases VP , adjectives ADJ and adverbs ADV ."
To gather these candidates we used the Brill trans-formational tagger[REF_CITE]for the part-of-speech step and the CASS partial parser for the pars-ing step[REF_CITE].
"However, because of the disfluencies (repairs, substitutions and omissions) encountered in the conversations, many errors oc-curred when parsing large constructions."
"So, we re-duced the set of grammatical rules used by CASS to cover only minimal chunks and discard large con-structions such as VP → VX NP ?"
ADV * or noun phrases NP → NP CONJ NP .
The evaluation of the semantic tagging process shows that about 14.4% of the semantic annotation errors are partially due to part-of-speech and parsing errors.
Topic segmentation takes part to several stages in our IE system (Figure 2).
Dialogue-based IE sys-tems have to deal with scattered information and disfluencies.
"Question-answer pairs, widely used in dialogues, are examples where information is con-veyed through consecutive utterances."
"By divid-ing the dialog into topical segment, we want to en-sure the extraction of coherent and complete key an-swers."
"Besides, topic segmentation is a valuable pre-processing for coreference resolution, which is a dif-ficult task in IE."
"Hence, for the extraction of relevant candidates and the coreference resolution which is part of the template generation stage (Figure 2), we use topic segment as context instead of the utterance or a word window of arbitrary size."
"The topic segmentation system we developed is based on a multi-knowledge source modeled by a hidden Markov model. (N. Boufaden and al., 2001) showed that by using linguistic features modeled by a Hidden Markov Model, it is possible to detect about 67% of topics boundaries."
The SAR ontology is an important component of our IE system.
"We build it using domain related infor-mations such as airplane names, locations, organi-zations, detection means (radar search, div-ing), status of a SAR mission (completed, con-tinuing, planned), instance of maritime inci-dents (drifting, overdue) and weather condi-tions (wind, rain, fog)."
All these informations were gathered from SAR manuals provided by the National Search and Rescue Secretari[REF_CITE]and from a sample of conversations (10 conversations about 10% of the corpus) to enumer-ate the different status informations.
Our ontology was designed for two tasks of the semantic tagging: 1.
Annotate with the corresponding concept all the extracted words that are instances of the on-tology.
This task is achieved by the named con-cept extraction process (section 4.1). 2.
"For each word not in the ontology, generate a concept-based representation composed of similarity scores that provide information about the closeness of the word to the SAR domain."
This is achieved by the sense tagging process (section 4.2).
"In addition to SAR manuals and corpus, we used the IE templates given by the DREV for the de-sign of the ontology."
We used a combination of the top-down and bottom-up design approaches[REF_CITE].
"For the former, we used the templates to enumerate the questions to be cov-ered by the ontology and distinguish the major top level classes (Figure 4)."
"For the latter, we collected the named entities along with airplane names, ves-sel types, detection means, alert types and incidents."
The taxonomy is based on two hierarchical relations: the is-a relation and the part-of relation.
The is-a re-lation is used for the semantic tagging.
"Whereas, the part-of relation will be used in the template genera-tion process."
The overall ontology is composed of 31 concepts.
"In the is-a hierarchy, each concept is represented by a set of instances and their textual definitions."
For each instance we added a set of synonyms and simi-lar words and their textual definitions to increase the size of the SAR vocabulary which was found to be insufficient to make the sense tagging approach ef-fective.
All the synonyms and similar words along with their definitions are provided by the Wordsmyth dictionary-thesaurus.
Figure 3 is an example of Wordsmyth entries.
Only textual definitions that fit the SAR context were kept.
This procedure in-creases the ontology size from 480 for a total of 783 instances.
The purpose of the semantic tagging process is to an-notate words with domain-specific informations.
"In our case, domain-specific informations are the con-cepts of the SAR ontology."
We want to determine the concept C k which is semantically the most ap-propriate to annotate a word w.
"Hence, we look for C ∗ which has the highest similarity score for the word w as shown in equation 1."
"C ∗ = argmax sim(w, C k ) (1) C k"
"Basically, our approach is a two part process (fig-ure 2)."
The named concept extraction is similar to named entity extraction based on gazetteer[REF_CITE].
"However it is a more general task since it also recognizes entities such as, aircraft names, boat names and detection means."
It uses a finite state automaton and the SAR ontology to recognize the named concepts.
The sense tagging process generates a based-concept representation for each word which couldn’t be tagged by the named concept extraction process.
The concept-based representation is a vector of sim-ilarity scores that measures how close is a word to the SAR domain.
"As we mentioned before (section 1), the concept-based representation using similarity scores is a way to get around the problem of small-scale corpora."
"Because we assume that the closer a word is to an SAR concept, the more relevant it is, this process is a key element for the selection of rel-evant words (figure 2)."
"In the next two sections, we detail each component of the semantic tagger."
"This task, like the named entity extraction task, an-notates words that are not instances of the ontol-ogy."
"Basically, for every chunk, we look for the first match with an instance concept."
The match is based on the word and its part-of-speech.
"When a match succeeds, the semantic tag assigned is the concept of the instance matched."
The propagation of the se-mantic tag is done by a two level automaton.
The first level propagates the semantic tag of the head to the whole chunk.
The second level deals with cases where the first level automaton fails to recog-nize collocations which are instances of the ontol-ogy.
These cases occur when : • the syntactic parser fails to produce a correct parse.
This mainly happens when the part of speech tag isn’t correct because of disfluencies encountered in the utterance or because of tran-scription errors. • the grammatical coverage is insufficient to parse large constructions.
"Whenever one of these reasons occur, the second level automaton tries to match chunk collocations in-stead of individual chunks."
"For example, the chunk Rescue Coordination Centre which is an organization, is an example where the parser pro-duces two NP chunks ( NP 1:Rescue Coordina-tion and NP 2:Centre) instead of only one chunk."
"In this case, the first level automaton fails to recog-nize the organization."
"However, in the second level automaton, the collocation NP 1 NP 2 is considered for matching with an instance of the concept organi-zation."
Figure 5 shows two output examples of the named concept extraction.
"Finally, if the automaton fails to tag a chunk, it assigns the tag OTHER if it’s an NP , OTHER - PROPERTIES if it’s a ADJ or ADV and OTHER - STATUS if it’s a VP ."
Sense tagging takes place when a chunk is not an instance of the ontology.
"In this case, the semantic tagger looks for the most appropriate concept to an-notate the chunk (equation 1)."
"However, a first step before annotation is to determine what word sense is intended in conversations."
Many studies[REF_CITE]tackle the sense tagging problem with approaches based on similar-ity measures.
Sense tagging is concerned with the selection of the right word sense over all the pos-sible word senses given some context or a particu-lar domain.
"Our assumption is that when conversa-tions are domain-specific, relevant words are too."
It means that sense tagging comes back to the prob-lem of selecting the closer word sense with regard to the SAR ontology.
"This assumption is translated in equation 2. w ∗ = argmax 1 Σ w(l) N l all concepts ksim(w(l), k) (2)"
Where N l is the number of positive similarity scores of the w(l) similarity vector. w(l) is the word w given the word sense l. The closer word sense w ∗ is the highest mean computed from element of the w(l) similarity vector.
"In what follows, we explain how are generated the similarity vectors and the result of our experiments."
A similarity vector is a vector where each element is a similarity score between a word(l) (the word w given the sense word l) and a concept C k from the SAR ontology.
The similarity score is based on the overlap coefficient similarity measure[REF_CITE].
This measure counts the number of lemmatized content words in common between the textual definition of the word and the concept.
"It is defined as : | D w(l) | ∩ | D C k | (3) sim(w(l), C k ) = min(| D w(l) |, | D C |) k where D w(l) and D C k are the sets of lemmatized content words extracted from the textual definitions of w(l) and C k ."
The textual definitions are provided by the Wordsmyth thesaurus-dictionary.
"However, since we have represented each concept by a set of instances and their synonyms in the SAR ontology (section 3.3), we modified the similarity measure to take into account the textual definition of concept instances and their synonyms."
"Basically, we compute the similarity score between w(l) and each synonym S i of a concept instance I j ."
"Then, the similarity score between w(l) and the instance concept I j is the median of the resulting similarity vector representing the similarity scores over all the synonyms."
"Finally, the similarity score between a concept C k and w(l) is the highest similarity score over all the concept instances."
The algorithm de-scribing these steps is given in Figure 6.
The evaluation of the semantic tagging process was done on 521 extracted chunks (about 10 conversa-tions).
Only relevant chunks where considered for the evaluation.
The evaluation criteria is an assess-ment about the appropriateness of the selected con-cept to annotate the word.
"For example, the concept time is appropriate for the word first light, whereas the concept incident is not for the word detachment which is closer to the search unit concept."
Table 2 shows the recall and precision scores for each component and for the overall semantic tagger.
The third column shows the input error rates for each component.
"The error rate in the first row comprises error rates of the part-of-speech tagger, the parsing and the manual transcription."
The error rate in the second row are mostly part-of-speech errors.
"In spite of the significant error rate, the approach based on partial parsing is effective."
The use of a minimal grammar coverage to produce chunks reduced con-siderably the parsing error rate.
"As far as we know, no previous published work on domain-specific WSD for speech transcriptions has been presented, although, word sense disam-biguation is an active research field as demonstrated by SENSEVAL competitions [Footnote_2] ."
Hence it is diffi-cult to compare our results to similar experiments.
"However, some comparative studies[REF_CITE]on domain-specific well-written texts show results ranging from 51,25% to 73,90%."
"Given the fact that our corpus is composed of speech transcriptions with the effect of increasing parsing errors, we con-sider our results to be very encouraging."
"Finally, results reported in Table 2 should be re-garded as a basis for further improvement."
"In partic-ular, the selection criteria in the sense tagging pro-cess could be improved by considering other mea-sures than the mean of all similarity scores as shown in equation 2."
Extraction of relevant words is a hub for several ap-plications such as question-answering and summa-rization.
It is based on semantically tagging words and selecting the most relevant ones given the con-text.
"In this paper, we developed a semantic tag-ging approach that uses a domain-specific ontology, a dictionary-thesaurus and the overlapping coeffi- cient similarity measure to annotate words."
We have shown how the use of concepts to represent words can alleviate the problem of small-scale corpora for the selection of relevant words.
The next step in our project is the selection of rel-evant words given the concepts annotating them and the topic segments where they appear.
Selection will be based on a combination of a probabilistic model taking into account the probability of observing a concept given a word and the probability of observ-ing that concept given a relevant topic.
"We investigate Global Index Gram-mars (GIGs), a grammar formalism that uses a stack of indices associated with productions and has restricted context-sensitive power."
We discuss some of the structural descriptions that GIGs can generate compared with those generated by LIGs.
We show also how GIGs can represent structural descriptions corresponding to HPSGs[REF_CITE]schemas.
The notion of Mildly context-sensitivity was in-troduced[REF_CITE]as a possible model to express the required properties of formalisms that might describe Natural Language (NL) phenomena.
"It requires three properties: [Footnote_1] a) constant growth property (or the stronger semi-linearity property); b) polynomial parsability; c) limited cross-serial dependencies, i.e. some limited context-sensitivity."
"1 See for example,[REF_CITE],[REF_CITE]."
"The canonical NL problems which exceed context free power are: multiple agreements, reduplication, crossing de-pendencies. [Footnote_2]"
"2 However other phenomena (e.g. scrambling, Geor-gian Case and Chinese numbers) might be considered to be beyond certain mildly context-sensitive formalisms."
Mildly Context-sensitive Languages (MCSLs) have been characterized by a geometric hierar-chy of grammar levels.
A level-2 MCSL (eg.
TALs/LILs) is able to capture up to 4 counting dependencies (includes L 4 = {a n b n c n d n |n ≥ [Footnote_1]} but not L 5 = {a n b n c n d n e n |n ≥ 1}).
"1 See for example,[REF_CITE],[REF_CITE]."
They were proven to have recognition algorithms with time complexity O(n 6 )[REF_CITE].
In general for a level-k MCSL the recognition problem is in O(n 3·2 k−1 ) and the descriptive power regard-ing counting dependencies is 2 k[REF_CITE].
"Even the descriptive power of level-2 MCSLs (Tree Adjoining Grammars (TAGs), Linear In-dexed Grammars (LIGs), Combinatory Catego-rial Grammars (CCGs) might be considered in-sufficient for some NL problems, therefore there have been many proposals [Footnote_3] to extend or modify them."
"3 There are extensions or modifications of TAGs, CCGs, IGs, and many other proposals that would be impossible to mention here."
On our view the possibility of modeling coordination phenomena is probably the most crucial in this respect.
In[REF_CITE]we introduced Global In-dex Grammars (GIGs) - and GILs the corre-sponding languages - as an alternative grammar formalism that has a restricted context sensitive power.
"We showed that GIGs have enough de-scriptive power to capture the three phenomena mentioned above (reduplication, multiple agree-ments, crossed agreements) in their generalized forms."
Recognition of the language generated by a GIG is in bounded polynomial time: O(n 6 ).
We presented a Chomsky-Schützenberger repre-sentation theorem for GILs.
In[REF_CITE]we presented the equivalent automaton model: LR-2PDA and provided a characterization the- orems of GILs in terms of the LR-2PDA and GIGs.
The family of GILs is an Abstract Fam-ily of Language.
The goal of this paper is to show the relevance of GIGs for NL modeling and processing.
This should not be understood as claim to propose GIGs as a grammar model with “linguistic con-tent” that competes with grammar models such as HPSG or LFG.
"It should be rather seen as a formal language resource which can be used to model and process NL phenomena beyond context free, or beyond the level-2 MCSLs (like those mentioned above) or to compile grammars created in other framework into GIGs."
"LIGs played a similar role to model the treatment of the SLASH feature in GPSGs and HPSGs, and to compile TAGs for parsing."
"GIGs offer addi-tional descriptive power as compared to LIGs or TAGs regarding the canonical NL problems mentioned above, and the same computational cost in terms of asymptotic complexity."
"They also offer additional descriptive power in terms of the structural descriptions they can generate for the same set of string languages, being able to produce dependent paths. 4"
This paper is organized as follows: section 2 reviews Global Index Grammars and their prop-erties and we give examples of its weak descrip-tive power.
Section 3 discusses the relevance of the strong descriptive power of GIGs.
"We discuss the structural description for the palin-drome, copy and the multiple copies languages {ww + |w ∈ Σ ∗ }."
Finally in section 4 we discuss how this descriptive power can be used to en-code HPSGs schemata.
"Indexed grammars, (IGs)[REF_CITE], and Linear Index Grammars, (LIGs;LILs)[REF_CITE], have the capability to associate stacks of indices with symbols in the grammar rules."
IGs are not semilinear.
LIGs are Indexed Grammars with an additional constraint in the form of the productions: the stack of indices can be “trans- mitted” only to one non-terminal.
As a con-sequence they are semilinear and belong to the class of MCSGs.
The class of LILs contains L 4 but not L 5 (see above).
"A Linear Indexed Grammar is a [Footnote_5]-tuple (V,T,I,P,S), where V is the set of variables, T the set of terminals, I the set of indices, S in V is the start symbol, and P is a finite set of productions of the form, where A,B ∈ V, α, γ ∈ (V ∪ T ) ∗ , i ∈"
"5 The notation in the rules makes explicit that oper-ation on the stack is associated to the production and neither to terminals nor to non-terminals. It also makes explicit that the operations are associated to the com-putation of a Dyck language (using such notation as used in e.g.[REF_CITE]). In another notation: a.1 [y..]A → [y..]α, a.2 [y..]A → [y..]α, b. [..]A → [x..]a β and c. [x..]A → [..]α"
I: a. A [..] → α B[..] γ b. A[i..] → α B[..] γ c. A [..] → αB[i..] γ
"Example 1 L(G wcw ) = {wcw |w ∈ {a, b} ∗ }, G ww = ({S, R}, {a, b}, {i, j}, S, P) and P is: 1.S [..] → aS[i..] 2.S [..] → bS[j..] 3.S [..] → cR[..] [Footnote_4].R[i..] → R[..]a 5.R[j..] → R[..]b 6. R [] → ²"
4 For the notion of dependent paths see for instance[REF_CITE]or[REF_CITE].
GIGs use the stack of indices as a global con-trol structure.
This formalism provides a global but restricted context that can be updated at any local point in the derivation.
GIGs are a kind of regulated rewriting mechanisms[REF_CITE]with global context and his-tory of the derivation (or ordered derivation) as the main characteristics of its regulating device.
The introduction of indices in the derivation is restricted to rules that have terminals in the right-hand side.
An additional constraint that is imposed on GIGs is strict leftmost derivation whenever indices are introduced or removed by the derivation.
"Definition 1 A GIG is a 6-tuple G = (N, T, I, S, #, P) where N, T, I are finite pair-wise disjoint sets and 1) N are non-terminals 2) T are terminals 3) I a set of stack indices 4) S ∈ N is the start symbol 5) # is the start stack symbol (not in I,N,T) and 6) P is a finite set of productions, having the following form, 5 where x ∈ I, y ∈ {I ∪ #}, A ∈ N, α, β ∈ (N ∪ T ) ∗ and a ∈ T . a.i"
A → α (epsilon) ² a.ii A → α (epsilon with constraints) [y] b.
A → a β (push) x c. A → α a β (pop) x̄
Note the difference between push (type b) and pop rules (type c): push rules require the right-hand side of the rule to contain a terminal in the first position.
Pop rules do not require a termi-nal at all.
That constraint on push rules is a crucial property of GIGs.
Derivations in a GIG are similar to those in a CFG except that it is possible to modify a string of indices.
"We de-fine the derives relation ⇒ on sentential forms, which are strings in I ∗ #(N ∪T ) ∗ as follows."
"Let β and γ be in (N ∪ T) ∗ , δ be in I ∗ , x in I, w be in T ∗ and X i in (N ∪ T). 1."
"If A → X 1 ...X n is a production of type (a.) µ (i.e. µ = ² or µ = [x], x ∈ I) then: i. δ#βAγ ⇒ δ#βX 1 ...X n γ µ ii. xδ#βAγ ⇒ xδ#βX 1 ...X n γ µ 2."
"If A → aX 1 ...X n is a production of type µ (b.) or push: µ = x, x ∈ I, then: δ#wAγ ⇒ xδ#waX 1 ...X n γ µ 3."
"If A → X 1 ...X n is a production of type (c.) µ or pop : µ = x̄, x ∈ I, then: xδ#wAγ ⇒ δ#wX 1 ......X n γ µ"
"The reflexive and transitive closure of ⇒ is denoted, as usual by ⇒ ∗ ."
"We define the language of a GIG, G, L(G) to be: {w|#S ⇒ ∗ #w and w is in T ∗ }"
"The main difference between, IGs, LIGs and GIGs, corresponds to the interpretation of the derives relation relative to the behavior of the stack of indices."
In IGs the stacks of indices are distributed over the non-terminals of the right-hand side of the rule.
"In LIGs, indices are asso-ciated with only one non-terminal at right-hand side of the rule."
"This produces the effect that there is only one stack affected at each deriva-tion step, with the consequence of the semilin-earity property of LILs."
GIGs share this unique-ness of the stack with LIGs: there is only one stack to be considered.
Unlike LIGs and IGs the stack of indices is independent of non-terminals in the GIG case.
GIGs can have rules where the right-hand side of the rule is composed only of terminals and affect the stack of indices.
Indeed push rules (type b) are constrained to start the right-hand side with a terminal as specified in (6.b) in the GIG definition.
The derives def-inition requires a leftmost derivation for those rules ( push and pop rules) that affect the stack of indices.
The constraint imposed on the push productions can be seen as constraining the con-text sensitive dependencies to the introduction of lexical information.
This constraint prevents GIGs from being equivalent to a Turing Machine as is shown[REF_CITE].
"The following example shows that GILs con-tain a language not contained in LILs, nor in the family of MCSLs."
This language is relevant for modeling coordination in NL.
Example 2 (Multiple Copies) .
"L(G wwn ) = {ww + | w ∈ {a, b} ∗ } G wwn = ({S, R, A, B, C, L}, {a, b}, {i, j}, S, #, P ) and where P is: S →"
AS | BS | C C → RC | L
R → RA R → RB R → ² ī [#]j̄ A → a B → b L → La | a L → Lb | b i j ī j̄
The next example shows the MIX (or Bach) language.[REF_CITE]conjectured the MIX language is not an IL.
"GILs are semilinear,[REF_CITE]therefore ILs and GILs could be incomparable under set inclusion."
"Example 3 (MIX language) .L(G mix ) = {w|w ∈ {a, b, c} ∗ and |a| w = |b| w = |c| w ≥ 1} G mix = ({S, D, F, L}, {a, b, c}, {i, j, k, l, m, n}, S, #, P) where P is:"
S → FS | DS | LS | ² F → c F → b F → a i j k D → aSb | bSa D → aSc | cSa D → bSc | cSb ī j̄ k̄
D → aSb | bSa D → aSc | cSa D → bSc | cSb l m n L → c L → b L → a l̄ m̄ n̄
The following example shows that the family of GILs contains languages which do not belong to the MCSL family.
"Example 4 (Multiple dependencies) L(G gdp ) = { a n (b n c n ) + | n ≥ 1}, G gdp = ({S, A, R, E, O, L}, {a, b, c}, {i}, S, #, P ) and P is:"
S → AR A → aAE A → a E → b i
R → bL L → OR | C C → cC | c i ī
O → c OE | c ī
The derivation of the string aabbccbbcc shows five dependencies. #S ⇒ #AR ⇒ #aAER ⇒ #aaER ⇒ i#aabR ⇒ ii#aabbL ⇒ ii#aabbOR ⇒ i#aabbcOER ⇒ #aabbccER ⇒ i#aabbccbR ⇒ ii#aabbccbbL ⇒ ii#aabbccbbC ⇒ i#aabbccbbcC ⇒ #aabbccbbcc
The recognition algorithm for GILs we presented[REF_CITE]is an extension of Earley’s al-gorithm (cf.[REF_CITE]) for CFLs.
It has to be modified to perform the computations of the stack of indices in a GIG.
In[REF_CITE]a graph-structured stack[REF_CITE]was used to efficiently represent ambiguous index opera-tions in a GIG stack.
"Earley items are modified adding three parameters δ, c, o: [δ, c, o, A → α • Aβ, i, j]"
The first two represent a pointer to an active node in the graph-structured stack ( δ ∈
I and c ≤ n).
The third parameter (o ≤ n) is used to record the ordering of the rules affecting the stack.
The O(n 6 ) time-complexity of this algorithm reported[REF_CITE]can be easily ver-ified.
The complete operation is typically the costly one in an Earley type algorithm.
"It can be verified that there are at most n [Footnote_6] instances of the indices (c 1 , c 2 , o, i, k, j) involved in this oper-ation."
6 Unambiguous indexing should be understood as those grammars that produce for each string in the lan-guage a unique indexing derivation.
"The counter parameters c 1 and c 2 , might be state bound, even for grammars with ambigu-ous indexing."
In such cases the time complex-ity would be determined by the CFG backbone properties.
The computation of the operations on the graph-structured stack of indices are per-formed at a constant time where the constant is determined by the size of the index vocabulary.
O(n 6 ) is the worst case; O(n 3 ) holds for gram-mars with state-bound indexing (which includes unambiguous indexing) 6 ; O(n 2 ) holds for unam-biguous context free back-bone grammars with state-bound indexing and O(n) for bounded-state [Footnote_7] context free back-bone grammars with state-bound indexing.
7 Context Free grammars where the set of items in each state set is bounded by a constant.
This discussion is ad-dressed not in terms of weak generative capac-ity but in terms of strong-generative capacity.
Similar approaches are also presented[REF_CITE]and[REF_CITE](see[REF_CITE]concerning weak and strong gen-erative capacity).
In this section we review some of the abstract configurations that are argued for[REF_CITE].
CFGs can recognize the language {ww R |w ∈ Σ ∗ } but they cannot generate the structural de-scription depicted in figure 1 (we follow Gazdar’s notation: the leftmost element within the brack-ets corresponds to the top of the stack):
Gazdar suggests that such configuration would be necessary to represent Scandinavian unbounded dependencies.
Such an structure can be obtained using a GIG (and of course a LIG).
But the mirror image of that structure can-not be generated by a GIG because it would require to allow push productions with a non terminal in the first position of the right-hand side.
"However the English adjective construc-tions that Gazdar argues that can motivate the LIG derivation, can be obtained with the follow-ing GIG productions as shown in figure 2."
It should be noted that the operations on indices follow the reverse order as in the LIG case.
"On the other hand, it can be noticed also that the introduction of indices is dependent on the pres-ence of lexical information and its transmission is not carried through a top-down spine, as in the LIG or TAG cases."
The arrows show the leftmost derivation order that is required by the operations on the stack.
Gazdar presents two possible LIG structural de-scriptions for the copy language.
Similar struc-tural descriptions can be obtained using GIGs.
However he argues that another tree structure could be more appropriate for some Natural Language phenomenon that might be modeled with a copy language.
"Such structure cannot be generated by a LIG, and can by an IG (see[REF_CITE]for a complete discussion and comparasion of GIG and LIG generated trees)."
"GIGs cannot produce this structural descrip-tion, but they can generate the one presented in figure 3, where the arrows depict the leftmost derivation order."
"GIGs can also produce similar structural descriptions for the language of mul-tiple copies (the language {ww + | w ∈ Σ ∗ } as shown in figure 4, corresponding to the gram-mar shown in example 2."
"We showed in the last section how GIGs can produce structural descriptions similar to those of LIGs, and others which are beyond LIGs and TAGs descriptive power."
Those structural de-scriptions corresponding to figure 1 were corre-lated to the use of the SLASH feature in GPSGs and HPSGs.
"In this section we will show how the structural description power of GIGs, is not only able to capture those phenomena but also additional structural descriptions, compatible with those generated by HPSGs."
This follows from the ability of GIGs to capture dependen-cies through different paths in the derivation.
"There has been some work compiling HPSGs into TAGs (cf.[REF_CITE],[REF_CITE])."
"One of the motivations was the potential to improve the processing efficiency of HPSG, performing HPSG deriva-tions at compile time."
Such compilation process allowed to identify significant parts of HPSG grammars that were mildly context-sensitive.
We will introduce informally some slight mod-ifications to the operations on the stacks per-formed by a GIG.
We will allow the productions of a GIG to be annotated with finite strings in I ∪ I¯ instead of single symbols.
This does not change the power of the formalism.
It is a standard change in PDAs (cf.[REF_CITE]) to allow to push/pop several symbols from the stack.
Also the symbols will be interpreted rel-ative to the elements in the top of the stack (as a Dyck set).
Therefore different derivations might be produced using the same production according to what are the topmost elements of the stack.
"This is exemplified with the produc-tions X → x and X → x, in particular in the n̄v [n]v first three cases where different actions are taken (the actions are explained in the parenthesis) : nnδ#wXβ ⇒ vnδ#wxβ (pop n and push v) n̄v nv̄δ#wXβ ⇒ δ#wxβ (pop n and v̄) n̄v vnδ#wXβ ⇒ vn̄vnδ#wxβ (push n̄ and v) n̄v nδ#wXβ ⇒ vnδ#wxβ ( check and push) [n]v"
"We exemplify how GIGs can generate similar structural descriptions as HPSGs do, in a very oversimplified and abstract way."
"We will ignore many details and try give an rough idea on how the transmission of features can be carried out from the lexical items by the GIG stack, obtain-ing very similar structural descriptions."
We presented GIGs and GILs and showed the descriptive power of GIGs is beyond CFGs.
CFLs are properly included in GILs by def-inition.
We showed also that GIGs include some languages that are not in the LIL/TAL family.
GILs do include those languages that are beyond context free and might be required for NL modelling.
"The similarity between GIGs and LIGs, suggests that LILs might be included in GILs."
"We presented a succinct comparison of the structural descriptions that can be gen-erated both by LIGs and GIGs, we have shown that GIGs generate structural descriptions for the copy language which can not be generated by LIGs."
We showed also that this is the case for other languages that can be generated by both LIGs and GIGs.
This corresponds to the ability of GIGs to generate dependent paths without copying the stack.
"We have shown also that those non-local relationships that are usually encoded in HPSGs as feature transmission, can be encoded in GIGs using its stack, exploiting the ability of Global stacks to encode dependencies through dependent paths and not only through a spine."
This paper investigates the correlation be-tween acoustic confidence scores as re-turned by speech recognizers with recog-nition quality.
We report the results of two machine learning experiments that predict the word error rate of recognition hypothe-ses and the confidence error rate for indi-vidual words within them.
Acoustic confidence scores as computed by speech recognizers play an important role in the design of spoken dialog systems.
"Often, systems solely de-cide on the basis of an overall acoustic confidence score whether they should accept (consider correct), clarify (ask for confirmation), or reject (prompt for repeat/rephrase) the interpretation of an user utter-ance."
"This behavior is usually achieved by setting two fixed confidence thresholds: if the confidence score of an utterance is above the upper threshold it is accepted, when it is below the lower threshold it is rejected, and clarification is initiated in case the con-fidence score lies in between the two thresholds."
The GoDiS spoken dialog system[REF_CITE]is an example of such a system.
More elabo-rated and flexible system behavior can be achieved by making use of individual word confidence scores or slot-confidences [Footnote_1] that allow more fine-grained de- cisions as to which parts of an utterance are not suf-ficiently well understood.
1 Some recognition platforms allow the application program-mer to associate semantic slot values with certain words of an input utterance. The slot-confi dence is then defi ned as the acoustic confi dence for the words that make up this slot.
The aim of this paper is to investigate how well acoustic confidences correlate with recognition quality and to use machine learning (ML) techniques to improve this correlation.
"In particular, we will conduct two different experiments."
"First, we try to predict the word error rate (WER) of a recogni-tion result based on its overall confidence score and show that we can improve on this by using ML clas-sifiers."
"Second, we will consider individual word confidence scores and again show that ML tech-niques can be fruitfully applied to the task of decid-ing whether individual words were recognized cor-rectly or not."
The paper is organized as follows.
"In the next sec-tion, we explain the general experimental setup, in-troduce acoustic confidences, and explain how we labeled our data."
Sections 3 and 4 report on the ac-tual experiments.
Section 5 summarizes and con-cludes the paper.
We use the ATIS2 corpus[REF_CITE]as our speech data source.
The corpus contains approx. 15.000 utterances and has a vocabulary size of about 1.000 words.
"In order to get “real” recognition data, we trained and tested the commercial NUANCE8.0 [URL_CITE] recognition engine on the ATIS2 corpus."
To this end we first split the corpus into two distinct sets.
With the first set we trained a statistical language model (trigram) for the recognizer.
This model was then used to recognize the other set of utterances (using 1-best recognition).
"Finally, we split the set of rec-ognized utterances into three different sets."
"A train-ing set (75%), a test set (20%) and a development set (5%)."
The NUANCE recognizer returns an overall acous-tic confidence score for each recognition hypothe-sis as well as individual word confidence scores for each word in the hypothesis.
Acoustic confidences are computed in an additional step after the actual recognition process.
The aim is to estimate a nor-malized probability of a (sub-)sequence of words that can be interpreted as a predictor whether the se-quence was correctly recognized or not (see[REF_CITE]for a comparison of different confidence estimators).
Acoustic confidence scores are there-fore different from the unnormalized scores com-puted by the standard Viterbi decoding in HMM based recognition which selects the best hypothesis among competing alternatives.
We will use acoustic confidence scores to derive baseline values for the two experiments reported in Sections 3 and 4.
We first give a general overview of the performance of the NUANCE speech recognizer.
"Table 1 reports the overall word error rate (WER) in terms of inser-tions, deletions, and substitutions as computed by the recognition engine (but see the discussion on the Levenstein distance in the next paragraph)."
"Table 2 shows the absolute number and percent-ages of the sentences that where recognized cor-rectly (WER0), recognized with a WER between 1% and 50%[REF_CITE], and with a WER greater than 50%[REF_CITE]."
Rejections and timeouts refer to the number of utterances completely rejected by the recognizer and utterances for which a process-ing timeout threshold was exceeded.
In both cases the recognizer did not return a hypothesis.
"In our first experiment we will use the three cate-gories WER0,[REF_CITE]and[REF_CITE]to establish a correlation between the overall acoustic confidence score for an utterance and its word error rate."
"The basic idea is that these three classes might be used by a system to decide whether it should accept, clar-ify, or reject an hypothesis."
We also labeled each word in the set of recognized utterances as either correctly or incorrectly recog-nized.
The labeling is based on the Levenstein dis-tance between the actual transcription of an utter-ance and its recognition hypothesis.
"The Leven-stein distance computes an alignment that minimizes the number of insertions, deletions, and substitutions when comparing two different sentences."
"However, this distance can be ambiguous between two or more alignment transcripts (i.e. there are can be several ways to convert one string into another using the minimum number of insertions, deletions, and sub-stitutions). (1) shows two possible alignments for a recognized utterance from the ATIS2 corpus, where ‘m’ stands for match, ‘i’ for insertion, and ‘s’ for substitution. (1) Ambiguous Levenstein alignment Trans: are there any stops on that flight Recog: what are the stops on the flight"
"To avoid this kind of ambiguity, we converted all words to their phoneme representations using the CMU pronunciation dictionary [Footnote_3] ."
We then ran the Levenstein distance algorithm on these repre-sentations and converted back the result to the word level.
This procedure gives us more intuitive align-ment results because it has a bias towards substi-tuting phonemically similar words (e.g. Align2 in (1) above).
"Of course, the Levenstein distance on the phoneme level can again be ambiguous but this is more unlikely since the to-be aligned strings are longer."
We will use the individually labeled words in our second experiment where we try to improve the con-fidence error rate and the detection-error tradeoff curve for the recognition results.
The purpose of the first experiment was to find out how well features that can be automatically derived from a recognition hypothesis can be used to predict its word error rate.
"As already mentioned in the previous section, all recognized sentences were assigned to one of the following classes depending on their actual WER:"
"WER0 (WER 0%, sentence correctly recognized),[REF_CITE](sentences with a WER between 1% and 50%), and[REF_CITE](sentences with a WER greater than 50%)."
"The motivation to split the data into these three classes was that they can be associated with the two fixed thresholds commonly used in spoken dia-log systems to decide whether an utterance should be accepted, clarified, or rejected."
We are aware that this might not be an optimal setting.
Some spoken dialog systems only spot for keywords or key-phrases in an utterance.
For them it does not matter whether “unimportant” words were recognized correctly or not and a WER greater than zero is often acceptable.
The main problem is that what counts as a keyword or key-phrase is system and domain depended.
"We cannot simply base our experiments on the WER for content words like nouns, verbs, and adjectives."
"In a travel agency application, for example, the prepositions ‘to’ and ‘from’ are quite important."
"In home automation, quantifiers/determiners are important to distinguish between the commands ‘switch off all lights’ and ‘switch off the hall lights’ (this example is borrowed from David Milward)."
For further examples see also[REF_CITE].
"We predicted the WER-class for recognized sen-tences based on their overall confidence score, and with the two machine learners TiMBL[REF_CITE]and Ripper[REF_CITE]."
"TiMBL is a software package that provides two different memory based learning algorithms, each with fine-tunable metrics."
All our TiMBL experiments were done with the IB1 algorithm that uses the k-nearest neighbor approach to classification: the class of a test item is derived from the training instances that are most similar to it.
Memory-based learning is often referred to as “lazy” learning because it ex-plicitly stores all training examples in memory with-out abstracting away from individual instances in the learning process.
"Ripper, on the other hand, implements a “greedy” learning algorithm that tries to find regularities in the training data."
It induces rule sets for each class with built-in heuristics to maximize accuracy and cover-age.
"With default settings, rules are first induced for low frequency classes, leaving the most frequent class being the default."
"We chose TiMBL and Rip-per as our two machine learners because they em-ploy different approaches to classification, are well-known, and widely available."
"For all experiments we proceeded as follows: First we used the training set to learn optimal con-fidence thresholds for the baseline classification and the development set to learn program parameters for the two machine learners, which were then trained on the training set."
We then tested these settings on the test set.
"To be able to statistically compare the results, in a third step, we used the learned program parameters to classify the recognition results in the combined training and test sets in a 10-fold cross-validation experiment."
The optimization and evalu-ation were always done on the weighted f 5 -score [Footnote_4]. for all three classes.
4 f 5 is the unbiased harmonic mean of precision (p) and re- . call (r): f 5 = 2pr/(p + r) .
As a baseline predictor for class assignment we use the overall confidence score of a recognition result returned by the NUANCE recognizer.
"To assign the three different classes, we have to learn two confi- dence thresholds."
"Whenever the overall confidence of the recognition result is below the lower thresh-old, we classify it[REF_CITE]whenever it is above the upper threshold we classify it as WER0, and when it is between we classify it[REF_CITE]."
We report the weighted f 5 -score for the test set and the . cross-validation experiment as well as the standard deviation for the cross-validation experiment in Ta-ble 3.
The confidence scores that maximized the results for the NUANCE recognizer on the test set were 66 and 43.
We computed a feature vector representation for each recognition result which served as input for the two machine learners TiMBL and Ripper.
These features can be grouped into the following seven different cate-gories. 1.
"Recognizer Confidences: Overall confidence score, max., min., and range of individual word confidences, descriptive statistics of the individual word confidences 2."
"Length of audio sample, number of words, syllables, and phonemes (CMU based) in recognition hypothesis 3. Tempo:"
"Length of audio sample divided by the number of words, phones, and syllables 4."
"Time needed for de-coding [Footnote_5]. Site Information: At which site the speech file was recorded 5 6. f0 Statistics: Mean and max. f0, variance, standard deviation, and number of unvoiced frames [Footnote_6] 7. RMS Statistics: Mean and max."
5 The ATIS2 data was recorded at several different sites.
6 The f0 and RMS (root mean square; a measure of the signal energy level) features were extracted with Entropic’s get f0 tool.
"RMS, vari-ance, standard deviation, number of frames with RMS &lt; 100"
Automatic classification of the recognition re-sults was done with different parameter and fea-ture settings for the machine learners.
We hereby coarsely followed[REF_CITE]who showed that parameter optimization and fea-ture selection techniques improved classification re-sults with TiMBL and Ripper for a variety of dif-ferent tasks.
"First, both learners were run with their default settings."
"Second, we optimized the param-eters for the two learners on the development set."
"Finally, we used a forward feature selection algo-rithm interleaved with parameter optimization for TiMBL."
"This algorithm starts out with zero features, adds one feature, and performs parameter optimiza-tion."
This is done for all features and the five best results are stored.
The algorithm then iterates and adds a second feature to these five best parameter settings.
"Again, parameter optimization is done for every possible feature combination."
The algorithm stops when there is no improvement for either of the five best candidates when adding an additional fea-ture.
Keeping the five best parameter settings en-sures that the feature selection is not too greedy.
"If, for example, a single feature gives good results but the combination with other features leads to a drop in performance, there is still a change that, say, the second or third best feature from the previous itera-tion combines well with a new feature and leads to better results."
"We report the results for TiMBL (Table 4) and Ripper (Table 5), respectively."
The results show that TiMBL profits from param-eter optimization and feature selection.
"One reason for this is that, with default settings, TiMBL only considers the nearest neighbor in deciding which class to assign to a test item."
"In our experiment, con-sidering more than one neighbor lead to a better f 5 - . score for the majority class (WER0) which in turn had an impact on overall weighted f 5 -score."
A sur- . prising finding is that the feature selection algorithm did not lead to an improvement.
We expected a bet-ter score based[REF_CITE]and because some aspects in the feature vector specifi-cation (e.g. tempo) are heavily correlated which can cause problems for memory based learners.
"How-ever, it turned out that our algorithm stopped after selecting only seven of the 27 features which indi-cates that it might still be too greedy."
"Another ex-planation for the results is that optimization with feature selection can be particularly prone to over-fitting: The weighted f 5 -score for the development . data, which we used to select features and optimize parameters, was 77.40% (almost 11% better than the performance on the test set)."
Parameter optimization did not improve the re-sults for Ripper.
Compared to TiMBL the smaller standard deviation in the cross-validation results in-dicates a more uniform/stable classification of the data.
We used related t-tests and Wilcoxon signed ranks statistics to compare the cross-validation results.
All test were done two-tailed at a significance level of p = .01.
We found that the results for TiMBL with default settings are significantly worse than all other results.
The other four machine learning re-sults (parameter optimization and feature selection for TiMBL as well as defaults and parameter op-timization for Ripper) significantly outperform the baseline.
We could not find a significant differ-ence between the TiMBL (excluding default set-tings) and Ripper results.
"In all comparisons, t-test and Wilcoxon signed ranks lead to the same results."
"During learning, Ripper generates a set of (human readable) decision rules that indicate which features were most important in the classification process."
"We cannot give a detailed analysis of the induced rules because of space constraints, but Table 6 pro-vides a simple breakdown by feature groups that shows how often features from each group appeared in the rule set. [Footnote_7]"
7 The fi gures reported in Table 6 were obtained by training Ripper on the training set with default parameters.[REF_CITE]classifi cation rules were generated.
We can see that all feature groups except “Site Information” contribute to the rule set.
"The single most often used feature was the mean of all individ-ual word confidences (9 times), followed by the min-imum individual word confidence and recognizer la-tency (both 8 times)."
The overall acoustic confi-dence score appeared in 4 rules only.
The aim of the second experiment was to investigate whether we can improve the confidence error rate (CER) for the recognized data.
The CER measures how good individual word confidence scores predict whether words are correctly recognized or not.
A confidence threshold is set according to which all words are either tagged as correct or incorrect.
CER is then simply defined as the number of in-correctly assigned tags divided by the total num-ber of recognized words.
The CER is a very sim-ple measure that strongly depends on the tagging threshold and the prior probability of the classes cor-rect and incorrect.
"Since we have a strong bias to-wards correct words in our data, we complement the CER evaluation with a second evaluation matrix, the detection-error tradeoff (DET) curve which plots the false acceptance rate (the number of incorrect words tagged as correct divided by the total number of in-correct words) over the false rejection rate (the num-ber of correct words tagged as incorrect divided by the total number of correct words)."
This curve is instructive because it shows the results for several different tagging thresholds and how they effect the prediction accuracy for the two classes.
The feature vector for the machine learners in the second experiment consisted of 17 features which were automatically derived from the recognition re-sults and the output of Experiment 1.
We can again group them into different categories. 1.
Overall Confidence: Overall confidence score of the hypothesis the to-be-classified word ap-pears in 2.
Left word context: The two word forms left of the to-be-classified word and their individual word confidence scores 3.
"Word: The to-be-classified word from, its individual word confidence, and two length measures 4."
Right word context: The two word forms left of the to-be-classified word and their individ-ual word confidence scores 5.
WER estimate: The WER class as assigned to the sentence the to-be-classified word appears in based on the best results from Experiment 1 6.
Sentence Length: Three different length mea-sures for the recognition hypothesis the to-be-classified word appears in
We report the confidence error rates for the test set and cross-validation on the combined train and test sets in Table 7.
The machine learners were only run with their default settings.
"As in Experiment 1, we used related t-tests and Wilcoxon signed ranks statistics to compare the re-sults."
"Unfortunately, we could not find a significant improvement for the machine learners as compared to the baseline."
Both tests show that there is no sig-nificant difference between either of the three results for two tailed tests at p = .01.
"Note, however, that the CER is strongly dependent on the prior probabil-ities of the classes correct and incorrect."
"It is there-fore interesting to compare the performance on the minority class (incorrect) for the baseline, TiMBL, and Ripper."
"Table 8 shows precision, recall, and f 5 - . scores on the test set."
We can see that the baseline performs very poor on the minority class.
Indeed the optimal thresh-old computed during training was 15 which means that almost every word is tagged as correct.
This difference does not show up in the CER because it is “overshadowed” by the majority class.
The next paragraph will show the advantage of the machine learners when we give equal weight to both the ma-jority and minority classes.
We use the data from all words in the training and test sets to plot detection-error tradeoff curves.
To get the baseline DET curve (based on the individual word confidence computed by the NUANCE recog-nizer) we simply vary the tagging threshold between 100 and 0 and apply it to the data.
"A threshold of 50, for example, will classify all words with a con-fidence higher or equal than 50 as correct and all others as incorrect."
"The result is a gradual decline in the false rejection rate: When the threshold is 100, all instances will be tagged as incorrect, when it is 0, all instances will be tagged as correct."
We classified the same data with the machine learn-ers using several 5-fold cross-validation experi-ments.
One big obstacle with the machine learn-ers was that we wanted to force them to gradually produce more false acceptances and less false rejec-tions.
"Ripper provides a parameter to change the “loss ratio”, i.e the ratio of the cost of a false neg-ative to the cost of a false positive."
This is exactly what we want but we found that we cannot linearly vary this parameter in a way that gives us a smooth transition between false acceptances and false rejec-tions.
We solved this problem by conducting experi-ments were we changed the ratio of examples from the two classes within the training set.
This was done as follows.
During cross-validation we first set aside an equal number of examples from both classes from the training set.
"Depending on the ra-tio value, we then added a certain fraction of one of these two sets to the other set."
"For example, to get a 50/50 ratio, we simply combined the two sets; for a 75/25 ratio we took the first set and added to it 50% (randomly selected) items from the second set."
This procedure in itself does not ensure a smooth transi-tion from false rejections to false acceptances but it worked very well in practice.
Note that we basically only take out a certain number of elements from the cross-validation training set.
We do still test every data point since we do not change the ratio within the test sets.
Figures 1 and 2 show the DET curves for TiMBL and Ripper as compared to the baseline respectively.
The DET curve for TiMBL is almost identical to the baseline.
"The curve for Ripper, however, does im- prove over the baseline, especially for false accep-tance rates (FAR) up to 0.5."
This is an interesting finding because we are often interested in a good performance for the minority class without loosing too much accuracy on the majority class.
For spoken dialog systems it is of major importance to be “con-servative” and to spot most of the erroneous words in order to avoid misunderstandings.
But this is al-ways at the cost of inefficient and annoying dialogs where the system rejects too many utterances or asks too many clarification questions.
Figure 2 shows an improvement on the part of the curve where the FAR is low (i.e. where not many erroneous words are accepted).
For a[REF_CITE]% (i.e. only every fifth incorrect word is not detected as such)
Ripper improves the false rejection rate (FRR) by 10% as compared to the baseline.
"Figure 2 also shows an improvement in the equal error rate (the point where FAR and FFR are the same) from 28,5% to 25%."
"Again, we can investigate the rule sets generated by Ripper to find out which features were particularly useful for classification."
"In Table 9, we report a breakdown by feature groups for one of the cross-validation folds that lead to a FAR of about 20% and a FRR of about 30.5% (i.e. one of the data points that showed the highest improvement over the base-line)."
The rule set included 15 different rules.
Table 9 shows that features from all six feature groups were used for classification.
"The single most often used feature was the individual word confi-dence of the target word (used in all rules), followed by the word confidence of the immediately preceed-ing word (which appeared in 11 rules)."
Spotting erroneous utterances and words is a ma-jor task in spoken dialog systems.
Depending on the judgment of recognition quality important deci-sions are made as to how the dialog should proceed.
"In this paper, we reported on two experiments that show how machine learning techniques can be used to predict the quality of recognition hypotheses."
We both looked at hypotheses as a whole (in terms of their WER) and the individual words within them (in terms of the CER and DET curves).
We found that by using the machine learners TiMBL and Rip-per we can improve the results in both tasks as com-pared to predicting recognition quality solely on the basis of the acoustic confidence scores returned by the speech recognizer.
Future work aims in two directions.
First we want to try to further improve the results presented in this paper by using better optimization methods for the machine learners (e.g. cross-validation op-timization to avoid over-fitting on the development data).
Further improvement of the results might also be achieved by considering other features for pre-diction.
"For example, we can add the words in the recognition hypothesis as a set-valued feature when using Ripper."
We also want to do a more thor-ough investigation of the rule sets generated by Rip-per to find out which features were most important for classification.
A long-term goal is to combine the (acoustic) quality prediction with a notion of se-mantic plausibility in an actual dialog system.
"In particular, we want to use semantic plausibility to rescore/rerank N-best recognition hypotheses."
We want to thank NUANCE Inc. for making avail-able their recognition software for research pur-poses.
Multimodal dialogue systems allow users to input information in multiple modali-ties.
These systems can handle simultane-ous or sequential composite multimodal input.
"Different coordination schemes re-quire such systems to capture, collect and integrate user input in different modali-ties, and then respond to a joint interpreta-tion."
We performed a study to understand the variability of input in multimodal dia-logue systems and to evaluate methods to perform the collection of input informa-tion.
An enhancement in the form of in-corporation of a dynamic time window to a multimodal input fusion module was proposed in the study.
We found that the enhanced module provides superior tem-poral characteristics and robustness when compared to previous methods.
A number of multimodal dialogue systems are be-ing developed in the research community.
"A com-mon component in these systems is a multimodal input fusion (MMIF) module which performs the functions of collecting the user input supplied in different modalities, determining when the user has finished providing input, fusing the collected in-formation to create a joint interpretation and send-ing the joint interpretation to a dialogue manager for reasoning and further processing (Oviatt et. al., 2000)."
A general requirement of the MMIF module is to allow flexibility in the user input and to relax any restrictions on the use of available modalities except those imposed by the application itself.
"The flexibility and the multiple ways to coordinate multimodal inputs pose a problem in determining, within a short time period after the last input, that a user has completed his or her turn."
"A method, Dy-namic Time Windows, is proposed to address this issue."
"Dynamic Time Windows allows the use of any modality, in any order and time, with very lit-tle delay in determining the end of a user turn."
"When providing composite multimodal input, i.e. input that needs to be interpreted or combined to-gether for proper understanding, the user has flexi-bility in the timing of those multimodal inputs."
"Considering two inputs at a time, the user can input them either sequentially or simultaneously."
"A mul-timodal input may consist of more than two inputs, leading to a large number of composite schemes."
MMIF needs to deal with these complex schemes and determine a suitable time when it is most unlikely to receive any further input and indicate the end of a user turn.
The determination of the end of a user turn be-comes a problem because of the following two conflicting requirements: 1.
"For naturalness, the user should not be constrained by pre-defined interaction re-quirements, e.g. to speak within a specified time after touching the display."
"To allow this flexibility in the sequential interaction metaphor, the user can provide coordinated multimodal input anytime after providing input in some modality."
Also each modal-ity has a unique processing time require- ment due to differing resource needs and capture times e.g. spoken input takes longer compared with touch.
The MMIF needs to consider such delays before send-ing information to a dialogue manager (DM).
These requirements tend to increase the time to wait for further information from input modalities. 2.
Users would expect the system to respond as soon as they complete their input.
"Thus, the fusion module should take as little time as possible before sending the integrated information to the dialogue manager."
We developed a multimodal input fusion module to perform a user study.
The MMIF module is based on the model proposed[REF_CITE].
The MMIF receives semantic information in the form of typed feature structures[REF_CITE]from the individual modalities.
"It combines typed fea-ture structures received from different modalities during a complete turn using an extended unifica-tion algorithm (Gupta et. al., 2002)."
The output is a joint interpretation of the multimodal input that is sent to a DM that can perform reasoning and pro-vide with suitable system replies.
"Based on current approaches, the following meth-ods were chosen to perform an analysis to deter-mine a suitable method for predicting the end of a user turn: 1."
"In this method, after receiv-ing an input, the MMIF waits for a speci-fied time for further input."
"After 3 seconds, the collected input is integrated and sent to the DM."
This is similar to Johnston et. al. (2002) who uses a 1 second wait period. 2.
"Two Inputs - In this method, multimodal input is assumed to consist of two inputs from two modalities."
"After inputs from two modalities have been received, the in-tegration process is performed and the re-sult sent to the DM."
A window of 3 seconds is used after receiving the first in-put. (Oviatt et. al. 1997) 3.
Information evaluation -
"In this method in-tegration is performed after receiving each input, and the result is evaluated to deter- mine if the information can be transformed to a command that the system can under-stand."
"If transformation is possible, the work of MMIF is deemed complete and the information is sent to the DM."
"In the case of an incomplete transformation, a windowing technique is used."
This ap-proach is similar to that[REF_CITE].
"We used a multimodal in-car navigation system (Gupta et. al., 2002), developed using the MMIF module and a dialogue manager[REF_CITE]to perform this study."
Users can inter-act with a map-based display to get information on various locations and driving instructions.
"The in-teraction is performed using speech, handwriting, touch and gesture, either simultaneously or sequen-tially."
The system was set-up on a 650MHz com-puter with 256MB of RAM and a touch screen.
The subjects for the study were both male and fe-male in the age group of 25-35.
All the subjects were working in technical fields and had daily in-teraction with computer-based systems at work.
"Before using the system, each of the subjects was briefed about the tasks they needed to perform and given a demonstration of using the system."
"The tasks performed by the subjects were: • Dialogue with the system to specify a few different destinations, e.g. a gas station, a hotel, an address, etc. and • Issue commands to control the map display e.g. zoom to a certain area on the map."
"Some of the tasks could be completed both un-imodally or multimodally, while others required multiple inputs from the same modality, e.g. pro-viding multiple destinations using touch."
We asked the users to perform certain tasks in both unimodal and multimodal manner.
The users were free to choose their preferred mode of interaction for a particular task.
We observed users’ behavior dur-ing the interaction.
The subjects answered a few questions after every interaction on acceptability of the system response.
"If it was not acceptable, we asked for their preference."
The following observations were made during and after analysis of the user study based on aggregate results from using all the three methods of collect-ing multimodal input.
Several statistical measures were extracted from the data collected during the user study.
"Following an analysis of the above observations and measurements, we came to the following conclusions: • Multimodal input is segmented with the user making a conscious effort to provide syn-chronization between inputs in multiple mo-dalities."
The synchronization technique applied is unique to every user.
Multimodal input is likely to have a limited number of segments provided in different modalities. • Processing time can be a key element for MMIF when deploying multimodal interac-tive systems on devices with limited re-sources. • Knowledge of the availability of current modalities and the task at hand can improve the performance of MMIF.
"Based on the current task for which the user has provided input, different techniques should be applied to determine the end of user turn. • Users need to be made aware of the status of the MMIF and the modes available to them."
"A uniform interface design methodology should be used, allowing the availability of all the modalities during all times. • Timing between inputs in different modali-ties is critical to determine the exact rela-tionship between the referent and the referred."
"Based on the observations, a fine-grained classifi-cation of the temporal relationship between user inputs is proposed."
Temporal relationship is de-fined to be the way in which the modalities are used during interaction.
Figure 2 shows the various temporal relationships between feature structures that are received from the modalities.
"A, B, C, D, E, and F are all feature structures and their extent denotes the capture period."
These relationships will allow for a better prediction of when and which modality is likely to be used next by the user.
It was proposed to augment the MMIF component with a wait mechanism that collects information from input modalities and adaptively determines the time when no further input is expected.
The following factors were used during the design of the adaptive wait mechanism: 1.
If the modality is specialized (i.e. it is usu-ally used unimodally) then the likelihood of getting information in another modality is greatly reduced. 2.
If the modality usually occurs in combina-tion with other modalities then the likeli-hood of receiving information in another modality is increased. 3.
If the number of segments of information within a turn is more than two or three then the likelihood of receiving further in-formation from other modalities is re-duced. 4.
"If the duration of information in a certain modality is greater than usual, it is likely that the user has provided most of the in-formation in that modality in a unimodal manner."
"The enhanced method is the same as the informa-tion evaluation method except, that instead of the static time window, a dynamic time window based on current input and previous learning is used."
The enhanced module was tested using the data collected in previous tests and further online tests.
The average delay in determining the end of turn reduced to 1.3 secs.
This represents a 40% im-provement on the earlier results.
"Also based on online experiments, with the same users and tasks, the number of times users repeated their input was reduced to 2% and collection errors reduced to 3% (compared to 8% and 6% respectively)."
The im-provement was partly due to the reduced delay in the determination of the end of the user’s turn and also due to prediction of the preferred interaction style.
It was also observed that the performance increased by a further 5% by using online learning.
The results demonstrate the effectiveness of the proposed approach to the robustness and temporal performance of MMIF.
An MMIF module with Dynamic Time Widows applied to an adaptive wait mechanism that can learn from user’s interaction style improved the interactivity in a multimodal system.
"By predicting the end of a user turn, the proposed method in-creased the usability of the system by reducing errors and improving response time."
Future work will focus on user adaptation and on the user inter-face to make best use of MMIF.
We describe an algorithm for recover-ing non-local dependencies in syntac-tic dependency structures.
The pattern-matching approach proposed[REF_CITE]for a similar task for phrase structure trees is extended with machine learning techniques.
The algorithm is es-sentially a classifier that predicts a non-local dependency given a connected frag-ment of a dependency structure and a set of structural features for this frag-ment.
"Evaluating the algorithm on the Penn Treebank shows an improvement of both precision and recall, compared to the results presented[REF_CITE]."
"Non-local dependencies (also called long-distance, long-range or unbounded) appear in many fre-quent linguistic phenomena, such as passive, WH-movement, control and raising etc."
"Although much current research in natural language parsing focuses on extracting local syntactic relations from text, non-local dependencies have recently started to attract more attention."
"In[REF_CITE]long-range dependencies are included in parser’s probabilistic model, while[REF_CITE]presents a method for recovering non-local dependencies after parsing has been performed."
"More specifically,[REF_CITE]describes a pattern-matching algorithm for inserting empty nodes and identifying their antecedents in phrase structure trees or, to put it differently, for recover-ing non-local dependencies."
"From a training corpus with annotated empty nodes Johnson’s algorithm first extracts those local fragments of phrase trees which connect empty nodes with their antecedents, thus “licensing” corresponding non-local dependen-cies."
"Next, the extracted tree fragments are used as patterns to match against previously unseen phrase structure trees: when a pattern is matched, the algo-rithm introduces a corresponding non-local depen-dency, inserting an empty node and (possibly) co-indexing it with a suitable antecedent."
In[REF_CITE]the author notes that the biggest weakness of the algorithm seems to be that it fails to robustly distinguish co-indexed and free empty nodes and it is lexicalization that may be needed to solve this problem.
"Moreover, the author suggests that the algorithm may suffer from over-learning, and using more abstract “skeletal” patterns may be helpful to avoid this."
"In an attempt to overcome these problems we de-veloped a similar approach using dependency struc-tures rather than phrase structure trees, which, more-over, extends bare pattern matching with machine learning techniques."
A different definition of pat-tern allows us to significantly reduce the number of patterns extracted from the same corpus.
"More-over, the patterns we obtain are quite general and in most cases directly correspond to specific linguistic phenomena."
This helps us to understand what in-formation about syntactic structure is important for the recovery of non-local dependencies and in which cases lexicalization (or even semantic analysis) is required.
"On the other hand, using these simpli-fied patterns, we may loose some structural infor-mation important for recovery of non-local depen-dencies."
"To avoid this, we associate patterns with certain structural features and use statistical classifi- cation methods on top of pattern matching."
The evaluation of our algorithm on data automat-ically derived from the Penn Treebank shows an in-crease in both precision and recall in recovery of non-local dependencies by approximately 10% over the results reported[REF_CITE].
"However, additional work remains to be done for our algorithm to perform well on the output of a parser."
This section describes the corpus of dependency structures that we used to evaluate our algorithm.
"The corpus was automatically derived from the Penn Treebank II corpus[REF_CITE], by means of the script chunklink.pl[REF_CITE]that we modified to fit our purposes."
"The script uses a sort of head percolation table to identify heads of constituents, and then converts the result to a de-pendency format."
"We refer[REF_CITE]for a thorough description of the conversion algorithm, and will only emphasize the two most important modifications that we made."
One modification of the conversion algorithm concerns participles and reduced relative clauses modifying NPs.
Regular participles in the Penn Treebank II are simply annotated as VPs adjoined to the modified NPs (see Figure 1(a)).
"These par-ticiples (also called reduced relative clauses, as they lack auxiliary verbs and complementizers) are both syntactically and semantically similar to full rela-tive clauses, but the Penn annotation does not in- troduce empty complementizers, thus preventing co-indexing of a trace with any antecedent."
"We perform a simple heuristic modification while converting the Treebank to the dependency format: when we en-counter an NP modified by a VP headed by a past participle, an object dependency is introduced be-tween the head of the VP and the head of the NP."
"Figure 1(b) shows an example, with solid arrows de-noting local and dotted arrows denoting non-local dependencies."
Arrows are marked with dependency labels and go from dependents to heads.
"This simple heuristics does not allow us to handle all reduced relative clauses, because some of them correspond to PPs or NPs rather than VPs, but the latter are quite rare in the Treebank."
The second important change to Buchholz’ script concerns the structure of VPs.
"For every verb clus-ter, we choose the main verb as the head of the clus-ter, and leave modal and auxiliary verbs as depen-dents of the main verb."
A similar modification was used[REF_CITE]for the study of dependency parsing models.
"As will be described below, this al-lows us to “factor out” tense and modality of finite clauses from our patterns, making the patterns more general."
"After converting the Penn Treebank to a dependency treebank, we first extracted non-local dependency patterns."
"As[REF_CITE], our patterns are minimal connected fragments containing both nodes involved in a non-local dependency."
"However, in our case these fragments are not connected sets of local trees, but shortest paths in local dependency graphs, leading from heads to non-local dependents."
"Pat-terns do not include POS tags of the involved words, but only labels of the dependencies."
"Thus, a pat-tern is a directed graph with labeled edges, and two distinguished nodes: the head and the dependent of a corresponding non-local dependency."
"When sev-eral patterns intersect, as may be the case, for exam-ple, when a word participates in more than one non-local dependency, these patterns are handled inde-pendently."
"Figure 2 shows examples of dependency graphs (above) and extracted patterns (below, with filled bullets corresponding to the nodes of a non-local dependency)."
"As before, dotted lines denote non-local dependencies."
"The definition of a structure matching a pattern, and the algorithms for pattern matching and pat-tern extraction from a corpus are straightforward and similar to those described[REF_CITE]."
The total number of non-local dependencies found in the Penn[REF_CITE]5.
The number of different extracted patterns is 987.
"These patterns were further cleaned up manually, e.g., most Penn func-tional tags (-TMP, -CLR etc., but not -OBJ, -SBJ, -PRD) were removed."
"Thus, we ended up with 16 structural patterns (covering the same 93,7% of the Penn Treebank)."
Table 1 shows some of the patterns found in the Penn Treebank.
The column Count gives the number of times a pattern introduces non-local dependen-cies in the corpus.
The Match column is the num-ber of times a pattern actually occurs in the corpus (whether it introduces a non-local dependency or not).
The patterns are shown as dependency graphs with labeled arrows from dependents to heads.
The column Dependency shows labels and directions of introduced non-local dependencies.
"Clearly, an occurrence of a pattern alone is not enough for inserting a non-local dependency and de-termining its label, as for many patterns Match is significantly greater than Count."
"For this reason we introduce a set of other structural features, associ-ated with patterns."
"For every occurrence of a pattern and for every word of this occurrence, we extract the following features: pos, the POS tag of the word; class, the simplified word class (similar[REF_CITE]); fin, whether the word is a verb and a head of a finite verb cluster (as opposed to infinitives, gerunds or participles); subj, whether the word has a dependent (prob-ably not included in the pattern) with a depen-dency label NP-SBJ; and obj, the same for NP-OBJ label."
"Thus, an occurrence of a pattern is associated with a sequence of symbolic features: five features for each node in the pattern."
"E.g., a pattern consisting of"
"Given a pattern instance and its feature vector, our task now is to determine whether the pattern intro-duces a non-local dependency and, if so, what the label of this dependency is."
"In many cases this is not a binary decision, since one pattern may introduce several possible labeled dependencies (e.g., the pat-tern  S in Table 1)."
"Our task is a classification task: an instance of a pattern must be assigned to two or more classes, corresponding to several possi-ble dependency labels (or absence of a dependency)."
"We train a classifier on instances extracted from a corpus, and then apply it to previously unseen in-stances."
"The procedure for finding non-local dependencies now consists of the two steps: 1. given a local dependency structure, find match-ing patterns and their feature vectors; 2. for each pattern instance found, use the clas-sifier to identify a possible non-local depen-dency."
In our experiments we used sections 02-22 of the Penn Treebank as the training corpus and section 23 as the test corpus.
"First, we extracted all non-local patterns from the Penn Treebank, which resulted in 987 different (pattern, non-local dependency) pairs."
"As described in Section 3, after cleaning up we took 16 of the most common patterns."
"For each of these 16 patterns, instances of the pat-tern, pattern features, and a non-local dependency label (or the special label “no” if no dependency was introduced by the instance) were extracted from the training and test corpora."
We performed experiments with two statistical classifiers: the decision tree induction system C4.5[REF_CITE]and the Tilburg Memory-Based Learner (TiMBL)[REF_CITE].
In most cases TiBML performed slightly better.
The re-sults described in this section were obtained using TiMBL.
"For each of the 16 structural patterns, a separate classifier was trained on the set of (feature-vector, label) pairs extracted from the training corpus, and then evaluated on the pairs from the test corpus."
"Ta-ble 1 shows the results for some of the most fre- quent patterns, using conventional metrics: preci-sion (the fraction of the correctly labeled dependen-cies among all the dependencies found), recall (the fraction of the correctly found dependencies among all the dependencies with a given label) and f-score (harmonic mean of precision and recall)."
The table also shows the number of times a pattern (together with a specific non-local dependency label) actually occurs in the whole Penn Treebank corpus (the col-umn Dependency count).
"In order to compare our results to the results pre-sented[REF_CITE], we measured the over-all performance of the algorithm across patterns and non-local dependency labels."
"This corresponds to the row “Overall” of Table 4[REF_CITE], re-peated here in Table 4."
"We also evaluated the pro-cedure on NP traces across all patterns, i.e., on non-local dependencies with NP-SBJ, NP-OBJ or NP-PRD labels."
"This corresponds to rows 2, 3 and 4 of Table 4[REF_CITE]."
Our results are pre-sented in Table 3.
"The first three columns show the results for those non-local dependencies that are ac-tually covered by our 16 patterns (i.e., for 93.7% of all non-local dependencies)."
"The last three columns present the evaluation with respect to all non-local dependencies, thus the precision is the same, but re-call drops accordingly."
These last columns give the results that can be compared to Johnson’s results for section 23 (Table 4).
It is difficult to make a strict comparison of our results and those[REF_CITE].
"The two algo-rithms are designed for slightly different purposes: while Johnson’s approach allows one to recover free empty nodes (without antecedents) , we look for non-local dependencies, which corresponds to identifica-tion of co-indexed empty nodes (note, however, the modifications we describe in Section 2, when we ac-tually transform free empty nodes into co-indexed empty nodes)."
"The results presented in the previous section show that it is possible to improve over the simple pattern matching algorithm[REF_CITE], using de-pendency rather than phrase structure information, more skeletal patterns, as was suggested by John-son, and a set of features associated with instances of patterns."
One of the reasons for this improvement is that our approach allows us to discriminate between dif-ferent syntactic phenomena involving non-local de-pendencies.
In most cases our patterns correspond to linguistic phenomena.
"That helps to understand why a particular construction is easy or difficult for our approach, and in many cases to make the nec-essary modifications to the algorithm (e.g., adding other features to instances of patterns)."
"For example, for patterns 11 and 12 (see Tables 1 and 2) our classi-fier distinguishes subject and object reasonably well, apparently, because the feature has a local object is explicitly present for all instances (for the examples 11 and 12 in Table 2, expand has a local object, but do doesn’t)."
"Another reason is that the patterns are general enough to factor out minor syntactic differences in linguistic phenomena (e.g., see example 4 in Ta-ble 2)."
"Indeed, the most frequent 16 patterns cover 93.7% of all non-local dependencies in the corpus."
"This is mainly due to our choices in the dependency representation, such as making the main verb a head of a verb phrase."
"During the conversion to a de-pendency treebank and extraction of patterns some important information may have been lost (e.g., the finiteness of a verb cluster, or presence of subject and object); for that reason we had to associate pat-terns with additional features, encoding this infor-mation and providing it to the classifier."
"In other words, we first take an “oversimplified” representa-tion of the data, and then try to find what other data features can be useful."
"This strategy appears to be successful, because it allows us to identify which in-formation is important for the recovery of non-local dependencies."
"More generally, the reasonable overall perfor-mance of the algorithm is due to the fact that for the most common non-local dependencies (extrac-tion in relative clauses and reduced relative clauses, passivization, control and raising) the structural in-formation we extract is enough to robustly identify non-local dependencies in a local dependency graph: the most frequent patterns in Table 1 are also those with best scores."
"However, many less frequent phe-nomena appear to be much harder."
"For example, per-formance for relative clauses with extracted objects or adverbs is much worse than for subject relative clauses (e.g., patterns 2 and 3 vs. 1 in Table 1)."
"Ap-parently, in most cases this is not due to the lack of training data, but because structural information alone is not enough and lexical preferences, subcat-egorization information, or even semantic properties should be considered."
We think that the aproach al-lows us to identify those “hard” cases.
The natural next step in evaluating our algorithm is to work with the output of a parser instead of the original local structures from the Penn Tree-bank.
"Obviously, because of parsing errors the per-formance drops significantly: e.g., in the experi-ments reported[REF_CITE]the overall f-score decreases from 0.75 to 0.68 when evaluating on parser output (see Table 4)."
"While experimenting with Collins’ parser[REF_CITE], we found that for our algorithm the accuracy drops even more dra-matically, when we train the classifier on Penn Tree-bank data and test it on parser output."
"One of the reasons is that, since we run our algorithm not on the parser’s output itself but on the output automat-ically converted to dependency structures, conver-sion errors also contribute to the performance drop."
"Moreover, the conversion script is highly tailored to the Penn Treebank annotation (with functional tags and empty nodes) and, when run on the parser’s out-put, produces structures with somewhat different de-pendency labels."
"Since our algorithm is sensitive to the exact labels of the dependencies, it suffers from these systematic errors."
"One possible solution to that problem could be to extract patterns and train the classification algorithm not on the training part of the Penn Treebank, but on the parser output for it."
This would allow us to train and test our algorithm on data of the same nature.
We have presented an algorithm for recovering long-distance dependencies in local dependency struc-tures.
"We extend the pattern matching approach[REF_CITE]with machine learning tech-niques, and use dependency structures instead of constituency trees."
Evaluation on the Penn Treebank shows an increase in accuracy.
"However, we do not have yet satisfactory results when working on a parser output."
"The conversion algorithm and the dependency labels we use are largely based on the Penn Treebank annotation, and it seems difficult to use them with the output of a parser."
"A parsing accuracy evaluation scheme based on grammatical relations (GR), presented[REF_CITE], provides a set of dependency labels (grammatical relations) and a manually annotated dependency corpus."
"Non-local dependencies are also annotated there, although no explicit difference is made between local and non-local dependencies."
"Since our classification algorithm does not depend on a particular set of dependency labels, we can also use the set of labels described by Briscoe et al, if we convert Penn Treebank to a GR-based dependency treebank and use it as the training corpus."
This will allow us to make the patterns independent of the Penn Treebank annotation details and simplify test-ing the algorithm with a parser’u output.
We will also be able to use the flexible and parameterizable scoring schemes discussed[REF_CITE].
"We also plan to develop the approach by using iteration of our non-local relations extraction algo-rithm, i.e., by running the algorithm, inserting the found non-local dependencies, running it again etc., until no new dependencies are found."
"While rais-ing an important and interesting issue of the order in which we examine our patterns, we believe that this will allow us to handle very long extraction chains, like the one in sentence “Aichi revised its tax calcu-lations after being challenged for allegedly failing to report...”, where Aichi is a (non-local) depen-dent of five verbs."
"Iteration of the algorithm will also help to increase the coverage (which is 93,7% with our 16 non-iterated patterns)."
This paper suggests the efficient indexing method based on a concept vector space that is capable of representing the semantic content of a document.
"The two informa-tion measure, namely the information quan-tity and the information ratio, are defined to represent the degree of the semantic im-portance within a document."
The proposed method is expected to compensate the lim-itations of term frequency based methods by exploiting related lexical items.
"Further-more, with information ratio, this approach is independent of document length."
"To improve the unstable performance of a traditional keyword-based search, a Web document should in-clude both an index and index weight that represent the semantic content of the document."
"However, most of the previous works on indexing and the weighting function, which depend on statistical methods, have limitations in extracting exact indexes[REF_CITE]."
The objective of this paper is to propose a method that extracts indexes efficiently and weights them accord-ing to their semantic importance degree in a document using concept vector space model.
A document is regarded as a conglomerate con-cept that comprises by many concepts.
"Hence, an n-dimensional concept vector space model is defined in such a way that a document is recognized as a vec-tor in n-dimensional concept space."
We used lexical chains for the extraction of concepts.
"With concept vectors and text vectors, semantic indexes and their semantic importance degree are computed."
"Further-more, proposed indexing method had an advantage in being independent of document length because we re-garded overall text information as a value 1 and repre-sented each index weight by the semantic information ratio of overall text information."
"Since index terms are not equally important regard-ing the content of the text, they have term weights as an indicator of importance."
Many weighting functions have been proposed and tested.
"However, most weight functions depend on the statistical methods or on the document’s term distribution tendency."
"Representa-tive weighting functions include such factors as term frequency, inverse document frequency, the product of the term and inverse document frequency, and length normalizati[REF_CITE]."
"Term frequency is useful in a long document, but not in a short document."
"In addition, term frequency cannot represent the exact term frequency because it does not include anaphoras, synonyms, and so on."
Inverse document frequency is inappropriate for a reference collection that changes frequently because the weight of an index term needs be recomputed.
"A length normalization method is proposed because term frequency factors are numerous for long docu-ments, and negligible for short ones, obscuring the real importance of terms."
"As this approach also uses term frequency function, it has the same disadvantage as term frequency does."
"Hence, we made an effort to use methods based on the linguistic phenomena to enhance the index-ing performance."
"Our approach focuses on proposing concept vector space for extracting and weighting in-dexes, and we intend to compensate limitations of the term frequency based methods by employing lexical chains."
"Lexical chains are to link related lexical items in a document, and to represent the lexical cohesion structure of a document[REF_CITE]."
Current approaches to index weighting for informa-tion retrieval are based on the statistic method.
We propose an approach that changes the basic index term weighting method by considering semantics and con-cepts of a document.
"In this approach, the concepts of a document are understood, and the semantic indexes and their weights are derived from those concepts."
We have developed a system that performs the index term weighting semantically based on concept vector space.
A schematic overview of the proposed system is as follows: A document is regarded as a complex concept that consists of various concepts; it is recog-nized as a vector in concept vector space.
"Then, each concept was extracted by lexical chains([REF_CITE]and 1991)."
Extracted concepts and lexical items were scored at the time of constructing lexical chains.
"Each scored chain was represented as a concept vector in concept vector space, and the overall text vector was made up of those concept vectors."
The semantic im-portance of concepts and words was normalized ac-cording to the overall text vector.
Indexes that include their semantic weight are then extracted.
The proposed system has four main components: • Lexical chains construction • Chains and nouns weighting • Term reweighting based on concept • Semantic index term extraction
"The former two components are based on concept extraction using lexical chains, and the latter two com-ponents are related with the index term extraction based on the concept vector space, which will be ex-plained in the next section."
"Lexical chains are employed to link related lexical items in a document, and to represent the lexical co-hesion structure in a document[REF_CITE]."
"In ac-cordance with the accepted view in linguistic works that lexical chains provide representation of discourse structures([REF_CITE]and 1991), we assume that each lexical chain is regarded as a concept that ex-presses the meaning of a document."
"Therefore, each concept was extracted by lexical chains."
"For example, Figure 1 shows a sample text com-posed of five chains."
"Since we can not deal all the concept of a document, we discriminate representative chains from lexical chains."
Representative chains are chains delegated to represent a representative concept of a document.
"A concept of the sample text is mainly composed of representative chains, such as chain 1, chain 2, and chain 3."
"Each chain represents each different representative concept: for example man, machine and anesthetic."
"As seen in Figure 1, a document consists of various concepts."
"These concepts represent the semantic con-tent of a document, and their composition generates a complex composition."
Therefore we suggest the con-cept space model where a document is represented by a complex of concepts.
"In the concept space model, lexical items are discriminated by the interpretation of concepts and words that constitute a document."
Lexical chains are employed for concept extraction.
Lexical chains are formed using WordNet and asso- ciated relations among words.
"Chains have four re-lations: synonym, hypernyms, hyponym, meronym."
The definitions on the score of each noun and chain are written as definition 2 and definition 3.
Definition 2 (Score of Noun) Let NR
Nk i denotes the number of relations that noun N i has with relation k. SR
Nk i represents the weight of relation k.
Then the score S NOUN (N i ) of a noun N i in a lexical chain is defined as:
X S NOUN (N i ) = (NR kN i × SR kN i ) (2) k where k ∈ set of relations.
Definition 3 (Score of Chain)
The score S CHAIN (Ch x ) of a chain Ch x is defined as:
X n S CHAIN (Ch x ) =
"S NOUN (N i ) + penalty (3) i=1 where S NOUN (N i ) is the score of noun N i , and N 1 , ..., N n ∈ Ch x ."
Representative chains are chains delegated to rep-resent concepts.
"If the number of the chains was m, chain Ch x , should satisfy the criterion of the defini-tion 4."
Definition 4 (Criterion of Representative Chain)
"The criterion of representative chain, is defined as: 1 X m S CHAIN (Ch x ) ≥ α · m S CHAIN ("
Ch i ) (4) i=1
We describe a method to normalize the semantic im-portance of each concept and lexical item on the con-cept vector space.
Figure 3 depicts the magnitude of the text vector derived from concept vectors C 1 and C 2 .
"When the magnitude of vector C 1 is a and√ that of vector C 2 is b, the overall text magnitude is a 2 + b 2 ."
Each concept is composed of words and its weight w i .
"In composing the text concept vector, the part that vector C 1 contributes to a text vector is x, and the part that vector C 2 contributes is y. By expanding the vector space property, the weight of lexical items and concepts was normalized as in definitions 5 and definition 6."
In this section we discuss a series of experiments con-ducted on the proposed system.
The results achieved below allow us to claim that the lexical chains and concept vector space effectively provide us with the semantically important index terms.
The goal of the experiment is to validate the performance of the pro-posed system and to show the potential in search per-formance improvement.
Five texts of Reader’s Digest from Web were selected and six subjects participated in this study.
"The texts were composed of average 11 lines in length(about five to seventeen lines long), each focused on a specific topic relevant to exercise, diet, holiday blues,yoga, and weight control."
"Most texts are re-lated to a general topic, exercise."
Each subject was presented with five short texts and asked to find index terms and weight each with value from 0 to 1.
"Other than that, relevancy to a general topic, exercise, was rated for each text."
The score that was rated by six subjects is normalized as an average.
The results of manually extracted index terms and their weights are given in Table 1.
The index term weight and the relevance score are obtained by aver-aging the individual scores rated by six subjects.
"Al-though a specific topic of each text is different, most texts are related to the exercise topic."
The percent agreement to the selected index terms is shown in Ta-ble 2[REF_CITE].
The average percent agreement is about 0.86.
This indicates the agreement among sub-jects to an index term is average 86 percent.
"We compared these ideal result with standard term frequency(standard TF, S-TF) and the proposed se-mantic weight."
Table 3 and Figures 4-6 show the com-parison results.
"We omitted a few words in represent-ing figures and tables, because standard TF method extracts all words as index terms."
"From Table 3, subjects regarded exercise, back, and pain as index terms in Text 1, and the other words are recognized as relatively unimportant ones."
"Even though exercise was mentioned only three times in Text 1, it had con-siderable semantic importance in the document; yet its standard TF weight did not represent this point at all, because the importance of exercise was the same as that of muscle, which is also mentioned three times in a text."
"The proposed approach, however, was able to differentiate the semantic importance of words."
"Fig-ure 4 shows the comparison chart version of Table 3, which contains three weight lines."
"As the weight line is closer to the subject weight line, it is expected to show better performance."
We find from the figure that the semantic weight line is analogous to the manually weighted value line than the the standard TF weight line is.
"Figures 5 and 6 show two of four texts(Text2, Text3, Text4, Text5)."
Figures on the other texts are omitted due to space consideration.
"In Figure 5, pound is mentioned most frequently in a text, con-sequently, standard TF rates the weight of pound very high."
"Nevertheless, subjects regarded it as unimpor-tant word."
Our approach discriminated its impor-tance and computed its weight lower than diet and exerciese.
"From the results, we see the proposed sys-tem is more analogous to the user weight line than the standard TF weight line."
"When semantically indexed texts are probed with a single query, exercise, the ranking result is expected to be the same as the order of the relevance score to the general topic exercise, which was rated by subjects."
"Table 4 lists the weight comparison to the index term exercise of five texts, and the subjects’ rele-vance rate to the general topic exercise."
"Subjects’ relevance rate is closely related with the subjects’ weight to the index term, exericise."
The expected ranking result is as following Table 5.
"TF weight method hardly discerns the subtle semantic impor-tance of each texts, for example, Text1 and Text2 have the same rank."
Length normalization(LN) and stan-dard TF discern each texts but fail to rank correctly.
"However, the proposed indexing method provides bet-ter ranking results than the other TF based indexing methods."
"In this paper, we intended to change the basic indexing methods by presenting a novel approach using a con-cept vector space model for extracting and weighting indexes."
"Our experiment for semantic indexing sup-ports the validity of the presented approach, which is capable of capturing the semantic importance of a word within the overall document."
"Seen from the experimental results, the proposed method achieves a level of performance comparable to major weighting methods."
"In an experiment, we didn’t compared our method with inverse document frequency(IDF) yet, because we will develop more sophisticated weight-ing method concerning IDF in future work."
This paper investigates an application of the ranked region algebra to information retrieval from large scale but unannotated documents.
"We automatically annotated documents with document structure and semantic tags by using taggers, and re-trieve information by specifying struc-ture represented by tags and words using ranked region algebra."
We report in detail what kind of data can be retrieved in the experiments by this approach.
"In the biomedical area, the number of papers is very large and increases, as it is difficult to search the in-formation."
"Although keyword-based retrieval sys-tems can be applied to a database of papers, users may not get the information they want since the re-lations between these keywords are not specified."
"If the document structures, such as “title”, “sentence”, “author”, and relation between terms are tagged in the texts, then the retrieval is improved by specify-ing such structures."
Models of the retrieval specify-ing both structures and words are pursued by many researchers[REF_CITE].
"However, these models are not robust unlike keyword-based retrieval, that is, they retrieve only the exact matches for queries."
"In the previous research[REF_CITE], we proposed a new ranking model that enables proximal and structural search for structured text."
This paper investigates an application of the ranked region al-gebra to information retrieval from large scale but unannotated documents.
We reports in detail what kind of data can be retrieved in the experiments.
"Our approach is to annotate documents with document structures and semantic tags by taggers automati-cally, and to retrieve information by specifying both structures and words using ranked region algebra."
"In this paper, we apply our approach to the OHSUMED test collecti[REF_CITE], which is a public test collection for information retrieval in the field of biomedical science but not tag-annotated."
We an-notate OHSUMED by various taggers and retrieve information from the tag-annotated corpus.
"We have implemented the ranking model in our retrieval engine, and had preliminary experiments to evaluate our model."
"In the experiments, we used the GENIA corpus[REF_CITE]as a small but manually tag-annotated corpus, and OHSUMED as a large but automatically tag-annotated corpus."
"The experiments show that our model succeeded in re-trieving the relevant answers that an exact-matching model fails to retrieve because of lack of robustness, and the relevant answers that a non-structured model fails because of lack of structural specification."
We report how structural specification works and how it doesn’t work in the experiments with OHSUMED.
Section 2 explains the region algebra.
"In Section 3, we describe our ranking model for the structured query and texts."
"In Section 4, we show the experi-mental results of this system."
"The region algebra[REF_CITE]is a set of operators representing the relation be-tween the extents (i.e. regions in texts), where an extent is represented by a pair of positions, begin-ning and ending position."
Region algebra allows for the specification of the structure of text.
"In this paper, we suppose the region algebra pro-posed[REF_CITE]."
"It has seven opera-tors as shown in Table 1; four containment opera-tors (¤, 6¤, ¢, ¢6 ) representing the containment re-lation between the extents, two combination oper-ators (4, 5) corresponding to “and” and “or” op-erator of the boolean model, and ordering operator (3) representing the order of words or structures in the texts."
"A containment relation between the ex-tents is represented as follows: e = (p s , p e ) contains e 0 = (p 0s , p 0e ) iff p s ≤ p 0s ≤ p 0e ≤ p e (we express this relation as e = e 0 )."
"The result of retrieval is a set of non-nested extents, that is defined by the following function Γ over a set of extents S:"
Γ(S) = { e|e ∈ S∧ 6 ∃e 0 ∈
S.(e 0 6= e ∧ e 0 &lt; e)}
"Intuitively, Γ(S) is an operation for finding the shortest matching."
A set of non-nested extents matching query q is expressed as G q .
"For convenience of explanation, we represent a query as a tree structure as shown in Figure 1 (‘[x]’ is a abbreviation of ‘hxi 3 h/xi’)."
This query rep-resents ‘Retrieve the books whose title has the word “retrieval.” ’
The algorithm for finding an exact match of a query works efficiently.
The time complexity of the algorithm is linear to the size of a query and the size of documents[REF_CITE].
This section describes the definition of the relevance between a document and a structured query repre-sented by the region algebra.
"The key idea is that a structured query is decomposed into subqueries, and the relevance of the whole query is represented as a vector of relevance measures of subqueries."
Our model assigns a relevance measure of the structured query as a vector of relevance measures of the subqueries.
"In other words, the relevance is defined by the number of portions matched with subqueries in a document."
"If an extent matches a subquery of query q, the extent will be somewhat relevant to q even when the extent does not exactly match q. Figure 2 shows an example of a query and its subqueries."
"In this example, even when an extent does not match the whole query exactly, if the ex-tent matches “retrieval” or ‘[title]¤“retrieval”’, the extent is considered to be relevant to the query."
Sub-queries are formally defined as follows.
"Definition 1 (Subquery) Let q be a given query and n 1 ,...,n m be the nodes of q. Subqueries q 1 ,...,q m of q are the subtrees of q."
Each q i has node n i as a root node.
"When a relevance σ(q i ,d) between a subquery q i and a document d is given, the relevance of the whole query is defined as follows."
"Definition 2 (Relevance of the whole query) Let q be a given query, d be a document and q 1 , ..., q m be subqueries of q."
"The relevance vector Σ(q, d) of d is defined as follows:"
"Σ(q, d) = hσ(q 1 , d), σ(q 2 , d), ..., σ(q m , d)i"
A relevance of a subquery should be defined simi-larly to that of keyword-based queries in the tradi-tional ranked retrieval.
"For example, TFIDF, which is used in our experiments in Section 4, is the most simple and straightforward one, while other rele-vance measures recently proposed[REF_CITE]can be applied."
"TF of a subquery is calculated using the number of extents matching the subquery, and IDF of a subquery is calculated using the number of documents includ-ing the extents matching the subquery."
"When a text is given as Figure 3 and document collection is {(1,15),(16,30)}, extents matching each subquery in each document are shown in Table 2."
TF and IDF are calculated using the number of extents matching subquery in Table 2.
"While we have defined a relevance of the struc-tured query as a vector, we need to arrange the doc-uments according to the relevance vectors."
"In this paper, we first map a vector into a scalar value, and then sort the documents according to this scalar measure."
Three methods are introduced for the mapping from the relevance vector to the scalar measure.
The first one simply works out the sum of the elements of the relevance vector.
Definition 3 (Simple Sum)
"X m ρ sum (q, d) = σ(q i , d) i=1"
The second appends a coefficient representing the rareness of the structures.
"When the query is A ¤ B or A ¢ B, if the number of extents matching the query is close to the number of extents matching A, matching the query does not seem to be very impor-tant because it means that the extents that match A mostly match A¤B or"
The case of the other operators is the same as with ¤ and ¢.
"Definition 4 (Structure Coefficient) When the op-erator op is 4, 5 or 3, the structure coefficient of the query A op B is: sc AopB = C(A) + C(B) − C(A op B) C(A) + C(B) and when the operator op is ¤ or ¢, the structure coefficient of the query A op B is: sc AopB = C(A) − C(A op B) C(A) where A and B are the queries and C(A) is the num-ber of extents that match A in the document collec-tion."
"The scalar measure ρ sc (q i , d) is then defined as"
"X m ρ sc (q, d) = sc q i · σ(q i , d) i=1"
The third is a combination of the measure of the query itself and the measure of the subqueries.
"Al-though we calculate the score of extents by sub-queries instead of using only the whole query, the score of subqueries can not be compared with the score of other subqueries."
We assume normalized weight of each subquery and interpolate the weight of parent node and children nodes.
Definition 5 (Interpolated Coefficient)
The inter-polated coefficient of the query q i is recursively de-fined as follows:
"P ρ ic (q i , d) = λ · σ(q i , d) + (1 − λ) c i ρ ic (q c i , d) l where c i is the child of node n i , l is the number of children of node n i , and 0 ≤ λ ≤ 1."
This formula means that the weight of each node is defined by a weighted average of the weight of the query and its subqueries.
"When λ = 1, the weight of a query is normalized weight of the query."
"When λ = 0, the weight of a query is calculated from the weight of the subqueries, i.e. the weight is calcu-lated by only the weight of the words used in the query."
"In this section, we show the results of our prelimi-nary experiments of text retrieval using our model."
We used the GENIA corpus[REF_CITE]and the OHSUMED test collecti[REF_CITE].
"We compared three retrieval models, i) our model, ii) exact matching of the region algebra (exact), and iii) not structured model (flat)."
The queries submit-ted to our system are shown in Table 3 and 4.
"In the flat model, the query was submitted as a query composed of the words in the queries connected by the “and” operator (4)."
"For example, in the case of Query 1, the query submitted to the system in the flat model is ‘ “G#DNA domain or region” 4 “in” 4 “G#tissue” 4 “G#body part” .’"
The system out-put the ten results that had the highest relevance for each model.
"In the following experiments, we used a computer that had Pentium III 1.27GHz CPU, 4GB memory."
The system was implemented in C++ with Berkeley DB library.
The GENIA corpus is an XML document com-posed of paper abstracts in the field of biomedi-cal science.
"The corpus consisted of 1,990 arti-cles, 873,087 words (including tags), and 16,391 sentences."
"In the GENIA corpus, the document structure was annotated by tags such as “harticlei” and “hsentencei”, technical terms were annotated by “hconsi”, and events were annotated by “heventi”."
The queries in Table 3 are made by an expert in the field of biomedicine.
The document was “sen-tence” in this experiments.
Query 1 retrieves sen-tences including a gene in a tissue.
Queries 2 and 3 retrieve sentences representing an event having a gene as an object and occurring in a tissue.
"In Query 2, a gene was represented by the word “gene,” and in Query 3, a gene was represented by the annotation “G#DNA domain or region.”"
"For the exact model, ten results were selected ran-domly from the exactly matched results if the num-ber of results was more than ten."
"The results are blind tested, i.e., after we had the results for each model, we shuffled these results randomly for each query, and the shuffled results were judged by an ex-pert in the field of biomedicine whether they were relevant or not."
Table 5 shows the number of the results that were judged relevant in the top ten results.
The results show that our model was superior to the exact and flat models for all queries.
"Compared to the exact model, our model output more relevant documents, since our model allows the partial matching of the query, which shows the robustness of our model."
"In addition, our model gives a better result than the flat model, which means that the structural specification of the query was effective for finding the relevant documents."
"Comparing our models, the number of relevant re-sults using ρ sc was the same as that of ρ sum ."
The re-sults using ρ ic varied between the results of the flat model and the results of the exact model depending on the value of λ.
The OHSUMED test collection is a document set composed of paper abstracts in the field of biomed- ical science.
The collection has a query set and a list of relevant documents for each query.
The query consisted of patient information and information request.
"We used ti-tle, abstract, and human-assigned MeSH term fields of documents in the experiments."
"Since the origi-nal OHSUMED is not annotated with tags, we an-notated it with tags representing document struc-tures such as “harticlei” and “hsentencei”, and an-notated technical terms with tags such as “hdiseasei” and “htherapeutici” by longest matching of terms of Unified Medical Language System (UMLS)."
"In the OHSUMED, relations between technical terms such as events were not annotated unlike the GENIA cor-pus."
"The collection consisted of 348,566 articles, 78,207,514 words (including tags), and 1,731,953 sentences. 12 of 106 queries of OHSUMED are converted into structured queries of Region Algebra by an ex-pert in the field of biomedicine."
"These queries are shown in Table 4, and submitted to the system."
The document was “article” in this experiments.
"For the exact model, all exact matches of the whole query were judged."
"Since there are documents that are not judged whether or not relevant to the query in the OHSUMED, we picked up only the documents that are judged."
Table 6 shows the number of relevant results in top ten results.
"The results show that our model suc-ceeded in finding the relevant results that the exact model could not find, and was superior to the flat model for Query 4, 5, and 6."
"However, our model was inferior to the flat model[REF_CITE]and 15."
"Comparing our models, the number of relevant results using ρ sc and ρ ic was lower than that using ρ sum ."
"In the experiments on OHSUMED, the number of relevant documents of our model were less than that of the flat model in some queries."
"We think this is because i) specifying structures was not effective, ii) weighting subqueries didn’t work, iii) MeSH terms embedded in the documents are effective for the flat model and not effective for our model, iv) or there are many documents that our system found relevant but were not judged since the OHSUMED test col-lection was made using keyword-based retrieval."
"As for i), structural specification in the queries is not well-written because the exact model failed to achieve high precision and its coverage is very low."
We used only tags specifying technical terms as structures in the experiments on OHSUMED.
This structure was not so effective because these tags are annotated by longest match of terms.
We need to use the tags representing relations between techni-cal terms to improve the results.
"Moreover, struc-tured query used in the experiments may not specify the request information exactly."
"Therefore we think converting queries written by natural language into the appropriate structured queries is important, and lead to the question answering using variously tag-annotated texts."
"As for ii), we think the weighting didn’t work because we simply use frequency of subqueries for weighting."
"To improve the weighting, we have to assign high weight to the structure concerned with user’s intention, that are written in the request in-formation."
This is shown in the results of Query 9.
"In Query 9, relevant documents were not re-trieved except the model using ρ ic , because although the request information was information concerned “lupus nephritis”, the weight concerned with “lu-pus nephritis” was smaller than that concerned with “thrombotic” and “thrombocytopenic purpura” in the models except ρ ic ."
"Because the structures con-cerning with user’s intention did not match the most weighted structures in the model, the relevant docu-ments were not retrieved."
"As for iii), MeSH terms are human-assigned key-words for each documents, and no relation exists across a boundary of each MeSH terms. in the flat model, these MeSH term will improve the re-sults."
"However, in our model, the structure some-times matches that are not expected."
"For example, In the case[REF_CITE]the subquery ‘ “chronic” 3 “fatigue” 3 “syndrome” ’ matched in the field of MeSH term across a boundary of terms when the MeSH term field was text such as “Affective Disor-ders/*CO; Chronic Disease; Fatigue/*PX; Human; Syndrome ” because the operator 3 has no limita-tion of distance."
"As for iv), the OHSUMED test collection was constructed by attaching the relevance judgement to the documents retrieved by keyword-based retrieval."
"To show the effectiveness of structured retrieval more clearly, we need test collection with (struc-tured) query and lists of relevant documents, and the tag-annotated documents, for example, tags repre-senting the relation between the technical terms such as “event”, or taggers that can annotate such tags."
Table 7 and 8 show that the retrieval time in-creases corresponding to the size of the document collection.
The system is efficient enough for infor-mation retrieval for a rather small document set like GENIA corpus.
"To apply to the huge databases such as Web-based applications, we might require a con- stant time algorithm, which should be the subject of future research."
We proposed an approach to retrieve information from documents which are not annotated with any tags.
"We annotated documents with document struc-tures and semantic tags by taggers, and retrieved information by using ranked region algebra."
We showed what kind of data can be retrieved from doc-uments in the experiments.
"In the discussion, we showed several points about the ranked retrieval for structured texts."
"Our future work is to improve a model, corpus etc. to improve the ranked retrieval for structured texts."
Discourse chunking is a simple way to segment dialogues according to how dia-logue participants raise topics and negoti-ate them.
"This paper explains a method for arranging dialogues into chunks, and also shows how discourse chunking can be used to improve performance for a dialogue act tagger that uses a case-based reasoning approach."
A dialogue act (hereafter DA) is an encapsulation of the speakerÕs intentions in dialogueÑwhat the speaker is trying to accomplish by saying some-thing.
"In DA tagging (similar to part-of-speech tagging), utterances in a dialogue are tagged with the most appropriate speech act from a tagset."
"DA tagging has application in NLP work, including speech recognition and language understanding."
"The Verbmobil-2 corpus was used for this study, with its accompanying tagset, shown in Table 1.1."
"Much of the work in DA tagging[REF_CITE]uses lexical information (the words or n-grams in an utterance), and to a lesser extent syntactic and phonological information (as with prosody)."
"However, there has traditionally been a lack of true discourse-level information in tasks involving dialogue acts."
Discourse information is typically limited to looking at surrounding DA tags[REF_CITE].
"Unfortunately, knowledge of prior DA tags does not always translate to an accurate guess of whatÕs coming next, especially when this information is imperfect."
"Theories about the structure of dialogue (for example, centering [Grosz, Joshi, &amp;[REF_CITE]], and more recently Dialogue Macrogame Theory [[REF_CITE]]) have not generally been applied to the DA tagging task."
"Their use amounts to a separate tagging task of its own, with the concomitant time-consuming corpus annotation."
"In this work, I present the results from a DA tagging project that uses a case-based reasoning system ([REF_CITE])."
I show how the results from this DA tagger are improved by the use of a concept I call Òdiscourse chunking.
"Ó Discourse chunking gives information about the patterns of topic raising and negotiation in dia- logue, and where an utterance fits within these patterns."
"It is also able to use existing DA tag information within the corpus, without the need for separate annotation."
"In order to accomplish a mutual goal (for example, two people trying to find a suitable appointment time), dialogue participants engage in predictable kinds of activity, structuring the conversation in a coherent way in order to accomplish their goals."
Hello The dialogue participants greet each other.
"They introduce themselves, unveil their affiliation, or the institution or location they are from."
Opening The topic to be negotiated is introduced.
"Negotiation The actual negotiation, between opening and closing."
"Closing The negotiation is finished (all participants have agreed), and the agreed-upon topic is (sometimes) recapitulated."
Good Bye The dialogue participants say good bye to each other.
"Within a conversation, the opening-negotiation-closing steps are often repeated in a cyclical pat-tern."
"This work on discourse chunking combines the opening, negotiation, and closing sections into a single chunk."
"One reason for this is that these parts of the conversation tend to act as a single chunk; when they appear, they regularly appear together and in the same order."
"Also, some of these parts may be missing; a topic of negotiation is frequently brought up and resolved without an explicit open-ing or closing."
"Very often, the act of beginning a topic of negotiation defines the opening by itself, and the act of beginning a new negotiation entails the closing of the previous one."
"A slightly simplified model of conversation, then, appears in Figure 2.1."
"In this model, participants greet each other, en-gage in a series of negotiations, and finish the conversation when the goals of the dialogue are satisfied."
These three parts of the conversation are Òdia-logue chunksÓ.
These chunks are relevant from a
DA tagging perspective.
"For example, the DA tags used in one of these chunks are often not used in other chunks."
"For an obvious example, it would be almost unheard of for the GREET tag to appear in the ÒGood ByeÓ chunk."
Other DAÕs (such as FEEDBACK_POSITIVE) can occur in any of the three chunks.
"Knowing which chunk we are in, and where we are within a chunk, can facilitate the tagging task."
"Within chunks, some patterns emerge."
"Note that in the example from the Verbmobil-2 corpus (shown in Table 2.1), a negotiation topic is raised, and dealt with (by an ACCEPT speech act)."
Then there follows a sequence of FEEDBACK_POSITIVEs as the negotiation topic winds down.
This Òwinding downÓ activity is common at the end of a negotiation chunk.
"Then a new topic is raised, and the process continues."
One-word utterances such as ÒokayÓ or ÒyeahÓ are particularly problematic in this kind of task because they have rather general semantic content and they are commonly used in a wide range of contexts.
"The word ÒyeahÓ on its own, for exam-ple, can indicate acceptance of a proposition, mere acknowledgement of a proposition, feedback, deliberation, or a few of these at once (Core &amp;[REF_CITE])."
"In Verbmobil-2, these utterances can be labeled either A C C E P T , FEEDBACK_POSITIVE, BACK-CHANNEL, or REQUEST_COMMENT."
"Without knowing where the utterance appears within the structure of the dialogue, these utterances are very difficult to classify."
Some previous work has used prosody to solve this kind of problem (as[REF_CITE]).
I propose discourse chunks as an alternative method.
"It can pull information from the text alone, without the computational overhead that prosody can entail."
Just where do the discourse chunk boundaries lie?
"For this exercise, I have constructed a very simple set of rules to determine chunk boundaries."
These rules come from my observations; future work will involve automatic chunk segmentation.
"However, these rules do arise from a principled assumption: the raising of a new topic shows the beginning of a discourse chunk."
"Therefore, a speech act that (according to the definitions[REF_CITE]) contains a topic or proposition represents the beginning of a discourse chunk."
"By definition, only four DAÕs contain or may contain a topic or proposition."
"These are INIT, EXCLUDE, REQUEST_SUGGEST, and SUGGEST."
The chunking rules are as follows: 1.
The first utterance in a dialogue is always the start of chunk 1 (hello). 2.
The first I N I T or S U G G E S T or REQUEST_SUGGEST or EXCLUDE in a dia-logue is the start of chunk 2 (negotiation). 3.
"INIT, SUGGEST, REQUEST_SUGGEST, or EXCLUDE marks the start of a subchunk within chunk 2. 4."
"If the previous utterance is also the start of a chunk, and if it is spoken by the same person, then this utterance is considered to be a con-tinuation of the chunk, and is not marked. 5."
The first BYE is the start of chunk 3 (good bye).
"Items within a chunk are numbered evenly from 1 (the first utterance in a chunk) to 100 (the last), as shown in Table 3.1."
This normalizes the chunk distances to facilitate comparison between utter-ances.
"A thorough discussion of this CBR tagger goes beyond the scope of this paper, but a few com-ments are in order."
Case-based reasoning[REF_CITE]is a form of machine learning that uses examples.
"In general, classification using a case-based reasoner involves comparing new instances (in this case, utterances) against a database of correctly-tagged instances."
"Each new instance is marked with the same tag of its Ònearest neighbourÓ (that is, the closest match) from the database."
"A k-nearest neighbour approach selects the closest k matches from the database to be committee members, and the committee members ÒvoteÓ on the correct classification."
"In this implementation, each com-mittee member gets a vote equal to its similarity to the test utterance."
"Different values of k performed better in different aspects of the test, but this work uses k = 7 to facilitate comparison of results."
"The choice of features largely follows those[REF_CITE]and are as follows: • Speaker change • Word number • Word similarity • n-gram similarity • Previous DA tag and the following two features not included in that study, • 2-previous DA tag"
Inclusion of this feature enables more complete analysis of previous DA tags.
"Both Ôprevious DA tagÕ and Ô2-previous DA tagÕ features use the Òbest guessÓ for previous utterances rather than the Òright answerÓ, so this run allows us to test per-formance even with incomplete information. ¥ Discourse chunk tag"
Distances for this tag were computed by dividing the larger discourse chunk number from the smaller.
"Comparing two Òchunk starterÓ utterances would give the highest similarity of 1, and com-paring a chunk starter (1) to a chunk-ender (100) would give a lower similarity (.01)."
"Not all features are equally important, and so an Evolutionary Programming algorithm (adapted[REF_CITE]) was used to weight the features."
"Weightings were initially chosen randomly for each member of a population of 100, and the 10 best performers were allowed to ÒsurviveÓ and ÒmutateÓ their weightings by a Gaussian random number."
"This was repeated for 10 generations, and the weightings from the highest performer were used for the CBR tagging runs."
"A total of ten stopwords were used (the, of, and, a, an, in, to, it, is, was), the ten most common words from the BNC (Leech, Rayson, &amp;[REF_CITE])."
"These stopwords were removed when considering word similarity, but not n-gram simi-larity, since these low-content words are useful for distinguishing sequences of words that would otherwise be very similar."
The database consisted of 59 hand-tagged dia-logues (8398 utterances) from the Verbmobil-2 corpus.
This database was also automatically tagged with discourse chunks according to the rules above.
The test corpus consisted of 20 dia-logues (2604 utterances) from Verbmobil-2.
"This corpus was tagged with correct information on discourse chunks; however, no information was given on the DA tags themselves."
"Table 5.1 shows the results from two DA tagging runs using the case-based reasoning tagger: one run without discourse chunks, and one with."
"To put these results in perspective, human per-formance has been estimated at about 84%[REF_CITE], since human taggers sometimes disagree about intentions, especially when speakers perform more than one dialogue act in the same utterance."
Much of the recent DA tagging work (using 18-25 tags) scores around the mid-fifty to mid-sixty percentiles in accuracy (see[REF_CITE]for a review of similar work).
This work uses the Verbmobil-2 tagset of 32 tags.
"It could be argued that the discourse chunk in-formation, being based on tags, gives the DA tagger extra information about the tags themselves, and thus gives an unfair ÔboostÕ to the perform-ance."
At present it is difficult to say if this is the only reason for the performance gains.
"If this were the case, we would expect to see improvement in recognition for the four tags that are Òchunk start-ersÓ, and less of a gain in those that are not."
"In the test run with discourse chunks, however, we see across-the-board gains in almost all catego-ries, regardless of whether they begin a chunk or not."
"Table 5.2 shows performance measured in terms of the well-known standards of precision, recall, and f-measure."
"One notable exception to the upward trend is EXCLUDE, a beginning-of-chunk marker, which performed slightly worse with discourse chunks."
This would suggest that chunk information alone is not enough to account for the overall gain.
"Both ACCEPT and FEEDBACK_POSITIVE improved slightly, suggesting that discourse chunks were able to help disambiguate these two very similar tags."
"Table 5.3 shows the improvement in tagging scores for one-word utterances, often difficult to tag because of their general use and low informa- tion."
"These words are more likely to be tagged ACCEPT when they appear near the beginning of a chunk, and FEEDBACK_POSITIVE when they appear nearer the end."
Discourse chunks help their classification by showing their place in the dia-logue cycle.
One weakness of this project is that it assumes knowledge of the correct chunk tag.
The test corpus was tagged with the Òright answersÓ for the chunks.
"Under normal circumstances, the corpus would be tagged with the Òbest guess,Ó based on the DA tags from an earlier run."
"However, the goal for this project was to see if, given perfect infor-mation, discourse chunking would aid DA tagging performance."
The performance gains are persua-sive evidence that it does.
"Ongoing work involves seeing how accurately a new corpus can be tagged with discourse chunks, even when the DA tags are unknown."
This work was supported by an Australian Post-graduate Award.
Thanks to Cara MacNish and Shelly Harrison for supervision and advice.
Many thanks to Verbmobil for generously allowing use of the corpus which formed the basis of this pro-ject.
"In this paper, we elucidate how Korean temporal markers, OE and DONGAN contribute to specifying the event time and formalize it in terms of typed lambda calculus."
We also present a computational method for constructing temporal representation of Korean sentences on the basis of G grammar proposed by [[REF_CITE];1996].
"Associated to a NP, Korean temporal markers OE and DONGAN build time adverbials. (1) ach’im ilgopshiOE morning/seven o’clock-OE at seven o’clock in the morning (2) han shigan DONGAN one/hour/DONGAN for an hour"
"As it is widely known, time adverbials play important roles in sentence meaning processing."
"Meanwhile, there is a significant divergence in opinions whether time adverbials or tense/aspect is a more efficient indicator leading to a correct temporal representation of sentences."
"To some [[REF_CITE]], [[REF_CITE]], [[REF_CITE]], [[REF_CITE]], tense or aspect is the only credible index to consult in establishing temporal interpretation, and the time adverbials are complementary."
"To others [[REF_CITE]], [[REF_CITE]], time adverbials are regarded as much more reliable than tense/aspect which is too ambiguous to provide coherent instructions about how to locate the event in time."
"We agree with the second point of view, as we observed that Korean tense markers fail to provide a solid and coherent way to capture the relevant time span."
"For example, the verbal infix ‘-at-’, generally considered as a typical past tense marker in Korean, brings about several time interpretation possibilities such as simple past (3), completion (4), resultant state (5) and progressiveness (6). (3) shiwidaega ôje hanshiOE shich’ôngul dulrôssatta demonstrators-NOM / yesterday /one o’clock-OE / the city hall-ACC/ surround-PA-DEC [Footnote_1]"
"1 We used the McCune-Reischauer system to transcribe the Korean data. For glossing grammatical morphemes, we use the following abbreviations: ACC: accusative, AS: attributive suffix, CIRCUM: circumstantial, CL: classifier, DEC: declarative, DUR: durative, INT: interrogative, LOC: locative, NOM: nominative, NS: nominal suffix, PA: past, TOP:topic."
"The demonstrators surrounded the city hall at one o’clock yesterday. (4) shiwidaega mach’imnae shich’ôngul dulrôssatta demonstrators-NOM / at last / the city hall-ACC /surround-PA-DEC At last, the demonstrators surrounded (succeeded in surrounding) the city hall. (5) shiwidaega harudongan shich’ôngul dulrôssatta demonstrators-NOM/one day-DUR/the city hall- ACC/surround-PA-DEC The demonstrators have surrounded the city hall for one day. (6) ônjebutô shiwidaega shich’ôngul dulrôssatssumnikka? since when/ demonstrators-NOM/ the city hall- ACC/ surround-PA-INT"
Since when have the demonstrators been surrounding the city hall?
"Moreover, what triggers these interpretation possibilities is still being discussed among Korean linguists [Footnote_2] ."
"2 See [[REF_CITE]], [Lee, Ch., 1987], [Lee, H., 1993] and [Lee, J., 1982] for more detailed discussion."
"In the following, we attempt to show how time adverbials can remedy this shortcoming and specify the event time."
"The assumption underlying our temporal description is that the linguistic time is ordered, discrete, infinite and consisting of instants corresponding to the natural numbers."
"The linguistic time can be expressed with one of these three notions: instant, extended interval and duration."
"Instants are unitary constituents of linguistic time and noted by a quintuplet of natural numbers [x1,x2,x3,x4,x5] of which x1 stands for year, x2 for month, x3 for day, x4 for hour and x5 for minute."
"An extended interval is a set of consecutive instants determined by a beginning instant and an ending instant. (ex)[REF_CITE]: interval [[2003, 4,5,0,0], [2003,4,5,23,59]]"
A duration refers to a temporal distance between two distinct instants.
"For the purpose of temporal description of a sentential event, we defined the following types and functional terms on the basis of typed lambda calculus [Footnote_3] ."
"3 [[REF_CITE]; 2002], [[REF_CITE]] and [[REF_CITE]]."
The symbol λ stands for abstraction and • stands for application [Footnote_4] .
"4 If M and N are lambda-terms, then M•N is a lambda-term."
Definitions of types i : type symbol denoting the type of individuals p : type symbol denoting the type of propositions e : type symbol denoting the type of events ent: type symbol denoting the type of natural numbers inst : type symbol denoting the type of instants inter : type symbol denoting the type of extended intervals dur: type symbol denoting the type of durations Type symbols may be omitted when no ambiguity is introduced.
Definitions of functional terms (λe. moment•e): e→inst
"Applying this function to any argument of type e, we obtain the moment of e of type inst. (λe. interv•e): e→inter"
"Applying this function to any argument of type e, we obtain the interval of e of type inter. (λx. beginning•x): e→inst (λx. ending•x): e→inst"
"Applying these functions to any argument x of type e, we obtain the beginning/ending instant of x of type inst. (λx. duration•x): e→dur"
"Applying this function to any argument x of type e, we obtain the duration of x of type dur. (λx. beg•x): inter→inst (λx. end•x):inter→inst"
"Applying this function to any argument x of type inter, we obtain the beginning/ending instant of x of type inst."
"By definition, beg•[A,B] = A and end•[A,B] = B (λx. length•x): inter→dur"
"Applying this function to any argument x of type inter, we obtain the length of x of type dur."
"By definition, length•[A,B]= |B-A| (λxλy. x &lt; «t» y): inst→inst→p"
It denotes that x of type inst is anterior to y of the same type.
"When no ambiguity is introduced, «t» will be omitted."
It denotes that x and y of type inst are simultaneous. (λxλy. x ≤ «t» y): inst→inst→p
It denotes that λxλy. (x &lt; «t» y ∨ x = «t» y). (λxλy. x ∈ «t» y): inst→inter→p
It denotes that x of type inst is a member of y of type inter.
"By definition, λxλy. (beg•y ≤ x ≤ end•y) (λxλy. x ⊂ «t » y): inter→inter→p"
It denotes that x of type inter is included by y of the same type.
"By definition, λxλy. (beg•y &lt; beg•x ∧ end•x &lt; end•y). (λxλy. x = «t » y): inter→inter→p x and y of type inter are simultaneous."
"By definition, λxλy. (beg•x=beg•y ∧ end•x=end•y). (λxλy. ⊆ «t » y): inter→inter→p"
It denotes that λxλy. (beg•y ≤ beg•x ∧ end•x ≤ end•y).
The temporal adverbials with OE or DONGAN do not bring the same semantic constraints in all the sentences.
It can be illustrated by the following examples of OE (7-10) and DONGAN (11-14). (7) ach’im ilgopshiOE nurôngoiga chugôtta. the morning /seven o’clock-OE / Nurôngoi-NOM /die-PA-DEC
"At seven o’clock in the morning, Nurôngoi died."
The OE adverbial of this example indicates the moment when the event described by the nuclear sentence [Footnote_5] happened. (sr 7) ∃e∃I die•e•nurôngoi [Footnote_6] ∧ moment•e&lt;pt_speech ∧
5 We call the independent sentences without modifiers such as temporal adverbials ‘nuclear sentence’.
"6 ‘die•e•nurôngoi’ is equivalent to die(e, nurôngoi) in predicate logic."
I=(7 o’clock) ∧ moment•e=I
"But in (8) and (9), OE adverbials indicate an interval of which an instant is identified with the moment of the event. (8) samwol shiboirOE nurôngoiga chugôtta."
March/the fifteenth-OE /Nurôngoi-NOM/die-PA-
"On the fifteenth of March, Nurôngoi died. (sr 8) ∃e∃I die•e•nurôngi ∧ moment•e&lt;pt_speech ∧ I=(the 15 th of March) ∧ moment•e∈∈I (9) chinan yôrumOE nurôngoiga chugôtta the last summer-OE/ Nurôngoi-NOM/die-PA-DEC Last summer, Nurôngoi died. (sr 9) ∃e∃I die•e•nurôngi ∧ moment•e&lt;pt_speech ∧ interval•I ∧ summer•I ∧ moment•e∈∈I"
"Moreover, OE adverbials can introduce a period of recurrent events as in (10). (10) iljuirOE so dasôt mariga chugôtta a week-OE/cow/five/classifier-NOM/die-PA-DEC Five cows died every week. (sr 10) ∃I interval•I ∧ length•I=(7 days) ∧ ∃J interval•J ∧ ∃P (equi-partition•I•P•J ∧∀K (P•K → |λx. cow•x ∧ ∃e die•e•x ∧ moment•e&lt;pt_speech ∧ interv•e⊆K|=5)) [Footnote_7]"
7 [[REF_CITE]] defines the equi-partition function as: equi-partition•D•P•N ≡ (N=(∪•P) ∧ | P |&gt;2 ∧ ∀K1 K2 ((P•K1 ∧ P•K2 ∧ K1≠K2) → (length•K1=length•K2=D ∧ K1∩K2=∅))) where ∪•R ≡ λx. ∃R (R•P ∧ P•x)
"As for DONGAN adverbials, they present the maximal duration of the described event as in (11). (11) hanshigan DONGAN kwanghoe bihaenggiga naratta an hour /DONGAN /of"
Kwangho /airplane-NOM /fly-PA-DEC Kwangho’s airplane flew for an hour. (sr 11) ∃x∃e airplane•x ∧ of•kwangho•x ∧ fly•e•x ∧ ending•e&lt;pt_speech ∧ duration•e=(1 hour)
"In other words, it is not clear, for the moment, whether the described event reached its end or not. (12) kyôul banghak DONGAN ukyunun mokgongsoesô ilhaetta winter vacation /DONGAN /Ukyu-TOP/carpenter’s shop-LOC/work-PA-DEC"
"During the winter vacation, Ukyu worked at the carpenter’s shop. (sr 12) ∃e∃I work•e•ukyu ∧ at•e•carpenter’s_shop ∧ beginning•e&lt;pt_speech ∧ interval•I ∧ winter_vacation•I ∧ I⊆⊆interv•e"
"DONGAN adverbials also indicate the interval to which the moment of the event belongs, as (13) shows. (13) kyôul banghak DONGAN nanun shine daehae saenggak’agi chijak’aetta winter vacation / DONGAN / I-TOP/ about God/ think /begin-PA-DEC During the winter vacation, I began to think about God. (sr 13) ∃I∃e interval•I ∧ winter_vacation•I ∧ begin•e•(λe1λx. think_about•e1•god•x)•speaker ∧ moment•e&lt;pt_speech ∧ moment•e∈∈I"
The following example (14) denotes that fishing of Yunsôk has been repeated in a regular way during the interval indicated by the DONGAN adverbial. (14) shimnyôn DONGAN yunsôkun môn badaesô kokijabirul haetta 10 years /DONGAN /Yunsôk-TOP /far ocean-LOC /fishing-ACC/do-PA-DEC
"For ten years, Yunsôk fished in the far ocean. (sr 14) λD. ∃P equi-partition•D•P•int ref ∧ ∃H H=(λJ. (P•J ∧ ∃e fish•e•yunsôk ∧ in•e•the_far_ocean ∧ ending•e&lt;pt_speech ∧ interv•e⊆J) ∧ ∃M max•(λN. N⊆(∪•H) ∧ [inferior•(∪•N), superior•(∪•N)] ∩ int ref = ∪•N)•M ∧ length•M = ([Footnote_10] years) [Footnote_8]"
"8 [[REF_CITE]] defines the function used in this formula as follows: int ref ≡ interval of reference [∪•I] ≡ [inferior•(∪•I), superior•(∪•I)] where the brackets denote an interval. max•E•M ≡ (E•M ∧ ¬∃N(M⊂N ∧ E•N))"
"Such a distributional pattern of events disappears when the nuclear sentence is modified by quantification, which is illustrated by (15). (15) shimnyôn DONGAN yunsôkun môn badaesô kokijabirul se bôn haetta 10 years /DONGAN /Yunsôk-TOP /far ocean-LOC /fishing-ACC/three times/do-PA-DEC"
"For ten years, Yunsôk had fished in the far ocean three times. (sr 15) ∃I interval•I ∧ length•I=(10 years) ∧ |λe. fish•e•yunsôk ∧ in•e•the_far_ocean ∧ ending•e&lt;pt_speech ∧ interv•e⊆I|=3"
"To find a strategy to solve such a multiple ambiguity, we investigated three thousand sentences for each temporal marker [Footnote_9] and discovered the following facts: 1."
9 We took the sentences from Yonsei malmunchi corpus built by Yonsei Center for Linguistic Information.
The semantic and syntactic properties of the phrase accompanying the temporal markers play an important role to locate the event in time. 2.
It is necessary to distinguish mono-occurrent sentences concerning a single event from multi-occurrent sentences concerning a set of different events 10 .
"The multi-occurrent nature is very often signaled by bare plurals in nominal phrases, adverb like ch’arero ‘in turn’, and quantification modifiers. 3."
"When it comes to the multi-occurrent sentences, DONGAN adverbials impose constraints on the distribution of events in some cases (see (14)). 4."
The quantification thenegates distributional meaning brought by DONGAN adverbials and gets them to indicate the temporal scope of this semantic operation (see (14) and (15)). 5.
"As for the verbal infix ‘-at-’, its common semantic value is to denote the fact that the beginning of the event is anterior to the point of speech 11 ; λe. beginning•e &lt; pt_speech. 6."
"The information relevant to the time interpretation is scattered over the whole sentence; in the verbal phrase, quantification modifiers ranging over individuals or events, determiners in the nominal phrases and time adverbials."
"Therefore, the temporal interpretation of a sentence should be constructed in a compositional way. 7."
"For the same reason, the aspectual value should be attributed to the nuclear sentence and not to the verbal phrase."
We discovered that Renaud’s G Grammar is suitable for the purpose of computational implementation of these facts.
This grammar loads information on word definitions as little as possible and charges the rules with detailed description.
This principle contributes to gathering the pieces of information scattered throughout a sentence and to establishing a semantic representation of the sentence in a compositional way.
"Moreover, it enables us to deal with all the other linguistic phenomena in the same way as with the temporal problems."
This grammar has been applied to French [[REF_CITE]; 2000; 2002] and Japanese [[REF_CITE]] as well.
This grammar is divided into word definitions called ‘dico’ and composition rules.
"Each of them consists of syntactic constraint, unification-based feature constraint and semantic constraint written in lambda-terms."
"In composition rules, the symbol ‘→’ stands for syntactic rewriting and ‘&lt;&lt;=’ stands for β-reduction."
We present here an example process establishing the temporal interpretation of a Korean sentence extended by a DONGAN adverbial. (11) hanshigan DONGAN kwanghoe bihaenggiga naratta an hour/ DONGAN/ of Kwangho/ airplane-NOM/ fly-PA-DEC
Kwangho’s airplane flew for an hour.
"We determine the semantic term of a DONGAN adverbial, according to the semantic and syntactic properties of the phrase preceding the temporal marker. (Syn) NP Clause (Sem) Moment/Extended Interval/Duration"
The time adverbial hanshigan[REF_CITE]includes a NP denoting duration and conforms to the syntactic condition E 12 .
"Thus, the semantic term of type t5 is assigned to this time adverbial."
"We also calculate the semantic term of the nuclear sentence relying on criteria such as quantification modification, mono/multi-occurrent and aspect 13 , which get involved in the feature constraint at the levels of both dico and of rules."
"Since the nuclear sentence of (11) is not modified by quantification, and since it concerns a single event of activity, it receives a semantic term of type c3 in the following figure."
"We excluded the last structure from our research because of its highly context dependent meaning. 13 Aspectual classification is done by the following method; first, we observed the compatibilities of nuclear sentences with linguistic expressions such as -go innun chungida, mane and dongan."
And then we investigated whether mane indicates the preparatory stage of the concerned event and whether dongan marks the resultant state of the event.
"As a result, we obtained seven distinct combinations as follows."
"At last, the semantic term of the time adverbial and that of the nuclear sentence are joined together by the following rule to put the final semantic representation of (11) [Footnote_14] :"
14 ‘/’ stands for disjunction. ‘λxλy. proj•x•y’ returns the member occurring in the x th place in the list y.
"As we mentioned above, one of the most important advantages of G Grammar consists of its capacity to establish semantic interpretations in a compositional way."
"Even if we presented only the final step of semantic processing, our Korean parser constructs a semantic representation at each step [Footnote_15] ."
"15 Our Korean parser is built in LPI Prolog. In Figure 6, ‘lb’ stands for λ-abstraction and ‘*’ stands for λ-application."
"The sentences extended by an OE adverbial are represented in the same way as those by a DONGAN adverbial, as will be seen in the following. (8) samwol shiboirOE nurôngoiga chugôtta."
"March/the fifteenth-OE /Nurôngoi-NOM/die-PA-DEC On the fifteenth of March, Nurôngoi died."
"Relying on the semantic and syntactic constraints of the phrase preceding OE at the same time, we determine the semantic term of the OE adverbial."
"Since samwol shiboirOE of (8) denotes an extended interval and it conforms to the syntactic condition C, this adverbial is attributed the semantic term of type t3."
The semantic representation of the nuclear sentence of (8) is established in the same way as explained above in Figure 3.
"At last, taking the semantic terms of the OE adverbial and of the nuclear sentence, the following rule serves to construct the final representation of the whole sentence 17 ."
"In this paper, we showed how OE adverbials and DONGAN adverbials contribute to constructing the temporal interpretation of Korean sentences."
We also formalized the semantic properties of these temporal markers with typed lambda calculus before we integrated them into the Korean parser that we built on the basis of Renaud’s G Grammar.
We showed the effectiveness of this grammar in representing compositionally semantic interpretations of Korean sentences.
"In the future, we will study the Korean time adverbials with MANE and zero particle."
The first temporal marker is believed to signal the telicity of the event and the second appears very frequently in informal discourses.
"This paper describes a classifier that assigns se-mantic thesaurus categories to unknown Chinese words (words not already in the CiLin thesaurus and the Chinese Electronic Dictionary, but in the Sinica Corpus)."
The focus of the paper differs in two ways from previous research in this particular area.
The biggest problem for assigning semantic cate-gories to words lies in the incompleteness of dic-tionaries.
It is impractical to construct a dictionary that will contain all words that may occur in some previously unseen corpora.
This issue is particu-larly problematic for natural language processing applications that work with Chinese texts.
"Specifi-cally, for the Sinica Corpus [Footnote_1] , Bai,[REF_CITE]found that articles contain on average 3.51% words that were not listed in the Chinese Electronic Dictionary [Footnote_2] of 80,000 words."
1 The Sinica Corpus is a balanced corpus contained five million part-of-speech words in Mandarin Chinese.
2 The Chinese Electronic Dictionary is from the Computational Linguistics Society of R.O.C.
"Because novel words are created daily, it is impossible to collect them all."
"Furthermore, across most of the corpora, many of these newly coined words seem to be used only once, and thus they may not even be worth collecting."
"However, the occurrence of unknown words makes a number of NLP (Natural Language Processing) tasks such as segmentation and word sense disambiguation more difficult."
"Consequently, it would be valuable to have some means of automatically assigning meaning to un-known words."
This paper describes a classifier that assigns semantic thesaurus categories to unknown Chinese words.
"While con-text is clearly an important feature, this paper fo-cuses on non-contextual features, which may play a key role for unknown words that occur only once and hence have limited context."
"The feature I focus on, following[REF_CITE], is morphological similarity to words whose semantic category is known."
Such a find-ing suggests that a reliable source of semantic in-formation lies in the morphology used to construct the unknown words.
"In Chinese morphology, the two ways to generate new words are compounding and affixation."
"Orthographically, such compounding and affixa-tion is represented by combinations of characters, and as a result, the character combinations and the morpho-syntactic relationship used to link them together can be clues for classification."
"Further-more, my analysis of the Sinica Corpus indicates that only 49.68% monosyllabic [Footnote_3] words have one word class, but 91.67% multisyallabic words have one word class in Table 1."
"3 ‘Monosyllabic word’ means a word with only a char-acter, and ‘multisynllabic word’ means a word with more than one character."
"Once characters merge together, only 8.33% words remain ambiguous."
"It implies that as characters are combined together, the degree of ambiguity tends to decrease."
"The remainder of this paper is organized in the fol-lowing manner: section 2 introduces the CiLin the-saurus, section 3 provides an analysis of unknown words in the Sinica Corpus, and section 4 details the algorithm used for the semantic classification and explains the results."
"The CiL[REF_CITE]is a thesaurus that con-tains 12 main categories: A-human, B-object, C-time and space, D-abstract, E-attribute, F-action, G-mental action, H-activity, I-state, J-association, K-auxiliary, and L-respect."
"The majority of words in the A-D categories are nouns, while the majority in the F-J categories are verbs."
"As shown in Figure 1, the main categories are further subdivided into more specific subcategories in a three-tier hierar-chy."
"Unknown words are the Sinica Corpus lexicons that are not listed in the Chinese Electronic Dic-tionary of 80,000 lexicons and the CiLin."
"The 5 million word Sinica[REF_CITE]866 un-known words consisting of 1.59% adjectives, 33.73% common nouns, 25.18% proper nouns, 12.48% location nouns, 2.98% time nouns, and 24.[Footnote_4]% verbs as shown in Table 2."
4 ‘Word Class’ means the number of each word’s word class.
"The focus of most other Chinese unknown word research is on identification of proper nouns such as proper names[REF_CITE], personal names (Lee,[REF_CITE]), abbreviation (Huang,[REF_CITE]), and organization names (Chen &amp;[REF_CITE])."
Unknown words in categories outside the class of proper nouns are seldom mentioned.
"One of the few examples of multiple class word prediction is Chen, Bai and Chen‘s 1997 work em-ploying statistical methods based on the prefix-category and suffix-category associations to pre-dict the syntactic function of unknown words."
"Al-though proper nouns may contain lots of useful and valuable information in a sentence, the majority of unknown words in Chinese are lexical words, and consequently, it is also important to classify lexical words."
"If not, the remaining 70% of unknown words [Footnote_5] will be an obstacle to Chinese NLP, where 24.04% of verbs are unknown can be a major prob-lem for parsers."
5 Part of location noun still contains some proper nouns like country names.
"In Chinese morphology, the two ways to generate new words are compounding and affixation."
"For the task of classifying unknown words, two algorithms are evaluated."
The first algorithm uses a simple heuristic where the semantic category of an unknown word is determined by the head of the unknown word.
The second algorithm adopts a more sophisticated nearest neighbor approach such that the distance between an unknown word and examples from the CiLin thesaurus computed based upon its morphological structure.
The first algorithm serves to provide a baseline against which the performance of the second can be evalu-ated.
The baseline method is to assign the semantic category of the morphological head to each word.
The algorithm for the nearest neighbor classifier is as follows: 1) An unknown word is parsed by a morphological analyzer[REF_CITE].
"The analyzer a) segments a word into a sequence of morphemes, b) tags the syntactic categories of morphemes, and c) predicts morpho-syntactic relationships between morphemes, such as modifier-head, verb-object and resultative verbs as shown as in Table 3."
"For example, if 舞蹈家/wudaojia DANCE-EXPERT ‘dancer’ is an unknown word, the morphological segmentation is 舞蹈/wudao DANCE ‘dance’ and 家/jia EXPERT ‘expert’, and the relation is modi-fier-head. 2) The CiLin thesaurus is then searched for entries (examples) that are similar to the unknown word."
"A list of words sharing at least one morpheme with the unknown word, in the same position, is con-structed."
"In the case of 舞蹈家/wudaojia, such a list would include 歌 唱 家 /gechangjia SING-EXPERT ‘singer’, 回家/huijia GO-HOME ‘go home’, 富貴家/fuguijia RICH-FAMILY ‘rich fam-ily’ and so on. 3) The examples that do not have the same mor-pho-syntactic relationships but shared morpheme belongs to the unknown word’s modifier are pruned away."
"If no examples are found, the system falls back to the baseline classification method. 4) The semantic similarity metric used to compute the distance between the unknown word and the selected examples from the CiLin thesaurus is based upon a method first proposed[REF_CITE]."
They assume that similarity of two semantic cate-gories is the information content of their parent’s node.
"For instance, the similarity of 哈密瓜 /hamigua ‘hami melon’[REF_CITE]and 番茄/fanqie ‘tomato’[REF_CITE]is based on the information con-tent of the node of their least common ancestor Bh."
"The CiLin thesaurus can be used as an information system, and the information content of each se-mantic category is defined as"
Entropy(System) − Entropy(Semantic category)
"The similarity of two words is the least common ancestor information content(IC), and hence, the higher the information content is, the more similar two the words are."
The information content is normalized by Entropy(system) in order to keep the similarity between 0 and 1.
"To simplify the computation, the probabilities of all leaf nodes are assumed equal."
"For example, the probability of Bh is .0064 and the information content of Bh is – log(.0064)."
"Hence, the similarity between 哈密瓜/ hamigua and 番茄/ fanqie is .61. 2 ( P ( W 1 IW 2 ))"
Sim ( W 1 IW 2 ) =
EntropyIC ( W ( System ) = −Entropylog ( System ) 1 IW 2 ) (1)
Sim ([REF_CITE]) = ICEntropy ([REF_CITE]I ([REF_CITE])) =
Entropy−log ( CiLin ) 2 ( P ( Bh )) = -log 2 0.0064 = 7.29 = 0.61 -log 2 0.0026 11.94
"Resnik (1995, 1998 and 2000) and[REF_CITE]also proposed information content algorithms for simi-larity measurement."
One problem for this algorithm is the insufficient coverage of the CiLin (CiLin may not cover all morphemes).
The backup method is to run the clas-sifier recursively to predict the possible categories of the unlisted morphemes.
"If a morpheme of an unknown word or of an unknown word’s example is not listed in the CiLin, the similarity measure-ment will suspend measuring the similarity be-tween the unknown word and the examples and run the classifier to predict he semantic category of the morpheme first."
"After the category of the mor-pheme is known, the classifier will continue to measure the similarity between the unknown word and its examples."
The probability of adopting this backup method in my experiment is on the average of 3%.
Here is an example of the recursive semantic measurement. 跑 碼 頭 /paomatou RUN-WHARF ‘wharf-worker’ is an example of an unknown word 跑旱船/paohanchuan RUN-DRY BOAT ‘folk ac-tivities’.
The morphological analyzer breaks the two words into 跑 碼頭/pao matou and 跑 旱船 /pao hanchuan.
"The measurement function will compute the similarity between 碼頭/matou and 旱 船/hanchuan, but in this case, 旱船/hanchuan is not listed in the CiLin."
The next approach is then to run the semantic classifier to guess the possible category of 旱船/hanchuan.
"Based on the predicted category, it then goes on to compute the similarity for 碼頭/matuo and 旱船/hanchuan."
"By applying this method, there will not be any words without a similarity measurement. 5) After the distances from the unknown word to each of the selected examples from the CiLin the-saurus are determined, the average distance to the K nearest neighbors from each semantic category is computed."
The category with the lowest distance is assigned to the unknown word.
"The similarity of 舞蹈/wudao and 歌唱/gechang is .87, of 舞蹈/wudao and 回/hui is .26, and of 舞 蹈/wudao and 富貴/fugui is 0."
"Thus, 舞蹈家 /wudaojia is more similar to 歌唱家/gechangjia than回家/huijia or富貴家/fuguijia."
The category of 舞蹈家/wudaojia is thus most likely to be 歌唱家 /gechangjia.
The semantic category is predicted as the category that gets the highest score in formula (2).
The lexi-cal similarity and frequency of examples of each category are considered as the most important fea-tures to decide a category.
"In formula (2), RankScore(C i ) includes SS(C i ) and FS(C i )."
"The score of SS(C i ) is a lexical similarity score, which is from the maximum score of Simi-larity (W 1 ,W 2 ) in the category of W 2 ."
FS(C i ) is a frequency score to show how many examples there are in a category. α and (1-α) are respectively weights for the lexical similarity score and the fre-quency score.
Let W 1 = unknownword W i = wordwhosesemanticcategorydefined in theCiLin i =
"Rankscore ( C i ) = α∗SS ( C i ) + ( 1− α ) ∗FS ( C i ) (2) SS ( C i ) = arg i=A max ...L Sim ( W 1 ,W i ) (3) C ( W i ) ∈C i FS ( C i ) ="
L Freq ( C ) (4) i ∑ Freq ( C i ) i=A
"For experi-ments, CiLin lexicons are divided into 2 sets: a training set of 80% CiLin words, a development set of 10% of CiLin words, and a test set of 10% CiLin words."
"All words in the test set are assumed to be unknown, which means the semantic catego-ries in both sets are unknown."
"Nevertheless, the morphological structures of proper nouns are dif-ferent from lexical words."
Their identification methods are also different and will be out of the scope of this paper.
"The correct category of the unknown word is the semantic category in the CiLin, and if an unknown word is ambiguous, which means it contains more than one category, the system then chooses only one possible category."
"In evaluation, any one of the categories of an am-biguous word is considered correct."
"On the test set, the baseline predicts 53.50% of adjectives, 70.84% of nouns and 47.19% of verbs correctly."
"The classifier reaches 64.20% in adjec- tives, 71.77% in nouns and 53.47% in verbs, when α is 0.5 and K is five."
Table 4 and table 5 show a comparison of the base-line and the classifier.
"Generally, nouns are easier to predict than the other categories, because their morpho-syntactic relation is not as complex as verbs and adjectives."
"The classifier improves on baseline semantic categorization performance for adjectives and verbs, but not for nouns."
The lack of a performance increase for nouns is most likely because nouns only have one kind of morpho-syntactic relation.
The advantage of the classifier is to filter out examples in different relations and to find out the most similar example in morphemes and morpho-syntactic relation.
"The classifier pre-dicts better than the baseline in word classes with multiple relations, such as adjectives and verbs."
"For example, 開 快 車 /kaikuaiche OPEN-FAST CAR ‘drive fast’ is a verb-object verb."
"The base-line wrongly predicted it due to the verb, 開/kai OPEN ‘open’."
"However, the semantic classifier grouped it to the category of its similar example, 開夜車/kaiyeche OPEN-NIGHT CAR ‘drive dur-ing the night’."
Error sources can be grouped into two types: data errors and the classifier errors.
The testing data is from the CiLin.
"Some of testing data are not se-mantically transparent such as idioms, metaphors, and slang."
The meaning of such words is different from the literal meaning.
"For instance, the literal meaning of 看門狗/kanmengou WATCH-DOOR-"
"DOG is a door-watching dog, and in fact it refers to a person with the belittling meaning. 母老虎 /mulaohu"
"FEMALE-TIGER is a female tiger liter-ally, and it refers to a mean woman."
These words do not carry the meaning of their head anymore.
An unknown word will be created such as 看門貓 /kanmenmao
"WATCH-DOOR-CAT ‘a door-watching cat’, but it is impossible for unknown words to carry similar meaning of words as 看門狗 /kanmengou."
"The classifier errors are due primarily to three fac-tors: a lack of examples, the preciseness of the similarity measurement, and the taxonomy of the CiLin."
"First, some errors occur when there are not enough examples in training data."
"For example, 鐵欄杆 /tielangan IRON-POLE ‘iron pole` does not have any similar examples after the classifier filters out examples whose relations are different and whose shared morphemes are not head. 鐵欄杆/tielangan is segmented as 鐵/tie IRON ‘iron’ and 欄杆 /langan POLE ‘pole’."
"There are examples of the first morpheme, 鐵/tie, but no similar examples of the second,欄杆/langan."
"Since 鐵欄杆/tielangan has modifier-head relation and 欄杆/langan is the head of the compound, then the classifier filters out the examples of 鐵/tie."
There are hence not enough examples.
Filtering examples in different structures is performed to make the remaining examples more similar since the similarity measurement may not be able to distinguish slight differences.
"How-ever, the cost of this filtering of different structure examples is that sometimes this leaves no exam-ples."
"Second, the similarity measurement is sometimes not powerful enough. 運 動 場 /yundongchang SPORT-SPACE ‘a sports ground` has a sufficient number of examples, but has problems with the similarity measurement."
The head 場/chang is am-biguous. 場/chang has two senses and both mean space.
One of them means abstract space and the other means physical space.
"Hence, in the CiLin thesaurus 場/chang can be found in C (time and space) and D (abstract)."
Words in C such as 商場 /shangchang
"BUSINESS-SPACE ‘a market’, 屠宰 場 /tuzaichang BUTCHER-SPACE ‘a slaughter house’ , 會場/huichang MEETING-SPACE ‘the place of a meeting’, and in D are 球場/ qiuchang BALL-SPACE ‘a court’, 體 育 場 /tiyuchang PHYSICAL TRAINING-SPACE ‘a stadium’. 運動 場/yundongchang should be more similar to 體育 場/tiyuchang than other space nouns, but the simi-larity score does not show that they are related and C group has more examples."
"Thus, the system chooses C incorrectly."
"Third, the taxonomy of the thesaurus is ambiguous."
"For instance, 體操房/tichaofang GYMNASTICS– ROOM ‘gymnastics room’ has similar examples in both B (object) and D (abstract)."
These two groups are very similar.
"Words in B group include 刑房 /xingfan PUNISHMENT-ROOM ‘punishment room’, 書房/shufan BOOK-ROOM ‘study room’, 暗房/anfan DARK-ROOM ‘dark room’, and 廚房 /chufan"
Words in D are such as 牢房/laofan PRISON-ROOM ‘a jail’ and 彈子房/danzifan BILLIARD-ROOM ‘a bil-liard room’.
There are no obvious features to dis-tinguish between these examples.
"According to the CiLin, 體操房/tichaofang belongs to D, but the classifier predicts it as B class which does not ac-tually differ much with D. Such problems may oc-cur with any semantic taxonomy."
The paper presents an algorithm for classifying the unknown words semantically.
The classifier adopts a nearest neighbor approach such that the distance between an unknown word and examples from the CiLin thesaurus is computed based upon its mor-phological structure.
"The main contributions of the system are: first, it is the first attempt in adding semantic knowledge to Chinese unknown words."
"Second, without contextual information, the system can still successfully classify 65.76% of adjectives, 71.39% of nouns and 52.84% of verbs."
Future work will explore the use of the contextual information of the unknown words and the contex-tual information of the lexicons in the predicted category of the unknown words to boost predictive power.
Coreference resolution systems usually at-tempt to find a suitable antecedent for (al-most) every noun phrase.
"Recent studies, however, show that many definite NPs are not anaphoric."
"The same claim, obviously, holds for the indefinites as well."
"In this study we try  to learn automatically   two  classifications  , and , relevant for this problem."
"We use a small training corpus (MUC-7), but also acquire some data from the Internet."
"Combining our classifiers sequentially, we achieve 88.9% precision and 84.6% recall for discourse new entities."
"We expect our classifiers to provide a good prefiltering for coreference resolution sys-tems, improving both their speed and per-formance."
"Most coreference resolution systems proceed in the following way: they first identify all the possible markables (for example, noun phrases) and   then &quot;! # check $   one %! %(&amp; &apos; by one candidate pairs , trying to find out whether the members of those pairs can be coreferent."
"As the final step, the pairs are ranked using a scoring algo-rithm in order to find an appropriate partition of all the markables into coreference classes."
"Those approaches require substantial processing: in the worst case one has to check )+*,+) 1 0. / candi- date pairs, where is the total number of mark-ables found by the system."
"However, R. Vieira and M. Poesio have recently shown[REF_CITE]that such an exhaustive search is not needed, because many noun phrases are not anaphoric at all — about 24365 of definite NPs in their corpus have no prior referents."
"Obviously, this num-ber is even higher if one takes into account all the other types of NPs — for example, indefinites are almost always non-anaphoric."
We can conclude that a coreference resolution en-gine might benefit a lot from a pre-filtering algo-rithm for identifying non-anaphoric entities.
"First, we save much processing time by discarding at least half of the markables."
"Second, we can hope to re-duce the number of mistakes: without pre-filtering, our coreference resolution system might misclassify a discourse new entity as coreferent to some previ-ous one."
"However, such a pre-filtering can also decrease the system’s performance if too many anaphoric NPs are classified as discourse new: as those NPs are not processed by the main coreference resolution module at all, we cannot find correct antecedents for them."
"Therefore, we are interested in an algo-rithm with a good precision, possibly sacrificing its recall to a reasonable extent."
V. Ng and C. Cardie analysed[REF_CITE]the impact of such a prefiltering on their coreference resolution engine.
It  turned  out that an automatically induced  classifier did not help to improve the overall performance and even decreased it.
"How-ever, when more NPs were considered   anaphoric  (that is, the precision for the 7 class increased and the recall decreased), the prefiltering resulted in improving the coreference resolution."
Several algorithms for identifying discourse new entities have been proposed in the literature.
"R. Vieira and M. Poesio use hand-crafted heuris-tics, encoding syntactic information."
"For exam-ple, the noun phrase “the inequities of the current land-ownership (&quot; system   ” is classified by their sys-tem as 7 , because it contains the restrictive postmodification “of the current land-ownership system”."
This approach leads to 72% precision and 69% recall for definite discourse new NPs.
The system described[REF_CITE]also makes use of syntactic heuristics.
But in ad-dition the authors mine discourse new entities from the corpus.
"Four types of entities can be classified as non-anaphoric: 1. having specific syntactic structure, 2. appearing in the first sentence of some text in the training corpus, 3. exhibiting the same pattern as several expres-sions of type (2), 4. appearing in the corpus at least 5 times and always with the definite article (“definites-only”)."
"Using various combinations of these methods, D. Bean and E. Riloff achieved an accuracy  for 5 def-(F-inite non-anaphoric NPs of about measure), with various combinations of precision and recall. [Footnote_1]"
"1 Bean and Riloff’s non-anaphoric NPs do not correspond to our +discourse new ones, but rather to the union of our +dis-course new and +unique classes."
"This algorithm, however, has two lim-itations."
"First, one needs a corpus consisting of many small texts."
"Otherwise it is impossible to find enough non-anaphoric entities of type (2) and, hence, to collect enough patterns for the entities of type (3)."
"Second, for an entity to be recognized as “definite-only”, it should be found in the corpus at least 5 times."
"This automatically results in the data sparseness problem, excluding many infrequent nouns and NPs."
In our approach we use machine learning to iden-tify non-anaphoric noun-phrases.
We combine syn-tactic heuristics with the “definite probability”.
"Un-like Bean and Riloff, we model definite probability using the Internet instead of the training corpus it-self."
This helps us to overcome the data sparseness problem to a large extent.
"As it has been shown re-cently[REF_CITE], Internet counts pro-duce reliable data for linguistic analysis, correlating well with corpus counts and plausibility judgements."
The rest of the paper is organised as follows: first we discuss our NPs classification.
"In Section 3, we describe briefly various data sources we used."
Sec-tion 4 provides an explanation of our learning strat-egy and evaluation results.
The approach is sum-marised in Section 5.
In our study we follow mainly E. Prince’s classifi-cation of NPs[REF_CITE].
Prince distinguishes between the discourse and the hearer givenness.
"The resulting taxonomy is summarised below: brand new NPs introduce entities which are both discourse and hearer new (“a bus”), sub-class of them, brand new anchored NPs con-tain explicit link to some given discourse entity (“a guy I work with”), unused NPs introduce discourse new, but hearer old entities (“Noam Chomsky”), evoked NPs introduce entities already present in the discourse model and thus discourse and hearer old: textually evoked NPs refer to enti-ties which have already been mentioned in the previous discourse (“he” in “A guy I worked with says he knows your sister”), whereas situ-ationally evoked are known for situational rea-sons (“you” in “Would you have change of a quarter?”), inferrables are not discourse or hearer old, however, the speaker assumes the hearer can infer them via logical reasoning from evoked entities or other inferrables (“the driver” in “I got on a bus yesterday and the driver was drunk”), containing inferrables make this in-ference link explicit (“one of these eggs”)."
For our present study we do not need such an elab-orate classification.
"Moreover, various experiments of Vieira and Poesio show that even humans have difficulties distinguishing, for example, between in-ferrables and new NPs, or trying to find an anchor for an inferrable."
"So, we developed a simple taxon-omy following the main Prince’s distinction between the discourse and the hearer givenness."
"First, we distinguish between discourse new and discourse old entities ( (  . ( An  entity is considered dis-course old ( ) if it refers to an ob-ject or a person mentioned in the previous discourse."
"For example, in “The Navy is considering a new ship that [..]"
"The Navy would like to spend about $ 200 million a year on the arsenal ship..” the first occurrence of  “The  Navy  ” and “a new ship” are classified as 7 , whereas the sec-ond occurrence of “The  Navy  ” and  “the arsenal ship” are classified as ."
"It must be noted that many researchers, in particular, Bean and Riloff, would consider the second “the Navy” non-anaphoric, because it fully specifies its referent and does not require information on the first NP to be in-terpreted successfully."
"However, we think that a link between two instances of “the Navy” can be very helpful, for example, in the Information Extraction task."
Therefore ( ( we  treat those NPs as discourse old.
Our class corresponds to Prince’s textually evoked NPs.
"Second, we distinguish between uniquely and non-uniquely referring  expressions."
Uniquely refer-ring expressions ( 7 ) fully specify their refer-ents and can be successfully interpreted without  any local supportive context.
"Main part of the 7 class constitute entities, known to the hearer (reader) already at the moment when she starts processing the text, for example “The Mount Everest”."
"In ad-dition, an NP (unknown to the reader in the very be-ginning) is considered unique if it fully specifies its referent due to its own content only and thus can be added as it is (maybe, for a very short time) to the reader’s World knowledge base after the processing of the text, for example, ”John Smith, chief exec-utive of John Smith Gmbh” or “the fact that John Smith is a chief executive   of John Smith Gmbh”."
"In Prince’s terms our 7 class corresponds to the unused and, partially, new."
"In our Navy example (cf. above) both occurrences of “The Navy” are consid- ered 7   , whereas “a new  ship  ” and “the ar-senal ship” are classified as ."
In our research we use 20 texts from the MUC-7 corpus[REF_CITE].
The texts were parsed by E. Charniak’s parser[REF_CITE].
Parsing errors were not corrected man-ually.
After this preprocessing step we have 20 lists of noun phrases.
There are discrepancies between our lists and the MUC-7 annotations.
"First, we consider only noun phrases, whereas MUC-7 takes into account more types of entities (for example, “his” in “his posi-tion” should be annotated according to the MUC-7 scheme, but is not included in our lists)."
"Second, the MUC-7 annotation identifies only markables, partic-ipating in some coreference chain."
"Our lists are pro-duced automatically and thus include all the NPs.  We  annotated   automatically our NPs as using the ( following (  simple rule: an NP is considered if and only if it is marked in the original MUC-7 corpus, and it has an antecedent in the MUC-7 corpus (even if this antecedent does not correspond to any NP in our corpus)."
"In  addition  , we annotated our NPs manually as ered 7  ."
The  : following expressions were consid-fully specifying the referent without any local or global context (the chairman[REF_CITE]or Washington).
"We do not take homonymy into account,  so, for  example, Washington is annotated as 7 although it can refer to many different entities: various persons, cities, counties, towns, islands, a state, the government and many others. time expressions that can be interpreted uniquely once some starting time point (global context) is specified."
The MUC-7 corpus con-sists of New York Times News Service articles.
"Obviously, they were designed to be read on some particular day."
"Thus, for a reader of such a text, the expressions on Thursday or tomor-row fully specify their referents."
"Moreover, the information on the starting time point can be easily extracted from the header of the text. expressions, denoting political or administra-tive objects (for example, “the Army”)."
"Al-though such expressions do not fully specify their referents without an appropriate global context (many countries have armies), in an U.S. newspaper they can be interpreted uniquely."
"Overall, we have 3710  noun (  phrases (  . 2628 of them were  annotated   as 7 and 1082 — as   . 2651  NPs were classified as and 1059 — as 7 ."
We provide these data to a machine learning system (Ripper).
Another source of data for our experiments is the World Wide Web.
"To model “definite probabil-ity” for a given NP, we construct various phrases, for example, “the NP”, and send them to the Al-taVista search engine."
Obtained counts (number of pages worldwide written in English and containing the phrases) are used to calculate values for several “definite probability” features (see Section 4.1 be-low).
We do not use morphological variants in this study.
In our  experiments (&quot;   we want to learn   both classifica-tions and automatically.
"However, not every learning algorithm would be ap-propriate due to the specific requirements we have."
"First, we need an algorithm that does not always require all the features to be specified."
"For exam-ple, we might want to calculate “definite probabil-ity” for a definite NP, but not for a pronoun."
"We also don’t want to decide a priori, which features are important and which ones are not in any particular case."
"This requirement rules out such approaches as Memory-based Learning, Naive Bayes, and many others."
"On the contrary, algorithms, providing tree-or rule-based classifications (for example, C4.5 and Ripper) would fulfil our first requirement ideally."
"Second, we want  to control (&quot;  precision-recall  trade-off, at least for the task."
For these reasons we have finally chosen the Ripper learner[REF_CITE].
Our feature set consists currently of 32 features.
They can be divided into three groups: 1.
We encode part of speech of the head word and type of the determiner.
"Several features contain information on the characters, constituting the NP’s string (dig-its, capital and low case letters, special sym-bols)."
We use several heuristics for restrictive postmodification.
"Two types of appositions are identified: with and without commas (“Rupert Murdoch, News Corp.’s chairman and chief ex-ecutive officer,” and “News Corp.’s chairman and chief executive officer Rupert Murdoch”)."
"In the MUC-7 corpus, appositions of the latter type are usually annotated as a whole."
"Char-niak’s parser, however, analyses these construc-tions as two NPs ([‘News Corp.’s chairman and chief executive officer] [Rupert Murdoch])."
Therefore those cases require special treatment. 2. Context Features.
For every NP we calculate the distance (in NPs and in sentences) to the previous NP with the same head if such an NP exists.
"Obtaining values for these features does not require exhaustive search when heads are stored in an appropriate data structure, for ex-ample, in a trie. 3. “Definite probability” features."
"Suppose is a noun phrase, is the same noun phrase without a determiner, and is its head."
"We obtain Internet  counts for “Det Y” and “Det H”, where stays for “the”, “a(n)”, or the empty string."
Then the following ratios are used as features:  ” $  ” $   ” $   ”  ” ”
"We expect our NPs to behave w.r.t. the “defi-nite probability” as follows: pronouns and long proper names are seldom used with any article: “he” was found on the[REF_CITE]times, “the he” — 134978 times (0.3%), and “a he” — 154204 times (0.3%)."
"Uniques (including short proper names) and plural non-uniques are used with the definite article much more of-ten than with the indefinite one: “government” was found 23197407 times, “the government” — 5539661 times (23.9%), and “a govern-ment” — 1109574 times (4.8%)."
"Singular non-unique expressions are used only slightly (if at all) more often with the definite article: “retailer” was found 1759272 times, “the re-tailer” — 204551 times (11.6%), and “a re-tailer” — 309392 times (17.6%)."
We use Ripper to learn the clas-sification from the feature representations described above.
The experiment is designed in the follow-ing way: one text is reserved for testing (we do not want to split our texts and always process them as a whole).
We perform 5-fold cross-validation on these 19 texts in order  to find  the  settings with the best precision for the 7 class.
These settings are then used to train Ripper on all the 19 files and test on the reserved one.
The whole proce-dure is repeated for all the 20 test files and the aver-age precision and recall are calculated.
The parame-ter “Loss Ratio” (ratio of the cost of a false negative to the cost of a false positive) is adjusted separately — we decreased it as much as possible (to 0.3) to have a classification with a good precision and a rea-sonable recall.
"The automatically induced classifier includes, for example, the following rules:"
R2: (applicable to such NPs as “you”)
"IF an NP is a pronoun,"
CLASSIFY it as discourse old.
R14: (applicable to such NPs as “Mexico” or “the Shuttle”)
"IF an NP has no premodifiers, is more often used with “the” than with “a(n)” (the ratio is between 2 and 10), and a same head NP is found within the 18-NPs window,"
CLASSIFY it as discourse old.
The performance is shown in table 1.
"Although the “definite probability” features could not  help  us much to classify NPs as  , we expect them to be useful for identifying unique expressions."
We  conducted  a similar experiment trying to learn a classifier.
"The only difference was in the optimisation strategy: as we did not know a pri-ori, what was more important, we looked for set-tings with the best precision for non-uniques, recall for non-uniques, and overall accuracy (number of correctly classified items of both classes) separately."
The results are summarised in table 2.
Unique and non-unique NPs demonstrate different behaviour w.r.t. the coreference: discourse entities are seldom introduced by vague descriptions and then referred to by fully specifying NPs.
"Therefore we can expect a unique NP to be discourse new, if obvious checks for coreference fail."
"The “obvi-ous checks” include in our case looking for same head expressions and appositive constructions, both of them requiring only constant time."
"On the other hand, unique expressions always have the same or similar form: “The Navy” can be either discourse new or discourse old."
"Non-unique NPs, on the contrary, look differently when introducing entities (for example, “a company” or “the company that . . . ”) and referring to the previ-ous ones (“it” or “the company” without postmod-ifiers)."
Therefore our syntactic features should be much  more  helpful  when classifying non-uniques as .
To investigate this difference we conducted an-other experiment (.
We  split ( our data into two parts — 7 and .
Then we learn the classification for both parts sepa-rately as described in section 4.2.
"Finally the rules are combined, producing a classifier for all the NPs."
The results are summarised in table 3.
"As far as the task is concerned, our system performed slightly, if at all, better with the definite probability features than without them: the improvement in precision (our main criterion) is compensated by the loss in recall."
"However, when only definite NPs are taken into account, the im-provement becomes significant."
"It’s not surprising, as these features bring much more information for definites than  for ( other NPs."
"For the classification our definite prob-ability features were more important, leading to sig-nificantly better results compared to the case when only syntactic and context features were used."
"Al-though the improvement is only about 0.5%, it must be taken into account that overall figures are high: 1% improvement on 90% and on 70% accuracy is not the same."
"We conducted the t-test to check the significance of these improvements, using weighted means and weighted standard deviations, as all the texts have different sizes."
"Table 2 shows in bold performance measures (precision, recall, or F-score) that improve significantly (   3  ) when we use the definite probability features."
"As our third experiment shows, non-unique entities   can  be classified very reliably into classes."
"Uniques, however, have shown quite poor performance, although we expected them to be resolved successfully by heuristics for appositions and same heads."
"Such a low performance is mainly due to the fact that many objects can be referred to by very similar, but not the same unique NPs: “Lockheed Martin Corp.”, “Lockheed Martin”, and “Lockheed”, for example, introduce the same object."
We hope to improve the accuracy by developing more sophisticated matching rules for unique descriptions.
"Although uniques currently perform poorly, the overall classification still benefits (  from the sequen-tial  processing   (identify first, then learn classifiers for uniques and non-uniques separately, and then combine them)."
And we hope to get a better overall accuracy once our matching rules are improved.
We have implemented a system for automatic iden-tification of discourse new and unique entities.
To learn the classification we use a small training cor-pus (MUC-7).
"However, much bigger corpus (the WWW, as indexed by AltaVista) is used to obtain values for some features."
Combining heuristics and Internet counts we are able to achieve 88.9% preci-sion and 84.6% recall for discourse new entities.
Our   system  !    can 4  also reliably classify NPs as.
The accuracy of this clas-sification is about 89–92% with various preci-sion/recall combinations.
"The classifier provide use-ful information  for coreference   resolution in general, as 7 and descriptions exhibit dif-ferent behaviour w.r.t. the anaphoricity."
"This fact is partially reflected by the performance of our se- quential classifier (table 3): the context information is not sufficient to determine whether a unique NP is a first-mention or not, one has to develop sophisti-cated names matching techniques instead."
"We expect our algorithms to improve both the speed and the performance of the main corefer-ence resolution module: once many NPs are dis-carded, the system can proceed quicker and make fewer mistakes (for example, almost all the pars-ing  errors  were  classified by our algorithm as 7 )."
Some issues are still open.
"First, we need sophis-ticated rules to compare unique expressions."
At the present stage our system looks only for full matches and for same head expressions.
"Thus, “China and Taiwan” and “Taiwan” (or “China”, depending on the rules one uses for coordinates’ heads) have much better chances to be considered coreferent, than “World Trade Organisation” and “WTO”."
"We also plan to conduct  more experiments   on the  interaction  between the and classifications  , treating, for example, time , or exploring the  influence  expressions as of various optimisation strategies for on the overall performance of the sequential classifier."
"Finally, we still have to estimate the impact of our pre-filtering algorithm on the overall corefer-ence resolution performance."
"Although we expect the  coreference   resolution  system  to benefit from the and classifiers, this hy-pothesis has to be verified."
"This paper describes our preliminary at-tempt to automatically recognize zero ad-nominals, a subgroup of zero pronouns, in Japanese discourse."
"Based on the corpus study, we define and classify what we call “argument-taking nouns (ATNs),” i.e., nouns that can appear with zero adnomi-nals."
"We propose an ATN recognition al-gorithm that consists of lexicon-based heuristics, drawn from the observations of our analysis."
We finally present the result of the algorithm evaluation and discuss future directions.
(1) Zebras always need to watch out for lions.
"Therefore, even while eating grass, so that able to see behind, eyes are placed at face-side."
This is a surface-level English translation of a naturally occurring “unambiguous” Japanese dis-course.
"By “unambiguous,” we mean that Japa-nese speakers find no difficulty in interpreting this discourse segment, including whose eyes are being talked about."
"Moreover, Japanese speakers find this segment quite “coherent,” even though there seems to be no surface level indication of who is eating or seeing, or whose eyes are being men-tioned in this four-clause discourse segment. [Footnote_1]"
1 This was verified by an informal poll conducted on 15 native speakers of Japanese.
"However, this is not always the case with Japanese as a Second Language (JSL) learners. [Footnote_2]"
2 Personal communication with a JSL teacher.
What constitutes “coherence” has been studied by many researchers.
"Reference is one of the lin-guistic devices that create textual unity, i.e., cohe- si[REF_CITE]."
"Reference also contributes to the semantic continuity and content connectivity of a discourse, i.e., coherence."
"Co-herence represents the natural and reasonable con-nections between utterances that make for easy understanding, and thus lower inferential load for hearers."
The Japanese language uses ellipsis as its major type of referential expression.
Certain elements are ellipted when they are recoverable from a given context or from relevant knowledge.
"These ellip-ses may include verbals and nominals; the missing nominals have been termed “zero pronouns,” “zero pronominals,” “zero arguments,” or simply “zeros” by researchers."
"How many zeros are contained in (1), for ex-ample, largely depends on how zeros are defined."
"In the literature, zeros are usually defined as ele-ments recoverable from the valency requirements of the predicate with which they occur."
"However, does this cover all the zeros in Japanese?"
Does this explain all the content connectivity created by nominal ellipsis in Japanese?
"In this paper, we introduce a subgroup of zeros, what we call “zero adnominals,” in contrast to other well-recognized “zero arguments” and inves-tigate possible approaches to recognizing these newly-defined zeros, in an attempt to incorporate them in an automatic zero detecting tool for JSL teachers that aims to promote effective instruction of zeros."
"In section 2, we provide the definition of zero adnominals, and present the results of their manual identification in the corpus."
Section 3 de-scribes the theoretical and pedagogical motivations for this study.
Section 4 illustrates the syntac-tic/semantic classification of the zero adnominal examples found in the corpus.
"Based on the classi-fication results, we propose lexical information-based heuristics, and present a preliminary evalua-tion."
"In the final two sections, we present related work, and discuss possible future directions."
Recall the discourse segment in (1).
"Japanese is analyzed in (2). (2) a. simauma-wa raion ni itumo zebra-TOP lion-DAT always ki-o-tuke-nakereba-narimasen. watch-out-for-need-to “Zebras always need to watch out for lions.” b. desukara, Ø kusa-o tabete-ite-mo, so Ø-NOM grass-ACC eating-even-while “So even while (they) are eating grass,” c. Ø Ø usiro-no-ho-made mieru-yo-ni Ø-NOM Ø-ADN-behind-even see-can-for “so that (they) can see even what is behind (them),” d. Ø me-ga Ø kao-no-yoko-ni Ø-ADN-eye-NOM Ø-ADN-face-side LOC tuite-imasu. placed-be “(their)eyes are on the sides of (their) faces.”"
"Zero arguments are unexpressed elements that are predictable from the valency requirements of their heads, i.e., a given predicate of the clause."
Zero nominatives in (2b) and (2c) are of this type.
"Zero adnominals, analogously, are missing elements that can be inferred from some features specified by their head nouns."
"A noun for body-part, me ‘eyes’ in (2d) usually calls hearers’ attention to “of-whom” information and hearers recover that in-formation in the flow of discourse."
"That missing information can be supplied by a noun phrase (NP) followed by an adnominal particle no, i.e., si-mauma-no ‘zebras’(= their)’ in the case of (2d) above."
"Hence, as a first approximation, we define a zero adnominal as an unexpressed “NP no” in the NP no NP (a.k.a., A no B) construction."
"Before we proceed, we will briefly describe the corpus that we investigated."
The corpus consists of a collection of 83 written narrative texts taken from seven different JSL textbooks with levels ranging from beginning to intermediate.
"Thus, it is a representative sample of naturally-occurring, but maximally canonical, free-from-deviation, and co-herent narrative discourse."
Our primary goal is to identify relevant informa-tion for recognizing zero adnominals.
"Since such information is unavailable in the surface text, the identification of missing adnominal elements and their referents in the corpus was based on the na-tive speaker intuitions and the linguistic expertise of the author, who used the definition in 2.1, with occasional consultation with a JSL teaching ex-pert/linguist."
"As a result, we located a total of 320 zero adnominals."
These adnominals serve as the zero adnominal samples on which our later analy-sis is based.
"One discourse account that models the perceived degree of coherence of a given discourse in rela-tion to local focus of attention and the choice of referring expressions is centering (e.g., Grosz,[REF_CITE])."
"The investigation of zeros behavior in our cor-pus, within the centering framework, shows that zero adnominals make a considerable contribution to center continuity in discourse by realizing the central entity in an utterance (called Cb) just as well-acknowledged zero arguments do."
Recall example (2).
Its center data structure is given in (3).
The Cf (forward-looking center) list is a set of discourse entities that appear in each utterance (U i ).
"The Cb (backward-looking center) is a special member of the Cf list, and is meant to represent the entity that the utterance is most cen-trally about; it is the most highly ranked element of the Cf (U i-1 ) that is realized in U i . (3) a. Cb: none [Cf: zebra, lion] b."
"Cb: zebra [Cf: zebra, grass] c. Cb: zebra [Cf: zebra, what is behind] d. Cb: zebra [Cf: zebra, eye, face-side]"
"In (3b) and (3c), the Cb is realized as a zero nomi-native, and in (3d), it is realized by the same entity (zebra) as a zero adnominal, maintaining the"
CONTINUE transition that by definition is maxi-mally coherent.
This matches the intuitively per-ceived degree of coherence in the utterance.
"Our corpus contains a total of 138 zero adnominals that refer to previously mentioned entities (15.56% of all the zero Cbs), and realize the Cb of the utter-ance in which they occur, as in (3d=2d)."
"Our corpus study shows that discourse coher-ence can be more accurately characterized, in the centering account, by recognizing the role of zero adnominals as a valid realization of Cbs (see Ya-mura-Takei et al., ms. for detailed discussion)."
This is our first motivation towards zero adnominal recognition.
"This program, Zero Detector (henceforth, ZD) takes Japanese written narrative texts as input and provides the zero-specified texts and their underlying structures as output."
"This aims to draw learners’ and teachers’ attention to zeros, on the basis of a hypothesis about ideal conditions for second language acquisi-tion, by making invisible zeros visible."
"ZD regards teachers as its primary users, and helps them pre-dict the difficulties with zeros that students might encounter, by analyzing text in advance."
"Such dif-ficulties often involve failure to recognize dis-course coherence created by invisible referential devices, i.e., the center continuity maintained by the use of various types of zeros."
"As our centering analysis above indicates, in-clusion of zero adnominals into ZD’s detecting capability enables a more comprehensive coverage of the zeros that contributes to discourse coherence."
This is our project goal.
Group # Definition A: argument I B: nominalized verbal element A: noun denoting an entity II B: abstract relational noun A: noun denoting an entity III B: abstract attribute noun A: nominalized verbal element IV B: argument A: noun expressing attribute V B: noun denoting an entity
Unexpressed elements need to be predicted from other expressed elements.
"Thus, we need to char-acterize B nouns (which are overt) in the (A no) B construction, assuming that zero adnominals (A) are triggered by their head nouns (B) and that cer-tain types of NPs tend to take implicit (A) argu-ments."
Our first approach is to use an existing A no B classification scheme.
"We adopted, from among many A no B works, a classification mod-eled on Shimazu, Naito and Nomura (1985, 1986, and 1987) because it offers the most comprehen-sive classification (Fais and Yamura-Takei, ms)."
Table 1 below describes the five main groups that we used to categorize (A no) B phrases.
We classified our 320 “(A no) B” examples into the five groups described in the previous section.
"Group V comprised the vast majority, while ap-proximately the same percentage of examples was included in Groups I, II and III."
There were no Group IV examples.
The number and percentage of examples of each group are presented in Table 2.
Example[REF_CITE]kotoba no rikai ‘word-no-understanding’ biru no mae ‘building-no-front’ hasi no nagasa ‘bridge-no-length’ kenka no hutari ‘argument-no-two people’ ningen no atama ‘human-no-head’
"We conjecture that certain nouns are more likely to take zero adnominals than others, and that the head nouns which take zero adnominals, ex-tracted from our corpus, are representative samples of this particular group of nouns."
We call them “argument-taking nouns (ATNs).”
ATNs syntacti-cally require arguments and are semantically de-pendent on their arguments.
"We use the term ATN only to refer to a particular group of nouns that can take implicit arguments (i.e., zero adnominals)."
"We closely examined the 127 different ATN tokens among the 320 cases of zero adnominals and classified them into the four types that corre-spond to Groups I, II, III and V in Table 1."
"Type Syntactic properties Semantic properties I Nominalized verbal, de- Humanactivity rived (from verb) noun, phenomenon common noun II formal noun, common Location then listed their syntactic/semantic properties based on the syntactic/semantic properties pre-sented in the Goi-Taikei Japanese Lexicon (hereaf-ter GT, Ikehara, Miyazaki, Shirai, Yokoo, Nakaiwa, Ogura, Oyama, and[REF_CITE])."
"GT is a se-mantic feature dictionary that defines 300,000 nouns based on an ontological hierarchy of ap-proximately 2,800 semantic attributes."
It also uses nine part-of-speech codes for nouns.
Table 3 lists the syntactic/semantic characterizations of the nouns in each type and the number of examples in the corpus.
What bold means in the table will be explained later in section 4.3.
"When we examine these four types, we see that they partially overlap with some particular types of nouns studied theoretically in the literature."
"These cover sub-sets of our Type I, II, III and V. In computational work, Bond, Ogura, and[REF_CITE]extracted 205 “trigger nouns” from a corpus aligned with English."
These nouns trigger the use of possessive pronouns when they are machine-translated into English.
They seem to correspond mostly to our Type V nouns.
Our result offers a comprehensive coverage which subsumes all of the types of nouns discussed in these accounts.
"Next, let us more closely look at the properties expressed by our samples."
The most prevalent ATNs (21 in number) are nominalized verbals in the semantic category of human activity.
"The next most common are kinship nouns (14 in number) and body-part nouns (14), both in the common noun category; location nouns (13), either in the common noun or formal noun category; and nouns that express amount (9) whose syntactic category is either common or de-adjectival."
"The others in-clude some “human” subcategories, etc."
"The part-of-speech subcategory, “nominalized verbal” (sahen-meishi) is a reasonably accurate indicator of Type 1 nouns."
"So is “formal noun” (keishiki-meishi) for Type II, although this does not offer a full coverage of this type."
Numeral noun and counter suffix noun compounds also represent a major subset of Type III.
"Semantic properties, on the other hand, seem helpful to extract certain groups such as location (Type II), amount (Type III), kinship, body-part, organization, and some human subcategories (Type V)."
But other low-frequency ATN samples are problematic for determining an appropriate level of categorization in GT’s semantic hierarchy tree.
Our goal is to build a system that can identify the presence of zero adnominals.
"In this section, we propose an ATN (hence zero adnominal) recogni-tion algorithm."
"The algorithm consists of a set of lexicon-based heuristics, drawn from the observa-tions in section 4.2."
The algorithm takes morphologically-analyzed text as input and provides ATN candidates as out-put.
"The process consists of the following three phases: (i) bare noun extraction, (ii) syntactic cate-gory (part-of-speech) checking, and (iii) semantic category checking."
Zero adnominals usually co-occur with “bare nouns.”
"Bare nouns, in our definition, are nouns without any pre-nominal modifiers, including de-monstratives, explicit adnominal phrases, relative clauses, and adjectives. [Footnote_3] Bare nouns are often sim-plex as in (4a), and sometimes are compound (e.g., numeral noun + counter suffix noun) as in (4b)."
3 Japanese do not use determiners for its nouns.
"These are immediately followed by case-marking, topic/focus-marking or other particles (e.g., ga, o, ni, wa, mo). (4) a. atama-ga head-NOM b. 70-paasento-o 70-percent-ACC"
The extracted nouns under this definition are initial candidates for ATNs.
"Once bare nouns are identified, they are checked against our syntactic-property- (i.e., part-of-speech, POS) based-, followed by semantic-attribute (SEM) based-heuristics."
"For semantic filtering, we decided to use the noun groups of high frequency (more than two tokens categorized in the same group; indicated in bold in Table 3 above) to minimize a risk of over-generalization."
"The algorithm checks the following two condi-tions, for each bare noun, in this order: [1] If POS = [nominalized verval, derived noun, formal noun, numeral + counter suffix com-pound], label it as ATN. [2] If SEM = [2610: location, 2585: amount, 362: organization, 552: animate (part), 111: hu-man (relation), 224: human (profession), 72: human (kinship), 866: housing (part), 813: cloth-ing], label it as ATN. 4"
"Therefore, nouns that pass condition [1] are labeled as ATNs, without checking their semantic proper-ties."
A noun that fails to pass condition [1] and passes condition [2] is labeled as ATN.
A noun that fails to match both [1] and [2] is labeled as non-ATN.
Consider the noun sintyo ‘height’ for example.
"Its POS code in GT is common noun, so it fails condition [1] and goes to [2]."
"This noun is categorized in the “2591: measures” group which is under the “2585: amount” node in the hierarchy tree, so it is labeled as ATN."
"In this way, the algo-rithm labels each bare noun as either ATN or non- ATN."
"To assess the performance of our algorithm, we ran it by hand on a sample text. [Footnote_5]"
"5 This is taken from the same genre as our corpus for the initial analysis, i.e., another JSL textbook."
The test corpus con-tains a total of 136 bare nouns.
We then matched the result against our manually-extracted ATNs (34 in number).
"The result is shown in Table [Footnote_4] below, with recall and precision metrics."
4 These numbers indicate the numbers assigned to each seman-tic category in Goi-Taikei Japanese Lexicon (GT).
"As a baseline measurement, we give the accuracy for classifying every bare noun as ATN."
"For comparison, we also provide the results when only either POS-based or semantic-based heuristics are applied."
Semantic categories make a greater contribution to identifying ATNs than POS.
"However, the POS/Semantic algorithm achieved a higher recall but a lower precision than the semantic-only algo-rithm did."
This is mainly because the former pro-duced more over-detected errors.
"Closer examination of those errors indicates that most of them (8 out of 9 cases) involve verbal idiomatic expressions that contain ATN candidate nouns, as example (5) shows."
"Although me ‘eye’ is a strong ATN candidate, as in example (2) above, case (5) should be treated as part of an idiomatic expression rather than as a zero adnominal expression. [Footnote_6] Thus, we decided to add another condition, [0] below, before we apply the POS/SEM checks."
"6[REF_CITE]also list “idiom” as one use of defi-nite descriptions (English equivalent to Japanese bare nouns), along with same head/associative anaphora, etc."
"The revised algorithm is as follows: [0] If part of idiom in [idiom list], [Footnote_7] label it as non-ATN. [1] If POS = [nominalized verval, derived noun, formal noun, numeral + counter suffix com-pound], label it as ATN. [2] If SEM = [2610: location, 2585: amount, 362: organization, 552: animate (part), 111: hu-man (relation), 224: human (profession), 72: human (kinship), 866: housing (part), 813: cloth-ing], label it as ATN."
"7 The list currently includes eight idiomatic samples from the test data, but it should of course be expanded in the future."
"When a noun matches condition [0], it will not be checked against [1] and [2]."
"When this applies, the evaluation result is now as shown below."
"The revised algorithm, with both syntac-tic/semantic heuristics and the additional idiom-filtering rule, achieved a precision of 96.96%."
"The result still includes some over/under-detecting er-rors, which will require future attention."
"Associative anaphora (e.g.,[REF_CITE]) and indirect anaphora (e.g.,[REF_CITE]) are virtually the same phenomena that this paper is concerned with, as illustrated in (6). (6) a. a house – the roof b. ie ‘house’ – yane ‘roof’ c. ie ‘house’ – (Ø-no) yane ‘(Ø’s) roof’"
"We take a zero adnominal approach, as in (6c), because we assume, for our pedagogical purpose discussed in section 3.2, that zero adnominals, by making them visible, more effectively prompt peo-ple to notice referential links than lexical relations, such as meronymy in (6a) and (6b)."
"However, insights from other approaches are worth attention."
"There is a strong resemblance between bare nouns (that zero adnominals co-occur with) in Japanese and definite descriptions in Eng-lish in their behaviors, especially in their referen-tial properties[REF_CITE]."
The task of classifying several different uses of definite de-scriptions[REF_CITE]is somewhat analogous to that for bare nouns.
Determining definiteness of Japanese noun phrases[REF_CITE][Footnote_8] is also relevant to ATN (which is definite in nature) recognition.
8 Their interests are in machine-translation of Japanese into languages that require determiners for their nouns.
"We have proposed an ATN (hence zero adnomi-nal) recognition algorithm, with lexicon-based heu-ristics that were inferred from our corpus investigation."
The evaluation result shows that the syntactic/semantic feature-based generalization (using GT) is capable of identifying potential ATNs.
"The evaluation on a larger corpus, of course, is essential to verify this claim."
Implemen-tation of the algorithm is also in our future agenda.
"This approach has its limitations, too, as is pointed out[REF_CITE]."
"One limi-tation is illustrated by a pair of Japanese nouns, sakusya ‘author’ and sakka ‘writer,’ which fall un-der the same GT semantic property group (at the deepest level). [Footnote_9] These nouns have an intuitively different status for their valency requirements; the former requires “of-what work” information, while the latter does not. [Footnote_10]"
9 This example pair is taken[REF_CITE].
10 This intuition was verified by an informal poll conducted on seven native speakers of Japanese.
"We risk over- or under-generation when we designate certain semantic properties, no matter how fine-grained they might be."
We proposed the idiom-filtering rule to solve one case of over-detection.
A larger-scale evalua-tion of the algorithm and its error analysis might lead to additional rules that refine extracted ATN candidates.
Insights from the works presented in the previous section could also be incorporated.
"Determining an appropriate level of generaliza-tion is a significant factor for this type of approach, and this was done, in this study, according to our introspective judgments."
More systematic methods should be explored.
"A related issue is the notoriously hard-to-define argument-adjunct distinction for nouns, which is closely related to the distinction between ATNs and non-ATNs."
We experimentally tested seven native-Japanese-speaking subjects in distinguish-ing these two.
"There were six nouns which all the subjects agreed on categorizing as ATNs, in-cluding sakusha ‘author.’"
"Five nouns, including sakka ‘writer,’ on the other hand, were judged as non-ATNs by all the subjects."
"For the remaining 15 nouns, however, their judgments varied widely."
This distinction might largely depend on the context in some cases.
This is also something we will need to address.
"In this study, we focused on “implicit argu-ment-taking nouns.”"
There may be a line (al-though it may be very thin) between nouns which take explicit arguments and those which take im-plicit arguments.
This distinction also needs fur-ther investigation in the corpus.
The development of multi-channel digital broadcasting has generated a demand not only for new services but also for smart and highly functional capabilities in all broadcast-related devices.
This is espe-cially true of the television receivers on the viewer&apos;s side.
"With the aim of achiev-ing a friendly interface that anybody can use with ease, we built a prototype inter-face system that operates a television through voice interactions using natural language."
"At the current stage of our re-search, we are using this system to inves-tigate the usefulness and problem areas of the spoken dialogue interface for televi-sion operations."
"In Japan, the television reception environment has become quite diverse in recent years."
"In addition to analog broadcasts, BS (Broadcast Satellite) digital television and data broadcasts have been operating since 2000."
"At the same time, TV operations for receiving such broadcasts are becoming increas-ingly complex, and an ever increasing variety of peripheral devices such as video tape recorders, disk recorders, DVD players, and game consoles are now being connected to televisions, and operat-ing such devices with different kinds of interfaces is becoming troublesome not only for the elderly but for general users as well[REF_CITE]."
Recently we conducted a usability test targeting data broadcasts in BS digital broadcasting.
The results of the test revealed that many subjects had trouble accessing hierarchically arranged data.
This finding revealed the need for an easy means of accessing desired programs.
One such means is a spoken natural language dialogue (here-after spoken dialogue) interface for TV operations.
"If spoken dialogue could be used to select and search for programs, to operate peripheral devices, and to give information in reply to system queries, we can envisage such an interface as being ex-tremely valuable in a multi-channel and multi-service function viewing environment."
"With this in mind, we have set out to build an interface system that could operate a television via spoken dialogue in place of manual operations."
"Assuming that a television is intelligent enough to understand the words spoken by a human, what kind of language expressions would a user use to give commands to that television?"
"In other words, it is important that the words spoken by a user in such a situation be carefully examined when de-signing a television interface using spoken dia-logues."
Therefore first we built an experimental environment that would enable us to collect dia-logue data based on WOZ (Wizard of OZ) method.
"We set up a television-operation environment ac-cording to the WOZ framework in which the sub-jects were instructed that “the character appearing on the television screen can understand anything you say, and that the character will operate the television for you.”"
"The number of channels that could be selected was 19, and screens displaying Electronic Program Guide (EPG) and user interface for program searching were presented as needed[REF_CITE]."
"This WOZ environment required two operators, one in charge of voice responses and the other of user interface operations."
The voice-response op-erator returns a voice response to the subject by a speech synthesizer after selecting a reply from about 50 previously prepared statements or input-ting replies directly from a keyboard.
"If the subject happens to be silent, the operator returns a re-sponse that introduces new services or prompts the subject to say something."
"The user interface opera-tor first determines what the subject wants, and then manipulates user interface or EPG and per-forms basic television operations such as changing channels."
"The subjects selected for data collection con-sisted of 10 men and 10 women ranging in age from 24 to 31 (average age: 28.7), and each was allowed to speak freely with the television for 5 minutes under an assumption that the “television has a certain amount of intelligence.”"
Figure 1 shows an example of dialogue data re-corded during a WOZ session.
"On analyzing col-lected utterances made by the subjects (1,268 utterances in total), it was found that 83% of user utterances concerned requests made to the televi-sion, and that 89% of those requests included words belonging to specific categories such as program title, genre, performer, station, time, and TV operation commands."
"Here, we consider the following reason why most utterances belonged to specific categories despite the fact that a variety of request could be made."
"In this system, TV program- and operation-related information is displayed on the television screen, and based on this information, subjects tended to underestimate television capability and to omit utterances not dealing with service functions they saw as possible."
It is also thought that the conventional image of television inside subjects’ minds served to restrict user utterances.
"As a part of this WOZ experiment, we also had the subjects fill out a questionnaire with regards to television operations by using spoken dialogue interface."
"When asked to give an opinion on oper-ating a television by voice, more than half replied “Yes, I would like to” therefore apparently indicat-ing a high demand for the spoken dialogue inter-face."
"On the other hand, most subjects that replied “No, I would not like to” gave simple embarrass-ment at speaking out loud as one reason and a re-luctance to vocalize commands when watching television together with their families as another."
"In this regard, we think that embarrassment could probably be reduced through user experience and appropriate environment configuration."
"Based on the results of the data analysis, we built a prototype system that enables television operations via spoken dialogue."
Figure 2 shows the configura-tion of this system.
The system allows users to se-lect real-time broadcast programs from 19 channels.
It also enables the presentation of program in- formation obtained from the Internet or overlaid data in digital broadcasts; the scheduling of pro-gram recording; and the browsing of program-related information from Internet.
All of these functions can be operated through spoken natural language interactions.
The main processing mod-ules of the system are described below.
"The user makes operation requests to interface ro-bot (IFR) as shown in Figure 3, and the IFR oper-ates the television accordingly for the user."
"The IFR is equipped with a super-unidirectional micro-phone and a speaker, and communicates and acti-vates the speech recognition and voice synthesis, and dialogue processing of the system."
The IFR has been given the appearance of a stuffed animal.
One advantage of this IFR is that it can be directly touched and manipulated to create a feeling of warmth and closeness.
"On hearing a greeting or being called by its name, the IFR opens its eyes and enters a state that can perform various operations."
"For example, the IFR can assist the user search for a program, can present information about any program on the tele-vision screen, and can return voice responses."
The speech recognition module uses an algorithm that can finalize recognition results in a sequential manner for a real-time operation and a high speech recognition rate.
"When applying this module to a news program, a speech recognition rate of about 95% can be obtained[REF_CITE]."
"In speech that occurs during television opera-tions, the words such as program titles, names of broadcast stations, names of entertainers and etc. have a high probability of occurring and are also updated frequently."
"For this reason, newly acquired word-lists are automatically registered in a diction-ary on a daily basis."
"In addition, as program titles often consist of multiple words, it is necessary to register them as a single word in order to improve the recognition rate."
"Despite several additional forms of tuning, it is still difficult to achieve perfect results with current speech recognition technology."
"To enable feedback to be given to the user at the time of erroneous rec-ognition, results of recognition are always dis-played on the lower left corner of the television screen."
"In dialogue processing, it is generally difficult to understand intent by performing only a lexical analysis of speech."
"If we limit tasks to dialogue used in television operation, the words spoken by a user have a high probability of falling into specific categories such as program name, as indicated by the results of the data analysis described in 2.2."
"As a consequence, user intent can be inferred from a combination of specific categories and predicates."
"From the viewpoint of processing speed, process-ing can be performed in real time if we use pattern-base approach."
This approach is also used in other dialogue systems such as PC-based agent televi-sion systems in the (FACTS) project and[REF_CITE].
The dialogue processing module performs real-time morphological analysis of input statements from the speech recognition module.
A statement is then identified by pattern matching in units of morphemes and the meaning ascribed beforehand to that statement is obtained.
An example of such pattern is shown in Figure 4 using the meta-characters listed in Table 1:
"In the pattern matching process, categories im-portant to television operations are stored as slots."
Table 2 lists these category-slots and examples of their members.
The words stored in these slots are then used as a basis for generating television op-eration commands and search expressions to access the TV program database.
"Response statements to input statements may take various forms depending on the patterns and current circumstances, and they are here generated by taking into account slot in-formation, response history, results of searching for program information."
We have built a spoken dialogue system based on the results of a WOZ experiment with the aim of achieving a television operation interface easy enough for anybody to use.
"In the preliminary system operation test, 5 sub-jects were asked to give some examples of TV pro-grams that they watch at home, and to use this system to see whether they could obtain informa-tion in relation to those programs."
Results of this test showed that all subjects could access informa-tion on desired programs.
"In a subsequent ques-tionnaire, moreover, all subjects stated that “program selection was easy, and particularly there was no need to know about hierarchical structure of program information.”"
"On the other hand, the test also revealed that some issues remain to be addressed in speech rec-ognition but that a favorable evaluation could be obtained from all subjects with regard to television operations via spoken dialogue."
We are currently conducting even more detailed experiments to demonstrate the usefulness of a spoken dialogue interface for television control and to examine problem areas.
This position paper argues for an interactive approach to text understanding.
The proposed model extends an existing semantics-based text authoring system by using the input text as a source of information to assist the user in re-authoring its content.
The approach per-mits a reliable deep semantic analysis by combining automatic information extraction with a minimal amount of human interven-tion.
Answering emails sent to a company by its cus-tomers — to take just one example among many similar text-processing tasks — requires a reli-able understanding of the content of incoming messages.
"This understanding can currently only be done by humans, and represents the main bot-tleneck to a complete automation of the process-ing chain: other aspects could be delegated to such procedures as database requests and text generation."
Current technology in natural lan-guage understanding or in information extraction is not at a stage where the understanding task can be accomplished reliably without human inter-vention.
"In this paper, which aims at proposing a fresh outlook on the problem of text understanding rather than at describing a completed implemen-tation, we advocate an interactive approach where: 1."
The building of the semantic representation is under the control of a human author; 2.
"In order to build the semantic representa-tion, the author interacts with an intuitive textual interface to that representation (obtained from it through an NLG process), where some “active” regions of the text are associated with menus that display a number of semantic choices for incre-menting the representation; 3."
"The raw input text to be analyzed serves as a source of information to the authoring system and permits to associate likelihood levels with the various authoring choices; in each menu the choices are then ranked according to their likeli-hood, allowing a speedier selection by the au-thor; when the likelihood of a choice exceeds a certain threshold, this choice is performed auto-matically by the system (but in a way that re-mains revisable by the author). 4."
"The system acts as a flexible understanding aid to the human operator: by tuning the thresh-old at a low level, it can be used as a purely automatic, but somewhat unreliable, information extraction or understanding system; by tuning the threshold higher, it can be used as a powerful interactive guide to building a semantic interpre-tation, with the advantage of a plain textual inter-face to that representation that is easily accessible to general users."
The paper is organized as follows.
"In section 2, we present a document authoring system, MDA, where the author constructs an internal semantic representation, but interacts with a tex-tual realization of that representation."
"In section 3, we explain how such a system may be ex-tended into an Interactive Text Understanding (ITU) aid."
A raw input document acts as an in-formation source that serves to rank the choices proposed to the author according to their likeli-hood of “accounting” for information present in the input document.
"In section 4, we present cur-rent work on using MDA for legacy-document normalization and show that this work can pro-vide a first approach to an ITU implementation."
"In section 5, we indicate some links between these ideas and current work on interactive statis-tical MT (TransType), showing directions to-wards more efficient implementations of ITU."
"The MDA (Multilingual Document Authoring) system [[REF_CITE]] is an instance (de-scended from Ranta’s Grammatical Framework [[REF_CITE]]) of a text-mediated interactive natural language generation system, a notion in-troduced by [[REF_CITE]] under the name of WYSIWYM."
"In such systems, an author gradually constructs a semantic representation, but rather than accessing the evolving representa-tion directly, she actually interacts with a natural language text generated from the representation; some regions of the text are active, and corre-spond to still unspecified parts of the representa-tion; they are associated with menus presenting collections of choices for extending the semantic representation; the choices are semantically ex-plicit and the resulting representation contains no ambiguities."
"The author thus has the feeling of only interacting with text, while in fact she is building a formal semantic object."
"One applica-tion of this approach is in multilingual authoring: the author interacts with a text in her own lan-guage, but the internal representation can be used to generate reliable translations in other lan-guages."
Fig. 1 gives an overview of the MDA architecture and Fig. 2 is a screenshot of the MDA interface.
Fig. 1: Authoring in MDA.
"A “semantic grammar” defines an enumerable collection of well-formed partial semantic structures, from which an output text containing active re-gions is generated, with which the author interacts."
Fig. 2: Snapshot of the MDA system applied to the author-ing of drug leaflets.
"In the current MDA system, menu choices are ordered statically once and for all in the semantic grammar [Footnote_1] ."
"1 While the order between choices listed in a menu does not vary, certain choices may be filtered out depending on the current authoring context; this mechanism relies on unifica-tion constraints in the semantic grammar."
"However, consider the situation of an author producing a certain text while using some input document as an informal reference source."
It would be quite natural to assume that the au-thoring system could use this document as a source of information in order to prime some of the menu choices.
"Thus, when authoring the description of a phar-maceutical drug, the presence in the input docu-ment of the words tablet and solution could serve to highlight corresponding choices in the menu corresponding to the pharmaceutical form of the drug."
"This would be relatively simple to do, but one could go further: rank menu choices and as-sign them confidence weights according to tex-tual and contextual hints found in the input document."
"When the confidence is sufficiently high, the choice could then be performed auto-matically by the authoring system, which would produce a new portion of the output text, with the author retaining the ability of accepting or reject-ing the system’s suggestion."
"In case the confi-dence is not high enough, the author’s choice would still be sped up through displaying the most likely choices on top of the menu list."
"This kind of functionality is what we call a text-mediated interactive text understanding system, or for short, an ITU system (see Fig. 3). [Footnote_2]"
"2 Note that we do not demand that the semantic representa-tion built with an ITU system be a complete representation of the input document, rather it can be a structured descrip-tion of some thematic aspects of that document. Similarly, it is OK for the input document not to contain enough infor-mation permitting the system or even the author to “answer” certain menus: then some active regions of the output text remain unspecified."
We will now consider some directions to im-plement an ITU system.
A first route towards achieving an ITU system is through an extension of ongoing work on docu-ment normalization [[REF_CITE]].
The departure point is the following.
"Assume an MDA system is available for author-ing a certain type of documents (for instance a certain class of drug leaflets), and suppose one is presented a “legacy” document of the same type, that is, a document containing the same type of information, but produced independently of the MDA system; using the system, a human could attempt to “re-author” the content of the input legacy document, thus obtaining a normalized version of it, as well as an associated semantic representation."
An attempt to automate the re-authoring proc-ess works as follows.
Consider the virtual space of semantic representations enumerated by the MDA grammar.
"For each such representation, produce, through the standard MDA realization process [Footnote_3] a certain more or less rough “descriptor” of what the input text should contain if its con-tent should correspond to that semantic represen-tation; then define a similarity measure between this descriptor and the input text; finally perform an admissible heuristic search [[REF_CITE]] of the virtual space to find the semantics whose de-scriptor has the best similarity with the input text."
"3 Which was initially designed to produce parallel texts in several languages, but can be easily adapted to the produc-tion of non-textual “renderings” of the semantic representa-tions."
"This architecture can accomodate more or less sophisticated descriptors: from bags of content-words to be intersected with the input text, up to predicted “top-down” predicate-argument tuples to be matched with “bottom-up” tuples extracted from the input text through a rough information-extraction process."
"Up to now the emphasis of this work has been more on automatic reconstruction of a legacy document than on interaction, but we have re-cently started to think about adapting the ap-proach to ITU."
The heuristic search that we mentioned above associates with a menu choice an estimate of the best similarity score that could be obtained by some complete semantic structure extending that choice.
"It is then possible to rank choices according to that heuristic estimate (or some refinement of it obtained by deepening the search a few steps down the line), and then to propose to the author a re-ranked menu."
"While we are currently pursuing this promis-ing line of research because of its conceptual and algorithmic simplicity, it has some weaknesses."
"It relies on similarity scores between an input text and a descriptor that are defined in a some-what ad hoc manner, it depends on parameters that are fixed a priori rather than by training, and it is difficult to associate with confidence levels having a clear interpretation."
A way of solving these problems is to move towards a more probabilistic approach that com-bines advantages of being built on accepted prin-ciples and of having a well-developed learning theory.
We finally turn our attention to existing work in this area that holds promise for improv-ing ITU.
Recent research on the interactive statistical ma-chine translation system TransType [[REF_CITE]] holds special interest in relation to ITU.
"This system, outlined in Fig. 4, aims at helping a translator type her (uncon-strained) translation of a source text by predict-ing sequences of characters that are likely to follow already typed characters in the target text; this prediction is done on the basis of informa-tion present in the source text."
"The approach is similar to standard statistical MT [Footnote_4] , but instead of producing one single best translation, the system ranks several completion proposals according to a probabilistic confidence measure and uses this measure to optimize the length of completions proposed to the translator for validation."
"4 Initially statistical MT used a noisy-channel approach [[REF_CITE]]; but recently [[REF_CITE]] have introduced a more general framework based on the maxi-mum-entropy principle, which shows nice prospects in terms of flexibility and learnability. An interesting research thread is to use more linguistic structure in a statistical translation model [[REF_CITE]], which has some relevance to ITU since we need to handle structured semantic data."
"Evalua-tions of the first version of TransType have al-ready shown significant gains in terms of the number of keystrokes needed for producing a translation, and work is continuing for making the approach effective in real translation envi-ronments."
"If we now compare Fig. 3 and Fig. 4, we see strong parallels between TransType and ITU: language model enumerating word sequences vs grammar enumerating semantic structures, source text vs input text as information sources, match between source text and target text vs match between input text and semantic structure."
"In TransType the interaction is directly with the target text, while in ITU the interaction with the semantic structure is mediated through an output text realization of that structure."
"We can thus hope to bring some of the techniques developed for TransType to ITU, but let us note that some of the challenges are different: for instance train-ing the semantic grammars in ITU cannot be done on a directly observable corpus of texts. [Footnote_5] Fig. 4: TransType."
"5 Let us briefly mention that we are not the first to note for-mal connections between natural language understanding and statistical MT. Thus, [[REF_CITE]], working in a non-interactive framework, draws the following parallel between the two tasks: while in MT, the aim is to produce a target text from a source text, in NLU, the aim is to produce a semantic representation from an input text. He then goes on to adapt the conventional noisy channel MT model of [[REF_CITE]] to NLU, where extracting a semantic representation from an input text corresponds to finding: argmax(Sem) {p(Input|Sem) p(Sem)}, where p(Sem) is a model for generating semantic representations, and p(Input|Sem) is a model for the relation between semantic representations and corresponding texts. See also [[REF_CITE]] and [[REF_CITE]] for paral-lels between statistical MT and Information Retrieval and Summarization respectively. On a different plane, in the context of interactive NLG, [[REF_CITE]] has recently proposed to rank semantic choices according to probabilities estimated from a corpus; but here the purpose is not text understanding, but improving the speed of authoring a new document from scratch."
"We have introduced an interactive approach to text understanding, based on an extension to the MDA document authoring system."
ITU at this point is more a research program than a com-pleted realization.
However we think it repre-sents an exciting direction towards permitting a reliable deep semantic analysis of input docu-ments by complementing automatic information extraction with a minimal amount of human in-tervention for those aspects of understanding that presently resist automation.
We demonstrate a text to sign language translation system for investigating sign language (SL) structure and assisting in production of sign narratives and informa-tive presentations [Footnote_1] .
"1 This work is incorporated within ViSiCAST, an EU Frame-work V supported project which builds on work supported by the UK Independent Television Commission and Post Office."
The system is demon-strable on a conventional PC laptop com-puter.
During the last half century sign languages have been recognized as genuine languages.
"Thus sign languages are now accepted as minority languages, which coexist with majority languages[REF_CITE]and which are the native languages for many deaf people."
Provision of information ac-cess and services in signed languages is as impor-tant as in other minority languages.
"Such provision, however, introduces theoretical and technical chal-lenges."
The use of a sign language gesture nota-tion to drive virtual humans (avatars) for present-ing signing has been investigated[REF_CITE].
Semi-automatic translation system from individual English sentences to such a sign language gesture notation has been demonstrated (self identifyinh ref-erences).
"Here, extension of this system to handle location of nominals at positions in the three dimen-sional space in front of the signer and noun verb agreement involving such allocated positions is de-scribed and illustrated."
Sign Languages (SLs) involve simultaneous manual and non-manual components for conveying mean-ing.
"Non-manual features are comprised of the pos-ture of the upper torso, the orientation of the head and facial expressions."
"Manual features have been often been decomposed as hand-shape, hand orienta-tion, hand position and moti[REF_CITE]."
The Ham-burg Notation System (HamNoSys)[REF_CITE]is an established phonetic transcription system for SLs comprising more than 200 iconically motivated symbols to describe these manual and non-manual features of signs.
The manual components of signs are constrained to occur within signing space.
"Signing space is the three-dimensional space in front of the signer which extends vertically from above the signer’s head to waist level, and horizontally from touching/close to the body to at arm’s length in front of and to the side of the signer."
Signs can be categorised in terms of the ways they use signing space.
Body anchored and fixed nominal and verbal signs are either signed at a fixed body location or involve internal motion which allow relatively little modification to the sign.
"In contrast, some nominal signs can be signed at varying locations and thus the location where they are signed has significance."
"Furthermore, directional verbs allow grammatical and semantic information to be encoded within signing space such that the spe-cific start and/or end positions of these signs have syntactic and semantic significance[REF_CITE]."
"A further distinction can be made between topo-graphic and syntactic use of space (Klima and Bel- lugi, 1979;[REF_CITE])."
"In the case of the former, signing space is used to gesture towards and point at objects and persons physically present and thus has similar-ities with body anchored signs where the location at which a sign is made has an iconic/deictic function."
"However, in cases where the signer describes rela-tionships between objects and persons which are not present, position within signing space can be used to denote abstract referents."
"Similarities between to-pographic and syntactic uses are apparent and of-ten there is overlap between the two, and there is some evidence to suggest that, contrary to expecta-tions, the granularity of the two may be comparable[REF_CITE]."
As our concerns are with transla-tion from English text to sign language (and hence physical presence is not an issue) we concentrate on the syntactic uses of signing space.
The architecture of the English text to British Sign Langauge (BSL) system is essentially a pipeline of four main translation stages 1.
"English syntactic parsing, 2. Discourse Representation Structure (DRS) generation, 3. Semantic transfer, 4. Generation of HamNoSys SL phonetic descriptions, as illustrated in Figure 1."
English text (Figure 2 top left) is parsed by the Carnegie Mellon University (CMU) link grammar parser[REF_CITE]to produce an appropriate linkage which characterises syntactic dependencies (Figure 2 bottom left).
"In cases where multiple linkages are generated, the user intervenes to select an appropriate linkage."
From a CMU parser generated linkage a Discourse Representation Structure DRS[REF_CITE]is generated to capture the semantic content of the text (Figure 2 top middle).
"DRSs allow iso-lation of specific semantic content (nominal, verbal and adjectival based predicates, discourse referents and temporal relationships)."
"Anaphora resolution is used to associate pronouns with discourse referents, and reuse of nouns is used to imply co-reference to the same linguistic referent."
"Currently, the most common 50% CMU links are transformed into DRS form."
An English oriented DRS is transformed into a SL oriented DRS.
"In particular, the number of argu-ments for some predicates is modified to a different number of arguments expected of a corresponding SL sign."
"For example, the English verb move obli-gatorily requires only one argument but is often ac-companied by optional adjuncts for the source and destination locations."
Its BSL equivalent (glossed as MOVE) requires three arguments - the start and end sign space positions and a (classifier or default) handshape consistent with the object being moved.
Such transformations are effected on the DRS.
The DRS is then transformed to an equivalent
HPSG semantic structure which is the starting point for SL generation.
A SL grammar and lexicon are used to drive deriva-tion of a HamNoSys phonetic description of a sign sequence from the HPSG semantic structure (Fig-ure 2 bottom middle).
The BSL lexicon contains ap-proximately 250 lexical items.
"Some lexical items are fully instantiated forms for fixed and body-anchored signs, however others are only partially in-stantiated forms for directional verbs and forms of modulation of lexical items."
"For nominal oriented signs, classifiers are associated with signs, and for directional verbs the lexical entries require incorpo-ration of specific forms of classifiers and sign space locations."
The SL grammar constitutes a collection of simul- taneous constraints which the phonology and syntax of selected signs must satisfy in order to constitute a valid sign sequence.
"These constraints enforce ap-propriate sign order, for example realising a topic comment ordering signs for the English sentence ” I saw an exciting video.”"
The resulting HamNoSys sign sequence descriptions are realised visually as virtual human behaviour[REF_CITE](Figure 2 bottom right) [Footnote_2] .
"2 The avatar illustrated was developed by Televirtual, Nor-wich UK and its HamNoSys interface by UEA colleagues within ViSiCAST."
"Cur-rently, the SL generation sub-system incorporates a lexicon and grammar whose coverage are represen-tative of a number of interesting SL phenomena and whose semantic, syntactic and phonological formal-isation is one of the most advanced SL characteri-sations available."
Such detail is essential to enable visualisation by a virtual human.
"The main omis-sion in the system currently is the absence of non-manual components of signing, though the SL gen-eration has been designed to be extended in this di-rection in the future."
The functionality of the system is demonstrable on a laptop computer.
This paper presents a novel information sys-tem integrating advanced information extrac-tion technology and automatic hyper-linking.
Extracted entities are mapped into a domain ontology that relates concepts to a selection of hyperlinks.
"For information extraction, we use SProUT, a generic platform for the develop-ment and use of multilingual text processing components."
"By combining finite-state and unification-based formalisms, the grammar formalism used in SProUT offers both pro-cessing efficiency and a high degree of decal-rativeness."
"The ExtraLink demo system show-cases the extraction of relevant concepts from German texts in the tourism domain, offering the direct connection to associated web docu-ments on demand."
"The utilization of language technology for the creation of hyperlinks has a long history (e.g.,[REF_CITE])."
Information extraction (IE) is a technology that can be applied to identifying both sources and targets of new hyperlinks.
IE systems are becoming commercially viable in supporting diverse information discovery and management tasks.
"Similarly, automatic hyperlinking is a matu-ring technology designed to interrelate pieces of information, using ontologies to define the rela-tionships."
"With ExtraLink, we present a novel information system that integrates both technolo-gies in order to reach at an improved level of informativeness and comfort."
Extraction and link generation occur completely in the background.
"Entities identified by the IE system are mapped into a domain ontology that relates concepts to a structured selection of predefined hyperlinks, which can be directly visualized on demand using a standard web browser."
"This way, the user can, while reading a text, immediately link up textual information to the Internet or to any other docu-ment base without accessing a search engine."
"The quality of the link targets is much higher than with standard search engines since, first of all, only domain-specific interpretations are sought, and second, the ontology provides additional structure, including related information."
"ExtraLink uses as its IE system SProUT, a gene-ric multilingual shallow analysis platform, which currently provides linguistic processing resources for English, German, Italian, French, Spanish, Czech, Polish, Japanese, and Chinese[REF_CITE]."
"SProUT is used for tokenization, mor-phological analysis, and named entity recognition in free texts."
"In Section 2 to 4, we describe innova-tive features of SProUT."
Section 5 gives details about the ExtraLink demonstrator.
The main motivation for developing SProUT comes from the need to have a system that (i) allows a flexible integration of different processing modules and (ii) to find a good trade-off between processing efficiency and linguistic expressive-ness.
"On the one hand, very efficient finite state devices have been successfully applied to real-world applications."
"On the other hand, unification-based grammars (UBGs) are designed to capture fine-grained syntactic and semantic constraints, resulting in better descriptions of natural language phenomena."
"In contrast to finite state devices, unification-based grammars are also assumed to be more transparent and more easily modifiable."
"SProUT’s mission is to take the best from these two worlds, having a finite state machine that operates on typed feature structures (TFSs)."
"I.e., transduction rules in SProUT do not rely on simple atomic symbols, but instead on TFSs, where the left-hand side of a rule is a regular expression over TFSs, representing the recognition pattern, and the right-hand side is a sequence of TFSs, specifying the output structure."
"Consequently, equality of atomic symbols is replaced by unifiability of TFSs and the output is constructed using TFS unification w.r.t. a type hierarchy."
"Such rules not only recog-nize and classify patterns, but also extract frag-ments embedded in the patterns and fill output templates with them."
"Standard finite state techniques such as minimi-zation and determinization are no longer applicable here, due to the fact that edges in our automata are annotated by TFSs, instead of atomic symbols."
"However, not every outgoing edge in such an automaton must be analyzed, since TFS annota-tions can be arranged under subsumption, and the failure of a general edge automatically causes the failure of several, more specialized edges, without applying the unifiability test."
Such information can in fact be precompiled.
This and other optimization techniques are described[REF_CITE].
"When compared to symbol-based finite state approaches, our method leads to smaller grammars and automata, which usually better approximate a given language."
"XTDL combines two well-known frameworks, viz., typed feature structures and regular ex-pressions."
"XTDL is defined on top of TDL, a defi-nition language for TFSs[REF_CITE]that is used as a descriptive device in several grammar systems (L KB , P AGE , P ET )."
"Apart from the integration into the rule definitions, we also employ TDL in SProUT for the establishment of a type hierarchy of linguistic entities."
"In the example definition below, the morph type inherits from sign and introduces three more morphologically motivated attributes with the corresponding typed values: morph := sign &amp; [ POS atom, STEM atom, INFL infl ]."
"A rule in XTDL is straightforwardly defined as a recognition pattern on the left-hand side, written as a regular expression, and an output description on the right-hand side."
A named label serves as a handle to the rule.
Regular expressions over TFSs describe sequential successions of linguistic signs.
We provide a couple of standard operators.
Con-catenation is expressed by consecutive items.
"Dis- junction, Kleene star, Kleene plus, and optionality are represented by the operators | , * , + , and ? , resp. { n } after an expression denotes an n-fold repetition. { m,n } repeats at least m times and at most n times."
The XTDL grammar rule below may illustrate the syntax.
It describes a sequence of morphologi-cally analyzed tokens (of type morph).
The first TFS matches one or zero items ( ? ) with part-of-speech Determiner.
"Then, zero or more Adjective items are matched ( * )."
"Finally, one or two Noun items ( {1,2} ) are consumed."
"The use of a variable (e.g., #1 ) in different places establishes a coreference between features."
"This example enfor-ces agreement in case, number, and gender for the matched items."
"Eventually, the description on the RHS creates a feature structure of type phrase, where the category is coreferent with the category Noun of the right-most token(s), and the agreement features corefer to features of the morph tokens. np :&gt; (morph &amp; [ POS Determiner, INFL [CASE #1, NUM #2, GEN #3 ]] )? (morph &amp; [ POS Adjective, INFL [CASE #1, NUM #2, GEN #3 ]] ) * (morph &amp; [ POS Noun &amp; #4,"
"INFL [CASE #1, NUM #2, GEN #3 ]] ){1,2} -&gt; phrase &amp; [CAT #4, AGR agr &amp; [CASE #1, NUM #2, GEN #3 ]]."
The choice of TDL has a couple of advantages.
TFSs as such provide a rich descriptive language over linguistic structures and allow for a fine-grained inspection of input items.
They represent a generalization over pure atomic symbols.
Unifia-bility as a test criterion in a transition is a generali-zation over symbol equality.
Coreferences in feature structures express structural identity.
Their properties are exploited in two ways.
"They provide a stronger expressiveness, since they create dynamic value assignments on the automaton transitions and thus exceed the strict locality of constraints in an atomic symbol approach."
"Further-more, coreferences serve as a means of information transport into the output description on the RHS of the rule."
"Finally, the choice of feature structures as primary citizens of the information domain makes composition of modules very simple, since input and output are all of the same abstract data type."
Functional (in contrast to regular) operators are a door to the outside world of SProUT.
"They either serve as predicates, helping to locate complex tests that might cancel a rule application, or they construct new material, involving pieces of information from the LHS of a rule."
The sketch of a rule below transfers numerals into their corresponding digits using the functional operator normalize() that is defined externally.
"For instance, &quot;one&quot; is mapped onto &quot;1&quot;, &quot;two&quot; onto &quot;2&quot;, etc. … numeral &amp; [ SURFACE #surf, ... ] .… -&gt; digit &amp; [ ID #id, ... ], where #id = normalize(#surf)."
"The core of SProUT comprises of the following components: (i) a finite-state machine toolkit for building, combining, and optimizing finite-state devices; (ii) a flexible XML-based regular com-piler for converting regular patterns into their cor-responding compressed finite-state representati[REF_CITE]; (iii) a JTFS package which provides standard operations for constructing and manipulating TFSs; and (iv) an XTDL grammar interpreter."
"Currently, SProUT offers three online compo-nents: a tokenizer, a gazetteer, and a morphological analyzer."
The tokenizer maps character sequences to tokens and performs fine-grained token classifi-cation.
The gazetteer recognizes named entities based on static named entity lexica.
"The morphology unit provides lexical resources for English, German (equipped with online shallow compound recognition), French, Italian, and Spanish, which were compiled from the full form lexica of MMorph[REF_CITE]."
"Considering Slavic languages, a component for Czech presented in (Haji þ , 2001), and Morfeusz[REF_CITE]for Polish."
"For Asian languages, we integrated Chasen[REF_CITE]for Japanese and Shanxi[REF_CITE]for Chinese."
"The XTDL-based grammar engineering plat-form has been used to define grammars for English, German, French, Spanish, Chinese and Japanese allowing for named entity recognition and extraction."
"To guarantee a comparable coverage, and to ease evaluation, an extension of the MUC-7 standard for entities has been adopted. ne-person := enamex &amp; [ TITLE list-of-strings, GIVEN_NAME list-of-strings, SURNAME list-of-strings, P-POSITION list-of-strings, NAME-SUFFIX string, DESCRIPTOR string ]."
"Given the expressiveness of XTDL expressions, MUC-7/MET-2 named entity types can be enhanced with more complex internal structures."
"For instance, a person name ne-person is defined as a subtype of enamex with the above structure."
"The named entity grammars can handle types such as person, location, organization, time point, time span (instead of date and time defined by MUC), percentage, and currency."
The core system together with the grammars forms a basis for developing applications.
SProUT is being used by several sites in both research and industrial contexts.
"A component for resolving coreferent named entities disambiguates and classifies incomplete named entities via dynamic lexicon search, e.g., Microsoft is coreferent with Microsoft corporation and is thus correctly classified as an organization."
"A methodology for automatically enriching web documents with typed hyperlinks has been develo-ped and applied to several domains, among them the domain of tourism information."
"A core compo-nent is a domain ontology describing tourist sites in terms of sights, accommodations, restaurants, cultural events, etc."
The ontology was specialized for major European tourism sites and regions (see Figure 1).
"It is associated with a large selection of link targets gathered, intellectually selected and continuously verified."
"Although language techno-logy could also be employed to prime target selection, for most applications quality require-ments demand the expertise of a domain specialist."
"In the case of the tourism domain, the selection was performed by a travel business professional."
The system is equipped with an XML interface and accessible as a server.
The ExtraLink GUI marks the relevant entities (usually locations) identified by SProUT (see second window on the left in Figure 2).
Clicking on a marked expression causes a query related to the entity being shipped to the server.
Coreferent concepts are handled as expanded queries.
"The server returns a set of links structured according to the ontology, which is presented in the ExtraLink GUI (Figure 2)."
The user can choose to visualize any link target in a new browser window that also shows the respective subsection of the ontology in an indented tree notation (see Figure 1).
"The ExtraLink demonstrator has been imple-mented in Java and C++, and runs under both MS Windows and Linux."
"It is operational for German, but it can easily be extended to other languages covered by SProUT."
This involves the adaptation of the mapping into the ontology and a multi-lingual presentation of the ontology in the link target page.
This paper proposes a method of collect-ing a dozen terms that are closely re-lated to a given seed term.
The proposed method consists of three steps.
"The first step, compiling corpus step, collects texts that contain the given seed term by us-ing search engines."
"The second step, au-tomatic term recognition, extracts impor-tant terms from the corpus by using Naka-gawa’s method."
These extracted terms be-come the candidates for the final step.
"The final step, filtering step, removes inappro-priate terms from the candidates based on search engine hits."
An evaluation result shows that the precision of the method is 85%.
This study aims to realize an automatic method of collecting technical terms that are related to a given seed term.
"In case “natural language processing” is given as a seed term, the method is expected to col-lect technical terms that are related to natural lan-guage processing, such as morphological analysis, parsing, information retrieval, and machine transla-tion."
The target application of the method is auto-matic or semi-automatic compilation of a glossary or technical-term dictionary for a certain domain.
Re-cursive application of the method enables to collect a list of terms that are used in a certain domain: the list becomes a glossary of the domain.
"A technical-term dictionary can be compiled by adding an explanation for every term in the glossary, which is performed by term explainer[REF_CITE]."
"Automatic acquisition of technical terms in a cer-tain domain has been studied as automatic term recogniti[REF_CITE], and the methods require a large corpus that are manually prepared for a target do-main."
"In contrast, our system, which is proposed in this paper, requires only a seed word; from this seed word, the system compiles a corpus from the Web by using search engines and produces a dozen technical terms that are closely related to the seed word."
Figure 1 shows the configuration of the system.
"The system consists of three steps: compiling corpus, au-tomatic term recognition (ATR), and filtering."
This system is implemented for Japanese language.
"The first step, compiling corpus, produces a corpus C s for a seed term s."
"In general, compiling corpus is to select the appropriate passages from a document set."
We use the Web for the document set and se-lect the passages that describe s for the corpus.
"The actual procedure of compiling corpus is: 1. Web page collection For a given seed term s, the system first makes four queries: “s toha”, “s toiu”, “s ha”, and “s”, where toha, ha, and toiu are Japanese functional words that are often used for defin-ing or explaining a term."
"Then, the system col-lects the top K (= 100) pages at maximum for each query by using a search engine."
"If a col-lected page has a link whose anchor string is s, the system collects the linked page too. 2."
"Sentence extraction The system decomposes each page into sen-tences, and extracts the sentences that contain the seed term s."
The reason why we use the additional three queries is that they work efficiently for collecting web pages that contain a definition or an explanation of s.
"We use two search engines, Goo [Footnote_1] and Infoseek [URL_CITE] ."
"1 if x is a single noun F(x, L) ="
"We send all four queries to Goo but only the query “s” to Infoseek, because Infoseek usually returns the same result for the four queries."
A typical corpus size is about 500 sentences.
"The second step, automatic term recognition (ATR), extracts important terms from the compiled cor-pus."
"We use Nakagawa’s ATR method[REF_CITE], which works well for Japanese text, with some modifications."
The procedure is as follows. 1.
Generation of term list To make the term list L by extracting every term that is a noun or a compound noun from the compiled corpus. 2.
Selection by scoring To select the top N (= 30) terms from the list L by using a scoring function.
"For the scoring function of a term x, we use the following function, which is multiplying Nak-agawa’s Imp 1 by a frequency factor F(x, L) α . score(x, L) ="
"Imp 1 (x, L) × F(x, L) α"
"While Nakagawa’s Imp [Footnote_1] does not consider term fre-quency, this function does: α is a parameter that con-trols how strongly the frequency is considered."
"1 if x is a single noun F(x, L) ="
We use α = 0.5 in experiments.
The result of automatic term recognition for “ 自然 言語処理 (natural language processing)” is shown in the column candidate in Table 1.
The filtering step is necessary because the obtained candidates are noisy due to the small corpus size.
This step consists of two tests: technical-term test and relation test.
The technical-term test removes the terms that do not satisfy conditions of technical terms.
We employ the following four conditions that a technical term should satisfy. 1.
The term is sometimes or frequently used in a certain domain. 2.
The term is not a general term. 3.
There is a definition or explanation of the term. 4.
There are several technical terms that are re-lated to the term.
"We have implemented the checking program of the first two conditions in the system: the third condition can be checked by integrating the system with term explainer[REF_CITE], which produces a definition or explanation of a given term; the fourth condition can be checked by using the system recursively."
There are several choices for implementing the checking program.
Our choice is to use the Web via a search engine.
"A search engine returns a number, hit, which is an estimated number of pages that sat-isfy a given query."
"In case the query is a term, its hit is the number of pages that contain the term on the Web."
We use the following notation.
H(x) = “the number of pages that contain the term x”
"The number H(x) can be used as an estimated frequency of the term x on the Web, i.e., on the hugest set of documents."
"Based on this number, we can infer whether a term is a technical term or not: in case the number is very small, the term is not a technical term because it does not satisfy the first condition; in case the number is large enough, the term is probably a general term so that it is not a technical term."
"Two parameters, Min and Max, are necessary here."
"We have decided that we use search engine Goo for H(x), and determined Min = 100 and Max = 100, 000, based on preliminary experi-ments."
"In summary, our technical-term test is:"
The relation test removes the terms that are not closely related to the seed term from the candidates.
"Our conditions of “x is closely related to s” is: (1) x is a broader or narrower term of s; or (2) relation degree between x and s is high enough, i.e., above a given threshold."
The candidate terms can be classified from the viewpoint of term composition.
"Under a given seed term, we introduce the following five types for clas-sification."
"Type 0 the given seed term s: e.g., 自然 言語 処理"
"First, we examined the precision of the system."
"We prepared fifty seed terms in total: ten terms for each of five genres; natural language processing, Japanese language, information technology, current topics, and persons in Japanese history."
"From these fifty terms, the system collected 610 terms in total; the average number of output terms per input is 12.2 terms."
We checked whether each of the 610 terms is a correct related term of the original seed term by hand.
The result is shown in the left half (Evaluation I) of Table 2.
"In this evaluation, 519 terms out of 610 terms were correct: the precision is 85%."
"From this high value, we conclude that the system can be used as a tool that helps us compile a glossary."
"Second, we tried to examine the recall of the system."
"It is impossible to calculate the actual re-call value, because the ideal output is not clear and cannot be defined."
"To estimate the recall, we first prepared three to five target terms that should be collected from each seed word, and then checked whether each of the target terms was included in the system output."
We counted the number of tar-get terms in the following five cases.
The right half (Evaluation II) in Table 2 shows the result.
S: the target term was collected by the system.
F: the target term was removed in the filtering step.
"A: the target term existed in the compiled corpus, but was not extracted by automatic term extrac-tion."
"C: the target term existed in the collected web pages, but did not exist in the compiled corpus."
R: the target term did not exist on the collected web pages.
This low recall primarily comes from the failure of automatic term recogni-tion (case A in the above classification).
Improve-ment of this step is necessary.
We also examined whether each of the 210 target terms passes the filtering step.
"The result was that 133 (63%) terms passed; 44 terms did not satisfy the condition H(x) ≥ 100; 15 terms did not satisfy the condition H(x) ≤ 100, 000; and 18 terms did not pass the relation test."
These experimental results suggest that the ATR step may be replaced with a simple and exhaustive term collector from a corpus.
We have a plan to examine this possibility next.
We describe iNeATS – an interactive multi-document summarization system that integrates a state-of-the-art summa-rization engine with an advanced user in-terface.
"Three main goals of the sys-tem are: (1) provide a user with control over the summarization process, (2) sup-port exploration of the document set with the summary as the staring point, and (3) combine text summaries with alternative presentations such as a map-based visual-ization of documents."
The goal of a good document summary is to provide a user with a presentation of the substance of a body of material in a coherent and concise form.
"Ideally, a summary would contain only the “right” amount of the interesting information and it would omit all the redundant and “uninteresting” material."
The quality of the summary depends strongly on users’ present need – a summary that focuses on one of several top-ics contained in the material may prove to be either very useful or completely useless depending on what users’ interests are.
An automatic multi-document summarization system generally works by extracting relevant sen-tences from the documents and arranging them in a coherent order[REF_CITE].
"The system has to make decisions on the summary’s size, redundancy, and focus."
Any of these deci-sions may have a significant impact on the quality of the output.
We believe a system that directly in-volves the user in the summary generation process and adapts to her input will produce better sum-maries.
"Additionally, it has been shown that users are more satisfied with systems that visualize their decisions and give the user a sense of control over the process[REF_CITE]."
"We see three ways in which interactivity and visualization can be incorporated into the multi-document summarization process: 1. give the user direct control over the summariza-tion parameters such as size, redundancy, and focus of the summaries. 2. support rapid browsing of the document set us-ing the summary as the starting point and com-bining the multi-document summary with sum-maries for individual documents. 3. incorporate alternative formats for organizing and displaying the summary, e.g., a set of news stories can be summarized by placing the sto-ries on a world map based on the locations of the events described in the stories."
In this paper we describe iNeATS (Interactive NExt generation Text Summarization) which ad-dresses these three directions.
The iNeATS system is built on top of the NeATS multi-document sum-marization system.
In the following section we give a brief overview of the NeATS system and in Sec-tion 3 describe the interactive version.
NeATS[REF_CITE]is an extraction-based multi-document summarization system.
It is among the top two performers[REF_CITE]and 2002[REF_CITE].
It consists of three main com-ponents:
Content Selection The goal of content selection is to identify important concepts mentioned in a document collection.
"NeATS computes the likelihood ratio[REF_CITE]to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic."
"Each sentence in the document set is then ranked, us-ing the key concept structures."
These n-gram key concepts are called topic signatures.
"Content Filtering NeATS uses three different fil-ters: sentence position, stigma words, and re-dundancy filter."
Sentence position has been used as a good important content filter since the late 60s[REF_CITE].
NeATS ap-plies a simple sentence filter that only retains the N lead sentences.
"Some sentences start with conjunctions, quotation marks, pronouns, and the verb “say” and its derivatives."
These stigma words usually cause discontinuities in summaries.
The system reduces the scores of these sentences to demote their ranks and avoid including them in summaries of small sizes.
"To address the redundancy problem, NeATS uses a simplified version of CMU’s MMR[REF_CITE]algorithm."
A sentence is added to the summary if and only if its content has less than X percent overlap with the summary.
"Content Presentation To ensure coherence of the summary, NeATS pairs each sentence with an introduction sentence."
It then outputs the final sentences in their chronological order.
Figure 1 shows a screenshot of the iNeATS system.
We divide the screen into three parts corresponding to the three directions outlined in Section 1.
The control panel displays the summarization parame-ters on the left side of the screen.
The document panel shows the document text on the right side.
The summary panel presents the summaries in the mid-dle of the screen.
The top of the control panel provides the user with control over the summarization process.
"The first set of widgets contains controls for the summary size, sentence position, and redundancy filters."
The sec-ond row of parameters displays the set of topic sig-natures identified by the iNeATS engine.
The se-lected subset of the topic signatures defines the con-tent focus for the summary.
"If the user enters a new value for one of the parameters or selects a different subset of the topic signatures, iNeATS immediately regenerates and redisplays the summary text in the top portion of the summary panel."
"iNeATS facilitates browsing of the document set by providing (1) an overview of the documents, (2) linking the sentences in the summary to the original documents, and (3) using sentence zooming to high-light the most relevant sentences in the documents."
The bottom part of the control panel is occupied by the document thumbnails.
The documents are ar-ranged in chronological order and each document is assigned a unique color to paint the text background for the document.
"The same color is used to draw the document thumbnail in the control panel, to fill up the text background in the document panel, and to paint the background of those sentences in the sum-mary that were collected from the document."
"For example, the screenshot shows that a user selected the second document which was assigned the or-ange color."
"The document panel displays the doc-ument text on orange background. iNeATS selected the first two summary sentences from this document, so both sentences are shown in the summary panel with orange background."
The sentences in the summary are linked to the original documents in two ways.
"First, the docu-ment can be identified by the color of the sentence."
"Second, each sentence is a hyperlink to the docu-ment – if the user moves the mouse over a sentence, the sentence is underlined in the summary and high-lighted in the document text."
"For example, the first sentence of the summary is the document sentence highlighted in the document panel."
"If the user clicks on the sentence, iNeATS brings the source document into the document panel and scrolls the window to make the sentence visible."
The relevant parts of the documents are illumi-nated using the technique that we call sentence zooming.
We make the text color intensity of each sentence proportional to the relevance score com-puted by the iNeATS engine and a zooming parame-ter which can be controlled by the user with a slider widget at the top of the document panel.
"The higher the sentence score, the darker the text is."
"Conversely, sentences that blend into the background have a very low sentence score."
The zooming parameter con-trols the proportion of the top ranked sentences vis-ible on the screen at each moment.
This zooming affects both the full-text and the thumbnail docu-ment presentations.
"Combining the sentence zoom-ing with the document set overview, the user can quickly see which document contains most of the relevant material and where approximately in the document this material is placed."
The document panel in Figure 1 shows sentences that achieve 50% on the sentence score scale.
"We see that the first half of the document contains two black sentences: the first sentence that starts with “US In-surers...”, the other starts with “President George...”."
Both sentences have a very high score and they were selected for the summary.
"Note, that the very first sentence in the document is the headline and it is not used for summarization."
"Note also that the sentence that starts with “However,...” scored much lower than the selected two – its color is approximately half diluted into the background."
There are quite a few sentences in the second part of the document that scored relatively high.
"How-ever, these sentences are below the sentence position cutoff so they do not appear in the summary."
We il-lustrate this by rendering such sentences in slanted style.
The bottom part of the summary panel is occupied by the map-based visualization.
We use BBN’s IdentiFinder[REF_CITE]to detect the names of geographic locations in the document set.
We then select the most frequently used location names and place them on world map.
Each location is iden-tified by a black dot followed by a frequency chart and the location name.
The frequency chart is a bar chart where each bar corresponds to a document.
The bar is painted using the document color and the length of the bar is proportional to the number of times the location name is used in the document.
"The document set we used in our example de-scribes the progress of the hurricane Andrew and its effect on Florida, Louisiana, and Texas."
Note that the source documents and therefore the bars in the chart are arranged in the chronological order.
"The name “Miami” appears first in the second document, “New Orleans” in the third document, and “Texas” is prominent in the last two documents."
We can make some conclusions on the hurricane’s path through the region – it traveled from south-east and made its landing somewhere in Louisiana and Texas.
The iNeATS system is implemented in Java.
It uses the NeATS engine implemented in Perl and C. It runs on any platform that supports these environ-ments.
We are currently working on making the sys-tem available on our web site.
We plan to extend the system by adding temporal visualization that places the documents on a timeline based on the date and time values extracted from the text.
We plan to conduct a user-based evaluation of the system to compare users’ satisfaction with both the automatically generated summaries and summaries produced by iNeATS.
"In every text, some words have frequency appearance and are considered as keywords because they have strong relationship with the subjects of their texts, these words frequencies change with time-series variation in a given period."
"However, in traditional text dealing methods and text search techniques, the importance of frequency change with time-series variation is not considered."
"Therefore, traditional methods could not correctly determine index of word’s popularity in a given period."
"In this paper, a new method is proposed to estimate automatically the stability classes (increasing, relatively constant, and decreasing) that indicate word’s popularity with time-series variation based on the frequency change in past texts data."
"At first, learning data was produced by defining four attributes to measure frequency change of word quantitatively, these four attributes were extracted automatically from electronic texts."
"According to the comparison between the evaluation of the decision tree results and manually (Human) results, F-measures of increasing, relatively constant and decreasing classes were 0.847, 0.851, and 0.768 respectively, and the effectiveness of this method is achieved."
"Keywords : time-series variation, words popularity, decision tree, CNN newspaper ."
"Recently, there are many large electronic texts and computers are processing (analysis) them widely."
Determination of important keywords is crucial in successful modern Information Retrieval (IR).
"Usually, frequency of some words in the texts are changing by time (time-series variation), and these words are commonly connected with particular period (e.g. “influenza” is more common in winter)."
According[REF_CITE]some Chinese characters (Kanji) appear in newspaper reports change with time-series variation.
"By Ohkubo method, it is confirmed that, word groups connected with search words change according to time when the search is done."
"Some words have a frequency of use that changes with time-series variation, and often those words attract the attention of the users in a particular period."
"Such words are often directly connected with the main subject of the text, and can be considered as keywords that express important characteristics of the text."
"In traditional text dealing methods (Fukumoto, Suzuki &amp;[REF_CITE]; Hara, Nakajima &amp;[REF_CITE]; Sagara &amp;[REF_CITE]) and text search techniques ([REF_CITE]; Swerts &amp;[REF_CITE]), words frequency change with time-series variation is not considered."
"Therefore, such methods can not correctly determine the importance of words in a given period (e.g. one-year)."
"If the change of word frequencies with time-series variation is considered, especially when searching for similar texts."
This paper presents a new method for estimating automatically the stability classes that indicate index of words popularity with time-series variation based on frequency change in past texts data.
"To estimate quantitatively the frequency change in the time-series variation of words in each class, this method defines four attributes (proper nouns attributes, slope of regression line, slice of regression line, and correlation coefficient) that are extracted automatically from past texts data."
These extracted data are classified manually (Human) into three stability classes.
"Decision Tree (DT) automatic algorithm C4.5 ([REF_CITE]; Weiss &amp;[REF_CITE]; Honda, Mochizuki, Ho &amp;[REF_CITE]; Passonneau &amp;[REF_CITE]; Okumura, Haraguchi &amp;[REF_CITE]) uses these data as learning data."
"Finally, DT automatically determines the stability classes of the input analysis data (test data)."
"CONSIDERING TIME-SERIES VARIATION 2.1 Stability Classes of the Words: To judge the index of popularity of words with time-series variation based on the frequency change, and create the stability classes of the words, we defined three classes as follow: (1) Increasing Class “The class that has an increasing 1) frequency with time-series variation” 2) (2) Relatively Constant Class “The class that has a 3) stable frequency with time-series variation” 4) (3) Decreasing Class “The class that has a decreasing frequency with time-series variation”."
We call these classes stability classes.
"The words belong to each class is called: increasing-words, relatively constant-words, and decreasing-words respectively."
Table 1 shows a sample of some classified words according to frequency change with time-series variation in each stability class.
"For example, the names of baseball players “Sammy-Sosa” and “McGwire” are included in increasing class because their frequencies increase with time-series variation."
The names of baseball teams “New-York-Mets” and “Texas-Rangers” are included in a relatively constant class because their frequencies relatively stable with time-series variation.
The names of baseball players “Hank-Aaron” and “Nap Lajoie” are included in a decreasing class because their frequencies decrease with time-series variation.
Words stability classes are decided by the change of their frequencies with time-series variation.
"In order to determine the change of frequency with time-series variation, texts were grouped according to a given period (one-year) and frequency of words in each group is estimated."
"However, to absorb the influence caused by difference of number of texts in each group and to judge the change with time-series more correctly, each frequency is normalized by being divided by the total frequencies of the words in each group."
"In this paper, five attributes are defined to decide the stability classes, and the words data that are divided into classes beforehand are input into the DT automatic algorithm C4.5 as the learning data."
Then we use the obtained DT to decide automatically the stability classes of increasing words.
"In the next section, the attributes that are used in the DT learning to judge the stability classes will be described . 3."
ATTRIBUTES USED IN JUDGING THE STABILITY CLASS
"To obtain the characteristics of the change of word’s frequencies quantitatively, the following attributes are defined."
The value of each attribute defined here is used as the input data for the DT describe in section 4.
Proper Nouns Attributes (pna)
Slope of regression straight line ( α )
Slice of regression straight line ( β )
Correlation coefficient (r)
"In this paper, we selected only three kinds of proper nouns attributes: “Player-name”, “Organization-name”, and “Team-name” to study the influence of the time-series variation and to obtain the characteristics of increasing or decreasing stability classes."
"Also we used “Ordinary-nouns” (e.g. “ball”, “coach”, “home-run”) for the relatively constant class."
The characteristics of the stability class are much easier and more correct by using these entities analysis.
"Regression Straight Line ( α &amp; β ): Regression analysis is a statistical method, which approximates the change of the sample value with straight line in two dimension rectangular coordinates, and this approximation straight line is called a regression straight line (Gonick &amp;[REF_CITE])."
"In this progress we take the standard years (x 1 = first year, x 2 = second year,………x i = i year, ………, x n = n year) as a horizontal axis, and the corresponding normalization frequency y i of the words as a vertical axis."
"The slope segmentation α and the slice β of the equation y = α x + β can be calculated by the following formula: n ∑ ( x − x )( y − y ) i i α = i x = ∑ ( x − x ) 1 y Λ Λ Λ Λ (1) n 2 i i =1 β = y − α xΛ Λ Λ Λ (2) where , are the average values of x i , y i respectively."
"By obtaining the cross point of the regression straight line and the current time period in rectangular coordinates, it is possible to get the estimated frequencies of the current words."
The slope of the regression straight line can estimate the stability classes of the words.
"In addition, from the slice of the regression straight line, the difference of frequencies between words groups in the same stability class can be estimated."
For example the frequency of the words in the same stability class (relatively constant) that have a regression straight line (1) in Fig. 5 is higher every period than that of straight line (2).
The value of the slice of regression straight line (1) is also higher than that of regression straight line (2).
"So, we can decide that the words of the regression straight line (1) are more important than the words in the regression straight line (2), even though all these words are in the same class."
"By obtaining the cross point of the regression straight line and the current time period in rectangular coordinates, the slope of the regression straight line can estimate the stability classes of the words."
"For example, when the stability class is stabilized, the regression straight line is close to the horizontal line and the slope is close to 0."
"When the stability class is increasing, its slope is positive, and the slope becomes negative when the stability class is decreasing."
"In addition, from the slice of the regression straight line, the difference of frequencies between words groups in the same stability class can be estimated."
For example the frequency of the words in the same stability class (relatively constant) that have a regression straight line (1) in Fig. 1 is higher every period than that of straight line (2).
The value of the slice of regression straight line (1) is also higher than that of regression straight line (2).
"So, we can decide that the words of the regression straight line (1) are more important than the words in the regression straight line (2), even though all these words are in the same class."
Correlation coefficient is used to judge the reliability of regression straight line.
"Although, stability classes of words are estimated by slope and slice of the regression straight line, there are some words with the same regression straight line have versus degree of scattering because of the arrangement of frequencies of words in rectangular coordinates as shown in Fig. 2."
"In such case, there will be some problems in the point of reliability if these different groups of words have the same stability class."
"So, in order to judge the reliability of the regression straight line that derived from the scattering of frequencies, a correlation coefficient was used that shows the scattering extent (degree) of the frequencies of words in rectangular coordinates."
"Correlation coefficient is also a statistical method (Gonick &amp;[REF_CITE]), and the calculation equation is shown as follows:"
"In the above formula, ŷ i are the predicted weights determined by regression line and α is the slope of the regression straight line."
"When the absolute value of correlation coefficient r is approaching to 1, the appearance frequency is concentrated around the regression straight line, and when it approaches to 0, it means that the appearance frequency is irregularly scattering around the regression straight line."
"In order to confirm the effectiveness of our method, an experiment is designed to study the effect of learning period lengths and all attributes on the distribution precision of DT output, as explained below : n ∑ ( yˆ − y ) 2 i r = (sign of α ) i=1"
Λ Λ Λ Λ Λ (3) n ∑ ( y − y ) 2 i i=1 ❈ ❈ ❈ ❈ ❈ Ferq. ❈
Periods Fig. 2
An Illustration of Regression Coefficient.
"The sports section of CNN newspapers (1997-2000) was used as an experimental collection data, because of the uniqueness of the words in this field and their tendency to change with the time-series variation."
"A specific sub-field from sports “professional baseball” was chosen because it has stabilized frequent reports every year, and it is relatively easy to determine how words frequencies affect by time-series variation."
"Words identify with four kinds of proper nouns attributes: “Player-name”, “Organization-name”, “Team-name”, and “Ordinary-nouns” were extracted from the selected reports, and the normalized frequency of the selected words in each year was obtained."
"Then, stability classes classified manually (Human) to these words."
The data is divided into two groups: one includes the reports of years (1997- 1999) are used as DT learning data.
"The other includes the reports of years (1997-2000), that are completely different data than the learning data, are used as test data."
For both data sets the attributes are obtained from the change of words frequency with time- series variation included in both periods.
The data of extracted words is shown in Table 2.
"In order to get the accuracy of the correct words that are words that are evaluated automatically by DT , we measured: Precision (P), and Recall (R) rate as follows:"
Number of correct words extracted by (DT) Precision =
Total number of words extracted by (DT)
Number of correct words extracted by DT)
") + * + +( 4 * + + + + + + + ( 4 + + * [Footnote_9] + + + + ( ) ) + ? * + * % + ( ) + ( 5 &apos; &apos; [Footnote_9] ? * [Footnote_9] ? ( ) + * &apos; +( 5* &apos; + * + ? * + * ( &amp; F &apos; * ( ( + * + &apos; ##E * % + .2E * + % ( ) * &quot;&quot;E * % %/ E * ( &apos; 9 ( 5 * ? + + * * ( , &apos; &apos; + * ( 8 &apos; * &apos; * ? * +( 5 & apos; &apos; &apos; &amp; , &apos; ( 4 + 6&gt;, * [Footnote_9] * ( 5 &apos; * 6&gt;, ? ( ) * 6&gt;, [Footnote_9] ?"
"F ( , + * *"
"We argue that verbal patient diagnosis is a promising application for limited-domain speech translation, and describe an ar-chitecture designed for this type of task which represents a compromise between principled linguistics-based processing on the one hand and efficient phrasal transla-tion on the other."
"We propose to demon-strate a prototype system instantiating this architecture, which has been built on top of the Open Source REGULUS 2 platform."
"The prototype translates spoken yes-no questions about headache symptoms from English to Japanese, using a vocabulary of about 200 words."
Language is crucial to medical diagnosis.
"Dur-ing the initial evaluation of a patient in an emer-gency department, obtaining an accurate history of the chief complaint is of equal importance to the physical examination."
In many parts of the world there are large recent immigrant populations that re-quire medical care but are unable to communicate fluently in the local language.
In the US these im-migrants are especially likely to use emergency fa-cilities because of insurance issues.
In an emer-gency setting there is acute need for quick accurate physician-patient communication but this communi-cation is made substantially more difficult in cases where there is a language barrier.
Our system is designed to address this problem using spoken ma-chine translation.
Designing a spoken translation system to obtain a detailed medical history would be difficult if not impossible using the current state of the art.
The reason that the use of spoken translation technol-ogy is feasible is because what is actually needed in the emergency setting is more limited.
"Since medi-cal histories traditionally are obtained through two-way physician-patient conversations that are mostly physician initiative, there is a preestablished limiting structure that we can follow in designing the trans-lation system."
This structure allows a physician to sucessfully use one way translation to elicit and re-strict the range of patient responses while still ob-taining the necessary information.
"Another helpful constraint on the conversational requirements is that the majority of medical condi-tions can be initiatlly characterized by a relatively small number of key questions about quality, quan-tity and duration of symptoms."
"For example, key questions about chest pain include intensity, loca-tion, duration, quality of pain, and factors that in-crease or decrease the pain."
"These answers to these questions can be sucessfully communicated by a limited number of one or two word responses (e.g. yes/no, left/right, numbers) or even gestures (e.g. pointing to an area of the body)."
"This is clearly a domain in which the constraints of the task are suf-ficient for a limited domain, one way spoken trans-lation system to be a useful tool."
The basic philosophy behind the architecture of the system is to attempt an intelligent compromise be-tween fixed-phrase translation on one hand (e.g.[REF_CITE]) and linguisti-cally motivated grammar-based processing on the other (e.g. V ERBMOBIL[REF_CITE]and Spo-ken Language Translator[REF_CITE]).
"At run-time, the system behaves essentially like a phrasal translator which allows some variation in the input language."
"This is close in spirit to the approach used in most normal phrase-books, which typically allow “slots” in at least some phrases (“How much does — cost?”; “How do I get to — ?”)."
"However, in order to minimize the overhead associated with defining and maintaining large sets of phrasal pat-terns, these patterns are derived from a single large linguistically motivated unification grammar; thus the compile-time architecture is that of a linguisti-cally motivated system."
Phrasal translation at run-time gives us speed and reliability; the linguistically motivated compile-time architecture makes the sys-tem easy to extend and modify.
The runtime system comprises three main mod-ules.
"These are respectively responsible for source language speech recognition, including parsing and production of semantic representation; transfer and generation; and synthesis of target language speech."
The speech processing modules (recognition and synthesis) are implemented on top of the standard Nuance Toolkit platform[REF_CITE].
"Recogni-tion is constrained by a CFG language model written in Nuance Grammar Specification Language (GSL), which also specifies the semantic representations produced."
This language model is compiled from a linguistically motivated unification grammar us-ing the Open Source REGULUS 2 platform[REF_CITE]; the compilation pro-cess is driven by a small corpus of examples.
The language processing modules (transfer and genera-tion) are a suite of simple routines written in SICStus
The speech and language processing mod-ules communicate with each other through a mini-mal file-based protocol.
The semantic representations on both the source and target sides are expressed as attribute-value structures.
"In accordance with the generally mini-malistic design philosophy of the project, semantic representations have been kept as simple as possi-ble."
"The basic principle is that the representation of a clause is a flat list of attribute-value pairs: thus for example the representation of “Did your headache start suddenly?” is the attribute-value list [[utterance_type,ynq],[tense,past], [symptom,headache],[state,start], [manner,suddenly]]"
"In a broad domain, it is of course trivial to con-struct examples where this kind of representation runs into serious problems."
"In the very narrow do-main of a phrasebook translator, it has many desir-able properties."
"In particular, operations on semantic representations typically manipulate lists rather than trees."
"In a broad domain, we would pay a heavy price: the lack of structure in the semantic represen-tations would often make them ambiguous."
"The very simple ontology of the phrasebook domain however means that ambiguity is not a problem; the compo-nents of a flat list representation can never be de-rived from more than one functional structure, so this structure does not need to be explicitly present."
Transfer rules define mappings of sets of attribute-value pairs to sets of attribute-value pairs; the ma-jority of the rules map single attribute-value pairs to single attribute-value pairs.
"Generation is han-dled by a small Definite Clause Grammar (DCG), which converts attribute-value structures into sur-face strings; its output is passed through a minimal post-transfer component, which applies a set of rules which map fixed strings to fixed strings."
"Speech syn-thesis is performed either by the Nuance Vocalizer TTS engine or by concatenation of recorded wave-files, depending on the output language."
One of the most important questions for a med-ical translation system is that of reliability; we ad-dress this issue using the methods[REF_CITE].
"The GSL form of the recognition grammar is run in generation mode using the Nu-ance generate utility to generate large numbers of random utterances, all of which are by construc-tion within system coverage."
These utterances are then processed through the system in batch mode us-ing all-solutions versions of the relevant processing algorithms.
The results are checked automatically to find examples where rules are either deficient or ambiguous.
"With domains of the complexity under consideration here, we have found that it is feasible to refine the rule-sets in this way so that holes and ambiguities are effectively eliminated."
We have built a prototype medical speech transla-tion system instantiating the functionality outlined in Section 1 and the architecture of Section 2.
"The system permits spoken English input of constrained yes/no questions about the symptoms of headaches, using a vocabulary of about 200 words."
This is enough to support most of the standard examina-tion questions for this subdomain.
"There are two versions of the system, producing spoken output in French and Japanese respectively."
"Since English → Japanese is distinctly the more interesting and chal-lenging language pair, we will focus on this version."
Speech recognition and source language analy-sis are performed using REGULUS 2.
The grammar is specialised from the large domain-independent grammar using the methods sketched in Section 2.
"The training corpus has been constructed by hand from an initial corpus supplied by a medical pro-fessional; the content of the questions was kept un-changed, but where necessary the form was revised to make it more appropriate to a spoken dialogue."
"When we felt that it would be difficult to remem-ber what the canonical form of a question would be, we added two or three variant forms."
"For exam-ple, we permit “Does bright light make the headache worse?” as a variant for “Is the headache aggra-vated by bright light?”, and “Do you usually have headaches in the morning?” as a variant for “Does the headache usually occur in the morning?”."
The current training corpus contains about 200 exam-ples.
"The granularity of the phrasal rules learned by grammar specialisation has been set so that the con-stituents in the acquired rules are VBARs, post-modifier groups, NPs and lexical items."
VBARs may include both inverted subject NPs and adverbs [Footnote_1] .
1 This non-standard definition of VBAR has technical advan-tages discussed[REF_CITE]
Thus for example the training example “Are the headaches usually caused by emotional upset?” in-duces a top-level rule whose context-free skeleton is
"The present paper will seek to present an approach to bilingual lexicon extrac-tion from non-aligned comparable cor-pora, phrasal translation as well as evalua-tions on Cross-Language Information Re-trieval."
"A two-stages translation model is proposed for the acquisition of bilin-gual terminology from comparable cor-pora, disambiguation and selection of best translation alternatives according to their linguistics-based knowledge."
Different re-scoring techniques are proposed and eval-uated in order to select best phrasal trans-lation alternatives.
Results demonstrate that the proposed translation model yields better translations and retrieval effective-ness could be achieved across Japanese-English language pair.
"Although, corpora have been an object of study of some decades, recent years saw an increased inter-est in their use and construction."
"With this increased interest and awareness has come an expansion in the application to knowledge acquisition, such as bilin-gual terminology."
"In addition, non-aligned com-parable corpora have been given a special inter-est in bilingual terminology acquisition and lexical resources enrichment[REF_CITE]."
"This paper presents a novel approach to bilin-gual terminology acquisition and disambiguation from scarce resources such as comparable corpora, phrasal translation through re-scoring techniques as well as evaluations on Cross-Language Information Retrieval (CLIR)."
CLIR consists of retrieving docu-ments written in one language using queries written in another language.
"An application is completed on a large-scale test collection, NTCIR for Japanese-English language pair."
"Figure 1 shows the overall design of the proposed translation model in CLIR consisting of three main parts as follows: - Bilingual terminology acquisition from bi-directional comparable corpora, completed through a two-stages term-by-term translation model. - Linguistic-based pruning, which is applied on the extracted translation alternatives in order to filter and detect terms and their translations that are mor-phologically close enough, i.e., with close or similar part-of-speech tags. - Phrasal translation, completed on the source query after re-scoring the translation alternatives re-lated to each source query term."
"The proposed re-scoring techniques are based on the World Wide Web (WWW), a large-scale test collection such as NTCIR, the comparable corpora or a possible inter-action with the user, among others."
"Finally, a linear combination to bilingual dictio-naries, bilingual thesauri and transliteration for the special phonetic alphabet of foreign words and loan-words, would be possible depending on the cost and availability of linguistic resources."
"The proposed two-stages approach on bilingual ter-minology acquisition and disambiguation from com-parable corpora[REF_CITE]is described as follows: - Bilingual terminology acquisition from source language to target language to yield a first translation model, represented by similarity vectors SIM S→T . - Bilingual terminology acquisition from target language to source language to yield a second translation model, represented by similarity vectors SIM T→S . - Merge the first and second models to yield a two-stages translation model, based on bi-directional comparable corpora and represented by similarity vectors SIM (S↔T ."
We follow strategies of previous researches[REF_CITE]for the first and second models and propose a merging and disambiguation process for the two-stages transla-tion model.
"Therefore, context vectors of each term in source and target languages are constructed fol-lowing a statistics-based metric."
"Next, context vec-tors related to source words are translated using a preliminary bilingual seed lexicon."
"Similarity vec-tors SIM S→T and SIM T→S related to the first and second models respectively, are constructed for each pair of source term and target translation using the cosine metric."
"The merging process will keep common pairs of source term and target translation (s,t) which appear in SIM S→T as (s,t) but also in SIM T→S as (t,s), to result in combined similarity vectors SIM S↔T for each pair (s,t).The product of similarity values in vectors SIM S→T and SIM( T→S will yield similar-ity values in SIM S↔T for each pair (s,t) of source term and target translation."
"Morphological knowledge such as Part-of-Speech (POS), context of terms extracted from thesauri could be valuable to filter and prune the extracted translation candidates."
POS tags are assigned to each source term (Japanese) via morphological anal-ysis.
"As well, a target language morphological anal-ysis will assign POS tags to the translation candi-dates."
"We restricted the pruning technique to nouns, verbs, adjectives and adverbs, although other POS tags could be treated in a similar way."
"For Japanese-English pair of languages, Japanese nouns and verbs are compared to English nouns and verbs, respec-tively."
"Japanese adverbs and adjectives are com-pared to English adverbs and adjectives, because of the close relationship between adverbs and adjec-tives in Japanese[REF_CITE]."
"Finally, the generated translation alternatives are sorted in decreasing order by similarity values and rank counts are assigned in increasing order."
A fixed number of top-ranked translation alternatives are se-lected and misleading candidates are discarded.
Query translation ambiguity can be drastically mit-igated by considering the query as a phrase and re-stricting the single term translation to those candi-dates that were selected by the proposed combined statistics-based and linguistics-based approach[REF_CITE].
"Therefore, after generating a ranked list of translation candidates for each source term, re-scoring techniques are proposed to estimate the coherence of the translated query and decide the best phrasal translation."
Assume a source query Q having n terms {s 1 ...s n }.
"Phrasal translation of the source query Q is completed according to the selected top-ranked translation alternatives for each source term s i and a re-scoring factor RF k , as follows:"
Q phras = X [Q k (s 1 ..s n )×RF k (t 1 ..t n ; s 1 ..s n )] k=1..thres
"Where, Q k (s 1 ..s n ) represents the phrasal translation candidate associated to rank k."
"The re-scoring factor RF k (t 1 ..t n ; s 1 ..s n ) is estimated using one of the re-scoring techniques, described below."
"Experiments have been carried out to measure the improvement of our proposal on bilingual Japanese-English tasks in CLIR, i.e. Japanese queries to re-trieve English documents."
Collections of news ar-ticles from Mainichi Newspapers (1998-1999) for Japanese and Mainichi Daily News (1998-1999) for English were considered as comparable corpora.
We have also considered documents of NTCIR-2 test collection as comparable corpora in order to cope with special features of the test collection during evaluations.
NTCIR-2[REF_CITE]test collec-tion was used to evaluate the proposed strategies in CLIR.
"SMART information retrieval system[REF_CITE], which is based on vector space model, was used to retrieve English documents."
"Thus, Content words (nouns, verbs, adjectives, adverbs) were extracted from English and Japanese texts."
"Morphological analyzers, ChaSen version 2.2.9 (Matsumoto and al., 1997) for texts in Japanese and OAK2[REF_CITE]for texts in En-glish were used in linguistic pre-processing."
EDR[REF_CITE]was used to translate context vectors of source and target languages.
First experiments were conducted on the several combinations of weighting parameters and schemes of SMART retrieval system for documents terms and query terms.
The best performance was realized by ATN.NTC combined weighting scheme.
The proposed two-stages model using comparable corpora showed a better improvement in terms of av-erage precision compared to the simple model (one- stage comparable corpora-based translation) with +27.1% and a difference of -32.87% in terms of av-erage precision of the monolingual retrieval.
"Com-bination to linguistics-based pruning showed a bet-ter performance in terms of average precision with +41.7% and +11.5% compared to the simple compa-rable corpora-based model and the two-stages com-parable corpora-based model, respectively."
"Applying re-scoring techniques to phrasal transla-tion yields significantly better results with 10.35%, 8.27% and 3.08% for the WWW-based, the NTCIR-based and comparable corpora-based techniques, re-spectively compared to the hybrid two-stages com-parable corpora and linguistics-based pruning."
The proposed approach based on bi-directional comparable corpora largely affected the translation because related words could be added as translation alternatives or expansion terms.
"Effects of extracting bilingual terminology from bi-directional compara-ble corpora, pruning using linguistics-based knowl-edge and re-scoring using different phrasal trans-lation techniques were positive on query transla-tion/expansion and thus document retrieval."
We investigated the approach of extracting bilin-gual terminology from comparable corpora in or-der to enrich existing bilingual lexicons and en-hance CLIR.
We proposed a two-stages translation model involving extraction and disambiguation of the translation alternatives.
Linguistics-based prun-ing was highly effective in CLIR.
Most of the se-lected terms can be considered as translation can-didates or expansion terms.
Exploiting different phrasal translation techniques revealed to be effec-tive in CLIR.
"Although we conducted experiments and evaluations on Japanese-English language pair, the proposed translation model is common across different languages."
"Ongoing research is focused on the integration of other linguistics-based techniques and combination to transliteration for katakana, the special phonetic alphabet to Japanese language."
This paper describes a method of detecting grammatical and lexical errors made by Japanese learners of English and other techniques that improve the accuracy of error detection with a limited amount of training data.
"In this paper, we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus, which contains information on learners’ errors."
"One of the most important things in keeping up with our current information-driven society is the acquisition of foreign languages, especially English for international communications."
"In developing a computer-assisted language teaching and learning environment, we have compiled a large-scale speech corpus of Japanese learner English, which provides a great deal of useful information on the construction of a model for the developmental stages of Japanese learners’ speaking abilities."
"In the support system for language learning, we have assumed that learners must be informed of what kind of errors they have made, and in which part of their utterances."
"To do this, we need to have a framework that will allow us to detect learners’ errors automatically."
"In this paper, we introduce a method of detect-ing learners’ errors, and we examine to what ex-tent this could be accomplished using our learner corpus data including error tags that are labeled with the learners’ errors."
"The corpus data was based entirely on audio-recorded data extracted from an interview test, the “Standard Speaking Test (SST)”."
The SST is a face-to-face interview between an examiner and the test-taker.
"In most cases, the examiner is a native speaker of Japanese who is officially certified to be an SST examiner."
"All the interviews are audio-recorded, and judged by two or three raters based on an SST evaluation scheme (SST levels 1 to 9)."
"We designed an original error tagset for learners’ grammatical and lexical errors, which were relatively easy to categorize."
"Our error tags contained three pieces of information, i.e., the part of speech, the grammatical/lexical system and the corrected form."
"We prepared special tags for some errors that cannot be categorized into any word class, such as the misordering of words."
Our error tagset currently consists of 45 tags.
"The following example is a sentence with an error tag. *I lived in &lt;at crr=&quot;&quot;&gt;the&lt;/at&gt; New Jersey. at indicates that it is an article error, and crr=”” means that the corrected form does not need an article."
"By referring to information on the corrected form indicated in an error tag, the sys-tem can convert erroneous parts into corrected equivalents."
"In this section, we would like to describe how we proceeded with error detection in the learner corpus."
We first divided errors into two groups de-pending on how their surface structures were dif-ferent from those of the correct ones.
"The first was an “omission”-type error, where the necessary word was missing, and an error tag was inserted to interpolate it."
"The second was a “replacement”-type error, where the erroneous word was en-closed in an error tag to be replaced by the cor-rected version."
We applied different methods to detecting these two kinds of errors.
"Omission-type errors were detected by estimat-ing whether or not a necessary word string was missing in front of each word, including delimit-ers."
We also estimated to which category the error belonged during this process.
What we call “error categories” here means the 45 error categories that are defined in our error tagset. (e.g. article and tense errors)
These are different from “error types” (omission or replacement).
"As we can see from Fig. 1, when more than one error category is given, we have two ways of choosing the best one."
Method A allows us to estimate whether there is a missing word or not for each error category.
This can be considered the same as deciding which of the two labels (E: “There is a missing word.” or C: “There is no missing word.”) should be inserted in front of each word.
"Here, there is an article miss-ing in front of “telephone”, so this can be consid-ered an omission-type error, which is categorized as an article error (“at” is a label that indicates that this is an article error.)."
"In Method B, if N error categories come up, we need to choose the most appropriate error category “k” from among N+1 categories, which means we have added one more category (+1) of “There is no missing word.” (la-beled with “C”) to the N error categories."
This can be considered the same as putting one of the N+1 labels in front of each word.
"If there is more than one error tag inserted at the same location, they are combined to form a new error tag."
"As we can see from Fig. 2, we referred to 23 pieces of information to estimate the error cate-gory: two preceding and following words, their word classes, their root forms, three combinations of these (one preceding word and one following word/two preceding words and one following word/one preceding word and two following words), and the first and last letter of the word immediately following. (In Fig. 2, “t” and “e” in “telephone”.)"
The word classes and root forms were acquired with “TreeTagger”.[REF_CITE]
Replacement-type errors were detected by es-timating whether or not each word should be de-leted or replaced with another word string.
The error category was also estimated during this process.
"As we did in detecting omission-type er-rors, if more than one error category was given, we use two methods of detection."
"Method C was used to estimate whether or not the word should be replaced with another word for each error cate-gory, and if it was to be replaced, the model esti-mated whether the word was located at the beginning, middle or end of the erroneous part."
"As we can see from Fig. 3, this can be considered the same as deciding which of the three labels (Eb: “The word is at the beginning of the erroneous part.”, Ee: “The word is in the middle or end.” or C: “The word is correct.”) must be applied to each word."
"Method D was used if N error categories came up and we chose an appropriate one for the word from among 2N+1 categories. “2N+1 cate-gories” means that we divided N categories into two groups, i.e., where the word was at the begin-ning of the erroneous part and where the word was not at the beginning, and we added one more where the word neither needed to be deleted nor replaced."
This can be considered the same as at-taching one of the 2N+1 labels to each word.
"To do this, we applied Ramshaw’s IOB scheme[REF_CITE]."
"If there was more than one error tag attached to the same word, we only referred to the tag that covered the highest number of words."
"As Fig. 4 reveals, 32 pieces of information are referenced to estimate an error category, i.e., the targeted word and the two preceding and follow-ing words, their word classes, their root forms, five combinations of these (the targeted word, the one preceding and one following/ the targeted word and the one preceding/ the targeted word and the one following/ the targeted word and the two preceding/ the targeted word and the two fol-lowing), and the first and last letters of the word."
The Maximum Entropy (ME) model[REF_CITE]is a general technique that is used to esti-mate the probability distributions of data.
"The over-riding principle in ME is that when nothing is known, the distribution should be as uniform as possible, i.e., maximum entropy."
"We calculated the distribution of probabilities p(a,b) with this method when Eq. 1 was satisfied and Eq. 2 was maximized."
"We then selected the category with maximum probability, as calculated from this dis-tribution of probabilities, to be the correct cate-gory. ∑ p(a,b)g j (a,b) = ∑ ~p(a,b)g j (a,b) (1) a∈A,b∈B a∈A,b∈B for ∀f j (1≤ j ≤ k) H(p) = − ∑ p(a,b)log(p(a,b)) (2) a∈A,b∈B"
We assumed that the constraint of feature sets f i (i ≦ j ≦ k) was defined by Eq. 1.
"This is where A is a set of categories and B is a set of contexts, and g j (a,b) is a binary function that returns value 1 when feature f j exists in context b and the category is a."
"Otherwise, g j (a,b) returns value 0. ~p (a,b) is the occurrence rate of the pair (a,b) in the training data."
We obtained data from 56 learners’ with error tags.
We tried to detect each error category using the methods discussed in Sections 3.2 and 3.3.
"There were some error categories that could not be de-tected because of the lack of training data, but we have obtained the following results for article er-rors which occurred most frequently."
We assumed that the results were inadequate because we did not have sufficient training data.
"To overcome this, we added the correct sentences to see how this would affect the results."
"As discussed in Section 2.1, our error tags pro-vided a corrected form for each error."
"If the erro-neous parts were replaced with the corrected forms indicated in the error tags one-by-one, ill-formed sentences could be converted into cor-rected equivalents."
We did this with the 50 items of training data to extract the correct sentences and then added them to the training data.
"We also added the interviewers’ utterances in the entire corpus data (totaling 1202 files, excluding 6 that were used as the test data) to the training data as correct sentences."
We added a total of 104925 correct new sentences.
The results we obtained by detecting article errors with the new data were as follows.
Omission- Recall rate 8/71 * 100 = 11.27(%) type errors Precision rate 8/11 * 100 = 72.73(%)
Replacement- Recall rate 0/43 * 100 = 0.00(%) type errors Precision rate 0/ 1 * 100 = 0.00(%)
"We found that although the recall rate de-creased, the precision rate went up through adding correct sentences to the training data."
We then determined how we could improve the results by adding the artificially made errors to the training data.
We did this only for article errors.
"We first ex-amined what kind of errors had been made with articles and found that “a”, “an”, “the” and the absence of articles were often confused."
We made up pseudo-errors just by replacing the correctly used articles with one of the others.
"The results of detecting article errors using the new training data, including the new corrected sentences described in Section 4.2, and 7558 sentences that contained artificially made errors were as follows."
We obtained a better recall and precision rate for omission-type errors.
There were no improvements for replacement-type errors.
"Since some more detailed context might be necessary to decide whether “a” or “the” must be used, the features we used here might be insufficient."
"In this paper, we explained how errors in learners’ spoken data could be detected and in the experiment, using the corpus as it was, the recall rate was about 30% and the precision rate was about 50%."
"By adding corrected sentences and artificially made errors, the precision rate rose to 80% while the recall rate remained the same."
This paper describes a spoken dialog Q-A system as a substitution for call centers.
"The system is capable of making dialogs for both fixing speech recognition errors and for clarifying vague questions, based on only large text knowledge base."
We in-troduce two measures to make dialogs for fixing recognition errors.
An experimental evaluation shows the advantages of these measures.
"When we use personal computers, we often en-counter troubles."
"We usually consult large manu-als, experts, or call centers to solve such troubles."
"However, these solutions have problems: it is diffi-cult for beginners to retrieve a proper item in large manuals; experts are not always near us; and call centers are not always available."
"Furthermore, op-eration cost of call centers is a big problem for en-terprises."
"Therefore, we proposed a spoken dialog Q-A system which substitute for call centers, based on only large text knowledge base."
"If we consult a call center, an operator will help us through a dialog."
The substitutable system also needs to make a dialog.
"First, asking backs for fixing speech recognition errors are needed."
Note that too many asking backs make the dialog inefficient.
"Sec-ondly, asking backs for clarifying users’ problems are also needed, because they often do not know their own problems so clearly."
"To realize such asking backs, we developed a sys-tem as shown in Figure 1."
The features of our system are as follows:
Precise text retrieval.
The system precisely retrieves texts from large
"Our system makes asking backs by showing them on a display, and users respond them by selecting the displayed buttons by mouses."
"Initially, we developed the system as a keyboard based Q-A system, and started its service[REF_CITE]at the web site of Microsoft Corporation."
The extension for speech input was done based on the one-year operation.
"Our system uses Julius[REF_CITE]as a Japanese speech recognizer, and it uses language model acquired from the text knowl-edge base of Microsoft Corporation."
"In this paper, we describe the above three features in Section 2, 3, and 4."
"After that, we show experi-mental evaluation, and then conclude this paper."
It is critical for a Q-A system to retrieve relevant texts for a question precisely.
"In this section, we describe the score calculation method, giving large points to modifier-head relations between bunsetsu [Footnote_1] based on the parse results of KNP[REF_CITE], to improve precision of text retrieval."
"1 Bunsetsu is a commonly used linguistic unit in Japanese, consisting of one or more adjoining content words and zero or more following functional words."
"Our system also uses question types, product names, and synonymous expression dictionary as described[REF_CITE]."
"First, scores of all sentences in each text are calcu-lated as shown in Figure 2."
Sentence score is the to-tal points of matching keywords and modifier-head relations.
"We give [Footnote_1] point to a matching of a key-word, and 2 points to a matching of a modifier-head relation (these parameters were set experimentally)."
"1 Bunsetsu is a commonly used linguistic unit in Japanese, consisting of one or more adjoining content words and zero or more following functional words."
Then sentence score is normalized by the maximum matching score (MMS) of both sentences as follows (the MMS is the sentence score with itself): sentence score the MMS of a the MMS of a user question text sentence
"Finally, the sentence that has the largest score in each text is selected as the representative sentence of the text."
"Then, the score of the sentence is regarded as the score of the text."
"In most cases, users’ questions are vague."
"To cope with such vagueness, our system uses the following two methods: asking backs using dialog cards and extraction of summaries that makes difference be-tween retrieved texts more clear (Figure 3)."
"If a question is very vague, it matches many texts, so users have to pay their labor on finding a rele-vant one."
Our system navigates users to the desired answer using dialog cards as shown in Figure 3.
We made about three hundred of dialog cards to throw questions back to users.
Figure 4 shows two dialog cards.
UQ (User Question) is fol-lowed by a typical vague user question.
"If a user question matches it, the dialog manager asks the back question after SYS , showing choices be-"
We define the confidence in recognition for each phrase in order to reject partial recognition errors.
"It is calculated based on word perplexity, which is of-ten used in order to evaluate suitability of language models for test-set sentences."
"We adopt word per-plexity because of the following reasons: incorrectly recognized parts are often unnatural in context, and words that are unnatural in context have high per-plexity values."
"As Julius uses trigram as its language model, the word perplexity is calculated as follows: tween SELECT and /SELECT ."
Every choice is followed by goto or retrieve . goto means that the system follow the another dialog cards if this choice is selected. retrieve means that the system retrieve texts using the query specified there.
"In most cases, the neighborhood of the part that matches the user question describes specific symp-toms and conditions of the problem users encounter."
Our system extracts such descriptions from the re-trieved texts as the summaries of them.
The algo-rithm is described[REF_CITE].
It is necessary for a spoken dialog system to deter-mine which portions of the speech input should be confirmed.
"Moreover, criteria for judging whether it should make confirmation or not are needed, be-cause too many confirmations make the dialog inef-ficient."
"Therefore, we introduce two criteria of con-fidence in recognition and significance for retrieval."
Our system makes two types of asking backs for fixing recognition errors (Figure 1).
"First, Julius out-puts -best candidates of speech recognition."
"Then, the system makes confirmation for significant parts based on confidence in recognition."
"After that, the system retrieves relevant texts in the text knowledge base using each candidate, and makes confirmation based on significance for retrieval.  s are summed up in each bunsetsu (phrases)."
"As a result, the system assigned the sum of s to each bunsetsu as the criterion for confidence in recognition."
We preliminarily defined the set of product names as significant phrases [Footnote_2] .
2 We are now developing a method to define the set of sig-nificant phrases semi-automatically.
"If the sums of s for any significant phrases are beyond the threshold (now, we set it 50), the system makes confirmation for these phrases."
The system calculates significance for retrieval us-ing -best candidates of speech recognition.
"Be-cause slight speech recognition errors are not harm-ful for retrieval results, we regard a difference that affects its retrieval result as significant."
"Namely, when the difference between retrieval results for each recognition candidate is large, we regard that the difference is significant."
Significance for retrieval is defined as a rate of disagreement of five high-scored retrieved texts among recognition candidates.
"For example, if there is a substituted part in two recognition candi-dates, and only one text is commonly retrieved out of five high-scored texts by both candidates, the sig-nificance for retrieval for the substituted part is 0.8 ( )."
"The system makes confirmation which candidate should be used, if significance for retrieval is beyond the threshold (now, we set it 0.5)."
We evaluated the system performance experimen-tally.
"For the experiments, we had 4 subjects, who were accustomed to using computers."
They made utterances by following given 10 scenarios and also made several utterances freely.
Figure 5 shows two successful dialogs by confirmation using confidence in recog-nition and by that using significance for retrieval.
"We experimented on the system using the 53 recorded utterances by the following methods: (1) Using correct transcription of recorded utter-ance, including fillers. (2) Using speech recognition results from which only fillers were removed. (3) Using speech recognition results and making confirmation by confidence in recognition. (4) Using -best candidates of speech recognition and making confirmation by significance for re-trieval."
"Here, . (5) Using -best candidates of speech recognition and both measures in (3) and (4)."
"In these experiments, we assumed that users al-ways correctly answer system’s asking backs."
We regarded a retrieval as a successful one if a relevant text was contained in ten high-scored retrieval texts.
Table 2 shows the result.
It indicates that our confirmation methods for fixing speech recognition errors improve the success rate.
"Furthermore, the success rate with both measures gets close to that with the transcriptions."
"Considering that the speech recognition correctness is about 70%, the proposed dialog strategy is effective."
We proposed a spoken dialog Q-A system in which asking backs for fixing speech recognition errors and those for clarifying vague questions are integrated.
"To realize dialog for fixing recognition errors based on large text knowledge base, we introduced two measures of confidence in recognition and signif-icance for retrieval."
The experimental evaluation shows the advantages of these measures.
"We have been investigating an interactive approach for Open-domain QA (ODQA) and have constructed a spoken interactive ODQA system, SPIQA."
The system de-rives disambiguating queries (DQs) that draw out additional information.
"To test the efficiency of additional information re-quested by the DQs, the system recon-structs the user’s initial question by com-bining the addition information with ques-tion."
The combination is then used for an-swer extraction.
Experimental results re-vealed the potential of the generated DQs.
"Open-domain QA (ODQA), which extracts answers from large text corpora, such as newspaper texts, has been intensively investigated in the Text REtrieval Conference (TREC)."
ODQA systems return an ac-tual answer in response to a question written in a natural language.
"However, the information in the first question input by a user is not usually sufficient to yield the desired answer."
Interactions for col-lecting additional information to accomplish QA are needed.
"To construct more precise and user-friendly ODQA systems, a speech interface is used for the interaction between human beings and machines."
Our goal is to construct a spoken interactive ODQA system that includes an automatic speech recognition (ASR) system and an ODQA system.
"To clarify the problems presented in building such a system, the QA systems constructed so far have been classified into a number of groups, depending on their target domains, interfaces, and interactions to draw out additional information from users to ac-complish set tasks, as is shown in Table 1."
"In this table, text and speech denote text input and speech input, respectively."
The term “addition” represents additional information queried by the QA systems.
This additional information is separate to that de-rived from the user’s initial questions.
"To construct spoken interactive ODQA systems, the following problems must be overcome: 1. Sys-tem queries for additional information to extract an-swers and effective interaction strategies using such queries cannot be prepared before the user inputs the question. 2."
Recognition errors degrade the perfor-mance of QA systems.
Some information indispens-able for extracting answers is deleted or substituted with other words.
"Our spoken interactive ODQA system, SPIQA, copes with the first problem by adopting disam-biguating users’ questions using system queries."
"In addition, a speech summarization technique is ap-plied to handle recognition errors."
"Figure 1 shows the components of our system, and the data that flows through it."
"This system com-prises an ASR system (SOLON), a screening filter that uses a summarization method, and ODQA en-gine (SAIQA) for a Japanese newspaper text corpus, a Deriving Disambiguating Queries (DDQ) module, and a Text-to-Speech Synthesis (TTS) engine (Fi-nalFluet)."
Questions consisting of 69 sentences read aloud by seven male speakers were transcribed by our ASR system.
The question transcriptions were processed with a screening filter and input into the ODQA engine.
Each question consisted of about 19 mor-phemes on average.
"The sentences were grammat-ically correct, formally structured, and had enough information for the ODQA engine to extract the cor-rect answers."
The mean word recognition accuracy obtained by the ASR system was 76%.
Screening was performed by removing recognition errors using a confidence measure as a threshold and then summarizing it within an 80% to 100% com-paction ratio.
"In this summarization technique, the word significance and linguistic score for summa-rization were calculated using text from Mainichi newspapers published from 1994 to 2001, compris-ing 13.6M sentences with 232M words."
"The SD-CFG for the word concatenation score was calcu-lated using the manually parsed corpus of Mainichi newspapers published from 1996 to 1998, consist-ing of approximately 4M sentences with 68M words."
The number of non-terminal symbols was 100.
The posterior probability of each transcribed word in a word graph obtained by ASR was used as the confi-dence score.
"The word generality score A G was computed using the same Mainichi newspaper text described above, while the SDCFG for the dependency ambiguity score A D for each phrase was the same as that used in (C. Hori et. al., 2003)."
Eighty-two types of inter-rogative sentences were created as disambiguating queries for each noun and noun-phrase in each ques-tion and evaluated by the DDQ module.
The linguis-tic score L indicating the appropriateness of inter-rogative sentences was calculated using 1000 ques-tions and newspaper text extracted for three years.
"The structural ambiguity score A D was calculated based on the SDCFG, which was used for the screen-ing filter."
The DQs generated by the DDQ module were eval-uated in comparison with manual disambiguation queries.
"Although the questions read by the seven speakers had sufficient information to extract ex-act answers, some recognition errors resulted in a loss of information that was indispensable for ob-taining the correct answers."
The manual DQs were made by five subjects based on a comparison of the original written questions and the transcription results given by the ASR system.
"The automatic DQs were categorized into two classes: APPRO-PRIATE when they had the same meaning as at least one of the five manual DQs, and INAPPRO-PRIATE when there was no match."
The QA per-formance in using recognized (REC) and screened questions (SCRN) were evaluated by MRR (Mean Reciprocal Rank)[URL_CITE]
SCRN was compared with the transcribed question that just had recognition errors removed (DEL).
"In addition, the questions reconstructed manually by merging these questions and additional information requested the DQs generated by using SCRN, (DQ) were also evaluated."
The additional information was extracted from the original users’ question without recognition errors.
"In this study, adding information by using the DQs was performed only once."
Table 2 shows the evaluation results in terms of the appropriateness of the DQs and the QA-system MRRs.
The results indicate that roughly 50% of the DQs generated by the DDQ module based on the screened results were APPROPRIATE.
The MRR for manual transcription (TRS) with no recognition errors was 0.43.
"In addition, we could improve the MRR from 0.25 (REC) to 0.28 (DQ) by using the DQs only once."
Experimental results revealed the potential of the generated DQs in compensating for the degradation of the QA performance due to recog-nition errors.
"The proposed spoken interactive ODQA system, SPIQA copes with missing information by adopt-ing disambiguation of users’ questions by system queries."
"In addition, a speech summarization tech-nique was applied for handling recognition errors."
"Although adding information was performed using DQs only once, experimental results revealed the potential of the generated DQs to acquire indispens-able information that was lacking for extracting an-swers."
"In addition, the screening filter helped to gen-erate the appropriate DQs."
Future research will in- clude an evaluation of the appropriateness of DQs derived repeatedly to obtain the final answers.
"In addition, the interaction strategy automatically gen-erated by the DDQ module should be evaluated in terms of how much the DQs improve QA’s total per-formance."
"In this paper, we proposed a new super-vised word sense disambiguation (WSD) method based on a pairwise alignment technique, which is used generally to mea-sure a similarity between DNA sequences."
The new method obtained 2.8%-14.2% improvements of the accuracy in our ex-periment for WSD.
"WSD has been recognized as one of the most impor-tant subjects in natural language processing, espe-cially in machine translation, information retrieval, and so[REF_CITE]."
"Most of previ-ous supervised methods can be classified into two major ones; approach based on association, and ap-proach based on selectional restriction."
"The former uses some words around a target word, represented by n-word window."
"The latter uses some syntactic relations, say, verb-object, including necessarily a target word."
"However, there are some words that one approach gets good result for them while another gets worse, and vice versa."
"For example, suppose that we want to distinguish between “go off or discharge” and “terminate the employment” as a sense of “fire”."
Consider the sentence in Brown Corpus [Footnote_1] :
"1 In this case, we consider only one sentential context for the simplicity."
"The words such as “musket”, “loaded” and “bird shot” would seem useful in deciding the sense of “fire”, and serve as clue to leading the sense to “go off or discharge”."
It seems that there is no clue to an-other sense.
"For this case, an approach based on as-sociation is useful for WSD."
"However, an approach based on selectional restriction would not be appro-priate, because these clues do not have the direct syntactic dependencies on “fire”."
"On the other hand, consider the sentence in EDR Corpus:"
Police said Haga was immediately fired from the force.
The most significant fact is that “Haga” (a person’s name) appears as the direct object of “fire”.
"A selec-tional restriction approach would use this clue ap-propriately, because there is the direct dependency between “fire” and “Haga”."
"However, an associa-tion approach would make an error in deciding the sense, because “Police” and “force” tend to be a noise, from the point of view of an unordered set of words."
"Generally, an association does not use a syn-tactic dependency, and a selectional restriction uses only a part of words appeared in a sentence."
"In this paper, we present a new method for WSD, which uses syntactic dependencies for a whole sen-tence as a clue."
They contain both of all words in-cluded in a sentence and all syntactic dependencies in it.
"Our method is based on a technique of pair-wise alignment, and described in the following two sections."
"Using our method, we have gotten appro-priate sense for various cases including above exam-ples."
"In section 4, we describe our experimental re-sult for WSD on some verbs in SENSEVAL-1[REF_CITE]."
Our method has the features on an association and a selectional restriction approach both.
It can be ap-plied with the various sentence types because our method can treat a local (direct) and a whole sen-tence dependency.
Our method is based on the fol-lowing steps;
"Parse the input sentence with syntactic parser 2 , and find all paths from root to leaves in the resulting dependency tree."
2 We assume that we can get the correct syntactic structure here. (See section 4)
Compare the paths from Step 1. with proto-type paths prepared for each sense of the target word.
Find a summation of similarity between each prototype and input path for each sense.
Select the sense with the maximum value of the summation.
We describe our method in detail in the followings.
"In our method, we consider paths from root to leaves in a dependency tree."
"For example, consider the sentence “we consider a path in a graph”."
"This sentence has three leaves in the dependency struc-ture, and consequently has three paths from root to leaves; (consider, SUB, we), (consider, OBJ, path, a) and (consider, OBJ, path, in, graph, a). “SUB” and “OBJ” in the paths are the elements added au-tomatically using some rules in order to make a re-markable difference between verb-subject and verb-object."
"We think this sequence structure of word would serve as a clue to WSD very well, and we regard a set of the sequences obtained from an input sentence as the context of a target word."
The general intuition for WSD is that words with similar context have the same sense[REF_CITE].
"That is, once we prepare the pro-totype sequences for each sense, we can determine the sense of the target word as one with the most similar prototype set."
We measure a similarity be-tween a set of prototype sequences T and a set of sequences from input sentence  Tp .
Let T and T p 1 p 2  p respectively. p and p arese- P T jm i quences of words.
"We define the similarity between T and T , sim T T , as following: ∑ f i max alignment p i p j sim T T (1) p i P T p j P T sim T T is not commutative."
"That is, sim T T  sim T T . alignment p i p j is an alignment score between the sequences p i and p j , defined in the next section. f i is a weight function characteristic of the sequence p i , defined as following:  u i if max alignment p i p j t i f i p j P T (2) v i otherwise where u i and v i are arbitrary constants and t i is arbi-trary threshold."
"Using equation (1), we can estimate a similarity between the context of a target word and prototype context, and can determine the sense of a target word by selecting the prototype with the maximum simi-larity."
An example of the prototype sequences for verb “fire” is shown in Figure 1.
A prototype sequence is represented like a regular expression.
"For the present, we obtain the sequence by hand."
The basic policy to obtain prototypes is to observe the common features on dependency trees in which target word is used in the same sense.
We have some ideas about a method to obtain prototypes automatically.
We attempt to apply the method of pairwise align-ment to measuring the similarity between sequences.
"Recently, the technique of pairwise alignment is used generally in molecular biology research as a basic method to measure the similarity between proteins or DNA sequences[REF_CITE]."
"There have been several ways to find the pairwise alignment, such as the method based on Dynamic Programming, one based on Finite State Automa-ton, and so on (Durbinet al., 1998)."
"In our method, we apply the method using DP matrix, as in Fig-ure 2."
"We have shown the pairwise alignment be-tween sequences p (worked, at, composition, the) and p (is, make, at, home) as an example."
"In a matrix, a vertical and horizontal transition means a gap and is assigned a gap score."
A diag-onal transition means a substitution and is assigned a score based on the similarity between two words corresponding to that point in the matrix.
"Actually, the following value is calculated in each node, using values which have been calculated in its three previ-ous nodes."
"F i 1 j subst - w j F i j max F i j 1 subst w i - (3) F i 1 j 1 subst w i w j where subst - w j and subst w i - represent re-spectively to substitute w j and w i with a gap (-), and return the gap score. subst w i w j represent the score of substituting w i with w j or vice versa."
"Now let the word w has synsets s 1 s 2  s and w has s 1 s 2  s on WordNet hierarchy (Miller et k l al., 1990)."
"For simplicity, we define the subst w w as following, based on the semantic distance (Stetina  and[REF_CITE]). subst w w 2 max sd s i s j 1 (4) i j where sd s i s j is the semantic distance between two synsets s i and s j ."
"Because 0 1,sd s i s j 1 subst w w 1."
"The score of the substitution between identical words is 1, and one between two words with no common ancestor in the hierarchy is 1."
We simply define the gap score as 1.
"Up to the present, we have obtained the experimental results on 7 verbs in SENSEVAL-1 [Footnote_3] ."
3 We have experimented on verbs in SENSEVAL-1 one by one alphabetically. The word “amaze” is omitted because it has only one verbal sense.
"In our exper-iment, for all sentences including target word in the training and test corpus of SENSEVAL-1, we make a parsing using Apple Pie Parser[REF_CITE]and additional vertices using some rules automati-cally."
"If the resulted parsing includes some errors, we remove them by hand."
Then we obtain the se-quence patterns by hand from training data and at-tempt WSD using equation (1) for test data.
"Because of various length of sequence, we assign score zero to the preceding and right-end gaps in an alignment."
We show our experimental results in Table 1.
"In SENSEVAL-1, precisions and recalls are calculated by three scoring ways, fine-grained, mixed-grained and coarse-grained scoring."
We show the results only by fine-grained scoring which is evaluated by distinguishing word sense in the strictest way.
It is impossible to make simple comparison with the participants in SENSEVAL-1 because our method needs supervised learning by hand.
"However, 2.8%- 14.2% improvements of the accuracy compared with the best system seems significant, suggesting that our method is promising for WSD."
There are two major limitations in our method; one of syntactic information and of knowledge acquisi- tion by hand.
The former is that our method assumes we can get the correct syntactic information.
"In fact, the accu-racy and performance of syntactic analyzer are being improved more and more, consequently this disad-vantage would become a minor problem."
"Because a similarity between sequences derived from syntactic dependencies is calculated as a numerical value, our method would also be suitable for integration with a probabilistic syntactic analyzer."
"The latter, which is more serious, is that the se-quence patterns used as clue to WSD are acquired by hand at the present."
"In molecular biology re- search, several attempts to obtain sequence patterns automatically have been reported, which can be ex-pected to motivate ours for WSD."
We plan to con-struct an algorithm for an automatic pattern acquisi-tion from large scale corpora based on those biolog-ical approaches.
"The FrameNet project has developed a lexical knowledge base providing a unique level of detail as to the the possible syn-tactic realizations of the specific seman-tic roles evoked by each predicator, for roughly 7,000 lexical units, on the ba-sis of annotating more than 100,000 ex-ample sentences extracted from corpora."
An interim version of the FrameNet data was released[REF_CITE]and is be-ing widely used.
"A new, more portable version of the FrameNet software is also being made available to researchers else-where, including the Spanish FrameNet project."
"This demo and poster will briefly ex-plain the principles of Frame Semantics and demonstrate the new unified tools for lexicon building and annotation and also FrameSQL, a search tool for finding pat-terns in annotated sentences."
We will dis-cuss the content and format of the data re-leases and how the software and data can be used by other NLP researchers.
"FrameNet [Footnote_1][REF_CITE]is a lexicographic research project which aims to produce a lexicon contain-ing very detailed information about the relation be- tween the semantics and the syntax of predicators, including verbs, nouns and adjectives, for a substan-tial subset of English."
"The basic unit of analysis is the semantic frame, defined as a type of event or state and the partici-pants and “props” associated with it, which we call frame elements (FEs). [Footnote_2]"
"2 In similar approaches, these have been referred to as schemas or scenarios, with their associated roles or slots."
Frames range from highly abstract to quite specific.
"An example of an abstract frame would be the Replacement frame, with FEs such as O LD and N EW as in the sentence Pat re-placed [ Old the curtains] [ New with wooden blinds]."
"One sense of the verb replace is associated with the Replacement frame, thus constituting one lexical unit (LU), the basic unit of the FrameNet lexicon."
"An example of a more specific frame is Ap-ply heat, with FEs such as COOK , FOOD , MEDIUM , and DURATION . as in Boil [ Food the rice] [ Duration for 3 minutes] [ Medium in water], then drain. [Footnote_3] LUs in Apply heat include char, fry, grill, and mi-crowave, etc."
"3 In this sentence, as in most examples of boil in recipes, the COOK is constructionally null-instantiated, because of the imperative."
"In our daily work, we define a frame and its FEs, make lists of words that evoke the frame (its LUs), extract example sentences containing these LUs from corpora, and semi-automatically annotate the parts of the sentences which are the realizations of these FEs, including marking the phrase type (PT) and grammatical function (GF)."
"We can then auto-matically create a report which constitutes a lexical entry for this LU, detailing all the possible ways in which these FEs can be syntactically realized."
"The annotated sentences and lexical entries for approxi-mately 7,000 LUs will be available on the FN web-site and the data will be released by the end of Au-gust in several formats."
"The development of the theory of Frame Semantics began more than 25 years ago[REF_CITE], but since 1997, thanks to two NSF grants [Footnote_4] , we have been able to apply it in a serious way to building a lexicon which we intend to be both usable by human beings and machine-tractable, so that it can serve as a lexical database for NLP, computational lexical semantics, etc."
"4 We are grateful to the National Science Foundation for funding the project through two grants, IRI #9618838 and ITR/HCI #0086132. We refer to these two three-year stages in the life of the project as FrameNet I and FrameNet II."
"In FrameNet II, all the data, including the definitions of frames, FEs, and LUs and all of the sentences and the an-notation associated with them is stored in one rela-tional database implemented in MySQL[REF_CITE]."
The FrameNet public website contains an index by frame and an index by LU which links to both the lexical entry and the full annotation for each LU.
The frame-to-frame relations which are now being entered in the database will be visible on the website soon.
"The HTML version of the data consists of all the files on the web site, so that users can set up a local copy and browse it with any web browser."
"It is fairly compact, less than 100 Mb in all."
"The plain XML version of the data consists of the following files: frames.xml This file contains the descriptions of all the 450 frames and their FEs, totaling more than 3,000."
Each frame also includes informa-tion as to frame-to-frame relations. luNNN.xml There is one such file per LU (roughly 7500) which contain the example sentences and annotation (if any) for each LU. relations.xml A file containing information about frame-to-frame and FE-to-FE relations and meta-relations between them.
"We intend to have a version of the XML that includes RDF of the DAML+OIL flavor, so that the FN frames and FEs can be related to existing ontologies and Semantic Web-aware applications can access FN data using a standard methodology."
"Narayanan has created such a version for the FN I data, and a new version reflecting the more complex FN II data is under constructi[REF_CITE]."
The FN software used for frame definition and an-notation has been fundamentally rewritten since the demo at the LREC conference last summer[REF_CITE].
"The two major changes are (1) combining the frame editing tools and the annotation tools into a single GUI, making the interface more intuitive and (2) moving to a client-server model."
"In the previous version, each client accessed the database directly, which made it very difficult to avoid collisions between users, and meant that each client was large, containing a lot of the logic of the application, MySQL-specific queries, etc."
"In the new version, the basic modules are now the MySQL database, an application server, and one or more client processes."
"This has a number of advantages: (1) All the database calls are made by the server, making it much easier to avoid conflicts between users. (2) The application server contains nearly all the logic, meaning that the clients are “thin” pro-cesses, concerned mainly with the GUI. (3) The sep-aration into client and server makes it easier to set up remote access to the FN database. (4) The increased overhead caused by the more complex architecture is at least offset by the ability to cache frequently-requested data on the server, making access much faster."
"The public FrameNet web pages contain static versions of several reports drawn from the database, notably, the lexical entry report, displaying all the valences of each LU."
"The working environment for the staff includes dynamic versions of these reports and several others, all written as java applets."
Par-tially shared code makes these reports accessible within the desktop package as well.
We are currently working on defining a FN API and writing libraries for accessing the database from other programs.
We plan to distribute a command-line utility as a demonstration of this API.
Prof. Hiroaki Sato of Senshu University has written a web-based tool which allows users to search ex-isting FN annotations in a variety of ways.
"The tool also makes conveniently available several other elec-tronic resources such as WordNet, and other on-line dictionaries."
It is especially useful for doing conven-tional lexicography.
"The major product of the project is the lexical database of frame descriptions and annotated sen-tences; although these clearly are potentially very useful in many sorts of NLP task, FrameNet (at least in its present phase) remains primarily lexi-cographic."
"Nevertheless, as a an intermediate step toward applications such as automatic text summa-rization, we have recently begun studying kernel dependency graphs (KDGs), which provide a sort of automatic summarization of annotated sentences."
"KDGs consist of the predicator (verb, noun, or adjective), the lexical heads of its dependents the “marking” on the dependents (prepositions, complementizers, etc. if any), and the FEs of the dependents."
"To take a simple example, (1-a), which is anno-tated for the target chained in the Attaching frame, could be represented as the KDG in (1-b). (1) a. [ Agent Four activists] chained [ Item themselves] [ Goal to an oil drilling rig being towed to the Barents Sea] [ Time in early August]. b. &lt;KDG frame=&quot;Attaching&quot; LU=&quot;chain.v&quot;&gt; &lt;Agent&gt;activists&lt;/Agent&gt; &lt;Item&gt;themselves&lt;/Item&gt; &lt;Goal&gt;to:oil\_drilling\_rig&lt;/Goal&gt; &lt;Time&gt;in:August&lt;/Time&gt; &lt;/KDG&gt;"
"The situation can be complicated by the pres-ence of higher control verbs and “transparent” nouns which bring about a mismatch between the semantic head and the syntactic head of an FE[REF_CITE], as in (2), which should have the same KDG as (1-a). (2) [ Agent Four activists] planned to chain [ Item themselves] [ Goal to the bottom of an oil drilling rig being towed to the Barents Sea] [ Time in early August]."
"A large majority of FEs are annotated with a triplet of labels, one for the FE name, one for the phrase type and one for the grammatical function of the constituent with regard to the target."
"But the FN software allows more than three layers of annotation for a single target, for situations such as when one FE contains another (e.g. in [ Agent You] ’re hurting [ Body part [ Victim my] arms])."
"In addition, the FN software allows us to annotate more than one target in a sentence."
A full represen-tation of the meaning of a sentence can be built up by composing the semantics of the frames evoked by the major predicators.
"In addition to the original lexicographic goal, a pre-liminary version of our frame descriptions and the set of more than 100,000 annotated sentences have been released to more than 80 research groups in more than 15 countries."
"The FN data is being used for a variety of purposes, some of which we had foreseen and others which we had not; these in-clude uses as teaching materials for lexical seman-tics classes, as a basis for developing multi-lingual lexica, as an interlingua for machine translation, and as training data for NLP systems that perform ques-tion answering, information retrieval[REF_CITE], and automatic semantic parsing[REF_CITE]."
A number of scholars have expressed interest in building FrameNets for other languages.
"Of these, three have already begun work: In Spain, a team from several universities, led by Prof. Carlos Subi-rats of U A Barcelona, is building using their own extraction software and the FrameNet desktop tools to build a Spanish FrameNet (Subirats and Petruck, forthcoming 2003)[URL_CITE]"
"In Saarbrücken, Germany, work is proceeding on hand-annotating a parsed corpus with FrameNet FE labels (Erk et al., )."
"And in Japan, researchers from Keio University and University of Tokyo are building a Japanese FrameNet in the domains of motion and communication, using a large newspaper corpus."
"We will demonstrate how the software can be used to create a frame, create a frame element, create a lexi-cal unit , define a set of rules for extracting example sentences (and, optionally, marking FEs on them), open an existing LU and annotate sentences, mark an LU as finished, create a frame-to-frame relation, and attach a semantic type to an FE or an LU."
We will demonstrate the reports available on the internal web pages.
"We will show the complex searches against the FrameNet data that can be run using FrameSQL, including displaying the result-ing sentences as KDGs."
We will demonstrate how frames can be composed to represent the meaning of sentences using a (manual) frame semantic pars-ing of a newspaper crime report as an example.
"In this paper, we present a method that automatically constructs a Named En-tity (NE) tagged corpus from the web to be used for learning of Named En-tity Recognition systems."
We use an NE list and an web search engine to col-lect web documents which contain the NE instances.
The documents are refined through sentence separation and text re-finement procedures and NE instances are finally tagged with the appropriate NE cat-egories.
Our experiments demonstrates that the suggested method can acquire enough NE tagged corpus equally useful to the manually tagged one without any human intervention.
"Current trend in Named Entity Recognition (NER) is to apply machine learning approach, which is more attractive because it is trainable and adaptable, and subsequently the porting of a machine learning sys-tem to another domain is much easier than that of a rule-based one."
Various supervised learning meth-ods for Named Entity (NE) tasks were successfully applied and have shown reasonably satisfiable per-formance.([REF_CITE])
"However, most of these systems heavily rely on a tagged corpus for training."
"For a machine learning approach, a large corpus is required to circumvent the data sparseness problem, but the dilemma is that the costs required to annotate a large training corpus are non-trivial."
"In this paper, we suggest a method that automati-cally constructs an NE tagged corpus from the web to be used for learning of NER systems."
We use an NE list and an web search engine to collect web doc-uments which contain the NE instances.
The doc-uments are refined through the sentence separation and text refinement procedures and NE instances are finally annotated with the appropriate NE categories.
This automatically tagged corpus may have lower quality than the manually tagged ones but its size can be almost infinitely increased without any hu-man efforts.
"To verify the usefulness of the con-structed NE tagged corpus, we apply it to a learn-ing of NER system and compare the results with the manually tagged corpus."
"We only focus on the three major NE categories (i.e., person, organization and location) because others are relatively easier to recognize and these three cat-egories actually suffer from the shortage of an NE tagged corpus."
Various linguistic information is already held in common in written form on the web and its quantity is recently increasing to an almost unlimited extent.
The web can be regarded as an infinite language re-source which contains various NE instances with di-verse contexts.
It is the key idea that automatically marks such NE instances with appropriate category labels using pre-compiled NE lists.
"However, there should be some general and language-specific con- siderations in this marking process because of the word ambiguity and boundary ambiguity of NE in-stances."
"To overcome these ambiguities, the auto-matic generation process of NE tagged corpus con-sists of four steps."
The process first collects web documents using a web search engine fed with the NE entries and secondly segments them into sen-tences.
"Next, each sentence is refined and filtered out by several heuristics."
An NE instance in each sentence is finally tagged with an appropriate NE category label.
Figure 1 explains the entire proce-dure to automatically generate NE tagged corpus.
It is not appropriate for our purpose to randomly col-lect documents from the web.
This is because not all web documents actually contain some NE instances and we also do not have the list of all NE instances occurring in the web documents.
We need to col-lect the web documents which necessarily contain at least one NE instance and also should know its category to automatically annotate it.
This can be accomplished by using a web search engine queried with pre-compiled NE list.
"As queries to a search engine, we used the list of Korean Named Entities composed of 937 per-son names, [Footnote_1],000 locations and 1,050 organizations."
1 We used Empas ([URL_CITE]
"Using a Part-of-Speech dictionary, we removed am-biguous entries which are not proper nouns in other contexts to reduce errors of automatic annotation."
"For example, ‘E¶(kyunggi, Kyunggi/business con- ditions/a game)’ is filtered out because it means a lo-cation (proper noun) in one context, but also means business conditions or a game (common noun) in other contexts."
"By submitting the NE entries as queries to a search engine 1 , we obtained the max-imum 500 of URL’s for each entry."
"Then, a web robot visits the web sites in the URL list and fetches the corresponding web documents."
Features used in the most NER systems can be clas-sified into two groups according to the distance from a target NE instance.
The one includes internal fea-tures of NE itself and context features within a small word window or sentence boundary and the other in-cludes name alias and co-reference information be-yond a sentence boundary.
"In fact, it is not easy to extract name alias and co-reference information di-rectly from manually tagged NE corpus and needs additional knowledge or resources."
"This leads us to focus on automatic annotation in sentence level, not document level."
"Therefore, in this step, we split the texts of the collected documents into sentences[REF_CITE]and remove sentences without target NE instances."
"The collected web documents may include texts ac-tually matched by mistake, because most web search engines for Korean use n-gram, especially, bi-gram matching."
This leads us to refine the sentences to ex-clude these erroneous matches.
"Sentence refinement is accomplished by three different processes: sep-aration of functional words, segmentation of com-pound nouns, and verification of the usefulness of the extracted sentences."
"An NE is often concatenated with more than one josa, a Korean functional word, to compose a Korean word."
"Therefore we need to separate the functional words from an NE instance to detect the boundary of the NE instance and this is achieved by a part-of-speech tagger, POSTAG, which can detect unknown words[REF_CITE]."
The separation of functional words gives us another benefit that we can resolve the ambiguities between an NE and a common noun plus functional words and filter out erroneous matches.
"For example, ‘E¶ê(kyunggi-do)’ can be interpreted as either ‘E¶ê(Kyunggi Province)’ or ‘E¶+ê(a game also)’ according to its context."
We can remove the sentence containing the latter case.
A josa-separated Korean word can be a com-pound noun which only contains a target NE as a substring.
This requires us to segment the compound noun into several correct single nouns to match with the target NE.
"If the segmented single nouns are not matched with a target NE, the sentence can be fil-tered out."
"For example, we try to search for an NE entry, ‘¶Á(Fin."
"KL, a Korean singer group)’ and may actually retrieve sentences including ‘˚¶Á ě(surfing club)’."
"The compound noun, ‘˚¶Áě’, can be divided into ‘˚¶(surfing)’ and ‘Áě(club)’ by a compound-noun segmenting method[REF_CITE]."
"Since both ‘˚¶’ and ‘Áě’ are not matched with our target NE, ‘¶Á’, we can delete the sentences."
"Although a sentence has a correct tar-get NE, if it does not have context information, it is not useful as an NE tagged corpus."
We also removed such sentences.
The sentences selected by the refining process ex-plained in previous section are finally annotated with the NE label.
"We acquired the NE tagged corpus in-cluding 68,793 NE instances through this automatic annotation process."
We can annotate only one NE instance per sentence but almost infinitely increase the size of the corpus because the web provides un-limited data and our process is fully automatic.
"For effectiveness of the learning, both the size and the accuracy of the training corpus are important."
"Generally, the accuracy of automatically created NE tagged corpus is worse than that of hand-made cor-pus."
"Therefore, it is important to examine the useful-ness of our automatically tagged corpus compared to the manual corpus."
"We separately trained the de-cision list learning features using the automatically annotated corpus and hand-made one, and compared the performances."
Table 1 shows the details of the corpus used in our experiments. [Footnote_2]
2 We used the manual corpus used[REF_CITE]as training and test data.
"Through the results in Table 2, we can verify that the performance with the automatic corpus is supe-rior to that with only the seeds and comparable to that with the manual corpus."
"Moreover, the domain of the manual training corpus is same with that of the test corpus, i.e., news and novels, while the do-main of the automatic corpus is unlimited as in the web."
This indicates that the performance with the automatic corpus should be regarded as much higher than that with the manual corpus because the per-formance generally gets worse when we apply the learned system to different domains from the trained ones.
"Also, the automatic corpus is pretty much self-contained since the performance does not gain much though we use both the manual corpus and the auto-matic corpus for training."
"As another experiment, we tried to investigate how large automatic corpus we should generate to get the satisfiable performance."
We measured the perfor-mance according to the size of the automatic cor-pus.
We carried out the experiment with the deci-sion list learning method and the result is shown in Table 3.
"Here, 5% actually corresponds to the size of the manual corpus."
"When we trained with that size of the automatic corpus, the performance was very low compared to the performance of the manual cor-pus."
The reason is that the automatic corpus is com- posed of the sentences searched with fewer named entities and therefore has less lexical and contextual information than the same size of the manual cor-pus.
"However, the automatic generation has a big merit that the size of the corpus can be increased al-most infinitely without much cost."
"From Table 3, we can see that the performance is improved as the size of the automatic corpus gets increased."
"As a result, the NER system trained with the whole au-tomatic corpus outperforms the NER system trained with the manual corpus."
We also conducted an experiment to examine the saturation point of the performance according to the size of the automatic corpus.
This experiment was focused on only ‘person’ category and the result is shown in Table 4.
"In the case of ‘person’ category, we can see that the performance does not increase any more when the corpus size exceeds 1.2 million words."
"In this paper, we presented a method that automat-ically generates an NE tagged corpus using enor-mous web documents."
We use an internet search en-gine with an NE list to collect web documents which may contain the NE instances.
The web documents are segmented into sentences and refined through sentence separation and text refinement procedures.
The sentences are finally tagged with the NE cat-egories.
We experimentally demonstrated that the suggested method could acquire enough NE tagged corpus equally useful to the manual corpus without any human intervention.
"In the future, we plan to ap-ply more sophisticated natural language processing schemes for automatic generation of more accurate NE tagged corpus."
"In the Japanese language, as a predicate is placed at the end of a sentence, the con-tent of a sentence cannot be inferred until reaching the end."
"However, when the con-tent is complicated and the sentence is long, people want to know at an earlier stage in the sentence whether the content is negative, affirmative, or interrogative."
"In Japanese, the grammatical form called the KO-OU relation exists."
The KO-OU relation is a kind of concord.
"If a KO ele-ment appears, then an OU element ap-pears in the latter part of a sentence."
It is being pointed out that the KO-OU relation gives advance notice to the element that appears in the latter part of a sentence.
"In this paper, we present the method of ex-tracting automatically the KO-OU expres-sion data from large-scale electronic corpus and verify the usefulness of the KO-OU expression data."
The Japanese language has a grammatical form called the KO-OU relation.
"The KO-OU relation is a kind of concord, also referring to a sort of bound relation that a KO element appearing in a sentence is followed by an OU element in the latter part of the same sentence."
"On the contrary, the cooccur-rence relation refers to two words appearing in the same sentence."
"Because Japanese predicates are usually located at the end of sentences, the contents of Japanese sentences cannot be decided until reaching the end."
"Furthermore, in Japanese, it is hard to comprehend the meaning of the sentence without reading through the entire sentence."
The KO-OU relation is the grammatical form which can be helpful for un-derstanding the sentence meaning at the early stage.
"While in archaic Japanese, KAKARI-MUSUBI, which had morphemic KO-OU relation between KAKARI-JOSI [Footnote_1] and the conjugation at the end of a sentence, had been used."
1 A Japanese particle.
KAKARI-MUSUBI gave advance notice to the elements that would appear toward the end of a sentence due to KAKARI-JOSI.
"Today, KAKARI-MUSUBI has dropped out of use."
"However, the KO-OU relation such as &quot;sika-nai (only) &quot; or &quot;kessite-nai (never) &quot; is present."
"In this research, we have attempted to collect such ele-ments to extract KO-OU expression data."
"In this paper, the main points of argument are as follows: (1) Method of extracting automatically the KO-OU expression data. (2) What the KO-OU expression data can be used for."
It suggested that there were certain adverbs that have replaced KAKARI-JOSI in the archaic Japanese words.[REF_CITE]described the KO-OU relation of sentence elements.
"According to Masuoka, some sentences have the KO-OU expressions as shown in Table 1."
"However, this has the following weaknesses."
"The KO and OU elements in a KO-OU relation are placed together in the same category, and there is no description as to the OU element."
"Furthermore, only a limited number of elements are listed."
And the objectivity of the KO and OU elements is not guaranteed.
The KO-OU expression data is useful as basic data to dissolve ambiguity in parsing and to decide on the modification relation.
"However, first of all, it is necessary for the data to have a certain length for being useful basic data."
"Secondly, it also needs to be objective."
"Therefore, we have attempted to extract KO-OU relations automatically from large-scale corpus."
The KO-OU expression data is useful for dissolv-ing ambiguity of parsing.
"Furthermore, it is useful for deciding the modification relation (Figure 1)."
Using the KO-OU expression data will enable the reader to guess the end expression midway through a sentence.
This is because as the KO elements appear it is possible to predict the appearance of the OU elements (Figure 2).
It can be used as a basic data for understanding sentences.
"In addition, this technology can be used to guess the point in the minutes of a meeting at which the speakers change."
They carried out experiments on extracting one-to-many relation of phenomena from corpus using complementary similarity measure (CSM) which can cope very well with inclusion relation of appearance patterns.
The KO-OU relation in this research can be regarded as a type of one-to-many relation.
"In this paper, we dealt with what is called FUKU-JOSI 2 , KAKARI-JOSI, and some adverbs shown below."
We proceeded on the assumption that these are the KO elements in the KO-OU relation.
"For our research, we used newspaper articles from the Mainichi Shimbun, Nihon Keizai Shimbun, and Yomiuri Shimbun issued between 1991 and 2000."
"Process flow is shown in Figure 3. (1) We calculated the similarity measure using CSM for newspaper articles data that had been morphologically analyzed with ChaSen [Footnote_3] . ([Footnote_2]) We extracted pairs containing the target words from the results of similarity measure calculation. (3) Out of the pairs in (2), we extracted words that appeared in the order of KO and OU elements. (We judge the pairs based on this word order.) (4) We carried out judgment based on reliability."
3 Morphological Analyzer ChaSen.[URL_CITE]
2 A Japanese particle.
"As a result of this process, we obtained 14 pairs of data which had &quot;kesshite&quot;[REF_CITE]which had &quot;sae,&quot; and 23 which had &quot;wa.&quot; Data of approximately 20 pairs was obtained per target word."
"If the KO-OU expression data is used for gradual understanding of sentences, it was necessary for the data to be given meaning/information."
"When the KO element appears, it will be possible to suf-ficiently grasp or guess the contents of a sentence by referring the KO-OU expression data (Figure2)."
"However it is difficult to give mean-ing/information using the data obtained from the process in Chapter 4 because the data is broken down into each morpheme by the morphological analysis, and each element is too short."
"In Japanese sentences, there are many cases in which continuation of a particle and an auxiliary verb builds a predicate."
This continuation plays an important role in determining the event of the sen-tence.
Particles and auxiliary verbs are functional words.
"Therefore, it is not possible to determine the meaning of some of the particles and auxiliary verbs when they appeared independently."
"Fur-thermore, there are some cases in which they change their meaning when paired with another word."
"Table 2 shows the OU elements obtained pursu-ant to the procedure in Chapter 4 for KO element &quot;kitto&quot;. &quot;Da&quot; listed in Table 2 has an assertive meaning when used in a sentence like &quot;kyou wa ame da . ( It is raining today.) &quot; On the other hand, it has an inferential meaning in the context of &quot;asu wa hareru daro-u . (It should be fine tomorrow.) &quot; In addition, although &quot;nai&quot; is a negative auxiliary verb, when it is paired as in &quot;ka-mo-shire-nai (may be) &quot; and &quot;chigai-nai (must be) ,&quot; the negative mean-ing disappears."
"And the overall pairing stands for guess and conviction. 



 (a) It can be an OU element by itself, (b) It can become an OU element when paired with others, (c) It does not have the possibility of becoming an OU element."
"Words of (c) were not found in the OU ele-ments obtained for KO element &quot;kitto.&quot; In the fol-lowing, we will describe the details on (a) and (b). (a) OU element by itself Out of the OU elements for KO element &quot;kitto&quot; in Table 2, &quot;hazu&quot; can be an OU element by itself. [1] koudaina umibe de miru sakuhin wa kitto miryokuteki ni utsuru hazu da . (Works that you see at the open seaside should look attractive.)"
This is the only sentence with an independent OU element for &quot;kitto&quot; in the data obtained from the process in Chapter 4.
"The same can be said of data for KO elements other than &quot;kitto.&quot; Because of morphological analysis, the row of letters has been shortened."
"As a result, there are few ele-ments that can be regarded as an OU element by itself."
And just looking at this element does not determine the meaning. (b) OU element when paired with others
"When &quot;chigai&quot; is paired with &quot;ni&quot; and &quot;nai&quot; to make &quot;ni-chigai-nai (must be) ,&quot; it becomes an OU element."
"Similarly, pairing &quot;da&quot; with &quot;u&quot; results in an OU element &quot;daro-u (perhaps) .&quot; &quot;Da&quot; is the original form of &quot;daro&quot; and becomes &quot;daro-u&quot; when paired with &quot;u.&quot; [2] kitto kintyou suru daro-u . (It is certain that one will be nervous.) [3] kamisimereba , kitto sake no aji ga suru ni-chiga-inai . (If you chew it, you will certainly taste salmon.)"
"If we look over the entire pairing shown above, we can give meaning to such guess and conviction."
"As we described in Chapter 5, it is necessary to pair multiple elements before giving mean-ing/information."
We currently persuade the issue of automatic generation of pairing multiple ele-ments.
"Now, we are carrying out experiments on calculating the similarity measure of pairing of elements."
These will give us pairing of automati-cally generated elements and the similarity meas-ure of the pairings.
This should be useful data for resolving ambiguity (Figure 1).
This paper presented the process of extracting KO-OU expression data using CSM and the use-fulness of the extracted KO-OU expression data.
We are planning to report on the findings of ex-periments on automatic generation of OU ele-ments pairings.
"Acknowledgments To compile this paper, we used newspaper articles from The Mainichi Newspapers, The Yomiuri Shimbun, and Nihon Keizai Shimbun."
We would like to sincerely thank Dr. M. Utiyama of the Communications Research Laboratory for allow-ing us to use a KWIC tool &quot;tea [Footnote_4] .&quot;
"We have developed willex, a tool that helps grammar developers to work effi-ciently by using annotated corpora and recording parsing errors."
Willex has two major new functions.
"First, it decreases ambiguity of the parsing results by com-paring them to an annotated corpus and removing wrong partial results both au-tomatically and manually."
"Second, willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically."
We applied willex to a large-scale HPSG-style grammar as an example.
"There is an increasing need for syntactical parsers for practical usages, such as information extrac-tion."
"For example,[REF_CITE]extracted argument structures from biomedical papers using a parser based on XHPSG[REF_CITE], which is a large-scale HPSG."
"Although large-scale and general-purpose grammars have been devel-oped, they have a problem of limited coverage."
The limits are derived from deficiencies of gram-mars themselves.
"For example, XHPSG cannot treat coordinations of verbs (ex. “Molybdate slowed but did not prevent the conversion.”) nor reduced rel-atives (ex. “Rb mutants derived from patients with retinoblastoma.”)."
Finding these grammar defects and modifying them require tremendous human ef-fort.
"Hence, we have developed willex that helps to im-prove the general-purpose grammars."
Willex has two major functions.
"First, it reduces a human workload to improve the general-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format."
"Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by priori-tizing them."
"There are already other grammar developing tools, such as a grammar writer of XTAG[REF_CITE], ALEP[REF_CITE], ConTroll[REF_CITE], a tool by Nara Institute of Sci-ence and Technology[REF_CITE], and [incr tsdb()][REF_CITE]."
"But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally."
"To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process."
The workflow of the conventional grammar devel-oping tools and willex are different in the following ways.
"With the conventional tools, human debug-gers must check each sentence to find out grammar defects and modify them one by one."
"On the other hand, with willex human debuggers check sentences that are tagged with syntactical structure, one by one, find grammar defects, and record them, while willex collects the whole grammar defect records."
Then human debuggers modify the found grammar defects.
"This process allows human debuggers to make priority over defects that appear more fre-quently in the corpora, or defects that are more crit-ical for purposes of syntactical parsing."
"Indeed, it is possible for human debuggers using the conven-tional tools to collect and modify the defects but willex saves the trouble of human debuggers to col-lect defects to modify them more efficiently."
"To create the new debugging tool, we have extended will[REF_CITE]."
Will is a browser of parsing results of grammars based on feature structures.
Will and willex are implemented in JAVA.
"Willex uses sentence boundaries, word chunking, and POSs/labels encoded in XML tagged corpora."
"First, with the information of sentence boundaries and word chunking, ambiguity of sentences is re-duced, and ambiguity at parsing phase is also re-duced."
A parser connected to willex is assumed to produce only results consistent with the information.
An example is shown in Figure 1 (&lt;su&gt; is a senten-tial tag and &lt;np&gt; is a tag for noun phrases).
"Next, willex compares POSs/labels encoded in XML tags and parsing results, and deletes improper parsing trees."
"Therefore, it reduces numbers of par-tial parsing trees, which appear in the way of parsing and should be checked by human debuggers."
"In ad-dition, human debuggers can delete partial parsing trees manually later."
Figure 2 shows a concrete ex-ample. (NP and S are labels for noun and sentential
Willex has a function to output information of gram-mar defects into a file in order to collect the de-fects data and treat them statistically.
"In addition, we can save a log of debugging experiences which show what grammar defects are found."
An example of an output file is shown in Table 1.
"It includes sentence numbers, word ranges in which parsing failed, and comments input by a hu-man debugger."
"For example, the first row of the ta-ble means that the sentence #0 has coordinations of verb phrases at position #3–#12, which cannot be parsed. “OK” in the second row means the sen-tence is parsed correctly (i.e., no grammar defects are found in the sentence)."
The third row means that the word #4 of the sentence #2 has no proper lexical entry.
"The word ranges are specified by human debug-gers using a GUI, which shows parsing results in CKY tables and parse trees."
The comments are input by human debuggers in a natural language or chosen from the list of previous comments.
A postprocess-ing module of willex sorts the error data by the com-ments to help statistical analysis.
"We have applied willex to rental-XTAG, an HPSG-style grammar converted from the XTAG English grammar (The XTAG[REF_CITE]) by a grammar conversi[REF_CITE]. [Footnote_1]"
"1 Since XTAG and rental-XTAG generate equivalent parse results for the same input, debugging rental-XTAG means de-bugging XTAG itself."
The corpus used is MEDLINE abstracts with tags based on a slightly modified version of GDA-DTD [Footnote_2][REF_CITE].
"2 GDA has no tags which specify prepositional phrases, so we add &lt;prep&gt; and &lt;prepp&gt;."
The corpus is “partially parsed”; the attachments of prepositional phrases are annotated manually.
"The tags do not always specify the correct struc-tures based on rental-XTAG (i.e., the grammar as-sumed by tags is different from rental-XTAG), so we prepared a POS/label conversion table."
We can use tagged corpora based on various grammars different from the grammar that the parser is assuming by us-ing POS/label conversion tables.
Thus the cover-age was 35.1%.
"Willex received three major positive feedbacks from a user; first, the function of restricting partial results was helpful, as it allows human debuggers to check fewer results, second, the function to delete incorrect partial results manually was useful, because there are some cases that tags do not specify POSs/labels, and third, human debuggers could use the record-ing function to make notes to analyze them carefully later."
"However, willex also received some negative eval-uations; the process of locating the cause of pars-ing failure in a sentence was found to be a bit trou-blesome."
"Also, willex loses its accuracy if the hu-man debuggers themselves have trouble understand-ing the correct syntactical structure of a sentence. 3"
"We found from these evaluations that the func-tions of willex can be used effectively, though more automation is needed."
Figure 3 shows the decrease in partial parsing trees caused by using the tagged corpus. ([REF_CITE]sen-tences among the 208 sentences are shown.)
The graph shows that human workload was reduced by using the tagged corpus.
"From this table, it is inferred that (1) lack of lexi-cal entries, (2) inability to parse reduced relative and ([Footnote_3]) inability to parse coordinations of verbs are seri-ous problems of rental-XTAG."
"3 Thus, we divided the process of identifying grammar de-fects to two steps. First, a non-expert roughly classifies pars-ing errors and records temporary memorandums. Then, the non-expert shows typical examples of sentences in each class to experts and identifies grammar defects based on experts’ in-ference. Here, we can make use of the recording function of"
Conflicts between rental-XTAG and the grammar on which the modified GDA based cause parsing fail-ures.
"Statistics of the conflicts is shown in Table 3. rental-XTAG modified GDA rental-XTAG # adjectival phrase verbal phrase 36 bracketing except “,” 10 bracketing of “,” 8 treatment of omitted words 2 misc. 5"
These conflicts cannot be resolved by a simple POS/label conversion table.
One resolution is insert-ing a preprocess module that deletes and moves tags which cause conflicts.
We do not consider these conflicts as grammar de-fects but the difference of grammars to be absorbed in the conversion phase.
"We developed a debug tool, willex, which uses XML tagged corpora and outputs information of grammar defects."
"By using tagged corpora, willex succeeded to reduce human workload."
"And by recording gram-mar defects, it provides debugging environment with a bigger perspective."
But there remains a prob-lem that a simple POS/label conversion table is not enough to resolve conflicts of a debugged grammar and a grammar assumed by tags.
The tool should support to handle the complicated conflicts.
"In the future, we will try to modify willex to infer causes of parsing errors (semi-)automatically."
"It is difficult to find a point of parsing failure automati-cally, because subsentences that have no correspon-dent partial results are not always the failed point."
"Hence, we will expand willex to find the longest subsentences that are parsed successfully."
"Words, POS/labels and features of the subsentences can be clues to infer the causes of parsing errors."
Speech interfaces to question-answering systems offer significant potential for find-ing information with phones and mo-bile networked devices.
"We describe a demonstration of spoken question answer-ing using a commercial dictation engine whose language models we have cus-tomized to questions, a Web-based text-prediction interface allowing quick cor-rection of errors, and an open-domain question-answering system, AnswerBus, which is freely available on the Web."
We describe a small evaluation of the effect of recognition errors on the precision of the answers returned and make some con-crete recommendations for modifying a question-answering system for improving robustness to spoken input.
This paper demonstrates a multimodal interface for asking questions and retrieving a set of likely an-swers.
Such an interface is particularly appropri-ate for mobile networked devices with screens that are too small to display general Web pages and documents.
"Palm and Pocket PC devices, whose screens commonly display 10–15 lines, are candi-dates."
But until recently no method has existed for inputting ques-tions in a reasonable amount of time.
The study[REF_CITE]concludes that questions tend to have a limited lexical structure that can be ex-ploited for accurate speech recognition or text pre-diction.
In this demonstration we test whether this result can endow a real spoken question answering system with acceptable precision.
"Kupiec and others (1994) at Xerox labs built one of the earliest spoken information retrieval systems, with a speaker-dependent isolated-word speech rec-ognizer and an electronic encyclopedia."
One rea-son they reported for the success of their system was their use of simple language models to exploit the observation that pairs of words co-occurring in a document source are likely to be spoken together as keywords in a query.
Later research at CMU built upon similar intuition by deriving the language-model of their Sphinx-II speech recognizer from the searched document source.
Colineau and others (1999) developed a system as a part of the THISL project for retrieval from broadcast news to respond to news-related queries such as What do you have on . . . ? and I am doing a report on . . . — can you help me?
"The queries the authors addressed had a sim-ple structure, and they successfully modelled them in two parts: a question-frame, for which they hand-wrote grammar rules; and a content-bearing string of keywords, for which they fitted standard lexical language-models from the news collection."
Extensive research[REF_CITE]has concluded that spoken documents can be effectively indexed and searched with word-error rates as high as 30–40%.
One might expect a much higher sensitivity to recognition errors with a short query or natural-language question.
"Two studies (et al., 1997;[REF_CITE]) have measured the detri-mental effect of speech recognition errors on the pre-cision of document retrieval and found that this task can be somewhat robust to 25% word-error rates for queries of 2–8 words."
Two recent systems are worthy of special men-tion.
"First, Google Labs deployed a speaker-in-dependent system in late 2001 as a demo of a telephone-interface to its popular search engine. (It is still live as[REF_CITE].)"
"Second, Chang and others (2002a; 2002b) have implemented systems for the Pocket PC that interpret queries spoken in English or Chinese."
This last group appears to be at the forefront of current research in spoken interfaces for document retrieval.
"None of the above are question-answering sys-tems; they boil utterances down to strings of key-words, discarding any other information, and return only lists of matching documents."
To our knowledge automatic answering of spoken natural-language questions has not previously been attempted.
"Our demonstration system has three components: a commercial speaker-dependent dictation system, a predictive interface for typing or correcting natural-language questions, and a Web-based open-domain question-answering engine."
We describe these in turn.
"The dictation system is Dragon NaturallySpeaking 6.1, whose language models we have customized to a large corpus of questions."
We performed tests with a head-mounted microphone in a relatively quiet acoustic environment. (The Dragon Audio Setup Wizard identified the signal-to-noise ratio as 22 dBs.)
"We tested a male native speaker of En-glish and a female non-native speaker, requesting each first to train the acoustic models with 5–10 min-utes of software-prompted dictation."
"We also trained the language models by present-ing the Vocabulary Wizard the corpus of 280,000 questions described[REF_CITE], of which Table 1 contains a random sample."
The primary function of this training feature in NaturallySpeak-ing is to add new words to the lexicon; the nature of the other adaptations is not clearly documented.
"New 2-grams and 3-grams also appear to be iden-tified, which one would expect to reduce the word-error rate by increasing the ‘hit rate’ over the 30– 50% of 3-grams in a new text for which a language model typically has explicit frequency estimates."
We have designed a predictive typing interface whose purpose is to save keystrokes and time in edit-ing misrecognitions.
"Such an interface is particu-larly applicable in a mobile context, in which text entry is slow and circumstances may prohibit speech altogether."
We fitted a 3-gram language model to the same corpus as above using the CMU–Cambridge SLM Toolkit[REF_CITE].
The inter-face in our demo is a thin JavaScript client accessible from a Web browser that intercepts each keystroke and performs a CGI request for an updated list of predictions.
The predictions themselves appear as hyperlinks that modify the question when clicked.
Figure 1 shows a screen-shot.
The AnswerBus system[REF_CITE]has been run-ning on the Web since[REF_CITE].
It serves thousands of users every day.
"The original engine was not designed for a spoken interface, and we have recently made modifications in two respects."
We de-scribe these in turn.
Later we propose other modifi-cations that we believe would increase robustness to a speech interface.
We evaluated the accuracy of the system subject to spoken input using 200 test questions from the[REF_CITE]QA track[REF_CITE].
"AnswerBus returns snippets from Web pages containing pos-sible answers; we compared these with the refer- ence answers used in the TREC competition, over-riding about 5 negative judgments when we felt the answers were satisfactory but absent from the TREC scorecard."
"For each of these 200 questions we passed two strings to the AnswerBus engine, one typed verbatim, the other transcribed from the speech of one of the people described above."
The results are in Tables 2 and 3.
We currently perform no automatic checking or cor-rection of spelling and no morphological stemming of words in the questions.
Table 3 indicates that these features would improve robustness to errors in speech recognition.
"We now make some specific points regarding homographs, which are typically troublesome for speech recognizers."
QA systems could relatively easily compensate for confusion in two common classes of homograph: • plural nouns ending –s versus possessive nouns ending –’s or –s’.
"Our system answered Q39 Where is Devil’s tower?, but not the transcribed question Where is Devils tower? • written numbers versus numerals."
Our system could not answer What is slang for a 5 dol-lar bill? although it could answer Q92 What is slang for a five dollar bill?.
More extensive ‘query expansion’ using syn-onyms or other orthographic forms would be trickier to implement but could also improve recall.
"For ex-ample,[REF_CITE]What city in Australia has rain forests? it answered correctly, but the transcription What city in Australia has rainforests (without a space), got no answers."
"Another example: Q35 Who won the No-bel Peace[REF_CITE]? got no answers, whereas Who was the winner . . . ? would have found the right answer."
This paper has described a multimodal interface to a question-answering system designed for rapid input of questions and correction of speech recognition errors.
"The interface for this demo is Web-based, but should scale to mobile devices."
"We described a small evaluation of the system’s accuracy given raw (uncorrected) transcribed questions from two speak-ers, which indicates that speech can be used for au-tomatic question-answering, but that an interface for correcting misrecognitions is probably necessary for acceptable accuracy."
In the future we will continue tightening the inte-gration of the components of the system and port the interface to phones and Palm or Pocket PC devices.
This paper proposes a principled approach for analysis of semantic relations between constituents in compound nouns based on lexical semantic structure.
One of the difficulties of compound noun analysis is that the mechanisms governing the deci-sion system of semantic relations and the representation method of semantic rela-tions associated with lexical and contex-tual meaning are not obvious.
The aim of our research is to clarify how lexical se-mantics contribute to the relations in com-pound nouns since such nouns are very productive and are supposed to be gov-erned by systematic mechanisms.
The results of applying our approach to the analysis of noun-deverbal compounds in Japanese and English show that lexical conceptual structure contributes to the re-strictional rules in compounds.
The difficulty of compound noun analysis is that the effective way of describing the semantic relations in compounds has not been identified.
The descrip-tion should not remain just a kind of categorization.
"Rather, it should take into account the construction of the analysis model."
The previous work proposed semantic approaches based on semantic categories[REF_CITE]had proposed detailed analy-sis of relations between constituents in compound nouns.
Some of approaches[REF_CITE]take the framework of Gen- erative Lexicon (GL)[REF_CITE].
Se-mantic approaches are especially well designed but they should still clarify the complete lexical factors needed for analysis model.
Probabilistic approaches[REF_CITE]have been proposed to disambiguate semantic relations between constituents in compounds.
"Their experimental results show a high performance, but only for shallow analysis of compounds using se-mantically tagged corpora."
"To be fully effective, they also need to incorporate factors that are effec-tive in disambiguating semantic relations."
It is there-fore necessary to clarify what kinds of factors are re-lated to the mechanisms that govern the relations in compounds.
"Against this background, we have carried out a re-search which aims at clarifying how lexical seman-tics contribute to, independently of languages, the relations in compound nouns."
"This paper proposes a principled approach for the analysis of semantic relations between constituents in compound nouns based on the theoretical framework of lexical con-ceptual structure (LCS), and shows that the frame-work originally developed on the basis of Japanese compound noun data works well for both Japanese and English compound nouns."
The relation between constituents in deverbal com-pounds [Footnote_1] can first be divided into two: (i) the modi-fier becomes an internal argument[REF_CITE]and (ii) the modifier functions as an adjunct.
"1 In the case of English the equivalent is nominalizations, but for simplicity we use deverbal compounds."
We as- sume these two kinds of relations are the target of our analysis model because argument/adjunct rela-tions are basic but extensible to more detailed se-mantic relations by assuming more complex seman-tic system.
"Besides these relations related to argu-ment structure of verbs are the boundary between syntax and semantics, then our approach must be ex-tendable to be incorporated into sytactic analysis."
We assume that the discrimination between argu-ment and adjunct relations can be done by the com-bination of the LCS (we call TLCS) on the side of deverbal heads and the consistent categorization of modifier nouns on the basis of their behavior vis-à-vis a few canonical TLCS types of deverbal heads.
Figure 1 shows examples of disambiguating re-lations using TLCS for the deverbal heads ‘sousa’ (operate) and ‘hon’yaku’ (translate).
"In TLCSes, the words written in capital letters are semantics predi-cates, ‘x’ denotes the external argument, and ‘y’ and ‘z’ denote the internal arguments (see Section 3)."
"The approach we propose consists of three ele-ments: categorization of deverbals and nominaliza-tions, categorization of modifier noun and restriction rules for identifying relations."
The framework of LCS[REF_CITE]has shown that semantic decom-position based on the LCS framework can system-atically explain the word formation as well as the syntax structure.
However existing LCS frameworks cannot be applied to the analysis of compounds straightforwardly because they do not give extensive semantic predicates for LCS.
"Therefore we construct an original LCS, called TLCS, based on the LCS framework with a clear set of LCS types and basic predicates."
We use the acronym “TLCS” to avoid the confusion with other LCS-based schemes.
"Table 1 shows the current complete set of TLC-Ses types we elaborated. [Footnote_2] The following list is for Japanese deverbals, but the same LCS types are ap-plied for nominalizations in English. [Footnote_3] 1 [x ACT ON y] enzan (calculate), sousa (operate) [Footnote_2] [x CONTROL[BECOME [y BE AT z]]] kioku (memorize), hon’yaku (translate) [Footnote_3] [x CONTROL[BECOME [y NOT BE AT z]]] shahei (shield), yokushi (deter) 4 [x CONTROL [y MOVE TO z]] densou (transmit), dempan (propagate) 5 [x=y CONTROL[BECOME [y BE AT z]]] kaifuku (recover), shuuryou (close) 6 [BECOME[y BE AT z]] houwa (become saturated) bumpu (be distributed) 7 [y MOVE TO z] idou (move), sen’i (transmit) 8 [x CONTROL[y BE AT z]] iji (maintain), hogo (protect) 9 [x CONTROL[BECOME[x BE WITH y]]] ninshiki (recognize), yosoku (predict) 10 [y BE AT z] sonzai (exist), ichi (locate) 11 [x ACT] kaigi (hold a meeting), gyouretsu (queue) 12 [x CONTROL[BECOME [ [FILLED]y BE AT z]]] shomei (sign-name)"
"2[REF_CITE]types are set by the combination of argu-ment structure and aspect analysis that is telic or atelic. After applying all the combination, we arrange the TLCS patterns by deleting patterns that does not appear and subcategorizing cer-tain patterns."
"3 At the moment, there are about 500 deverbals[REF_CITE]nominalizations in English."
"2[REF_CITE]types are set by the combination of argu-ment structure and aspect analysis that is telic or atelic. After applying all the combination, we arrange the TLCS patterns by deleting patterns that does not appear and subcategorizing cer-tain patterns."
"3 At the moment, there are about 500 deverbals[REF_CITE]nominalizations in English."
The number attached to each TLCS type in Table 1 will be used throughout the paper refer to specific TLCS types.
"In Table 1, the capital letters (such as ‘ACT’ and ‘BE’) are semantic predicates, which are 11 types. ‘x’ denotes an external argument and ‘y’ and ‘z’ denote an internal argument (see[REF_CITE]). [Footnote_4]"
"4 In this paper, we limit the types of arguments are three, i.e. x (Agent), y (Theme) and z (Goal)."
"In Japanese compounds, some of modifiers can not take an accusative case."
This is an adjectival stem and it does not appear with inflections.
"Therefore, the modifier is always the adjunct in the compounds."
So we introduce the distinction of ‘-ACC’ (unac-cusative) and ‘+ACC’ (accusative).
"ACC ‘kimitsu’ (secrecy) and ‘kioku’ (memory) are ‘+ACC’, and ‘sougo’ (mutual-ity) and ‘kinou’ (inductiv-e/ity) are ‘-ACC’."
"In English, they correspond to adjective modifier such as ‘-ent’ of ‘recurrent’ or ‘-al’ of ‘serial’."
"If, as argued by some theoretical linguists, the LCS representation can contribute to explaining these phenomena related to the arguments and aspect structure consistently, and if the combination of LCS and noun categorization can explain properly these phenomena related to argumet/adjunct, then there should be a level of consistent noun categorization which matches the LCS on the side of deverbals."
We used the predicates of some TLCS types to explore the noun categorizations.
"In the preliminary examination, we have found that some TLCS types can be formed into the groups that correspond to modifier categories in Table 2."
Below are examples of modifier nouns catego-rized as negative or positive in terms of each of these TLCS groups.
"ON ‘koshou’ (fault) and ‘seinou’ (performance) are ‘+ON’, and ‘heikou’ (parallel) and ‘rensa’ (chain) are ‘-ON’. (‘ON’ stands for the predi-cate in ‘ACT ON’.)"
"EC ‘imi’ (semantic) and ‘kairo’ (circuit) are ‘+EC’, and ‘kikai’ (machine) and ‘densou’ (transmis-sion) are ‘-EC’. (‘EC’ stands for an External argument Controls an internal argument’.)"
"AL ‘fuka’ (load) and ‘jisoku’ (flux) are ‘+AL’, and ‘kakusan’ (diffusion) and ‘senkei’ (linearly) are ‘-AL’. (‘AL’ stands for alternation verbs.)"
"UA ‘jiki’ (magnetic) and ‘joutai’ (state) are ‘+UA’, and ‘junjo’ (order) and ‘heikou’ (parallel) are ‘-UA’. (‘UA’ stands for UnAccusative verbs.)"
The noun categories introduced in Section 4 can be used for disambiguating the intra-term relations in deverbal compounds with various deverbal heads that take different TLCS types.
The range of ap-plication of the noun categorizations with respect to TLCS groups is summarized in Table 2.
The num-ber in the TLCS column corresponds to the number given in Table 1.
"Step 1 If the modifier has the category ‘-ACC’, then declare the relation as adjunct and terminate."
"If not, go to next."
"Step 2 If the TLCS of the deverbal head is 10, 11, or 12 in Table 1, then declare the relation as adjunct and terminate."
"If not, go to next."
The analyzer determines the relation from the interaction of lexical meanings between a deverbal head and a modifier noun.
"In the case of ‘-ON’, ‘-EC’,‘-AL’ or ‘-UA’, declare the re-lation as adjunct and terminate."
"If not, go to next."
Step 4 Declare the relation as internal argument and terminate.
"With these rules and categories of nouns, we can analyze the relations between words in com-pounds with deverbal heads."
"For example, when the modifier ‘kikai’ (machine) is categorized as ‘-EC’ but ‘+ON’, the modifier in kikai-hon’yaku (machine-translation) is analyzed as adjunct (that means ‘translation by a machine’), and the modi-fier in kikai-sousa (machine-operation) is analyzed as internal argument (that means ‘operation of a ma-chine’), both correctly."
"We applied the method to 1223 two-constituent compound nouns with deverbal heads in Japanese. 809 of them are taken from a dictionary of techni-cal terms[REF_CITE], and 414 from news articles in a newspaper."
We also applied the method to 200 compound nouns of technical terms[REF_CITE]in English.
They are extracted randomly.
"According to the manual evaluation of the exper-iment, 99.3% (1215/ 1223) of the results were cor-rect in Japanese, and 97% (194/200) in English."
The performance is very high.
Table 2 shows the details of how the rules are applied to disambiguating the relations between constituents in the deverbal com-pounds.
These results indicate that our set of LCS and categorization of modifiers has the enough to disambiguate the relationships we assumed. ture we call it TLCS.
"The results of experiment for Japanese compounds and English compounds show our approach is highly promising, also the contribu-tion of the lexical factor to disambiguation rule."
"Roughly speaking, our LCS-based approach can be available both Japanese and English deverbal nouns."
"Comparing with the results between Japanese com-pounds and English compounds, the factor ‘-ACC’ looks effective to disambiguate relations."
The rea-son is that the most of modifiers indicate adjec-tive function by adding suffixes in English.
"While in Japanese, adjectival nouns of modifiers have no inflecitons, then the semantic-based approach is needed for Japanese compound noun analysis."
We found that a small number of modifier nouns deviate from our assumptions.
The most typical case is that our analysis model fails in a word with mul-tiple semantics.
"For example, ‘right justify’ is mis-understood as internal argument relation because of ambiguity of the word ‘right’ which has both mean-ings of an adjective and a noun."
"We consider dealing with them as each different words like ‘right adj’, ‘right noun’ in future work."
This paper proposes a principled approach for anal-ysis of semantic relations between constituents in compound nouns based on lexical conceptual struc-
An empirical comparison of CFG filtering techniques for LTAG and HPSG is pre-sented.
We demonstrate that an approx-imation of HPSG produces a more effec-tive CFG filter than that of LTAG.
We also investigate the reason for that difference.
"Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG)[REF_CITE], and Head-Driven Phrase Structure Gram-mar (HPSG)[REF_CITE]."
"Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms ([REF_CITE]; van[REF_CITE])."
"However, these realiza-tions sometimes exhibit quite different performance in each grammar formalism[REF_CITE]."
"If we could identify an al-gorithmic difference that causes performance differ-ence, it would reveal advantages and disadvantages of the different realizations."
"This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the fur-ther advancement of the whole parsing community."
"In this paper, we compare CFG filtering tech-niques for LTAG[REF_CITE]and HPSG[REF_CITE], following an approach to parsing comparison among different grammar for-malisms[REF_CITE]."
"The key idea of the approach is to use strongly equivalent gram-mars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated[REF_CITE]."
The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar.
Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing.
Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques.
"We performed a comparison between the exist-ing CFG filtering techniques for LTAG[REF_CITE]and HPSG[REF_CITE], using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Tree-bank[REF_CITE]into HPSG-style."
We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter.
"In this section, we introduce a grammar conver-si[REF_CITE]and CFG filter-ing[REF_CITE]."
The grammar conversion consists of a conversion of LTAG elementary trees to HPSG lexical entries and an emulation of substitution and adjunction by pre-determined grammar rules.
An LTAG elemen-tary tree is first converted into canonical elementary trees which have only one anchor and whose sub-trees of depth n (≥1) contain at least one anchor.
A canonical elementary tree is then converted into an HPSG lexical entry by regarding the leaf nodes as arguments and by storing them in a stack.
We can perform a comparison between LTAG and HPSG parsers using strongly equivalent grammars obtained by the above conversion.
This is because strongly equivalent grammars can be a substitute for the same grammar in different grammar formalisms.
An initial offline step of CFG filtering is performed to approximate a given grammar with a CFG.
The obtained CFG is used as an efficient device to com-pute the necessary conditions for parse trees.
The CFG filtering generally consists of two steps.
"In phase 1, the parser first predicts possible parse trees using the approximated CFG, and then filters out irrelevant edges by a top-down traversal starting from roots of successful context-free derivations."
"In phase 2, it then eliminates invalid parse trees by us-ing constraints in the given grammar."
We call the remaining edges that are used for the phase 2 pars-ing essential edges.
"The parsers with CFG filtering used in our ex-periments follow the above parsing strategy, but are different in the way the CF approximation and the elimination of impossible parse trees in phase 2 are performed."
"In the following sections, we briefly de-scribe the CF approximation and the elimination of impossible parse trees in each realization."
"In CFG filtering techniques for LTAG[REF_CITE], every branching of elementary trees in a given grammar is extracted as a CFG rule as shown in Figure 1."
"Because the obtained CFG can reflect only local constraints given in each local structure of the el-ementary trees, it generates invalid parse trees that connect local trees in different elementary trees."
"In order to eliminate such parse trees, a link between branchings is preserved as a node number which records a unique node address (a subscript attached to each node in Figure 1)."
We can eliminate these parse trees by traversing essential edges in a bottom-up manner and recursively propagating ok-flag from a node number x to a node number y when a connec-tion between x and y is allowed in the LTAG gram-mar.
We call this propagation ok-prop.
"In CFG filtering techniques for HPSG[REF_CITE], the extrac-tion process of a CFG from a given HPSG gram-mar starts by recursively instantiating daughters of a grammar rule with lexical entries and generated fea-ture structures until new feature structures are not generated as shown in Figure 2."
"We must impose restrictions on values of some features (i.e., ignor-ing them) and/or the number of rule applications in order to guarantee the termination of the rule appli-cation."
A CFG is obtained by regarding each initial and generated feature structures as nonterminals and transition relations between them as CFG rules.
"Although the obtained CFG can reflect local and global constraints given in the whole structure of lexical entries, it generates invalid parse trees be-cause they do not reflect upon constraints given by the values of features that are ignored in phase 1."
These parse trees are eliminated in phase 2 by apply-ing a grammar rule that corresponds to the applied CFG rule.
We call this rule application rule-app.
"In this section, we compare a pair of CFG filter-ing techniques for LTAG[REF_CITE]and HPSG[REF_CITE]described in Sec-tion 2.2.1 and 2.2.2."
"We hereafter refer to PB and TNT for the C++ implementations of the former and a valiant 1 of the latter, respectively. [Footnote_2]"
"2 In phase 1, PB performs Earley[REF_CITE]parsing while TNT performs CKY[REF_CITE]parsing."
We first acquired LTAGs by a method pro-posed[REF_CITE]from Sections 2-21 of the Wall Street Journal (WSJ) in the Penn Tree-bank[REF_CITE]and its subsets. [Footnote_3] We then converted them into strongly equivalent HPSG-style grammars using the grammar conversion described in Section 2.1.
3 The elementary trees in the LTAGs are binarized.
Table 1 shows the size of CFG ap-proximated from the strongly equivalent grammars.
"G x , CFG PB , and CFG TNT henceforth refer to the LTAG extracted from Section x of WSJ and CFGs approximated from G x by PB and TNT, respectively."
The size of CFG TNT is much larger than that of CFG PB .
"By investigating parsing performance using these CFGs, we show that the larger size of CFG TNT resulted in better parsing performance."
Table 2 shows the parse time with 254 sentences of length n (≤10) from Section 2 of WSJ (the av-erage length is 6.72 words). [Footnote_4] This result shows not only that TNT achieved a drastic speed-up against
4 We used a subset of the training corpus to avoid the com-plication of using default lexical entries for unknown words.
"PB, but also that performance difference between them increases with the larger size of the grammars."
"In order to estimate the degree of CF approxima-tion, we measured the number of essential (inactive) edges of phase [Footnote_1]."
1 All daughters of rules are instantiated in the approximation.
Table 3 shows the number of the essential edges.
The number of essential edges pro-duced by PB is much larger than that produced by TNT.
We then investigated the effect on phase 2 as caused by the different number of the essential edges.
Table 4 shows the success rate of ok-prop and rule-app.
"The success rate of rule-app is 100%, [Footnote_5] whereas that of ok-prop is quite low. [Footnote_6] These results indicate that CFG TNT is superior to CFG PB with re-spect to the degree of the CF approximation."
5 This means that the extracted LTAGs should be compatible with CFG and were completely converted to CFGs by TNT.
"6 Similar results were obtained in preliminary experiments using the XTAG English grammar (The XTAG[REF_CITE]) without features (parse time (sec.)/success rate (%)[REF_CITE].3/30.6 and 0.606/71.2 with the same sen-tences), though space limitations preclude complete results."
We can explain the reason for this difference by investigating how TNT approximates HPSG-style grammars converted from LTAGs.
"As described in Section 2.1, the grammar conversion preserves the whole structure of each elementary tree (pre-cisely, a canonical elementary tree) in a stack, and grammar rules manipulate a head element of the stack."
A generated feature structure in the approxi-mation process thus corresponds to the whole unpro-cessed portion of a canonical elementary tree.
This implies that successful context-free derivations ob-tained by CFG TNT basically involve elementary trees in which all substitution and adjunction have suc-ceeded.
"However, CFG PB (also a CFG produced by the other work[REF_CITE]) cannot avoid generating invalid parse trees that connect two lo- cal structures where adjunction takes place between them."
We measured with G 2 - 21 the proportion of the number of ok-prop between two node numbers of nodes that take adjunction and its success rate.
These results sug-gest that the global contexts in a given grammar is essential to obtain an effective CFG filter.
It should be noted that the above investigation also tells us another way of CF approximation of LTAG.
We first define a unique way of tree traversal such as head-corner traversal (van[REF_CITE]) on which we can perform a sequential application of substitu-tion and adjunction.
We then recursively apply sub-stitution and adjunction on that traversal to an ele-mentary tree and a generated tree structure.
"Because the processed portions of generated tree structures are no longer used later, we regard the unprocessed portions of the tree structures as nonterminals of CFG."
We can thereby construct another CFG filter-ing for LTAG by combining this CFG filter with an existing LTAG parsing algorithm (van[REF_CITE]).
We presented an empirical comparison of LTAG and HPSG parsers with CFG filtering.
We compared the parsers with strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Tree-bank into HPSG-style.
Experimental results showed that the existing CF approximation of HPSG[REF_CITE]produced a more effective filter than that of LTAG[REF_CITE].
"By in-vestigating the different ways of CF approximation, we concluded that the global constraints in a given grammar is essential to obtain an effective filter."
We are going to integrate the advantage of the CF approximation of HPSG into that of LTAG in order to establish another CFG filtering for LTAG.
We will also conduct experiments on trade-offs between the degree of CF approximation and the size of approx-imated CFGs as[REF_CITE].
This paper proposes an automatic method of detecting grammar elements that de-crease readability in a Japanese sentence.
"The method consists of two components: (1) the check list of the grammar elements that should be detected; and (2) the de-tector, which is a search program of the grammar elements from a sentence."
"By defining a readability level for every gram-mar element, we can find which part of the sentence is difficult to read."
We always prefer readable texts to unreadable texts.
"The texts that transmit crucial information, such as instructions of strong medicines, must be completely readable."
"When texts are unreadable, we should rewrite them to improve readability."
"In English, measuring readability as reading age is well studied[REF_CITE]."
The reading age is the chronological age of a reader who could just understand the text.
The value is usually calculated from the sentence length and the number of sylla-bles.
"From this value, we find whether a text is read-able or not for readers of a specific age; however, we do not find which part we should rewrite to improve readability when the text is unreadable."
The goal of our study is to present tools that help rewriting work of improving readability in Japanese.
"The first tool is to help detect the sentence frag-ments (words and phrases) that should be rewrit-ten; in other words, it is a checker of “hard-to-read” words and phrases in a sentence."
Such a checker can be realized with two components: the check list and its detector.
The check list provides check items and their readability levels.
The detector is a program that searches the check items in a sentence.
"From the detected items and their readability levels, we can identify which part of the sentence is difficult to read."
"We are currently working on three aspects con-cerned with readability of Japanese: kanji charac-ters, vocabulary, and grammar."
"In this paper, we re-ports the readability checker for the grammar aspect."
"The first component of the readability checker is the check list; in this list, we should define every Japanese grammar element and its readability level."
"A grammar element is a grammatical phenomenon concerned with readability, and its readability level indicates the familiarity of the grammar element."
"In Japanese, grammar elements are classified into four categories. 1."
Conjugation: the form of a verb or an adjective changes appropriately to the proceed word. 2.
Functional word: postpositional particles work as case makers; auxiliary verbs represent tense and modality. 3.
"Sentential pattern: negation, passive form, and question are represented as special sentence patterns. 4."
"Functional phrase: there are idiomatic phrases works functionally, like “not only ... but also ...” in English."
"A grammar section exists in a part of the Japanese Language Proficiency Test, which is used to measure and certify the Japanese language ability of a person who is a non-Japanese."
"There are four levels in this test; Level 4 is the elementary level, and Level 1 is the advanced level."
Test Content Specifications (TCS) (Foundation and Association[REF_CITE]) is intended to serve as a reference guide in question compilation of the Japanese Language Proficiency Test.
"This book describes the list of grammar ele-ments, which can be tested at each level."
These lists fit our purpose: they can be used as the check list for the readability checker.
TCS describes grammar elements in two ways.
"In the first way, a grammar element is described as a 3-tuple: its name, its patterns, and its example sen-tences."
"The following 3-tuple is an example of the grammar element that belongs to Level 4. daimeishi Name 代名詞 (Pronoun) kore sore Patterns コレ (this), ソレ (that) kore hahon desu."
"Examples これは本です。 (This is a book.), sore ha nōto desu. それはノートです。 (That is a note.)"
"Grammar elements of Level 3 and Level 4 are con-jugations, functional words and sentential patterns that are defined in this first way."
"In the second way, a grammar element is described as a pair of its pat-terns and its examples."
The following pair is an ex-ample of the grammar element that belongs to Level 2. ta tokoro Patterns 〜たところ (when ...) sensei no otaku he ukagatta tokoro
Examples 先生のお宅へ伺ったところ (When visiting the teacher’s home)
Grammar elements of Level 1 and Level 2 are func-tional phrases that are defined in this second way.
"We decided to use this example-based definition for the check list, because the check list should be in-dependent from the implementation of the detector."
"If the check list depends on detector’s implementa-tion, the change of implementation requires change of the check list."
"Each item of the check list is defined as a 3-tuple: (1) readability level, (2) name, and (3) a list of exam-ple pairs."
There are four readability levels according to the Japanese Language Proficiency Test.
An ex-ample pair consists of an example sentence and an instance of the grammar element.
It is an implicit description of the pattern detecting the grammar el-ement.
"For example, the check item for ‘Adjective (predicative, negative, polite)’ is shown as follows,"
"Name Adjective (predicative, negative, polite)"
Test Pairs kono heya ha hiroku nai desu.
Sentence 1 この部屋は広くないです。 (This room is not large.) hiroku nai desu Instance 1 広くないです (is not large)
"The instance 広くないです /hirokunaidesu/ consists of three morphemes: (1) 広く /hiroku/, the adjective means ‘large’ in renyo form, (2) ない /nai/, the ad-jective means ‘not’ in root form, and (3) です /desu/, the auxiliary verb ends a sentence politely."
"So, this test pair represents implicitly that the grammar el-ement can be detected by a pattern “Adjective(in renyo form) + nai + desu”."
All example sentences are originated from TCS.
Some check items have several test pairs.
Table 1 shows the size of the check list.
"The check list must be converted into an explicit rule set, because each item of the check list shows no explicit description of its grammar element, only shows one or more pairs of an example sentence and an instance."
Four categories of grammar elements leads that each rule of the explicit rule set may take three different types. • Type M: A rule detecting a sequence of mor-phemes • Type B: A rule detecting a bunsetsu. • Type R: A rule detecting a modifier-modifee re-lationship.
"Type M is the basic type of them, because almost of grammar elements can be detected by morphologi-cal sequential patterns."
Conversion from a check item to a Type M rule is almost automatic.
This conversion process con-sists of three steps.
"First, an example sentence of the check item is analyzed morphologically and syn-tactically."
"Second, a sentence fragment covered by the target grammar element is extracted based on signs and fixed strings included in the name of the check item."
"Third, a part of a generated rule is re-laxed based on part-of-speech tags."
"For example, the check item of the grammar element whose name is “Adjective (predicative, negative, polite)” is con-verted to the following rule. np( 4, ’Adjective (predicative,negative,polite)’, Dm({ H1=&gt;’Adjective’, K2=&gt;’Basic Renyou Form’ }, { G=&gt;’ ない /nai/’, H1=&gt;’Postfix’, K2=&gt;’Root Form’ }, { G=&gt;’ です /desu/’, H1=&gt;’Auxiliary Verb’ }) );"
"The function np() makes the declaration of the rule, and the function Dm() describes a morphologi-cal sequential pattern which matches the target."
"This example means that this grammar element belongs to Level 4, and can be detected by the pattern which consists of three morphemes."
Type B rules are used to describe grammar ele-ments such as conjugations including no functional words.
They are not generated automatically; they are converted by hand from type M rules that are generated automatically.
"For example, the rule de-tecting the grammar element whose name is “Adjec-tive in Root Form” is defined as follows. np( 4, ’Adjective in Root Form’, Db( { H1=&gt;’Adjective’, K2=&gt;’Root Form’ } ) );"
The function Db() describes a pattern which matches a bunsetsu which consists of specified mor-phemes.
"This example means that this grammar el-ement belongs to Level 3, and shows the detection pattern of this grammar element."
Type R rules are used to describe grammar ele-ments that include modifier-modifee relationships.
"In the case of the grammar element whose name is “Verb Modified by Adjective”, it includes a structure that an adjective modifies a verb."
"It is impossible to detect this grammar element by a morphological continuous pattern, because any bunsetsus can be in-serted between the adjective and the verb."
"For such a grammar element, we introduce the function Dk() that takes two arguments: the former is a modifier and the latter is its modifee. np( 4, ’Verb Modified by Adjective’, Dk( Db({ H1=&gt;’Adjective’, K2=&gt;’Basic Renyou Form’ }), Dm({ H1=&gt;’Verb’ }) ) );"
The architecture of the detector is shown in Figure 1.
"The detector uses a morphological analyzer, Juman, and a syntactic analyzer, KNP[REF_CITE]."
The rule set is converted into the format that KNP can read and it is added to the standard rule set of KNP.
This addition enables KNP to detect can-didates of grammar elements.
The ‘Detection’ part selects final results from these candidates based on preference information given by the rule set.
"Figure 2 shows grammar elements detected by our chizu ha oroka, ryakuzu detector from the sentence “ 地図はおろか、略図 sae mo kubarare nakatta. さえも配られなかった。 ” which means “Neither a map nor a rough map was not distributed.”"
"We conducted two experiments, in order to check the performance of our detector."
"The first test is a closed test, where we examine whether grammar elements in example sentences of TCS are detected correctly."
"From the rest 38 sentences, our detector failed to detect the right grammar element."
This result shows that our program achieves the sufficient recall 95% in the closed test.
Almost of these errors are caused failure of morphological analysis.
"The second test is an open test, where we examine whether grammar elements in example sentences of the textbook, which is written for learners preparing for the Japanese Language Proficiency Test[REF_CITE], are detected correctly."
"The text-book gives 1110 example sentences, and there are 680 sentences from which their grammar elements are detected correctly."
"Wrong grammar elements are detected from 71 sentences, and no grammar el-ements are detected from the rest 359 sentences."
"So, the recall of automatic detection of grammar ele-ments is 61%, and the precision is 90%."
The ma-jor reason of these failures is strictness of several rules; several rules that are generated from example pairs automatically are overfitting to example pairs so that they cannot detect variations in the textbook.
We think that relaxation of such rules will eliminate these failures.
We will demonstrate the latest version of an ongoing project to create an intelli-gent procedure assistant for use by as-tronauts on the International Space Sta-tion (ISS).
"The system functionality in-cludes spoken dialogue control of nav-igation, coordinated display of the pro-cedure text, display of related pictures, alarms, and recording and playback of voice notes."
The demo also exempli-fies several interesting component tech-nologies.
Speech recognition and lan-guage understanding have been devel-oped using the Open Source REGULUS 2 toolkit.
This implements an approach to portable grammar-based language mod-elling in which all models are derived from a single linguistically motivated uni-fication grammar.
"Domain-specific CFG language models are produced by first specialising the grammar using an au-tomatic corpus-based method, and then compiling the resulting specialised gram-mars into CFG form."
"Translation between language centered and domain centered semantic representations is carried out by ALTERF , another Open Source toolkit, which combines rule-based and corpus-based processing in a transparent way."
Astronauts aboard the ISS spend a great deal of their time performing complex procedures.
"This often in-volves having one crew member reading the proce-dure aloud, while while the other crew member per-forms the task, an extremely expensive use of as-tronaut time."
"The Intelligent Procedure Assistant is designed to provide a cheaper alternative, whereby a voice-controlled system navigates through the pro-cedure under the control of the astronaut perform-ing the task."
"This project has several challenging features including: starting the project with no tran-scribed data for the actual target input language, and rapidly changing coverage and functionality."
We are using REGULUS 2 and ALTERF to address these challenges.
"Together, they provide an example-based framework for constructing the portion of the system from recognizer through intepretation that allows us to make rapid changes and take advan-tage of both rule-base and corpus-based information sources."
"In this way, we have been able to extract maximum utility out of the small amounts of data initial available to the project and also smoothly ad-just as more data has been accumulated in the course of the project."
"The following sections describe the procedure as-sistant application and domain, REGULUS 2 and AL - TERF ."
"The system, an early version of which was described[REF_CITE], is a prototype intelligent voice enabled personal assistant, intended to support astro- nauts on the International Space Station in carrying out complex procedures."
The first production ver-sion is tentatively scheduled for introduction some time during 2004.
"The system reads out each pro-cedure step as it reaches it, using a TTS engine, and also shows the corresponding text and supplemen-tary images in a visual display."
"Core functionality consists of the following types of commands: • Navigation: moving to the following step or substep (“next”, “next step”, “next substep”), going back to the preceding step or substep (“previous”, “previous substep”), moving to a named step or substep (“go to step three”, “go to step ten point two”). • Visiting non-current steps, either to preview fu-ture steps or recall past ones (“read step four”, “read note before step nine”)."
"When this func-tionality is invoked, the non-current step is dis-played in a separate window, which is closed on returning to the current step. • Recording, playing and deleting voice notes (“record voice note”, “play voice note on step three point one”, “delete voice note on substep two”). • Setting and cancelling alarms (“set alarm for five minutes from now”, “cancel alarm at ten twenty one”). • Showing or hiding pictures (“show the small waste water bag”, “hide the picture”). • Changing the TTS volume (“increase/decrease volume”). • Querying status (“where are we”, “list voice notes”, “list alarms”). • Undoing and correcting commands (“go back”, “no I said increase volume”, “I meant step four”)."
"The system consists of a set of modules, written in several different languages, which communicate with each other through the SRI Open Agent Ar-chitecture[REF_CITE]."
Speech recogni-tion is carried out using the Nuance Toolkit[REF_CITE].
REGULUS 2[REF_CITE]is an Open Source environment that supports effi-cient compilation of typed unification grammars into speech recognisers.
The basic intent is to provide a set of tools to support rapid prototyping of spo-ken dialogue applications in situations where little or no corpus data exists.
The environment has al-ready been used to build over half a dozen appli-cations with vocabularies of between 100 and 500 words.
The core functionality provided by the REGU - LUS 2 environment is compilation of typed unifi-cation grammars into annotated context-free gram-mar language models expressed in Nuance Gram-mar Specification Language (GSL) notati[REF_CITE].
"GSL language models can be con-verted into runnable speech recognisers by invoking the Nuance Toolkit compiler utility, so the net result is the ability to compile a unification grammar into a speech recogniser."
Experience with grammar-based spoken dialogue systems shows that there is usually a substantial overlap between the structures of grammars for dif-ferent domains.
"This is hardly surprising, since they all ultimately have to model general facts about the linguistic structure of English and other natural lan-guages."
"It is consequently natural to consider strate-gies which attempt to exploit the overlap between domains by building a single, general grammar valid for a wide variety of applications."
A grammar of this kind will probably offer more coverage (and hence lower accuracy) than is desirable for any given spe-cific application.
It is however feasible to address the problem using corpus-based techniques which extract a specialised version of the original general grammar.
REGULUS implements a version of the grammar specialisation scheme which extends the Explana-tion Based Learning method described[REF_CITE].
"There is a general unification gram-mar, loosely based on the Core Language Engine grammar for English[REF_CITE], which has been developed over the course of about ten individ-ual projects."
The semantic representations produced by the grammar are in a simplified version of the Core Language Engine’s Quasi Logical Form nota- tion (van[REF_CITE]).
A grammar built on top of the general grammar is transformed into a specialised Nuance grammar in the following processing stages: [Footnote_1].
"1 The current system post-processes Alterf semantic atom lists to represent domain dependancies between semantic atoms more directly before passing on the result. e.g. (correction, set alarm, 5, minutes) is repack-aged as (correction(set alarm(time(0,5))))"
The training corpus is converted into a “tree-bank” of parsed representations.
This is done using a left-corner parser representation of the grammar. 2.
"The treebank is used to produce a specialised grammar in REGULUS format, using the EBL algorithm (van[REF_CITE]). 3."
The final specialised grammar is compiled into a Nuance GSL grammar.
"ALTERF[REF_CITE]is another Open Source toolkit, whose purpose is to allow a clean combination of rule-based and corpus-driven pro-cessing in the semantic interpretation phase."
"There is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind ALTERF is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven."
ALTERF characterises semantic analysis as a task slightly extending the “decision-list” classification algorithm[REF_CITE].
"We start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms."
"For example, in the procedure assistant do-main we represent the utterances please speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as {increase volume} {show, sample syringe} {set alarm, 5, minutes} {correction, next step} where increase volume, show, sample syringe, set alarm, 5, minutes, correction and next step are semantic atoms."
"As well as specifying the permitted semantic atoms themselves, we also define a target model which for each atom specifies the other atoms with which it may legitimately combine."
"Thus here, for example, correction may legitimately combine with any atom, but minutes may only combine with correction, set alarm or a number. 1 ."
"Training data consists of a set of utterances, in either text or speech form, each tagged with its in-tended semantic representation."
"We define a set of feature extraction rules, each of which associates an utterance with zero or more features."
Feature ex-traction rules can carry out any type of processing.
"In particular, they may involve performing speech recognition on speech data, parsing on text data, ap-plication of hand-coded rules to the results of pars-ing, or some combination of these."
"Statistics are then compiled to estimate the probability p(a | f) of each semantic atom a given each separate feature f, using the standard formula p(a | f) = (N fa + 1)/(N f + 2) where N f is the number of occurrences in the train-ing data of utterances with feature f, and N fa is the number of occurrences of utterances with both fea-ture f and semantic atom a."
The decoding process follows[REF_CITE]in assuming complete dependence between the fea-tures.
"Note that this is in sharp contrast with the Naive Bayes classifier[REF_CITE], which as-sumes complete independence."
"Of course, neither assumption can be true in practice; however, as ar-gued[REF_CITE], there are good reasons for preferring the dependence alternative as the better option in a situation where there are many features extracted in ways that are likely to overlap."
"We are given an utterance u, to which we wish to assign a representation R(u) consisting of a set of semantic atoms, together with a target model com-prising a set of rules defining which sets of seman- tic atoms are consistent."
"The decoding process pro-ceeds as follows: 1. Initialise R(u) to the empty set. 2. Use the feature extraction rules and the statis-tics compiled during training to find the set of all triples hf, a, pi where f is a feature associ-ated with u, a is a semantic atom, and p is the probability p(a | f) estimated by the training process. 3. Order the set of triples by the value of p, with the largest probabilities first."
"Call the ordered set T . 4. Remove the highest-ranked triple hf, a, pi from T. Add a to R(u) iff the following conditions are fulfilled: • p ≥ p t for some pre-specified threshold value p t . • Addition of a to R(u) results in a set which is consistent with the target model. 5."
Repeat step (4) until T is empty.
"Intuitively, the process is very simple."
"We just walk down the list of possible semantic atoms, start-ing with the most probable ones, and add them to the semantic representation we are building up when this does not conflict with the consistency rules in the target model."
"We stop when the atoms suggested are too improbable, that is, they have probabilies be-low a cut-off threshold."
We have described a non-trivial spoken language di-alogue application built using generic Open Source tools that combine rule-based and corpus-driven processing.
"We intend to demo the system with par-ticular reference to these tools, displaying intermedi-ate results of processing and showing how the cover-age can be rapidly reconfigured in an example-based fashion."
"Since written Chinese has no space to de-limit words, segmenting Chinese texts be-comes an essential task."
"During this task, the problem of unknown word occurs."
It is impossible to register all words in a dictio-nary as new words can always be created by combining characters.
We propose a unified solution to detect unknown words in Chinese texts.
"First, a morphological analysis is done to obtain initial segmen-tation and POS tags and then a chunker is used to detect unknown words."
"Like many other Asian languages (Thai, Japanese, etc), written Chinese does not delimit words by spaces and there is no clue to tell where the word boundaries are."
"Therefore, it is usually required to segment Chinese texts prior to further processing."
"Previous research has been done for segmentation, however, the results obtained are not quite satisfac-tory when unknown words occur in the texts."
An unknown word is defined as a word that is not found in the dictionary.
"As for any other language, all pos-sibilities of derivational morphology cannot be fore-seen in the form of a dictionary with a fixed number of entries."
"Therefore, proper solutions are necessary for the detection of unknown words."
"Along traditional methods, unknown word detec-tion has been done using rules for guessing their location."
"This can ensure a high precision for the detection of unknown words, but unfortunately the recall is not quite satisfactory."
"It is mainly due to the Chinese language, as new patterns can always be created, that one can hardly efficiently maintain the rules by hand."
"Since the introduction of statis-tical techniques in NLP, research has been done on Chinese unknown word detection using such tech-niques, and the results showed that statistical based model could be a better solution."
The only resource needed is a large corpus.
"Fortunately, to date, more and more Chinese tagged corpora have been created for research purpose."
"We propose an “all-purpose” unknown word de-tection method which will extract person names, or-ganization names and low frequency words in the corpus."
We will treat low frequency words as gen-eral unknown words in our experiments.
"First, we segment and assign POS tags to words in the text using a morphological analyzer."
"Second, we break segmented words into characters, and assign each character its features."
"At last, we use a SVM-based chunker to extract the unknown words."
We shall now describe the 3 steps successively.
ChaSen is a widely used morphological analyzer for Japanese texts[REF_CITE].
It achieves over 97% precision for newspaper articles.
"We as-sume that Chinese language has similar characteris-tics with Japanese language to a certain extent, as both languages share semantically heavily loaded characters, i.e. kanji for Japanese, hanzi for Chinese."
"Based on this assumption, a model for Japanese may do well enough on Chinese."
This morphological an-alyzer is based on Hidden Markov Models.
The tar-get is to find the word and POS sequence that max-imize the probability.
The details can be found[REF_CITE].
Character based features allow the chunker to detect unknown words more efficiently.
It is especially the case when unknown words overlap known words.
"For  example. . .,”ChaSen(Deng Yingchaowill segmentbeforethe deathphrase) into” ” / / / /. . . ” (Deng Ying before next life)."
"If we use word based features, it is impossible to detect the unknown person name ” ” because it will not break up the word ” ” (next life)."
Breaking words into characters enables the chunker to look at characters individually and to identify the unknown person name above.
The POS tag from the output of morphological analysis is subcategorized to include the position of the character in the word.
The list of positions is shown in Table 1.
"For example, if a word contains three characters, then the first character is POS -B, the second is POS -I and the third is POS -E."
A single character word is tagged as POS -S.
Character types can also be used as features for chunking.
"However, the only information at our dis-posal is the possibility for a character to be a fam-ily name."
The set of characters used for translitera-tion may also be useful for retrieving transliterated names.
"We use a Support Vector Machines-based chunker, YamCha[REF_CITE], to extract unknown words from the output of the morphologi-cal analysis."
The chunker uses a polynomial kernel of degree 2.
Please refer to the paper cited for de-tails.
"Basically we would like to classify the characters into 3 categories, B (beginning of a chunk), I (inside a chunk) and O (outside a chunk)."
A chunk is con-sidered as an unknown word in this case.
"We can either parse a sentence forwardly, from the begin-ning of a sentence, or backwardly, from the end of a sentence."
There are always some relationships be-tween the unknown words and the their contexts in the sentence.
We will use two characters on each left and right side as the context window for chunking.
Figure 1 illustrates a snapshot of the chunking process.
"During forward parsing, to infer the un-known word tag “I” at position i, the chunker uses the features appearing in the solid box."
Reverse is done in backward parsing.
We conducted an open test experiment.
A one-month news of year 1998 from the People’s Daily was used as the corpus.
"It contains about 300,000 words (about 1,000,000 characters) with 39 POS tags."
The corpus was divided into 2 parts randomly with a size ratio for training/testing of 4/1.
All person names and organization names were deleted from the dictionary for extraction.
"There were 4,690 person names and 2,871 organization names in the corpus."
"For general unknown word, all words that occurred only once in the corpus were deleted from the dictionary, and were treated as un-known words. 12,730 unknown words were created under this condition."
"We now present the results of our experiments in re-call, precision and F-measure, as usual in such ex-periments."
Table 2 shows the results of person name extraction.
The accuracy for retrieving person names was quite satisfiable.
"We could also extract names overlap-ping with the next known word  ./vFor/fexample  , for the sequence “ /Ng /Ag /v /v /u /n” (The things that Deng Yingchao used before death), the system  ” althoughwas ablethetolastcorrectlycharacterre-trieve the name “ tify transliterated foreign names such as “  ” is part of a known word “ ”."
"It could also iden- (Filali) 1 , “ !#&quot; . $!% ” (Frank Kahn) 2 , “ &amp;#% ” (Boraine) [Footnote_3] , etc."
3 Truth Commission Deputy[REF_CITE]
"Furthermore, it was proved that if we have the in-formation that a character is a possible character for family name, it helps to increase the accuracy of the system, as the last two rows of Table 2 show. are - /asuch” (Laoas in the sequence “ ( /a ) /q * /d + /d ,"
"Some person names that could not be extracted ample, “ ( Zhang ). * ” wasis stillextractedvery positiveas a person)."
"In thisnameex-, however the right name is “ ( ) ” only."
"This is be-cause the next character of the unknown ones is a monosyllabic word, thus there is higher possibility Another example is “ / /q .) /v 0 /n [Footnote_1] /n” (The that it is joined with the unknown word as a chunk. owner Zhang Baojun), where the family name “ ) ” has been joined with the known word “ [Footnote_2]) ” (sug-gest) before it."
1 the former Prime Minister of Morocco
2 Western Cape Attorney General of South[REF_CITE]
"Therefore, the person name “ ) ” be “ / /n ) /nr”). was not extracted (the correct segmentation should"
Table 3 shows the result for organization name ex-traction.
Organization names are best extracted by using backward parsing.
"This may be explained by the fact that, in Chinese, the last section of a word nization name, such as, “ 425 ” (company), “  ” is usually the keyword showing that it is an orga- (group), “ 829 ” (organization), etc."
"By parsing the sentence backwardly, these keywords will be first looked at and will have higher possibility to be iden-tified."
"There are quite a number of organization names that BA2C@ could D not 4 5 ? Company), “  ” (Xiangfan City Zhida Car Rental” (Shanghai Zhuang Mother Jingcaishe Service Lim-ited Company)."
"This could be because the names are too long, and the 2 characters left and right con-text window is not enough for the system to make a correct judgement."
"As mentioned above, we deleted all words that occur only once from the dictionary to artificially create unknown words."
"Those “unknown words” included common nouns, verbs, numbers, etc."
The results for this experiment are shown in Table 4.
"In general, around 60% accuracy (F-measure) was achieved for unknown word detection, and back-ward parsing seems doing slightly better than for-ward parsing."
"As to ensure that character based chunking is better than word based chunking, we have carried out an experiment with word based chunking as well."
The results showed that character based chunking f-measure ( U word based V vs U character based V ) yields better results than word based chunking.
"The for person name extraction is (81.28 vs 84.69), for organization name is (67.88 vs 70.40), and for gen-eral unknown word is (56.96 vs 61.00) respectively."
"There are basically two methods to extract unknown words, statistical and rule based approaches."
"In this section, we compare our results with previous re-ported work.[REF_CITE]present an approach that au-tomatically generates morphological rules and sta-tistical rules from a training corpus."
"They use a very large corpus to generate the rules, therefore the rules generated can represent patterns of unknwon words as well."
"While we use a different corpus for the experiment, it is difficult to perform a comparison."
They report a precision of 89% and a recall of 68% for all unknown word types.
This is better than our system which achieves only 65% for precision and 58% for recall.
"In[REF_CITE], local statistics information are used to identify the location of unknown words."
They assume that the frequency of the occurences of an unknown word is normally high in a fixed cache size.
They have also investigated on the relationship between the size of the cache and its performance.
"They report that the larger the cache, the higher the recall, but not the case for precision."
"They report a recall of 54.9%, less than the 58.43% we achieved.[REF_CITE]suggest a method that is based on role tagging for unknown words recogni-tion."
Their method is also based on Markov Mod- els.
Our method is closest to the role tagging idea as this latter is also a sort of character based tagging.
The extension in our method is that we first do mor-phological analysis and then use chunking based on SVM for unknown word extraction.
"In their paper, they report an F-measure of 79.30% in open test en-vironment for person name extraction."
Our method seems better with an F-measure of 86.78% for per-son name extraction (for both Chinese and foreign names).
We proposed an “all-purpose” method for Chinese unknown word detection.
"Our method is based on an morphological analysis that generates segmenta-tions and POS tags using Markov Models, followed by a chunking based on character features using Support Vector Machines."
We have also shown that character based features yields better results than word based features in the chunking process.
Our experiments showed that the proposed method is able to detect person names and organization names quite accurately and is also quite satisfactory even for low frequency unknown words in the corpus.
"This paper describes a Web-based Eng-lish-Chinese concordance system, Total-Recall, developed to promote translation reuse and encourage authentic and idio-matic use in second language writing."
We exploited and structured existing high-quality translations from the bilingual Si-norama Magazine to build the concor-dance of authentic text and translation.
"Novel approaches were taken to provide high-precision bilingual alignment on the sentence, phrase and word levels."
A browser-based user interface (UI) is also developed for ease of access over the Internet.
"Users can search for word, phrase or expression in English or Chi-nese."
The Web-based user interface facili-tates the recording of the user actions to provide data for further research.
"A concordance tool is particularly useful for study-ing a piece of literature when thinking in terms of a particular word, phrase or theme."
"It will show ex-actly how often and where a word occurs, so can be helpful in building up some idea of how differ-ent themes recur within an article or a collection of articles."
Concordances have been indispensable for lexicographers and increasingly considered useful for language instructor and learners.
"A bilingual concordance tool is like a monolingual concor-dance, except that each sentence is followed by its translation counterpart in a second language."
"It could be extremely useful for bilingual lexicogra-phers, human translators and second language vit.edu.tw jschang@cs.nthu.edu.tw learners."
"Pierre Isabelle, in 1993, pointed out: “ex-isting translations contain more solutions to more translation problems than any other existing re-source.”"
It is particularly useful and convenient when the resource of existing translations is made available on the Internet.
A web based bilingual system has proved to be very useful and popular.
"For example, the English-French concordance sys-tem, TransSearch[REF_CITE]."
"Pro-vides a familiar interface for the users who only need to type in the expression in question, a list of citations will come up and it is easy to scroll down until one finds one that is useful."
TotalRecall comes with an additional feature making the solu-tion more easily recognized.
"The user not only get all the citations related to the expression in ques-tion, but also gets to see the translation counterpart highlighted."
TotalRecall extends the translation memory technology and provide an interactive tool intended for translators and non-native speakers trying to find ideas to properly express themselves.
"Total-Recall empower the user by allow her to take the initiative in submitting queries for searching au-thentic, contemporary use of English."
"These que-ries may be single words, phrases, expressions or even full sentence, the system will search a sub-stantial and relevant corpus and return bilingual citations that are helpful to human translators and second language learners."
Central to TotalRecall is a bilingual corpus and a set of programs that provide the bilingual analyses to yield a translation memory database out of the bilingual corpus.
"Currently, we are working with a collection of Chinese-English articles from the Si-norama magazine."
A large bilingual collection of
Studio Classroom English lessons will be provided in the near future.
That would allow us to offer bilingual texts in both translation directions and with different levels of difficulty.
"Currently, the articles from Sinaroma seems to be quite usefully by its own, covering a wide range of topics, re-flecting the personalities, places, and events in Taiwan for the past three decade."
"The concordance database is composed of bi-lingual sentence pairs, which are mutual transla-tion."
"In addition, there are also tables to record additional information, including the source of each sentence pairs, metadata, and the information on phrase and word level alignment."
"With that ad-ditional information, TotalRecall provides various functions, including 1. viewing of the full text of the source with a simple click. 2. highlighted translation counterpart of the query word or phrase. 3. ranking that is pedagogically useful for transla-tion and language learning."
"We are currently running an experimental pro-totype with Sinorama articles, dated mainly from 1995 to 2002."
"There are approximately 50,000 bi-lingual sentences and over 2 million words in total."
We also plan to continuously updating the database with newer information from Sinorama magazine so that the concordance is kept current and relevant to the .
To make these up to date and relevant.
The bilingual texts that go into TotalRecall must be rearranged and structured.
We describe the main steps below:
"After parsing each article from files and put them into the database, we need to segment articles into sentences and align them into pairs of mutual translation."
"While the length-based approach[REF_CITE]to sentence alignment produces surprisingly good results for the close language pair of French and English at success rates well over 96%, it does not fair as well for distant language pairs such as English and Chinese."
"Work on sentence alignment of English and Chi-nese texts[REF_CITE], indicates that the lengths of English and Chinese texts are not as highly corre-lated as in French-English task, leading to lower success rate (85-94%) for length-based aligners."
Table 1 The result of Chinese collocation candi-dates extracted.
The shaded collocation pairs are selected based on competition of whole phrase log likelihood ratio and word-based translation prob-ability.
"Un-shaded items 7 and 8 are not selected because of conflict with previously chosen bilin-gual collocations, items 2 and 3."
"Simard, Foster, and[REF_CITE]pointed out cognates in two close languages such as English and French can be used to measure the likelihood of mutual translation."
"However, for the English-Chinese pair, there are no orthographic, phonetic or semantic cognates readily recognizable by the computer."
"Therefore, the cognate-based approach is not applicable to the Chinese-English tasks."
"At first, we used the length-based method for sentence alignment."
The average precision of aligned sentence pairs is about 95%.
We are now switching to a new alignment method based on punctuation statistics.
"Although the average ratio of the punctuation counts in a text is low (less than 15%), punctuations provide valid additional evi-dence, helping to achieve high degree of alignment precision."
"It turns out that punctuations are telling evidences for sentence alignment, if we do more than hard matching of punctuations and take into consideration of intrinsic sequencing of punctua-tion in ordered comparison."
Experiment results show that the punctuation-based approach outper-forms the length-based approach with precision rates approaching 98%.
"After sentences and their translation counterparts are identified, we proceeded to carry out finer-grained alignment on the phrase and word levels."
We employ part of speech patterns and statistical analyses to extract bilingual phrases/collocations from a parallel corpus.
The preferred syntactic pat-terns are obtained from idioms and collocations in the machine readable English-Chinese version of Longman Dictionary of Contemporary of English.
Phrases matching the patterns are extract from aligned sentences in a parallel corpus.
Those phrases are subsequently matched up via cross lin-guistic statistical association.
Statistical association between the whole phrase as well as words in phrases are used jointly to link a collocation and its counterpart collocation in the other language.
See Table 1 for an example of extracting bilingual col-locations.
"The word and phrase level information is kept in relational database for use in processing queries, hightlighting translation counterparts, and ranking citations."
Sections 3 and 4 will give more details about that.
The goal of the TotalRecall System is to allow a user to look for instances of specific words or ex- pressions.
"For this purpose, the system opens up two text boxes for the user to enter queries in any one of the languages involved or both."
We offer some special expressions for users to specify the following queries: • Exact single word query - W.
"For instance, enter “work” to find citations that contain “work,” but not “worked”, “working”, “works.” • Exact single lemma query – W+."
"For in-stance, enter “work+” to find citations that contain “work”, “worked”, “working”, “works.” • Exact string query."
"For instance, enter “in the work” to find citations that contain the three words, “in,” “the,” “work” in a row, but not citations that contain the three words in any other way. • Conjunctive and disjunctive query."
"For in-stance, enter “give+ advice+” to find cita-tions that contain “give” and “advice.”"
"It is also possible to specify the distance between “give” and “advice ,” so they are from a VO construction."
"Similarly, enter “hard | diffi-cult | tough” to find citations that involve difficulty to do, understand or bear some-thing, using any of the three words."
"Once a query is submitted, TotalRecall dis-plays the results on Web pages."
"Each result ap-pears as a pair of segments, usually one sentence each in English and Chinese, in side-by-side for-mat."
"The words matching the query are high-lighted, and a “context” hypertext link is included in each row."
"If this link is selected, a new page ap-pears displaying the original document of the pair."
"If the user so wishes, she can scroll through the following or preceding pages of context in the original document."
It is well known that the typical user usual has no patient to go beyond the first or second pages re-turned by a search engine.
"Therefore, ranking and putting the most useful information in the first one or two is of paramount importance for search en-gines."
This is also true for a concordance.
Experiments with a focus group indicate that the following ranking strategies are important: • Citations with a translation counterpart should be ranked first. • Citations with a frequent translation coun-terpart appear before ones with less frequent translation • Citations with same translation counterpart should be shown in clusters by default.
"The cluster can be called out entirely on demand. • Ranking by nonlinguistic features should also be provided, including date, sentence length, query position in citations, etc."
"With various ranking options available, the users can choose one that is most convenient and productive for the work at hand."
"In this paper, we describe a bilingual concordance designed as a computer assisted translation and language learning tool."
"Currently, TotalRecall uses Sinorama Magazine corpus as the translation memory and will be continuously updated as new issues of the magazine becomes available."
We have already put a beta version on line and ex-perimented with a focus group of second language learners.
"Novel features of TotalRecall include highlighting of query and corresponding transla-tions, clustering and ranking of search results ac-cording translation and frequency."
TotalRecall enable the non-native speaker who is looking for a way to express an idea in English or Chinese.
"We are also adding on the basic func-tions to include a log of user activities, which will record the users’ query behavior and their back-ground."
We could then analyze the data and find useful information for future research.
"Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings."
"Unlike previous statistical formalisms (limited to isomorphic trees), synchronous TSG allows local distortion of the tree topol-ogy."
"We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding."
Statistical machine translation systems are trained on pairs of sentences that are mutual translations.
"For exam-ple, (beaucoup d’enfants donnent un baiser à"
"Sam, kids kiss Sam quite often)."
"This translation is somewhat free, as is common in naturally occurring data."
The first sen-tence is literally Lots of’children give a kiss to Sam.
This short paper outlines “natural” formalisms and al-gorithms for training on pairs of trees.
Our methods work on either dependency trees (as shown) or phrase-structure trees.
Note that the depicted trees are not isomorphic. donnent kiss baiser a
Sam often beaucoup un Sam kids d’ quite enfants
Our main concern is to develop models that can align and learn from these tree pairs despite the “mismatches” in tree structure.
"Many “mismatches” are characteristic of a language pair: e.g., preposition insertion (of → ), multiword locutions (kiss ↔ give a kiss to; misinform ↔ wrongly inform), and head-swapping (float down ↔ descend by floating)."
"Such systematic mismatches should be learned by the model, and used during translation."
It is even helpful to learn mismatches that merely tend to arise during free translation.
Knowing that beaucoup d’ is often deleted will help in aligning the rest of the tree.
When would learned tree-to-tree mappings be useful?
"Obviously, in MT, when one has parsers for both the source and target language."
Systems for “deep” anal-ysis and generation might wish to learn mappings be-tween deep and surface trees[REF_CITE]or between syntax and semantics[REF_CITE].
Systems for summarization or paraphrase could also be trained on tree pairs[REF_CITE].
Non-NLP applications might include comparing student-written programs to one another or to the correct solution.
Our methods can naturally extend to train on pairs of forests (including packed forests obtained by chart pars-ing).
The correct tree is presumed to be an element of the forest.
"This makes it possible to train even when the correct parse is not fully known, or not known at all."
We make the quite natural proposal of using a syn-chronous tree substitution grammar (STSG).
An STSG is a collection of (ordered) pairs of aligned elementary trees.
These may be combined into a derived pair of trees.
Both the elementary tree pairs and the operation to combine them will be formalized in later sections.
"As an example, the tree pair shown in the introduction might have been derived by “vertically” assembling the 6 elementary tree pairs below."
"The _ symbol denotes a frontier node of an elementary tree, which must be replaced by the circled root of another elementary tree."
"If two frontier nodes are linked by a dashed line labeled with the state X, then they must be replaced by two roots that are also linked by a dashed line labeled with X."
The elementary trees represent idiomatic translation “chunks.”
"The frontier nodes represent unfilled roles in the chunks, and the states are effectively nonterminals that specify the type of filler that is required."
"Thus, don-nent un baiser à (“give a kiss to”) corresponds to kiss, with the French subject matched to the English subject, and the French indirect object matched to the English direct object."
"The states could be more refined than those shown above: the state for the subject, for exam-ple, should probably be not NP but a pair (N pl , NP 3s )."
STSG is simply a version of synchronous tree-adjoining grammar or STAG[REF_CITE]that lacks the adjunction operation. (It is also equivalent to top-down tree transducers.)
"What, then, is new here?"
"First, we know of no previous attempt to learn the “chunk-to-chunk” mappings."
"That is, we do not know at training time how the tree pair of section 1 was derived, or even what it was derived from."
"Our approach is to reconstruct all possible derivations, using dynamic pro-gramming to decompose the tree pair into aligned pairs of elementary trees in all possible ways."
"This produces a packed forest of derivations, some more probable than others."
"We use an efficient inside-outside algorithm to do Expectation-Maximization, reestimating the model by training on all derivations in proportion to their probabil-ities."
The runtime is quite low when the training trees are fully specified and elementary trees are bounded in size. [Footnote_1]
"1[REF_CITE]presents efficient TSG parsing with un-bounded elementary trees. Unfortunately, that clever method does not permit arbitrary models of elementary tree probabili-ties, nor does it appear to generalize to our synchronous case. (It would need exponentially many nonterminals to keep track of an matching of unboundedly many frontier nodes.)"
"Second, it is not a priori obvious that one can reason-ably use STSG instead of the slower but more powerful STAG."
TSG can be parsed as fast as CFG.
"But without an adjunction operation, [Footnote_2] , one cannot break the training trees into linguistically minimal units."
"2 Or a sister-adjunction operation, for dependency trees."
"An elementary tree pair A = (elle est finalement partie, finally she left) cannot be further decomposed into B = (elle est partie, she left) and C = (finalement, finally)."
This appears to miss a generalization.
"Our perspective is that the gener-alization should be picked up by the statistical model that defines the probability of elementary tree pairs. p(A) can be defined using mainly the same parameters that define p(B) and p(C), with the result that p(A) ≈ p(B) · p(C)."
The balance between the STSG and the statistical model is summarized in the last paragraph of this paper.
"Third, our version of the STSG formalism is more flexible than previous versions."
"We carefully address the case of empty trees, which are needed to handle free-translation “mismatches.”"
"In the example, an STSG can-not replace beaucoup d’ (“lots of”) in the NP by quite often in the VP; instead it must delete the former and in-sert the latter."
"Thus we have the alignments (beaucoup d’, ) and ( , quite often)."
These require innovations.
"The tree-internal deletion of beaucoup d’ is handled by an empty elementary tree in which the root is itself a fron-tier node. (The subject frontier node of kiss is replaced with this frontier node, which is then replaced with kids.)"
The tree-peripheral insertion of quite often requires an English frontier node that is paired with a French null.
We also formulate STSGs flexibly enough that they can handle both phrase-structure trees and dependency trees.
"The latter are small and simple[REF_CITE]: tree nodes are words, and there need be no other structure to recover or align."
Selectional preferences and other in-teractions can be accommodated by enriching the states.
Any STSG has a weakly equivalent SCFG that gen-erates the same string pairs.
So STSG (unlike STAG) has no real advantage for modeling string pairs. [Footnote_3]
"3 However, the binary-branching SCFGs used[REF_CITE]and[REF_CITE]are strictly less powerful than STSG."
"But STSGs can generate a wider variety of tree pairs, e.g., non-isomorphic ones."
"So when actual trees are provided for training, STSG can be more flexible in aligning them."
"Most statistical MT derives from IBM-style models[REF_CITE], which ignore syntax and allow ar-bitrary word-to-word translation."
"Hence they are able to align any sentence pair, however mismatched."
"However, they have a tendency to translate long sentences into word salad."
"Their alignment and translation accuracy improves when they are forced to translate shallow phrases as con-tiguous, potentially idiomatic units[REF_CITE]."
"Several researchers have tried putting “more syntax” into translation models: like us, they use statistical ver-sions of synchronous grammars, which generate source and target sentences in parallel and so describe their cor-respondence. [Footnote_4] This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word’s translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding[REF_CITE]."
"4 The joint probability model can be formulated, if desired, as a language model times a channel model."
Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar[REF_CITE].
"This means that a sentence and its trans-lation must have isomorphic syntax trees, although they may have different numbers of surface words if null words are allowed in one or both languages."
This rigid-ity does not fully describe real data.
"The one exception is the synchronous DOP approach[REF_CITE], which obtains an STSG by decom-posing aligned training trees in all possible ways (and us-ing “naive” count-based probability estimates)."
"However, we would like to estimate a model from unaligned data."
"For expository reasons (and to fill a gap in the literature), first we formally present non-synchronous TSG."
Let Q be a set of states.
Let L be a set of labels that may decorate nodes or edges.
Node labels might be words or nontermi-nals.
Edge labels might include grammatical roles such as Subject.
"In many trees, each node’s children have an order, recorded in labels on the node’s outgoing edges."
"An elementary tree is a a tuple hV,V i ,E,`,q,si where V is a set of nodes; V i ⊆ V is the set of internal nodes, and we write V f = V − V i for the set of frontier nodes; E ⊆ V i × V is a set of directed edges (thus all frontier nodes are leaves)."
"The graph hV, Ei must be con-nected and acyclic, and there must be exactly one node r ∈ V (the root) that has no incoming edges."
"The func-tion ` : (V i ∪ E) → L labels each internal node or edge; q ∈ Q is the root state, and s : V f → Q assigns a fron-tier state to each frontier node (perhaps including r)."
A TSG is a set of elementary trees.
"The generation process builds up a derived tree T that has the same form as an elementary tree, and for which V f = ∅. Initially, T is chosen to be any elementary tree whose root state T.q = Start."
"As long as T has any frontier nodes, T.V f , the process expands each frontier node d ∈ T.V f by sub-stituting at d an elementary tree t whose root state, t.q, equals d’s frontier state, T.s(d)."
This operation replaces T with hT.V ∪t.
"V −{d}, T.V i ∪t."
"V i , T.E 0 ∪t."
"E, T.`∪ t.`, T.q, T.s ∪ t.s − {d, t.q}i."
"Note that a function is re-garded here as a set of hinput,outputi pairs."
T.E 0 is a version of T.E in which d has been been replaced by t.r.
"A probabilistic TSG also includes a function p(t | q), which, for each state q, gives a conditional probability distribution over the elementary trees t with root state q."
The generation process uses this distribution to randomly choose which tree t to substitute at a frontier node of T having state q.
The initial value of T is chosen from p(t | Start).
"Thus, the probability of a given derivation is a product of p(t | q) terms, one per chosen elementary tree."
There is a natural analogy between (probabilistic) TSGs and (probabilistic) CFGs.
"An elementary tree t with root state q and frontier states q 1 . . . q k (for k ≥ 0) is analogous to a CFG rule q → t q 1 . . . q k . (By including t as a terminal symbol in this rule, we ensure that distinct elementary trees t with the same states correspond to dis-tinct rules.)"
"Indeed, an equivalent definition of the gener-ation process first generates a derivation tree from this derivation CFG, and then combines its terminal nodes t (which are elementary trees) into the derived tree T."
"Given a a grammar G and a derived tree T, we may be in-terested in constructing the forest of T’s possible deriva-tion trees (as defined above)."
"We call this tree parsing, as it finds ways of decomposing T into elementary trees."
"Given a node c ∈ T.v, we would like to find all the potential elementary subtrees t of T whose root t.r could have contributed c during the derivation of T. Such an elementary tree is said to fit c, in the sense that it is iso-morphic to some subgraph of T rooted at c."
The following procedure finds an elementary tree t that fits c. Freely choose a connected subgraph U of T such that U is rooted at c (or is empty).
"Let t.V i be the vertex set of U. Let t.E be the set of outgoing edges from nodes in t.V i to their children, that is, t.E = T.E ∩ (t.V i × T.V )."
"Let t.` be the restriction of T.` to t.V i ∪ t.E, that is, t.` = T.` ∩ ((t.V i ∪ t.E) × L)."
"Let t.V be the set of nodes mentioned in t.E, or put t.V = {c} if t.V i = t.E = ∅. Finally, choose t.q freely from Q, and choose s : t.V f → Q to associate states with the frontier nodes of t; the free choice is because the nodes of the derived tree T do not specify the states used during the derivation."
How many elementary trees can we find that fit c?
Let us impose an upper bound k on |t.
V i | and hence on |U|.
"Then in an m-ary tree T, the above procedure considers at k −1 most mm−1 connected subgraphs U of order ≤ k rooted at c. For dependency grammars, limiting to m ≤ 6 and k = 3 is quite reasonable, leaving at most 43 subgraphs U rooted at each node c, of which the biggest contain only c, a child c 0 of c, and a child or sibling of c 0 ."
"These will constitute the internal nodes of t, and their remaining children will be t’s frontier nodes."
"However, for each of these 43 subgraphs, we must jointly hypothesize states for all frontier nodes and the root node."
"For |Q| &gt; 1, there are exponentially many ways to do this."
"To avoid having exponentially many hy-potheses, one may restrict the form of possible elemen-tary trees so that the possible states of each node of t can be determined somehow from the labels on the corre-sponding nodes in T. As a simple but useful example, a node labeled NP might be required to have state NP."
Rich labels on the derived tree essentially provide supervision as to what the states must have been during the derivation.
The tree parsing algorithm resembles bottom-up chart parsing under the derivation CFG.
"But the input is a tree rather than a string, and the chart is indexed by nodes of the input tree rather than spans of the input string: [Footnote_5] 1. for each node c of T, in bottom-up order 2. for each q ∈ Q, let β c (q) = 0 3. for each elementary tree t that fits c 4. increment β c (t.q) by p(t | t.q) · Q d∈t.V f β d (t.s(d))"
"5 We gloss over the standard difficulty that the derivation CFG may contain a unary rule cycle. For us, such a cycle is a problem only when it arises solely from single-node trees."
The β values are inside probabilities.
"After running the algorithm, if r is the root of T, then βr(Start) is the prob-ability that the grammar generates T. p(t | q) in line 4 may be found by hash lookup if the grammar is stored explicitly, or else by some probabilistic model that analyzes the structure, labels, and states of the elementary tree t to compute its probability."
"One can mechanically transform this algorithm to compute outside probabilities, the Viterbi parse, the parse forest, and other quantities[REF_CITE]."
One can also apply agenda-based parsing strategies.
"For a fixed grammar, the runtime and space are only O(n) for a tree of n nodes."
The grammar constant is the number of possible fits to a node c of a fixed tree.
"As noted above, there usually not many of these (unless the states are uncertain) and they are simple to enumerate."
"As discussed above, an inside-outside algorithm may be used to compute the expected number of times each elementary tree t appeared in the derivation of T. That is the E step of the EM algorithm."
"In the M step, these ex-pected counts (collected over a corpus of trees) are used to reestimate the parameters θ~ of p(t | q)."
One alternates E and M steps till p(corpus | θ~)·p(~θ) converges to a local maximum.
The prior p(~θ) can discourage overfitting.
We are now prepared to discuss the synchronous case.
A synchronous TSG consists of a set of elementary tree pairs.
"An elementary tree pair t is a tuple ht 1 , t 2 , q, m, si."
"Here t 1 and t 2 are elementary trees without state la- 2. bels: we write t j = hV j ,V ji ,E j ,` j i. q ∈ Q is the 3. root state as before. m ⊆ V 1f × V 2f is a matching 4. between t 1 ’s and t 2 ’s frontier nodes, [Footnote_6] . Let m̄ denote m ∪ {(d 1 , null) : d 1 is unmatched in m} ∪ {(null, d 2 ) : d 2 is unmatched in m}."
6 A matching between A and B is a 1-to-1 correspondence between a subset of A and a subset of B.
"Finally, s : m̄ → Q assigns a state to each frontier node pair or unpaired frontier node."
"In the figure of section 2, donnent un baiser à has 2 frontier nodes and kiss has 3, yielding 13 possible match-ings."
"Note that least one English node must remain un-matched; it still generates a full subtree, aligned with null."
"As before, a derived tree pair T has the same form as an elementary tree pair."
The generation process is similar to before.
"As long as T.m̄ =6 ∅, the process expands some node pair (d 1 , d 2 ) ∈ T.m̄."
"It chooses an elementary tree pair t such that t.q = T.s(d 1 , d 2 )."
"Then for each j = 1, 2, it substitutes t j at d j if non-null. (If d j is null, then t.q must guarantee that t j is the special null tree.)"
"In the probabilistic case, we have a distribution p(t | q) just as before, but this time t is an elementary tree pair."
Several natural algorithms are now available to us: • Training.
"Given an unaligned tree pair (T 1 ,T 2 ), we can again find the forest of all possible derivations, with expected inside-outside counts of the elementary tree pairs."
This allows EM training of the p(t | q) model.
The algorithm is almost as before.
The outer loop iter-ates bottom-up over nodes c 1 of T 1 ; an inner loop iter-ates bottom-up over c 2 of T 2 .
"Inside probabilities (for example) now have the form β c 1 ,c 2 (q)."
"Although this brings the complexity up to O(n 2 ), the real complica-tion is that there can be many fits to (c 1 , c 2 )."
"There are still not too many elementary trees t 1 and t 2 rooted at c 1 and c 2 ; but each (t 1 , t 2 ) pair may be used in many ele-mentary tree pairs t, since there are exponentially many matchings of their frontier nodes."
"Fortunately, most pairs of frontier nodes have low β values that indicate that their subtrees cannot be aligned well; pairing such nodes in a matching would result in poor global proba-bility."
This observation can be used to prune the space of matchings greatly. • 1-best Alignment (if desired).
"This is just like train-ing, except that we use the Viterbi algorithm to find the single best derivation of the input tree pair."
This deriva-tion can be regarded as the optimal syntactic alignment. [Footnote_7] • Decoding.
"7 As free-translation post-processing, one could try to match pairs of stray subtrees that could have aligned well, according to the chart, but were forced to align with null for global reasons."
We create a forest of possible synchronous derivations (cf.[REF_CITE]).
"We chart-parse T 1 as much as in section 5, but fitting the left side of an elementary tree pair to each node."
"Roughly speaking: 1. for c 1 = null and then c 1 ∈ T 1 .V , in bottom-up order for each q ∈ Q, let β c 1 (q) = −∞ for each probable t = (t 1 , t 2 , q, m, s) whose t 1 fits c 1 max p(t | q) ·"
"Q (d 1 ,d 2 )∈m̄ β d 1 (s(d 1 , d 2 )) into β c 1 (q) We then extract the max-probability synchronous derivation and return the T 2 that it derives."
"This algo-rithm is essentially alignment to an unknown tree T 2 ; we do not loop over its nodes c 2 , but choose t 2 freely."
"We have sketched an EM algorithm to learn the probabil-ities of elementary tree pairs by training on pairs of full trees, and a Viterbi decoder to find optimal translations."
"We developed and implemented these methods at the 2002 CLSP Summer Workshop at Johns Hopkins Univer-sity, as part of a team effort (led by Jan Hajič) to translate dependency trees from surface Czech, to deep Czech, to deep English, to surface English."
"For the within-language translations, it sufficed to use a simplistic, fixed model of p(t | q) that relied entirely on morpheme identity."
"Team members are now developing real, trainable models of p(t | q), such as log-linear models on meaning-ful features of the tree pair t. Cross-language translation results await the plugging-in of these interesting models."
"The algorithms we have presented serve only to “shrink” the modeling, training and decoding problems from full trees to bounded, but still complex, elementary trees."
"In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models."
The units of translation are blocks – pairs of phrases.
"During decod-ing, we use a block unigram model and a word-based trigram language model."
"Dur-ing training, the blocks are learned from source interval projections using an un-derlying high-precision word alignment."
The system performance is significantly increased by applying a novel block exten-sion algorithm using an additional high-recall word alignment.
The blocks are fur-ther filtered using unigram-count selection criteria.
The system has been successfully test on a Chinese-English and an Arabic-English translation task.
Various papers use phrase-based translation systems[REF_CITE]that have shown to improve translation quality over single-word based transla-tion systems introduced[REF_CITE].
"In this paper, we present a similar system with a much simpler set of model parameters."
"Specifically  , we compute the probability of a block sequence ."
A block is a pair consisting of a contiguous source and a contiguous target phrase.
"The block sequence 



     (1)     &quot;! $#"
"We try % to % find  the &amp; *( ) block ,+ &amp; /*. 10 sequence  %$ ."
"Thethat modelmaximizespro-  : posed is a joint model as[REF_CITE], since target and source phrases are generated jointly."
The approach is illustrated in Figure 1.
The source phrases are given on the 2 -axis and the target phrases are given on the 3 -axis.
During block decod-ing a bijection between source and target phrases is generated.
The two types of parameters in Eq 1 are defined as: 4 Block unigram model 576*8 ;9 : : We compute un-igram probabilities for the blocks.
The blocks are simpler than the alignment templates[REF_CITE]in that they do not have an internal structure. 4 Trigram language model: the probability 576*8&lt;9&gt;=8%A? @ : between adjacent blocks is com-puted as the probability of the first target word in the target clump of 8 9 given the final two words of the target clump of A? @ .
The exponent B is set in informal experiments to be CEDGF.
No other parameters such as distortion proba-bilities are used.
"To select blocks 8 from training data, we compute unigram block co-occurrence counts HI6*: ."
HI6*8J: cannot be computed for all blocks in the training data: we would obtain hundreds of millions of blocks.
The blocks are restricted by an underlying word alignment.
"In this paper, we present a block generation algorithm similar to the one[REF_CITE]in full detail: source intervals are pro-jected into target intervals under a restriction derived from a high-precision word alignment."
The projec-tion yields a set of high-precision block links.
These block links are further extended using a high-recall word alignment.
The block extension algorithm is shown to improve translation performance signifi-cantly.
The system is tested on a Chinese-English (CE) and an Arabic-English (AE) translation task.
"The paper is structured as follows: in Section 2, we present the baseline block generation algorithm."
The block extension approach is described in Sec-tion 2.[Footnote_1].
1  and  denote a source positions.  and  denote a target positions.
Section 3 describes a DP-based decoder using blocks.
Experimental results are presented in Section 4.
Starting point for the block generation algorithm is a word alignment obtained from an HMM Viterbi training[REF_CITE].
The HMM Viterbi training is carried out twice with English as target language and Chinese as source language and vice versa.
We obtain two alignment relations:
K @ML  Q&gt;RTSO: =RVU$PXWZY\[ ^K ]
"L NO6*%8 _ : = 8aU ,Y WZP&quot;["
RbU/PcWdY is an alignment function from source to target positions and 8aUTYeWfP is an alignment func-tion from target to source positions 1 .
We compute the union K and the Kg intersection ] of the two alignment relations @ and :
Kg] h L K @jl K ] k L h
"We call the intersection relation , because it rep-resents a high-precision k alignment, and the union alignment , because it is taken to be a lower pre-cision higher recall h alignment[REF_CITE]."
The intersection is also a (partial) bijection be-tween the target and source positions: it covers the same number of target and source positions and there is a bijection between source and target po-sitions that are covered.
"For F the CE experiments reported in Section 4 about m % of the target h and source positions are covered F by word links in , for the AE experiments about n % are covered hpoqk ."
"The ex-tension algorithm presented assumes h that k , which is valid in this case since and are de-rived from intersection and union."
"We introduce the following additional piece of notation:  6 h : L = u`Y and  xw: h [ (2)  6 h : is the set of all source h positions that are cov-ered by some word links in , where the source po-sitions are shown along the P -axis and the target po-sitions are shown along the Y -axis."
"To derive high-precision block links from the high-precision word links, we use the following projection definitions: y S {\| : ~} = u`wzP\{ | and  E} xw: h [ y"
"Here, yS 6: projects source intervals into target in-tervals. `6_ : projects target intervals into source in-tervals and is defined accordingly h ."
"Starting from the high-precision word alignment , we try to derive a high-precision block alignment: we project source intervals z P {  | , where P {  w  6 h : ."
We compute the minimum target index Y { and h maximum target index Y for the word links 5w that fall into the
Figure 2: The left picture shows three blocks that are learned from projecting three source intervals.
The right picture shows three blocks that cannot be obtain from source interval projections . interval    \ .
"This way, we obtain a mapping of source intervals into target intervals:   © ª¹¬ ­J°± ¡   ¢,© µ·¶ ­J°± ¡  (3)     \¼»"
"The approach is illustrated in Figure 2, where in the left picture, for example, the source interval &lt;À¸ is projected into the target interval &lt;À¸ ."
The pair  \ \; « ¡  ¡  defines a block alignment link º .
We use this notation to emphasize that the identity of the words is not used in the block learning algorithm.
"To denote the block consisting of the target ÃÂ and source words at the link positions, we write Á ºE :"
Á ÄÂ   
Â     Â ¡   ¡   *Æ ­ ® &lt;Æ ­ %«*È ½ ® Ç &lt;È ½ % where Æ ­Â denote target words and È ½ denote source words. Ç denotes a function that maps intervals to the words at these intervals.
The algorithm for generating the high-precision block alignment links is given in Table 1.
The order in which the source intervals are generated does not change the final link set.
"Empirically, we find that expanding the high-precision block links significantly improves perfor-mance."
The expansion is parameterised  and de-scribed below.
"For a block link º     ;« ¡   ¡  , we compute its frontier ÆÉ ºE by looking at all word links that lie on one of the four boundary lines of a block."
"We make the following observation as shown in Figure 3: the number of links (filled dots in the picture) on the frontier  is less or equal Ê , since in every column and row there is at most one link in  , which is a partial bijetion."
"To learn blocks from a general word alignment that is not a bijection more than Ê word links may lie on the frontier of a block, but to compute all possible blocks, it is sufficient to look at all possible quadruples of word links."
"We extend the links on the frontier by links of the high-recall alignment Ë , where we use a parameterised way of locally extending a given word link."
We com-pute an extended link set Ì by extending each word link on the frontier separately and taking the union of the resulting links.
The way a word link is ex-tended is illustrated in Figure 4.
The filled dot in the center of the picture is an element of the high-precision set  .
"Starting from this link, we look for extensions in its neighborhood that lie in Õ , where the neighborhood is defined by a cell width parame-ter Þ and a distance parameter ß ."
"For instance, link à á* in Figure 4 is reached  with cell width ÞÃâäã and distance  , à¹é the link is reached with Þçâåã and ß âåè , the link à is reached with Þêâëè and ^ß âåì ."
The word link is added to í and it is itself extended using the same scheme.
"Here, we never make use of a row or a column covered by Ô other than the rows î and î`ï and the columns ð and ð/ï ."
"Also, we do not cross such a row or column using an exten-sion with ^ß ñêè : this way only a small fraction of the word links in Õ is used for extending a single block link."
The extensions are carried out iteratively until no new alignment links from Õ are added to í .
"The block extension algorithm in Table 2 uses the exten-sion set í to generate all word link quadruples: the extended block ò that is defined by a given quadru- 



 ú ïû &quot; ü úð ï \ü and ú ï û &quot; ü úî ï ;û ö ï ö ÿ â where the block  ïû ;ü û«ú *ï û  is said to be  ð ï %  $ ;ü û«ú îî . ï  The. ’seed’ ú ï û # ü"
Mú î ï  included in öùâ holds iff ï ñåî`ï and block link ö is extended ’outwardly’: all extended blocks ò in-clude the high-precision block ö .
"The block link ö links ö ï on its part, but &amp; ö äò  ö ï holds."
"An ex-itself may be included in other high-precision block tended block ò derived from the block ö never vio-lates the projection restriction relative to Ô i.e., we do not have to re-check the projection restriction for any generated block, which simplifies and fastens up the generation algorithm."
"The approach is illustrated in Figure 5, where a high-precision block with ì ele-ments on its frontier is extended by two blocks con-taining it."
The block link extension algorithm produces block links that contain new source and target intervals úð ï \ü and úî ï  that extend the interval mapping in Eq. 3.
"This mapping is no longer a function, but rather a relation between source and target intervals i.e., a single source interval is mapped to several tar-get intervals and vice versa."
"The extended block set constitutes a subset of the following set of interval pairs: ( ) * + ,.+ 0-1/ * 23,/.&amp;5 687&quot;9:) * + ,;:+ ./ =5 &gt;&lt; * 23., /@?"
The set of high-precision blocks is contained in this set.
"We cannot use the entire set of blocks defined by all pairs in the above relation, the resulting set of blocks cannot be handled due to memory restric-tions, which motivates our extension algorithm."
"We also tried the following symmetric restriction and tested the resulting block set: ) * + , + /.5=&gt;&lt; * / 7&quot;BC) * 2 , /.=5 &lt;&gt;* + , + / and (4)"
"The modified restriction is implemented in the con-text of the extension scheme in Table 1 by insert-ing an if statement before the alignment link D is restrictionextended: the 7EBC) alignment * 2 , /.5GH&lt; * link + , :+ is / extended only if thealso holds."
"Considering only block links for which the two way esting interpretation: assuming a bijection I that is projection in Eq. 4 holds has the following inter-complete i.e., all source and target positions are cov-ered, an efficient block segmentation algorithm ex-ists to compute a Viterbi block alignment as in Fig-ure 1 for a given training sentence pair."
The com-plexity of the algorithm is quadratic in the length of the source sentence.
This dynamic programming technique is not used in the current block selection but might be used in future work.
"For selecting blocks from the candidate block links, source phrases are equal or less than J words long. we restrict ourselves to block links where target and This way we obtain some tens of millions of blocks on our training data including blocks that occur only once."
This baseline )4LM5 set is further filtered using the unigram L count )4LM5QK P KON for which K : N .
"Fordenotesour theChinese-Englishset of blocks experiments, we use the KOR restriction as our base-line, and for the Arabic-English experiments the KOS clump are of length T are kept regardless of their 4LM5) restriction."
Blocks where the target and the source count [Footnote_2] .
"2 To apply the restrictions exhaustively, we have VXZ W imple- Y mented tree-based data structures to store up to million blocks with phrases of up to length in less than gigabyte of RAM."
We compute the unigram probability U
An example of [ blocks obtained from the Chinese-as relative frequency over all selected blocks.
English training data is shown in Figure 6. ’$ Lc DATE’ is a placeholder for L \ a date La expression.
Block con-to .
All [ blocks are LMc tains the blocks selected in training:
"La the unigram decoder prefers L \ L ^ even if , , and word links are word links in I , the striped word are much more frequent."
The solid links are word links in d .
"Using the links in d , we can learn \ one-to-many block translations, e.g. the pair ( e ,’Xinhua news agency’) is learned from the training data."
"We use a DP-based beam search procedure similar We maximize over all block segmentationsto the one presented in ([REF_CITE]LMf\ ).for which the source phrases yield a segmentation of the input source sentence, generating the target sentence simultaneously."
"The decoder processes search states of the following form: g ikj ,;- j -mlE-Ano-qp.,.h and j , are the two predecessor l j words used for the trigram language model, is the so-called cover-age vector to keep n track of the already processed sition.source position p , is the last processed source po-is the source phrase length of the block currently being matched. 0 is the length of the ini-tial fragment of the source phrase that has been pro-cessed so far.  is smaller or equal  :    ."
"Note, that the partial hypotheses are not distinguished ac-cording to the identity of the block itself."
The de-coder processes the input sentence ’cardinality syn-chronously’: all partial hypotheses that are active at a given point cover the same number of input sen-tence words.
The same beam-search pruning as de-scribed[REF_CITE]is used.
The so-called observation pruning threshold is modified matched by a block source phrase at most the bestas follows: for each source interval that is being  target phrases according to the joint unigram proba-bility are hypothesized.
The list of blocks that cor-respond to a matched source interval is stored in a chart for each input sentence.
This way the match-ing is carried out only once for all partial hypotheses that try to match the same input sentence interval.
"In the current experiments, decoding without block decoder translates aboutre-ordering yields the best ` translation results."
Thewords per second.
The translation system is tested on a Chinese-to- English translation task.
"For testing, we use the DARPA/[REF_CITE]dry-run testing `  consists of `  data, which sentences with words ar-ranged in documents [Footnote_3] ."
3 We removed the first _  documents that are contained in the training data.
The training data is pro-vided by the LDC and labeled by NIST as the Large Data condition for the[REF_CITE]evaluation.
Chinese sentences are  segmented into words.  
The training data contains C million Chinese and  million English words.
The block selection algo-rithm described  below runs less than one hour ona single -Gigahertz linux machine.
Table 3 presents results for various block extension schemes.
The first column describes the extension scheme used.
The second column reports the total number of blocks in millions collected - including all the blocks that occurred only once.
The third column reports the number of blocks that occurred at least twice.
These blocks are used to compute the results in the fourth column: the BLEU score[REF_CITE]with  reference translation us-ing  -grams along  with 95%  confidence interval isreported [Footnote_4] .
4 The test data is split into a certain number of subsets. The BLEU score is computed on each subset. We use the t-test to compare these scores.
Line and line of this table show re-sults where only the source interval projection #  with- w  out any extension is carried out.
"For the ex-tension scheme, the high-recall union set itself is used for projection."
"The results are worse than for all other schemes, since a lot of smaller blocks are  w discarded due to the projection approach."
"The u scheme, where just the u word links are used is too restrictive leaving out bigger blocks that are admis-sible according to u ."
"For the Chinese-English test data, there is only a minor difference between the tained for the u w and the u w  extension schemes. different extension schemes, the best results are ob-threshold, where the u8 w  blocks are used."
The sec- Table 4 shows the effect of the unigram selection ond column shows the number of blocks y&apos; selected y . and the  The best results are obtained for the sets.
The number of blocks can be reduced dras-tically where the translation performance declines only gradually.
Table 5 shows the effect of the maximum phrase length on the BLEU score for the O block set.
"In-cluding blocks with longer phrases actually helps to improve  performance, although already a length ofobtains nearly identical results. (using % 4M¡=¢£ as threshold): we obtained a block We carried out the following control experiments set of ¤C¥¦¨§ million blocks by generating blocks from all quadruples of word links in © [Footnote_5] ."
"5 We cannot compute ³ the block set resulting from all word link quadruples in , which is much bigger, due to CPU and memory restrictions."
This set is a proper superset of the blocks learned for the ©ªb«ª experiment in Table 3.
The resulting BLEU score is  .
Including additional smaller blocks even hurts translation performance in this case.
"Also, for the extension scheme ©¯ «° , we carried out the in-verse projection as described in Section 2.1 to obtain a block set of  million blocks and a BLEU score of  ."
"This number is smaller than the BLEU score of  for the © ¯ «° restriction: for the trans-lation direction Chinese-to-English, selecting blocks with longer English phrases seems to be important for good translation performance."
"It is interesting to note, that the unigram translation model is sym-metric: the translation direction can be switched to English-to-Chinese without re-training the model -just a new Chinese language model is needed."
"Our experiments, though, show that there is an unbalance with respect to the projection direction that has a sig-we carried out an experiment where we used the ©ªb« ª nificant influence on the translation results."
"Finally, applied only to blocks of target and source length § block set as a baseline."
"The extension algorithm was producing 1¯ and ° inone-to-manyFigure 6. translationsThe BLEU,scoree.g. theimprovedblocks to  with a block set of  million blocks."
It seems to be important to carry out the block exten-sion also for larger blocks.
We also ran the N2 system on the[REF_CITE]DARPA TIDES Large Data evaluation test set.
Six re-search sites and four commercial off-the-shelf sys-tems were evaluated in Large Data track.
A major-ity of the systems were phrase-based translation sys-tems.
"For comparison with other sites, we quote the"
NIST score[REF_CITE]on this test set: the N2 system scores 7.56 whereas the official top two systems scored 7.65 and 7.34 respectively.
We also carried out experiments for the translation direction Arabic to English using training data from UN documents.
"For testing, we use a test set of  sentences with ­C¸] words arranged in §¦ docu-ments The training data contains  million Ara-bic and  million English words."
The train-ing data is pre-processed using some morphologi-cal analysis.
"For the Arabic experiments, we have tested the ¤ extension schemes ©ºªb« ª , © ¯ « ¯ , and © ¯ « ° as shown in Table 6."
"Here, the results for the differ-ent schemes differ significantly and the ©»¯ « ° scheme produces the best results."
"For the AE experiments, only blocks up to a phrase length of ² are computed is split into several chunks of  training sen-due to disk memory restrictions."
"The training data tence pairs each, and the final block set together with the unigram count is obtained by merging the block files for each of the chunks written onto disk mem- ½ iterations ½ of the IBM Model ¾ ory."
The word-to-word alignment is trained using training followed by iterations of the HMM Viterbi training.
This training procedure takes about a day to execute on a single machine.
"Additionally, the ½ overall block se-lection procedure takes about ¿CÀ hours to execute."
Block-based translation units are used in several pa-pers on statistical machine translation.[REF_CITE]describe the alignment template system for statistical MT: alignment templates correspond to blocks that do have an internal structure.
Block unigram counts are used to filter the blocks.
The phrasal model is included into a syntax-based model.
Projection of phrases has also been used[REF_CITE].
A word link extension al-gorithm similar to the one presented in this paper is given[REF_CITE].
"In this paper, we describe a block-based unigram model for SMT."
A novel block learning algorithm is presented that extends high-precision interval pro-jections by elements from a high-recall alignment.
The extension method is shown to improve transla-tion performance significantly.
"For the Chinese-to-  English task, we obtained a NIST score of Á3À on the[REF_CITE]DARPA TIDES Large Data evalua-tion test set."
"We define, implement and evaluate a novel model for statistical machine translation, which is based on shal-low syntactic analysis (part-of-speechtagging and phrase chunking) in both the source and target languages."
It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language.
"We also examine aspects of lexical transfer, suggesting and exploring a concept of transla-tion coercion across parts of speech, as well as a transfer model based on lemma-to-lemma translation probabili-ties, which holds promise for improving machine trans-lation of low-density languages."
Experiments are per-formed in both Arabic-to-English and French-to-English translation demonstrating the efficacy of the proposed techniques.
Performance is automatically evaluated via the Bleu score metric.
"In this work we define, implement and evaluate a novel model for statistical machine translation (SMT)."
"Our goal was to produce a SMT system for translat-ing foreign languages into English which utilizes some syntactic information in both the foreign language and English without, however, requiring a full parse in either language."
Some advantages of not relying on full parses include that (1) there is a lack of availability of parsers for many languages of interest; (2) parsing time com-plexity represents a potential bottleneck for both model training and testing.
"Intuitively, the explicit modeling of syntactic phenom-ena should be of benefit in the machine translation task; the ability to handle long-distance motion in an intelli-gently constrained way is a salient example of such a benefit."
Allowing unconstrained translation reorderings at the word level generates a very large set of permu-tations that pose a difficult search problem at decoding time.
We propose a model that makes use of shallow parses (text chunking) to support long-distance motion of phrases without requiring deeper analysis of syntax.
"The resources required to train this system on a new lan-guage are minimal, and we gain the ability to model long-distance movement and some interesting proper-ties of lexical translation across parts of speech."
"One of the source languages we examine in this paper, Arabic, has a canonical sentence-level order of Verb-Subject-Object, which means that translation into English (with a standard ordering of Subject-Verb-Object) commonly requires motion of entire phrasal constituents, which is not true of French-to-English translation, to cite one lan-guage pair whose characteristics have wielded great in-fluence in the history of work on statistical machine translation."
A key motivation for and objective of this work was to build a translation model and feature space to handle the above-described phenomenon effectively.
"Statistical machine translation, as pioneered by IBM (e.g.[REF_CITE]), is grounded in the noisy chan-nel model."
"And similar to the related channel problems of speech and handwriting recognition, the original SMT language pair French-English exhibits a relatively close linear correlation in source and target sequence."
"Much common local motion that is observed for French, such as adjective-noun swapping, is adequately modeled by the relative-position-based distortion models of the clas-sic IBM approach."
"Unfortunately, these distortion mod-els are less effective for languages such as Japanese or Arabic, which have substantially different top-level sen-tential word orders from English, and hence longer dis-tance constituent motion."
"Yamada and Knight (2000, 2001) and[REF_CITE]have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively."
"While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a lim-itation shared with probabilistic context free grammars that the preferred ordering of subtrees is insufficiently constrained by their embedding context, which is espe-cially problematic for very deep syntactic parses."
"In contrast,[REF_CITE]have avoided the con-straints of tree-based syntactic models and allow the rel- atively flat motion of empirically derived phrasal chunks, which need not adhere to traditional constituent bound-aries."
"Our current paper takes a middle path, by grounding motion in syntactic transduction, but in a much flatter 2-level model of syntactic analysis, based on flat embed-ded noun-phrases in a flat sentential constituent-based chunk sequence that can be driven by syntactic brack-eters and POS tag models rather than a full parser, facili-tating its transfer to lower density languages."
"The flatter 2-level structures also better support transductions condi-tioned to full sentential context than do deeply embedded tree models, while retaining the empirically observed ad-vantages of translation ordering independence of noun-phrases."
"Another improvement over Och et al. and Yamada and Knight is the use of the finite state machine (FSM) mod-elling framework (e.g.[REF_CITE]), which offers the considerable advantage of a flexible framework for decoding, as well as a representation which is suitable for the fixed two-level phrasal mod-elling employed here."
"Finally, the original cross-part-of-speech lexical coer-cion models presented in Section 4.3.3 have related work in the primarily-syntactic coercion models utilized[REF_CITE]and[REF_CITE], although their induction and modelling are quite differ-ent from the approach here."
"As in other SMT approaches, the primary training re-source is a sentence-aligned parallel bilingual corpus."
We further require that each side of the corpus be part-of-speech (POS) tagged and phrase chunked; our lab has previously developed techniques for rapid training of such tools[REF_CITE].
Our trans-lation experiments were carried out on two languages: Arabic and French.
The Arabic training corpus was a subset of the United Nations (UN) parallel corpus which is being made available by the Linguistic Data Consor-tium.
"For French-English training, we used a portion of the Canadian Hansards."
Both corpora utilized sentence-level alignments publicly distributed by the Linguistic Data Consortium.
POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit[REF_CITE]; both were trained from the annotated Penn Treebank corpus[REF_CITE].
French POS tagging was done using the trained French lexical tagger also provided with the fnTBL soft-ware.
"For Arabic, we used a colleague’s POS tagger and tokenizer (clitic separation was also performed prior to POS tagging), which was rapidly developed in our lab-oratory."
"Simple regular-expression-based phrase chun-kers were developed by the authors for both Arabic and French, requiring less than a person-day each using ex-isting multilingual learning tools."
A further input to our system is a set of word alignment links on the parallel corpus.
These are used to compute word translation probabilities and phrasal alignments.
"The word alignments can in principle come from any source: a dictionary, a specialized alignment program, or another SMT system."
"We used alignments generated by Giza++[REF_CITE]by running it in both di-rections (e.g., Arabic → English and English → Arabic) on our parallel corpora."
"The union of these bidirectional alignments was used to compute cross-language phrase correspondences by simple majority voting, and for pur-poses of estimating word translation probabilities, each link in this union was treated as an independent instance of word translation."
Now we turn to a detailed description of the proposed translation model.
"The exposition will give a formal specification and also will follow a running example throughout, using one of the actual Arabic test set sen-tences."
"This example, its gloss, system translation and reference human translation are shown in Table 1."
"The translation model (TM) we describe is trained di-rectly from counts in the data, and is a direct model, not a noisy channel model."
"It consists of three nested com-ponents: (1) a sentence-level model of phrase correspon-dence and reordering, (2) a model of intra-phrase trans-lation, and (3) models of lexical transfer, or word transla-tion."
We make a key assumption in our construction that translation at each of these three levels is independent of the others.
"As mentioned, both the foreign language and English corpora are input with “hard” phrase bracketings and la-beled with “hard” phrase types (e.g., NP, VP 1 , PPNP [Footnote_2] , etc.) as given by the output of the phrase chunker."
2 PPNP consists of a NP with its prepositional head attached.
These are denoted in the top-level model presentation in Table 2(1).
"Given word alignment links, as described in Sec-tion 2, we compute phrasal alignments on training data."
We contrain these to have cardinality (foreign)N → [Footnote_1](English).
"1 VP in our parlance is perhaps more properly called a verb chunk: it consists of a verb, its auxiliaries, and contiguous adverbs."
"Next, we collect counts over aligned phrase sequences and use the relative frequen-cies to estimate the probability distribution in Table 2(2)."
"Particularly for smaller training corpora, unseen foreign-language phrase sequences are a problem, so we imple-mented a simple backoff method which assigns proba-bility to translations of unseen foreign-language phrase sequences."
"Table 2(3) encapsulates the remainder of the translation model, which is described below."
"As an example, Table 3 shows the most probable aligned English phrase sequence generations given an Arabic simple sentence having the canonical VSO or-dering."
"Also, note that all probabilities in the following figures and tables are from the actual Arabic and French trained systems."
"Given an Arabic test sentence, a distribution of aligned English phrase sequences is proposed by the sentence-level model described in the previous section and in Ta-ble 2."
"Each proposed English phrase in each of the phrase sequence possibilities, therefore, comes to the middle level of the translation model with access to the identity of the French phrases aligned to it."
Phrase translation is implemented as shown in Table 4.
"The phrase transla-tion model is structured with several levels of backoff: if no observations exist from training data for a particular level, the model backs off to the next-more-general level."
"In all cases, generation of an English phrase is condi-tioned on the foreign phrase as well as the type (NP, VP, etc.) of the English phrase."
Table 4 (1) describes the initial phrase translation model.
It comes into play if the precise sequence of foreign words has been observed aligning to an En-glish phrase of the appropriate type.
"In the example, we are trying to generate an NP given the Arabic word string “Al- ljnp Al- sAdsp” (literally: “the committee the sixth”)."
"If this has been observed in data, then that rela-tive frequency distribution serves as the translation prob-ability distribution."
"The next stage of backoff from the above, literal level is a model that generates aligned English POS tag se-quences given foreign POS tag sequences: details and an example can be found in Table 4(2)."
The sequence alignments determine the position in English phrase and the part-of-speech into which we translate the foreign word.
"Again, translation is also conditioned on the En-glish phrase type."
Table 5 and Table 6 show the most probable aligned English sequence generations for two of the phrases in the example sentence.
"If there were no counts for (foreign-POS-sequence, english-phrase-type) then we back off to counts collected over (foreign-coarse-POS-sequence, english-phrase-type), where a coarse POS is, for example, N in-stead of NOUN-SG."
This is shown in Table 4(3).
"In case further backoff is needed, as shown in Table 4(4), we begin stripping POS-tags off the “less signifi-cant” (non-head) end of the foreign POS-sequence until we are left with a phrase sequence that has been seen in training, and from this a corresponding English phrase distribution is observable."
"We define the “less signifi-cant” end of a phrase to be the end if it is head-initial, or the beginning if it is head-final, and at this point ig-nore issues such as nested structure in French and Arabic NP’s."
"In the basic model of word generation, phrases may be translated directly as single atomic entities (as in Table 4(1)), or via phrasal decomposition to individual words translated independently, conditioned only on the source word and target POS."
Word translation in the latter case
"Table 4: The phrase translation model, with backoff."
"Examples on the left side are from one of the Arabic test sentences. (1) is the direct, lexical translation level. (2) - (4) constitute the backoff path to handle detailed phenomena unseen in the training set. (2) is a model of fine POS-tag reordering and lexical generation; (3) is similar, but conditions generation on coarse POS-tag sequences in the foreign language. (4) is a model for progressively stripping off POS-tags from the “less significant” end of a foreign sequence."
"The idea is to do this until we reach a subsequence that has been seen in training data, and which we therefore have a distribution of valid generatons for."
The term Ξ i in (2) - (4) is a position alignment matrix.
"At all times, we generate not just an English POS-tag sequence, but rather an aligned sequence."
"Similarly, in the lexical transfer probabilities shown in this table, there is a function Ξ i () which takes an English sequence position index and returns the (unique) foreign word position to which it is aligned 4 . is done in the context that the model has already pro-posed a sequence of POS tags for the phrase."
Thus we know the English POS of the word we are trying to gen-erate in addition to the foreign word that is generating it.
"Consequently, we condition translation on English POS as well as the foreign word."
Table 7 describes the backoff path for basic lexical transfer and presents a motivating example in the French word droit.
Translation probabili-ties for one of the words in the example Arabic sentence can be found in Table 8.
"To counter sparse data problems in estimating word translation probabilities, we also implemented a lemma- based model for word translation."
"Under this model, translation distributions are estimated by counting word alignment links between foreign and English lemmas, as-suming a lemmatization of both sides of the parallel cor-pus as input."
The form of the model is illustrated below:
"First, note that P( lemma F | W F , T coarse F ) is very simply a hard lemma assignment by the foreign lan-guage lemmatizer."
"Second, English word generation from English lemma and coarse POS (P( W E | lemma E , T fine E )) is programmatic, and can be handled by means of rules in conjunction with a lookup table for irregular forms."
"The only distribution here that must be estimated from data is P( lemma E | lemma F , T coarse E )."
This is done as described above.
"Furthermore, given an electronic translation dictionary, even this distribution can be pre-loaded: indeed, we expect this to be an advantage of the lemma model, and an example of a good opportunity for integrating compiled human knowledge about language into an SMT system."
Some examples of the lemma model combating sparse data problems inherent in the basic word-to-word models can be found in Table 9.
Lexical coercion is a phenomenon that sometimes occurs when we condition translation of a foreign word on the word and the English part-of-speech.
"We find that the system we have described frequently learns this behav-ior: specifically, the model learns in some cases how to generate, for instance, a nominal form with similar meaning from a French adjective, or an adjectival real-ization of a French verb’s meaning; some examples of this phenomenon are shown[REF_CITE]."
We find this coercion effect to be of interest because it identifies in-teresting associations of meaning.
"For example,[REF_CITE]“willing” and “ready” are both sensible ways to re-alize the meaning of the action “to accept” in a passive, descriptive mode. droit behaves similarly."
"Though the English verb “to right’ or “to be righted” does not have the philosophical/judicial entitlement sense of the noun “right”, we see that the model has learned to realize the meaning in an active, verbal form: e.g., VBG ‘receiving” and VB “qualify”."
Decoding was implemented by constructing finite-state machines (FSMs) per evaluation sentence to encode relevant portions (for the individual sentence in ques-tion) of the component translation distributions described above.
Operations on these FSMs are performed using the AT&amp;T FSM Toolkit[REF_CITE].
The FSM constructed for a test sentence is subsequently composed with a FSM trigram language model created via the SRI Language Modeling Toolkit[REF_CITE].
Thus we use the trigram language model to implement rescoring of the (direct) translation probabilities for the English word sequences in the translation model lattice.
"We found that using the finite-state framework and the general-purpose AT&amp;T toolkit greatly facilitates decoder development by freeing the implementation from details of machine composition and best-path searching, etc."
P The structure of the translation model finite-state ma- 0SSSS_S__&gt;.6748/&lt;:s_&lt;s:&gt;.6657/[REF_CITE]_/:226.&lt;s6&gt;6__ P__:&lt;s_&gt;s&lt;/6546.:11S__6.711/&lt;s:&gt;6.3817P__&gt;/S___.8756/:&lt;s&gt; _ P_O___:&lt;.s6825/&gt;15S:&lt;s&gt;5.83518P__S_/_/&gt;_s&lt;_:[REF_CITE].3__P__:&lt;&gt;529.s/6S chines is as illustrated in Figure 1.
SS__:&lt;s&gt;./5940.942&gt;/6_:s&lt;_ (aligned phrase sequence generation) and phrase-level (aligned intra-phrase sequence generation) translation probabilities are encoded on epsilon arcs in the ma-chines.
"Word translation probabilities are placed onto arcs emitting the word as an output symbol (in the fig-ure, note the arcs emitting “committee”, “the”, etc.)."
The FSM in Figure 1 corresponds to the Arabic example sen-tence used throughout this paper.
"In the portion of the machine shown, the (best) path which generated the ex-ample sentence is drawn in bold."
"Finally, Figure 2 is a rendering of the actual FSM (aggressively pruned for display purposes) that generated the example Arabic sen-tence; although labels and details are not visible, it may provide a visual aid for better understanding the structure of the FSM lattices generated here."
"As a practical matter in decoding, during translation model FSM construction we modified arc costs for out-put words in the following way: a fixed bonus was as-signed for generating a “content” word translating to a “content” word."
Determining what qualifies as a con-tent word was done on the basis of a list of content POS tags for each language.
"For example, all types of nouns, verbs and adjectives were listed as content tags; deter-miners, prepositions, and most other closed-class parts of speech were not."
This implements a reasonable penalty on undesirable output sentence lengths.
"Without such a penalty, translation outputs tend to be very short: long sentence hypotheses are penalized de facto merely by containing many word translation probabilities."
"An ad-ditional trick in decoding is to use only the N-best trans-lation options for sentence-level, phrase-level, and word-level translation."
We found empirically (and very consis-tently) in dev-test experiments that restricting the syntac-tic transductions to a 30-best list and word translations to a 15-best list had no negative impact on Bleu score.
"The benefit, of course, is that the translation lattices are dra-matically reduced in size, speeding up composition and search operations."
Results Tables A and B below list evaluation results for translation on the Arabic and French test sets respec-tively.
"In each case, results for a comparison system – the Giza++ IBM Model 4 implementati[REF_CITE]with the ReWrite decoder[REF_CITE]– are included as a benchmark."
Results were gen-erated for training corpora of varying sizes.
"For Arabic, we ran our system on two large subsets of the UN cor-pus and evaluated on a 200-sentence held-out set (refer to Results Table A below)."
"We were unable to obtain evaluation numbers for Giza++/ReWrite on the large Arabic training set, however, since its language model component has a vocabulary size limit which was exceeded in the larger corpus."
In French we observed the systems to perform similarly on the small training sets we used (Results Table B).
"We performed some exper-iments in classifier combination using the two compat-ible (150K-training-sentence) Arabic systems, wherein a small devtest set was used to identify simple system combination parameters based on model confidence and sentence length."
"In situations where our system was con-fident we used its output, and used Giza++ output other-wise."
"We achieved a 3% boost in Bleu score over Giza++ performance on the evaluation set with these very sim-ple classifier combination techniques, and anticipate that research in this direction – classifier combination of di-versely trained SMT systems – could yield significant performance improvements."
"Results Table A: Results comparison for Arabic to English translation on the UN corpus, with a 200-sentence evaluation set."
"Note that Giza++/ReWrite cannot be run for the 500K sentence training set; the CMU Language Modeling Toolkit, which ReWrite uses, has a vocabulary size limit which is exceeded in the 500K corpus."
We have described and implemented an original syntax-based statistical translation model that yields baseline re-sults which compete successfully with other state-of-the-art SMT models.
This is particularly encouraging in that the authors are not well-versed in Arabic or French and it appears that the quality of the rule-based phrase chun-kers we developed in a single person-day offers substan-tial room for improvement.
"We expect to be able to at-tain improved bracketings from native speakers and, in addition, via translingual projection of existing brack-eters."
"Secondly, the lemma model we have proposed for lexical transfer provides an efficient framework for in-tegrating electronic dictionaries into SMT models."
"Al-though we have at this time no large electronic dictionar-ies for either Arabic or French, efforts are underway to acquire electronic or scanned paper dictionaries for this purpose."
"We did evaluate the lemma models in isola-tion for French and Arabic without dictionary inclusion, but in each experiment the results did not differ signifi-cantly from the word-specific lexical transfer models, de-spite their substantially reduced dimensionality."
"We an-ticipate that the relatively seamless direct incorporation of dictionaries into the lemma-based models will be par-ticularly effective for translating low-density languages, which suffer from data sparseness in the face of limited parallel text."
"Finally, we incorporated lexical translation coercion models into this full SMT framework, the in-duction of which is a phenomenon of interest in its own right."
We propose new methods to take advan-tage of text in resource-rich languages to sharpen statistical language models in resource-deficient languages.
"We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihood-based adaptation scheme for combining a trigger model with an -gram model."
We describe the application of such lan-guage models for automatic speech recog-nition.
"By exploiting a side-corpus of con-temporaneous English news articles for adapting a static Chinese language model to transcribe Mandarin news stories, we demonstrate significant reductions in both perplexity and recognition errors."
"We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via cross-lingual information retrieval and machine translation, proposed elsewhere."
Statistical techniques have been remarkably suc-cessful in automatic speech recognition (ASR) and natural language processing (NLP) over the last two decades.
"This success, however, depends crucially on the availability of accurate and large amounts of suitably annotated training data and it is difficult to build a usable statistical model in their absence."
"Most of the success, therefore, has been witnessed in the so called resource-rich languages."
"More re-cently, there has been an increasing interest in lan-guages such as Mandarin and Arabic for ASR and NLP, and data resources are being created for them at considerable cost."
"The data-resource bottleneck, however, is likely to remain for a majority of the world’s languages in the foreseeable future."
Methods have been proposed to bootstrap acous-tic models for ASR in resource deficient languages by reusing acoustic models from resource-rich lan-guages[REF_CITE].
"Morphological analyzers, noun-phrase chun-kers, POS taggers, etc., have also been developed for resource deficient languages by exploiting trans-lated or parallel text[REF_CITE]."
"When transcribing a news story in a resource-deficient language, their core idea is to use the first pass output of a rudimentary ASR system as a query for CLIR, identify a contem-poraneous English document on that news topic, fol-lowed by MT to provide a rough translation which, even if not fluent, is adequate to update estimates of word frequencies and the LM vocabulary."
They re-port up to a 28% reduction in perplexity on Chinese text from the Hong Kong News corpus.
"In spite of their considerable success, some short-comings remain in the method used[REF_CITE]."
"Specifically, stochastic translation lexicons estimated using the IBM method[REF_CITE]from a fairly large sentence-aligned Chinese-English parallel corpus are used in their ap-proach — a considerable demand for a resource-deficient language."
"It is suggested that an easier-to-obtain document-aligned comparable corpus may suffice, but no results are reported."
"Furthermore, for each Mandarin news story, the single best match-ing English article obtained via CLIR is translated and used for priming the Chinese LM, no matter how good the CLIR similarity, nor are other well-matching English articles considered."
This issue clearly deserves further attention.
"Finally, ASR re-sults are not reported in their work, though their pro-posed solution is clearly motivated by an ASR task."
We address these three issues in this paper.
"Section 2 begins, for the sake of completeness, with a review of the cross-lingual story-specific LM proposed[REF_CITE]."
"A notion of cross-lingual lexical triggers is proposed in Sec-tion 3, which overcomes the need for a sentence-aligned parallel corpus for obtaining translation lex-icons."
"After a brief detour to describe topic-dependent LMs in Section 4, a description of the ASR task is provided in Section 5, and ASR results on Mandarin Broadcast News are presented in Sec-tion 6."
The issue of how many English articles to retrieve and translate into Chinese is resolved by a likelihood-based scheme proposed in Section 6.1.
"For the sake of illustration, consider the task of sharpening a Chinese language model for transcrib-ing Mandarin news stories by using a large corpus of contemporaneous English newswire text."
"Man-darin Chinese is, of course, not resource-deficient for language modeling — 100s of millions of words are available on-line."
"However, we choose it for our experiments partly because it is sufficiently different from English to pose a real challenge, and because the availability of large text corpora in fact permits us to simulate controlled resource deficiency."
"Let   denote the text of test sto-ries to be transcribed by an ASR system, and let    denote their corresponding or aligned English newswire articles."
Correspondence here does not imply that the English document  needs to be an exact translation of the Mandarin story .
"It is quite adequate, for instance, if the two stories re-port the same news event."
"This approach is expected to be helpful even when the English document is merely on the same general topic as the Mandarin story, although the closer the content of a pair of ar-ticles the better the proposed methods are likely to work."
Assume for the time being that a sufficiently good Chinese-English story alignment is given.
"Assume further that we have at our disposal a stochastic translation dictionary — a probabilistic model of the form   — which provides the Chinese translation ! of each English word &quot;$# , where and # respectively denote our Chi-nese and English vocabularies."
"Let &amp;%   denote the relative frequency of a word in the document , ()# , *(+-., + ."
"It seems plausible that, / ,   %  &lt;: ;&gt;=@&gt;?   (1) would be a good unigram model for the , -th Man-darin story ."
We use this cross-lingual unigram statistic to sharpen a statistical Chinese LM used for processing the test story .
One way to do this is via linear interpolation 2[REF_CITE]: 8QPSR&apos;= ?XPSR&apos;Y   6   \6 [  (2) ] 2[REF_CITE]^:&lt;&gt;; S= &gt;?
"A ^ _ &gt;*cb ] d(^Z 6  6\[ of the cross-lingual unigram model (1) with a static trigram ] model for Chinese, where the interpolation weight may be chosen off-line to maximize the likelihood of some held-out Mandarin stories."
"The improvement in (2) is expected from the fact that unlike the static text from which the Chinese trigram LM is estimated, is semantically close to and even the adjustment of unigram statistics, based on a stochastic translation model, may help."
"Figure 1 shows the data flow in this cross-lingual LM adaptation approach, where the output of the first pass of an ASR system is used by a CLIR sys-tem to find an English document , an MT system computes the statistic of (1), and the ASR system uses the LM of (2) in a second pass."
"To illustrate how one may obtain the English doc-ument to match a Mandarin story , let us assume that we also have a stochastic reverse-translation lexicon   ."
"One obtains from the first pass ASR output, cf."
"Figure 1, the relative fre-quency estimate (%   of Chinese words in  ,  , and uses the translation lexicon   to compute, / # ,  Ei&gt;   % ^&lt;: ;&gt;=S?&gt;  (3) an English bag-of-words representation of the Man-darin story as used in standard vector-based in-formation retrieval."
The document with the highest TF-IDF weighted cosine-similarity to is selected: %  v &gt; C  (rItlqs p sim ^89:&lt;;&gt;=S?&gt; a
Readers familiar with information retrieval litera-ture will recognize this to be the standard query-translation approach to CLIR.
"The translation lexicons   and   may be created out of an available electronic translation lexicon, with multiple translations of a word being treated as equally likely."
Stemming and other mor-phological analyses may be applied to increase the vocabulary-coverage of the translation lexicons.
"Alternately, they may also be obtained auto-matically from a parallel corpus of translated and sentence-aligned Chinese-English text using statisti-cal machine translation techniques, such as the pub-licly available GIZA++ tools[REF_CITE], as done[REF_CITE]."
"Unlike stan-dard MT systems, however, we apply the translation models to entire articles, one word at a time, to get a bag of translated words — cf. (1) and (3)."
"Finally, for truly resource deficient languages, one may obtain a translation lexicon via optical character recognition from a printed bilingual dictionary (cf.[REF_CITE])."
This task is arguably easier than obtaining a large LM training corpus.
It seems plausible that most of the information one gets from the cross-lingual unigram LM of (1) is in the form of the altered statistics of topic-specific Chinese words conveyed by the statistics of content-bearing English words in the matching story.
"The translation lexicon used for obtaining the informa-tion, however, is an expensive resource."
"Yet, if one were only interested in the conditional distribution of Chinese words given some English words, there is no reason to require translation as an intermedi-ate step."
"In a monolingual setting, the mutual infor-mation between lexical pairs co-occurring anywhere within a long “window” of each-other has been used to capture statistical dependencies not covered by -gram LMs[REF_CITE]."
We use this inspiration to propose the follow-ing notion of cross-lingual lexical triggers.
"In a monolingual setting, a pair of words  is considered a trigger-pair if, given a word-position in a sentence, the occurrence of x in any of the pre-ceding word-positions significantly alters the (con-ditional) probability that the following word in the sentence is z : x is said to trigger z ."
E.g. the occur-rence of either significantly increases the proba-bility of or subsequently in the sentence.
"The set of preceding word-positions is variably defined to in-clude all words from the beginning of the sentence, paragraph or document, or is limited to a fixed num- ber of preceding words, limited of course by the be-ginning of the sentence, paragraph or document."
"In the cross-lingual setting, we consider a pair of words   , {# and { , to be a trigger-pair if, given an English-Chinese pair of aligned docu-ments, the occurrence of in the English document significantly alters the (conditional) probability that the word appears in the Chinese document: is said to trigger ."
It is plausible that translation-pairs will be natural candidates for trigger-pairs.
"It is, however, not necessary for a trigger-pair to also be a translation-pair."
"E.g., the occurrence of Belgrade in the English document may trigger the Chinese transliterations of Serbia and Kosovo, and pos-sibly the translations of China, embassy and bomb!"
"By infering trigger-pairs from a document-aligned corpus of Chinese-English articles, we ex-pect to be able to discover semantically- or topically-related pairs in addition to translation equivalences."
"Average mutual information, which measures how much knowing the value of one random variable reduces the uncertainty of about another, has been used to identify trigger-pairs."
We compute the av-erage mutual information for every English-Chinese word pair   as follows.
"Let | ~} , ,C*  , now be a document-aligned training corpus of English-Chinese article pairs."
"Let   denote the document frequency, i.e., the number of aligned article-pairs, in which occurs in the English article and in the Chinese."
Let \  denote the number of aligned article-pairs in which occurs in the English articles but does not occur in the Chinese article.
Let (    lw\(  \ 
The quantities (   and (  are similarly de-fined.
"Next let  denote the number of English articles in which occurs, and define ( \ and (  (("
"Similarly define &amp;  , &amp; via the document fre-quency     ; define &amp; via the document frequency  , etc."
"Finally, let iQ"
GX iI GX    ( 3 i $_ ( 3@@ id iI G iI G (  33 i _$(   33@@i  _
We propose to select word pairs with high mutual information as cross-lingual lexical triggers.
There are #  possible English-Chinese word pairs which may be prohibitively large to search for the pairs with the highest mutual information.
"We filter out infrequent words in each language, say,  words appearing less than 5 times, then mea-sure   for all possible  pairs from the remaining words, sort them by   , and select, say, the top 1 million pairs."
"Once we have chosen a set of trigger-pairs, the next step is to estimate a probability U= &lt;: ;  in lieu of the translation probability   in (1), and a probability w=@: ;   in (3)."
"Following the maximum likelihood approach pro-posed[REF_CITE], one could choose the trigger probability @= :&lt;;  to be based on the unigram frequency of among Chinese word tokens in that subset of aligned documents which have in , namely   r s  G ^¡r   =U&lt;: ;   id¢H j   r s  G (4) rI¡ ^"
"As an ad hoc alternative to (4), we also use    w=U&lt;: ;         (5)  where we set   whenever   is not a trigger-pair, and find it to be somewhat more effec-tive (cf. Section 6.2)."
Thus (5) is used henceforth in this paper.
"Analogous to (1), we set"
"U= :&lt;; 657989:&lt;&gt;; =@?&gt;  (6)  GQE HqJ @= : ;  % and, again, we build the interpolated model  =U&lt;: ; 65: 8QPSR&apos;@= T VXW&lt;XP? @R&apos;Y ^  6w ^Z 6\[  (7) ] w=U&lt;: ; 657989:&lt;&gt;; =@&gt;?"
A  _&gt;cb* ] d(  6  6\[
"The linear interpolation of the story-dependent un-igram models (1) and (6) with a story-independent trigram model, as described above, is very reminis-cent of monolingual topic-dependent language mod-els (cf. e.g.[REF_CITE])."
This moti-vates us to construct topic-dependent LMs and con-trast their performance with these models.
"To this end, we represent each Chinese article in the training corpus by a bag-of-words vector, and cluster the vectors using a standard K-means algo-rithm."
"We use random initialization to seed the al-gorithm, and a standard TF-IDF weighted cosine-similarity as the “metric” for clustering."
"We per-form a few iterations of the K-means algorithm, and deem the resulting clusters as representing differ-ent topics."
We then use a bag-of-words centroid created from all the articles in a cluster to repre-sent each topic.
"Topic-dependent trigram LMs, de-noted v   6  \6 [ , are also computed for each topic exclusively from the articles in the © -th cluster, Bª©+ &amp;{+ « . *"
"Each Mandarin test story is represented by a bag-of-words vector %  generated from the first-pass ASR output, and the topic-centroid ¬ having the highest TF-IDF weighted cosine-similarity to it is chosen as the topic of ."
Topic-dependent LMs are then constructed for each story  as &lt;: &gt;­ 65PS=U:&lt;&gt;; =S&gt;? ^  6 ^Z \6 [ &gt;¬  (8) ]   ^ 6 ^Z 6\[ _&gt;*.b ] d&amp;^ 6 ^Z 6\[ and used in a second pass of recognition.
Alternatives to topic-dependent LMs for exploit-ing long-range dependencies include cache LMs and monolingual lexical triggers; both unlikely to be as effective in the presence of significant ASR errors.
We investigate the use of the techniques described above for improving ASR performance on Man-darin news broadcasts using English newswire texts.
"We have chosen the experimental ASR setup cre-ated in the 2000 Johns Hopkins Summer Workshop to study Mandarin pronunciation modeling, exten-sive details about which are available[REF_CITE]."
"The acoustic training data ( ¯ 10 hours) for their ASR system was obtained from the 1997 Mandarin Broadcast News distribution, and context-dependent state-clustered models were estimated us-ing initials and finals as subword units."
Two Chinese text corpora and an English corpus are used to esti-mate LMs in our experiments.
"A vocabulary of 51K Chinese words, used in the ASR system, is also used to segment the training text."
This vocabulary gives an OOV rate of 5% on the test data.
"We use the Xinhua News corpus of about 13 million words to represent the scenario when the amount of available LM training text bor-ders on adequate, and estimate a baseline trigram LM for one set of experiments."
We also estimate a trigram model from only the 96K words in the transcriptions used for training acoustic models in our ASR system.
This corpus represents the scenario when little or no additional text is available to train LMs.
NAB-TDT: English text contemporaneous with the test data is often easily available.
"For our test set, described below, we select (from the North Ameri-can News Text corpus) articles published in 1997 in The Los Angeles Times and The Washington Post, and articles from 1998 in the New York Times and the Associated Press news service (from TDT-2 cor-pus)."
"This amounts to a collection of roughly 45,000 articles containing about 30-million words of En-glish text; a modest collection by CLIR standards."
"Our ASR test set is a subset[REF_CITE]of the[REF_CITE]and 1998 HUB-4NE bench-mark tests, containing Mandarin news broadcasts from three sources for a total of about 9800 words."
We generate two sets of lattices using the baseline acoustic models and bigram LMs estimated from XINHUA and HUB-4NE.
All our LMs are evaluated by rescoring  -best lists extracted from these two sets of lattices.
"The  -best lists from the XINHUA bigram LM are used in all XINHUA experiments, and those from the HUB-4NE bigram LM in all HUB-4NE experiments."
"We report both word error rates (WER) and character error rates (CER), the lat-ter being independent of any difference in segmenta-tion of the ASR output and reference transcriptions."
We begin by rescoring the  -best lists from the bigram lattices with trigram models.
"For each test story , we perform CLIR using the first pass ASR output to choose the most similar English docu-ment from NAB-TDT."
Then we create the cross- lingual unigram model ] of (1).
We also find the inter-polation weight which maximizes the likelihood of the 1-best hypotheses of all test utterances from the first ASR pass.
Table 1 shows the perplexity and WER for XINHUA and HUB-4NE.
"All ± -values reported in this paper are based on the standard NIST MAPSSWE test[REF_CITE], and indicate the statistical significance of a WER improvement over the corresponding trigram baseline, unless otherwise specified."
"Evidently, the improvement brought by CL-interpolated LM is not statistically significant on XINHUA."
"On HUB-4NE however, where Chinese text is scarce, the CL-interpolated LM delivers con-siderable benefits via the large English corpus."
of Interpolation Weights and the Number of English Documents per Mandarin Story
"The experiments above naı̈vely used the one most similar English ] document for each Mandarin story, and a global in (2), no matter how similar the best matching English document is to a given Mandarin news story."
"Rather than choosing one most simi-lar English document from NAB-TDT, it stands to reason that choosing more than one English docu-ment may be helpful if many have a high similarity score, and perhaps not using even the best matching document may be fruitful if the match is sufficiently poor."
"It may ] also help to have a greater interpola-tion weight ] for stories with good matches, and a smaller for others."
"For experiments ] in this sub-section, we select a different for each test story, again based on maximizing the likelihood of the * -best output given a CL-Unigram model."
The other issue then is the choice and the number of English documents ³ to translate. -best documents: One could choose a predeter-mined number of the best matching English doc- uments for each Mandarin story.
"We experimented with values of * , * ¥ ,  ,  ,  and * ¥w¥ , and found that  gave us the best LM performance, but only marginally better than C·* as described above."
"Details are omitted, as they are uninteresting."
All documents above a similarity threshold: The argument against always taking a predetermined number of the best matching documents may be that it ignores the goodness of the match.
An alternative is to take all English documents whose similarity to a Mandarin story exceeds a certain predetermined threshold.
"As this threshold is lowered, starting from a high value, the order in which English documents are selected for a particular Mandarin story is the same as the order when choosing the -best docu-ments, but the number of documents selected now varies from story to story."
"It is possible that for some stories, even the best matching English doc-ument falls below the threshold at which other sto-ries have found more than one good match."
"We ex-perimented with various thresholds, and found that while a threshold of ¥N*¸ gives us the lowest per-plexity on the test set, the reduction is insignificant."
"This points to the need for a story-specific strategy for choosing the number of English documents, in-stead of a global threshold."
Likelihood-based selection of the number of English documents: Figure 2 shows the perplex-ity of the reference transcriptions of one typical test story under the LM (2) as a function of the number of English documents chosen for creating (1).
"For each choice of the number ] of English documents, the interpolation weight in (2) is chosen to max-imize the likelihood (also shown) of the first pass output."
This suggests that choosing the number of English documents to maximize the likelihood of the first pass ASR output is a good strategy.
"For each Mandarin test story, we choose the 1000-best-matching English documents and divide the dynamic range of their similarity scores evenly into 10 intervals."
"Next, we choose the documents in the top  -th of the range of similarity scores, not necessarily the top- * ¥w¥ documents ] , compute 2[REF_CITE]^89:&lt;;&gt;S= ?&gt;A  , determine the in (2) that max-imizes the likelihood of the first pass output of only the utterances in that story, and record this likeli- [ hood."
"We repeat this with documents in the top  -th of the range of similarity scores, the top º ¹ -th, etc., and obtain the likelihood as a function of the simi-larity threshold."
We choose the threshold that max-imizes the likelihood of the first pass output.
"Thus in (1), as well the number of English documents ] as the interpolation weight in (2), are chosen dy-namically for each Mandarin story to maximize the likelihood of the ASR output."
Table 2 shows ASR results for this likelihood-based story-specific adap-tation scheme.
Note that significant WER improvements are obtained from the CL-interpolated LM using likelihood-based story-specific adaptation even for the case of the XINHUA LM.
"Furthermore, the per-formance of the CL-interpolated LM is even better than the topic-dependent LM."
"This is remarkable, since the CL-interpolated LM is based on unigram statistics from English documents, while the topic-trigram LM is based on trigram statistics."
We be-lieve that the contemporaneous and story-specific nature of the English document leads to its rela-tively higher effectiveness.
"Our conjecture, that the contemporaneous cross-lingual statistics and static topic-trigram statistics are complementary, is sup-ported by the significant further improvement in WER obtained by the interpolation of the two LMs, as shown on the last line for XINHUA."
The significant gain in ASR performance in the resource deficient HUB-4NE case are obvious.
The small size of the HUB-4NE corpus makes topic-models ineffective.
"Once we select cross-lingual trigger-pairs as de-scribed in Section 3,   in (1) is replaced by U= &lt;: ;  of (5), and  in (3) by =@:&lt;;  ."
"Therefore, given a set of cross-lingual trigger-pairs, the trigger-based models are free from requiring a translation lexicon."
"Furthermore, a document-aligned comparable corpus is all that is required to construct the set of trigger-pairs."
We otherwise fol-low the same experimental procedure as above.
"As Table 2 shows, the trigger-based model (Trig-interpolated) performs only slightly worse than the CL-interpolated model."
One explanation for this degradation is that the CL-interpolated model is trained from the sentence-aligned corpus while the trigger-based model is from the document-aligned corpus.
"There are two steps which could be affected by this difference, one being CLIR and the other be-ing the translation of the ’s into Chinese."
"Some errors in CLIR may however be masked by our likelihood-based story-specific adaptation scheme, since it finds optimal retrieval settings, dynamically adjusting the number of English documents as well as the interpolation weight, even if CLIR performs somewhat suboptimally."
"Furthermore, a document-aligned corpus is much easier to build."
"Thus a much bigger and more reliable comparable corpus may be used, and eventually more accurate trigger-pairs will be acquired."
We note with some satisfaction that even simple trigger-pairs selected on the basis of mutual infor-mation are able to achieve perplexity and WER re-ductions comparable to a stochastic translation lex-icon: the smallest ± -value at which the difference between the WERs of the CL-interpolated LM and the Trig-interpolated LM in Table 2 would be signif-icant is ¥N&lt;» for XINHUA and ¥N½¼ for HUB-4NE.
Triggers (4) vs (5): We compare the alternative =U:&lt;; @ definitions (4) and (5) for replacing @ in (1).
"The resulting CL-interpolated LM (2) yields a perplexity of 370 on the XINHUA test set using (4), compared to 367 using (5)."
"Similarly, on the HUB-4NE test set, using (4) yields 736, while (5) yields 727."
"Therefore, (5) has been used throughout."
"We have demonstrated a statistically significant im-provement in ASR WER (1.4% absolute) and in perplexity (23%) by exploiting cross-lingual side-information even when nontrivial amount of train-ing data is available, as seen on the XINHUA cor-pus."
Our methods are even more effective when LM training text is hard to come by in the language of interest: 47% reduction in perplexity and 1.3% ab-solute in WER as seen on the HUB-4NE corpus.
Most of these gains come from the optimal choice of adaptation parameters.
"The ASR test data we used in our experiments is derived from a different source than the corpus on which the translation and trigger models are trained, and the techniques work even when the bilingual corpus is only document-aligned, which is a realistic reflection of the situation in a resource-deficient language."
"We are developing maximum entropy models to more effectively combine the multiple information sources we have used in our experiments, and expect to report the results in the near future."
"We address the problem of sentence align-ment for monolingual corpora, a phe-nomenon distinct from alignment in par-allel corpora."
Aligning large compara-ble corpora automatically would provide a valuable resource for learning of text-to-text rewriting rules.
We incorporate con-text into the search for an optimal align-ment in two complementary ways: learn-ing rules for matching paragraphs using topic structure and further refining the matching through local alignment to find good sentence pairs.
Evaluation shows that our alignment method outperforms state-of-the-art systems developed for the same task.
Text-to-text generation is an emerging area of re-search in NLP[REF_CITE].
"Unlike in traditional concept-to-text generation, text-to-text generation applica-tions take a text as input and transform it into a new text satisfying specific constraints, such as length in summarization or style in text simplification."
One exciting new research direction is the automatic in-duction of such transformation rules.
This is a par-ticularly promising direction given that there are nat-urally occurring examples of comparable texts that convey the same information yet are written in dif-ferent styles.
"Presented with two such texts, one can pair sentences that convey the same information, thereby building a training set of rewriting examples for the domain to which the texts belong."
"We believe that automating this process will provide researchers in text-to-text generation with valuable training and testing resources, just as techniques to align multi-lingual parallel corpora boosted research in Machine Translation (MT)."
"In this paper, we address the task of aligning sen-tences in text pairs."
"We focus on monolingual com-parable corpora, that is, texts in the same language (e.g., English) that overlap in the information they convey."
Stories about the same events from different press agencies and texts presenting the same infor-mation to experts and lay people are two examples.
"In MT, the task of sentence alignment was exten-sively studied for parallel corpora. [Footnote_1]"
1 Sentence alignment for comparable multilingual corpora was not addressed in previous research. Comparable cor-pora have primarily been used to build bilingual lexical re-sources[REF_CITE].
"A typical sen-tence alignment algorithm can be roughly described as a two-step process: (1) for each sentence pair compute a local similarity value, independently of the other sentences; (2) find an overall sequence of mapped sentences, using both the local similarity values and additional features."
"In the case of monolingual corpora, step (2) might seem unnecessary."
"Since the texts share the same language, it would be enough to choose for local similarity a function based on lexical cues only and select sentence pairs with high lexical similarity."
"Even a simple lexical function (e.g., one that counts word overlap) could produce an accurate alignment."
"After all, two sentences which share most of their words are likely to paraphrase each other."
The prob-lem is that there are many sentences that convey the same information but have little surface resem-blance.
"As a result, simple word counts cannot dis-tinguish the matching pair (A) in Figure 1 from the unrelated pair (B)."
An accurate local similarity mea-sure would have to account for many complex para-phrasing phenomena.
"A simple, weak lexical simi-larity function alone is not sufficient."
"In MT, a weak similarity function is compensated for by searching for a globally optimal alignment, using dynamic programming or taking advantage of the geometric/positional or contextual properties of the text pair[REF_CITE]."
But these techniques oper-ate on the assumptions that there are limited inser-tions and deletions between the texts and that the order of the information is roughly preserved from one text to another.
"Texts from comparable corpora, as opposed to parallel corpora, contain a great deal of “noise.”"
"In Figure 2 which plots the manually identified align-ment for a text pair in our corpus, only a small frac-tion of the sentences got aligned (35 out of 31 × 270 sentence pairs), which illustrates that there is no complete information overlap."
"Consider two texts written by different press agencies: while both re-port on the same events, one may contain additional interviews and the other, background information."
Another distinguishing characteristic of comparable corpora is that the order in which the information is presented can differ greatly from one text to an-other.
Analysis of comparable texts in different do-mains[REF_CITE]showed that there is wide variability in the order in which the same information can be presented.
This is also illustrated in Figure 2.
We investigate a novel approach informed by text structure for sentence alignment.
"Our method em-phasizes the search for an overall alignment, while relying on a simple local similarity function."
We incorporate context into the search process in two complementary ways: (1) we map large text frag-ments using hypotheses learned in a supervised fash-ion and (2) we further refine the match through lo-cal alignment within mapping fragments to find sen-tence pairs.
"When the documents in the collection belong to the same domain and genre, the fragment mapping takes advantage of the topical structure of the texts."
This structure is derived in an unsuper-vised fashion by analyzing commonalities among texts on each side of the comparable corpora sep-arately.
Our experiments show that our overall ap-proach identifies even pairs with low lexical sim-ilarity.
"We also found that a fully unsupervised method using a minimalist representation of contex-tual information, viz., paragraph-level lexical simi-larity, outperforms existing methods based on com-plex local similarity functions."
"In the next section, we provide an overview of existing work on monolingual sentence alignment."
Section 3 describes our algorithm.
"In sections 4 and 5, we report on data collection and evaluation."
Most of the work in monolingual corpus alignment is in the context of summarization.
"In single docu-ment summarization, alignment between full docu- ments and summaries written by humans is used to learn rules for text compression."
"However, both of these methods construct an alignment by looking at sentences one at a time, independently of the de-cisions made about other sentences."
"Because sum-maries often reuse original document text to a large extent, these methods achieve good results."
"In the context of multidocument summarization, SimFinder[REF_CITE]identifies sentences that convey similar information across in-put documents to select the summary content."
"Even though the input documents are about the same sub-ject, they exhibit a great deal of lexical variability."
"To address this issue, SimFinder employs a com-plex similarity function, combining features that ex-tend beyond a simple word count and include noun phrase, proper noun, and WordNet sense overlap."
"Since many documents are processed in parallel, clustering is used to combine pairwise alignments."
"In contrast to our approach, SimFinder does not take the context around sentences into account."
"Given a comparable corpus consisting of two collec-tions and a training set of manually aligned text pairs from the corpus, the algorithm follows four main steps."
Steps 1 and 2 take place at training time.
"Steps 3 and 4 are carried out when a new text pair (T ext 1 , T ext 2 ) is to be aligned. 1."
"Topical structure induction: by analyzing multiple instances of paragraphs within the texts of each collection, the topics characteris-tic of the collections are identified through clus-tering."
Each paragraph in the training set gets assigned the topic it verbalizes (Section 3.1.1.) [Footnote_2].
2 Texts without adequate paragraph marking could be seg-mented using tools such as TextTiling[REF_CITE].
"Learning of structural mapping rules: us-ing the training set, rules for mapping para-graphs are learned in a supervised fashion (Sec-tion 3.1.2). [Footnote_3]. Macro alignment: given a new unseen pair (Text 1 , Text 2 ), each paragraph is automati- cally assigned its topic."
"3 We use the term topic differently than it is commonly used in the topic detection task– there, a “topic” would designate which disease is described."
"Paragraphs are mapped following the learned rules (Section 3.2). 4. Micro alignment: for each mapped paragraph pair, a local alignment is computed at the sen-tence level."
The final alignment for the text pair is the union of all the aligned sentence pairs (Section 3.3).
"Given two sentences with moderate lexical similar-ity, we may not have enough evidence to decide ac-curately whether they should be aligned."
"Looking at the broader context they appear in can provide addi-tional insight: if the types of information expressed in the contexts are similar, then the specific infor-mation expressed in the sentences is more likely to be the same."
"On the other hand, if the types of in-formation in the two contexts are unrelated, chances are that the sentences should not be aligned."
"In our implementation, context is represented by the para-graphs to which the sentences belong. 2"
"Our goal in this phase is to learn rules for determining whether two paragraphs are likely to contain sentences that should be aligned, or whether, on the contrary, two paragraphs are unrelated and, therefore, should not be considered for further processing."
A potentially fruitful way to do so is to take ad-vantage of the topical structure of texts.
"In a given domain and genre, while the texts relate different subjects, they all use a limited set of topics to con-vey information; these topics are also known as the Domain Communication Knowledge[REF_CITE]."
"For instance, most texts describing dis-eases will have topics such as “symptoms” or “treat-ment.” 3 If the task is to align a disease description written for physicians and a text describing the same disease for lay people, it is most likely that sentences within the topic “symptoms” in the expert version will map to sentences describing the symptoms in the lay version rather than those describing treat-ment options."
"If we can automatically identify the topic each paragraph conveys, we can decide more accurately whether two paragraphs are related and should be mapped for further processing."
"In the field of text generation, methods for representing the semantic structure of texts have been investigated through text schemata[REF_CITE]or rhetorical structures[REF_CITE]."
"In our framework, we want to identify the different topics of the text, but we are not concerned with the relations holding between them or the order in which they typically appear."
"We propose to iden-tify the topics typical to each collection in the com-parable corpus by using clustering, such that each cluster represents a topic in the collection."
"The process of learning paragraph mapping rules is accomplished in two stages: first, we identify the topics of each collection, Corpus 1 and Corpus 2 , and label each paragraph with its specific topic."
"Sec-ond, using a training set of manually aligned text pairs, we learn rules for mapping paragraphs from Corpus 1 to Corpus 2 ."
Two paragraphs are consid-ered mapped if they are likely to contain sentences that should be aligned.
We perform a clustering at the paragraph level for each collection.
"We call this stage Vertical Clus-tering because all the paragraphs of all the doc-uments in Corpus 1 get clustered, independently of Corpus 2 ; the same goes for the paragraphs in Corpus 2 ."
"At this stage, we are only interested in identifying the topics of the texts in each collection, each cluster representing a topic."
We apply a hierarchical complete link clustering.
"Similarity is a simple cosine measure based on the word overlap of the paragraphs, ignoring function words."
"Since we want to group together paragraphs that convey the same type of information across the documents in the same collection, we replace all the text-specific attributes, such as proper names, dates and numbers, by generic tags. [Footnote_4] This way, we ensure that two paragraphs are clustered not because they relate the same specific information, but rather, be-cause they convey the same type of information (an example of two automatically clustered paragraphs is shown in Figure 3)."
"4 We crudely consider any words with a capital letter a proper name, except for each sentence’s first word."
The number of clusters for each collection is a parameter tuned on our training set (see Section 4).
"Once the different topics, or clusters, are identi-fied inside each collection, we can use this informa-tion to learn rules for paragraph mapping (Horizon-tal Mapping between texts from Corpus 1 and texts from Corpus 2 )."
"Using a training set of text pairs, manually aligned at the sentence level, we consider two paragraphs to map each other if they contain at least one aligned sentence pair (see Figure 4)."
"Our problem can be framed as a classification task: given training instances of paragraph pairs (P , Q) from a text pair, classify them as mapping or not."
"The features for the classification are the lexical sim-ilarity of P and Q, the cluster number of P , and the cluster number of Q. Here, similarity is again a sim-ple cosine measure based on the word overlap of the two paragraphs. 5 These features are weak indicators by themselves."
"Consequently, we use the publicly-available classification tool BoosTexter[REF_CITE]to combine them accurately. [Footnote_6]"
"6 Because BoosTexter cannot form conjunctive hypotheses, we add a feature which encodes the combination of two cluster numbers."
"At this stage, the clustering and training are com-pleted."
"Given a new unseen text pair (Text 1 , T ext 2 ), the goal is to find a sentence alignment be-tween them."
Two sentences with very high lexical similarity are likely to be aligned.
We allow such pairs in the alignment independently of their context.
This step allows us to catch the “easy” paraphrases.
We focus next on how our algorithm identifies the less obvious matching sentence pairs.
"For each paragraph in each text, we identify the cluster in its collection it is the closest to."
Similarity between the paragraph and each cluster is computed the same way as in the Vertical Clustering step.
We then apply mapping classification to find the map-ping paragraphs in the text pair (see Figure [Footnote_5]).
"5 At this stage, we want to match on text-specific informa-tion, unlike in the Vertical Clustering. We therefore use the original text, without any substitution, to compute the similarity."
"Once the paragraph pairs are identified in (Text 1 , Text 2 ), we want to find, for each paragraph pair, the (possibly empty) subsets of sentence pairs which constitute a good alignment."
"Context is used in the following way: given two sentences with moder-ate similarity, their proximity to sentence pairs with high similarity can help us decide whether to align them or not."
"To combine the lexical similarity (again using co-sine measure) and the proximity feature, we com- pute local alignments on each paragraph pair, us-ing dynamic programming."
The local alignment we construct fits the characteristics of the data we are considering.
"In particular, we adapt it to our frame-work to allow many-to-many alignments and some flips of order among aligned sentences."
"Given sen-tences i and j, their local similarity sim(i, j) is: sim(i, j) = cos(i, j) − mismatch penalty"
"The weight s(i, j) of the optimal alignment between the two sentences is computed as follows: s(i, j−1) − skip penalty  ( − skip penalty  s i−1, j−1) s(i, j) = max  ss((i−1i−1,, jj ) + sim(i, j)−2) + sim(i, j) + sim(i, j−1)  s(i−2, j−2) + sim(i, j−1) + sim(i−1, j)  s(i−2, j−1) + sim(i, j) + sim(i−1, j)"
"The mismatch penalty penalizes sentence pairs with very low similarity measure, while the skip penalty prevents only sentence pairs with high similarity from getting aligned."
We compiled two collections from the Encyclopedia Britannica and Britannica Elementary.
"In contrast to the long (up to 15-page) detailed arti-cles of the Encyclopedia Britannica, Britannica Ele-mentary contains one- to two-page entries targeted towards children."
"The elementary version gener-ally contains a subset of the information presented in the comprehensive version, but there are numerous cases when the elementary entry contains additional or more up-to-date pieces of information. [Footnote_7] The two collections together exhibit many instances of com-plex rewriting."
"7 Britannica Elementary is a new feature of the encyclopedia, not all entries in the original Britannica have been fully updated."
We set aside a testing set of 11 text pairs.
The rest (92 pairs) was used for the Vertical Clustering.
Nine text pairs were used for training (see Table 1 for statistics).
Each text pair in the train-ing and testing sets was annotated by two annota-tors. [Footnote_8]
8 All the annotators were native speakers of English. The authors did not take part in the annotation.
"In our guidelines to them, we defined two sen-tences as aligned if they contain at least one clause that expresses the same information."
We allowed many-to-many alignments.
"On average, each anno-tator spent 50 minutes per text pair."
"While the an-notators agreed for most of the sentence pairs they identified, there were some cases of disagreement."
"Alignment is a tedious task, and sentence pairs can easily be missed even by a careful human annotator."
"For each text pair, a third annotator went through contested sentence pairs, deciding on a case-by-case basis whether to include it in the alignment."
"The other sentence pairs which were not aligned served as negative ex-amples, yielding a total of 4192 training instances and 3884 testing instances. [Footnote_9]"
9 Our corpus is available[URL_CITE]
"As a confirmation that there is no order preserva-tion in comparable corpora, there were up to nine or-der shifts in each of the annotated text pairs."
Table 2 shows that a large fraction of manually aligned sen-tence pairs have low lexical similarity.
"Similarity is measured here by the number of words in common, normalized by the number of types in the shorter sentence."
"We tuned all the parameters on our training set, obtaining the following values: the skip penalty is 0.001, and the cosine threshold for selecting pairs with high lexical similarity is 0.5."
BoosTexter was trained in 200 iterations.
"To find the optimal number of clusters for each collection, Ver-tical Clustering was performed with different num-bers of clusters, ranging from 10 to 40; we selected the alternatives with the best performance on the training set: 20 for both collections."
We first present the comparison of our method with other systems developed for the same task.
Next we focus on the impact of individual components on the performance of our method.
An obvious choice for a baseline in this task is the following: any two sentences are considered aligned if their cosine similarity exceeds a certain threshold.
"We also compare our algorithm with two state-of-the-art systems, SimFinder[REF_CITE]and Decompositi[REF_CITE]. [Footnote_10] Figure 6 shows Precision/Recall curves for the different sys-tems."
"10[REF_CITE]reports that Decomposition outperforms the al-gorithm[REF_CITE]; we, therefore, did not compare our method against his system."
"For our full system, we obtain different values of Recall keeping constant the skip penalty and the cosine threshold and varying the value of the mis-match penalty from 0 to 0.45. [Footnote_11]"
"11 Varying the mismatch penalty is a natural choice: varying the skip penalty produces a narrow range of Recall values, while the cosine threshold controls only a small portion of the sen-tence pairs that can be identified (the ones with high similarity)."
This setup results in recall values in a 25%–65% range.
"The curve for SimFinder was obtained by running SimFinder with different similarity thresholds, ranging from 0.1 to 0.95."
"In the case of Decomposition, there are sev-eral hard-coded parameters which are not trainable."
"As a result, we were able to obtain results for De-composition only at a 55.8% Recall level."
Table 3 reports Precision values at this level of Recall.
Our full method outperforms both the base-line (“Cosine”) and the more complex systems (“SimFinder” and “Decomposition”).
"Interestingly, methods that use simple local similarity functions significantly outperform SimFinder (SimFinder was trained on newswire texts; we did not have access to SimFinder’s training component for retraining on our corpus)."
"This confirms our hypothesis that while it is an appealing idea, putting all one’s eggs in the basket of a sophisticated local similarity measure to achieve good performance may be too hard a task."
"The simple cosine baseline is competitive with the Hidden Markov Model of Decomposition (Decom-position was specifically developed to identify sen-tence pairs with cut-and-paste transformations, not all possible paraphrase pairs)."
"This suggests that when looking for an alignment, Cosine is a good, yet simple, starting local similarity measure."
"Adding on top of it an explicit search mechanism relying on the context surrounding the sentences, as in our method, results in a performance improvement of 19% at 55.8% Recall."
Figure 7 shows examples of pairs identified by our method.
Impact of Horizontal Paragraph Mapping.
"We hypothesize that exploiting the regularity in map-ping between semantic units, such as topics, im-proves the alignment."
We compared the perfor-mance of our full method with a variation that does not take any topical information into account.
"For the paragraph mapping, we replaced the learned rules by a single rule based on lexical similarity: two paragraphs are mapped if their cosine measure is above a pre-specified threshold. [Footnote_12] This new map-ping is a good point of comparison because it does not rely on any knowledge inferred from the other texts in the corpus."
12 The threshold was tuned on our training data.
"The results confirm our hypoth-esis: learning paragraph mapping based on topical structures improves the performance (see “Without Topic Mapping” vs. “Full Method”, Table 3)."
"This experiment also shows that representing con-text, even simply using only the paragraphs and their lexical similarity, achieves higher performance than methods based on more complex local similarity functions."
"It is an important finding, because this simplified method can be used when topic struc-ture cannot be derived (e.g., in heterogeneous col-lections) or when no training data is available, since it is unsupervised."
Impact of Cluster Quality.
Our method uses clustering to identify the different topics of each collection.
It is important to know how sensitive our overall algorithm is to the quality of the iden-tified clusters.
"Fortunately, in our corpus, some of the texts contain section headings (e.g., “Climate” or “Demography”)."
"Even though our method ig-nores this piece of information, we used it to derive manually “ideal” clusters. [Footnote_13]"
"13 The process was performed manually because the sections are different from one text to another, both in names and levels of detail, and because we needed to infer clusters for the para-graphs that did not have section headings."
We obtained eight clus-ters for the elementary version and 11 for the com-prehensive one.
"When feeding these ideal clusters instead of the automatically identified ones to the learning module for paragraph mapping, we achieve 4% improvement in Precision (at 55.8% Recall)."
"We interpret this as a sign that the algorithm handles im-perfect, automatically induced clusters fairly well."
Impact of Local Alignment.
"Our hypothesis for computing local alignments between pairs of mapped paragraphs is that our approach allows us to identify additional matching sentence pairs: if two sentences paraphrase each other but have a low co-sine measure, looking at the sentence pairs around them may increase their chances of getting selected."
We compared our full method with a version of our algorithm that does not perform local alignment (“Without Local Alignment”).
"Instead, it simply se-lects sentence pairs inside the mapped paragraphs based on their cosine measure."
"This incomplete ver-sion of the algorithm achieves 73.3% Precision (at 55.8% Recall), 3% lower than the full method, vali-dating our hypothesis."
Impact of Lexical Similarity.
We investigated how the performance of our method depends on the lexical similarity of the input sentences.
Table 4 shows Precision and Recall for our method and oth-ers at three sentence-similarity ranges based on word overlap counts (at the overall[REF_CITE].8%).
Our method outperforms the Cosine baseline and De-composition on all similarity ranges.
The central finding of our work is that context plays an important role in the task of sentence alignment for monolingual comparable corpora.
A weak sen-tence similarity measure combined with contextual information outperforms methods based on sophis-ticated sentence similarity functions.
Experiments show that a simple representation of context is help-ful.
"Relying on a more elaborate representation, such as topical text structure, has an even stronger impact on performance."
This paper explores the problem of find-ing non-local dependencies.
"First, we isolate a set of features useful for this task."
"Second, we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which finds non-local dependencies while parsing."
We find that the former outperforms the latter be-cause it makes better use of the features we isolate.
Many broad-coverage statistical parsers[REF_CITE]are not able to give a full interpretation for sentences such as: (1) It is difficult to guess what she wants to buy.
Building the semantic interpretation of this sentence requires recovering three non-local relations: (i) the object of buy is what ; [Footnote_1] (ii) the subject of buy is she ; and (iii) guess does not have a subject in the sen-tence.
1[REF_CITE]can handle this case (Model 3).
Three approaches have been proposed to de-tect such relations: (i) post-processing the output of a parser not designed to detect extraction sites[REF_CITE]; (ii) integrating antecedent recov-ery into the parser (henceforth in-processing) by ei-ther enriching a syntactically simple model[REF_CITE]or using a more powerful syntactic framework[REF_CITE]; and (iii) de-tecting non-local dependencies as a pre-processing step before parsing[REF_CITE].
"While the pre-processing approach is reported to give state-of-the-art performance using unlexi-calized parsers, it has not been tested using lexi-calized models."
"Our main claim is that that the pre-processing approach, coupled with a lexical-ized parser outperforms both state-of-the-art post-processing and in-processing."
"However, we show that Model 3[REF_CITE]can be generalized to handle all types of long-distance dependencies with performance close to the pre-processing archi-tecture."
A general contribution of this paper is that it gives important insights about the nature of the problem.
Recovering non-local semantic relations is regarded to be a difficult problem.
The successes (and fail-ures) of the simple architecture outlined here help determine what features are to be incorporated into a parser in order to improve recovery of non-local dependencies.
The overall organization of the paper is as fol-lows.
"First, Section 2 sketches the material we use for the experiments in the paper."
"In Section 3, we discuss a finite-state system, a trace tagger, that de-tects extraction sites without knowledge of phrase-structure and we isolate important cues for the task."
Section 4 combines the trace tagger with a parser in order to recover antecedents.
"Finally, in Section 5, we investigate whether and how detection of extrac-tion sites and antecedent recovery can be integrated into a lexicalized stochastic parser."
"In the experiments we use the same train-ing, test, and development data as[REF_CITE], where non-local de-pendencies are annotated with the help of empty elements ( EE s) co-indexed with their controlling constituents (if any)."
The most frequent types of EE s are summarized in Table 1.
"Thus, the example sentence (1) will get the annotation: (2) It is difficult PRO - NP to guess what she wants NP - NP to buy WH - NP ."
"For the parsing and antecedent recovery exper-iments, in the case of WH -traces ( WH –  ) and controlled NP -traces ( NP – NP ), we follow the stan-dard technique of marking nodes dominating the empty element up to but not including the par-ent of the antecedent as defective (missing an ar-gument) with a gap feature[REF_CITE]."
"Furthermore, to make antecedent co-indexation possible with many types of EE s, we generalize Collins’ approach by enriching the an-notation of non-terminals with the type of the EE in question (eg."
"WH – NP ), using different gap+ fea-tures (gap+WH-NP; c.f. Figure 1)."
The original non-terminals augmented with gap+ features serve as new non-terminal labels.
"Note, however, that not all EE s have antecedents."
"In these cases, the gap+ feature does not show up in the dominating non-terminal (Figure 2)."
"Previous work[REF_CITE]shows that detecting empty elements can be performed fairly reliably before parsing using a trace tagger, which tags words with information on EE s immediately preceding them."
"For example, the first occurrence of the word to in our example sentence (2) gets the tag EE = TT - NP , whereas the word wants is tagged as having no EE ."
The trace tagger uses three main types of features: (i) combination of POS tags in a win-dow of five words around the EE s; (ii) lexical fea-tures of the words in a window of three lexical items; and (iii) long-distance cues (Table 2).
An EE is cor-rectly detected if and only if (i) the label matches that of the gold standard and (ii) it occurs between the same words.
This can be achieved by testing how the model fares if only a subset of the features are switched on (performance analysis).
Another way to investigate the problem is to analyze the average weight and the activation frequency of each feature type.
"According to the performance analysis, the most important features are the ones encoding POS-information."
"Indeed, by turning only these features on, the accuracy of the system is already fairly high: the labeled F-score is 71 2%."
A closer look at the feature weights shows that the right context is slightly more informative than the left one.
"Lex-icalization of the model contributes further 6% to the overall score (the following word being slightly more important than the preceding one), whereas the features capturing long-distance cues only im-prove the overall score by around 2%."
"Interestingly, long-distance features get higher weights in general, but their contribution to the overall performance is small since they are rarely activated."
"Finally, the model with only lexical features performs surpris-ingly well: the labeled F-score is 68 9%, showing that a very small window already contains valuable information for the task."
"In summary, the most important result here is that a relatively small window of up to five words con-tains important cues for detecting EE s."
"Antecedent recovery requires knowledge of phrase structure, and hence calls for a parsing component."
"In this section, we show how to recover the an-tecedents given a parse tree, and how to incorporate information about EE -sites into the parser."
The main motivation for the introduction of gap+ variables is that they indicate a path from the EE to the antecedent.
"In case of a non-binary-branching grammar, however, this path only determines the node immediately dominating the antecedent, but does not indicate the child the EE should be co-indexed with."
"Moreover, a node might contain sev-eral gap+ variables, which further complicates an-tecedent recovery, even in the case of perfect trees."
This calls for a sophisticated algorithm to recover antecedents.
"The algorithm, presented in Figure 3, runs af-ter the best parse has been selected."
"It works in a bottom-up fashion, and for each empty node the main recursive function find antecedent is called separately (lines 1 and 2)."
"At every call, the number of gap+ variables of type “gap” are calculated for the parent par of the current node node (p; line 6) and for all the children (ch; line 7)."
"If the parent has at least as many unresolved gap+ variables as its children, we conclude that the current EE is re- solved further up in the tree and call the same al-gorithm for the parent (line 20)."
"If, however, the parent has fewer unresolved gaps (p ch), the an-tecedent of the EE is among the children."
Thus the algorithm attempts to find this antecedent (lines 11– 18).
"For an antecedent to be selected, the syntactic category must match, i.e. an NP – NP must resolve to a NP ."
"The algorithm searches from left to right for a possible candidate, preferring non-adjuncts over ad-juncts."
The node found (if any) is returned as the antecedent for the EE .
"Finally, note that in line 9, we have to remove the threaded gap+ feature in order to avoid confusion if the same parent is visited again while resolving another EE ."
"Although the algorithm is simple and works in a greedy manner, it does perform well."
"Tested on the gold standard trees containing the empty nodes with-out antecedent co-reference information, it is able to recover the antecedents with an F-score of 95% (c.f. Section 4.3)."
Antecedent recovery is tested using two parsers: an unlexicalized PCFG[REF_CITE]and a lexicalized parser with near state-of-the-art perfor-mance[REF_CITE].
Both parsers treat EE s as words.
"In order to recover antecedents, both were modified to thread gap+ variables in the nontermi-nals as described in Section 2."
Each parser is evaluated in two cases: (i) an upper bound case which uses the perfect EE s of the tree-bank (henceforth PERFECT ) and (ii) a case that uses EE s suggested by the finite-state mechanism (hence-forth TAGGER ).
"In the TAGGER case, the parser sim-ply takes the hypotheses of the finite-state mecha-nism as true."
We evaluate on all sentences in the test section of the treebank.
"As with trace detection, we use the measure introduced[REF_CITE]."
"This metric works by treating EE s and their antecedents as four-tuples, consisting of the type of the EE , its location, the type of its antecedent and the location(s) (begin-ning and end) of the antecedent."
An antecedent is correctly recovered if all four values match the gold standard.
"We calculate the precision, recall, and F-score; however for brevity’s sake we only report the F-score for most experiments in this section."
"In addition to antecedent recovery, we also re-port parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall[REF_CITE]."
The results of the experiments are summarized in Table 3.
"UNLEX and LEX refer to the unlexicalized and lexicalized models, respectively."
"In the upper-bound case, PERFECT , the F-score for antecedent recovery is quite high in both the unlexicalized and lexicalized cases: 91.4% and 93.3%."
"To test how well the antecedent-detection algorithm works, it is useful, however, to count the results of only those EE s which have antecedents in the tree ( NP – NP , PSEUDO attachments, and all WH traces)."
"In these cases, the unlexicalized parser has an F-score of 70.4%, and the lexicalized parser 83.9%, both in the PERFECT case."
"In the TAGGER case, which is our main con-cern, the unlexicalized parser achieves an F-score of 72.6%, better than the 68.0% reported[REF_CITE]."
"The lexicalized parser outperforms both, yielding results of F-score of 74.6%."
"Table 4 gives a closer look at the antecedent recovery score for some common EE types using the lexicalized parser, also showing the results[REF_CITE]for comparison."
"The pre-processing system does quite well, manag-ing an F-score 6.6% higher than the post-processing system[REF_CITE]."
"However, while the lexi-calized parser performs better than the unlexicalized one, the difference is quite small: only 2%."
This suggests that many of the remaining errors are actu-ally in the pre-processor rather than in the parser.
Two particular cases of interest are NP – NP s and PRO – NP s.
"In both cases, a NP is missing, often in a to-infinitival clause."
"The two are only distinguished by their antecedent: NP – NP has an antecedent in the tree, while PRO – NP has none."
"The lexicalized parser has, for most types of EE s, quite high antecedent de-tection results, but the difficulty in telling the differ-ence between these two cases results in low F-scores for antecedent recovery of NP – NP and PRO – NP , de-spite the fact that they are among the most common EE types."
"Even though this is a problem, our system still does quite well: 70.4% for NP – NP , and 69.5% for PRO – NP compared to the 60.0% and 50.0% re-ported[REF_CITE]."
"Since it appears the pre-processor is the cause of most of the errors, in-processing with a state-of-the-art lexicalized parser might outperform the pre-processing approach."
"In the next section, we explore this possibility."
"Having compared pre-processing to post-processing in the previous section, in this section, we consider the relative advantages of pre-processing as com-pared to detecting EE s while parsing, with both an unlexicalized and a lexicalized model."
"In making the comparison between detecting EE s during pre-processing versus parsing, we are not only concerned with the accuracy of parsing, EE detection and antecedent recovery, but also with the running time of the parsers."
"In particular,[REF_CITE]found that detecting EE s is infeasible with an unlexicalized parser: the parser was slow and inaccurate at EE detection."
Recall that the runtime of many parsing algo-rithms depends on the size of the grammar or the number of nonterminals.
The unlexicalized CYK parser we use has a worst-case asymptotic runtime of O n 3 N 3 where n is the number of words and N is the number of nonterminals.
The O N 3 bound becomes important when the parser is to insert traces because there are more non-terminals.
"Three factors contribute to this larger nonterminal set: (i) nonterminals are augmented with EE types that contain the parent node of the EE (i.e. S may become S  , S  , etc.) (ii) we must include combinations of EE s as nonterminals may dominate more than one unbound EE (i.e. S    and (iii) a single nonterminal may be repeated in the presence of co-ordination (i.e. S     )."
"These three factors greatly increase the number of nonterminals, potentially reducing the efficiency of a parser that detects EE s. On the other hand, when EE -sites are pre-determined, the effect of the number of nonterminals on parsing speed is moot: the parser can ignore large parts of the gram-mar."
"In this section, we empirically explore the relative advantages of pre-processing over in-processing, with respect to runtime efficiency and the accuracy of parsing and antecedent recovery."
"As in Section 4, we use the unlexicalized parser[REF_CITE], and as a lexicalized parser, an extension of Model 3[REF_CITE]."
"While Model 3 inserts WH – NP traces, it makes some assumptions that preclude it from being used here directly: (i) it cannot handle multiple types of EE s; (ii) it does not allow multiple instances of EE s at a node; (iii) it expects all EE s to be complements, though some are not (e.g. WH – ADVP ); (iv) it expects all EE s to have antecedents, though some do not (e.g. PRO – NP ); (v) it cannot model EE s with dependents, for ex-ample COMP –. . . ."
"Hence, Model 3 must be generalized to other types of discontinuities."
"In order to handle the first four problems, we propose generating ‘gap-categorization’ frames in the same way as subcat-egorization frames are used in the original model."
"We do not offer a solution to the final problem, as the syntactic structure (usually the unary production SBAR S ) will identify these cases."
"After calculating the probability of the head (with its gaps), the left and right gapcat frame are generated independently of each other (and of the subcat frames)."
"For example, the probability for the rule:"
VP ( to ) (+gap= WH-NP ) TO ( to ) (+gap= )
VP ( buy ) (+gap= WH-NP ) is generated as:
"P h ( TO | VP , to )"
"P RGC ( WH-NP | VP , TO , to )"
"P LGC ( | VP , TO , to ) P RC ( VP - C | VP , TO , to ) P LC ( | VP , TO , to )"
"P r ( VP - C WH-NP ( buy )| VP , TO , to , VP - C , WH-NP )"
"P r ( STOP | VP , TO , to , , )"
"P l ( STOP | VP , TO , to , , )"
Generating the actual EE is done in a similar fash-ion: the EE cancels the corresponding ‘gapcat’ re-quirement.
"If it is a complement (e.g. WH – NP ), it also removes the corresponding element from the subcat frame."
The original parsing algorithm was modified to accommodate ‘gapcat’ requirements and generate multiple types of EE s.
"We compare the parsing performance of the two parsers in four cases: the NOTRACE model which re-moves all traces from the test and training data, the TAGGER model of Section 4, and two cases where the parser inserts EE s (we will collectively refer to these cases as the INSERT models)."
"In order to show the effects of increasing the size of nontermi-nal vocabulary, the first INSERT model only consid-ers one EE type, WH – NP while the second (hence-forth PRO &amp; WH ) considers all WH traces as well as NP – NP and PRO – NP discontinuities."
"The results of the unlexicalized and lexicalized ex-periments are summarized in Tables 5 and Table 6, respectively."
"The tables compare relative pars-ing time (slowdown with respect to the NOTRACE model), and in the lexicalized case, PARSEVAL-style bracketing scores."
"However, in the case of the unlexicalized model, the increasing number of missed parses precludes straightforward comparison of bracketing scores, therefore we report the per-centage of sentences where the parser fails."
"In the case of the lexicalized parser, less than 1% of the parses are missed, hence the comparisons are re-liable."
"Finally, we compare EE detection and an-tecedent recovery F-scores of the TAGGER and the PRO &amp; WH models for the overlapping EE types (Ta-ble 7)."
"As noted[REF_CITE], unlexical-ized parsing with EE s does not seem to be viable without pre-processing."
"However, the lexicalized parser is competitive with the pre-processing ap-proach."
"As for the bracketing scores, there are two inter-esting results."
"First, lexicalized models which han-dle EE s have lower bracketing scores than the NO - TRACE model."
"Indeed, as the number of EE s in-creases, so does the number of nonterminals, which results in increasingly severe sparse data problem."
"Consequently, there is a trade-off between finding local phrase structure and long-distance dependen-cies."
"Second, comparing the TAGGER and the PRO &amp; WH models, we find that the bracketing results are nearly identical."
"Nonetheless, the PRO &amp; WH model inserting EE s can match neither the accuracy for antecedent recovery nor the time efficiency of the pre-processing approach."
"Thus, the results show that treating EE -detection as a pre-processing step is beneficial to both to antecedent recovery accuracy and to parsing efficiency."
"Nevertheless, pre-processing is not necessarily the only useful strategy for trace detection."
"Indeed, by taking advantage of the insights that make the finite-state and lexicalized parsing models success-ful, it may be possible to generalize the results to other strategies as well."
There are two key observa-tions of importance here.
"The first observation is that lexicalization is very important for detecting traces, not just for the lex-icalized parser, but, as discussed in Section 3, for the trace-tagger as well."
"The two models may con-tain overlapping information: in many cases, the lex-ical cue corresponds to the immediate head-word the EE depends on."
"However, other surrounding words (which frequently correspond to the head-word of grandparent of the empty node) often carry important information, especially for distinguishing NP – NP and PRO – NP nodes."
"Second, local information (i.e. a window of five words) proves to be informative for the task."
"This explains why the finite-state tagger is more accurate than the parser: this window always crosses a phrase boundary, and the parser cannot consider the whole window."
These two observations give a set of features that seem to be useful for EE detection.
We conjecture that a parser that takes advantage of these features might be more accurate in detecting EE s while pars-ing than the parsers presented here.
"Apart from the pre-processing approach presented here, there are a number of ways these features could be used: 1. in a pre-processing system that only detects EE s, as we have done here; 2. as part of a larger syntactic pre-processing sys-tem, such as supertagging[REF_CITE]; 3. with a more informative beam search[REF_CITE]; 4. or directly integrated into the parsing mecha-nism, for example by combining the finite-state and the parsing probability models."
One of the main contributions of this paper is that a two-step pre-processing approach to finding EE s outperforms both post-processing and in-processing.
We found the pre-processing technique was success-ful because it used features not explicitly incorpo-rated into the other models.
"Furthermore, we found that the result presented[REF_CITE], i.e. pre-processing is better for antecedent recovery than unlexicalized in-processing, also holds when comparing lexical-ized models."
"However, comparing the lexicalized pre-processing system to the unlexicalized one, we find that although lexicalization results in much bet-ter trees, there is only a slight improvement in an-tecedent recovery."
"Third, we present a generalization of Model 3[REF_CITE]to handle a broader range of EE s. While this particular model was not able to outper-form the pre-processing method, it can be further de-veloped into a parsing model which can handle non-local dependencies by incorporating the local cues we found relevant."
"In particular, a local window of five words, ac-companied by the gap+ threads proved to be crucial."
"Thus we claim that, in order to detect long-distance dependencies, a robust stochastic parser should in-tegrate lexical information as well as local cues cut-ting across phrase boundaries by either incorporat-ing them into the probability model or using them in the beam-search."
"We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features."
We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only sur-face syntactic features.
Syntax mediates between surface word order and meaning.
The goal of parsing (syntactic analysis) is ultimately to provide the first step towards giv-ing a semantic interpretation of a string of words.
"So far, attention has focused on parsing, because the semantically annotated corpora required for learn-ing semantic interpretation have not been available."
The completion of the first phase of the PropBank[REF_CITE]represents an important step.
The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB)[REF_CITE].
"The arc labels chosen for the arguments are specific to the predicate, not universal."
"In this paper, we find that the use of deep lin-guistic representations to predict these semantic la-bels are more effective than the generally more surface-syntax representations previously employed[REF_CITE]."
"Specifically, we show that the syntactic dependency structure that results from the extraction of a Tree Adjoining Grammar (TAG) from the PTB, and the features that accom-pany this structure, form a better basis for determin-ing semantic role labels."
"Crucially, the same struc-ture is also produced when parsing with TAG."
"We suggest that the syntactic representation chosen in the PTB is less well suited for semantic process-ing than the other, deeper syntactic representations."
"In fact, this deeper representation expresses syntac-tic notions that have achieved a wide acceptance across linguistic frameworks, unlike the very partic-ular surface-syntactic choices made by the linguists who created the PTB syntactic annotation rules."
The outline of this paper is as follows.
In Sec-tion 2 we introduce the PropBank and describe the problem of predicting semantic tags.
Section 3 presents an overview of our work and distinguishes it from previous work.
Section 4 describes the method used to produce the TAGs that are the basis of our experiments.
Section 5 specifies how train-ing and test data that are used in our experiments are derived from the PropBank.
"Next, we give re-sults on two sets of experiments."
Those that predict semantic tags given gold-standard linguistic infor-mation are described in Section 6.
Those that do prediction from raw text are described in Section 7.
"Finally, in Section 8 we present concluding remarks."
"The PropBank[REF_CITE]annotates the PTB with dependency structures (or ‘predicate-argument’ structures), using sense tags for each word and local semantic labels for each argument and adjunct."
"Argument labels are numbered and used consistently across syntactic alternations for the same verb meaning, as shown in Figure 1."
"Ad-juncts are given special tags such as TMP (for tem-poral), or LOC (for locatives) derived from the orig-inal annotation of the Penn Treebank."
"In addition to the annotated corpus, PropBank provides a lexi-con which lists, for each meaning of each annotated verb, its roleset, i.e., the possible arguments in the predicate and their labels."
"As an example, the entry for the verb kick, is given in Figure 2."
"The notion of “meaning” used is fairly coarse-grained, typically motivated from differing syntactic behavior."
"Since each verb meaning corresponds to exactly one role-set, these terms are often used interchangeably."
"The roleset also includes a “descriptor” field which is in-tended for use during annotation and as documenta-tion, but which does not have any theoretical stand-ing."
Each entry also includes examples.
"Currently there are frames for about 1600 verbs in the corpus, with a total of 2402 rolesets."
"Since we did not yet have access to a corpus an-notated with rolesets, we concentrate in this paper on predicting the role labels for the arguments."
"It is only once we have both that we can interpret the relation between predicate and argument at a very fine level (for example, truck in he kicked the truck withhay as the destination of the loading action)."
We will turn to the problem of assigning rolesets to pred-icates once the data is available.
"We note though that preliminary investigations have shown that for about 65% of predicates (tokens) in the WSJ, there is only one roleset."
"In a further 7% of predicates (tokens), the set of semantic labels on the arguments of that predicate completely disambiguates the roleset."
"Fur-thermore, they show that this method can be used in conjunction with a parser to produce parses anno-tated with semantic labels, and that the parser out-performs a chunker."
The features they use in their experiments can be listed as follows.
Head Word (HW.)
The predicate’s head word as well as the argument’s head word is used.
This feature represents the type of phrase expressing the semantic role.
In Figure 3 phrase type for the argument prices is NP.
This feature captures the surface syntactic relation between the argument’s constituent and the predicate.
See Figure 3 for an example.
This binary feature represents whether the argument occurs before or after the predicate in the sentence.
This binary feature represents whether the predicate is syntactically realized in either passive or active voice.
"Notice that for the exception of voice, the fea-tures solely represent surface syntax aspects of the input parse tree."
This should not be taken to mean that deep syntax features are not impor-tant.
"For example, in their inclusion of voice,[REF_CITE]note that this deep syntax feature plays an important role in connecting seman-tic role with surface grammatical function."
"Aside from voice, we posit that other deep lin-guistic features may be useful to predict semantic role."
"In this work, we explore the use of more gen-eral, deeper syntax features."
We also experiment with semantic features derived from the PropBank.
Our methodology is as follows.
The first stage en-tails generating features representing different lev-els of linguistic analysis.
This is done by first auto-matically extracting several kinds of TAG from the PropBank.
This may in itself generate useful fea-tures because TAG structures typically relate closely syntactic arguments with their corresponding pred-icate.
"Beyond this, our TAG extraction procedure produces a set of features that relate TAG structures on both the surface-syntax as well as the deep-syntax level."
"Finally, because a TAG is extracted from the PropBank, we have a set of semantic features de-rived indirectly from the PropBank through TAG."
The second stage of our methodology entails using these features to predict semantic roles.
We first experiment with prediction of semantic roles given gold-standard parses from the test corpus.
We sub-sequently experiment with their prediction given raw text fed through a deterministic dependency parser.
Our experiments depend upon automatically extract-ing TAGs from the PropBank.
"In doing so, we fol-low the work of others in extracting grammars of various kinds from the PTB, whether it be TAG[REF_CITE], combinatory categorial grammar[REF_CITE], or constraint depen-dency grammar[REF_CITE]."
"We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described[REF_CITE]including ex-tensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features from the resulting TAG."
A TAG is defined to be a set of lexicalized el-ementary trees[REF_CITE].
They may be composed by several well-defined opera-tions to form parse trees.
A lexicalized elementary tree where the lexical item is removed is called a tree frame or a supertag.
The lexical item in the tree is called an anchor.
"Although the TAG for-malism allows wide latitude in how elementary trees may be defined, various linguistic principles gener-ally guide their formation."
"An important principle is that dependencies, including long-distance depen-dencies, are typically localized the same elementary tree by appropriate grouping of syntactically or se-mantically related elements."
The extraction procedure fragments a parse tree from the PTB that is provided as input into elemen-tary trees.
See Figure 4.
These elementary trees can be composed by TAG operations to form the origi-nal parse tree.
The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics.
"Salient heuristics include the use of a head percolation ta-ble[REF_CITE], and another table that distin-guishes between complements and adjunct nodes in the tree."
"For our current work, we use the head per-colation table to determine heads of phrases."
"Also, we treat a PropBank argument (ARG0 . . ."
"ARG9) as a complement and a PropBank adjunct (ARGM’s) as an adjunct when such annotation is available. [Footnote_1] Oth-erwise, we basically follow the approach[REF_CITE]. [Footnote_2]"
"1 The version of the PropBank we are using is not fully an-notated with semantic role information, although the most com-mon predicates are."
"2 Specifically, CA1."
"Besides introducing one kind of TAG extraction procedure,[REF_CITE]introduces the notion of grouping linguistically-related extracted tree frames together."
"In one approach, each tree frame is decom-posed into a feature vector."
Each element of this vec-tor describes a single linguistically-motivated char-acteristic of the tree.
The elements comprising a feature vector are listed in Table 1.
Each elementary tree is decom-posed into a feature vector in a relatively straightfor-ward manner.
"For example, the POS feature is ob-tained from the preterminal node of the elementary tree."
There are also features that specify the syntac-tic transformations that an elementary tree exhibits.
Each such transformation is recognized by struc-tural pattern matching the elementary tree against a pattern that identifies the transformation’s existence.
"For more details, see[REF_CITE]."
"Given a set of elementary trees which compose a TAG, and also the feature vector corresponding to each tree, it is possible to annotate each node rep-resenting an argument in the tree with role informa-tion."
These are syntactic roles including for example subject and direct object.
Each argument node is la-beled with two kinds of roles: a surface syntactic role and a deep syntactic role.
The former is ob-tained through determining the position of the node with respect to the anchor of the tree using the usu-ally positional rules for determining argument status in English.
The latter is obtained from the former and also from knowledge of the syntactic transfor-mations that have been applied to the tree.
"For ex-ample, we determine the deep syntactic role of a wh-moved element by “undoing” the wh-movement by using the trace information in the PTB."
The PropBank contains all of the notation of the Penn Treebank as well as semantic notation.
"For our current work, we extract two kinds of TAG from the PropBank."
"One grammar, SEM-TAG, has elemen-tary trees annotated with the aforementioned syntac-tic information as well as semantic information."
Se-mantic information includes semantic role as well as semantic subcategorization information.
"The other grammar, SYNT-TAG, differs from SEM-TAG only by the absence of any semantic role information."
"For our experiments, we use a version of the Prop-Bank where the most commonly appearing predi-cates have been annotated, not all."
Our extracted TAGs are derived[REF_CITE]-21 of the PTB.
"Furthermore, training data for our experiments are always derived from these sections."
The entire set of semantic roles that are found in the PropBank are not used in our experiments.
"In particular, we only include as semantic roles those instances in the propbank such that in the ex-tracted TAG they are localized in the same elemen-tary tree."
"As a consequence, adjunct semantic roles (ARGM’s) are basically absent from our test cor-pus."
"Furthermore, not all of the complement seman-tic roles are found in our test corpus."
"For example, cases of subject-control PRO are ignored because the surface subject is found in a different tree frame than the predicate."
"Still, a large majority of com-plement semantic roles are found in our test corpus (more than 87%)."
This section is devoted towards evaluating different features obtained from a gold-standard corpus in the task of determining semantic role.
We use the fea-ture set mentioned in Section 3 as well as features derived from TAGs mentioned in Section 4.
"In this section, we detail the latter set of features."
We then describe the results of using different feature sets.
These experiments are performed using the C4.5 de-cision tree machine learning algorithm.
The stan-dard settings are used.
"Furthermore, results are al-ways given using unpruned decision trees because we find that these are the ones that performed the best on a development set."
These features are determined during the extrac-tion of a TAG:
This is a path in a tree frame from its preterminal to a particular argument node in a tree frame.
The supertag path of the subject of the right-most tree frame in Figure 4 is VBG↑VP↑S↓NP.
This can be the tree frame correspond-ing to either the predicate or the argument.
This is the surface-syntactic role of an ar-gument.
Example of values include 0 (subject) and 1 (direct object).
This is the surface-syntactic subcate-gorization frame.
"For example, the ssubcat cor-responding to a transitive tree frame would be NP0 NP1."
PPs as arguments are always annotated with the preposition.
"For example, the ssubcat for the passive version of hit would be NP1 NP2(by)."
This is the deep-syntactic role of an argu-ment.
Example of values include 0 (subject) and 1 (direct object).
This is the deep-syntactic subcate-gorization frame.
"For example, the dsubcat cor-responding to a transitive tree frame would be NP0 NP1."
"Generally, PPs as arguments are anno-tated with the preposition."
"For example, the dsub-cat for load is NP0 NP1 NP2(into)."
The exception is when the argument is not realized as a PP when the predicate is realized in a non-syntactically trans-formed way.
"For example, the dsubcat for the pas-sive version of hit would be NP0 NP1."
This is the semantic subcategoriza-tion frame.
"We first experiment with the set of features de-scribed[REF_CITE]: Pred HW, Arg HW, Phrase Type, Position, Path, Voice."
Call this feature set GP0.
"The error rate, 10.0%, is lower than that reported[REF_CITE], 17.2%."
This is presumably because our training and test data has been assembled in a different manner as mentioned in Section 5.
"Our next experiment is on the same set of fea-tures, with the exception that Path has been replaced with Supertag Path. (Feature set GP1)."
The er-ror rate is reduced from 10.0% to 9.7%.
"This is statistically significant (t-test, p &lt; 0.05), albeit a small improvement."
One explanation for the im-provement is that Path does not generalize as well as Supertag path does.
"For example, the path fea-ture value VBG↑VP↑VP↑S↓NP reflects surface sub-ject position in the sentence Prices are falling but so does VBG↑VP↑S↓NP in the sentence Sellers regret prices falling."
"Because TAG localizes dependencies, the corresponding values for Supertag path in these sentences would be identical."
"We now experiment with our surface syntax fea-tures: Pred HW, Arg HW, Ssubcat, and Srole. (Feature set SURFACE.)"
"Its performance on SEM-TAG is 8.2% whereas its performance on SYNT-TAG is 7.6%, a tangible improvement over previ-ous models."
"One reason for the improvement could be that this model is assigning semantic labels with knowledge of the other roles the predicate assigns, unlike previous models."
"Our next experiment involves using deep syntax features: Pred HW, Arg HW, Dsubcat, and Drole. (Feature set DEEP.)"
"Its performance on both SEM-TAG and SYNT-TAG is 6.5%, better than previous models."
Its performance is better than SURFACE presumably because syntactic transformations are taken to account by deep syntax features.
Note also that the transformations which are taken into ac-count are a superset of the transformations taken into account[REF_CITE].
"This experiment considers use of semantic fea-tures: Pred HW, Arg HW, Semsubcat, and Drole. (Feature set SEMANTIC.)"
"Of course, there are only results for SEM-TAG, which turns out to be 1.9%."
This is the best performance yet.
"In our final experiment, we use supertag features: Pred HW, Arg HW, Pred Supertag, Arg Su- pertag, Drole. (Feature set SUPERTAG.)"
The error rates are 2.8% for SEM-TAG and 7.4% for SYNT-TAG.
"Considering SEM-TAG only, this model per-forms better than its corresponding DEEP model, probably because supertag for SEM-TAG include crucial semantic information."
"Considering SYNT-TAG only, this model performs worse than its cor-responding DEEP model, presumably because of sparse data problems when modeling supertags."
This sparse data problem is also apparent by com-paring the model based on SEM-TAG with the cor-responding SEM-TAG SEMANTIC model.
"In this section, we are concerned with the problem of finding semantic arguments and labeling them with their correct semantic role given raw text as input."
"In order to perform this task, we parse this raw text us-ing a combination of supertagging and LDA, which is a method that yields partial dependency parses an-notated with TAG structures."
We perform this task using both SEM-TAG and SYNT-TAG.
"For the for-mer, after supertagging and LDA, the task is accom-plished because the TAG structures are already an-notated with semantic role information."
"For the lat-ter, we use the best performing model from Section 6 in order to find semantic roles given syntactic fea-tures from the parse."
Supertagging[REF_CITE]is the task of assigning a single supertag to each word given raw text as input.
"For example, given the sen-tence Prices are falling, a supertagger might return the supertagged sentence in Figure 4."
Supertagging returns an almost-parse in the sense that it is per-forming much parsing disambiguation.
"The typi-cal technique to perform supertagging is the trigram model, akin to models of the same name for part-of-speech tagging."
This is the technique that we use here.
Data sparseness is a significant issue when supertagging with extracted grammar[REF_CITE].
"For this reason, we smooth the emit probabilities P(w|t) in the trigram model using distributional similarity fol-lowing[REF_CITE]."
"In particular, we use Jaccard’s coefficient as the similarity metric with a similarity threshold of 0.04 and a radius of 25 because these were found to attain optimal results[REF_CITE]."
Training data for supertagging is[REF_CITE]-21 of the PropBank.
A supertagging model based on SEM-TAG performs with 76.32% accuracy[REF_CITE].
The corresponding model for SYNT-TAG performs with 80.34% accuracy.
Accuracy is mea-sured for all words in the sentence including punc-tuation.
"The SYNT-TAG model performs better than the SEM-TAG model, understandably, because SYNT-TAG is the simpler grammar."
LDA is an acronym for Lightweight Dependency Analyzer[REF_CITE].
"Given as input a su-pertagged sequence of words, it outputs a partial de-pendency parse."
"It takes advantage of the fact that supertagging provides an almost-parse in order to dependency parse the sentence in a simple, deter-ministic fashion."
Basic LDA is a two step procedure.
The first step involves linking each word serving as a modifier with the word that it modifies.
The sec-ond step involves linking each word serving as an ar-gument with its predicate.
Linking always only oc-curs so that grammatical requirements as stipulated by the supertags are satisfied.
The version of LDA that is used in this work differs[REF_CITE]in that there are other constraints on the linking pro-cess. [Footnote_3]
3 We thank Srinivas for the use of his LDA software.
"In particular, a link is not established if its existence would create crossing brackets or cycles in the dependency tree for the sentence."
We perform LDA on two versions[REF_CITE]one supertagged with SEM-TAG and the other with SYNT-TAG.
The results are shown in Table 3.
Eval-uation is performed on dependencies excluding leaf-node punctuation.
Each dependency is evaluated ac-cording to both whether the correct head and depen-dent is related as well as whether they both receive the correct part of speech tag.
"The F-measure scores, in the 70% range, are relatively low compared[REF_CITE]which has a corresponding score of around 90%."
This is perhaps to be expected because[REF_CITE]is based on a full parser.
Note also that the accuracy of LDA is highly dependent on the accuracy of the supertagged input.
"This explains, for example, the fact that the accuracy on SEM-TAG supertagged input is lower than the accuracy with SYNT-TAG supertagged input."
The output of LDA is a partial dependency parse an-notated with TAG structures.
We can use this output to predict semantic roles of arguments.
The manner in which this is done depends on the kind of gram-mar that is used.
The LDA output using SEM-TAG is already annotated with semantic role information because it is encoded in the grammar itself.
"On the other hand, the LDA output using SYNT-TAG con-tains strictly syntactic information."
"In this case, we use the highest performing model from Section 6 in order to label arguments with semantic roles."
Evaluation of prediction of semantic roles takes the following form.
Each argument labeled by a se-mantic role in the test corpus is treated as one trial.
Certain aspects of this trial are always checked for correctness.
These include checking that the seman-tic role and the dependency-link are correct.
"There are other aspects which may or may not be checked, depending on the type of evaluation."
"One aspect, “bnd,” is whether or not the argument’s bracketing as specified in the dependency tree is correct."
"An- other aspect, “arg,” is whether or not the headword of the argument is chosen to be correct."
Table 4 show the results when we use SEM-TAG in order to supertag the input and perform LDA.
"When the boundaries are found, finding the head word additionally does not result in a decrease of performance."
"However, correctly identifying the head word instead of the boundaries leads to an im-portant increase in performance."
"Furthermore, note the low recall and high precision of the “base + arg” evaluation."
In part this is due to the nature of the PropBank corpus that we are using.
"In par-ticular, because not all predicates in our version of the PropBank are annotated with semantic roles, the supertagger for SEM-TAG will sometimes annotate text without semantic roles when in fact it should contain them."
Table 5 shows the results of first supertagging the input with SYNT-TAG and then using a model trained on the DEEP feature set to annotate the re-sulting syntactic structure with semantic roles.
This two-step approach greatly increases performance over the corresponding SEM-TAG based approach.
"These results are comparable to the results[REF_CITE], but only roughly because of differences in corpora."
"They also experiment with using a chunker which yields a recall of 0.35, a precision of 0.50, and an F-measure of 0.41."
We have presented various alternative approaches to predicting PropBank role labels using forms of lin-guistic information that are deeper than the PTB’s surface-syntax labels.
"These features may either be directly derived from a TAG, such as Supertag path, or indirectly via aspects of supertags, such as deep syntactic features like Drole."
These are found to produce substantial improvements in ac-curacy.
We believe that such improvement is due to these features better capturing the syntactic infor-mation that is relevant for the task of semantic la-beling.
"Also, these features represent syntactic cate-gories about which there is a broad consensus in the literature."
"Therefore, we believe that our results are portable to other frameworks and differently anno-tated corpora such as dependency corpora."
We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.
"Im-provements along this line may be attained by use of a full TAG parser, such[REF_CITE]for exam-ple."
"The development of FrameNet, a large database of semantically annotated sen-tences, has primed research into statistical methods for semantic tagging."
We ad-vance previous work by adopting a Maximum Entropy approach and by using previous tag information to find the high-est probability tag sequence for a given sentence.
Further we examine the use of sentence level syntactic pattern features to increase performance.
"We analyze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data."
Experiments indicate a statistically significant im-provement (p&lt;0.01) of over 6%.
"Recent work in the development of FrameNet, a large database of semantically annotated sentences, has laid the foundation for statistical approaches to the task of automatic semantic classification."
The FrameNet project seeks to annotate a large subset of the British National Corpus with seman-tic information.
"Annotations are based on Frame Semantics[REF_CITE], in which frames are defined as schematic representations of situations involving various frame elements such as partici-pants, props, and other conceptual roles."
"In each FrameNet sentence, a single target predicate is identified and all of its relevant frame elements are tagged with their semantic role (e.g., Agent, Judge), their syntactic phrase type (e.g.,"
"NP, PP), and their grammatical function (e.g., ex-ternal argument, object argument)."
Figure 1 shows an example of an annotated sentence and its appro-priate semantic frame.
"As of its first release[REF_CITE]FrameNet has made available 49,000 annotated sentences."
"The release contains 99,000 annotated frame ele-ments for 1462 distinct lexical predicates (927 verbs, 339 nouns, and 175 adjectives)."
"While considerable in scale, the FrameNet da-tabase does not yet approach the magnitude of re-sources available for other NLP tasks."
"Each target predicate, for example, has on average only 30 sen-tences tagged."
"This data sparsity makes the task of learning a semantic classifier formidable, and in-creases the importance of the modeling framework that is employed."
"To our knowledge,[REF_CITE]is the only work to use FrameNet to build a statis-tically based semantic classifier."
They split the problem into two distinct sub-tasks: frame element identification and frame element classification.
"In the identification phase, syntactic information is extracted from a parse tree to learn the boundaries of the frame elements in a sentence."
"In the classi-fication phase, similar syntactic information is used to classify those elements into their semantic roles."
In both phases[REF_CITE]build a model of the conditional probabilities of the classification given a vector of syntactic features.
The full conditional probability is decomposed into simpler conditional probabilities that are then in-terpolated to make the classification.
"Their best performance on held out test data is achieved using a linear interpolation model: m p(r | x) = ∑ α i p(r | x i ) i=0 where r is the class to be predicted, x is the vector of syntactic features, x i is a subset of those fea-tures, α i is the weight given to that subset condi-tional probability (as determined using the EM algorithm), and m is the total number of subsets used."
"Using this method, they report a test set ac-curacy of 78.5% on classifying semantic roles and precision/recall scores of .726/.631 on frame ele-ment identification."
We extend[REF_CITE]’s initial effort in three ways.
"First, we adopt a maximum entropy (ME) framework in order to learn a more accurate classification model."
"Second, we include features that look at previous tags and use previous tag information to find the highest probability se-mantic role sequence for a given sentence."
"Finally, we examine sentence-level patterns that exploit more global information in order to classify frame elements."
We compare the results of our classifier to that[REF_CITE]on matched test sets of both human annotated and automati-cally identified frame elements.
"Training (36,993 sentences / 75,548 frame ele-ments), development (4,000 sentences / 8,167 frame elements), and held out test sets ([URL_CITE],865 sen-tences / 7,899 frame elements) were obtained in order to exactly match those used[REF_CITE][Footnote_1] ."
1 Data sets (including parse trees) were obtained from Dan Gildea via personal communication.
"In the experiments presented below, features are extracted for each frame ele-ment in a sentence and used to classify that ele- ment into one of 120 semantic role categories."
The boundaries of each frame element are given based on the human annotations in FrameNet.
"In Section 4, experiments are performed using automatically identified frame elements."
"For each frame element, features are extracted from the surface text of the sentence and from an automatically generated syntactic parse tree[REF_CITE]."
"The features used are described below: • Target predicate (tar): Although there may be many predicates in a sentence with associ-ated frame elements, classification operates on only one target predicate at a time."
The target predicate is the only feature that is not ex-tracted from the sentence itself and must be given by the user.
"Note that the frame which the target predicate instantiates is not given, leaving any word sense ambiguities to be han-dled implicitly by the classifier. [Footnote_2] • Phrase type (pt): The syntactic phrase type of the frame element (e.g. NP, PP) is extracted from the parse tree of the sentence by finding the constituent in the tree whose boundaries match the human annotated boundaries of the element."
"2 Because of the interaction of head word features with the target predicate, we suspect that ambiguous lexical items do not account for much error. This question, however, will be addressed explicitly in future work."
"In cases where there exists no con-stituent that perfectly matches the element, the constituent is chosen which matches the largest text span of the element and has the same left-most boundary. • Syntactic head (head): The syntactic heads of the frame elements are extracted from the frame element’s matching constituent (as de-scribed above) using a heuristic method de-scribed by Michael Collins. 3 This method extracts the syntactic heads of constituents; thus, for example, the second frame element in Figure 1 has head “hands,” while the third frame element has head “in.” • Logical Function (lf): A simplification of the grammatical function annotation (see section 1) is extracted from the parse tree."
"Unlike the 



 predicate, and “her head” is an object argu-ment Noun Phrase."
"Thus, the syntactic pattern associated with the sentence is [NP-ext, target, NP-obj]."
"ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence, but other- wise is as uniform as possible[REF_CITE]."
"We model the probability of a semantic role r given a vector of features x according to the ME formulation below: n p(r | x) = 1Z exp[ ∑ λ i f i (r, x)] x i=0"
"Here Z x is a normalization constant, f i (r,x) is a fea-ture function which maps each role and vector element (or combination of elements) to a binary value, n is the total number of feature functions, and λ i is the weight for a given feature function."
The final classification is just the role with highest probability given its feature vector and the model.
The feature functions that we employ can be divided into feature sets based upon the types and combinations of features on which they operate.
"Table 1 lists the feature sets that we use, as well as the number of individual feature functions they contain."
The feature combinations were chosen based both on previous work and trial and error.
In future work we will examine more principled fea-ture selection techniques.
It is important to note that the feature functions described here are not equivalent to the subset conditional distributions that are used in the Gildea and Jurafsky model.
"ME models are log-linear models in which feature functions map specific instances of syntactic features and classes to binary values (e.g., if a training element has head=”in” and role=C AUSE , then, for that element, the feature function f( CAUSE , ”in”) will equal 1)."
"Thus, ME is not here being used as another way to find weights for an interpolated model."
"Rather, the ME ap-proach provides an overarching framework in which the full distribution of semantic roles given syntactic features can be modeled."
We train the ME models using the GIS algo-rithm[REF_CITE]as implemented in the YASMET ME package[REF_CITE].
We use the YASMET MEtagger[REF_CITE]to perform the Viterbi search.
The classifier was trained until performance on the development set ceased to improve.
Feature weights were smoothed using Gaussian priors with mean 0[REF_CITE].
The standard devia-tion of this distribution was optimized on the de-velopment set for each experiment.
We present three experiments in which different feature sets are used to train the ME classifier.
The first experiment uses only those feature combina-tions described[REF_CITE](fea-ture sets 0-7 from Table 1).
The second experiment uses a super set of the first and incor-porates the syntactic pattern features described above (feature sets 0-9).
The final experiment uses the previous tags and implements Viterbi search to find the best tag sequence (feature sets 0-11).
"We further investigate the effect of varying two aspects of classifier training: the standard deviation of the Gaussian priors used for smoothing, and the number of sentences used for training."
"To examine the effect of optimizing the standard deviation, a range of values was chosen and a classifier was trained using each value until performance on a development set ceased to improve."
"To examine the effect of training set size on performance, five data sets were generated from the original set with 36, 367, 3674, 7349, and 24496 sentences, respectively."
"These data sets were created by going through the original set and selecting every thousandth, hundredth, tenth, fifth, and every second and third sentence, respectively."
Figure 2 shows the results of our experiments alongside those[REF_CITE]on identical held out test sets.
"The difference in per-formance between each classifier is statistically significant at (p&lt;0.01)[REF_CITE], with the exception of Exp 2 and Exp 3, whose difference is statistically significant at (p&lt;0.05)."
Table 2 shows the effect of varying the stan-dard deviation of the Gaussian priors used for smoothing in Experiment 1.
The difference in per-formance between the classifiers trained using standard deviation 1 and 2 is statistically signifi-cant at (p&lt;0.01).
Figure 3 shows the change in performance as a function of training set size.
Classifiers were trained using the full set of features described for Experiment 3.
Table 3 shows the confusion matrix for a subset of semantic roles.
Five roles were chosen for pres-entation based upon their high contribution to clas-sifier error.
Confusion between these five account for 27% of all errors made amongst the 120 possi-ble roles.
"The tenth role, other, represents the sum of the remaining 115 roles."
Table 4 presents ex-ample errors for five of the most confused roles.
It is clear that the ME models improve perform-ance on frame element classification.
There are a number of reasons for this improvement.
"First, for this task the log-linear model employed in the ME framework is better than the linear interpolation model used by Gildea and Jurafsky."
One possible reason for this is that semantic role classification benefits from the ME model’s bias for more uniform probability distributions that sat-isfy the constraints placed on the model by the training data.
Another reason for improved performance comes from ME’s simpler design.
"Instead of having to worry about finding proper backoff strategies amongst distributions of features subsets, ME al-lows one to include many features in a single model and automatically adjusts the weights of these features appropriately."
"Also, because the ME models find weights for many thousands of features, they have many more degrees of freedom than the linear interpolated models of Gildea and Jurafsky."
"Although many degrees of freedom can lead to overfitting of the training data, the smoothing procedure employed in our experiments helps to counteract this prob-lem."
"As evidenced in Table 2, by optimizing the standard deviation used in smoothing the ME models are able to show significant increases in performance on held out test data."
"Finally, by including in our model sentence-level pattern features and information about previ-ous classes, global information can be exploited for improved classification."
The accuracy gained by including such global information confirms the intuition that the semantic role of an element is much related to the entire sentence of which it is a part.
"Having discussed the advantages of the models presented here, it is interesting to look at the errors that the system makes."
It is clear from the confu-sion matrix in Table 3 that a great deal of the sys-tem error comes from relatively few semantic roles. 4 Table 4 offers some insight into why these errors occur.
"For example, the confusions exem-plified in 1 and 2 are both due to the fact that the particular phrases employed can be used in multi-ple roles (including the roles hypothesized by the system)."
"Thus, while “across the counter” may be considered a goal when one is talking about a per-son and their head, the same phrase would be con-sidered a path if one were talking about a mouse who is running."
"Examples 3 and [Footnote_4], while showing phrases with similar confusions, stand out as being errors caused by an inability to deal with passive sentences."
4 44% of all error is due to confusion between only nine roles.
"Such errors are not unexpected; for, even though the voice of the sentence is an explicit feature, the system suffers from the paucity of passive sen-tences in the data (approximately 5%)."
"Finally, example 5 shows an error that is based on the difficult nature of the decision itself (i.e., it is unclear whether “the efficiency” is the reason for admiration, or what is being admired)."
"Often times, phrases are assigned semantic roles that are not obvious even to human evaluators."
In such cases it is difficult to determine what information might be useful for the system.
"Having looked at the types of errors that are common for the system, it becomes interesting to examine what strategy may be best to overcome such errors."
"Aside from new features, one solution is obvious: more data."
The curve in Figure 2 shows that there is still a great deal of performance to be gained by training the current ME models on more data.
"The slope of the curve indicates that we are far from a plateau, and that even constant increases in the amount of available training data may push classifier performance above 90% accu-racy."
"Having demonstrated the effectiveness of the ME approach on frame element classification given hand annotated frame element boundaries, we next examine the value of the approach given automatically identified boundaries."
Gildea and Jurafsky equate the task of locating frame element boundaries to one of identifying frame elements amongst the parse tree constituents of a given sentence.
"Because not all frame element boundaries exactly match constituent boundaries, this approach can perform no better than 86.9% (i.e. the number of elements that match constitu-ents (6864) divided by the total number of ele-ments (7899)) on the test set."
"Frame element identification is a binary classifica-tion problem in which each constituent in a parse tree is described by a feature vector and, based on that vector, tagged as either a frame element or not."
In generating feature vectors we use a subset of the features described for role tagging as well as an additional path feature.
Gildea and Jurafsky introduce the path feature in order to capture the structural relationship be-tween a constituent and the target predicate.
The path of a constituent is represented by the nodes through which one passes while traveling up the tree from the constituent and then down through the governing category to the target.
Figure 4 shows an example of this feature for a frame ele-ment from the sentence presented in Figure 1.
We use the ME formulation described in Section 3.2 to build a binary classifier.
The classifier fea-tures follow closely those used in Gildea and Juraf-sky.
"We model the data using the feature sets: f(fe, path), f(fe, path, tar), and f(fe, head, tar), where fe represents the binary classification of the constitu-ent."
"While this experiment only uses three feature sets, the heterogeneity of the path feature is so great that the classifier itself uses 1,119,331 unique binary features."
"With the constituents having been labeled, we apply the ME frame element classifier described above."
"Results are presented using the classifier of Experiment 1, described in section 3.3."
We then investigate the effect of varying the number of constituents used for training on identification per-formance.
"Five data sets of approximately 100,000 10,000, 1,000, and 100 constituents were generated from the original set by random selection and used to train ME models as described above."
"Table 5 compares the results[REF_CITE]and the ME frame element identifier on both the task of frame element identification alone, and the combined task of frame element identifica-tion and classification."
"In order to be counted cor-rect on the combined task, the constituent must have been correctly identified as a frame element, and then must have been correctly classified into one of the 120 semantic categories."
"Recall is calculated based on the total number of frame elements in the test set, not on the total number of elements that have matching parse con-stituents."
"Thus, the upper limit is 86.9%, not .631 .675 .67 .468 .551 .679 .706 .6 .554 .576 100%."
Precision is calculated as the number of correct positive classifications divided by the num-ber of total positive classifications.
The difference in the F-scores on the identifica-tion task alone and on the combined task are statis-tically significant at the (p&lt;0.01) level 5 .
"The accuracy of the ME semantic classifier on the automatically identified frame elements is 81.[Footnote_5]%, not a statistically significant difference from its performance on hand labeled elements, but a statis-tically significant difference from the classifier[REF_CITE](p&lt;0.01)."
"5 G&amp;J’s results for the combined task were generated with a threshold applied to the FE classifier (Dan Gildea, personal communication). This is why their precision/recall scores are dissimilar to their accuracy scores, as reported in section 3. Because the ME classifier does not employ a threshold, com-parisons must be based on F-score."
Figure 5 shows the results of varying the train-ing set size on identification performance.
"For each data set, thresholds were chosen to maximize F-Score."
It is clear from the results above that the perform-ance of the ME model for frame element classifica-tion is robust to the use of automatically identified frame element boundaries.
"Further, the ME framework yields better results on the frame ele-ment identification task than the simple linear in-terpolation model of Gildea and Jurafsky."
This result is not surprising given the discussion in Sec-tion 3.
"What is striking, however, is the drastic overall reduction in performance on the combined identification and classification task."
The bottleneck here is the identification of frame element boundaries.
"Unlike with classification though, Figure 5 indicates that a plateau in the learning curve has been reached, and thus, more data will not yield as dramatic an improvement for the given feature set and model."
"The results reported here show that ME models provide higher performance on frame element clas-sification tasks, given both human and automati-cally identified frame element boundaries, than the linear interpolation models examined in previous work."
"We attribute this increase to the benefits of the ME framework itself, the incorporation of sen-tence-level syntactic patterns into our feature set, and the use of previous tag information to find the most probable sequence of roles for a sentence."
But perhaps most striking in our results are the effects of varying training set size on the perform-ance of the classification and identification models.
"While for classification, the learning curve appears to be still increasing with training set size, the learning curve for identification appears to have already begun to plateau."
"This suggests that while classification will continue to improve as the Fra-meNet database gets larger, increased performance on identification will rely on the development of more sophisticated models."
"In future work, we intend to apply the lessons learned here to the problem of frame element iden-tification."
Gildea and Jurafsky have shown that improvements in identification can be had by more closely integrating the task with classification (they report an F-Score of .719 using an integrated model).
We are currently exploring a ME ap-proach which integrates these two tasks under a tagging framework.
Initial results show that sig-nificant improvements can be had using techniques similar to those described above.
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
"This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argu-ment roles."
"Correctly identifying the semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the infor-mation extraction problem, can serve as an interme-diate step in machine translation or automatic sum-marization."
"Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary."
John will meet Mary.
John and Mary will meet. (2) The door opened.
Mary opened the door.
"Recently, attention has turned to creating cor-pora annotated with argument structures."
The PropBank[REF_CITE]and the FrameNet[REF_CITE]projects both doc-ument the variation in syntactic realization of the arguments of predicates in general English text.
"In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system."
We compare a parser based on Combinatory Categorial Grammar (CCG)[REF_CITE]with the Collins parser.
"As the CCG parser is trained and tested on a cor-pus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both gold-standard and automatic parses for both CCG and the traditional Treebank representation."
"The Treebank-parser returns skeletal phrase-structure trees with-out the traces or functional tags in the original Penn Treebank, whereas the CCG parser returns word-word dependencies that correspond to the under-lying predicate-argument structure, including long-range dependencies arising through control, raising, extraction and coordination."
The Proposition Bank[REF_CITE]provides a human-annotated corpus of semantic verb-argument relations.
"For each verb appearing in the corpus, a set of semantic roles is defined."
"Roles for each verb are simply numbered Arg0, Arg1, Arg2, etc."
"As an example, the entry-specific roles for the verb offer are given below:"
Arg0 entity offering
Arg3 benefactive or entity offered to
"These roles are then annotated for every instance of the verb appearing in the corpus, including the following examples: (3) [ ARG0 the company] to offer [[REF_CITE]% stake] to [ ARG2 the public]. (4) [ ARG0 Sotheby’s] ... offered [ ARG2 the Dorrance heirs] [ ARG1 a money-back guarantee] (5) [ ARG1 an amendment] offered by [ ARG0 Rep. Peter DeFazio] (6) [ ARG2 Subcontractors] will be offered [ ARG1 a settlement]"
A variety of additional roles are assumed to apply across all verbs.
"These secondary roles can be thought of as being adjuncts, rather than arguments, although no claims are made as to optionality or other traditional argu-ment/adjunct tests."
"The secondary roles include: and are represented in PropBank as “ArgM” with an additional function tag, for example ArgM-TMP for temporal."
We refer to PropBank’s numbered argu-ments as “core” arguments.
Core arguments repre-sent 75% of the total labeled roles in the PropBank data.
"Our system predicts all the roles, including core arguments as well as the ArgM labels and their function tags."
"Combinatory Categorial Grammar (CCG)[REF_CITE], is a grammatical theory which provides a completely transparent interface between surface syntax and underlying semantics, such that each syntactic derivation corresponds directly to an in-terpretable semantic representation which includes long-range dependencies that arise through control, raising, coordination and extraction."
"In CCG, words are assigned atomic cate-gories such as NP, or functor categories like (S[dcl]\NP)/NP (transitive declarative verb) or S/S (sentential modifier)."
Adjuncts are represented as functor categories such as S/S which expect and return the same type.
"We use indices to number the arguments of functor categories, eg. (S[dcl]\NP 1 )/NP 2 , or S/S 1 , and indicate the word-word dependencies in the predicate-argument struc-ture as tuples hw h , c h , i, w a i, where c h is the lexical category of the head word w h , and w a is the head word of the constituent that fills the ith argument of c h ."
Long-range dependencies can be projected through certain types of lexical categories or through rules such as coordination of functor categories.
"For example, in the lexical category of a relative pronoun, (NP\NP i )/(S[dcl]/NP i ), the head of the NP that is missing from the relative clause is unified with (as indicated by the indices i) the head of the NP that is modified by the entire relative clause."
"Figure 1 shows the derivations of an ordinary sentence, a relative clause and a right-node-raising construction."
"In all three sentences, the predicate-argument relations between London and denied and plans and denied are the same, which in CCG is expressed by the fact that London fills the first (ie. subject) argument slot of the lexical category of de-nied, (S[dcl]\NP 1 )/NP 2 , and plans fills the second (object) slot."
The relations extracted from the CCG derivation for the sentence “London denied plans on Monday” are shown in Table 1.
The CCG parser returns the local and long-range word-word dependencies that express the predicate-argument structure corresponding to the derivation.
These relations are recovered with an accuracy of around 83% (labeled recovery) or 91% (unlabeled recovery)[REF_CITE].
"By contrast, stan-dard Treebank parsers such[REF_CITE]only return phrase-structure trees, from which non-local dependencies are difficult to recover. w h c h i w a denied (S[dcl]\NP 1 )/NP 2 1 London denied (S[dcl]\NP 1 ) /NP 2 2 plans on ((S\NP 1 )\(S\NP) 2 )/NP 3 2 denied on ((S\NP 1 )\(S\NP) 2 )/NP 3 3 Monday"
"The CCG parser has been trained and tested on CCGbank[REF_CITE], a treebank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data."
Our aim is to use CCG derivations as input to a sys-tem for automatically producing the argument labels of PropBank.
"In order to do this, we wish to cor-relate the CCG relations above with PropBank ar-guments."
PropBank argument labels are assigned to nodes in the syntactic trees from the Penn Tree-bank.
"While the CCGbank is derived from the Penn Treebank, in many cases the constituent structures do not correspond."
"That is, there may be no con-stituent in the CCG derivation corresponding to the same sequence of words as a particular constituent in the Treebank tree."
"For this reason, we compute the correspondence between the CCG derivation and the PropBank labels at the level of head words."
"For each role label for a verb’s argument in PropBank, we first find the head word for its constituent accord-ing to the the head rules[REF_CITE]."
We then look for the label of the CCG relation between this head word and the verb itself.
"In previous work using the PropBank corpus,[REF_CITE]developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser[REF_CITE]."
We will briefly review their proba-bility model before adapting the system to incorpo- rate features from the CCG derivations.
"For the Treebank-based system, we use the proba-bility model[REF_CITE]."
Proba-bilities of a parse constituent belonging to a given semantic role are calculated from the following fea-tures:
"The phrase type feature indicates the syntactic type of the phrase expressing the semantic roles: ex-amples include noun phrase (NP), verb phrase (VP), and clause (S)."
The parse tree path feature is designed to capture the syntactic relation of a constituent to the pred-icate.
"It is defined as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 2."
"Although the path is composed as a string of sym-bols, our systems will treat the string as an atomic value."
"The path includes, as the first element of the string, the part of speech of the predicate, and, as the last element, the phrase type or syntactic category of the sentence constituent marked as an argument. cate ate to the argument NP He can be represented as VB↑VP↑S↓NP, with ↑ indicating upward movement in the parse tree and ↓ downward movement."
The position feature simply indicates whether the constituent to be labeled occurs before or after the predicate.
"This feature is highly correlated with grammatical function, since subjects will generally appear before a verb, and objects after."
"This feature may overcome the shortcomings of reading gram-matical function from the parse tree, as well as errors in the parser output."
"The voice feature distinguishes between active and passive verbs, and is important in predicting se-mantic roles because direct objects of active verbs correspond to subjects of passive verbs."
"An instance of a verb was considered passive if it is tagged as a past participle (e.g. taken), unless it occurs as a descendent verb phrase headed by any form of have (e.g. has taken) without an intervening verb phrase headed by any form of be (e.g. has been taken)."
"The head word is a lexical feature, and provides information about the semantic type of the role filler."
Head words of nodes in the parse tree are determined using the same deterministic set of head word rules used[REF_CITE].
"The system attempts to predict argument roles in new data, looking for the highest probabil-ity assignment of roles r i to all constituents i in the sentence, given the set of features F i = {pt i , path i , pos i , v i , h i } at each constituent in the parse tree, and the predicate p: argmax r 1..n P (r 1..n |F 1..n , p)"
"We break the probability estimation into two parts, the first being the probability P(r i |F i , p) of a constituent’s role given our five features for the consituent, and the predicate p. Due to the sparsity of the data, it is not possible to estimate this proba-bility from the counts in the training data."
"Instead, probabilities are estimated from various subsets of the features, and interpolated as a linear combina-tion of the resulting distributions."
"The interpolation is performed over the most specific distributions for which data are available, which can be thought of as choosing the topmost distributions available from a backoff lattice, shown in Figure 3."
"The probabilities P(r i |F i , p) are combined with the probabilities P({r 1..n }|p) for a set of roles ap-pearing in a sentence given a predicate, using the following formula:"
"Y P (r i |F i , p) P (r 1..n |F 1..n , p) ≈ P ({r 1..n }|p) P (r i |p) i"
"This approach, described in more detail[REF_CITE], allows interaction be-tween the role assignments for individual con-stituents while making certain independence as-sumptions necessary for efficient probability estima-tion."
"In particular, we assume that sets of roles ap-pear independent of their linear order, and that the features F of a constituents are independent of other constituents’ features given the constituent’s role."
"In the CCG version, we replace the features above with corresponding features based on both the sen-tence’s CCG derivation tree (shown in Figure 1) and the CCG predicate-argument relations extracted from it (shown in Table 1)."
"The parse tree path feature, designed to capture grammatical relations between constituents, is re-placed with a feature defined as follows: If there is a dependency in the predicate-argument structure of the CCG derivation between two words w and w 0 , the path feature from w to w 0 is defined as the lexical category of the functor, the argument slot i occupied by the argument, plus an arrow (← or →) to indicate whether w or w 0 is the categorial functor."
"For exam-ple, in our sentence “London denied plans on Mon-day”, the relation connecting the verb denied with plans is (S[dcl]\NP)/NP.2.←, with the left arrow indicating the lexical category included in the rela-tion is that of the verb, while the relation connecting denied with on is ((S\NP)\(S\NP))/NP.2.→, with the right arrow indicating the the lexical category in-cluded in the relation is that of the modifier."
"If the CCG derivation does not define a predicate-argument relation between the two words, we use the parse tree path feature described above, defined over the CCG derivation tree."
"In our training data, 77% of PropBank arguments corresponded directly to a relation in the CCG predicate-argument repre-sentation, and the path feature was used for the re- maining 23%."
Most of these mismatches arise be-cause the CCG parser and PropBank differ in their definition of head words.
"For instance, the CCG parser always assumes that the head of a PP is the preposition, whereas PropBank roles can be as-signed to the entire PP (7), or only to the NP argu-ment of the preposition (8), in which case the head word comes from the NP: (7) ... will be offered [PP ARGM-LOC in the U.S]."
"In embedded clauses, CCG assumes that the head is the complementizer, whereas in PropBank, the head comes from the embedded sentence itself."
"In com-plex verb phrases (eg. “might not have gone”), the CCG parser assumes that the first auxiliary (might) is head, whereas PropBank assumes it is the main verb (gone)."
"Therefore, CCG assumes that not mod-ifies might, whereas PropBank assumes it modi-fies gone."
"Although the head rules of the parser could in principle be changed to reflect more di-rectly the dependencies in PropBank, we have not attempted to do so yet."
"Further mismatches occur because the predicate-argument structure returned by the CCG parser only contains syntactic depen-dencies, whereas the PropBank data also contain some anaphoric dependencies, eg.: (9) [ ARG0 Realist ’s] negotiations to acquire Ammann Laser Technik AG... (10) When properly applied, [ ARG0 the adhesive] is designed to..."
"Such dependencies also do not correspond to a rela-tion in the predicate-argument structure of the CCG derivation, and cause the path feature to be used."
The phrase type feature is replaced with the lex-ical category of the maximal projection of the Prop-Bank argument’s head word in the CCG derivation tree.
"For example, the category of plans is N, and the category of denied is (S[dcl]\NP)/NP."
"The voice feature can be read off the CCG cate-gories, since the CCG categories of past participles carry different features in active and passive voice (eg. sold can be (S[pt]\NP)/NP or S[pss]\NP)."
The head word of a constituent is indicated in the derivations returned by the CCG parser.
We use data from the[REF_CITE]release of PropBank.
"The dataset contains annotations for 72,109 predicate-argument structures with 190,815 individual arguments (of which 75% are core, or numbered, arguments) and has includes examples from 2462 lexical predicates (types)."
Annotations from Sections 2 through 21 of the Treebank were used for training;[REF_CITE]was the test set.
Both parsers were trained on Sections 2 through 21.
"Because of the mismatch between the constituent structures of CCG and the Treebank, we score both systems according to how well they identify the head words of PropBank’s arguments."
"Table 2 gives the performance of the system on both PropBank’s core, or numbered, arguments, and on all PropBank roles including the adjunct-like ArgM roles."
"In order to analyze the impact of errors in the syntactic parses, we present results using features extracted from both automatic parser output and the gold standard parses in the Penn Treebank (without functional tags) and in CCGbank."
Using the gold standard parses pro- vides an upper bound on the performance of the sys-tem based on automatic parses.
"Since the Collins parser does not provide trace information, its up-per bound is given by the system tested on the gold-standard Treebank representation with traces removed."
"In Table 2, “core” indicates results on PropBank’s numbered arguments (ARG0...ARG5) only, and “all” includes numbered arguments as well as the ArgM roles."
Most of the numbered argu-ments (in particular ARG0 and ARG1) correspond to arguments that the CCG category of the verb di-rectly subcategorizes for.
"The CCG-based system outperforms the system based on the Collins parser on these core arguments, and has comparable perfor-mance when all PropBank labels are considered."
"We believe that the superior performance of the CCG system on this core arguments is due to its ability to recover long-distance dependencies, whereas we at-tribute its lower performance on non-core arguments mainly to the mismatches between PropBank and CCGbank."
The importance of long-range dependencies for our task is indicated by the fact that the performance on the Penn Treebank gold standard without traces is significantly lower than that on the Penn Treebank with trace information.
"Long-range dependencies are especially important for core arguments, shown by the fact that removing trace information from the Treebank parses results in a bigger drop for core arguments (83.5 to 76.3 F-score) than for all roles (74.1 to 70.2)."
"The ability of the CCG parser to re-cover these long-range dependencies accounts for its higher performance, and in particular its higher re-call, on core arguments."
The CCG gold standard performance is below that of the Penn Treebank gold standard with traces.
We believe this performance gap to be caused by the mismatches between the CCG analyses and the PropBank annotations described in Section 5.2.
"For the reasons described, the head words of the con-stituents that have PropBank roles are not necessar-ily the head words that stand in a predicate-argument relation in CCGbank."
"If two words do not stand in a predicate-argument relation, the CCG system takes recourse to the path feature."
"This feature is much sparser in CCG: since CCG categories encode sub-categorization information, the number of categories in CCGbank is much larger than that of Penn Tree-bank labels."
"Analysis of our system’s output shows that the system trained on the Penn Treebank gold standard obtains 55.5% recall on those relations that require the CCG path feature, whereas the system using CCGbank only achieves 36.9% recall on these."
"Also, in CCG, the complement-adjunct distinction is represented in the categories for the complement (eg. PP) or adjunct (eg. (S\NP)\(S\NP) and in the categories for the head (eg. (S[dcl]\NP)/PP or S[dcl]\NP)."
"In generating the CCGbank, various heuristics were used to make this distinction."
"In par-ticular, for PPs, it depends on the “closely-related” (CLR) function tag, which is known to be unreli- able."
"The decisions made in deriving the CCGbank often do not match the hand-annotated complement-adjunct distinctions in PropBank, and this inconsis-tency is likely to make our CCGbank-based features less predictive."
A possible solution is to regenerate the CCGbank using the Propbank annotations.
"The impact of our head-word based scoring is an-alyzed in Table 3, which compares results when only the head word must be correctly identified (as in Ta-ble 2) and to results when both the beginning and end of the argument must be correctly identified in the sentence (as[REF_CITE])."
"Even if the head word is given the correct label, the bound-aries of the entire argument may be different from those given in the PropBank annotation."
"Since con-stituents in CCGbank do not always match those in PropBank, even the CCG gold standard parses ob-tain comparatively low scores according to this met-ric."
This is exacerbated when automatic parses are considered.
"Our CCG-based system for automatically labeling verb arguments with PropBank-style semantic roles outperforms a system using a traditional Treebank-based parser for core arguments, which comprise 75% of the role labels, but scores lower on adjunct-like roles such as temporals and locatives."
"The CCG parser returns predicate-argument structures that in-clude long-range dependencies; therefore, it seems inherently better suited for this task."
"However, the performance of our CCG system is lowered by the fact that the syntactic analyses in its training corpus differ from those that underlie PropBank in impor-tant ways (in particular in the notion of heads and the complement-adjunct distinction)."
We would expect a higher performance for the CCG-based system if the analyses in CCGbank resembled more closely those in PropBank.
"Our results also indicate the importance of recov-ering long-range dependencies, either through the trace information in the Penn Treebank, or directly, as in the predicate-argument structures returned by the CCG parser."
"We speculate that much of the performance improvement we show could be ob-tained with traditional (ie. non-CCG-based) parsers if they were designed to recover more of the infor-mation present in the Penn Treebank, in particular the trace co-indexation."
An interesting experiment would be the application of our role-labeling sys-tem to the output of the trace recovery system[REF_CITE].
"Our results also have implications for parser evaluation, as the most frequently used constituent-based precision and recall measures do not evaluate how well long-range dependencies can be recovered from the output of a parser."
"Measures based on dependencies, such as those[REF_CITE]and[REF_CITE], are likely to be more rele-vant to real-world applications of parsing."
"Acknowledgments This work was supported by the In-stitute for Research in Cognitive Science at the University of Pennsylvania, the Propbank project (DoD[REF_CITE]-00C-2136), an EPSRC studentship and grant GR/[REF_CITE]and NSF[REF_CITE]456."
"We thank Mark Steedman, Martha Palmer and Alexandra Kinyon for their comments on this work."
In this paper we explore the variation of sentences as a function of the sentence number.
"We demonstrate that while the entropy of the sentence increases with the sentence number, it decreases at the para-graph boundaries in accordance with the Entropy Rate Constancy principle (intro-duced in related work)."
We also demon-strate that the principle holds for differ-ent genres and languages and explore the role of genre informativeness.
"We investi-gate potential causes of entropy variation by looking at the tree depth, the branch-ing factor, the size of constituents, and the occurrence of gapping."
"In many natural language processing applications, such as parsing or language modeling, sentences are treated as natural self-contained units."
Yet it is well-known that for interpreting the sentences the dis-course context is often very important.
"The later sentences in the discourse contain references to the entities in the preceding sentences, and this fact is often useful, e.g., in caching for language model-ing[REF_CITE]."
"The indirect influence of the context, however, can be observed even when a sen-tence is taken as a stand-alone unit, i.e., without its context."
"It is possible to distinguish between a set of earlier sentences and a set of later sentences with-out any direct comparison by computing certain lo-cal statistics of individual sentences, such as their entropy[REF_CITE]."
In this work we provide additional evidence for this hypothesis and investigate other sentence statistics.
"Entropy, as a measure of information, is often used in the communication theory."
"If humans have evolved to communicate in the most efficient way (some evidence for that is provided[REF_CITE]), then they would communicate in such a way that the entropy rate would be constant, namely, equal to the channel capacity[REF_CITE]."
In our previous work[REF_CITE]we propose that entropy rate is indeed con-stant in human communications.
"When read in con-text, each sentence would appear to contain roughly the same amount of information, per word, whether it is the first sentence or the tenth one."
"Thus the tenth sentence, when taken out of context, must ap-pear significantly more informative (and therefore harder to process), since it implicitly assumes that the reader already knows all the information in the preceding nine sentences."
"Indeed, the greater the sentence number, the harder to process the sentence must appear, though for large sentence numbers this would be very difficult to detect."
"This makes intu-itive sense: out-of-context sentences are harder to understand than in-context ones, and first sentences can never be out of context."
It is also demonstrated empirically through estimating entropy rate of vari-ous sentences.
In the first part of the present paper (Sections 2 and 3) we extend and further verify these results.
"In the second part (Section 4), we investigate the poten-tial reasons underlying this variation in complexity by looking at the parse trees of the sentences."
We also discuss how genre and style affect the strength of this effect.
In our previous work we demonstrate that the word entropy rate increases with the sentence number; we do it by estimating entropy of Wall Street Journal articles in Penn Treebank in three different ways.
"It may be the case, however, that this effect is corpus-and language-specific."
"To show that the Entropy Rate Constancy Principle is universal, we need to confirm it for different genres and different lan-guages."
We will address this issue in Section 3.
"Furthermore, if the principle is correct, it should also apply to the sentences numbered from the be-ginning of a paragraph, rather than from the begin-ning of the article, since in either case there is a shift of topic."
We will discuss this in Section 2.
"We have previously demonstrated (see[REF_CITE]for detailed derivation) that the conditional entropy of the ith word in the sentence (X i ), given its local context L i (the preceding words in the same sentence) and global context C i (the words in all preceding sentences) can be represented as"
"H(X i |C i , L i ) ="
"H(X i |L i ) − I(X i , C i |L i ) where H(X i |L i ) is the conditional entropy of the ith word given local context, and I(X i ,C i |L i ) is the conditional mutual information between the ith word and out-of-sentence context, given the local context."
"Since C i increases with the sentence num-ber, we will assume that, normally, it will provide more and more information with each sentence."
"This would cause the second term on the right to increase with the sentence number, and since H(X i |C i , L i ) must remain constant (by our assumption), the first term should increase with sentence number, and it had been shown to do so[REF_CITE]."
"Our assumption about the increase of the mutual information term is, however, likely to break at the paragraph boundary."
"If there is a topic shift at the boundary, the context probably provides more infor-mation to the preceding sentence, than it does to the new one."
"Hence, the second term will decrease, and so must the first one."
In the next section we will verify this experimen-tally.
We use the Wall Street Journal text (years 1987- 1989) as our corpus.
"We take all articles that con-tain ten or more sentences, and extract the first ten sentences."
Then we: [Footnote_1]. Group extracted sentences according to their sentence number into ten sets of 49559 sen-tences each. 2.
"1 First sentences are, of course, all paragraph-starting."
"Separate each set into two subsets, paragraph-starting and non-paragraph-starting sentences 1 . 3."
We use a simple smoothed trigram language model:
"P (x i |x 1 . . . x i−1 ) ≈ P (x i |x i−2 x i−1 ) = λ 1 P̂ (x i |x i−2 x i−1 ) + λ 2 P̂ (x i |x i−1 ) + (1 − λ 1 − λ 2 )P̂ (x i ) where λ 1 and λ [Footnote_2] are the smoothing coefficients 2 , and P̂ is a maximum likelihood estimate of the cor-responding probability, e.g.,"
"2 We have arbitrarily chosen the smoothing coefficients to be 0.5 and 0.3, correspondingly."
C(x i−2 x i−1 x i ) P̂ (x i |x i−2 x i−1 ) =
C(x i−2 x i−1 ) where C(x i . . . x j ) is the number of times this se-quence appears in the training data.
"We then evaluate the resulting model on each of the testing sets, computing per-word entropy of the set:"
Ĥ(X) = 1 X log P (x i |x i−2 x i−1 ) |X| x i ∈X
"As outlined above, we have ten testing sets, one for each sentence number; each set (except for the first) is split into two subsets: sentences that start a para-graph, and sentences that do not."
"The results for full sets, paragraph-starting subsets, and non-paragraph-starting subsets are presented in Figure 1."
"First, we can see that the the entropy for full sets (solid line) is generally increasing."
This re-sult corresponds to the previously discussed effect of entropy increasing with the sentence number.
"We also see that for all sentence numbers the paragraph-starting sentences have lower entropy than the non-paragraph-starting ones, which is what we intended to demonstrate."
"In such a way, the paragraph-starting sentences are similar to the first sentences, which makes intuitive sense."
"All the lines roughly show that entropy increases with the sentence number, but the behavior at the second and the third sentences is somewhat strange."
"We do not yet have a good explanation of this phe-nomenon, except to point out that paragraphs that start at the second or third sentences are probably not “normal” because they most likely do not indi-cate a topic shift."
Another possible explanation is that this effect is an artifact of the corpus used.
"We have also tried to group sentences based on their sentence number within paragraph, but were unable to observe a significant effect."
"This may be due to the decrease of this effect in the later sen-tences of large articles, or perhaps due to the relative weakness of the effect [Footnote_3] ."
"3 We combine into one set very heterogeneous data: both 1st and 51st sentence might be in the same set, if they both start a paragraph. The experiment in Section 2.2 groups only the paragraph-starting sentences with the same sentence number."
All the work on this problem so far has focused on the Wall Street Journal articles.
The results are thus naturally suspect; perhaps the observed effect is simply an artifact of the journalistic writing style.
"To address this criticism, we need to perform com-parable experiments on another genre."
"Wall Street Journal is a fairly prototypical exam-ple of a news article, or, more generally, a writing with a primarily informative purpose."
One obvious counterpart of such a genre is fiction [Footnote_4] .
"4 We use prose rather than poetry, which presumably is even less informative, because poetry often has superficial con-straints (meter); also, it is hard to find a large homogeneous poetry collection."
Another al-ternative might be to use transcripts of spoken dia-logue.
"Unfortunately, works of fiction, are either non-homogeneous (collections of works) or relatively short with relatively long subdivisions (chapters)."
"This is crucial, since in the sentence number experi-ments we obtain one data point per article, therefore it is impossible to use book chapters in place of arti-cles."
"For our experiments we use War and Peace[REF_CITE], since it is rather large and publicly avail-able."
It contains only about 365 rather long chap-ters [Footnote_5] .
"5 For comparison, Penn Treebank contains over 2400 (much shorter) WSJ articles."
"Unlike WSJ articles, each chapter is not writ-ten on a single topic, but usually has multiple topic shifts."
"These shifts, however, are marked only as paragraph breaks."
"We, therefore, have to assume that each paragraph break represents a topic shift, and treat each paragraph as being an equivalent of a WSJ article, even though this is obviously subopti-mal."
The experimental setup is very similar to the one used in Section 2.2.
"We use roughly half of the data for training purposes and split the rest into testing sets, one per each sentence number, counted from the beginning of a paragraph."
We then evaluate the results using the same method as in Section 2.2.
"We expect that the en-tropy would increase with the sentence number, just as in the case of the sentences numbered from the article boundary."
"This effect is present, but is not very pronounced."
"To make sure that it is statistically significant, we also do 1000 control runs for com-parison, with paragraph breaks inserted randomly at the appropriate rate."
The results (including 3 ran-dom runs) can be seen in Figure 2.
"To make sure our results are significant we compare the correla-tion coefficient between entropy and sentence num-ber to ones from simulated runs, and find them to be significant (P=0.016)."
"It is fairly clear that the variation, especially be-tween the first and the later sentences, is greater than it would be expected for a purely random oc-currence."
We will see further evidence for this in the next section.
"To further verify that this effect is significant and universal, it is necessary to do similar experiments in other languages."
"Luckily, War and Peace is also digitally available in other languages, of which we pick Russian and Spanish for our experiments."
We follow the same experimental procedure as in Section 3.1.2 and obtain the results for Russian (Fig-ure 3(a)) and Spanish (Figure 3(b)).
We see that re-sults are very similar to the ones we obtained for English.
The results are again significant for both Russian (P=0.004) and Spanish (P=0.028).
We have established that entropy increases with the sentence number in the works of fiction.
"We ob-serve, however, that the effect is smaller than re-ported in our previous work[REF_CITE]for Wall Street Journal articles."
"This is to be expected, since business and news writing tends to be more structured and informative in nature, grad-ually introducing the reader to the topic."
"Context, therefore, plays greater role in this style of writing."
To further investigate the influence of genre and style on the strength of the effect we perform exper-iments on data from British National Corpus[REF_CITE]which is marked by genre.
"For each genre, we extract first ten sentences of each genre subdivision of ten or more sentences. 90% of this data is used as training data and 10% as testing data."
"Testing data is separated into ten sets: all the first sentences, all the second sentences, and so on."
We then use a trigram model trained on the training data set to find the average per-word en-tropy for each set.
"We obtain ten numbers, which in general tend to increase with the sentence num-ber."
"To find the degree to which they increase, we compute the correlation coefficient between the en-tropy estimates and the sentence numbers."
We report these coefficients for some genres in Table 1.
"To en-sure reliability of results we performed the described process 400 times for each genre, sampling different testing sets."
The results are very interesting and strongly sup-port our assumption that informative and struc-tured (and perhaps better-written) genres will have stronger correlations between entropy and sentence number.
"There is only one genre, tabloid newspa-pers [Footnote_6] , that has negative correlation."
"6 Perhaps, in this case the readers are only expected to look at the headlines."
"The four gen-res with the smallest correlation are all quite non-informative: tabloids, popular magazines, advertise-ments [Footnote_7] and poetry."
"7 Advertisements could be called informative, but they tend to be sets of loosely related sentences describing various fea-tures, often in no particular order."
Academic writing has higher correlation coefficients than non-academic.
"Also, humanities and social sciences writing is probably more structured and better stylistically than science and engineering writing."
"At the bottom of the table we have genres which tend to be produced by pro-fessional writers (biography), are very informative (TV news feed) or persuasive and rhetorical (parlia-mentary proceedings)."
"We have demonstrated that paragraph boundaries of-ten cause the entropy to decrease, which seems to support the Entropy Rate Constancy principle."
"The effects are not very large, perhaps due to the fact that each new paragraph does not necessarily rep-resent a shift of topic."
"This is especially true in a medium like the Wall Street Journal, where articles are very focused and tend to stay on one topic."
"In fiction, paragraphs are often used to mark a topic shift, but probably only a small proportion of para-graph breaks in fact represents topic shifts."
"We also observed that more informative and structured writ-ing is subject to stronger effect than speculative and imaginative one, but the effect is present in almost all writing."
In the next section we will discuss the potential causes of the entropy results presented both in the preceding and this work.
In our previous work we discuss potential causes of the entropy increase.
We find that both lexical (which words are used) and non-lexical (how the words are used) causes are present.
In this section we will discuss possible non-lexical causes.
We know that some non-lexical causes are present.
The most natural way to find these causes is to examine the parse trees of the sentences.
"There-fore, we collect a number of statistics on the parse trees and investigate if any statistics show a signifi-cant change with the sentence number."
We use the whole Penn Treebank corpus[REF_CITE]as our data set.
This corpus contains about 50000 parsed sentences.
Many of the statistics we wish to compute are very sensitive to the length of the sentence.
"For example, the depth of the tree is almost linearly related to the sentence length."
This is important because the aver-age length of the sentence varies with the sentence number.
"To make sure we exclude the effect of the sentence length, we need to normalize for it."
We proceed in the following way.
"Let T be the set of trees, and f : T → R be some statistic of a tree."
"Let l(t) be the length of the underlying sentence for tree t. Let L(n) = {t|l(t) = n} be the set of trees of size n. Let L f (n) be defined as |L(1n)| P t∈L(n) f(t), the average value of the statistic f on all sentences of length n."
"We then define the sentence-length-adjusted statistic, for all t, as f 0 (t) = f(t)"
L f (l(t))
"The average value of the adjusted statistic is now equal to 1, and it is independent of the sentence length."
"We can now report the average value of each statistic for each sentence number, as we have done before, but instead we will group the sentence num-bers into a small number of “buckets” of exponen-tially increasing length [Footnote_8] ."
8 For sentence number n we compute the bucket number as blog 1.5 nc
"We do so to capture the behavior for all the sentence numbers, and not just for the first ten (as before), as well as to lump to-gether sentences with similar sentence numbers, for which we do not expect much variation."
The first statistic we consider is also the most nat-ural: tree depth.
The results can be seen in Figure 4.
"In the first part of the graph we observe an in-crease in tree depth, which is consistent with the in-creasing complexity of the sentences."
"In the later sentences, the depth decreases slightly, but still stays above the depth of the first few sentences."
"Another statistic we investigate is the average branching factor, defined as the average number of children of all non-leaf nodes in the tree."
"It does not appear to be directly correlated with the sentence length, but we normalize it to make sure it is on the same scale, so we can compare the strength of re-sulting effect."
"Again, we expect lower entropy to correspond to flatter trees, which corresponds to large branching factor."
"Therefore we expect the branching factor to decrease with the sentence number, which is indeed what we observe (Figure 5, solid line)."
Each non-leaf node contributes to the average branching factor.
"It is likely, however, that the branching factor changes with the sentence num-ber for certain types of nodes only."
The most obvi-ous contributors for this effect seem to be NP (noun phrase) nodes.
"Indeed, one is likely to use several words to refer to an object for the first time, but only a few words (even one, e.g., a pronoun) when refer-ring to it later."
"We verify this intuitive suggestion, by computing the branching factor for NP, VP (verb phrase) and PP (prepositional phrase) nodes."
"Only NP nodes show the effect, and it is much stronger (Figure 5, dashed line) than the effect for the branch- ing factor."
"Furthermore, it is natural to expect that most of this effect arises from base NPs, which are defined as the NP nodes whose children are all leaf nodes."
"Indeed, base NPs show a slightly more pronounced effect, at least with regard to the first sentence (Fig-ure 5, dotted line)."
"We need to determine whether we have accounted for all of the branching factor effect, by proposing that it is simply due to decrease in the size of the base NPs."
"To check, we compute the average branching factor, excluding base NP nodes."
"By comparing the solid line in Figure 6 (the origi-nal average branching factor result) with the dashed line (base NPs excluded), you can see that base NPs account for most, though not all of the effect."
"It seems, then, that this problem requires further in-vestigation."
Another potential reason for the increase in the sen-tence complexity might be the increase in the use of gapping.
We investigate whether the number of the ellipsis constructions varies with the sentence num-ber.
We again use Penn Treebank for this experi- ment 9 .
"As we can see from Figure 7, there is indeed a sig-nificant increase in the use of ellipsis as the sentence number increases, which presumably makes the sen-tences more complex."
"Only about 1.5% of all the sentences, however, have gaps."
We have discovered a number of interesting facts about the variation of sentences with the sentence number.
It has been previously known that the com-plexity of the sentences increases with the sentence number.
We have shown here that the complexity tends to decrease at the paragraph breaks in accor-dance with the Entropy Rate Constancy principle.
We have verified that entropy also increases with the sentence number outside of Wall Street Journal do-main by testing it on a work of fiction.
We have also verified that it holds for languages other than En-glish.
We have found that the strength of the effect depends on the informativeness of a genre.
"We also looked at the various statistics that show a significant change with the sentence number, such as the tree depth, the branching factor, the size of noun phrases, and the occurrence of gapping."
"Unfortunately, we have been unable to apply these results successfully to any practical problem so far, primarily because the effects are significant on av-erage and not in any individual instances."
Finding applications of these results is the most important direction for future research.
"Also, since this paper essentially makes state-ments about human processing, it would be very ap-propriate to to verify the Entropy Rate Constancy principle by doing reading time experiments on hu-man subjects."
We would like to acknowledge the members of the Brown Laboratory for Linguistic Information Pro-cessing and particularly Mark Johnson for many useful discussions.
This research has been supported in part by NSF grants[REF_CITE]and[REF_CITE].
"This paper compares a range of methods for classifying words based on linguis-tic diagnostics, focusing on the task of learning countabilities for English nouns."
"We propose two basic approaches to feature representation: distribution-based representation, which simply looks at the distribution of features in the cor-pus data, and agreement-based represen-tation which analyses the level of token-wise agreement between multiple pre-processor systems."
"We additionally com-pare a single multiclass classifier archi-tecture with a suite of binary classifiers, and combine analyses from multiple pre-processors."
"Finally, we present and evalu-ate a feature selection method."
"Lexical acquisition can be described as the process of populating a grammar skeleton with lexical items, through a process of mapping word lemmata onto lexical types described in the grammar."
"Depending on the linguistic precision of the base grammar, lex-ical acquisition can range in complexity from sim-ple part-of-speech tagging (shallow lexical acquisi-tion) to the acquisition of selectionally-constrained subcategorisation frame clusters or constructional compatibilities (deep lexical acquisition)."
Our par-ticular interest is in the latter task of deep lexical acquisition with respect to English nouns.
"We are interested in developing learning tech-niques for deep lexical acquisition which take a fixed set of linguistic diagnostics, and classify words ac-cording to corpus data."
"We propose a range of gen-eral techniques for this task, as exemplified over the task of English countability acquisition."
"Countabil-ity is the syntactic property that determines whether a noun can take singular and plural forms, and af-fects the range of permissible modifiers."
"Many nouns have both countable and uncountable lemmas, with differences in meaning: I submitted two papers “documents” (countable) vs. Please use white paper “substance to be written on” (uncountable)."
"This research complements that described[REF_CITE], where we present the lin-guistic foundations and features drawn upon in the countability classification task, and motivate the claim that countability preferences can be learned from corpus evidence."
"In this paper, we focus on the methods used to tackle the task of countability classification based on this fixed feature set."
The remainder of this paper is structured as fol-lows.
"Section 2 outlines the countability classes, resources and pre-processors."
Section 3 presents two methods of representing the feature space.
"Sec-tion 4 details the different classifier designs and the dataset, which are then evaluated in Section 5."
"Fi-nally, we conclude the paper with a discussion in Section 6."
"In this section, we describe the countability classes, the resources used in this research, and the feature extraction method."
These are described in greater detail[REF_CITE].
"Nouns are classified as belonging to one or more of four possible classes: countable, uncountable, plural only and bipartite."
"Countable nouns can be modi-fied by denumerators, prototypically numbers, and have a morphologically marked plural form: one dog, two dogs."
"Uncountable nouns cannot be mod-ified by denumerators, but can be modified by un-specific quantifiers such as much; they do not show any number distinction (prototypically being singu-lar): *one equipment, some equipment, *two equip-ments."
"Plural only nouns only have a plural form, such as goods, and cannot be either denumerated or modified by much; many plural only nouns, such as clothes, use the plural form even as modifiers: a clothes horse."
"Bipartite nouns are plural when they head a noun phrase (trousers), but generally singu-lar when used as a modifier (trouser leg); they can be denumerated with the classifier pair: a pair of scissors."
Information about noun countability was obtained from two sources: COMLEX 3.0[REF_CITE]and the common noun part of ALT-J/E ’s Japanese-to-English semantic transfer dictio-nary[REF_CITE].
"Of the approximately 22,000 noun entries in COMLEX , 13,622 are marked as countable , 710 as uncountable and the remainder are unmarked for countability."
"ALT-J/E has 56,245 English noun types with distinct countability."
"Features used in this research are divided up into feature clusters, each of which is conditioned on the occurrence of a target noun in a given construc-tion."
"Feature clusters are either one-dimensional (describing a single multivariate feature) or two-dimensional (describing the interaction between two multivariate features), with each dimension describ-ing a lexical or syntactic property of the construc-tion in question."
"An example of a one-dimensional feature cluster is head noun number, i.e. the num-ber ( singular or plural ) of the target noun when it oc-curs as the head of an NP; an example of a two-dimensional feature cluster in subject–verb agree-ment, i.e. the number ( singular or plural ) of the tar-get noun when it occurs as head of a subject NP vs. number agreement on the verb ( singular or plu-ral )."
"Below, we provide a basic description of the 10 feature clusters used in this research and their di-mensionality ( [x] =1-dimensional feature cluster with x unit features, [x×y] =2-dimensional feature cluster with x × y unit features)."
These represent a total of 206 unit features.
Head noun number: [] the number of the target noun when it heads an NP
Modifier noun number: [] the number of the target noun when a modifier in an NP
Subject–verb agreement: [×] the number of the target noun in a subject position vs. number agreement on the governing verb
Coordinate noun number: [×] the number of the target noun vs. the number of the head nouns of conjuncts
"N of N constructions: [×] the type of the N  (e.g. COLLECTIVE , TEMPORAL ) vs. the number of the target noun (N  ) in an N  of N  construction Occurrence in PPs: [×] the preposition type vs. the presence or absence of a determiner when the target noun occurs in singular form in a PP Pronoun co-occurrence: [×] what personal, pos-sessive and reflexive pronouns (e.g. he, their, itself) occur in the same sentence as singular and plural instances of the target noun"
"Singular determiners: [] what singular-selecting determiners (e.g. a, much) occur in NPs headed by the target noun in singular form"
"Plural determiners: [] what plural-selecting de-terminers (e.g. many, various) occur in NPs headed by the target noun in plural form"
"Non-bounded determiners: [×] what non-bounded determiners (e.g. more, sufficient) occur in NPs headed by the target noun, and what is the number of the target noun for each"
"The values for the features described above were ex-tracted from the written component of the British National Corpus (BNC,[REF_CITE]) using three different pre-processors: (a) a POS tagger, (b) a full-text chunker and (c) a dependency parser."
"These are used independently to test the efficacy of the differ-ent systems at capturing features used in the clas-sification process, and in tandem to consolidate the strengths of the individual methods."
"With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger[REF_CITE]trained over the Brown and WSJ cor-pora and based on the Penn POS tagset."
We then lemmatised this data using a Penn tagset-customised version of morph[REF_CITE].
"Finally, we implemented a range of high-precision, low-recall POS-based templates to extract out the features from the processed data."
"For the chunker, we ran fnTBL over the lem-matised tagged data, training[REF_CITE]-style (Tjong[REF_CITE]) chunk-converted versions of the full Brown and WSJ cor-pora."
"For the NP-internal features (e.g. determin-ers, head number), we used the noun chunks directly, or applied POS-based templates locally within noun chunks."
"For inter-chunk features (e.g. subject–verb agreement), we looked at only adjacent chunk pairs so as to maintain a high level of precision."
"We read dependency tuples directly off the output of RASP[REF_CITE]in grammati-cal relation mode. [Footnote_1] RASP has the advantage that re-call is high, although precision is potentially lower than chunking or tagging as the parser is forced into resolving phrase attachment ambiguities and com-mitting to a single phrase structure analysis."
"1 We used the first parse in the experiments reported here. An alternative method would be to use weighted dependency tuples, as described[REF_CITE]."
"After generating the different feature vectors for each noun based on the above configurations, we fil-tered out all nouns which did not occur at least 10 times in NP head position in the output of all three systems."
"This resulted in a total of 20,530 nouns, of which 9,031 are contained in the combined COM-LEX and ALT-J/E lexicons."
"The evaluation is based on these 9,031 nouns."
"We test two basic feature representations in this re-search: distribution-based, which simply looks at the relative occurrence of different features in the corpus data, and agreement-based, which analyses the level of token-wise agreement between multiple systems."
"In the distribution-based feature representation, we take each target noun in turn and compare its amal-gamated value for each unit feature with (a) the val-ues for other target nouns, and (b) the value of other unit features within that same feature cluster."
"That is, we focus on the relative prominence of features globally within the corpus and locally within each feature cluster."
"In the case of a one-dimensional feature cluster (e.g. singular determiners), each unit feature f s for target noun w is translated into 3 separate feature values: freq(f s |w) corpfreq(f s ,w) = (1) freq(∗) freq(f s |w) wordfreq(f s ,w) = (2) freq(w) featfreq(f s ,w) ="
P freq(f s |w) i (3) i freq(f i |w) where freq(∗) is the frequency of all words in the cor-pus.
"That is, for each unit feature we capture the rel-ative corpus frequency, frequency relative to the tar-get word frequency, and frequency relative to other features in the same feature cluster."
"Thus, for an n-valued one-dimensional feature cluster, we generate 3n independent feature values."
"In the case of a two-dimensional feature ma-trix (e.g. subject-position noun number vs. verb number agreement), each unit feature f s,t for tar-get noun w is translated into corpfreq(f s,t ,w), wordfreq(f s,t ,w) and featfreq(f s,t ,w) as above, and 2 additional feature values: featdimfreq  (f s,t ,w) ="
"P freq(f s,t |w) (4) i freq(f i,t |w) featdimfreq  (f s,t ,w) ="
"P freq(f s,t |w) (5) j freq(f s,j |w) which represent the featfreq values calculated along each of the two feature dimensions."
"Additionally, we calculate cumulative totals for each row and column of the feature matrix and describe each as for the one-dimensional features above (in the form of 3 values)."
"Thus, for an m × n-valued two-dimensional feature cluster, we generate a total of 5mn + 3(m + n) independent feature values."
The feature clusters produce a combined total of 1284 individual feature values.
The agreement-based feature representation con-siders the degree of token agreement between the features extracted using the three different pre-processors.
This allows us to pinpoint the reliable di-agnostics within the corpus data and filter out noise generated by the individual pre-processors.
"It is possible to identify the features which are positively-correlated with a unique countability class (e.g. occurrence of a singular noun with the determiner a occurs only for countable nouns), and for each to determine the token-level agreement be-tween the different systems."
"The number of diagnos-tics considered for each of the countability classes is: 32 for countable nouns, 19 for uncountable nouns and 1 for each of plural only and bipartite nouns."
The total number of diagnostics we test agreement across is thus 53.
"The token-level correlation for each feature f s is calculated fourfold according to relative agreement, the κ statistic, correlated frequency and correlated weight."
"The relative agreement between systems sys  and sys  wrt f s for target noun w is defined to be: |tok (fs,w) (sys  ) ∩ tok (fs,w) (sys  )| agr (fs,w) (sys  ,sys  ) = |tok (fs,w) (sys  ) ∪ tok (fs,w) (sys  )| where tok (f s ,w) (sys i ) returns the set of token in-stances of (f s , w)."
The κ statistic[REF_CITE]is recast as:
"P agr(fs,∗)(sys,sys) agr (fs,w) (sys  P ,sys  ) − N κ (fs,w) (sys  ,sys  ) =  − agr(fs,∗)(sys,sys)N"
"In this modified form, κ (f s ,w) represents the diver-gence in relative agreement wrt f s for target noun w, relative to the mean relative agreement wrt f s over all words."
"Correlated frequency is defined to be: |tok (fs,w) (sys  ) ∩ tok (fs,w) (sys  )| cfreq (fs,w) (sys  ,sys  ) = freq(w)"
"It describes the occurrence of tokens in agreement for (f s , w) relative to the total occurrence of the tar-get word."
The metrics are used to derive three separate fea-ture values for each diagnostic over the three pre-processor system pairings.
We additionally calcu-late the mean value of each metric across the system pairings and the overall correlated weight for each countability class C as:
"P cw (C,w) (sys  ,sys  ) ="
"P fs∈C |tok (fs,w) (sys  ) ∩ tok (fs,w) (sys  )| i |tok (fi,w) (sys  ) ∩ tok (fi,w) (sys  )|"
Correlated weight describes the occurrence of corre-lated features in the given countability class relative to other correlated features.
"We test agreement: (a) for each of these diag-nostics individually and within each countability class (Agree(Token,∗)), and (b) across the amalgam of diagnostics for each of the countability classes (Agree(Class,∗))."
"For Agree(Token,∗), we calculate agr, κ and cfreq values for each of the 53 diag-nostics across the [Footnote_3] system pairings, and addition-ally calculate the mean value for each value."
"3 We additionally experimented with the kernel-based TinySVM system, but found TiMBL to be the marginally supe-rior performer in all cases, a somewhat surprising result given the high-dimensionality of the feature space."
We additionally calculate the overall cw value for each countability class.
This results in a total of 640 fea-ture values (3 × 53 × 3 + 53 × 3 + 4).
"In the case of Agree(Class,∗), we average the agr, κ and cfreq values across each countability class for each of the three system pairings, and also calculate the mean value in each case."
"We further calculate the overall cw value for each countability class, culminating in 52 feature values (3 × 4 × 3 + 4 × 3 + 4)."
"Below, we outline the different classifiers tested and describe the process used to generate the gold-standard data."
"We propose a variety of unsupervised and super-vised classifier architectures for the task of learning countability, and also a feature selection method."
"In all cases, our classifiers are built using TiMBL ver-sion 4.2[REF_CITE], a memory-based classification system based on the k-nearest neigh-bour algorithm."
"As a result of extensive parame-ter optimisation, we settled on the default configu-ration [Footnote_2] for TiMBL with k set to 9. 3"
"2 IB1 with weighted overlap, gain ratio-based feature weighting and equal weighting of neighbours."
Training data was generated independently for the SINGLE and SUITE classifiers.
"In each case, we first extracted all countability-annotated nouns from each of the ALT-J/E and COMLEX lexicons which are at-tested at least 10 times in the BNC, and composed the training data from these pre-filtered sets."
"In the case of the SINGLE classifier, we simply classified words according to the union of all countabilities from ALT-J/E and COMLEX , resulting in the follow-ing dataset:"
"From this, it is evident that some class combinations (e.g. plural only + bipartite ) are highly infrequent, hint-ing at a problem with data sparseness."
"For the SUITE classifier, we generate the positive exemplars for the countable and uncountable classes from the intersection of the COMLEX and ALT-J/E data for that class; negative exemplars, on the other hand, are those not annotated as belonging to that class in either lexicon."
"With the plural only and bipartite data, COMLEX cannot be used as it does not describe these two classes."
"We thus took all members of each class listed in ALT-J/E as our pos-itive exemplars, and all remaining nouns with non-identical singular and plural forms as negative ex-emplars."
This resulted in the following datasets:
"Evaluation of the supervised classifiers was carried out based on 10-fold stratified cross-validation over the relevant dataset, and results presented here are averaged over the 10 iterations."
Classifier perfor-mance is rated according to classification accuracy (the proportion of instances classified correctly) and F-score (β = 1).
"In the case of the SINGLE classifier, the class-wise F-score is calculated by decomposing the multiclass labels into their components."
"A count-able+uncountable instance misclassified as countable , for example, would count as a misclassification in terms of classification accuracy, a correct classifica-tion in the calculation of the countable F-score, and a misclassification in the calculation of the uncountable F-score."
"Note that the SINGLE classifier is run over a different dataset to each member of the SUITE clas-sifier, and cross-comparison of the classification ac-curacies is not representative of the relative system performance (classification accuracies for the SIN - GLE classifier are given in parentheses to reinforce this point)."
"Classification accuracies are thus simply used for classifier comparison within a basic classi-fier architecture ( SINGLE or SUITE ), and F-score is the evaluation metric of choice for overall evalua-tion."
We present the results for two baseline systems for each countability class: a majority-class clas-sifier and the unsupervised method.
"The Majority class system is run over the binary data used by the SUITE classifier for the given class, and sim-ply classifies all instances according to the most commonly-attested class in that dataset."
"Irrespective of the majority class, we calculate the F-score based on a positive-class classifier, i.e. a classifier which naively classifies each instance as belonging to the given class; in the case that the positive class is not the majority class, the F-score is given in parenthe-ses."
"The results for the different system configurations over the four countability classes are presented in Tables 1–4, in which the highest classification accu-racy and F-score values for each class are presented in boldface."
"The classifier Dist(All CON , SUITE ), for example, applies the distribution-based feature rep-resentation in a SUITE classifier configuration (i.e. it tests for binary membership in each countability class), using the concatenated feature vectors from each of the tagger, chunker and RASP."
"Items of note in the results are: • all system configurations surpass both the majority-class baseline and unsupervised clas-sifier in terms of F-score • for all other than bipartite nouns, the SUITE classifier outperforms the SINGLE classifier in terms of F-score • the best of the distribution-based classifiers was, without exception, superior to the best of the agreement-based classifiers • chunk-based feature extraction generally pro-duced superior performance to POS tag-based feature extraction, which was in turn gener-ally better than RASP-based feature extraction; statistically significant differences in F-score (based on the two-tailed t-test, p &lt; .05) were observed for both chunking and tagging over RASP for the plural only class, and chunking over RASP for the countable class • for the SUITE classifier, system combination by either concatenation (Dist(All CON , SUITE )) or averaging over the individual feature val-ues (Dist(All MEAN , SUITE )) generally led to a statistically significant improvement over each of the individual systems for the countable and uncountable classes, 4 but there was no statistical difference between these two archi-tectures for any of the 4 countability classes; for the SINGLE classifier, system combination (Dist(All CON , SUITE )) did not lead to a signifi-cant performance gain"
"To evaluate the effects of feature selection, we graphed the F-score value and processing time (in instances processed per second [Footnote_5] ) over values of N from 25 to the full feature set."
5 As evaluated on an[REF_CITE]+ CPU with 3GB of memory.
"We targeted the Dist(All CON , SUITE ) system for evaluation (3852 features), and ran it over both the countable and un-countable classes. [Footnote_6] We additionally carried out ran-dom feature selection as a baseline to compare the feature selection results against."
6 We focus exclusively on countable and uncountable nouns here and in the remainder of supplementary evaluation as these are by far the most populous countability classes.
"Note that the x-axis (N) and right y-axis (instances/sec) are both log-arithmic, such that the linear right-decreasing time curves are indicative of the direct proportionality be-tween the number of features and processing time."
The differential in F-score for the best-N configura-tion as compared to the full feature set is statistically insignificant for N &gt; 100 for countable nouns and N &gt; 50 for uncountable nouns.
"That is, feature se-lection facilitates a relative speed-up of around 30× without a significant drop in F-score."
"Comparing the results for the best-N and rand-N features, the dif-ference in F-score was statistically significant for all values of N &lt; 1000."
"The proposed method of fea-ture selection thus allows us to maintain the full clas-sification potential of the feature set while enabling a speedup greater than an order of magnitude, po-tentially making the difference in practical utility for the proposed method."
"To determine the relative impact of the com-ponent feature values on the performance of the distribution-based feature representation, we used the Dist(All MEAN , SUITE ) configuration to build: (a) a classifier using a single binary value for each unit feature, based on simple corpus occurrence (Bi-nary); and (b) 3 separate classifiers based on each of the corpfreq, wordfreq and featfreq features values only (without the 2D feature cluster totals)."
"In each case, the total number of feature values is 206."
"The results for each of these classifiers over countable and uncountable nouns are pre-sented in Table 5, as compared to the basic Dist(All MEAN , SUITE ) classifier with all 1,284 features (All features) and also the best-200 features[REF_CITE]."
Results which differ from those for All features to a level of statistical significance are asterisked.
"The binary classifiers performed signif-icantly worse than All features for both countable and uncountable nouns, underlining the utility of the distribution-based feature representation. wordfreq is marginally superior to corpfreq as a standalone feature representation, and both of these were on the whole slightly below the full feature set in performance (although no significant difference was observed). featfreq performed slightly worse again, significantly below the level of the full feature set."
"Results for the best-200 classifier were marginally higher than those for each of the individual feature representations in the case of the countable class, but marginally below the results for corpfreq and wordfreq in the case of the uncountable class."
"The differences here are not statistically significant, and additional evaluation is required to determine the relative success of feature selection over simply using wordfreq values, for example."
There have been at least three earlier approaches to the automatic determination of countability: two using semantic cues and one using cor- pora.
"O’[REF_CITE]implemented a sim-ilar approach using the much larger Cyc on-tology and achieved 89.5% accuracy, mapping onto the 2 classes of countable and uncount-able."
Our results compare favourably with each of these.
"In a separate evaluation, we took the best-performing classifier (Dist(All CON , SUITE )) and ran it over open data, using best-500 feature selecti[REF_CITE]."
"The output of the classifier was evaluated relative to hand-annotated data, and the level of agreement found to be around 92.4%, which is approximately equivalent to the agreement between COMLEX and ALT-J/E of 93.8%."
"In conclusion, we have presented a plethora of learning techniques for deep lexical acquisition from corpus data, and applied each to the task of classify-ing English nouns for countability."
"We specifically compared two feature representations, based on rel-ative feature occurrence and token-level classifica-tion, and two basic classifier architectures, using a suite of binary classifiers and a single multi-class classifier."
"We also analysed the effects of comb-ing the output of multiple pre-processors, and pre-sented a simple feature selection method."
"Overall, the best results were obtained using a distribution-based suite of binary classifiers combining the out-put of multiple pre-processors."
@8 [^ C[ ¡ 8¢ nn ¢¤£I¥  ¢   ¢  ¢ ¦ ¢ 3§ £ n ¡ ¨ ©  ª  ª K  «    ª ¢  ¢ ( ¡   ª ¦F¬®­ ¢ [ ¯ fK °8% KnK=  Kn ¢ B° ¢  © ¢ ± © ¢  %K²    ¢ ^ n ¡ ¢µ´ K @Y³¯ ¢ n ¢   ¢  ¢ X ¢ 3§¶% ¥   w·§¸% ¢ ª ©7©  ¥ · ¢ ®B~¼» £  ¥ BY³-8¤  ¡ ¬½@@  ©BeD ©8¾ ¯  ¢ ^ ¢   ¢ B%%nKnK`nKnnY¯F[¯F ¢ ©BY¯3B6À¥ n¢ !  K¢ ¦[l¢ ¿§ ^© ¢ ©Á¥  ¡ ¤eÂf (¥ Kºg ª ¥§¾ ¢ ¨¥  K¬n-%[ 
ÈÄ ©BKÅ  EÆN9&gt;  ^;/;P23ÆÇ!&gt; n· ¢ 8 8 ¢
We propose the use of Lexicalized Tree Adjoining Grammar (LTAG) as a source of features that are useful for reranking the output of a statistical parser.
"In this paper, we extend the notion of a tree ker-nel over arbitrary sub-trees of the parse to the derivation trees and derived trees pro-vided by the LTAG formalism, and in ad-dition, we extend the original definition of the tree kernel, making it more lexi-calized and more compact."
We use LTAG based features for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0%[REF_CITE]of Penn Treebank for sentences of length ≤ 100 words.
Our results show that the use of LTAG based tree kernel gives rise to a 17% relative difference in f-score im-provement over the use of a linear kernel without LTAG based features.
Recent work in statistical parsing has explored al-ternatives to the use of (smoothed) maximum likeli-hood estimation for parameters of the model.
"These alternatives are distribution-free[REF_CITE], providing a discriminative method for resolving parse ambiguity."
"Discriminative methods provide a ranking between multiple choices for the most plau-sible parse tree for a sentence, without assuming that a particular distribution or stochastic process gener-ated the alternative parses."
Discriminative methods permit the use of feature functions that can be used to condition on arbitrary aspects of the input.
This flexibility makes it possi-ble to incorporate features of various of kinds.
"Fea-tures can be defined on characters, words, part of speech (POS) tags and context-free grammar (CFG) rules, depending on the application to which the model is applied."
Features defined on n-grams from the input are the most commonly used for NLP applications.
"Such n-grams can either be defined explicitly us-ing some linguistic insight into the problem, or the model can be used to search the entire space of n-gram features using a kernel representation."
One example is the use of a polynomial kernel over se-quences.
"However, to use all possible n-gram fea-tures typically introduces too many noisy features, which can result in lower accuracy."
"One way to solve this problem is to use a kernel function that is tailored for particular NLP applications, such as the tree kernel[REF_CITE]for statistical parsing."
"In addition to n-gram features, more complex high-level features are often exploited to obtain higher accuracy, especially when discriminative models are used for statistical parsing."
"For ex-ample, all possible sub-trees can be used as fea-tures[REF_CITE]."
"How-ever, most of the sub-trees are linguistically mean-ingless, and are a source of noisy features thus limit-ing efficiency and accuracy."
An alternative to the use of arbitrary sets of sub-trees is to use the set of ele-mentary trees as defined in Lexicalized Tree Adjoin-ing Grammar (LTAG)[REF_CITE].
"LTAG based features not only allow a more limited and a linguistically more valid set of features over sub-trees, they also provide the use of features that use discontinuous sub-trees which are outside the scope of previous tree kernel definitions using arbi-trary sub-trees."
"In this paper, we use the LTAG based features in the parse reranking problem[REF_CITE]."
We use the Sup-port Vector Machine (SVM)[REF_CITE]based algorithm proposed[REF_CITE]as the reranker in this paper.
"We apply the tree kernel to derivation trees of LTAG, and extract features from derivation trees."
Both the tree kernel and the linear kernel on the richer feature set are used.
Our exper-iments show that the use of tree kernel on derivation trees makes the notion of a tree kernel more power-ful and more applicable.
"In this section, we give a brief introduction to the Lexicalized Tree Adjoining Grammar (more details can be found[REF_CITE])."
"In LTAG, each word is associated with a set of elemen-tary trees."
Each elementary tree represents a possi-ble tree structure for the word.
"There are two kinds of elementary trees, initial trees and auxiliary trees."
"Elementary trees can be combined through two op-erations, substitution and adjunction."
"Substitution is used to attach an initial tree, and adjunction is used to attach an auxiliary tree."
"In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described[REF_CITE]. 1"
The tree resulting from the combination of elementary trees is is called a derived tree.
The tree that records the history of how a derived tree is built from the elementary trees is called a derivation tree. [Footnote_2]
2 Each node ηhni in the derivation tree is an elementary tree name η along with the location n in the parent elementary tree where η is inserted. The location n is the Gorn tree address (see Fig. 4).
We illustrate the LTAG formalism using an exam-ple.
Example [Footnote_1]: Pierre Vinken will join the board as a non-executive director.
"1 Adjunction is used in the case where both the root node and the foot node appear in the Treebank tree. Sister adjunction is used in generating modifier sub-trees as sisters to the head, e.g in basal NPs."
"The derived tree for Example 1 is shown in Fig. 1 (we omit the POS tags associated with each word to save space), and Fig. 2 shows the elementary trees for each word in the sentence."
Fig. 3 is the deriva-tion tree (the history of tree combinations).
"One of the properties of LTAG is that it factors recursion in clause structure from the statement of linguistic con-straints, thus making these constraints strictly local."
"For example, in the derivation tree of Examples [Footnote_1], α 1 (join) and α 2 (V inken) are directly connected whether there is an auxiliary tree β 2 (will) or not."
"1 Adjunction is used in the case where both the root node and the foot node appear in the Treebank tree. Sister adjunction is used in generating modifier sub-trees as sisters to the head, e.g in basal NPs."
We will show how this property affects our redefined tree kernel later in this paper.
"In our experiments in this paper, we only use LTAG grammars where each elementary tree is lexicalized by exactly one word (terminal symbol) on the frontier."
"In recent years, reranking techniques have been suc-cessfully used in statistical parsers to rerank the out-put of history-based models[REF_CITE]."
"In this paper, we will use the LTAG based features to improve the performance of reranking."
"Our motiva-tions for using LTAG based features for reranking are the following: • Unlike the generative model, it is trivial to in-corporate features of various kinds in a rerank-ing setting."
"Furthermore the nature of rerank-ing makes it possible to use global features, α 1 (join)hi α 2 (Vinken)h00i β 2 (will)h01i α 3 (board)h011i β 4 (as)h01i which allow us to combine features that are de-fined on arbitrary sub-trees in the parse tree and features defined on a derivation tree. • Several hand-crafted and arbitrary features have been exploited in the statistical parsing task, especially when parsing the WSJ Penn Treebank dataset where performance has been finely tuned over the years."
Showing a positive contribution in this task will be a convincing test for the use of LTAG based features. • The parse reranking dataset is well established.
We use the dataset defined[REF_CITE].
"In[REF_CITE], two reranking algorithms were proposed."
"One was based on Markov Random Fields, and the other was based on the Boosting al-gorithm."
"In both these models, the loss functions were computed directly on the feature space."
"Fur-thermore, a rich feature set was introduced that was specifically selected by hand to target the limitations of generative models in statistical parsing."
"In[REF_CITE], the Voted Percep-tron algorithm was used for parse reranking."
The tree kernel was used to compute the number of com-mon sub-trees of two parse trees.
The features used by this tree kernel contains all the hand selected fea-tures[REF_CITE].
It is worth mentioning that the f-scores reported[REF_CITE]are about 1% less than the results[REF_CITE].
"In[REF_CITE], a SVM based rerank-ing algorithm was proposed."
"In that paper, the no-tion of preference kernels was introduced to solve the reranking problem."
"Two distinct kernels, the tree kernel and the linear kernel were used with prefer-ence kernels."
"While the tree kernel is an easy way to compute sim-ilarity between two parse trees, it takes too many lin-guistically meaningless sub-trees into consideration."
Let us consider the example sentence in Example 1.
"The parse tree, or derived tree, for this sentence is shown in Fig. 1. Fig. 5 shows one of the lin-guistically meaningless sub-trees."
The number of meaningless sub-trees is a misleading measure for discriminating good parse trees from bad.
"Further-more, the number of meaningless sub-trees is far greater than the number of useful sub-trees."
This limits both efficiency and accuracy on the test data.
"The use of unwanted sub-trees greatly increases the hypothesis space of a learning machine, and thus de-creases the expected accuracy on test data."
"In this work, we consider the hypothesis that linguistically meaningful sub-trees reveal correlations of interest and therefore are useful in stochastic models."
We notice that each sub-tree of a derivation tree is linguistically meaningful because it represents a valid sub-derivation.
We claim that derivation trees provide a more accurate measure of similarity be-tween two parses.
This is one of the motivations for applying tree kernels to derivation trees.
"Note that the use of features on derivation trees is differ-ent from the use of features on dependency graphs, derivation trees include many complex patterns of tree names and attachment sites and can represent word to word dependencies that are not possible in traditional dependency graphs."
"For example, the derivation tree for Example 1 with and without optional modifiers such as β 4 (as) are minimally different."
"In contrast, in derived (parse) trees, there is an extra VP node which changes quite drastically the set of sub-trees with and without the PP modifier."
"In addition, using only sub-trees from the derived tree, we cannot repre-sent a common sub-tree that contains only the words Vinken and join since this would lead to a discontin-uous sub-tree."
"However, LTAG based features can represent such cases trivially."
The comparison[REF_CITE]and[REF_CITE]in §3 shows that it is hard to add new features to improve performance.
Our hypothesis is that the LTAG based features provide a novel set of abstract features that complement the hand selected features[REF_CITE]and the LTAG based features will help improve performance in parse reranking.
Before we can use LTAG based features we need to obtain an LTAG derivation tree for each parse tree under consideration by the reranker.
Our solu-tion is to extract elementary trees and the derivation tree simultaneously from the parse trees produced by an n-best statistical parser.
Our training and test data consists of n-best output from the Collins parser (see[REF_CITE]for details on the dataset).
"Since the Collins parser uses a lexicalized context-free grammar as a basis for its statistical model, we obtain parse trees that are of the type shown in Fig. 6."
From this tree we extract elementary trees and derivation trees by recursively traversing the spine of the parse tree.
The spine is the path from a non-terminal lexicalized by a word to the terminal sym-bol on the frontier equal to that word.
Every sub-tree rooted at a non-terminal lexicalized by a different word is excised from the parse tree and recorded into the derivation tree as a substitution.
Repeated non-terminals on the spine (e.g. VP(join) . ..
VP(join) in Fig. 6) are excised along with the sub-trees hang-ing off of it and recorded into the derivation tree as an adjunction.
The only other case is those sub-trees rooted at non-terminals that are attached to the spine.
These sub-trees are excised and recorded into the derivation tree as cases of sister adjunction.
"Each sub-tree excised is recursively analyzed with this method, split up into elementary trees and then recorded into the derivation tree."
The output of our algorithm for the input parse tree in Fig. 6 is shown in Fig. 2 and Fig. 3.
"Our algorithm is similar to the derivation tree extraction explained[REF_CITE], except we extract our LTAG from n-best sets of parse trees, while[REF_CITE]the LTAG is extracted from the Penn Treebank. [Footnote_3]"
3 Also note that the path from the root node to the foot node in auxiliary trees can be greater than one (for trees with S roots).
For other tech-niques for LTAG grammar extraction see[REF_CITE].
"In this paper, we have described two models to em-ploy derivation trees."
Model 1 uses tree kernels on derivation trees.
"In order to make the tree kernel more lexicalized, we extend the original definition of the tree kernel, which we will describe below."
Model 2 abstracts features from derivation trees and uses them with a linear kernel.
"In Model 1, we combine the SVM results of the tree kernel on derivation trees with the SVM results given by a linear kernel based on features on the de-rived trees."
"In Model 2, the vector space of the linear kernel consists of both LTAG based features defined on the derived trees and features defined on the derivation tree."
The following LTAG features have been used in Model 2. • Elementary tree.
Each node in the derivation tree is used as a feature. • Bigram of parent and its child.
"Each pair of parent elementary tree and child elementary tree, as well as the type of operation (substi-tution, adjunction or sister adjunction) and the Gorn address on parent (see Fig. 4) is used as a feature. • Lexicalized elementary tree."
Each elemen-tary tree associated with its lexical item is used as a feature. • Lexicalized bigram.
"In Bigram of parent and its child, each elementary tree is lexicalized (we use closed class words, e.g. adj, adv, prep, etc. but not noun or verb)."
"In[REF_CITE], the notion of a tree ker-nel is introduced to compute the number of common sub-trees of two parse trees."
"For two parse trees, p 1 and p 2 , the tree kernel Tree(p 1 , p 2 ) is defined as:"
"Tree(p 1 , p 2 ) = X (1)T (n 1 , n 2 ) n 1 in p 1 n 2 in p 2"
"The recursive function T is defined as follows: If n 1 and n 2 have the same bracketing tag (e.g. S, NP, VP, . . .) and the same number of children,"
"T (n 1 , n 2 ) = λ Y (1 + T (n (2) 1i , n 2i )), i where, n ki is the ith child of the node n k , λ is a weight coefficient used to control the importance of large sub-trees and 0 &lt; λ ≤ 1."
"If n 1 and n 2 have the same bracketing tag but dif-ferent number of children, T(n 1 ,n 2 ) = λ."
"If they don’t have the same bracketing tag, T (n 1 , n 2 ) = 0."
"In[REF_CITE], lexical items are all located at the leaf nodes of parse trees."
Therefore sub-trees that do not contain any leaf node are not lexicalized.
"Furthermore, due to the introduction of parameter λ, lexical information is almost ignored for sub-trees whose root node is not close to the leaf nodes, i.e. sub-trees rooted at S node."
"In order to make the tree kernel more lexicalized, we associate each node with a lexical item."
"For ex-ample, Fig. 7 shows a lexicalized sub-tree and its decomposition into features."
As shown in Fig. 7 the lexical information lex(t) extracted from the lexical-ized tree consists of words from the root and its im-mediate children.
This is because we wish to ig-nore irrelevant lexicalizations such as NP(board) in Fig. 7.
A lexicalized sub-tree rooted on node n is split into two parts.
"One is the pattern tree of n, ptn(n)."
"The other is the vector of lexical information of n, lex(n), which contains the lexical items of the root node and the children of the root."
"For two tree nodes n 1 and n 2 , the recursive func-tion LT (n 1 , n 2 ) used to compute the lexicalized tree kernel is defined as follows."
"LT (n 1 , n 2 ) = (1 + Cnt(lex(n 1 ), lex(n 2 ))) × T 0 (ptn(n 1 ), ptn(n 2 )), (3) where T 0 is the same as the original recursive func-tion T defined in (2), except that T is defined on parse tree nodes, while T 0 is defined on patterns of parse tree nodes."
"Cnt(x,y) counts the number of common elements in vector x and y."
"For example, Cnt((join, join, as), (join, join, in)) = 2, since 2 elements of the two vectors are the same."
"It can be shown that the lexicalized tree kernel counts the number of common sub-trees that meet the following constraints. • None or one node in the sub-tree is lexicalized • The lexicalized node is the root node or a child of the root, if applicable."
Therefore our new tree kernel is more lexicalized.
"On the other hand, it immediately follows that the lexicalized tree kernel is well-defined."
It means that we can embed the lexicalized tree kernel into a high dimensional space.
The proof is similar to the proof for the tree kernel[REF_CITE].
Another important advantage of the lexicalized tree kernel is that it is more compressible.
It is noted[REF_CITE]that training trees can be combined by sharing sub-trees to speed up the test.
"As far as the lexicalized tree kernel is con-cerned, the pattern trees are more compressible be-cause there is no lexical item at the leaf nodes of pattern trees."
Lexical information can be attached to the nodes of the result pattern forest.
"In our ex-periment, we select five parses from each sentence in Collins’ training data and represent these parses with shared structure."
The number of the nodes in the pattern forest is only 1/7 of the total number of the nodes the selected parse trees.
"In order to apply the (lexicalized) tree kernel to derivation trees, we need to make some modifica-tions to the original recursive definition of the tree kernel."
"For derivation trees, the recursive function is trig-gered if the two root nodes have the same non-lexicalized elementary tree (sometimes called su-pertag)."
Note that these two nodes will have the same number of children which are initial trees (aux-iliary trees are not counted).
"In comparison, the re-cursive function in (2), T (n 1 , n 2 ) is computed if and only if n 1 and n 2 have the same bracketing tag and they have the same number of children."
"For each node, its children are attached with one of the two distinct operations, substitution or adjunc-tion."
"For substituted children, the computation of the tree kernel is almost the same as that for CFG parse tree."
"However, there is a problem with the adjoined children."
Let us first have a look at a sentence in Penn Treebank.
Example 2: COMMERCIAL PAPER placed di-rectly by General Motors Acceptance Corp.: 8.55% 30 to 44 days; 8.25% 45 to 59 days; 8.45% 60 to 89 days; 8% 90 to 119 days; 7.90% 120 to 149 days; 7.80% 150 to 179 days; 7.55% 180 to 270 days.
"In this example, seven sub-trees of the same type are sister adjoined to the same place of an initial tree."
So the number of common sub-trees increases dra-matically if the tree kernel is applied on two similar parses of this sentence.
Experimental evidence indi-cates that this is harmful to accuracy.
"Therefore, for derivation trees, we are only interested in sub-trees that contain at most 2 adjunction branches for each node."
"The number of constrained common sub-trees for the derivation tree kernel can be computed by the recursive function DT over derivation tree nodes n 1 , n 2 :"
"DT (n 1 , n 2 ) = (1 + A 1 (n 1 , n 2 ) +"
"A 2 (n 1 , n 2 )) × T ”(sub(n 1 ), sub(n 2 )) (4) where sub(n k ) is the sub-tree of n k in which chil-dren adjoined to the root of n k are pruned."
"T” is similar to the original recursive function T defined in (2), but it is defined on derivation tree nodes re-cursively."
A 1 and A 2 are used to count the number of common sub-trees whose root nodes only contain one or two adjunction children respectively.
"A 1 (n 1 , n 2 ) = X DT (a 1i , a 2j ), i,j where, a 1i is the ith adjunct of n 1 , and a 2j is the jth adjunct of n 2 ."
"Similarly, we have:"
"A 2 (n 1 , n 2 ) = X DT (a 1i , a 2j ) · DT (a 1k , a 2l ) i&lt;k,j&lt;l"
The tree kernel for derivation trees is a well-defined kernel function because we can easily define an em-bedding space according to the definition of the new tree kernel.
"By substituting DT for T 0 in (3), we ob-tain the lexicalized tree kernel for LTAG derivation trees (using LT in (1))."
"As described above, we use the SVM based voting algorithm[REF_CITE]in our reranking experiments."
We use preference kernels and pair-wise parse trees in our reranking models.
We use the same data set as described[REF_CITE].
"Section 2-21 of the Penn WSJ Treebank are used as training data, and section 23 is used for fi-nal test."
"The training data contains around 40,000 sentences, each of which has 27 distinct parses on average."
"The remaining 4,000 sentences are used as development data."
"Due to the computational complexity of SVM, we have to divide training data into slices to speed up training."
Each slice contain two pairs of parses from every sentence.
"Specifically, slice i contains pos-itive samples ((p̃ k ,p ki ),+1) and negative samples ((p ki , p̃ k ), −1), where p̃ k is the best parse for sen-tence k, p ki is the parse with the ith highest log-likelihood in all the parses for sentence k and it is not the best parse[REF_CITE]."
There are about 60000 samples in each slice in average.
"For the tree kernel SVMs of Model 1, we take 3 slices as a chunk, and train an SVM for each chunk."
"Due to the limitation of computing resource, we have only trained on 3 chunks."
The results of tree kernel SVMs are combined with simple com-bination.
Then the outcome is combined with the result of the linear kernel SVMs trained on features extracted from the derived trees which are reported[REF_CITE].
"For each parse, the num-ber of the brackets in it and the log-likelihood given by Collins’ parser Model 2 are also used in the com-putation of the score of a parse."
"For each parse p, its score Sco(p) is defined as follows:"
"Sco(p) = M T (p) + γ · M L (p) + β · l(p) + α · b(p), where M T (p) is the output of the tree kernel SVMs, M L (p) is the output of linear kernel SVMs, l(p) is the log-likelihood of parse p, and b(p) is the num-ber of brackets in parse p."
We noticed that the SVM systems prefers to give higher scores to the parses with less brackets.
"As a result, the system has a high precision but a low recall."
"Therefore, we take the number of brackets, b(p), as a feature to make the recall and precision balanced."
The three weight pa-rameters are tuned on the development data.
The results are shown in Table 1.
"With Model 1, we achieve LR/[REF_CITE].7%/90.0% on sentences with ≤ 100 words."
Our results show a 17% rel-ative difference in f-score improvement over the use of a linear kernel without LTAG based features[REF_CITE].
"In addition, we also get non-trivial improvement on the number of crossing brackets."
These results verify the benefit of using LTAG based features and confirm the hypothesis that LTAG based features provide a novel set of abstract features that complement the hand selected features[REF_CITE].
Our results on Model 1 show a 1% error reduction on the previous best reranking result using the dataset reported[REF_CITE].
"Also, Model 1 provides a 10% reduction in error[REF_CITE]where the features from tree kernel were over arbitrary sub-trees."
"For Model 2, we first train 22[REF_CITE]dis-tinct slices."
Then we combine the results of individ-ual SVMs with simple combination.
"However, the overall performance does not improve."
"But we no-tice that the use of LTAG based features gives rise to improvement on most of the single SVMs, as shown in Fig. 8."
We think there are several reasons to account for why our Model 2 doesn’t work as well for the full task when compared with Model 1.
"Firstly, the train-ing slice is not large enough."
Local optimization on each slice does not result in global optimization (as seen in Fig. 8).
"Secondly, the LTAG based features that we have used in the linear kernel in Model 2 are not as useful as the tree kernel in Model 1. [Footnote_4]"
"4 In Model 1, we implicitly take every sub-tree of the deriva-tion trees as a feature, but in Model 2, we only consider a small set of sub-trees in a linear kernel."
The last reason is that we do not set the importance of LTAG based features.
One shortcoming of kernel methods is that the coefficient of each feature must be set be-fore the training[REF_CITE].
"In our case, we do not tune the coefficients for the LTAG based fea-tures in Model 2."
"In this paper, we have proposed methods for using LTAG based features in the parse reranking task."
The experimental results show that the use of LTAG based features gives rise to improvement over al-ready finely tuned results.
We used LTAG based fea-tures for the parse reranking task and obtain labeled recall and precision of 89.7%/90.0% on WSJ sec-tion 23 of Penn Treebank for sentences of length ≤ 100 words.
Our results show that the use of LTAG based tree kernel gives rise to a 17% relative differ-ence in f-score improvement over the use of a linear kernel without LTAG based features.
"In future work, we will use some light-weight machine learning al-gorithms for which training is faster, such as vari-ants of the Perceptron algorithm."
This will allow us to use larger training data chunks and take advan-tage of global optimization in the search for relevant features.
This paper describes log-linear pars-ing models for Combinatory Categorial Grammar ( CCG ).
"Log-linear models can easily encode the long-range dependen-cies inherent in coordination and extrac-tion phenomena, which CCG was designed to handle."
"Log-linear models have pre-viously been applied to statistical pars-ing, under the assumption that all possible parses for a sentence can be enumerated."
"Enumerating all parses is infeasible for large grammars; however, dynamic pro-gramming over a packed chart can be used to efficiently estimate the model parame-ters."
We describe a parellelised implemen-tation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation.
"Statistical parsing models have recently been de-veloped for Combinatory Categorial Grammar ( CCG ,[REF_CITE]) and used in wide-coverage parsers applied to the WSJ Penn Treebank[REF_CITE]."
"An attraction of CCG is its elegant treatment of coor-dination and extraction, allowing recovery of the long-range dependencies inherent in these construc-tions."
"We would like the parsing model to include long-range dependencies, but this introduces prob-lems for generative parsing models similar to those described[REF_CITE]for attribute-value gram-mars; hence Hockenmaier and Steedman do not in-clude such dependencies in their model, and Clark et al. include the dependencies but use an incon-sistent model."
"Following Abney, we propose a log-linear framework which incorporates long-range de-pendencies as features without loss of consistency."
Log-linear models have previously been ap-plied to statistical parsing[REF_CITE].
"Typically, these approaches have enu-merated all possible parses for model estimation and finding the most probable parse."
"For gram-mars extracted from the Penn Treebank (in our case CCGbank[REF_CITE]), enumerating all parses is infeasible."
"One approach to this prob-lem is to sample the parse space for estimation, e.g.[REF_CITE]."
"In this paper we use a dynamic pro-gramming technique applied to a packed chart, simi-lar to those proposed[REF_CITE]and[REF_CITE], which efficiently esti-mates the model parameters over the complete space without enumerating parses."
The estimation method is similar to the inside-outside algorithm used for es-timating a PCFG[REF_CITE].
"However, their model has significant memory requirements which limits them to using 868 sentences as training data."
"We use a parallelised version of Generalised Iterative Scal-ing ( GIS ,[REF_CITE]) on a Beowulf cluster which allows the complete WSJ Penn Tree- bank to be used as training data."
This paper assumes a basic knowledge of CCG ; see[REF_CITE]and[REF_CITE]for an introduction.
"For example, the extended category for per-suade is as follows: persuade := (( S [ dcl ] persuade \ NP 1 )/( S [ to ] 2 \ NP X ))/"
"NP X,3 ([Footnote_1])"
1 We use the term decoding to refer to the process of finding the most probable dependency structure from a packed chart.
The feature [ dcl ] indicates a declarative sentence; the resulting S[ dcl ] is headed by persuade; and the num-bers indicate dependency relations.
"The variable X denotes a head, identifying the head of the infiniti-val complement’s subject with the head of the ob-ject, thus capturing the object control relation."
"For example, in Microsoft persuades IBM to buy Lotus, IBM fills the subject slot of buy."
"Formally, a dependency is defined as a 5-tuple: hh f , f, s,h a ,li, where h f is the head word of the functor, f is the functor category (extended with head and dependency information), s is the argu-ment slot, and h a is the head word of the argument."
The l is an additional field used to encode whether the dependency is long-range.
"For example, the de-pendency encoding Lotus as the object of bought (as in IBM bought Lotus) is represented as follows: hbought, (S[ dcl ] bought \NP 1 )/NP 2 , 2, Lotus, nulli (2)"
"If the object has been extracted using a relative pro-noun with the category (NP\NP)/(S[ dcl ]/NP) (as in the company that IBM bought), the dependency is as follows: hbought, (S[ dcl ] bought \NP 1 )/NP 2 , 2, company, ∗i (3) where ∗ is the category (NP\NP)/(S[ dcl ]/NP) as-signed to the relative pronoun."
A dependency struc-ture is simply a set of these dependencies.
Every argument in every lexical category is en-coded as a dependency.
"Unlike Clark et al., we do not require dependencies to be always marked on atomic categories."
"For example, the marked up cat-egory for about (as in about 5,000 pounds) is: (N X /N X ) Y /(N/N) Y,1 (4)"
Previous parsing models for CCG include a genera-tive model over normal-form derivations[REF_CITE]and a conditional model over dependency structures[REF_CITE].
"We follow Clark et al. in modelling dependency structures, but, unlike Clark et al., do so in terms of derivations."
An advantage of our approach is that the model can potentially include derivation-specific features in addition to dependency informa-tion.
"Also, modelling derivations provides a close link between the model and the parsing algorithm, which makes it easier to define dynamic program-ming techniques for efficient model estimation and decoding 1 , and also apply beam search to reduce the search space."
"The probability of a dependency structure, π ∈ Π, given a sentence, S , is defined as follows:"
X P(π|S ) =
"P(d, π|S ) (5) d∈∆(π,S) where ∆(π, S ) is the set of derivations for S which lead to π and Π is the set of dependency structures."
"Note that ∆(π, S ) includes the non-standard deriva-tions allowed by CCG ."
"This model allows the pos-sibility of including features from the non-standard derivations, such as features encoding the use of type-raising or function composition."
"A log-linear model of a parse, ω ∈ Ω, given a sentence S , is defined as follows:"
P(ω|S ) = 1 Y µ f i (ω) (6) Z S i i
"This model can be applied to any kind of parse, but for this paper a parse, ω, is a hd,πi pair (as given in (5))."
The function f i is a feature of the parse which can be any real-valued function over the space of parses Ω.
In this paper f i (ω) is a count of the num-ber of times some dependency occurs in ω.
Each feature f i has an associated weight µ i which is a pa-rameter of the model to be estimated.
Z S is a normal-ising constant which ensures that P(ω|S ) is a proba-bility distribution:
X Y Z S = µ if i (ω 0 ) (7) ω 0 ∈ρ(S) i where ρ(S ) ⊆ Ω is the set of possible parses for S .
The advantage of a log-linear model is that the features can be arbitrary functions over parses.
"This means that any dependencies – including overlap-ping and long-range dependencies – can be included in the model, irrespective of whether those depen-dencies are independent."
The theory underlying log-linear models is described[REF_CITE]and[REF_CITE].
"Briefly, the log-linear form in (6) is derived by choosing the model with maximum entropy from a set of models that satisfy a certain set of constraints[REF_CITE]."
"The constraints are that, for each feature f i :"
X X P̃(S )
"P(ω|S ) f i (ω) = P̃(ω, S ) f i (ω) (8) ω,S ω,S where the sums are over all possible parse-sentence pairs and P̃(S ) is the relative frequency of sentence S in the data."
"The value on the left of (8) is the expected value of f i according to the model, E p f i , and the value on the right is the empirical expected value of f i , E p̃ f i ."
Estimating the parameters of a log-linear model requires the values in (8) to be calculated for each feature.
Calculating the empirical expected val-ues requires a treebank of CCG derivations plus dependency structures.
"For this we use CCG-bank[REF_CITE], a corpus of normal-form CCG derivations derived semi-automatically from the Penn Treebank."
"Following Clark et al., gold standard dependency structures are obtained for each derivation by running a dependency-producing parser over the derivations."
The empirical expected value of a feature f i is calculated as follows:
E p̃ f i = 1 X f N i (ω j ) (9) N j=1 where ω 1 . . . ω N are the parses in the training data (consisting of a normal-form derivation plus depen-dency structure) and f i (ω j ) is the number of times f i appears in parse ω j . [Footnote_2]
"2 An alternative is to use feature counts from all derivations leading to the gold standard dependency structure, including the non-standard derivations, to calculate E p̃ f i ."
"Parameter estimation also requires calculation of expected values of the features according to the model, E p f i ."
"This requires summing over all parses (derivation plus dependency structure) for the sen-tences in the data, a difficult task since the total num-ber of parses can grow exponentially with sentence length."
"For some sentences in CCGbank, the parser described in Section 6 produces trillions of parses."
"The next section shows how a packed chart can ef-ficiently represent the parse space, and how GIS ap-plied to the packed chart can be used to estimate the parameters."
We apply Miyao and Tsujii’s method to the derivations and dependency structures produced by our CCG parser.
"The dynamic programming method relies on a packed chart, in which chart entries of the same type in the same cell are grouped together, and back pointers to the daughters keep track of how an indi-vidual entry was created."
"The intuition behind the dynamic programming is that, for the purposes of building a dependency structure, chart entries of the same type are equivalent."
Consider the following composition of will with buy using the forward com-position rule: ((S[dcl] will \NP)/NP) ((S[dcl] will \NP)/(S[b]\NP)) ((S[b] buy \NP)/NP)
"The type of the resulting chart entry is deter-mined by the CCG category plus heads, in this case ((S[dcl] will \NP)/NP), plus the dependencies yet to be filled."
"The dependencies are not shown, but there are two subject dependencies on the first NP, one encoding the subject of will and one encoding the subject of buy 3 , and there is an object dependency on the second NP encoding the object of buy."
En-tries of the same type are identical for the purposes of creating new dependencies for the remainder of the parsing.
Any rule instantiation [Footnote_4] used by the parser creates both a set of dependencies and a set of features.
4 By rule instantiation we mean the local tree arising from the application of a CCG combinatory rule.
"For the previous example, one dependency is created: hwill, (S[ dcl ] will \NP X,1 )/(S [ b ] 2 \NP X ), 2, buyi"
This dependency will be a feature created by the rule instantiation.
"We also use less specific features, such as the dependency with the words replaced by POS tags."
Section 7 describes the features used.
The feature forests of Miyao and Tsujii are de-fined in terms of conjunctive and disjunctive nodes.
"For our purposes, a conjunctive node is an individual entry in a cell, including the features created when the entry was derived, plus pointers to the entry’s daughters."
"A disjunctive node represents an equiva-lence class of nodes in a cell, using the type equiva-lence relation described above."
"A conjunctive node results from either the combination of two disjunc-tive nodes using a binary rule, e.g. forward composi-tion; or results from a single disjunctive node using a unary rule, e.g. type-raising; or is a leaf node (a word plus lexical category)."
Features in the model can only result from a sin-gle rule instantiation.
It is possible to define features covering a larger part of the dependency structure; for example we might encode all three elements of the triple in a PP -attachment as a single feature.
The disadvantage of using such features is that this re-duces the efficiency of the dynamic programming.
"Note, however, that the equivalence relation defin-ing disjunctive nodes takes into account unfilled de-pendencies, which may be long-range dependencies being “passed up” the derivation tree."
"This means that long-range dependencies can be features in our model, even though the lexical items involved may be far apart in the sentence."
"The packed structure we have described is an ex-ample of a feature forest[REF_CITE], defined as follows:"
"A feature forest Φ is a tuple hC, D, R, γ, δi where"
C is a set of conjunctive nodes;
D is a set of disjunctive nodes;
R ⊆ D is a set of root disjunctive nodes; [Footnote_5] γ : D → 2 C is a conjunctive daughter function; δ : C → 2 D is a disjunctive daughter function.
5 Miyao and Tsujii have a single root conjunctive node; the disjunctive root nodes we define correspond to the roots of CCG derivations.
"For each feature function f i : Ω → N, there is a corresponding feature function f i : C → N which counts the number of times f i appears on a particular conjunctive node. [Footnote_6] The value of f i for a parse is then the sum of the values of f i for each conjunctive node in the parse."
"6 The value of f i (c) for c ∈ C will typically be 0 or 1, but it is possible for the count to be greater than 1."
GIS is a very simple algorithm for estimating the pa-rameters of a log-linear model.
The parameters are initialised to some arbitrary constant and the follow-ing update rule is applied until convergence: ! 1 µ (it+1) = µ i(t)
"E p̃ f i C (10) E p (t) f i where (t) is the iterationP index and the constant C is defined as max ω,"
S i f i (ω).
In practice C is max-imised over the sentences in the training data.
"Im-plementations of GIS typically use a “correction fea-ture”, but following[REF_CITE]we do not use such a feature, which simplifies the algo-rithm."
Calculating E p (t) f i requires summing over all derivations which include f i for each packed chart in the training data.
The key to performing this sum efficiently is to write the sum in terms of inside and outside scores for each conjunctive node.
"The inside and outside scores can be defined recursively, as in the inside-outside algorithm for PCFG s. If the inside score for a conjunctive node c is denoted φ c , and the outside score denoted ψ c , then the expected value of f i can be written as follows: 7"
X 1 X E p f i = f i (c) φ c ψ c (11)P̃(S )
S Z S c∈C s where C s is the set of conjunctive nodes for S .
Consider the example feature forest in Figure 1.
The figure shows the nodes used to calculate the in-side and outside scores for conjunctive node c 5 .
"The inside score for a disjunctive node, φ d , is the sum of the inside scores for its conjunctive node daughters:"
X φ d = φ c (12) c∈γ(d)
"The inside score for a conjunctive node, φ c , can then be defined recursively:"
Y Y φ c = µ if i (c) φ d ( 13) d∈δ(c) i
"The intuition for calculating outside scores is sim-ilar, but a little more involved."
"The outside score for a conjunctive node, ψ c , is the outside score for its disjunctive node mother: ψ c = ψ d where c ∈ γ(d) (14)"
"The outside score for a disjunctive node is a sum over the mother nodes, of the product of the outside score of the mother, the inside score of the sister, and the feature weights on the mother. [Footnote_8]"
"8[REF_CITE]ignore the feature weights on the mother, but this ignores some of the probability mass for the outside (at least for the feature forests we have defined)."
"For example, the outside score of d 4 in Figure 1 is the sum of the fol-lowing two values: the product of the outside score of c 5 , the inside score of d 5 and the feature weights at c 5 ; and the product of the outside score of c 2 , the inside score of d 3 and the feature weights at c 2 ."
The recursive definition is as follows.
"The outside score for a root disjunctive node is 1, otherwise:"
X ψ  µ fi i (c)  (15)
"Y Y ψ d = φ d 0 c {c|d∈δ(c)} {d 0 |d 0 ∈δ(c),d 0 ,d} i"
The normalisation constant Z S is the sum of the inside scores for the root disjunctive nodes:
X Z S = φ d r (16) d r ∈R
"In order to calculate inside scores, the scores for daughter nodes need to be calculated before the scores for mother nodes (and vice versa for the out-side scores)."
This can easily be achieved by ordering the nodes in the bottom-up CKY parsing order.
"Note that the inside-outside approach can be com-bined with any maximum entropy estimation proce-dure, such as those evaluated[REF_CITE]."
"Finally, in order to avoid overfitting, we use a Gaussian prior on the parameters of the model[REF_CITE], which requires a slight modi-fication to the update rule in (10)."
A Gaussian prior also handles the problem of “pseudo-maximal” fea-tures[REF_CITE].
The parser is based[REF_CITE]and takes as input a POS -tagged sentence with a set of possi-ble lexical categories assigned to each word.
"The supertagger[REF_CITE]provides the lexical cat-egories, with a parameter setting which assigns around 4 categories per word on average."
The pars-ing algorithm is the CKY bottom-up chart-parsing algorithm described[REF_CITE].
"The com-binatory rules used by the parser are functional ap-plication (forward and backward), generalised for-ward composition, backward composition, gener-alised backward-crossed composition, and type rais-ing."
There is also a coordination rule which con-joins categories of the same type.
"Restrictions are placed on some of the rules, such as that given by"
"Steedman (2000, p.62) for backward-crossed com-position."
"Type-raising is applied to the categories NP, PP and S[ adj ]\NP (adjectival phrase), and is imple-mented by adding the relevant set of type-raised categories to the chart whenever an NP, PP or S[ adj ]\NP is present."
"The sets of type-raised cate-gories are based on the most commonly used type-raising rule instantiations in sections 2-21 of CCG-bank, and contain 8 type-raised categories for NP and 1 each for PP and S[ adj ]\NP."
The parser also uses a number of lexical rules and punctuation rules.
These rules are based on those occurring roughly more than 200 times in sections 2-21 of CCGbank.
"An example of a lexical rule used by the parser is the following, which takes a passive form of a verb and creates a nominal modifier:"
"S[ pss ]\NP ⇒ NP X \NP X,1 (17)"
This rule is used to create NPs such as the role played by Kim Cattrall.
Note that there is a de-pendency relation on the resulting category; in the previous example role would fill a nominal modifier dependency headed by played.
"Currently, the only punctuation marks handled by the parser are commas, and all other punctuation is removed after the supertagging phase."
An example of a comma rule is the following:
"S X /S X , ⇒ S X /S X (18)"
"This rule takes a sentential modifier followed by a comma (for example Currently , in the sentence above in the text) and returns a sentential modifier of the same type."
The next section describes the efficient implemen-tation of the parser and model estimator.
"The non-standard derivations allowed by CCG , to-gether with the wide coverage grammar, result in extremely large charts."
This means that efficient im-plementation of the parsing process is imperative for performing large-scale experiments.
The packed chart prevents combinatorial explo-sion in the number of category combinations by grouping equivalent categories into a single entry.
"The speed of the parser is heavily dependent on the efficiency of equivalence testing, and category uni-fication and construction."
These are performed effi-ciently by always creating categories in a canonical form which can then be compared rapidly using hash functions over categories.
The parser produces a packed chart from which the most probable dependency structure can be re-covered.
"Since the same dependency structure can be generated by more than one derivation, a depen-dency structure’s score is the sum of the log-linear scores for each derivation."
"Finding the structure with the highest score is not trivial, since filled de-pendencies are only stored at the conjunctive nodes where they are created."
This means that a depen-dency appearing in a structure can be created in dif-ferent parts of the chart for different derivations.
"We solve this in practice using a hash function over de-pendencies, which can be used to quickly determine whether two derivations lead to the same structure."
"For each node in the chart, we can keep track of the derivation leading to the set of dependencies with the highest score for that node."
Data for model estimation is created in two steps.
"First, the parser is run over the normal-form deriva-tions in Sections 2-21 of CCGbank outputting the corresponding dependencies and other features."
The features used in our preliminary implementation are as follows: dependency features; lexical category features; root category features.
Dependency features are 5-tuples as defined in Section 2.
"Further dependency features are formed by substituting POS tags for the words, which leads to a total of 4 features for each dependency."
Lexical category features are word category pairs on the leaf nodes and root features are head-word category pairs on root nodes.
Extra features are formed by replac-ing words with their POS tags.
"The total number of features is 817,658, but we reduce this to 243,603 by only including features which appear at least twice in the data."
"The second step of data generation involves using the parser to create a feature forest for each sentence, using the feature set extracted from CCGbank."
"The parser is interrupted if a sentence takes longer than 60 seconds to process or if more than 500,000 con-junctive nodes are created in the chart."
"If this oc-curs, the process is repeated but with a smaller num-ber of categories assigned to each word by the su-pertagger."
"Creating the forests takes approximately one hour using 40 nodes of our Be-owulf cluster, and produces 19.9 GB of data."
The parse forests regularly represent trillions of pos-sible parses for a sentence.
"The estimation pro-cess involves summing feature weights over all these parses, a total which cannot be represented using double precision arithmetic (limited to less than 10 308 )."
"Our implementation uses the sum, rather than product, form of (6), so that logarithms can be used to avoid numerical overflow."
"For converting the sum of products[REF_CITE]to log space, we use a technique commonly used in speech recognition (p.c."
"We have implemented a parallel version of our GIS code using the MPICH library[REF_CITE], an open-source implementation of the Mes-sage Passing Interface ( MPI ) standard."
MPI parallel programming involves explicit synchronisation and information transfer between the parallel processes using messages.
It is ideal for development of paral-lel programs for cluster architectures.
GIS over parse forests is straightforward to par-allelise.
"The parse forests are divided among the machines in the cluster (in our current implemen-tation, each machine receives 979 forests)."
Each machine calculates the inside and outside scores for each node in the parse forest and updates the es-timated feature expectations.
The feature expecta-tions are then summed across all of the machines using a global operation (called a reduce operation).
Every machine receives this sum which is then used to calculate the normal GIS weight update.
"In our preliminary tests, each process used approximately 750 MB of RAM , giving a total usage of 30 GB across the cluster."
One iteration of GIS takes approximately 1 minute.
"Given the large number of features, we estimate at least 1,000 iterations will be needed for convergence."
"Table 1 gives the overall statistics for the model estimation process, and compares them[REF_CITE]."
These numbers represent the largest-scale parsing model of which we are aware.
Parsing and model estimation on this scale introduce a number of interesting theoretical and computational challenges.
We have demonstrated how packed charts and feature forests can be com-bined to meet the theoretical challenges.
We have also described an MPI implementation of GIS which solves the computational challenges.
These tech-niques are necessary for discriminative estimation techniques applied to wide-coverage parsing.
We have just begun the process of evaluating parsing performance using the same test data[REF_CITE].
"We are especially interested in the effectiveness of incorporating long-range depen-dencies as features, which CCG was designed to han-dle and for which we expect a log-linear model to be particularly effective."
This paper presents a bootstrapping process that learns linguistically rich extraction pat-terns for subjective (opinionated) expressions.
"High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm."
The learned patterns are then used to identify more subjective sentences.
The bootstrapping process learns many subjec-tive patterns and increases recall while main-taining high precision.
Many natural language processing applications could benefit from being able to distinguish between factual and subjective information.
"Subjective remarks come in a variety of forms, including opinions, rants, allega-tions, accusations, suspicions, and speculations."
"Ideally, information extraction systems should be able to distin-guish between factual information (which should be ex-tracted) and non-factual information (which should be discarded or labeled as uncertain)."
Question answering systems should distinguish between factual and specula-tive answers.
Multi-perspective question answering aims to present multiple answers to the user based upon specu-lation or opinions derived from different sources.
Multi-document summarization systems need to summarize dif-ferent opinions and perspectives.
"Spam filtering systems must recognize rants and emotional tirades, among other things."
"In general, nearly any system that seeks to iden-tify information could benefit from being able to separate factual and subjective information."
"Some existing resources contain lists of subjective words (e.g., Levin’s desire verbs (1993)), and some em-pirical methods in NLP have automatically identified ad-jectives, verbs, and N-grams that are statistically associ-ated with subjective language (e.g.,[REF_CITE])."
"However, subjective language can be ex-hibited by a staggering variety of words and phrases."
"In addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g., preposterous, un-seemly) and metaphorical or idiomatic phrases (e.g., dealt a blow, swept off one’s feet)."
"Consequently, we believe that subjectivity learning systems must be trained on ex-tremely large text collections before they will acquire a subjective vocabulary that is truly broad and comprehen-sive in scope."
"To address this issue, we have been exploring the use of bootstrapping methods to allow subjectivity classifiers to learn from a collection of unannotated texts."
Our re-search uses high-precision subjectivity classifiers to au-tomatically identify subjective and objective sentences in unannotated texts.
This process allows us to generate a large set of labeled sentences automatically.
The sec-ond emphasis of our research is using extraction patterns to represent subjective expressions.
These patterns are linguistically richer and more flexible than single words or N-grams.
"Using the (automatically) labeled sentences as training data, we apply an extraction pattern learning algorithm to automatically generate patterns represent-ing subjective expressions."
"The learned patterns can be used to automatically identify more subjective sentences, which grows the training set, and the entire process can then be bootstrapped."
Our experimental results show that this bootstrapping process increases the recall of the high- precision subjective sentence classifier with little loss in precision.
We also find that the learned extraction pat-terns capture subtle connotations that are more expressive than the individual words by themselves.
This paper is organized as follows.
Section 2 discusses previous work on subjectivity analysis and extraction pat-tern learning.
"Section 3 overviews our general approach, describes the high-precision subjectivity classifiers, and explains the algorithm for learning extraction patterns as-sociated with subjectivity."
"Section 4 describes the data that we use, presents our experimental results, and shows examples of patterns that are learned."
"Finally, Section 5 summarizes our findings and conclusions."
Much previous work on subjectivity recognition has fo-cused on document-level classification.
"For example,[REF_CITE]developed a system to identify inflamma-tory texts and[REF_CITE]developed methods for classifying reviews as positive or negative."
"Some research in genre classification has included the recognition of subjective genres such as editorials (e.g.,[REF_CITE])."
"In contrast, the goal of our work is to classify individ-ual sentences as subjective or objective."
"Document-level classification can distinguish between “subjective texts”, such as editorials and reviews, and “objective texts,” such as newspaper articles."
"But in reality, most documents contain a mix of both subjective and objective sentences."
Subjective texts often include some factual information.
"For example, editorial articles frequently contain factual information to back up the arguments being made, and movie reviews often mention the actors and plot of a movie as well as the theatres where it’s currently playing."
"Even if one is willing to discard subjective texts in their entirety, the objective texts usually contain a great deal of subjective information in addition to facts."
"For example, newspaper articles are generally considered to be rela-tively objective documents, but in a recent study[REF_CITE]44% of sentences in a news collection were found to be subjective (after editorial and review articles were removed)."
One of the main obstacles to producing a sentence-level subjectivity classifier is a lack of training data.
"To train a document-level classifier, one can easily find col-lections of subjective texts, such as editorials and reviews."
"For example,[REF_CITE]collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g., number of stars) given by the reviewer."
It is much harder to obtain collections of individual sentences that can be easily identified as sub- jective or objective.
Previous work on sentence-level sub-jectivity classificati[REF_CITE]used training corpora that had been manually annotated for subjectiv-ity.
"Manually producing annotations is time consuming, so the amount of available annotated sentence data is rel-atively small."
The goal of our research is to use high-precision sub-jectivity classifiers to automatically identify subjective and objective sentences in unannotated text corpora.
"The high-precision classifiers label a sentence as subjective or objective when they are confident about the classification, and they leave a sentence unlabeled otherwise."
"Unanno-tated texts are easy to come by, so even if the classifiers can label only 30% of the sentences as subjective or ob-jective, they will still produce a large collection of labeled sentences."
"Most importantly, the high-precision classi-fiers can generate a much larger set of labeled sentences than are currently available in manually created data sets."
Information extraction (IE) systems typically use lexico-syntactic patterns to identify relevant information.
"The specific representation of these patterns varies across sys-tems, but most patterns represent role relationships sur-rounding noun and verb phrases."
"For example, an IE system designed to extract information about hijackings might use the pattern hijacking of &lt;x&gt;, which looks for the noun hijacking and extracts the object of the prepo-sition of as the hijacked vehicle."
"The pattern &lt;x&gt; was hijacked would extract the hijacked vehicle when it finds the verb hijacked in the passive voice, and the pattern &lt;x&gt; hijacked would extract the hijacker when it finds the verb hijacked in the active voice."
One of our hypotheses was that extraction patterns would be able to represent subjective expressions that have noncompositional meanings.
"For example, consider the common expression drives (someone) up the wall, which expresses the feeling of being annoyed with some-thing."
"The meaning of this expression is quite different from the meanings of its individual words (drives, up, wall)."
"Furthermore, this expression is not a fixed word sequence that could easily be captured by N-grams."
"It is a relatively flexible construction that may be more gener-ally represented as &lt;x&gt; drives &lt;y&gt; up the wall, where x and y may be arbitrary noun phrases."
"This pattern would match many different sentences, such as “George drives me up the wall,” “She drives the mayor up the wall,” or “The nosy old man drives his quiet neighbors up the wall.”"
We also wondered whether the extraction pattern rep-resentation might reveal slight variations of the same verb or noun phrase that have different connotations.
"For ex-ample, you can say that a comedian bombed last night, which is a subjective statement, but you can’t express this sentiment with the passive voice of bombed."
"In Sec-tion 3.2, we will show examples of extraction patterns representing subjective expressions which do in fact ex-hibit both of these phenomena."
A variety of algorithms have been developed to au-tomatically learn extraction patterns.
"Most of these algorithms require special training resources, such as texts annotated with domain-specific tags (e.g., Au-toSlog[REF_CITE], CRYSTAL[REF_CITE], RAPIER[REF_CITE], SRV[REF_CITE], WHISK[REF_CITE]) or manually defined key-words, frames, or object recognizers (e.g., PALKA[REF_CITE]and LIEP[REF_CITE])."
"AutoSlog-TS[REF_CITE]takes a different approach, requiring only a corpus of unannotated texts that have been separated into those that are related to the target do-main (the “relevant” texts) and those that are not (the “ir-relevant” texts)."
"Most recently, two bootstrapping algo-rithms have been used to learn extraction patterns."
Meta-bootstrapping[REF_CITE]learns both extrac-tion patterns and a semantic lexicon using unannotated texts and seed words as input.
ExDisco[REF_CITE]uses a bootstrapping mechanism to find new ex-traction patterns using unannotated texts and some seed patterns as the initial input.
"For our research, we adopted a learning process very similar to that used by AutoSlog-TS, which requires only relevant texts and irrelevant texts as its input."
We describe this learning process in more detail in the next section.
"We have developed a bootstrapping process for subjec-tivity classification that explores three ideas: (1) high-precision classifiers can be used to automatically iden-tify subjective and objective sentences from unannotated texts, (2) this data can be used as a training set to auto-matically learn extraction patterns associated with sub-jectivity, and (3) the learned patterns can be used to grow the training set, allowing this entire process to be boot-strapped."
Figure 1 shows the components and layout of the boot-strapping process.
The process begins with a large collec-tion of unannotated text and two high precision subjec-tivity classifiers.
"One classifier searches the unannotated corpus for sentences that can be labeled as subjective with high confidence, and the other classifier searches for sentences that can be labeled as objective with high confidence."
All other sentences in the corpus are left unlabeled.
"The labeled sentences are then fed to an ex-traction pattern learner, which produces a set of extrac-tion patterns that are statistically correlated with the sub-jective sentences (we will call these the subjective pat- terns)."
These patterns are then used to identify more sen-tences within the unannotated texts that can be classified as subjective.
The extraction pattern learner can then re-train using the larger training set and the process repeats.
The subjective patterns can also be added to the high-precision subjective sentence classifier as new features to improve its performance.
The dashed lines in Figure 1 represent the parts of the process that are bootstrapped.
"In this section, we will describe the high-precision sen-tence classifiers, the extraction pattern learning process, and the details of the bootstrapping process."
The high-precision classifiers (HP-Subj and HP-Obj) use lists of lexical items that have been shown in previous work to be good subjectivity clues.
"Most of the items are single words, some are N-grams, but none involve syntac-tic generalizations as in the extraction patterns."
Any data used to develop this vocabulary does not overlap with the test sets or the unannotated data used in this paper.
"Many of the subjective clues are from manually de-veloped resources, including entries[REF_CITE], Framenet lemmas with frame element experiencer[REF_CITE], adjec-tives manually annotated for polarity[REF_CITE], and subjectivity clues listed[REF_CITE]."
"Others were derived from corpora, in-cluding subjective nouns learned from unannotated data using bootstrapping[REF_CITE]."
"The subjectivity clues are divided into those that are strongly subjective and those that are weakly subjective, using a combination of manual review and empirical re-sults on a small training set of manually annotated data."
"As the terms are used here, a strongly subjective clue is one that is seldom used without a subjective meaning, whereas a weakly subjective clue is one that commonly has both subjective and objective uses."
The high-precisionsubjective classifier classifies a sen-tence as subjective if it contains two or more of the strongly subjective clues.
"On a manually annotated test set, this classifier achieves 91.5% precision and 31.9% recall (that is, 91.5% of the sentences that it selected are subjective, and it found 31.9% of the subjective sentences in the test set)."
"This test set consists of 2197 sentences, 59% of which are subjective."
The high-precision objective classifier takes a different approach.
"Rather than looking for the presence of lexical items, it looks for their absence."
"It classifies a sentence as objective if there are no strongly subjective clues and at most one weakly subjective clue in the current, previous, and next sentence combined."
"Why doesn’t the objective classifier mirror the subjective classifier, and consult its own list of strongly objective clues?"
"There are certainly lexical items that are statistically correlated with the ob- jective class (examples are cardinal numbers[REF_CITE], and words such as per, case, market, and to-tal), but the presence of such clues does not readily lead to high precision objective classification."
"Add sarcasm or a negative evaluation to a sentence about a dry topic such as stock prices, and the sentence becomes subjec-tive."
"Conversely, add objective topics to a sentence con-taining two strongly subjective words such as odious and scumbag, and the sentence remains subjective."
"The performanceof the high-precisionobjective classi-fier is a bit lower than the subjective classifier: 82.6% pre-cision and 16.4% recall on the test set mentioned above (that is, 82.6% of the sentences selected by the objective classifier are objective, and the objective classifier found 16.4% of the objective sentences in the test set)."
"Al-though there is room for improvement, the performance proved to be good enough for our purposes."
"To automatically learn extraction patterns that are associ-ated with subjectivity, we use a learning algorithm similar to AutoSlog-TS[REF_CITE]."
"For training, AutoSlog-TS uses a text corpus consisting of two distinct sets of texts: “relevant” texts (in our case, subjective sentences) and “irrelevant” texts (in our case, objective sentences)."
A set of syntactic templates represents the space of pos-sible extraction patterns.
The learning process has two steps.
"First, the syntac-tic templates are applied to the training corpus in an ex-haustive fashion, so that extraction patterns are generated for (literally) every possible instantiation of the templates that appears in the corpus."
The left column of Figure 2 shows the syntactic templates used by AutoSlog-TS.
The right column shows a specific extraction pattern that was learned during our subjectivity experiments as an instan-tiation of the syntactic form on the left.
"For example, the pattern &lt;subj&gt; was satisfied [Footnote_1] will match any sentence where the verb satisfied appears in the passive voice."
1 This is a shorthand notation for the internal representation.
The pattern &lt;subj&gt; dealt blow represents a more complex ex-pression that will match any sentence that contains a verb phrase with head=dealt followed by a direct object with head=blow.
This would match sentences such as “The experience dealt a stiff blow to his pride.”
"It is important to recognize that these patterns look for specific syntactic constructions produced by a (shallow) parser, rather than exact word sequences."
The second step of AutoSlog-TS’s learning process ap-plies all of the learned extraction patterns to the train-ing corpus and gathers statistics for how often each pattern occurs in subjective versus objective sentences.
AutoSlog-TS then ranks the extraction patterns using a metric called RlogF[REF_CITE]and asks a human to review the ranked list and make the final decision about which patterns to keep.
"In contrast, for this work we wanted a fully automatic process that does not depend on a human reviewer, and we were most interested in finding patterns that can iden-tify subjective expressions with high precision."
So we ranked the extraction patterns using a conditional proba-bility measure: the probability that a sentence is subjec-tive given that a specific extraction pattern appears in it.
The exact formula is:
"Pr(subjective | pattern i ) = subjffreqreq(pattern(pattern i ) i ) where subjfreq(pattern i ) is the frequency of pattern i in subjective training sentences, and freq(pattern i ) is the frequency of pattern i in all training sentences. (This may also be viewed as the precision of the pattern on the training data.)"
"Finally, we use two thresholds to select ex-traction patterns that are strongly associated with subjec-tivity in the training data."
We choose extraction patterns for which freq(pattern i ) ≥ θ 1 and Pr(subjective | pattern i ) ≥ θ 2 .
"Figure 3 shows some patterns learned by our system, the frequency with which they occur in the training data (FREQ) and the percentage of times they occur in sub-jective sentences (%SUBJ)."
"For example, the first two rows show the behavior of two similar expressions us-ing the verb asked. 100% of the sentences that contain asked in the passive voice are subjective, but only 63% of the sentences that contain asked in the active voice are subjective."
A human would probably not expect the ac-tive and passive voices to behave so differently.
"To un-derstand why this is so, we looked in the training data and found that the passive voice is often used to query someone about a specific opinion."
"For example, here is one such sentence from our training set: “Ernest Bai Ko-roma of RITCORP was asked to address his supporters on his views relating to ‘full blooded Temne to head APC’.”"
"In contrast, many of the sentences containing asked in the active voice are more general in nature, such as “The mayor asked a newly formed JR about his petition.”"
"Figure 3 also shows that expressions using talk as a noun (e.g., “Fred is the talk of the town”) are highly cor-related with subjective sentences, while talk as a verb (e.g., “The mayor will talk about...”) are found in a mix of subjective and objective sentences."
"Not surprisingly, longer expressions tend to be more idiomatic (and sub-jective) than shorter expressions (e.g., put an end (to) vs. put; is going to be vs. is going; was expected from vs. was expected)."
"Finally, the last two rows of Figure 3 show that expressions involving the noun fact are highly correlated with subjective expressions!"
"These patterns match sen-tences such as The fact is... and ... is a fact, which appar-ently are often used in subjective contexts."
"This example illustrates that the corpus-based learning method can find phrases that might not seem subjective to a person intu-itively, but that are reliable indicators of subjectivity."
"The text collection that we used consists of English-language versions of foreign news documents from FBIS, the U.S. Foreign Broadcast Information Service."
The data is from a variety of countries.
"Our system takes unannotated data as input, but we needed annotated data to evaluate its performance."
"We briefly describe the man-ual annotation scheme used to create the gold-standard, and give interannotator agreement results."
We only mention aspects of the annotation scheme relevant to this paper.
"The scheme was inspired by work in linguistics and literary theory on subjectiv-ity, which focuses on how opinions, emotions, etc. are expressed linguistically in context[REF_CITE]."
The goal is to identify and characterize expressions of private states in a sentence.
"Private state is a general covering term for opinions, evaluations, emotions, and specula-tions[REF_CITE]."
"For example, in sentence (1) the writer is expressing a negative evaluation. (1) “The time has come, gentlemen, for Sharon, the as-sassin, to realize that injustice cannot last long.”"
Sentence (2) reflects the private state of Western coun-tries.
"Mugabe’s use of overwhelmingly also reflects a pri-vate state, his positive reaction to and characterization of his victory. (2) “Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had over-whelmingly won Zimbabwe’s presidential election.”"
Annotators are also asked to judge the strength of each private state.
"A private state may have low, medium, high or extreme strength."
"To allow us to measure interannotator agreement, three annotators (who are not authors of this paper) indepen-dently annotated the same 13 documents with a total of 210 sentences."
"We begin with a strict measure of agree-ment at the sentence level by first considering whether the annotator marked any private-state expression, of any strength, anywhere in the sentence."
"If so, the sentence is subjective."
"Otherwise, it is objective."
The average pair-wise percentage agreement is 90% and the average pair-wise κ value is 0.77.
"One would expect that there are clear cases of objec-tive sentences, clear cases of subjective sentences, and borderline sentences in between."
The agreement study supports this.
"In terms of our annotations, we define a sentence as borderline if it has at least one private-state expression identified by at least one annotator, and all strength ratings of private-state expressions are low."
"When those sentences are removed, the average pairwise percentage agreement increases to 95% and the average pairwise κ value increases to 0.89."
"As expected, the majority of disagreement cases in-volve low-strength subjectivity."
The annotators consis-tently agree about which are the clear cases of subjective sentences.
This leads us to define the gold-standard that we use when evaluating our results.
A sentence is subjec-tive if it contains at least one private-state expression of medium or higher strength.
"The second class, which we call objective, consists of everything else."
"Our pool of unannotated texts consists of 302,163 indi-vidual sentences."
"The HP-Subj classifier initially labeled roughly 44,300 of these sentences as subjective, and the HP-Obj classifier initially labeled roughly 17,000 sen-tences as objective."
"In order to keep the training set rel-atively balanced, we used all 17,000 objective sentences and 17,000 of the subjective sentences as training data for the extraction pattern learner. 17,073 extraction patterns were learned that have frequency ≥ 2 and Pr(subjective | pattern i ) ≥ .60 on the training data."
"We then wanted to determine whether the extraction patterns are, in fact, good indicators of sub-jectivity."
"To evaluate the patterns, we applied different subsets of them to a test set to see if they consistently oc-cur in subjective sentences."
"This test set consists of 3947 sentences, 54% of which are subjective."
Figure 4 shows sentence recall and pattern (instance-level) precision for the learned extraction patterns on the test set.
"In this figure, precision is the proportion of pat-tern instances found in the test set that are in subjective sentences, and recall is the proportion of subjective sen-tences that contain at least one pattern instance."
"We tried all combinations of θ 1 = {2,10} and θ 2 = {.60,.65,.70,.75,.80,.85,.90,.95,1.0}."
"The data points corresponding to θ 1 =2 are shown on the upper line in Figure 4, and those corresponding to θ 1 =10 are shown on the lower line."
"For example, the data point correspond-ing to θ 1 =10 and θ 2 =.90 evaluates only the extraction pat-terns that occur at least 10 times in the training data and with a probability ≥ .90 (i.e., at least 90% of its occur-rences are in subjective training sentences)."
"Overall, the extraction patterns perform quite well."
"The precision ranges from 71% to 85%, with the expected tradeoff between precision and recall."
This experiment confirms that the extraction patterns are effective at rec-ognizing subjective expressions.
"In our second experiment, we used the learned extrac-tion patterns to classify previously unlabeled sentences from the unannotated text collection."
The new subjec-tive sentences were then fed back into the Extraction Pat-tern Learner to complete the bootstrapping cycle depicted by the rightmost dashed line in Figure 1.
The Pattern-based Subjective Sentence Classifier classifies a sentence as subjective if it contains at least one extraction pattern with θ 1 ≥5 and θ 2 ≥1.0 on the training data.
"This process produced approximately 9,500 new subjective sentences that were previously unlabeled."
"Since our bootstrapping process does not learn new ob-jective sentences, we did not want to simply add the new subjective sentences to the training set, or it would be-come increasingly skewed toward subjective sentences."
"Since HP-Obj had produced roughly 17,000 objective sentences used for training, we used the 9,500 new sub-jective sentences along with 7,500 of the previously iden-tified subjective sentences as our new training set."
"In other words, the training set that we used during the sec-ond bootstrapping cycle contained exactly the same ob-jective sentences as the first cycle, half of the same sub-jective sentences as the first cycle, and 9,500 brand new subjective sentences."
"On this second cycle of bootstrapping, the extraction pattern learner generated many new patterns that were not discovered during the first cycle. 4,248 new patterns were found that have θ 1 ≥2 and θ 2 ≥.60."
"If we consider only the strongest (most subjective) extraction patterns, 308 new patterns were found that had θ 1 ≥10 and θ 2 ≥1.0."
This is a substantial set of new extraction patterns that seem to be very highly correlated with subjectivity.
An open question was whether the new patterns pro-vide additional coverage.
"To assess this, we did a sim-ple test: we added the 4,248 new patterns to the origi-nal set of patterns learned during the first bootstrapping cycle."
Then we repeated the same analysis that we de-pict in Figure 4.
"In general, the recall numbers increased by about 2-4% while the precision numbers decreased by less, from 0.5-2%."
"In our third experiment, we evaluated whether the learned patterns can improve the coverage of the high-precision subjectivity classifier (HP-Subj), to complete the bootstrapping loop depicted in the top-most dashed line of Figure 1."
"Our hope was that the patterns would al-low more sentences from the unannotated text collection to be labeled as subjective, without a substantial drop in precision."
"For this experiment, we selected the learned extraction patterns that had θ 1 ≥ 10 and θ 2 ≥ 1.0 on the training set, since these seemed likely to be the most reli-able (high precision) indicators of subjectivity."
We modified the HP-Subj classifier to use extraction patterns as follows.
All sentences labeled as subjective by the original HP-Subj classifier are also labeled as sub-jective by the new version.
"For previously unlabeled sen-tences, the new version classifies a sentence as subjective if (1) it contains two or more of the learned patterns, or (2) it contains one of the clues used by the original HP-Subj classifier and at least one learned pattern."
Table 1 shows the performance results on the test set mentioned in Section 3.1 (2197 sentences) for both the original HP-Subj classifier and the new version that uses the learned extraction patterns.
"The extraction patterns produce a 7.2 percentage point gain in coverage, and only a 1.1 percent- age point drop in precision."
"This result shows that the learned extraction patterns do improve the performance of the high-precision subjective sentence classifier, allow-ing it to classify more sentences as subjective with nearly the same high reliability."
Table 2 gives examples of patterns used to augment the HP-Subj classifier which do not overlap in non-function words with any of the clues already known by the original system.
"For each pattern, we show an example sentence from our corpus that matches the pattern."
This research explored several avenues for improving the state-of-the-art in subjectivity analysis.
"First, we demon-strated that high-precision subjectivity classification can be used to generate a large amount of labeled training data for subsequent learning algorithms to exploit."
"Second, we showed that an extraction pattern learning technique can learn subjective expressions that are linguistically richer than individual words or fixed phrases."
"We found that similar expressions may behave very differently, so that one expression may be strongly indicative of subjectivity but the other may not."
"Third, we augmented our origi-nal high-precision subjective classifier with these newly learned extraction patterns."
This bootstrapping process resulted in substantially higher recall with a minimal loss in precision.
"In future work, we plan to experiment with different configurations of these classifiers, add new sub-jective language learners in the bootstrapping process, and address the problem of how to identify new objec-tive sentences during bootstrapping."
We are very grateful to Theresa Wilson for her invaluable programming support and help with data preparation.
Successful application of multi-view co-training algorithms relies on the ability to factor the available features into views that are compatible and uncorrelated.
This can potentially preclude their use on problems such as coreference resolution that lack an obvious feature split.
"To bootstrap coref-erence classifiers, we propose and eval-uate a single-view weakly supervised al-gorithm that relies on two different learn-ing algorithms in lieu of the two different views required by co-training."
"In addition, we investigate a method for ranking un-labeled instances to be fed back into the bootstrapping loop as labeled data, aiming to alleviate the problem of performance deterioration that is commonly observed in the course of bootstrapping."
"Co-training[REF_CITE]is a weakly supervised paradigm that learns a task from a small set of labeled data and a large pool of unlabeled data using separate, but redundant views of the data (i.e. using disjoint feature subsets to represent the data)."
"To ensure provable performance guaran-tees, the co-training algorithm assumes as input a set of views that satisfies two fairly strict condi-tions."
"First, each view must be sufficient for learn-ing the target concept."
"Second, the views must be conditionally independent of each other given the class."
Empirical results on artificial data sets[REF_CITE]and[REF_CITE]confirm that co-training is sensitive to these assump-tions.
"Indeed, although the algorithm has been ap-plied successfully to natural language processing (NLP) tasks that have a natural view factorization (e.g. web page classificati[REF_CITE]and named entity classificati[REF_CITE]), there has been little success, and a number of reported problems, when applying co-training to NLP data sets for which no natural fea-ture split has been found (e.g. anaphora resoluti[REF_CITE])."
"As a result, researchers have begun to investigate co-training procedures that do not require explicit view factorization."
"1[REF_CITE]bootstrap two parsers that use dif-ferent statistical models via co-training. Hence, the two parsers can effectively be viewed as two different learning algorithms."
The intuition is that the two learning algorithms can potentially substitute for the two views: different learners have different rep-resentation and search biases and can complement each other by inducing different hypotheses from the data.
"Despite their similarities, the principles under-lying the Goldman and Zhou and Steedman et al. co-training algorithms are fundamentally different."
"In particular, Goldman and Zhou rely on hypothesis testing to select new instances to add to the labeled data."
"On the other hand, Steedman et al. use two learning algorithms that correspond to coarsely dif-ferent features, thus retaining in spirit the advantages provided by conditionally independent feature splits in the Blum and Mitchell algorithm."
The goal of this paper is two-fold.
"First, we propose a single-view algorithm for bootstrapping coreference classifiers."
"Like anaphora resolution, noun phrase coreference resolution is a problem for which a natural feature split is not readily available."
"In related work[REF_CITE], we com-pare the performance of the Blum and Mitchell co-training algorithm with that of two existing single-view bootstrapping algorithms — self-training with bagging[REF_CITE]and EM[REF_CITE]— on coreference resolution, and show that single-view weakly supervised learners are a vi-able alternative to co-training for the task."
This pa-per instead focuses on developing a single-view al-gorithm that combines aspects of each of the Gold-man and Zhou and Steedman et al. algorithms.
"Second, we investigate a new method that, in-spired[REF_CITE], ranks unlabeled instances to be added to the labeled data in an at-tempt to alleviate a problem commonly observed in bootstrapping experiments — performance deterio-ration due to the degradation in the quality of the labeled data as bootstrapping progresses[REF_CITE]."
"In a set of baseline experiments, we first demon-strate that multi-view co-training fails to boost the performance of the coreference system under var-ious parameter settings."
"We then show that our single-view weakly supervised algorithm success-fully bootstraps the coreference classifiers, boost-ing the F-measure score by 9-12% on two standard coreference data sets."
"Finally, we present experi-mental results that suggest that our method for rank-ing instances is more resistant to performance dete-rioration in the bootstrapping process than Blum and Mitchell’s “rank-by-confidence” method."
Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a doc-ument. [Footnote_2]
2 Concrete examples of the coreference task can be found[REF_CITE]and[REF_CITE].
"In this section, we give an overview of the coreference resolution system to which the boot- strapping algorithms will be applied."
The framework underlying the coreference sys-tem is a standard combination of classification and clustering (see[REF_CITE]for details).
"Coreference resolution is first recast as a classifica-tion task, in which a pair of NPs is classified as co-referring or not based on constraints that are learned from an annotated corpus."
A separate clustering mechanism then coordinates the possibly contradic-tory pairwise classifications and constructs a parti-tion on the set of NPs.
"When the system operates within the weakly supervised setting, a weakly su-pervised algorithm bootstraps the coreference classi-fier from the given labeled and unlabeled data rather than from a much larger set of labeled instances."
"The clustering algorithm, however, is not manipulated by the bootstrapping procedure."
"We employ naive Bayes and decision list learners in our single-view, multiple-learner framework for bootstrapping coreference classifiers."
This section gives an overview of the two learners.
"A naive Bayes (NB) classifier is a generative classi-fier that assigns to a test instance i with feature val-ues &lt;x 1 , . . . , x m &gt; the maximum a posteriori (MAP) label y ∗ , which is determined as follows: y ∗ = arg max y"
P(y | i) = arg max y P(y)P(i | y) m = arg max y P(y) P(x i | y) i=1
"The first equality above follows from the definition of MAP, the second one from Bayes rule, and the last one from the conditional independence assumption of the feature values."
We determine the class priors P(y) and the class densities P(x i | y) directly from the training data using add-one smoothing.
Our decision list (DL) algorithm is based on that de-scribed[REF_CITE].
"For each avail-able feature f i and each possible value v j of f i in the training data, the learner induces an element of the decision list for each class y."
"The elements in the list are sorted in decreasing order of the strength associ-ated with each element, which is defined as the con-ditional probability P(y | f i = v j ) and is estimated based on the training data as follows:"
P(y | f i = v j ) =
"NN((ff ii ==vv jj ,)y+) +kαα"
"N(x) is the frequency of event x in the training data, α a smoothing parameter, and k the number of classes."
"In this paper, k = 2 and we set α to 0.01."
A test instance is assigned the class associated with the first element of the list whose predicate is satis-fied by the description of the instance.
"While generative classifiers estimate class densi-ties, discriminative classifiers like decision lists fo-cus on approximating class boundaries."
"Table 1 pro-vides the justifications for choosing these two learn-ers as components in our single-view, multi-learner bootstrapping algorithm."
"Based on observations of the coreference task and the features employed by our coreference system, the justifications suggest that the two learners can potentially compensate for each other’s weaknesses."
"In this section, we describe the Blum and Mitchell (B&amp;M) multi-view co-training algorithm and apply it to coreference resolution."
The intuition behind the B&amp;M co-training algorithm is to train two classifiers that can help augment each other’s labeled data by exploiting two separate but redundant views of the data.
"Specifically, each clas-sifier is trained using one view of the labeled data and predicts labels for all instances in the data pool, which consists of a randomly chosen subset of the unlabeled data."
"Each then selects its most confident predictions, and adds the corresponding instances with their predicted labels to the labeled data while maintaining the class distribution in the labeled data."
The number of instances to be added to the la-beled data by each classifier at each iteration is lim-ited by a pre-specified growth size to ensure that only the instances that have a high probability of be-ing assigned the correct label are incorporated.
The data pool is replenished with instances from the un-labeled data and the process is repeated.
"During testing, each classifier makes an indepen-dent decision for a test instance."
"In this paper, the decision associated with the higher confidence is taken to be the final prediction for the instance."
One of the goals of the experiments is to enable a fair comparison of the multi-view algorithm with our single-view bootstrapping algorithm.
"Since the B&amp;M co-training algorithm is sensitive not only to the views employed but also to other input parame- ters such as the pool size and the growth size[REF_CITE], we evaluate the algorithm under different parameter settings, as described below."
We use the[REF_CITE]and[REF_CITE]coreference data sets for evaluation.
"The training set is composed of 30 “dry run” texts, from which 491659 and 482125 NP pair instances are generated for the MUC-6 and MUC-7 data sets, re-spectively."
The remaining instances are used as un-labeled data.
Testing is performed by applying the bootstrapped coreference classifier and the cluster-ing algorithm described in section 2 on the 20–30 “formal evaluation” texts for each of the MUC-6 and MUC-7 data sets.
"Two sets of experiments are conducted, one using naive Bayes as the underlying supervised learning algorithm and the other the decision list learner."
All results reported are averages across five runs.
The co-training param-eters are set as follows.
"We used three methods to generate the views from the 25 features used by the coreference system: Mueller et al.’s (2002) greedy method, ran-dom splitting of features into views, and splitting of features according to the feature type (i.e. lexico-syntactic vs. non-lexico-syntactic features). [Footnote_4]"
4 Space limitation precludes a detailed description of these methods.[REF_CITE]for details.
"We tested values of 500, 1000, 5000."
"We tested values of 10, 50, 100, 200."
"Results are shown in Table 2, where performance is reported in terms of recall, precision, and F-measure using the model-theoretic MUC scoring program[REF_CITE]."
"The baseline coreference sys-tem, which is trained only on the initially labeled data using all of the features, achieves an F-measure of 51.6 (NB) and 28.7 (DL) on the MUC-6 data set and 40.1 (NB) and 45.8 (DL) on MUC-7."
The results shown in row 2 of Table 2 correspond to the best F-measure scores achieved by co-training across all of the parameter combinations described in the previous subsection.
"In comparison to the baseline, co-training is able to improve system per-formance in only two of the four classifier/data set combinations: F-measure increases by 2% and 6% for MUC-6/DL and MUC-7/NB, respectively."
"Nev-ertheless, co-training produces high-precision clas-sifiers in all four cases (at the expense of recall)."
"In practical applications in which precision is critical, the co-training classifiers may be preferable to the baseline classifiers despite the fact that they achieve similar F-measure scores."
Figure 1 depicts the learning curve for the co-training run that gives rise to the best F-measure for the MUC-6 data set using naive Bayes.
"The hor-izontal (dotted) line shows the performance of the baseline system, as described above."
"As co-training progresses, F-measure rises to 48.7 at iteration ten and gradually drops to and stabilizes at 42.9."
We ob-serve similar performance trends for the other clas-sifier/data set combinations.
The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances[REF_CITE].
"In this section, we describe and evaluate our single-view, multi-learner bootstrapping algorithm, which combines ideas[REF_CITE]and[REF_CITE]."
We will start by giving an overview of these two co-training algorithms.
The Goldman and Zhou (G&amp;Z) Algorithm.
"This single-view algorithm begins by training two classifiers on the initially labeled data using two different learning algorithms; it requires that each classifier partition the instance space into a set of equivalence classes (e.g. in a decision tree, each leaf node defines an equivalence class)."
Each classi-fier then considers each equivalence class and uses hypothesis testing to determine if adding all unla-beled instances within the equivalence class to the other classifier’s labeled data will improve the per-formance of its counterparts.
The process is then repeated until no more instances can be labeled.
The Steedman et al. (Ste) Algorithm.
This algo-rithm is a variation of B&amp;M applied to two diverse statistical parsers.
"Initially, each parser is trained on the labeled data."
"Each then parses and scores all sentences in the data pool, and then adds the most confidently parsed sentences to the training data of the other parser."
"The parsers are retrained, and the process is repeated for several iterations."
The algorithm differs from B&amp;M in three main respects.
"First, the training data of the two parsers diverge after the first co-training iteration."
"Second, the data pool is flushed and refilled entirely with in-stances from the unlabeled data after each iteration."
This reduces the possibility of having unreliably la-beled sentences accumulating in the pool.
"Finally, the two parsers, each of which is assumed to hold a unique “view” of the data, are effectively two differ-ent learning algorithms."
"As mentioned before, our algorithm uses two dif-ferent learning algorithms to train two classifiers on the same set of features (i.e. the full feature set)."
"At each bootstrapping iteration, each classifier la-bels and scores all instances in the data pool."
The highest scored instances labeled by one classifier are added to the training data of the other classifier and vice versa.
"Since the two classifiers are trained on the same view, it is important to maintain a separate training set for each classifier: this reduces the prob-ability that the two classifiers converge to the same hypothesis at an early stage and hence implicitly in-creases the ability to bootstrap."
"Like Ste, the entire data pool is replenished with instances drawn from the unlabeled data after each iteration, and the pro-cess is repeated."
"So our algorithm is effectively Ste applied to coreference resolution — instead of two parsing algorithms that correspond to different fea-tures, we use two learning algorithms, each of which relies on the same set of features as in G&amp;Z."
"The similarities and differences among B&amp;M, G&amp;Z, Ste, and our algorithm are summarized in Table 3."
We tested different pool sizes and growth sizes as specified in section 4.2 to determine the best pa-rameter setting for our algorithm.
"For both data sets, the best F-measure score is achieved using a pool size of 5000 and a growth size of 50."
The re-sults under this parameter setting are given in row 3 of Table 2.
"In comparison to the baseline, we see dramatic improvement in F-measure for both clas-sifiers and both data sets."
"In addition, we see si-multaneous gains in recall and precision in all cases except MUC-7/DL."
"Furthermore, single-view boot-strapping beats co-training (in terms of F-measure scores) by a large margin in all four cases."
"These results provide suggestive evidence that single-view, multi-learner bootstrapping might be a better alter-native to its multi-view, single-learner counterparts for coreference resolution."
The bootstrapping run that corresponds to this pa-rameter setting for the MUC-6 data set using naive Bayes is shown in Figure 2.
"Again, we see a “typi- cal” bootstrapping curve: an initial rise in F-measure followed by a gradual deterioration."
"In comparison to Figure 1, the recall level achieved by co-training is much lower than that of single-view bootstrapping."
This appears to indicate that each co-training view is insufficient for learning the target concept: the fea-ture split limits any interaction of features that can produce better recall.
"Finally, Figure 2 shows that performance in-creases most rapidly in the first 200 iterations."
This provides indirect evidence that the two classifiers have acquired different hypotheses from the ini-tial data and are exchanging information with each other.
"To ensure that the classifiers are indeed bene-fiting from each other, we conducted a self-training experiment for each classifier separately: at each self-training iteration, each classifier labels all 5000 instances in the data pool using all available features and selects the most confidently labeled 50 instances for addition to its labeled data. [Footnote_5] The best F-measure scores achieved by self-training are shown in the last row of Table 2."
"5 Note that this is self-training without bagging, unlike the self-training algorithm discussed[REF_CITE]."
"Overall, self-training only yields marginal performance gains over the baseline."
"Nevertheless, self-training outperforms co-training in both cases where naive Bayes is used."
"While these results seem to suggest that co-training is inherently handicapped for coreference resolu-tion, there are two plausible explanations against this conclusion."
"First, the fact that self-training has access to all of the available features may account for its superior performance to co-training."
This is again partially supported by the fact that the recall level achieved by co-training is lower than that of self-training in both cases in which self-training outperforms co-training.
"As we have seen before, F-measure scores ulti-mately decrease as bootstrapping progresses."
"If the drop were caused by the degradation in the quality of the bootstrapped data, then a more “conservative” instance selection method than that of B&amp;M would help alleviate this problem."
"Our hypothesis is that selection methods that are based solely on the con-fidence assigned to an instance by a single classifier i 1 &gt; i 2 if any of the following is true: [µ(C 1 (i 1 )) = µ(C 2 (i 1 ))] ∧ [µ(C 1 (i 2 )) = µ(C 2 (i 2 ))] [µ(C 1 (i 1 )) = µ(C 2 (i 1 ))] ∧ [µ(C 1 (i 2 )) = µ(C 2 (i 2 ))] ∧ [|C 1 (i 1 ) [µ(C 1 (i 1 )) = µ(C 2 (i 1 ))] ∧ [µ(C 1 (i 2 )) = µ(C 2 (i 2 ))] ∧ [max(C 1 − C 2 (i 1 )| &gt; |C 1 (i 2 ) − C 2 (i 2 )|] (i 1 ), 1 − C 1 (i 1 )) &gt; max(C 1 (i 2 ), 1 − C 1 (i 2 ))] may be too liberal."
"In particular, these methods al-low the addition of instances with opposing labels to the labeled data; this can potentially result in in-creased incompatibility between the classifiers."
"Consequently, we develop a new procedure for ranking instances in the data pool."
The bootstrap-ping algorithm then selects the highest ranked in-stances to add to the labeled data in each iteration.
The method favors instances whose label is agreed upon by both classifiers (Preference 1).
"However, incorporating instances that are confidently labeled by both classifiers may reduce the probability of acquiring new information from the data."
"There-fore, the method imposes an additional preference for instances that are confidently labeled by one but not both (Preference 2)."
"If none of the instances receives the same label from the classifiers, the method resorts to the “rank-by-confidence” method used by B&amp;M (Preference 3)."
"More formally, define a binary classifier as a func-tion that maps an instance to a value that indicates the probability that it is labeled as positive."
"Now, let µ be a function that rounds a number to its near-est integer."
"Given two binary classifiers C 1 and C 2 and instances i 1 and i 2 , the ranking method shown in Figure 3 uses the three preferences described above to impose a partial ordering on the given instances for incorporation into C 2 ’s labeled data."
"The method similarly ranks instances to be added to C 1 ’s labeled data, with the roles of C 1 and C 2 reversed."
6[REF_CITE]tackle this idea of balancing
"In contrast, our focus here is on examining whether a more conservative ranking method can alleviate the problem of perfor-mance deterioration."
"Nevertheless, Preference 2 is inspired by their S int-n selection method, which se-lects an instance if it belongs to the intersection of the set of the n percent highest scoring instances of one classifier and the set of the n percent lowest scoring instances of the other."
"To our knowledge, no previous work has examined a ranking method that combines the three preferences described above."
"To compare our ranking procedure with B&amp;M’s rank-by-confidence method, we repeat the boot-strapping experiment shown in Figure 2 except that we replace B&amp;M’s ranking method with ours."
The learning curves generated using the two ranking methods with naive Bayes for the MUC-6 data set are shown in Figure 4.
The results are consistent with our intuition regarding the two ranking meth-accuracy and coverage by combining EM and active learning. ods.
The B&amp;M ranking method is more liberal.
"In particular, each classifier always selects the most confidently labeled instances to add to the other’s la-beled data at each iteration."
"If the underlying learn-ers have indeed induced two different hypotheses from the data, then each classifier can potentially ac-quire informative instances from the other and yield performance improvements very rapidly."
"In contrast, our ranking method is more conserva-tive in that it places more emphasis on maintaining labeled data accuracy than the B&amp;M method."
"As a result, the classifier learns at a slower rate when compared to that in the B&amp;M case: it is not until iter-ation 600 that we see a sharp rise in F-measure."
"Due to the “liberal” nature of the B&amp;M method, however, its performance drops dramatically as bootstrapping progresses, whereas ours just dips temporarily."
This can potentially be attributed to the more rapid injec-tion of mislabeled instances into the labeled data in the B&amp;M case.
"Overall, our ranking method does not exhibit the performance trend observed with the B&amp;M method: except for the spike between iterations 0 and 100, F-measure does not deteriorate as bootstrapping progresses."
"Since it is hard to deter-mine a “good” stopping point for bootstrapping due to the paucity of labeled data in a weakly supervised setting, our ranking method can potentially serve as an alternative to the B&amp;M method."
"We have proposed a single-view, multi-learner boot-strapping algorithm for coreference resolution and shown empirically that the algorithm is a better al-ternative to the Blum and Mitchell co-training al-gorithm for this task for which no natural feature split has been found."
"In addition, we have investi-gated an example ranking method for bootstrapping that, unlike Blum and Mitchell’s rank-by-confidence method, can potentially alleviate the problem of per-formance deterioration due to the pollution of the la-beled data in the course of bootstrapping."
A Natural Language Generation system produces text using as input semantic data.
One of its very first tasks is to decide which pieces of information to convey in the output.
"This task, called Content Se-lection, is quite domain dependent, requir-ing considerable re-engineering to trans-port the system from one scenario to an-other."
"In this paper, we present a method to acquire content selection rules automat-ically from a corpus of text and associated semantics."
"Our proposed technique was evaluated by comparing its output with in-formation selected by human authors in unseen texts, where we were able to fil-ter half the input data set without loss of recall."
"C ONTENT S ELECTION is the task of choosing the right information to communicate in the output of a Natural Language Generation (NLG) system, given semantic input and a communicative goal."
"In gen-eral, Content Selection is a highly domain dependent task; new rules must be developed for each new do-main, and typically this is done manually."
"Morevoer, it has been argued[REF_CITE]that Content Selection is the most important task from a user’s standpoint (i.e., users may tolerate errors in wording, as long as the information being sought is present in the text)."
Designing content selection rules manually is a tedious task.
"A realistic knowledge base contains a large amount of information that could potentially be included in a text and a designer must examine a sizable number of texts, produced in different sit-uations, to determine the specific constraints for the selection of each piece of information."
Our goal is to develop a system that can auto-matically acquire constraints for the content selec-tion task.
"Our algorithm uses the information we learned from a corpus of desired outputs for the sys-tem (i.e., human-produced text) aligned against re-lated semantic data (i.e., the type of data the sys-tem will use as input)."
"It produces constraints on every piece of the input where constraints dictate if it should appear in the output at all and if so, under what conditions."
"This process provides a filter on the information to be included in a text, identifying all information that is potentially relevant (previously termed global focus[REF_CITE]or viewpoints[REF_CITE])."
"The resulting informa-tion can be later either further filtered, ordered and augmented by later stages in the generation pipeline (e.g., see the spreading activation algorithm used in ILEX[REF_CITE])."
"We focus on descriptive texts which realize a sin-gle, purely informative, communicative goal, as op-posed to cases where more knowledge about speaker intentions are needed."
"In particular, we present ex-periments on biographical descriptions, where the planned system will generate short paragraph length texts summarizing important facts about famous people."
The kind of text that we aim to generate is shown in Figure 1.
The rules that we aim to acquire will specify the kind of information that is typically included in any biography.
"In some cases, whether the information is included or not may be condi-tioned on the particular values of known facts (e.g., the occupation of the person being described —we may need different content selection rules for artists than politicians)."
"To proceed with the experiments described here, we acquired a set of semantic infor-mation and related biographies from the Internet and used this corpus to learn Content Selection rules."
Our main contribution is to analyze how varia-tions in the data influence changes in the text.
We perform such analysis by splitting the semantic input into clusters and then comparing the language mod-els of the associated clusters induced in the text side (given the alignment between semantics and text in the corpus).
"By doing so, we gain insights on the rel-ative importance of the different pieces of data and, thus, find out which data to include in the generated text."
"The rest of this paper is divided as follows: in the next section, we present the biographical domain we are working with, together with the corpus we have gathered to perform the described experiments."
Sec-tion 3 describes our algorithm in detail.
"The exper-iments we perform to validate it, together with their results, are discussed in Section 4."
Section 5 sum-marizes related work in the field.
"Our final remarks, together with proposed future work conclude the pa-per."
"The research described here is done for the auto-matic construction of the Content Selection mod-ule of P RO G EN IE[REF_CITE], a biography generator under construction."
Biogra-phy generation is an exciting field that has attracted practitioners of NLG in the past[REF_CITE].
"It has the advantages of being a constrained domain amenable to current generation approaches, while at the same time of-fering more possibilities than many constrained do-mains, given the variety of styles that biographies exhibit, as well as the possibility for ultimately gen-erating relatively long biographies."
We have gathered a resource of text and asso-ciated knowledge in the biography domain.
"More specifically, our resource is a collection of human-produced texts together with the knowledge base a generation system might use as input for gener-ation."
"The knowledge base contains many pieces of information related to the person the biography talks about (and that the system will use to generate that type of biography), not all of which necessarily will appear in the biography."
"That is, the associated knowledge base is not the semantics of the target text but the larger set [Footnote_1] of all things that could possibly be said about the person in question."
"1 The semantics of the text normally contain information not present in our semantic input, although for the sake of Content Selection is better to consider it as a “smaller” set."
The intersection between the input knowledge base and the semantics of the target text is what we are interested in captur-ing by means of our statistical techniques.
"To collect the semantic input, we crawled 1,100 HTML pages containing celebrity fact-sheets from the E!"
Online website. [URL_CITE]
"The pages comprised infor-mation in 14 categories for actors, directors, produc-ers, screenwriters, etc."
We then proceeded to trans-form the information in the pages to a frame-based knowledge representation.
"The final corpus con-tains 50K frames, with 106K frame-attribute-value triples, for the 1,100 people mentioned in each fact-sheet."
An example set of frames is shown in Fig-ure 3.
"The text part was mined from two different web-sites, biography.com, containing typical biogra-phies, with an average of 450 words each; and imdb.com, the Internet movie database, 250-word average length biographies."
"In each case, we ob-tained the semantic input from one website and a separate biography from a second website."
We linked the two resources using techniques from record linkage in census statistical analysis[REF_CITE].
"We based our record linkage on the Last Name, First Name, and Year of Birth at-tributes."
Figure 2 illustrates our two-step approach.
"In the first step (shaded region of the figure), we try to identify and solve the easy cases for Content Selec-tion."
The easy cases in our task are pieces of data that are copied verbatim from the input to the out-put.
"In biography generation, this includes names, dates of birth and the like."
The details of this pro-cess are discussed in Section 3.1.
"After these cases have been addressed, the remaining semantic data is clustered and the text corresponding to each cluster post-processed to measure degrees of influence for different semantic units, presented in Section 3.2."
Further techniques to improve the precision of the algorithm are discussed in Section 3.3.
Central to our approach is the notion of data paths in the semantic network (an example is shown in Figure 3).
"Given a frame-based representation of knowledge, we need to identify particular pieces of knowledge inside the graph."
"We do so by selecting a particular frame as the root of the graph (the person whose biography we are generating, in our case, doubly circled in the figure) and considering the paths in the graph as identifiers for the different pieces of data."
We call these data paths.
"Each path will identify a class of values, given the fact that some attributes are list-valued (e.g., the relative attribute in the figure)."
We use the notation attribute attribute  attribute  to denote data paths.
"In the first stage (cf. Fig. 2(1)), the objective is to identify pieces from the input that are copied ver-batim to the output."
"These types of verbatim-copied anchors are easy to identify and they allow us do two things before further analyzing the input data: re-move this data from the input as it has already been selected for inclusion in the text and mark this piece of text as a part of the input, not as actual text."
"The rest of the semantic input is either verbal-ized (e.g., by means of a verbalization rule of the form brother age  “young”) or not included at all."
This situation is much more chal-lenging and requires the use of our proposed statis-tical selection technique.
"For each class in the semantic input that was not ruled out in the previous step (e.g., brother age ), we proceed to cluster (cf. Fig. 2(2)) the possible values in the path, over all people (e.g.,  &quot;$! %# &amp;( &quot;) $! %# &amp; (* &quot;+ ) ! for age)."
Clustering details can be found[REF_CITE].
"In the case of free-text fields, the top level, most informative terms, [Footnote_3] are picked and used for the clus-tering."
3 We use the maximum value of the TF*IDF weights for each term in the whole text collection. That has the immediate effect of disregarding stop words.
"For example, for “played an insecure young resident” it would be played , insecure , resident ! ."
"Having done so, the texts associated with each cluster are used to derive language models (in our case we used bi-grams, so we count the bi-grams appearing in all the biographies for a given cluster —e.g., all the people with age between 25 and 50 years old, &amp; * &quot;) ! )."
We then measure the variations on the language models produced by the variation (clustering) on the data.
What we want is to find a change in word choice correlated with a change in data.
"If there is no correlation, then the piece of data which changed should not be selected by Content Selection."
"In order to compare language models, we turned to techniques from adaptive NLP (i.e., on the ba-sis of genre and type distinctions)[REF_CITE]."
"In particular, we employed the cross entropy [Footnote_4] between two language models and , defined as follows (where  is the probability that assigns to the -gram ): ,   &quot;#! %$&amp; (1)"
"4 Other metrics would have been possible, in as much as they measure the similarity between the two models."
"Smaller values of , &apos; indicate that is more similar to ."
"On the other hand, if we take to be a model of randomly selected documents and a model of a subset of texts that are associ-ated with the cluster, then a greater-than-chance value would be an indicator that the cluster in the se-mantic side is being correlated with changes in the text side."
"We then need to perform a sampling process, in which we want to obtain values that would rep-resent the null hypothesis in the domain."
We sample two arbitrary subsets of ( elements each from the total set of documents and compute the of their derived language models (these values consti-tute our control set).
"We then compare, again, a ran-dom sample of size ( from the cluster against a ran-dom sample of size ( from the difference between the whole collection and the cluster (these val-ues constitute our experiment set)."
"To see whether the values in the experiment set are larger (in a stochastic fashion) than the values in the control set, we employed the Mann-Whitney U test (Siegel and Castellan Jr., 1988) (cf. Fig. 2(3))."
Note the list-valued attribute relative. ) ) ( significance level.
"Finally, if the cross-entropy values for the experiment set are larger than for the control set, we can infer that the values for that se-mantic cluster do influence the text."
"Thus, a positive U test for any data path was considered as an indica-tor that the data path should be selected."
"Using simple thresholds and the U test, class-based content selection rules can be obtained."
"These rules will select or unselect each and every in-stance of a given data path at the same time (e.g., if - relative person name first . is selected, then both “Dashiel” and “Jason” will be selected in Fig-ure 3)."
By counting the number of times a data path in the exact matching appears in the texts (above some fixed threshold) we can obtain baseline con-tent selection rules (cf. Fig. 2(A)).
Adding our statis-tically selected (by means of the cross-entropy sam-pling and the U test) data paths to that set we obtain class-based content selection rules (cf.
"By means of its simple algorithm, we expect these rules to overtly over-generate, but to achieve excel-lent coverage."
These class-based rules are relevant to the KR concept of Viewpoints[REF_CITE]; [Footnote_5] we extract a slice of the knowledge base that is relevant to the domain task at hand.
"5 they define them as a coherent sub-graph of the knowl-edge base describing the structure and function of objects, the change made to objects by processes, and the temporal at-tributes and temporal decompositions of processes."
"However, the expressivity of the class-based ap-proach is plainly not enough to capture the idiosyn-crasies of content selection: for example, it may be the case that children’s names may be worth men-tioning, while grand-children’s names are not."
"That is, in Figure 3, - relative person name first . is dependent on - relative TYPE . and therefore, all the information in the current instance should be taken into account to decide whether a particular data path and it values should be included or not."
Our approach so far simply determines that an at-tribute should always be included in a biography text.
"These examples illustrate that content selection rules should capture cases where an attribute should be included only under certain conditions; that is, only when other semantic attributes take on specific values."
"We turned to ripper [Footnote_6][REF_CITE], a supervised rule learner categorization tool, to elucidate these types of relationships."
"6 We chose ripper to use its set-valued attributes, a desir-able feature for our problem setting."
"We use as features a flattened version of the input frames, [Footnote_7] plus the actual value of the data in question."
"7 The flattening process generated a large number of fea-tures, e.g., if a person had a grandmother, then there will be a “grandmother” column for every person. This gets more com-plicated when list-valued values are taken into play. In our bi-ographies case, an average-sized 100-triples biography spanned over 2,300 entries in the feature vector."
"To obtain the right label for the training instance we do the following: for the exact-matched data paths, matched pieces of data will correspond to positive training classes, while unmatched pieces, negative ones."
"That is to say, if we know that brother age , and that appears in the text, we can conclude that the data of this particular person can be used as a positive train-ing instance for the case age , ."
"Similarly, if there is no match, the opposite is inferred."
"For the U-test selected paths, the situation is more complex, as we only have clues about the impor-tance of the data path as a whole."
"That is, while we know that a particular data path is relevant to our task (biography construction), we don’t know with which values that particular data path is being ver-balized."
We need to obtain more information from the sampling process to be able to identify cases in which we believe that the relevant data path has been verbalized.
"To obtain finer grained information, we turned to a -gram distillation process (cf."
"Fig. 2(4)), where the most significant -grams (bi-grams in our case) were picked during the sampling process, by looking at their overall contribution to the CE term in Equation 1."
"For example, our sys-tem found the bi-grams screenwriter director and has screenwriter [Footnote_8] as relevant for the cluster occupation TYPE , c-writer , while the cluster occupation TYPE , c-comedian , c-actor &amp; will not include those, but will include sitcom Time and Comedy Musical ."
"8 Our bi-grams are computed after stop-words and punctu-ation is removed, therefore these examples can appear in texts like “he is an screenwriter,director,...” or “she has an screen-writer award..."
"These -grams thus indicate that the data path occupation TYPE , is included in the text; a change in value does affect the output."
We later use the matching of these -grams as an indicator of that particular instance as being selected in that document.
"Finally, the training data for each data path is gen-erated. (cf. Fig. 2(5))."
"The selected or unselected label will thus be chosen either via direct extraction from the exact match or by means of identification of distiled, relevant -grams."
"After ripper is run, the obtained rules are our sought content selection rules (cf. Fig. 2(5))."
We used the following experimental setting: 102 frames were separated from the full set together with their associated 102 biographies from the biogra-phy.com site.
This constituted our development corpus.
We further split that corpus into develop-ment training (91 people) and development test and hand-tagged the 11 document-data pairs.
"The annotation was done by one of the authors, by reading the biographies and checking which triples (in the RDF sense, frame, slot, value ) were actu-ally mentioned in the text (going back and forth to the biography as needed)."
Two cases required spe-cial attention.
"The first one was aggregated infor-mation, e.g., the text may say “he received three"
"Grammys” while in the semantic input each award was itemized, together with the year it was received, the reason and the type (Best Song of the Year, etc.)."
"In that case, only the name of award was selected, for each of the three awards."
The second case was factual errors.
"For example, the biography may say the person was born in MA and raised in WA, but the fact-sheet may say he was born in WA."
"In those cases, the intention of the human writer was given priority and the place of birth was marked as se-lected, even though one of the two sources were wrong."
"The annotated data total 1,129 triples."
"From them, only 293 triples (or a 26%) were verbalized in the associated text and thus, considered selected."
"That implies that the “select all” tactic (“select all” is the only trivial content selection tactic, “select none” is of no practical value) will achieve an F-measure of 0.41 (26% prec. at 100% rec.)."
"Following the methods outlined in Section 3, we utilized the training part of the development corpus to mine Content Selection rules."
We then used the development test to run different trials and fit the different parameters for the algorithm.
"Namely, we determined that filtering the bottom 1,000 TF*IDF weighted words from the text before building the -gram model was important for the task (we com-pared against other filtering schemes and the use of lists of stop-words)."
The best parameters found and the fitting methodology are described[REF_CITE].
We then evaluated on the rest of the semantic input (998 people) aligned with one other textual corpus (imdb.com).
"As the average length-per-biography are different in each of the corpora we worked with (450 and 250, respectively), the content selection rules to be learned in each case were dif-ferent (and thus, ensure us an interesting evaluation of the learning capabilities)."
"In each case, we split the data into training and test sets, and hand-tagged the test set, following the same guidelines explained for the development corpus."
The linkage step also required some work to be done.
We were able to link 205 people in imdb.com and separated 14 of them as the test set.
The results are shown in Table 1 [Footnote_9] .
"9 We disturbed the dataset to obtain some cross-validation over these figures, obtaining a std dev. of 0.02 for the F*, the full details are given[REF_CITE]."
Several things can be noted in the table.
The first is that imdb.com represents a harder set than our de-velopment set.
"That is to expect, as biogra-phy.com’s biographies have a stable editorial line, while imdb.com biographies are submitted by In-ternet users."
"However, our methods offer compara-ble results on both sets."
"Nonetheless, the tables por-tray a clear result: the class-based rules are the ones that produce the best overall results."
They have the highest F-measure of all approaches and they have high recall.
"In general, we want an approach that favors recall over precision in order to avoid losing any information that is necessary to include in the output."
Overgeneration (low precision) can be cor-rected by later processes that further filter the data.
Further processing over the output will need other types of information to finish the Content Selection process.
"The class-based rules filter-out about 50% of the available data, while maintaining the relevant data in the output set."
An example rule from the ripper approach can be seen in Figure 4.
"The rules themselves look inter-esting, but while they improve in precision, as was our goal, their lack of recall makes their current im-plementation unsuitable for use."
We have identified a number of changes that we could make to improve this process and discuss them at the end of the next section.
"Given the experimental nature of these re-sults, we would not yet draw any conclusions about the ultimate benefit of the ripper approach."
Very few researchers have addressed the problem of knowledge acquisition for content selection in gen-eration.
"A notable exception is[REF_CITE]’s work, which discusses a rainbow of knowledge en-gineering techniques (including direct acquisition from experts, discussion groups, etc.)."
"They also mention analysis of target text, but they abandon it because it was impossible to know the actual crite-ria used to chose a piece of data."
"In contrast, in this paper, we show how the pairing of semantic input with target text in large quantities allows us to elicit statistical rules with such criteria."
"Aside from that particular work, there seems to exist some momentum in the literature for a two-level Content Selection process (e.g.,[REF_CITE],[REF_CITE], and[REF_CITE])."
"For instance, distinguish two levels of content determination, “local” content determination is the “selection of relatively small knowledge structures, each of which will be used to generate one or two sentences” while “global” con-tent determination is “the process of deciding which of these structures to include in an explanation”."
"Our technique, then, can be thought of as picking the global Content Selection items."
"One of the most felicitous Content Selection al-gorithms proposed in the literature is the one used in the ILEX project[REF_CITE], where the most prominent pieces of data are first chosen (by means of hardwired “importance” values on the input) and intermediate, coherence-related new ones are later added during planning."
"For example, a painting and the city where the painter was born may be worth mentioning."
"However, the painter should also be brought into the discussion for the sake of coher-ence."
"Finally, while most classical approaches, exem-plified[REF_CITE]tend to perform the Content Selection task integrated with the document planning, recently, the interest in automatic, bottom-up content planners has put forth a simplified view where the information is en-tirely selected before the document structuring pro-cess begins[REF_CITE]."
"While this approach is less flexible, it has important ramifications for machine learning, as the resulting algorithm can be made simpler and more amenable to learning."
"We have presented a novel method for learning Con-tent Selection rules, a task that is difficult to per-form manually and must be repeated for each new domain."
The experiments presented here use a re-source of text and associated knowledge that we have produced from the Web.
The size of the cor-pus and the methodology we have followed in its construction make it a major resource for learning in generation.
"Our methodology shows that data currently available on the Internet, for various do-mains, is readily useable for this purpose."
"Using our corpora, we have performed experimentation with three methods (exact matching, statistical selection and rule induction) to infer rules from indirect ob-servations from the data."
"Given the importance of content selection for the acceptance of generated text by the final user, it is clear that leaving out required information is an error that should be avoided."
"Thus, in evaluation, high recall is preferable to high precision."
"In that respect, our class-based statistically selected rules perform well."
This significant reduction in data makes the task of developing further rules for content selection a more feasible task.
"It will aid the practitioner of NLG in the Content Selection task by reducing the set of data that will need to be examined manually (e.g., discussed with domains experts)."
We find the results for ripper disappointing and think more experimentation is needed before dis-counting this approach.
It seems to us ripper may be overwhelmed by too many features.
"Or, this may be the best possible result without incorporating do- main knowledge explicitly."
We would like to investi-gate the impact of additional sources of knowledge.
These alternatives are discussed below.
"In order to improve the rule induction results, we may use spreading activation starting from the par-ticular frame to be considered for content selection and include the semantic information in the local context of the frame."
"For example, to content select birth date year , only values from frames birth date and birth would be consid-ered (e.g., relative  will be completely dis-regarded)."
Another improvement may come from more intertwining between the exact match and sta-tistical selector techniques.
"Even if some data path appears to be copied verbatim most of the time, we can run our statistical selector for it and use held out data to decide which performs better."
"Finally, we are interested in adding a domain paraphrasing dictionary to enrich the exact match-ing step."
"This could be obtained by running the se-mantic input through the lexical chooser of our biog-raphy generator, P RO G EN IE, currently under con-struction."
Opinion question answering is a challenging task for natural language processing.
"In this paper, we discussa necessarycomponentfor an opinion ques-tion answering system: separating opinions from fact, at both the document and sentence level."
"We present a Bayesian classifier for discriminating be-tween documentswith a preponderanceof opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opin-ions at the sentence level."
We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective be-ing expressed in the opinion.
"Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classi-fication (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy)."
"Newswire articles include those that mainly present opinions or ideas, such as editorials and letters to the editor, and those that mainly report facts such as daily news articles."
Text materials from many other sources also contain mixed facts and opinions.
"For many natural language processing applications, the ability to detect and classify factual and opinion sen-tences offers distinctadvantages in deciding what in-formation to extract and how to organize and present this information."
"For example, information extrac-tion applications may target factual statements rather than subjective opinions, and summarization sys-tems may list separately factual information and ag-gregate opinions according to distinct perspectives."
"At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g., filtering or re- trieving only editorials in favor of a particular policy decision)."
Our motivation for building the opinion detec-tion and classification system described in this pa-per is the need for organizing information in the context of question answering for complex ques-tions.
"Unlike questions like “Who was the first man on the moon?” which can be answered with a simple phrase, more intricate questions such as “What are the reasons for the US-Iraq war?” require long answers that must be constructed from multi-ple sources."
"In such a context, it is imperative that the question answering system can discriminate be-tween opinions and facts, and either use the appro-priate type depending on the question or combine them in a meaningful presentation."
"Perspective in-formation can also help highlight contrasts and con-tradictions between different sources—there will be significant disparity in the material collected for the question mentioned above between Fox News and the Independent, for example."
Fully analyzing and classifying opinions involves tasks that relate to some fairly deep semantic and syntactic analysis of the text.
"These include not only recognizing that the text is subjective, but also de-termining who the holder of the opinion is, what the opinion is about, and which of many possible posi-tions the holder of the opinion expresses regarding that subject."
"In this paper, we are presenting three of the components of our opinion detection and or-ganization subsystem, which have already been in-tegrated into our larger question-answering system."
"These components deal with the initial tasks of clas-sifying articles as mostly subjective or objective, finding opinion sentences in both kinds of articles, and determining, in general terms and without refer-ence to a specific subject, if the opinions are positive or negative."
The three modules of the system dis-cussed here provide the basis for ongoing work for further classification of opinions according to sub-ject and opinion holder and for refining the original positive/negative attitude determination.
"We review related work in Section 2, and then present our document-level classifier for opinion or factual articles (Section 3), three implemented tech-niques for detecting opinions at the sentence level (Section 4), and our approach for rating an opinion as positive or negative (Section 5)."
We have evalu-ated these methods using a large collection of news articles without additional annotation (Section 6) and an evaluation corpus of 400 sentences anno-tated for opinion classifications (Section 7).
"The results, presented in Section 8, indicate that we achieve very high performance (more than 97%) at document-level classification and respectable per-formance (86–91%) at detecting opinion sentences and classifying them according to orientation."
"Much of the earlier research in automated opinion detection has been performed by Wiebe and col-leagues[REF_CITE], who proposed methods for dis-criminating between subjective and objective text at the document, sentence, and phrase levels."
"Subsequently,[REF_CITE]showed that automatically detected gradable adjectives are a useful feature for subjec-tivity classification, while[REF_CITE]introduced lexical features in addition to the presence/absence of syntactic categories."
"More recently,[REF_CITE]report on document-level subjectivity classi-fication, using a k-nearest neighbor algorithm based on the total count of subjective words and phrases within each document."
Psychological studies[REF_CITE]found measurable associations between words and human emotions.
"He then used these phrases to automati-cally separate positive and negative movie and prod-uct reviews, with accuracy of 66–84%."
Our approach to document and sentence classi-fication of opinions builds upon the earlier work by using extended lexical models with additional features.
"Unlike the work cited above, we do not rely on human annotations for training but only on weak metadata provided at the document level."
"Our sentence-level classifiers introduce additional crite-ria for detecting subjective material (opinions), in-cluding methods based on sentence similarity within a topic and an approach that relies on multiple clas-sifiers."
"At the document level, our classifier uses the same document labels that the method[REF_CITE]does, but automatically detects the words and phrases of importance without further analy-sis of the text."
"For determining whether an opin-ion sentence is positive or negative, we have used seed words similar to those produced[REF_CITE]and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed[REF_CITE]."
"Our focus is on the sentence level, un[REF_CITE]and[REF_CITE]; we employ a significantly larger set of seed words, and we explore as indicators of orientation words from syntactic classes other than adjectives (nouns, verbs, and adverbs)."
"To separate documents that contain primarily opin-ions from documents that report mainly facts, we ap- plied Naive Bayes 1 , a commonly used supervised machine-learning algorithm."
"This approach pre-supposes the availabilityof at least a collection of ar-ticles with pre-assigned opinion and fact labels at the document level; fortunately, Wall Street Journal ar-ticles contain such metadata by identifying the type of each article as Editorial, Letter to editor, Business and News."
"These labels are used only to provide the correct classification labels during training and eval-uation, and are not included in the feature space."
"We used as features single words, without stemming or stopword removal."
Naive Bayes assigns a document to the class that maximizes   by applying Bayes’ rule  and assuming con-ditional independence of the features.
"Although Naive Bayes can be outperformed in text classification tasks by more complex methods such as SVMs,[REF_CITE]report similar per-formance for Naive Bayes and other machine learn-ing techniques for a similar task, that of distinguish-ing between positive and negative reviews at the document level."
"Further, we achieved such high per-formance with Naive Bayes (see Section 8) that ex-ploring additional techniques for this task seemed unnecessary."
We developed three different approaches to clas-sify opinions from facts at the sentence level.
"To avoid the need for obtaining individual sentence an-notations for training and evaluation, we rely in-stead on the expectation that documents classified as opinion on the whole (e.g., editorials) will tend to have mostly opinion sentences, and conversely doc-uments placed in the factual category will tend to have mostly factual sentences."
"Our first approach to classifying sentences as opin-ions or facts explores the hypothesis that, within a given topic, opinion sentences will be more simi-lar to other opinion sentences than to factual sen- tences."
"We used S IM F INDER[REF_CITE], a state-of-the-art system for measuring sentence similarity based on shared words, phrases, and WordNet synsets."
"To measure the overall simi-larity of a sentence to the opinion or fact documents, we first select the documents that are on the same topic as the sentence in question."
"We obtain topics as the results of IR queries (for example, by search-ing our document collection for “welfare reform”)."
We then average its S IM F INDER -provided similari-ties with each sentence in those documents.
Then we assign the sentence to the category for which the average is higher (we call this approach the “score” variant).
"Alternatively, for the “frequency” variant, we do not use the similarity scores themselves but instead we count how many of them, for each cate-gory, exceed a predetermined threshold (empirically set to 0.65)."
"Our second method trains a Naive Bayes classifier (see Section 3), using the sentences in opinion and fact documents as the examples of the two cate-gories."
"The features include words, bigrams, and trigrams, as well as the parts of speech in each sen-tence."
"In addition, the presence of semantically ori-ented (positive and negative) words in a sentence is an indicator that the sentence is subjective[REF_CITE]."
"Therefore, we in-clude in our features the counts of positive and neg-ative words in the sentence (which are obtained with the method of Section 5.[Footnote_1]), as well as counts of the polarities of sequences of semantically oriented words (e.g., “++” for two consecutive positively ori-ented words)."
"1 Using the Rainbow implementation, available[URL_CITE]"
"We also include the counts of parts of speech combined with polarity information (e.g., “JJ+” for positive adjectives), as well as features en-coding the polarity (if any) of the head verb, the main subject, and their immediate modifiers."
Syn-tactic structure was obtained with Charniak’s statis-tical parser[REF_CITE].
"Finally, we used as one of the features the average semantic orientation score of the words in the sentence."
Our designationof all sentences in opinion or factual articles as opinion or fact sentences is an approxima-tion.
"To address this, we apply an algorithm using multiple classifiers, each relying on a different sub-set of our features."
"The goal is to reduce the training set to the sentences that are most likely to be cor-rectly labeled, thus boosting classification accuracy."
"Given separate sets of features   , we train separate Naive Bayes classifiers  ,  corresponding to each feature set."
"Assum-ing as ground truth the information provided by the document labels and that all sentences inherit the status of their document as opinions or facts, we first train on the entire training set, then use the resulting classifier to predict labels for the training set."
"The sentences that receive a label different from the assumed truth are then removed, and we train on the remaining sentences."
This process is re-peated iteratively until no more sentences can be re-moved.
"We report results using five feature sets, starting from words alone and adding in bigrams, tri-grams, part-of-speech, and polarity."
"Having distinguished whether a sentence is a fact or opinion, we separate positive, negative, and neutral opinions into three classes."
We base this decision on the number and strength of semantically oriented words (either positive or negative) in the sentence.
"We first discuss how such words are automatically found by our system, and then describe the method by which we aggregate this information across the sentence."
"To determine which words are semantically ori-ented, in what direction, and the strength of their orientation, we measured their co-occurrence with words from a known seed set of semantically ori-ented words."
"The approach is based on the hypothe-sis that positive words co-occur more than expected by chance, and so do negative words; this hypothe-sis was validated, at least for strong positive/negative words,[REF_CITE]."
"As seed words, we used subsets of the 1,336 adjectives that were manually classified as positive (657) or negative (679)[REF_CITE]."
In earlier work[REF_CITE]only singletons were used as seed words; varying their number allows us to test whether multiple seed words have a positive effect in detection performance.
"We experimented with seed sets containing 1, 20, 100 and over 600 positive and negative pairs of adjectives."
"For a given seed set size, we denote the set of positive seeds as ADJ and the set of negative seeds as ADJ ."
"We then cal-culate a modified log-likelihood ratio  POS for a word with part of speech POS ( can be adjective, adverb, noun or verb) as the ratio of its collocation frequency with ADJ and ADJ within a sentence, where Freq all POS ADJ represents the collo-cation frequency of all words all of part of speech ( 9 POS ;: with =&lt; in ourADJcaseand)."
We 9 isuseda smoothingBrill’s taggerconstant[REF_CITE]to obtain part-of-speech information.
As our measure of semantic orientation across an entire sentence we used the average per word log-likelihood scores defined in the preceding section.
"To determine the orientation of an opinion sentence, all that remains is to specify cutoffs &gt;/ and &gt;? so that sentences for which the average log-likelihood score exceeds &gt; are classified as positive opinions, sentences with scores lower than &gt;? are classified as negative opinions, and sentences with in-between scores are treated as neutral opinions."
"Optimal val-ues for &gt; and &gt; are obtained from the training data via density estimation—using a small, hand-labeled subset of sentences we estimate the proportion of sentences that are positive or negative."
The values of the average log-likelihood score that correspond to the appropriate tails of the score distribution are then determined via Monte Carlo analysis of a much larger sample of unlabeled training data.
"We used the TREC [URL_CITE] 8, 9, and 11 collections, which consist of more than 1.7 million newswire arti-cles."
"The aggregate collection covers six differ-ent newswire sources including 173,252 Wall Street"
Journal (WSJ) articles from 1987 to 1992.
"Some of the WSJ articles have structured headings that include Editorial, Letter to editor, Business, and News (2,877, 1,695, 2,009 and 3,714 articles, re-spectively)."
"We randomly selected 2,000 articles [Footnote_3] from each category so that our data set was approx-imate evenly divided between fact and opinion ar-ticles."
"3 Except for Letters to Editor, for which we included all 1,695 articles available."
Those articles were used for both document and sentence level opinion/fact classification.
"For classification tasks (i.e., classifying between facts and opinions and identifying the semantic ori-entation of sentences), we measured our system’s performance by standard recall and precision."
We evaluated the quality of semantically oriented words by mapping the extracted words and labels to an ex-ternal gold standard.
"We took the subset of our out-put containing words that appear in the standard, and measured the accuracy of our output as the portion of that subset that was assigned the correct label."
"A gold standard for document-level classification is readily available, since each article in our Wall Street Journal collection comes with an article type label (see Section 6)."
"We mapped article types News and Business to facts, and article types Editorial and Letter to the Editor to opinions."
"We cannot auto-matically select a sentence-level gold standard dis-criminating between facts and opinions, or between positive and negative opinions."
We therefore asked human evaluators to classify a set of sentences be-tween facts and opinions as well as determine the type of opinions.
"Since we have implemented our methods in an opinion questionanswering system, we selected four different topics (gun control, illegal aliens, social security, and welfare reform)."
"For each topic, we randomly selected 25 articles from the entire com-bined TREC corpus (not just the WSJ portion); these were articles matching the corresponding topical phrase given above as determined by the Lucene search engine. [URL_CITE] From each of these documents we randomly selected four sentences."
"If a document happened to have less than four sentences, additional documents from the same topic were retrieved  to supply  &lt; the missing :$: sentencessentenceswere. thenThe interleavedresulting so that successive sentences came from different top-ics and documents and divided into ten 50-sentence blocks."
"Each block shares ten sentences with the preceding and following block (the last block is con-sidered to precede the first one), so that 100 of the 400 sentences appear in two blocks."
"Each of ten hu-man evaluators (all with graduate training in com-putational linguistics) was presented with one block and asked to select a label for each sentence among the following: “fact”, “positive opinion”, “negative opinion”, “neutral opinion”, “sentence contains both positive and negative opinions”, “opinion but cannot determine orientation”, and “uncertain”. [Footnote_5]"
5 The full instructions can be viewed online[URL_CITE]
"Since we have one judgment for 300 sentences and two judgments for 100 sentences, we created two gold standards for sentence classification."
The first (Standard A) includes the 300 sentences with one judgment and a single judgment for the remain-ing 100 sentences. [Footnote_6] The second standard (Standard B) contains the subset of the 100 sentences for which we obtained identical labels.
"6 In order to assign a unique label, we arbitrarily chose the first evaluator for those sentences."
Statistics of these two standards are given in Table 1.
"We measured the pairwise agreement among the 100 sentences that were judged by two evaluators, as the ratio of sen-tences that receive a label from both evaluators divided by the total number of sentences receiving label from any evaluator."
"The agreement across the 100 sentences for all seven choices was 55%; if we group together the five subtypes of opinion sentences, the overall agreement rises to 82%."
The low agreement for some labels was not surprising because there is much ambiguity between facts and opinions.
"An example of an arguable sentence is “A lethal guerrilla war between poachers and wardens now rages in central and eastern Africa”, which one rater classified as “fact” and another rater classified as “opinion”."
"Finally, for evaluating the quality of extracted words with semantic orientation labels, we used two distinct manually labeled collections as gold stan-dards."
One set consists of the previously described 657 positive and 679 negative adjectives[REF_CITE].
"We also used the ANEW list which was constructed during psy-cholinguistic experiments[REF_CITE]and contains 1,031 words of all four open classes."
"As described[REF_CITE], humans assigned valence scores to each score according to dimensions such as pleasure, arousal, and domi-nance; following heuristics proposed in psycholin-guistics [URL_CITE] we obtained 284 positive and 272 negative words from the valence scores."
"Document Classification We trained our Bayes classifier for documents on 4,000 articles from the WSJ portion of our combined TREC collection, and evaluated on 4,000 other articles also from the WSJ part."
Table 2 lists the F-measure scores (the har-monic mean of precision and recall) of our Bayesian classifier for document-level opinion/fact classifica-tion.
"The results show the classifier achieved 97% F-measure, which is comparable or higher than the 93% accuracy reported[REF_CITE], who evaluated their work based on a similar set of WSJ articles."
The high classification performance is also consistent with a high inter-rater agreement (kappa=0.95) for document-level fact/opinion anno-tati[REF_CITE].
"Note that we trained and evaluated only on WSJ articles for which we can ob-tain article class metadata, so the classifier may per-form less accurately when used for other newswire articles."
"Sentence Classification Table 3 shows the re-call and precision of the similarity-based approach, while Table 4 lists the recall and precision of naive Bayes (single and multiple classifiers) for sentence-level opinion/fact classification."
"In both cases, the results are better when we evaluate against Stan-dard B, containing the sentences for which two hu-mans assign the same label; obviously, it is easier for the automatic system to produce the correct label in these more clear-cut cases."
Our Naive Bayes classifier has a higher recall and precision (80–90%) for detecting opinions than for facts (around 50%).
"While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach[REF_CITE]."
"In general, the additional features helped the classi-fier; the best performance is achieved when words, bigrams, trigrams, part-of-speech, and polarity are included in the feature set."
"Further, using multiple classifiers to automatically identify an appropriate subset of the data for training slightly increases per-formance."
"Polarity Classification Using the method of Sec-tion 5.1, we automatically identified a total of 39,652 (65,773), 3,128 (4,426), 144,238 (195,984), and 22,279 (30,609) positive (negative) adjectives, ad-verbs, nouns, and verbs, respectively."
"Extracted pos-itive words include inspirational, truly, luck, and achieve."
"Negative ones include depraved, disas-trously, problem, and depress."
"Figure 1 plots the recall and precision of extracted adjectives by us-ing randomly selected seed sets of 1, 20, and 100 pairs of positive and negative adjectives from the list[REF_CITE]."
Both re-call and precision increase as the seed set becomes larger.
We obtained similar results with the ANEW list of adjectives (Section 7).
"As an additional ex-periment, we tested the effect of ignoring sentences with negative particles, obtaining a small increase in precision and recall."
We subsequently used the automatically extracted polarity score for each word to assign an aggregate polarity to opinion sentences.
Table 5 lists the accu-racy of our sentence-level tagging process.
"We ex-perimented with different combinations of part-of-speech classes for calculating the aggregate polarity scores, and found that the combined evidence from adjectives, adverbs, and verbs achieves the highest accuracy (90% over a baseline of 48%)."
"As in the case of sentence-level classification between opin-ion and fact, we also found the performance to be higher on Standard B, for which humans exhibited consistent agreement."
"We presented several models for distinguishing be-tween opinions and facts, and between positive and negative opinions."
"At the document level, a fairly straightforward Bayesian classifier using lexical in-formation can distinguish between mostly factual and mostly opinion documents with very high pre-cision and recall (F-measure of 97%)."
The task is much harder at the sentence level.
"For that case, we described three novel techniques for opinion/fact classification achieving up to 91% precision and re-call on the detection of opinion sentences."
"We also examined an automatic method for assigning polar-ity information to single words and sentences, accu-rately discriminating between positive, negative, and neutral opinions in 90% of the cases."
"Our work so far has focused on characterizing opinions and facts in a generic manner, without ex-amining who the opinion holder is or what the opin-ion is about."
"While we have found presenting in-formation organized in separate opinion and fact classes useful, our goal is to introduce further analy-sis of each sentence so that opinion sentences can be linked to particular perspectives on a specific sub-ject."
We intend to cluster together sentences from the same perspective and present them in summary form as answers to subjective questions.
A maximum entropy (ME) model is usu-ally estimated so that it conforms to equal-ity constraints on feature expectations.
"However, the equality constraint is inap-propriate for sparse and therefore unre-liable features."
"This study explores an ME model with box-type inequality con-straints, where the equality can be vio-lated to reflect this unreliability."
We eval-uate the inequality ME model using text categorization datasets.
"We also propose an extension of the inequality ME model, which results in a natural integration with the Gaussian MAP estimation."
Experi-mental results demonstrate the advantage of the inequality models and the proposed extension.
"The maximum entropy model[REF_CITE]has attained great popularity in the NLP field due to its power, robustness, and suc-cessful performance in various NLP tasks[REF_CITE]."
"In the ME estimation, an event is decomposed into features, which indicate the strength of certain aspects in the event, and the most uniform model among the models that satisfy:"
E p̃ [f i ] =
"E p [f i ], (1) for each feature."
"E p̃ [f i ] represents the expectation of feature f i in the training data (empirical expec-tation), and E p [f i ] is the expectation with respect to the model being estimated."
"A powerful and ro-bust estimation is possible since the features can be as specific or general as required and does not need to be independent of each other, and since the most uniform model avoids overfitting the training data."
"In spite of these advantages, the ME model still suffers from a lack of data as long as it imposes the equality constraint (1), since the empirical expecta-tion calculated from the training data of limited size is inevitably unreliable."
A careful treatment is re-quired especially in NLP applications since the fea-tures are usually very sparse.
"In this study, text cat-egorization is used as an example of such tasks with sparse features."
"Previous work on NLP proposed several solutions for this unreliability such as the cut-off, which sim-ply omits rare features, the MAP estimation with the Gaussian prior[REF_CITE], the fuzzy maximum entropy model[REF_CITE], and fat constraints[REF_CITE]."
"Currently, the Gaussian MAP estimation (com-bined with the cut-off) seems to be the most promis-ing method from the empirical results."
It succeeded in language modeling[REF_CITE]and text categorizati[REF_CITE].
"As described later, it relaxes constraints like E p̃ [f i ] − E p [f i ] = λσ 2i , where λ i is the model’s parameter."
"This study follows this line, but explores the fol-lowing box-type inequality constraints:"
"A i ≥ E p̃ [f i ] − E p [f i ] ≥ −B i ,"
"A i , B i &gt; 0. (2)"
"Here, the equality can be violated by the widths A i and B i ."
We refer to the ME model with the above inequality constraints as the inequality ME model.
"This inequality constraint falls into a type of fat con-straints, a i ≤ E p [f i ] ≤ b i , as suggested[REF_CITE]."
"However, as noted[REF_CITE], this type of constraint has not yet been applied nor evaluated for NLPs."
"The inequality ME model differs from the Gaus-sian MAP estimation in that its solution becomes sparse (i.e., many parameters become zero) as a re-sult of optimization with inequality constraints."
The features with a zero parameter can be removed from the model without changing its prediction behavior.
"Therefore, we can consider that the inequality ME model embeds feature selection in its estimation."
"Recently, the sparseness of the solution has been rec-ognized as an important concept in constructing ro-bust classifiers such as SVMs[REF_CITE]."
We believe that the sparse solution improves the robust-ness of the ME model as well.
We also extend the inequality ME model so that the constraint widths can move using slack vari-ables.
"If we penalize the slack variables by their 2-norm, we obtain a natural integration of the inequal-ity ME model and the Gaussian MAP estimation."
"While it incorporates the quadratic stabilization of the parameters as in the Gaussian MAP estimation, the sparseness of the solution is preserved."
"We evaluate the inequality ME models empiri-cally, using two text categorization datasets."
The results show that the inequality ME models outper-form the cut-off and the Gaussian MAP estimation.
"Such high accuracies are achieved with a fairly small number of active features, indicating that the sparse solution can effectively enhance the performance."
"In addition, the 2-norm extended model is shown to be more robust in several situations."
"The ME estimation of a conditional model p(y|x) from the training examples {(x i , y i )} is formulated as the following optimization problem. [Footnote_1] maximize H(p) = p̃(x) p(y|x) log p(y|x) p x y subject to E p̃ [f i ] − E p [f i ] = 0 1 ≤ i ≤ F . (3)"
"1 To be precise, we have also the constraints P y p(y|x) −"
The empirical expectations and model expectations in the equality constraints are defined as follows.
"E p̃ [f i ] = x p̃(x) y p̃(y|x)f i (x, y), (4) E p [f i ] = x p̃(x) y p(y|x)f i (x, y), (5) p̃(x) = c(x)/L, p̃(y|x) = c(x, y)/c(x), (6) where c(·) indicates the number of times · occurred in the training data, and L is the number of training examples."
"By the Lagrange method, p(y|x) is found to have the following parametric form: 1 λ i f i (x, y)), p λ (y|x) = Z(x) exp( (7) i where Z(x) = y exp( i λ i f i (x, y))."
The dual objective function becomes:
"L(λ) = x p̃(x) y p̃(y|x) i λ i f i (x, y) (8) − x p̃(x) log y exp( i λ i f i (x, y))."
The ME estimation becomes the maximization of L(λ).
"And it is equivalent to the maximization of the log-likelihood: LL(λ) = log x,y p λ (y|x) p̃(x,y) ."
This optimization can be solved using algo-rithms such as the GIS algorithm[REF_CITE]and the IIS algorithm[REF_CITE].
"In addition, gradient-based algorithms can be applied since the objective function is concave."
We also observed that the LMVM method converges very quickly for the text catego-rization datasets with an improvement in accuracy.
"Therefore, we use the LMVM method (and its vari-ant for the inequality models) throughout the exper-iments."
"Thus, we only show the gradient when men-tioning the training."
The gradient of the objective function (8) is computed as: ∂L(λ) =
E p̃ [f i ] − E p [f i ]. (9) ∂λ i
"The maximum entropy model with the box-type in-equality constraints (2) can be formulated as the fol- lowing optimization problem: maximize p̃(x) p(y|x) log p(y|x), p x y subject to E p̃ [f i ] − E p [f i ] − A i ≤ 0, (10) E p [f i ] − E p̃ [f i ] − B i ≤ 0. (11)"
"By using the Lagrange method for optimization problems with inequality constraints, the following parametric form is derived. 1 p α,β (y|x) = Z(x) exp( (α i − β i ) f i (x, y)), i α i ≥ 0, β i ≥ 0, (12) where parameters α i and β i are the Lagrange mul-tipliers corresponding to constraints (10) and (11)."
"The Karush-Kuhn-Tucker conditions state that, at the optimal point, α i (E p̃ [f i ] − E p [f i ] − A i ) = 0, β i (E p [f i ] − E p̃ [f i ] − B i ) = 0."
"These conditions mean that the equality constraint is maximally violated when the parameter is non-zero, and if the violation is strictly within the widths, the parameter becomes zero."
"We call a feature upper active when α i &gt; 0, and lower active when β i &gt; 0."
"When α i −β i = 0, we call that feature active. 2 Inac-tive features can be removed from the model without changing its behavior."
"Since A i &gt; 0 and B i &gt; 0, any feature should not be upper active and lower active at the same time. [Footnote_3]"
3 This is only achieved with some tolerance in practice.
"The inequality constraints together with the con-y p(y|x)− 1 = 0 define the feasible re-straints gion in the original probability space, on which the entropy varies and can be maximized."
"The larger the widths, the more the feasible region is enlarged."
"Therefore, it can be implied that the possibility of a feature becoming inactive (the global maximal point is strictly within the feasible region with respect to that feature’s constraints) increases if the corre-sponding widths become large."
The solution for the inequality ME model would become sparse if the optimization determines many features as inactive with given widths.
The relation between the widths and the sparseness of the solu-tion is shown in the experiment.
The dual objective function becomes:
"L(α, β) = x p̃(x) y p̃(y|x) i (α i − β i ) f i (x, y) − x p̃(x) log y exp( i (α i − β i ) f i (x, y)) − i α i"
A i − i β i B i . (13)
"Thus, the estimation is formulated as: maximize L(α, β). α i ≥0,β i ≥0"
"Unlike the optimization in the standard maximum entropy estimation, we now have bound constraints on parameters which state that parameters must be non-negative."
"In addition, maximizing L(α, β) is no longer equivalent to maximizing the log-likelihood LL(α, β)."
"Instead, we maximize:"
"LL(α, β) − i α i"
A i − i β i B i . (14)
"Although we can use many optimization algorithms to solve this dual problem since the objective func-tion is still concave, a method that supports bounded parameters must be used."
"In this study, we use the BLMVM algorithm (Benson and Moré, ), a variant of the limited-memory variable metric (LMVM) al-gorithm, which supports bound constraints. [Footnote_4] The gradient of the objective function is: ∂L(α,β) ="
"4 Although we consider only the gradient-based method here as noted earlier, an extension of GIS or IIS to support bounded parameters would also be possible."
"E p̃ [f i ] − E p [f i ] − A i , ∂α i ∂L(α,β) = E p [f i ] − E p̃ [f i ] − B i . (15) ∂β i"
"In this section, we present an extension of the in-equality ME model, which we call soft width."
"The soft width allows the widths to move as A i + δ i and −B i − γ i using slack variables, but with some penalties in the objective function."
"This soft width extension is analogous to the soft margin extension of the SVMs, and in fact, the mathematical discus-sion is similar."
"If we penalize the slack variables by their [Footnote_2]-norm, we obtain a natural combination of the inequality ME model and the Gaussian MAP es-timation."
"2 The term ’active’ may be confusing since in the ME re-search, a feature is called active when f i (x,y) &gt; 0 for an event. However, we follow the terminology in the constrained optimization."
We refer to this extension using 2-norm penalty as the 2-norm inequality ME model.
"As the Gaussian MAP estimation has been shown to be suc-cessful in several tasks, it should be interesting em-pirically, as well as theoretically, to incorporate the Gaussian MAP estimation into the inequality model."
"We first review the Gaussian MAP estimation in the following, and then we describe our extension."
"In the Gaussian MAP ME estimati[REF_CITE], the objective function is:"
"LL(λ) − i ( 2σ1 2i )λ 2i , (16) which is derived as a consequence of maximizing the log-likelihood of the posterior probability, using a Gaussian distribution centered around zero with the variance σ 2i as a prior on parameters."
The gra-dient becomes: ∂L(λ) =
E p̃ [f i ] − E p [f i ] − σλ 2i . (17) ∂λ i i
"At the optimal point, E p̃ [f i ] − E p [f i ] − λσ i2 = 0. i Therefore, the Gaussian MAP estimation can also be considered as relaxing the equality constraints."
"The significant difference between the inequality ME model and the Gaussian MAP estimation is that the parameters are stabilized quadratically in the Gaus-sian[REF_CITE], while they are stabilized linearly in the inequality[REF_CITE]."
"Our 2-norm extension to the inequality ME model is as follows. [Footnote_5] maximize H(p) − C 1 i δ i2 − C 2 i γ 2i , p,δ,γ subject to E p̃ [f i ] − E p [f i ] − A i ≤ δ i , (18) E p [f i ] − E p̃ [f i ] − B i ≤ γ i , (19) where C 1 and C 2 is the penalty constants."
5 It is also possible to impose 1-norm penalties in the objec-tive function. It yields an optimization problem which is iden-tical to the inequality ME model except that the parameters are upper-bounded as 0 ≤ α i ≤ C 1 and 0 ≤ β i ≤ C 2 . We will not investigate this 1-norm extension in this paper and leave it for future research.
The para-metric form is identical to the inequality[REF_CITE].
"However, the dual objective function becomes:"
"LL(α, β) − i α i"
A i + 4Cα 2i1 − i β i B i + 4[REF_CITE].
"Accordingly, the gradient becomes: ∂L(α,β) ="
"E p̃ [f i ] − E p [f i ] − A i + 2Cα i1 , ∂α i ∂L(α,β) = E p [f i ] − E p̃ [f i ] − B i + 2Cβ i2 . (20) ∂β i"
It can be seen that this model is a natural combina-tion of the inequality ME model and the Gaussian MAP estimation.
It is important to note that the so-lution sparseness is preserved in the above model.
"The widths, A i and B i , in the inequality constraints are desirably widened according to the unreliability of the feature (i.e., the unreliability of the calculated empirical expectation)."
"In this paper, we examine two methods to determine the widths."
The first is to use a common width for all features fixed by the following formula.
"A i = B i = W × L1 , (21) where W is a constant, width factor, to control the widths."
This method can only capture the global re-liability of all the features.
"That is, only the reli-ability of the training examples as a whole can be captured."
We call this method single.
"The second, which we call bayes, is a method that determines the widths based on the Bayesian frame-work to differentiate between the features depending on their reliabilities."
"For many NLP applications including text catego-rization, we use the following type of features. f j,i (x, y) = h i (x) if y = y j , 0 otherwise. (22)"
"In this case, if we assume the approximation, p̃(y|x) ≈ p̃(y|h i (x) &gt; 0), the empirical expectation can be interpreted as follows. [Footnote_6]"
"6 This is only for estimating the unreliability, and is not used to calculate the actual empirical expectations in the constraints."
"E p̃ [f j,i ]= p̃(x)p̃(y = y j |h i (x)&gt;0)h i (x). x: h i (x)&gt;0"
"Here, a source of unreliability is p̃(y|h i (x)&gt;0)."
We consider p̃(y|h i (x) &gt; 0) as the parameter θ of the Bernoulli trials.
"That is, p(y|h i (x) &gt; 0) = θ and p(ȳ|h i (x)&gt;0) = 1 − θ."
"Then, we estimate the pos-terior distribution of θ from the training examples by Bayesian estimation and utilize the variance of the distribution."
"With the uniform distribution as the prior, k times out of n trials give the posterior distri-bution: p(θ) = Be(1+k, 1+n−k), where Be(α, β) is the beta distribution."
The variance is calculated as follows.
V [θ] = (1(2++kn)()1 2 +(nn−k+3)) . (23)
"Letting k = c(f j,i (x, y) &gt; 0) and n = c(h i (x) &gt; 0), we obtain fine-grained variances narrowed accord-ing to c(h i (x) &gt; 0) instead of a single value, which just captures the global reliability."
"Assuming the in-dependence of training examples, the variance of the empirical expectation becomes:"
"V E p̃ [f j,i ] =   x: h i (x)&gt;0 {p̃(x)h i (x)} 2 V [θ j,i ]."
"Then, we calculate the widths as follows:"
"A i = B i = W × V E p̃ [f j,i ] . (24)"
"For the evaluation, we use the “[REF_CITE]Dis-tribution 1.0” dataset and the “OHSUMED” dataset."
"The Reuters dataset developed by David D. Lewis is a collection of labeled newswire articles. 7 We adopted “ModApte” split to split the collection, and we obtained [Footnote_7], 048 documents for training, and 2,991 documents for testing."
The OHSUMED dataset[REF_CITE]is a collection of clinical paper abstracts from the MED-LINE database.
Each abstract is manually assigned MeSH terms.
"We simplified a MeSH term, like “A/B/C  A”, and used the most frequent 100 simplified terms as the target categories."
"We ex-tracted 9, 947 abstracts for training, and 9, 948 ab-stracts for testing from the file “ohsumed.91.”"
"A documents is converted to a bag-of-words vec-tor representation with TFIDF values, after the stop words are removed and all the words are downcased."
"Since the text categorization task requires that mul-tiple categories are assigned if appropriate, we con-structed a binary categorizer, p c (y ∈ {+1,−1}|d), for each category c."
"If the probability p c (+1|d) is greater than 0.5, the category is assigned."
"To con-struct a conditional maximum entropy model, we used the feature function of the form (22), where h i (d) returns the TFIDF value of the i-th word of the document vector."
"We implemented the estimation algorithms as an extension of an ME estimation tool, Amis, [Footnote_8] using the Toolkit for Advanced Optimization (TAO)[REF_CITE], which provides the LMVM and the BLMVM optimization modules."
8 Developed by Yusuke Miyao so as to support various ME estimations such as the efficient estimation with compli-cated event structures[REF_CITE]. Available[URL_CITE]
"For the inequal-ity ME estimation, we added a hook that checks the KKT conditions after the normal convergence test. [Footnote_9] We compared the following models: • ME models only with cut-off (cut-off ), • ME models with cut-off and the Gaussian MAP estimation (gaussian), • Inequality ME models (ineq), • Inequality ME models with 2-norm extension described in Section 4 (2-norm), [Footnote_10]"
9 The tolerance for the normal convergence test (relative im-provement) and the KKT check is 10 −4 . We stop the training if the KKT check has been failed many times and the ratio of the bad (upper and lower active) features among the active features is lower than 0.01.
"10 Here, we fix the penalty constants C 1 = C 2 = 10 16 ."
"For the inequality ME models, we compared the two methods to determine the widths, single and bayes, as described in Section 5."
"Although the Gaussian MAP estimation can use different σ i for each fea-ture, we used a common variance σ for gaussian."
"Thus, gaussian roughly corresponds to single in the way of dealing with the unreliability of features."
"Note that, for inequality models, we started with all possible features and rely on their ability to re-move unnecessary features automatically by solu-tion sparseness."
"The average maximum number of features in a categorizer is 63, 150.0 for the Reuters dataset and 116, 452.0 for the OHSUMED dataset."
"We first found the best values for the control param-eters of each model, W , σ, and the cut-off threshold, by using the development set."
We show that the in-equality models outperform the other methods in the development set.
We then show that these values are valid for the evaluation set.
"We used the first half of the test set as the development set, and the second half as the evaluation set."
Figure 1 shows the accuracies of the inequality ME models for various width factors.
The accura-cies are presented by the “micro averaged” F-score.
The horizontal lines show the highest accuracies of cut-off and gaussian models found by exhaustive search.
"For cut-off, we varied the cut-off thresh-old and found the best threshold."
"For gaussian, we varied σ with each cut-off threshold, and found the best σ and cut-off combination."
We can see that the inequality models outperform the cut-off method and the Gaussian MAP estimation with an appro-priate value for W in both datasets.
"Although the OHSUMED dataset seems harder than the Reuters dataset, the improvement in the OHSUMED dataset is greater than that in the Reuters dataset."
This may be because the OHSUMED dataset is more sparse than the Reuters dataset.
"The 2-norm extension boosts the accuracies, especially for bayes, at the moderate Ws (i.e., with the moderate numbers of active features)."
"However, we can not observe the apparent advantage of the 2-norm extension in terms of the highest accuracy here."
Figure 2 shows the average number of active fea-tures of each inequality ME model for various width factors.
We can see that active features increase when the widths become small as expected.
Figure 3 shows the accuracy of each model as a function of the number of active features.
"We can see that the inequality ME models achieve the high-est accuracy with a fairly small number of active fea-tures, removing unnecessary features on their own."
"Besides, they consistently achieve much higher ac-curacies than the cut-off and the Gaussian MAP es-timation with a small number of features."
"Table 1 summarizes the above results including the best control parameters for the development set, and shows how well each method performs for the evaluation set with these parameters."
"We can see that the best parameters are valid for the evaluation sets, and the inequality ME models outperform the other methods in the evaluation set as well."
This means that the inequality ME model is generally superior to the cut-off method and the Gaussian MAP estima-tion.
"At this point, the 2-norm extension shows the advantage of being robust, especially for the Reuters dataset."
"That is, the 2-norm models outperform the normal inequality models in the evaluation set."
"To see the reason for this, we show the average cross entropy of each inequality model as a function of the width factor in Figure 4."
"The average cross en-tropy was calculated as − C1 1 c L i log p c (y i |d i ), where C is the number of categories."
The cross en-tropy of the 2-norm model is consistently more sta-ble than that of the normal inequality model.
"Al-though there is no simple relation between the abso-lute accuracy and the cross entropy, this consistent difference can be one explanation for the advantage of the 2-norm extension."
"Besides, it is possible that the effect of 2-norm extension appears more clearly in the Reuters dataset because the robustness is more important in the Reuters dataset since the develop-ment set is rather small and easy to overfit."
"Lastly, we could not observe the advantage of bayes method in these experiments."
"However, since our method is still in development, it is premature to conclude that the idea of using different widths according to its unreliability is not successful."
"It is possible that the uncertainty of p̃(x), which were not concerned about, is needed to be modeled, or the Bernoulli trial assumption is inappropriate."
Further investigation on these points must be done.
"We have shown that the inequality ME models outperform the cut-off method and the Gaussian MAP estimation, using the two text categoriza-tion datasets."
"Besides, the inequality ME models achieved high accuracies with a small number of features due to the sparseness of the solution."
"How-ever, it is an open question how the inequality ME model differs from other sophisticated methods of feature selection based on other criteria."
Future work will investigate the details of the in-equality model including the effect of the penalty constants of the 2-norm extension.
Evaluations on other NLP tasks are also planned.
"In addition, we need to analyze the inequality ME model further to clarify the reasons for its success."
"Acknowledgments We would like to thank Yusuke Miyao, Yoshimasa Tsuruoka, and the anonymous reviewers for many helpful comments."
Discriminative models have been of inter-est in the NLP community in recent years.
Previous research has shown that they are advantageous over generative mod-els.
"In this paper, we investigate how dif-ferent objective functions and optimiza-tion methods affect the performance of the classifiers in the discriminative learning framework."
"We focus on the sequence la-belling problem, particularly POS tagging and NER tasks."
Our experiments show that changing the objective function is not as effective as changing the features in-cluded in the model.
"Until recent years, generative models were the most common approach for many NLP tasks."
"Recently, there is a growing interest on discriminative mod-els in the NLP community, and these models were shown to be successful for different tasks[REF_CITE]."
"Dis-criminative models do not only have theoretical ad-vantages over generative models, as we discuss in Section 2, but they are also shown to be empirically favorable over generative models when features and objective functions are fixed[REF_CITE]."
"In this paper, we use discriminative models to investigate the optimization of different objective functions by a variety of optimization methods."
We focus on label sequence learning tasks.
Part-of- Speech (POS) tagging and Named Entity Recogni-tion (NER) are the most studied applications among these tasks.
"However, there are many others, such as chunking, pitch accent prediction and speech edit detection."
"These tasks differ in many aspects, such as the nature of the label sequences (chunks or indi-vidual labels), their difficulty and evaluation meth-ods."
"Given this variety, we think it is worthwhile to investigate how optimizing different objective func-tions affects performance."
"In this paper, we varied the scale (exponential vs logarithmic) and the man-ner of the optimization (sequential vs pointwise) and using different combinations, we designed 4 differ-ent objective functions."
We optimized these func-tions on NER and POS tagging tasks.
"Despite our intuitions, our experiments show that optimizing ob-jective functions that vary in scale and manner do not affect accuracy much."
"Instead, the selection of the features has a larger impact."
The choice of the optimization method is impor-tant for many learning problems.
"We would like to use optimization methods that can handle a large number of features, converge fast and return sparse classifiers."
"The importance of the features, and therefore the importance of the ability to cope with a larger number of features is well-known."
"Since training discriminative models over large corpora can be expensive, an optimization method that con-verges fast might be advantageous over others."
A sparse classifier has a shorter test time than a denser classifier.
"For applications in which the test time is crucial, optimization methods that result in sparser classifiers might be preferable over other methods even if their training time is longer."
"In this paper we investigate these aspects for different optimization methods, i.e. the number of features, training time and sparseness, as well as the accuracy."
"In some cases, an approximate optimization that is more ef-ficient in one of these aspects might be preferable to the exact method, if they have similar accuracy."
We experiment with exact versus approximate as well as parallel versus sequential optimization methods.
"For the exact methods, we use an off-the-shelf gradi- ) ent based optimization routine."
"For the approximate methods, we use a perceptron and a boosting algo-rithm for sequence labelling which update the fea-ture weights parallel and sequentially respectively."
"Label sequence learning is, formally, the problem of learning a function  that  maps  a sequence of ob-servations   to  a &quot; label ! sequence , where each , the set of individual # labels."
"For example, in POS tagging, the words ’s construct a sentence , and is the la-belling of the sentence where is the part of speech tag of the word ."
"We are interested in the super-vised learning %&amp; setting, % where &amp; ( we &apos; are &apos; ingivenordera corpusto learn, $ the classifier."
The most popular model for label sequence learn-ing is the Hidden Markov Model (HMM).
"An HMM, as a generative model, is trained by finding the joint probability distribution over the observation and $ la-bel sequences ) that explains the corpus the best (Figure 1a)."
"In this model, each random vari-able is assumed to be independent of the other ran-dom variables, given its parents."
"Because of the long distance dependencies of natural languages that can-not be modeled by sequences, this conditional inde-pendence assumption is violated in many NLP tasks."
"Another shortcoming of this model is that, due to its generative nature, overlapping features are difficult to use in HMMs."
"For this reason, HMMs have been standardly used with current word-current label, and previous label(s)-current label features."
"However, if we incorporate information about the neighboring words and/or information about more detailed char-acteristics of the current word directly to our model, rather than propagating it through the previous la-bels, we may hope to learn a better classifier."
"Many different models, such as Maximum En-tropy Markov Models (MEMMs)[REF_CITE], Projection based Markov Models (PMMs)[REF_CITE]and Conditional Ran-dom Fields (CRFs)[REF_CITE], have been proposed to overcome these problems."
The common property of these models is their discriminative ap-proach.
They model the probability distribution of the + label * . sequences given the observation sequences:
The best performing models of label sequence learning are MEMMs or PMMs (also known as Maximum Entropy models) whose features are care-fully designed for the specific tasks[REF_CITE].
"However, maximum entropy models suffer from the so called label bias problem, the problem of making local de-cisions[REF_CITE]."
"CRFs define a probability distribution over the whole sequence , globally conditioning over the whole observation sequence (Figure 1b)."
"Be-cause they condition on the observation (as opposed to generating it), they  can use overlapping features."
"The features , used in this paper are of the form: 1."
"Current label and information about the obser-vation sequence, such as the identity or spelling features of a word that is within a window of the word currently labelled."
Each of these /. features  corresponds  to % a choice ;: of 5 and where 0 and is the half window size 2.
"Current label and the neighbors of that label, i.e. features that capture the inter-label depen-dencies."
"Each of these features corresponds to a choice of and the  neighbors  of , e.g. in a bigram model, , ."
"The conditional probability distribution defined by this model is :     +*  , where ’s are the parameters  to be estimated from the training corpus C and is a normaliza-tion term to assure a proper probability distribu-tion."
"In order to simplify the  notation, we in-troduce , , which is the number of times feature , is observed in pair and, , which is the linear combination of all the features with parameterization. tic of is the sufficient  * statis-as:.  "
"Then   ,  we can rewrite  ."
"Given the theoretical advantages of discriminative models over generative models and the empirical support[REF_CITE], and that CRFs are the state-of-the-art among discriminative models for label sequences, we chose CRFs as our model, and trained $ by optimizing various objective $ functions with respect to the corpus ."
The application of these models to the label sequence problems vary widely.
"The individual labels might constitute chunks (e.g. Named-Entity Recognition, shallow parsing), or they may be single entries (e.g. POS tagging)."
"The difficulty, therefore the accuracy of the tasks are very different from each other."
"The evaluation of the systems differ from one task to another, and the nature of the statistical noise level is task and corpus dependent."
"Given this variety, using objective functions tailored for each task might result in better classifiers."
"We consider two dimensions in designing objective functions: exponential versus logarithmic loss func-tions, and sequential versus pointwise optimization functions."
Most estimation procedures in NLP proceed by maximizing the likelihood of the training data.
"To overcome the numerical problems of working with a product of a large number of small probabilities, usually the logarithm of the likelihood of the data is optimized."
"However, most of the time, these sys-tems, sequence labelling systems in particular, are tested with respect to their error rate on test data, i.e. the fraction of times the function assigns a higher score to a label sequence (such that ! &quot; ) than the correct label sequence &quot; for every observation &quot; in test data."
"Then, the rank loss of might be a more natural objective to minimize. $# &amp;% $ &quot;  3 &quot; &quot;  (&apos; &apos; &quot; *+) ,/. # is the total number of label sequences that ranks higher than the correct label $ sequences for the training instances in the corpus ."
"Since optimizing the rank loss is NP-complete, one can optimize an upper bound instead, e.g. an exponential loss func-tion: # # % $ 1&apos; &quot;  3   &quot; &quot; &apos; &quot; +) ,"
The exponential loss function is well studied in the Machine Learning domain.
"The advantage of the exp-loss over the log-loss is its property of pe-nalizing incorrect labellings very severely, whereas it penalizes almost nothing when the label sequence is correct."
This is a very desirable property for a classifier.
Figure 2 shows this property of exp-loss in contrast to log-loss in a binary classification prob-lem.
"However this property also means that, exp-loss has the disadvantage of being sensitive to noisy data, since systems optimizing exp-loss spends more effort on the outliers and tend to be vulnerable to noisy data, especially label noise."
"In many applications it is very difficult to get the whole label sequence correct since most of the time classifiers are not perfect and as the sequences get longer, the probability of predicting every label in the sequence correctly decreases exponentially."
"For this reason performance is usually measured point-wise, i.e. in terms of the number of individual la-bels that are correctly predicted."
"Most common op-timization functions in the literature, however, treat the whole label sequence as one label, penalizing a label sequence that has one error and a label se-quence that is all wrong in the same manner."
We may be able to develop better classifiers by using a loss function more similar to the evaluation func-tion.
One possible way of accomplishing this may be minimizing pointwise loss functions.
"Sequential optimizations optimize + the * ,jointwhereasconditionalpointwiseproba-op-bility distribution timizations that we propose optimize  the marginal * &quot; conditional probability +* &quot; . distribution,  + ,"
We derive four loss functions by taking the cross product of the two dimensions discussed above:
"Sequential Log-loss function: This function, based on the standard maximum likelihood op-timization, is used with CRFs[REF_CITE]. $ 3  &quot; * &quot;&apos; 3 &quot; &quot; (8  &quot; &apos; &quot; (1) &quot;"
"Sequential Exp-loss function: This loss func-tion, was first introduced[REF_CITE]for NLP tasks with a structured output domain."
"However, there, the sum is not over the whole possible label sequence set, but over the best label sequences generated by an external mechanism."
Here we include all possible la-bel sequences; so we do not require an external mechanism to identify the best sequences..
"As shown[REF_CITE]it is possible to sum over all label sequences by using a dy-namic algorithm. $ &quot;  3 &quot;  &quot; 4 2&apos; *) , &quot; + &quot; * &quot; 3 2  4 &apos; (2) &quot;"
Note that the exponential loss function is just the inverse conditional probability plus a con-stant.
"Pointwise Log-loss function: This function op-timizes the marginal probability of the labels at each position conditioning on the observation sequence: $ 3   &quot; * &quot;&apos; &apos; &quot; 3   +* &quot;&apos; &apos; &quot;  + ,"
"Obviously, this function reduces to the sequen-tial log loss if the length of the sequence is ."
"Pointwise Exp-loss function: Following the parallelism in log-loss vs exp-loss functions of sequential optimization (log vs inverse condi-tional probability), we propose minimizing the pointwise exp-loss function below, which re-duces to the standard multi-class exponential loss when the length of the sequence is .  $ &quot;  3  4  &apos;   *2 &quot;  &apos; + , # &quot; &apos; &quot; * &quot; &quot;"
We now compare the performance of the four loss functions described above.
"Although[REF_CITE]proposes a modification of the iterative scaling algorithm for parameter estimation in se-quential log-loss function optimization, gradient-based methods have often found to be more efficient for minimizing the convex loss function in Eq. (1)[REF_CITE]."
"For this reason, we use a gradient based method to optimize the above loss functions."
The gradients of the four loss function can be com-puted as follows:
Sequential Log-loss function: * &quot; 4 3 &quot;  &quot; (3) 2 &apos; &quot; where expectations are taken w.r.t.   + * .
Thus at the optimum the empirical and ex-pected values of the sufficient statistics are equal.
The loss function and the derivatives can be calculated with one pass of the forward-backward algorithm.
Sequential Exp-loss function:  *  3 &quot; &quot; &quot; &quot; * &quot; &apos; (4) &quot;
At the optimum the empirical values of the suf-ficient statistics equals their conditional expec-tations where the contribution of each instance is weighted by the inverse conditional proba-bility of the instance.
"Thus this loss function focuses on the examples that have a lower con-ditional probability, which are usually the ex-amples that the model labels incorrectly."
The computational complexity is the same as the log-loss case.
Pointwise Log-loss function: &apos; 2 * &quot; 4 3 * &quot;  &quot; 42 &quot;
At the optimum the expected value of the suf-ficient statistics conditioned on the observation &quot; are equal to their expected value when also conditioned on the correct label sequence &quot; .
"The computations can be done using the dy-namic programming described[REF_CITE], with the computational complexity of the forward-backward algorithm scaled by a constant."
Pointwise Exp-loss function:  * &quot; 3  * &quot;  &quot; &quot; * &quot; &apos;  &quot;
"At the optimum the expected value of the suf-ficient statistics conditioned on &quot; are equal to the value when also conditioned on &quot; , where each point is weighted by &quot; * &quot; ."
Com-putational complexity is the same as the log-loss case.
"Before presenting the experimental results of the comparison of the four loss functions described above, we describe our experimental setup."
We ran experiments on Part-of-Speech (POS) tagging and Named-Entity-Recognition (NER) tasks.
"For POS tagging, we used the Penn TreeBank cor-pus."
"Following the convention in POS tagging, we used a Tag Dictionary for frequent words."
We used Sec-tions 1-21 for training and[REF_CITE]for testing.
"For NER, we used a Spanish corpus which was provided for the Special Session[REF_CITE]on NER."
There are training and test data sets and the training data consists of about 7200 sentences.
"The individual label set in the corpus consists of 9 la-bels: the beginning and continuation of Person, Or-ganization, Location and Miscellaneous names and nonname tags."
"We  usedis thethreesetdifferentof bigramfeaturefeaturessets, i:.e. the current tag and the current word, the current tag and previous tags.  consists of features and spelling fea-tures of the current word (e.g. ”Is the current word capitalized and the current tag is Person-Beginning?”)."
"Some of the spelling features, which are mostly adapted[REF_CITE]are the last one, two and three letters of the word; whether the first letter is lower case, upper case or alphanumeric; whether the word is capitalized and contains a dot; whether all the letters are capitalized; whether the word con-tains a hyphen.  includes features not only for the current word but also [Footnote_5] for the words within a fixed win-dow 5 of size . is an instance of"
5 where . An example of features for is ”Does the previous word ends with a dot and the current tag is Organization-Intermediate?”.
"For NER, we used a window of size 3 (i.e. consid-ered features for the previous and next words)."
"Since the Penn TreeBank is very large, including fea-tures, i.e. incorporating the information in the neigh-boring words directly to the model, is intractable."
"Therefore, we limited our experiments to and features for POS tagging."
"As a gradient based optimization method, we used an off-the-shelf optimization tool that uses the limited-memory updating method."
We observed that this method is faster to converge than the conju-gate gradient descent method.
"It is well known that optimizing log-loss functions may result in over-fitting, especially with noisy data."
"For this rea-son, we used a regularization term in our cost func-tions."
We experimented with different regularization terms.
"As expected, we observed that the regular-ization term increases the accuracy, especially when the training data is small; but we did not observe much difference when we used different regulariza-tion terms."
The results we report are with the Gaus-sian prior regularization term described[REF_CITE].
"Our goal in this paper is not to build the best tag-ger or recognizer, but to compare different loss func-tions and optimization methods."
"Since we did not spend much effort on designing the most useful fea-tures, our results are slightly worse than, but compa-rable to the best performing models."
We extracted corpora of different sizes (ranging from 300 sentences to the complete corpus) and ran experiments optimizing the four loss functions us-ing different feature sets.
"In Table 1 and Table 2, we report the accuracy of predicting every individ-ual label."
It can be seen that the test accuracy ob-tained by different loss functions lie within a rela-tively small range and the best performance depends on what kind of features are included in the model.
We observed similar behavior when the training set is smaller.
The accuracy is highest when more fea-tures are included to the model.
"From these results we conclude that when the model is the same, opti-mizing different loss functions does not have much effect on the accuracy, but increasing the variety of the features included in the model has more impact."
"In Section 4, we showed that optimizing differ-ent loss function does not have a large impact on the accuracy."
"In this section, we investigate differ-ent methods of optimization."
The conjugate based method used in Section 4 is an exact method.
"If the training corpus is large, the training may take a long time, especially when the number of features are very large."
"In this method, the optimization is done in a parallel fashion by updating all of the pa-rameters at the same time."
"Therefore, the resulting classifier uses all the features that are included in the model and lacks sparseness."
We now consider two approximation methods to optimize two of the loss functions described above.
We first present a perceptron algorithm for labelling sequences.
This algorithm performs parallel opti-mization and is an approximation of the sequential log-loss optimization.
"Then, we present a boosting algorithm for label sequence learning."
This algo-rithm performs sequential optimization by updating one parameter at a time.
It optimizes the sequential exp-loss function.
We compare these methods with the exact method using the experimental setup pre-sented in Section 4.2.
"Calculating the gradients, i.e. the expectations of features for every instance in the training corpus can be computationally expensive if the corpus is very large."
"In many cases, a single training instance might be as informative as all of the corpus to update the parameters."
"Then, an online algorithm which makes updates by using one training example may converge much faster than a batch algorithm."
"If the distribution is peaked, one label is more likely than others and the contribution of this label dominates the expectation values."
"If we assume this is the case, i.e. we make a Viterbi assumption, we can calculate a good approximation of the gradients by consider-ing only the most likely, i.e. the best label sequence according to the current model."
"The following on-line perceptron algorithm (Algorithm 1), presented[REF_CITE], uses these two approximations:"
Algorithm 1 Label sequence Perceptron algorithm . 1: initialize   2: repeat 3: for all training patterns &quot; do compute  &quot; 4: if &quot; then 8 &quot; 3 5: & quot; &quot; 6: 7: end if 8: end for 9: until stopping criteria
"At each iteration, the perceptron algorithm calcu-lates an approximation of the gradient of the sequen-tial log-loss function (Eq. 3) based on the current training instance."
"The batch version of this algo-rithm is a closer approximation of the optimization of sequential log-loss, since the only approximation is the Viterbi assumption."
"The stopping criteria may be convergence, or a fixed number of iterations over the training data."
"The original boosting algorithm (AdaBoost), pre-sented[REF_CITE], is a sequen-tial learning algorithm to induce classifiers for sin-gle random variables.[REF_CITE]presents a boosting algorithm for learning classifiers to predict label sequences."
This algorithm minimizes an upper bound on the sequential exp-loss function (Eq. 2).
"As in AdaBoost, a distribution over observations is defined:       &quot; * &quot; 3 * 3  (5)"
"This distribution which expresses the importance of every training instance is updated at each round, and the algorithm focuses on the more difficult exam-ples."
The sequence Boosting algorithm (Algorithm 2) optimizes an upper bound on the sequential exp-loss function by using the convexity of the exponen-tial function.   is the maximum difference of the sufficient statistic in any label sequence and the
"As it can be seen from Line 4 in Algorithm 2, the feature that was added to the ensemble at each round is determined by a function of the gradient of the se-quential exp-loss function (Eq. 4)."
"At each round, one pass of the forward backward algorithm over the training data is sufficient to calculate ) ’s for all ."
"Considering the sparseness of the features in each training instance, one can restrict the forward back-ward pass only to the training instances that contain the feature that is added to the ensemble in the last round."
"The stopping criteria may be a fixed number of rounds, or by cross-validation on a heldout cor-pus."
The results summarized in Table 3 compares the per-ceptron and the boosting algorithm with the gradi-ent based method.
"Performance of the standard per-ceptron algorithm fluctuates a lot, whereas the aver-age perceptron is more stable."
We report the results of the average perceptron here.
"Not surprisingly, it does slightly worse than CRF, since it is an approx-imation of CRFs."
The advantage of the Perceptron algorithm is its dual formulation.
"In the dual form, explicit feature mapping can be avoided by using the kernel trick and one can have a large number of fea-tures efficiently."
"As we have seen in the previous sections, the ability to incorporate more features has a big impact on the accuracy."
"Therefore, a dual per-ceptron algorithm may have a large advantage over other methods."
"When only HMM features are used, Boosting as a sequential algorithm performs worse than the gra-dient based method that optimizes in a parallel fash-ion."
This is because there is not much information in the HMM features other than the identity of the word to be labeled.
"Therefore, the boosting algo-rithm needs to include almost all the features one by one in the ensemble."
"When there are just a few more informative features, the boosting algorithm makes better use of them."
This situation is more dramatic in POS tagging.
"The gradient based method uses all of the available fea-tures, whereas boosting uses only about 10% of the features."
"Due to the loose upper bound that Boosting optimizes, the estimate of the updates are very con-servative."
"Therefore, the same features are selected many times."
"This negatively effects the convergence time, and the other methods outperform Boosting in terms of training time."
"In this paper, we investigated how different objec-tive functions and optimization methods affect the accuracy of the sequence labelling task in the dis-criminative learning framework."
Our experiments show that optimizing different objective functions does not have a large affect on the accuracy.
Ex-tending the feature space is more effective.
"We con- clude that methods that can use large, possibly infi-nite number of features may be advantageous over others."
We are running experiments where we use a dual formulation of the perceptron algorithm which has the property of being able to use infinitely many features.
Our future work includes using SVMs for label sequence learning task.
This paper describes a fast algorithm that se-lects features for conditional maximum en-tropy modeling.
"In this new algorithm, instead, we only compute the approximate gains for the top-ranked features based on the models obtained from previous stages."
Experiments on WSJ data in Penn Treebank are conducted to show that the new algorithm greatly speeds up the feature selec-tion process while maintaining the same qual-ity of selected features.
One variant of this new algorithm with look-ahead functionality is also tested to further confirm the good quality of the selected features.
"The new algo-rithm is easy to implement, and given a fea-ture space of size F, it only uses O(F) more space than the original IFS algorithm."
"Maximum Entropy (ME) modeling has received a lot of attention in language modeling and natural language processing for the past few years (e.g.,[REF_CITE])."
One of the main advantages using ME modeling is the ability to incorporate various features in the same framework with a sound mathematical foundation.
There are two main tasks in ME modeling: the feature selection process that chooses from a feature space a subset of good features to be included in the model; and the parameter estimation process that estimates the weighting factors for each selected feature in the exponential model.
This paper is primarily con-cerned with the feature selection process in ME modeling.
"While the majority of the work in ME modeling has been focusing on parameter estimation, less effort has been made in feature selection."
This is partly because feature selection may not be neces-sary for certain tasks when parameter estimate al-gorithms are fast.
"However, when a feature space is large and complex, it is clearly advantageous to perform feature selection, which not only speeds up the probability computation and requires smaller memory size during its application, but also shortens the cycle of model selection during the training."
Feature selection is a very difficult optimization task when the feature space under investigation is large.
"This is because we essentially try to find a best subset from a collection of all the possible feature subsets, which has a size of 2 | W | , where |W| is the size of the feature space."
"In the past, most researchers resorted to a sim-ple count cutoff technique for selecting features[REF_CITE], where only the features that occur in a corpus more than a pre-defined cutoff threshold get selected."
It is a simple and probably effective tech-nique for language modeling tasks.
"Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship between c 2 test score † and the likelihood gain, which, however, is absent."
"While this greedy search assumption is reasonable, the speed of the IFS al-gorithm is still an issue for complex tasks."
"For better understanding its performance, we re-implemented the algorithm."
"Given a task of 600,000 training instances, it takes nearly four days to select 1000 features from a feature space with a little more than 190,000 features."
"While this technique is applicable for certain feature sets, such as word link features re-ported in their paper, the f -orthogonal condition usually does not hold if part-of-speech tags are dominantly present in a feature subset."
"Past work, including[REF_CITE]and[REF_CITE], has shown that the IFS algorithm utilizes much fewer features than the count cutoff method, while maintaining the similar precision and recall on tasks, such as prepositional phrase attachment, text categorization and base NP chunking."
This leads us to further explore the possible improve-ment on the IFS algorithm.
"In section 2, we briefly review the IFS algo-rithm."
"Then, a fast feature selection algorithm is described in section 3."
"Section 4 presents a number of experiments, which show a massive speed-up and quality feature selection of the new algorithm."
"Finally, we conclude our discussion in section 5."
"For better understanding of our new algorithm, we start with briefly reviewing the IFS feature selec-tion algorithm."
"Suppose the conditional ME model takes the following form: p(y | x) = Z1(x) exp( Â l j f j (x, y)) j where f j are the features, l j are their corre-sponding weights, and Z(x) is the normalization factor."
"The algorithm makes the approximation that the addition of a feature f in an exponential model af-fects only its associated weight a , leaving un-changed the l-values associated with the other features."
Here we only present a sketch of the algo-rithm in Figure 1.
Please refer to the original paper for the details.
"In the algorithm, we use I for the number of training instances, Y for the number of output classes, and F for the number of candidate features or the size of the candidate feature set."
One difference here from the original IFS algo-rithm is that we adopt a technique[REF_CITE]for optimizing the parameters in the condi-tional ME training.
"Specifically, we use array z to store the normalizing factors, and array sum for all the un-normalized conditional probabilities sum[i, y]."
"Thus, one only needs to modify those sum[i, y] that satisfy f * (x i , y)=1, and to make changes to their † corresponding normalizing factors z[i]."
"In contrast to what is shown[REF_CITE]’s paper, here is how the different values in this variant of the IFS algorithm are computed."
"Let us denote sum(y | x) = exp( Â l j f j (x, y)) j Z(x) ="
Â sum(y | x) y
"Then, the model can be represented by sum(y|x) and Z(x) as follows: p( y | x) = sum( y | x) / Z(x) where sum(y|x i ) and Z(x i ) correspond to sum[i,y] and z[i] in Figure 1, respectively."
"Assume the selected feature set is S, and f is currently being considered."
"The goal of each se-lection stage is to select the feature f that maxi-mizes the gain of the log likelihood, where the a and gain of f are derived through following steps:"
"Let the log likelihood of the model be L( p) ≡ - Â ~p(x, y)log( p(y | x)) x,y = - Â ~p(x, y)log(sum(y | x) / Z(x)) x,y and the empirical expectation of feature f be E p˜ ( f ) ="
"Â ˜p(x,y) f (x,y) x,y"
"With the approximation assumption[REF_CITE]’s paper, the un-normalized component † and the normalization factor of the model have the following recursive forms: sum S a » f (y | x) = sum S (y | x) ⋅ e a Z a S»f (x) = Z S (x) - sum S (y | x) + sum a S»f (y | x)"
The approximate gain of the log likelihood is computed by
"G S» f ( a ) ≡ L(p a S» f ) - L(p S ) = - Â ˜p(x)(logZ S» f, a (x)/Z S (x)) x + a E p˜ ( f ) (1)"
"The maximum approximate gain and its corre-sponding a are represented as: ~ DL(S, f ) = max G S» f ( a ) a ~ a (S, f ) = arg max G S»f ( a ) a"
The inefficiency of the IFS algorithm is due to the following reasons.
"The algorithm considers all the candidate features before selecting one from them, and it has to re-compute the gains for every feature at each selection stage."
"In addition, to compute a parameter using Newton’s method is not always efficient."
"Therefore, the total computation for the whole selection processing can be very expensive."
"Let g(j, k) represent the gain due to the addition of feature f j to the active model at stage k."
"In our experiments, it is found even if D (i.e., the addi-tional number of stages after stage k) is large, for most j, g(j, k+ D ) - g(j, k) is a negative number or at most a very small positive number."
"This leads us to use the g(j, k) to approximate the upper bound of g(j, k+ D )."
"The intuition behind our new algorithm is that when a new feature is added to a model, the gains for the other features before the addition and after the addition do not change much."
"When there are changes, their actual amounts will mostly be within a narrow range across different features from top ranked ones to the bottom ranked ones."
"Therefore, we only compute and compare the gains for the features from the top-ranked downward until we reach the one with the gain, based on the new model, that is bigger than the gains of the remain-ing features."
"With a few exceptions, the gains of the majority of the remaining features were com-puted based on the previous models."
"As in the IFS algorithm, we assume that the ad-dition of a feature f only affects its weighting fac-tor a ."
"Because a uniform distribution is assumed as the prior in the initial stage, we may derive a closed-form formula for a (j, 0) and g(j, 0) as fol-lows. a ( j,0) = log( R e p( f ) ⋅ 1-1R- p 0 ) † 0 e ( f ) g( j,0) = L( p ∅» a (i,0f) ) - L( p ∅ ) ="
"E d ( f ) [R e ( f )log R e p( f ) 0 + (1- R e ( f ))log 1-1-R e p( f ) ] 0 where ∅ denotes an empty set, p ∅ is the uni- † form distribution."
The other steps for computing the gains and selecting the features are given in Figure 2 as a pseudo code.
"Because we only com-pute gains for a small number of top-ranked fea-tures, we call this feature selection algorithm as Selective Gain Computation (SGC) Algorithm."
"In the algorithm, we use array g to keep the sorted gains and their corresponding feature indi-ces."
"In practice, we use a binary search tree to maintain the order of the array."
The key difference between the IFS algorithm and the SGC algorithm is that we do not evaluate all the features for the active model at every stage (one stage corresponds to the selection of a single feature).
"Initially, the feature candidates are or-dered based on their gains computed on the uni-form distribution."
"The feature with the largest gain gets selected, and it forms the model for the next stage."
"In the next stage, the gain of the top feature in the ordered list is computed based on the model just formed in the previous stage."
This gain is compared with the gains of the rest features in the list.
"If this newly computed gain is still the largest, this feature is added to form the model at stage 3."
"If the gain is not the largest, it is inserted in the ordered list so that the order is maintained."
"In this case, the gain of the next top-ranked feature in the ordered list is re-computed using the model at the current stage, i.e., stage 2."
This process continues until the gain of the top-ranked feature computed under the current model is still the largest gain in the ordered list.
"Then, the model for the next stage is created with the addi- tion of this newly selected feature."
"The whole fea-ture selection process stops either when the number of the selected features reaches a pre-defined value in the input, or when the gains be-come too small to be useful to the model."
"In addition to this basic version of the SGC al-gorithm, at each stage, we may also re-compute additional gains based on the current model for a pre-defined number of features listed right after feature f * (obtained in step 2) in the ordered list."
This is to make sure that the selected feature f * is indeed the feature with the highest gain within the pre-defined look-ahead distance.
We call this vari-ant the look-ahead version of the SGC algorithm.
A number of experiments have been conducted to verify the rationale behind the algorithm.
"In par-ticular, we would like to have a good understand-ing of the quality of the selected features using the SGC algorithm, as well as the amount of speed-ups, in comparison with the IFS algorithm."
"The first sets of experiments use a dataset {(x, y)}, derived from the Penn Treebank, where x is a 10 dimension vector including word, POS tag and grammatical relation tag information from two ad-jacent regions, and y is the grammatical relation tag between the two regions."
Examples of the grammatical relation tags are subject and object with either the right region or the left region as the head.
"The total number of different grammatical tags, i.e., the size of the output space, is 86."
"There are a little more than 600,000 training instances generated from section 02-22 of WSJ in Penn Treebank, and the test corpus is generated from section 23."
"In our experiments, the feature space is parti-tioned into sub-spaces, called feature templates, where only certain dimensions are included."
Con-sidering all the possible combinations in the 10-dimensional space would lead to 2 10 feature tem-plates.
"To perform a feasible and fair comparison, we use linguistic knowledge to filter out implausi-ble subspaces so that only 24 feature templates are actually used."
"With this amount of feature tem-plates, we get more than 1,900,000 candidate fea-tures from the training data."
"To speed up the experiments, which is necessary for the IFS algo-rithm, we use a cutoff of 5 to reduce the feature space down to 191,098 features."
"On average, each candidate feature covers about 485 instances, which accounts for 0.083% over the whole training instance set and is computed through: ac = ÂÂ f j (x, y)/ Â 1 j x,y j"
The first experiment is to compare the speed of the IFS algorithm with that of SGC algorithm.
"Theoretically speaking, the IFS algorithm com-putes the gains for all the features at every stage."
"This means that it requires O(NF) time to select a feature subset of size N from a candidate feature set of size F. On the other hand, the SGC algorithm considers much fewer features, only 24.1 features on average at each stage, when selecting a feature from the large feature space in this experiment."
"Figure 3 shows the average number of features computed at the selected points for the SGC algo-rithm,[REF_CITE]look-ahead, as well as the IFS algorithm."
"The averaged number of features is taken over an interval from the initial stage to the current feature selection point, which is to smooth out the fluctuation of the numbers of features each selection stage considers."
"The second algorithm looks at an additional fixed number of features, 500 in this experiment, beyond the ones considered by the basic SGC algorithm."
"The last algorithm has a linear decreasing number of features to select, because the selected features will not be consid-ered again."
"In Figure 3, the IFS algorithm stops after 1000 features are selected."
This is because it takes too long for this algorithm to complete the entire selection process.
"The same thing happens in Figure 4, which is to be explained below."
"To see the actual amount of time taken by the SGC algorithms and the IFS algorithm with the currently available computing power, we use a Linux workstation with 1.6Ghz dual Xeon CPUs and 1 GB memory to run the two experiments si-multaneously."
"As it can be expected, excluding the beginning common part of the code from the two algorithms, the speedup from using the SGC algo-rithm is many orders of magnitude, from more than 100 times to thousands, depending on the number of features selected."
The results are shown in Fig-ure 4.
"To verify the quality of the selected features using our SGC algorithm, we conduct four experi-ments: one uses all the features to build a condi-tional ME model, the second uses the IFS algorithm to select 1,000 features, the third uses our SGC algorithm, the fourth uses the SGC algo-rithm with 500 look-ahead, and the fifth takes the top n most frequent features in the training data."
The precisions are computed on section 23 of the WSJ data set in Penn Treebank.
The results are listed in Figure 5.
Three factors can be learned from this figure.
"First, the three IFS and SGC algo-rithms perform similarly."
The inferior performance of the model with all the fea-tures at the right side of the chart is likely due to the data over-fitting problem.
"Third, the simple count cutoff algorithm significantly under-performs the other feature selection algorithms when feature subsets with no more than 10,000 features are considered."
"To further confirm the findings regarding preci-sion, we conducted another experiment with Base NP recognition as the task."
"The experiment uses section 15-18 of WSJ as the training data, and sec-tion 20 as the test data."
"When we select 1,160 fea-tures from a simple feature space using our SGC algorithm, we obtain a precision/recall of 92.75%/93.25%."
"The best reported ME work on this task includes[REF_CITE]that has the pre-cision/recall of 92.84%/93.18% with a cutoff of 5, and[REF_CITE]has reached the perform-ance of 93.04%/93.31% with cutoff of 7 and reached a performance of 92.46%/92.74% with 615 features using the IFS algorithm."
"While the results are not directly comparable due to different feature spaces used in the above experiments, our result is competitive to these best numbers."
This shows that our new algorithm is both very effective in selecting high quality features and very efficient in performing the task.
Feature selection has been an important topic in both ME modeling and linear regression.
"In the past, most researchers resorted to count cutoff technique in selecting features for ME modeling[REF_CITE]."
"A more refined algorithm, the incremental feature selection algo-rithm[REF_CITE], allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages."
"As discussed[REF_CITE], the count cutoff technique works very fast and is easy to implement, but has the drawback of containing a large number of re- dundant features."
"In contrast, the IFS removes the redundancy in the selected feature set, but the speed of the algorithm has been a big issue for complex tasks."
"Having realized the drawback of the IFS algorithm,[REF_CITE]pro-posed an f -orthogonal condition for selecting k features at the same time without affecting much the quality of the selected features."
"While this technique is applicable for certain feature sets, such as link features between words, the f -orthogonal condition usually does not hold if part-of-speech tags are dominantly present in a feature subset."
It is a simple and probably effective technique for language model-ing tasks.
"Since ME models are optimized using their likelihood or likelihood gains as the criterion, it is important to establish the relationship between c 2 test score and the likelihood gain, which, how-ever, is absent."
"There is a large amount of literature on feature selection in linear regression, where least mean squared errors measure has been the primary opti-mization criterion."
Two issues need to be ad-dressed in order to effectively use these techniques.
"One is the scalability issue since most statistical literature on feature selection only concerns with dozens or hundreds of features, while our tasks usually deal with feature sets with a million of features."
"The other is the relationship between mean squared errors and likelihood, similar to the concern expressed in the previous paragraph."
These are important issues and require further in-vestigation.
"In summary, this paper presents our new im-provement to the incremental feature selection al-gorithm."
The new algorithm runs hundreds to thousands times faster than the original incre-mental feature selection algorithm.
"In addition, the new algorithm selects the features of a similar quality as the original Berger et al algorithm, which has also shown to be better than the simple cutoff method in some cases."
We investigate the performance of the Structured Language Model (SLM) in terms of perplexity (PPL) when its compo-nents are modeled by connectionist mod-els.
"The connectionist models use a dis-tributed representation of the items in the history and make much better use of contexts than currently used interpolated or back-off models, not only because of the inherent capability of the connection-ist model in fighting the data sparseness problem, but also because of the sub-linear growth in the model size when the context length is increased."
"The connec-tionist models can be further trained by an EM procedure, similar to the previously used procedure for training the SLM."
"Our experiments show that the connectionist models can significantly improve the PPL over the interpolated and back-off mod-els on the UPENN Treebank corpora, after interpolating with a baseline trigram lan-guage model."
"The EM training procedure can improve the connectionist models fur-ther, by using hidden events obtained by the SLM parser."
In many systems dealing with natural speech or lan-guage such as Automatic Speech Recognition and
"Statistical Machine Translation, a language model is a crucial component for searching in the often prohibitively large hypothesis space."
"Most of the state-of-the-art systems use n-gram language mod-els, which are simple and effective most of the time."
Many smoothing techniques that improve lan-guage model probability estimation have been pro-posed and studied in the n-gram literature[REF_CITE].
"Recent efforts have studied various ways of us-ing information from a longer context span than that usually captured by normal n-gram language mod-els, as well as ways of using syntactical informa-tion that is not available to the word-based n-gram models[REF_CITE]."
All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees.
"Since these language models capture useful hierar-chical characteristics of language, they can improve the PPL significantly for various tasks."
"Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM)[REF_CITE], a severe data sparse-ness problem was observed[REF_CITE]when the number of conditioning features was increased."
There has been recent promising work in us-ing distributional representation of words and neu-ral networks for language modeling[REF_CITE]and parsing[REF_CITE].
One great ad-vantage of this approach is its ability to fight data sparseness.
The model size grows only sub-linearly with the number of predicting features used.
It has been shown that this method improves significantly on regular n-gram models in perplexity[REF_CITE].
"The ability of the method to accommo-date longer contexts is most appealing, since exper-iments have shown consistent improvements in PPL when the context of one of the components of the SLM is increased in length[REF_CITE]."
"Moreover, because the SLM provides an EM train-ing procedure for its components, the connectionist models can also be improved by the EM training."
"In this paper, we will study the impact of neural network modeling on the SLM, when all of its three components are modeled with this approach."
An EM training procedure will be outlined and applied to further training of the neural network models.
"Recently, a relatively new type of language model has been introduced where words are represented by points in a multi-dimensional feature space and the probability of a sequence of words is computed by means of a neural network."
"The neural network, having the feature vectors of the preceding words as its input, estimates the probability of the next word[REF_CITE]."
The main idea behind this model is to fight the curse of dimensionality by inter-polating the seen sequences in the training data.
The generalization this model aims at is to assign to an unseen word sequence a probability similar to that of a seen word sequence whose words are similar to those of the unseen word sequence.
The similarity is defined as being close in the multi-dimensional space mentioned above.
"In brief, this model can be described as follows."
"A feature vector is associated with each token in the input vocabulary, that is, the vocabulary of all the items that can be used for conditioning."
Then the conditional probability of the next word is expressed as a function of the input feature vectors by means of a neural network.
This probability is produced for every possible next word from the output vocab-ulary.
"In general, there does not need to be any rela-tionship between the input and output vocabularies."
The feature vectors and the parameters of the neural network are learned simultaneously during training.
"The input to the neural network are the feature vec- tors for all the inputs concatenated, and the output is the conditional probability distribution over the output vocabulary."
"The idea here is that the words which are close to each other (close in the sense of their role in predicting words to follow) would have similar (close) feature vectors and since the proba-bility function is a smooth function of these feature values, a small change in the features should only lead to a small change in the probability."
"Model   conditional   probability  The function input and output vocabularies  andand arerespectivelyfrom the, where is determined in two parts: the input vocabulary  a real vector of fixed 1."
A mapping that associates with each word in length 2.
A conditional probability function which takes vectors of the input itemsas the input the concatenation  of   the feature  .
"The function produces a probability distribu-tion (a vector) over  , the &quot;$! # element being the conditional probability of the %!$# member of  ."
This probability function is realized by a standard multi-layer neural network.
A soft-max function (Equation 4) is used at the output of the neural net to make sure probabilities sum to 1. &amp;
"Training is achieved by searching for parameters of the neural network and the values of feature vectors that maximize the penalized log-likelihood of the training corpus: )+( ,* -/10.  :. 9; .*$&gt;&lt; &gt;= =&gt;&gt;= &lt; ; ?. @ *:A BDC  6 BDC &apos; (1) where F     is the probability of word (network &amp; output at time ! ), J is the training data size and K is a regularization term, sum of the parameters’ squares in our case."
The model architecture is given in Figure 1.
The neural network is a simple fully connected network with one hidden layer and sigmoid transfer func-tions.
The input to the function is the concatenation of the feature vectors of the input items.
"The out-put of the output layer is passed though a softmax to make sure that the scores are positive and sum up to one, hence are valid probabilities."
"More specifically, by: the output of the ? @ hidden layer is given ( &lt; &gt;&lt; =&gt;=&gt;= &lt;  (  6 -  * *    C (2) where # ! # input ! # is the output of the hidden layer, ! is the &quot; of the network, # and $ are weight and bias elements for the hidden layer respectively, and % is the number of hidden units."
"Furthermore, the outputs are given by: &amp; ( - ( &lt; &lt;&gt;=&gt;=&gt;= &lt; 9 &apos; ) 9 (&apos; ) (3) ( &lt; &gt;&lt; =&gt;&gt;= = &lt; 9 &apos; ) 9 * 5( - + , + , (4) where and $ are weight and bias elements for the output layer before the softmax layer."
"The soft-max layer (equation 4) ensures that the outputs are positive and sum to one, hence are valid probabili- $ ! # output ! $# ties."
"The of the neural network, corre-sponding to the item of the output vocab-ulary, is exactly  the F sought   conditional F probability, that is - /. . ."
Standard back-propagation is used to train the pa-rameters of the neural network as well as the feature vectors.
See[REF_CITE]for details about neural networks and back-propagation.
The function we try to maximize is the log-likelihood of the training data given by equation 1.
"It is straightforward to com-pute the gradient of the likelihood function for the feature vectors and the neural network parameters, and hence compute their updates."
We should note from equation 4 that the neural network model is similar in functional form to the maximum entropy model[REF_CITE]ex-cept that the neural network learns the feature func-tions by itself from the training data.
"However, unlike the G/IIS algorithm for the maximum en-tropy model, the training algorithm (usually stochas-tic gradient descent) for the neural network models is not guaranteed to find even a local maximum of the objective function."
"It is very important to mention that one of the great advantages of this model is that the number of inputs can be increased causing only sub-linear increase in the number of model parameters, as op-posed to exponential growth in n-gram models."
"This makes the parameter estimation more robust, espe-cially when the input span is long."
"An extensive presentation of the SLM can be found probabilityin (Chelba and  # Jelinek  % to, 2000every). sentenceThe model # assignsand ev-a 0 0 ery possible binary parse ."
"The terminals of are 0 the words of # with POS tags, and the nodes of are annotated with phrase headwords and non-terminal labels."
Let # be a sentence of length 1 words to which we have prepended the sentence be-ginning marker &lt;s&gt; and appended the sentence end Let # . marker &lt;/s&gt; so  that 2 243 be . the&lt;s&gt;wordand 2 -prefix . &lt;of/sthe&gt;. 263 sentence — the words from the beginning of the 0 sentence up to the current position — and # the word-parse -prefix.
"Figure 2 shows a word-parse -prefix; h_0, .., h_{-m} are the ex-posed heads, each head being a pair (headword, non-terminal label), or (word, POS tag) in the case of a root-only tree."
The exposed heads at a given po-sition in the input sentence are a function of the word-parse -prefix.
# and a complete parse can be broken up into:  & lt; 7 ? 9 C ( 8A * * : 4 6 &lt;; 9 @ * 7 @ * C &gt;=  F 9 @ * 7 @ * &lt; ; C ?= 8
"B * &lt;@  * B 9 @ * 7 @ * &lt; ; &lt;F &lt; * * &gt;= =&gt;= * B @ * DC C (5) where  : 0 # is the word-parse &quot; -prefix   ! 2 isisthethetagwordassignedpredictedto 2 by WORD-PREDICTORby the TAGGER is the number of operations the CON-STRUCTOR executes at sentence position before passing control to the WORD-PREDICTOR (the -th operation at position 0 k is the null transi-tion); is a function of - denotes the -th CONSTRUCTOR operation carried out at position k in the word string; the op-erations performed by the CONSTRUCTOR ensure that all possible binary branching parses, with all possible headword  and non-terminal label assign-ments for the 2  - 2 word sequence, can be gen-erated."
The - sequence of CONSTRUC-TOR operations at position grows the word-parse &quot; -prefix into a word-parse -prefix.
"The SLM is based on three probabilities, each can be specified using various smoothing methods and parameterized (approximated) by using differ-ent contexts."
The bottom-up nature of the SLM parser enables us to condition the three probabili-ties on features related to the identity of any exposed head and any structure below the exposed head.
"Since the number of parses for a given  !0  word  prefix # grows exponentially with , , the state space of our model is huge even for rela-tively short sentences, so we have to use a search strategy that prunes it."
One choice is a synchronous multi-stack search algorithm[REF_CITE]which is very similar to a beam search.
"The language model probability assignment for the word at position  in the input sentence is made using:  6 &lt;; 6 9 * 9 &lt; C   7  C   - &lt;; 9 * 9 4 6 4 - 7 C ?= 6 7 C &lt; &lt; 7 C &lt; . 7 C . (6) which ensures a proper probability normalization over strings  # , where is the set of all parses present in our stacks at the current stage ."
"Each model component of the SLM —WORD-PREDICTOR, TAGGER, CONSTRUCTOR— is initialized from a set of parsed sentences after under-going headword percolation and binarization."
An N-best EM[REF_CITE]variant is then employed to jointly reestimate the model parameters such that the PPL on training data is decreased — the likelihood of the training data under our model is increased.
"The reduction in PPL is shown experi-mentally to  carry over to the test data. denote 0 the joint sequence of # % with Let # quenceparse structure  #  .isThe, accordingprobabilityto Equationof a # 5, se-the product of the corresponding elementary events."
"This product form makes the three components of the SLM separable, therefore, we can estimate the parameters separately."
"According to the EM algo-rithm, the auxiliary function can be written as: &quot;! ! (6 $ &lt; # C % -  7 9 $ A !# C 0  &lt; 7 ! C = (7)"
"The E step * in the EM algorithm is * to find  0 &apos; # &amp;)( under the model parameters ( of the ters that maximize the auxiliary function 0 + previous * iteration, the M step is to find parame- * * ( above."
"In practice, since the space of , all possi-ble parses, is huge, we normally use a synchronous multi-stack search algorithm to sample the most probable parses and approximate the space by the N-best parses.[REF_CITE]showed that as long as the N-best parses remain invariant, the M step will increase the likelihood of the train-ing data."
"As described in the previous section, the three com-ponents of the SLM can be parameterized in various ways."
"The neural network model, because of its abil-ity in fighting the data sparseness problem, is a very natural choice when we want to use longer contexts to improve the language model performance."
"The training criterion for the neural network model is given by Equation 1 , when we have la-beled training data for the SLM."
The labels —the parse structure— are used to get the conditioning variables.
"In order to take advantage of the ability of the SLM in generating many hidden parses, we need to modify the training criterion for the neural network model."
"Actually, if we take the EM auxil-iary function in Equation 7 and find parameters * * ( of the neural network models to maximize + , the solution will be very simple."
"When standard back-propagation is used to optimize Equation 1, the derivative of with respect to the parameters is calculated and used as the direction * * ( for the gra-dient descent algorithm."
"Since + is nothing but a weighted average of the log-likelihood func-tions, the derivative of + with respect to the param-eters is then a weighted average of the derivatives of the log-likelihood functions."
"In practice, we use the SLM with all components modeled by neural net-works to generate N-best parses in the E step, and for the M step, we use the modified back-propagation algorithm to estimate the parameters of the neural network models based on the weights calculated in the E step."
We should be aware that there is no proof that this EM procedure can actually increase the likelihood of the training data.
"Not only are we using a small portion of the entire hidden parse space, but we also use the stochastic gradient descent algorithm that is not guaranteed to converge, for training the neural network models."
"Bearing this in mind, we will show experimentally that this flawed EM procedure can still lead to improvements in PPL."
We have used the UPenn Treebank portion of the WSJ corpus to carry out our experiments.
The UPenn[REF_CITE]sections of hand-parsed sentences.
"We used section 00-20 for training our models, section 21-22 for tuning some param-eters (i.e., estimating discount constant for smooth-ing, and/or making sure overtraining does not occur) and section 23-24 to test our models."
"Before car-rying out our experiments, we normalized the text in the following ways: numbers in Arabic form are replaced by a single token “N”, punctuations are re-moved, all words are mapped to lower case, extra in-formation in the parse (such like traces) are ignored."
The word vocabulary contains 10k words including a special token for unknown words.
"All of the experimen-tal results in this section are based on this corpus and split, unless otherwise stated."
"Since better performance of the SLM was reported recently[REF_CITE]by using Kneser-Ney smoothing, we first improved the baseline model by using a variant of Kneser-Ney smoothing: the in-terpolated Kneser-Ney smoothing as[REF_CITE], which is also implemented in the SRILM toolkit[REF_CITE]."
There are three notable differences in our imple-mentation of the interpolated Kneser-Ney smooth-ing related to that in the SRILM toolkit.
"First, we used one discount constant for each n-gram level, in-stead of three different discount constants."
"Second, our discount constant was estimated by maximizing the log-likelihood of the heldout data (assuming the discount constant is between 0 and 1), instead of the Good-Turing estimate."
"Finally, in order to deal with the fractional counts we encounter during the EM training procedure, we developed an approxi-mate Kneser-Ney smoothing for fractional counts."
"For lack of space, we do not go into the details of this approximation, but our approximation becomes the exact Kneser-Ney smoothing when the counts are in-tegers."
"In order to test our Kneser-Ney smoothing im-plementation, we built a trigram language model and compared the performance with that from the SRILM."
"Our[REF_CITE].6 and the SRILM[REF_CITE].3, therefore, although there are differences in the implementation details, we think our result is close enough to the SRILM."
"Having tested the smoothing method, we applied it to the SLM."
We used the Kneser-Ney smooth-ing to all components with the same parameteriza-tion as the h-2 scheme[REF_CITE].
Table 1 is the comparison between the deleted-interpolation (DI) smoothing and the Kneser-Ney (KN) smooth-ing.
The in Table 1 is the interpolation weight between the SLM and the trigram language model ( =1.0 being the trigram language model).
The no-tation “En” indicates the models were obtained af-ter “n” iterations of EM training [Footnote_1] .
"1 In particular, E0 simply means initialization."
"Since Kneser-Ney smoothing is consistently better than deleted-interpolation, we later on report only the Kneser-Ney smoothing results when comparing to the neural network models."
We used the neural network models for all of the three components of the SLM.
The neural network models are exactly as described in Section 2.1.
"Since the inputs to the networks are always a mixture of words and NT/POS tags, while the output probabili-ties are over words in the PREDICTOR, POS tags in the TAGGER, and adjoint actions in the PARSER, we used separate input and output vocabularies in all cases."
"In all of our experiments with the neu-ral network models, we used 30 dimensional feature vectors as input encoding of the mixed items, 100 hidden units and a starting learning rate of 0.001."
Stochastic gradient descent was used for training the models for a maximum of 50 iterations.
The initial-ization for the parameters is done randomly with a uniform distribution centered at zero.
"In order to study the behavior of the SLM when longer context is used for conditioning the probabilities, we gradually increased the context of the PREDICTOR model."
"First, the third exposed previous head was added."
"Since the syntactical head gets the head word from one of the children, either left or right, the child that does not contain the head word (hence called opposite child) is never used later on in predicting."
This is particularly not appropriate for the prepositional phrase because the preposition is always the head word of the phrase in the UPenn Treebank annotation.
"Therefore, we also added the opposite child of the first exposed previous head into the context for predicting."
Both Kneser-Ney smoothing and the neural network model were studied when the context was gradually increased.
The results are shown in Table 2.
"In Table 2, “nH” stands for “n” exposed previous heads are used for conditioning in the PREDICTOR component, “nOP” stands for “n” opposite children are used, starting from the most recent one."
"As we can see, when the length of the context is increased,"
Kneser-Ney smoothing saturates quickly and could not improve the PPL further.
"On the other hand, the neural network model can still consistently im-prove the PPL, as longer context is used for predict-ing."
"Overall, the best neural network model (after interpolation with a trigram) achieved 8% relative improvement over the best result from Kneser-Ney smoothing."
Another interesting result is that it seems the neu-ral network model can learn a probability distribu-tion that is less correlated to the normal trigram model.
"Although before interpolating with the tri-gram, the PPL results of the neural network models are not as good as the Kneser-Ney smoothed models, they become much better when combined with the trigram."
"In the results of Table 2, the trigram model is a Kneser-Ney smoothed model that gave[REF_CITE].6 by itself."
"The interpolation weight with the tri-gram is 0.4 and 0.5 respectively, for the Kneser-Ney smoothed SLM and neural network based SLM."
"To better understand why using the neural net-work models can result in such behavior, we should look at the difference between the training PPL and test PPL."
Figure 3 shows the ratio between the test PPL and train PPL.
"We can see that for the neural network models, the ratios are much smaller than that for the Kneser-Ney smoothed models."
"Further-more, as the length of context increases, the ratio for the Kneser-Ney smoothed model becomes greater — a clear sign of over-parameterization."
"However, the ratio for the neural network model changes very little even when the length of the context increases from 4 (2H) to 8 (3H-1OP)."
"The exact reason why the neural network models are more uncorrelated to the trigram is not completely understood, but we conjecture that part of the reason is that the neural network models can learn a probability distribution very different from the trigram by putting much less probability mass on the training examples."
After the neural network models were trained from the labeled data —the UPenn Treebank— we per-formed one iteration of the EM procedure described in Section 4.
"The neural network model based SLM was used to get N-best parses for each training sen-tence, via the multi-stack search algorithm."
This E step provided us a bigger collection of parse struc-tures with weights associated with them.
"In the next M step, we used the stochastic gradient descent al-gorithm (modified to utilize the weights associated with each parse structure) to train the neural network models."
The modified stochastic gradient descent al-gorithm was run for a maximum of 30 iterations and the initial parameter values are those from the the previous iteration.
"Table 3 shows the PPL results after one EM train-ing iteration for both the neural network models and the approximated Kneser-Ney smoothed mod-els, compared to the results before EM training."
"For the neural network models, the EM training did improve the PPL further, although not a lot."
The improvement from training is consistent with the training results showed[REF_CITE]where deleted-interpolation smoothing was used for the
It is worth noting that the ap-proximated Kneser-Ney smoothed models could not improve the PPL after one iteration of EM training.
"One possible reason is that in order to apply Kneser-Ney smoothing to fractional counts, we had to ap-proximate the discounting."
The approximation may degrade the benefit we could have gotten from the EM training.
"Similarly, the M step in the EM proce-dure for the neural network models also has the same problem: the stochastic gradient descent algorithm is not guaranteed to converge."
This can be clearly seen in Figure 4 in which we plot the learning curves of the 3H-1OP model (PREDICTOR component) on both training and heldout data at EM iteration 0 and iteration 1.
"For EM iteration 0, because we started from parameters drawn from a uniform distribution, we only plot the last 30 iterations of the stochastic gradient descent."
"As we expected, the learning curve of the train-ing data in EM iteration 1 is not as smooth as that in EM iteration 0, and even more so for the heldout data."
"However, the general trend is still decreasing."
"Although we can not prove that the EM training of the neural network models via the SLM can improve the PPL, we observed experimentally a gain that is favorable comparing to that from the usual Kneser-Ney smoothed models or deleted interpolation mod-els."
"By using connectionist models in the SLM, we achieved significant improvement in PPL over the baseline trigram and SLM."
The neural network en-henced SLM resulted in a language model that is much less correlated with the baseline Kneser-Ney smoothed trigram than the Kneser-Ney smoothed
"Overall, the best studied model gave a 21% relative reduction in PPL over the trigram and 8.7% relative reduction over the corresponding Kneser-Ney smoothed SLM."
A new EM training procedure improved the performance of the SLM even further when applied to the neural network models.
"However, reduction in PPL for a language model does not always mean improvement in performance of a real application such as speech recognition."
"Therefore, future study on applying the neural net-work enhenced SLM to real applications needs to be carried out."
A preliminary study[REF_CITE]already showed that this approach is promis-ing in reducing the word error rate of a large vocab-ulary speech recognizer.
There are still many interesting problems in ap-plying the neural network enhenced SLM to real ap-plications.
"Among those, we think the following are of most of interest:"
"Speeding up the stochastic gradient descent algorithm for neural network training: Since training the neural network models is very time-consuming, it is essential to speed up the training in order to carry out many more inter-esting experiments."
"Interpreting the word representations learned in this framework: For example, word clustering, context clustering, etc."
"In particular, if we use separate mapping matrices for word/NT/POS at different positions in the context, we may be able to learn very different representations of the same word/NT/POS."
"Bearing all the challenges in mind, we think the ap-proach presented in this paper is potentially very powerful for using the entire partial parse structure as the conditioning context and for learning useful features automatically from the data."
We present a new framework for classify-ing common nouns that extends named-entity classification.
"We used a fixed set of 26 semantic labels, which we called su-persenses."
These are the labels used by lexicographers developing WordNet.
This framework has a number of practical ad-vantages.
We show how information con-tained in the dictionary can be used as ad-ditional training data that improves accu-racy in learning new nouns.
We also de-fine a more realistic evaluation procedure than cross-validation.
"Lexical semantic information is useful in many nat-ural language processing and information retrieval applications, particularly tasks that require com-plex inferences involving world knowledge, such as question answering or the identification of co-referential entities[REF_CITE]."
"However, even large lexical databases such as WordNet[REF_CITE]do not include all of the words encountered in broad-coverage NLP ap-plications."
"Ideally, we would like a system that automatically extends existing lexical resources by identifying the syntactic and semantic properties of unknown words."
"In terms of the WordNet lexical database, one would like to automatically assign un-known words a position in the synset hierarchy, in-troducing new synsets and extending the synset hier-archy where appropriate."
"Doing this accurately is a difficult problem, and in this paper we address a sim-pler problem: automatically determining the broad semantic class, or supersense, to which unknown words belong."
"Systems for thesaurus extensi[REF_CITE], information extrac-ti[REF_CITE]or named-entity recog-niti[REF_CITE]each partially ad-dress this problem in different ways."
"The goal in these tasks is automatically tagging words with semantic labels such as “vehicle”, “organization”, “person”, etc."
In this paper we extend the named-entity recogni-tion approach to the classification of common nouns into 26 different supersenses.
"Rather than define these ourselves, we adopted the 26 “lexicographer class” labels used in WordNet, which include labels such as person, location, event, quantity, etc."
We be-lieve our general approach should generalize to other definitions of supersenses.
Using the WordNet lexicographer classes as su-persenses has a number of practical advantages.
"First, we show how information contained in the dic-tionary can be used as additional training data that improves the system’s accuracy."
"Secondly, it is pos-sible to use a very natural evaluation procedure."
"A system can be trained on an earlier release of Word-Net and tested on the words added in a later release, since these labels are constant across different re-leases."
"This new evaluation defines a realistic lexi-cal acquisition task which is well defined, well mo-tivated and easily standardizable."
The heart of our system is a multiclass perceptron classifier[REF_CITE].
"The features used are the standard ones used in word-sense classi-fication and named-entity extraction tasks, i.e., col-location, spelling and syntactic context features."
The experiments presented below show that when the classifier also uses the data contained in the dic-tionary its accuracy improves over that of a tradition-ally trained classifier.
"Finally, we show that there are both similarities and differences in the results ob-tained with the new evaluation and standard cross-validation."
This might suggest that in fact that the new evaluation defines a more realistic task.
The paper is organized as follows.
In Section 2 we discuss the problem of unknown words and the task of semantic classification.
"In Section 3 we de-scribe the WordNet lexicographer classes, how to extract training data from WordNet, the new evalu-ation method and the relation of this task to named-entity classification."
"In Section 4 we describe the experimental setup, and in Section 5 we explain the averaged perceptron classifier used."
In Section 6 and 7 we discuss the results and the two evaluations.
"Language processing systems make use of “dictio-naries”, i.e., lists that associate words with useful information such as the word’s frequency or syn-tactic category."
"In tasks that also involve inferences about world knowledge, it is useful to know some-thing about the meaning of the word."
"This lexical semantic information is often modeled on what is found in normal dictionaries, e.g., that “irises” are flowers or that “exane” is a solvent."
"This information can be crucial in tasks such as question answering - e.g., to answer a ques-tion such as “What kind of flowers did Van Gogh paint?”[REF_CITE]- or the indi-viduation of co-referential expressions, as in the pas- ... thissage “...  the  preruncan canbe consideredbe performed...” (withPustejovsky  et al., 2002)."
Lexical semantic information can be extracted from existing dictionaries such as WordNet.
"How-ever, these resources are incomplete and systems that rely on them often encounter unknown words, even if the dictionary is large."
"As an example, in the Bllip corpus (a very large corpus of Wall Street Jour-nal text) the relative frequency of common nouns that are unknown to WordNet 1.6 is approximately 0.0054; an unknown noun occurs, on average, ev-ery eight sentences."
"WordNet 1.6 lists 95,000 noun types."
"For this reason the importance of issues such as automatically building, extending or customizing lexical resources has been recognized for some time in computational linguistics[REF_CITE]."
"Solutions to this problem were first proposed in AI in the context of story understanding, cf.[REF_CITE]."
The goal is to label words using a set of semantic labels specified by the dictionary.
"Several studies have addressed the problem of ex-panding one semantic category at a time, such as “vehicle” or “organization”, that are relevant to a particular task[REF_CITE]."
"In named-entity clas-sification a large set of named entities (proper nouns) are classified using a comprehensive set of semantic labels such as “organization”, “person”, “location” or “other”[REF_CITE]."
This latter approach assigns all named entities in the data set a semantic label.
We extend this approach to the clas-sification of common nouns using a suitable set of semantic classes.
WordNet[REF_CITE]is a broad-coverage machine-readable dictionary.
"Release 1.71 of the English version lists about 150,000 entries for all open-class words, mostly nouns (109,000 types), but also verbs, adjectives, and adverbs."
"WordNet is or-ganized as a network of lexicalized concepts, sets of synonyms called synsets; e.g., the nouns chairman, chairwoman, chair, chairperson form a synset."
A word that belongs to several synsets is ambiguous.
"To facilitate the development of WordNet, lexi-cographers organize synsets into several domains, based on syntactic category and semantic coherence."
Each noun synset is assigned one out of 26 broad categories [Footnote_1] .
"1 There are also 15 lexicographer classes for verbs, 3 for ad-jectives and 1 for adverbs."
"Since these broad categories group to-gether very many synsets, i.e., word senses, we call them supersenses."
The supersense labels that Word-Net lexicographers use to organize nouns are listed in Table 1 [Footnote_2] .
"2 The label “Tops” refers to about 40 very general synsets, such as “phenomenon” “entity” “object” etc."
"Notice that since the lexicographer la-bels are assigned to synsets, often ambiguity is pre-served even at this level."
"For example, chair has three supersenses: “person”, “artifact”, and “act”."
This set of labels has a number of attractive fea-tures for the purposes of lexical acquisition.
It is fairly general and therefore small.
The reasonable size of the label set makes it possible to apply state-of-the-art machine learning methods.
"Otherwise, classifying new words at the synset level defines a multiclass problem with a huge class space - more than 66,000 noun synsets in WordNet 1.6, more than 75,000 in the newest release, 1.71 (cf. also[REF_CITE]on this problem)."
At the same time the labels are not too abstract or vague.
Most of the classes seem natural and easily recognizable.
That is probably why they were chosen by the lexicog-raphers to facilitate their task.
But there are more important practical and methodological advantages.
WordNet contains a great deal of information about words and word senses.
"The information contained in the dictionary’s glosses is very similar to what is typically listed in normal dictionaries: synonyms, definitions and example sentences."
This suggests a very simple way in which it can be put into use: it can be compiled into training data for supersense la-bels.
This data can then be added to the data ex-tracted from the training corpus.
For several thousand concepts WordNet’s glosses are very informative.
"The synset “chair” for example looks as follows:   : president, chairman, chairwoman, chair, chairperson – (the officer who presides at the meetings of an organization); “address your remarks to the chairperson”."
"In WordNet 1.6, 66,841 synsets contain definitions (in parentheses above), and 6,147 synsets contain example sentences (in quotation marks)."
"As we show below, this information about word senses is useful for supersense tagging."
"Presumably this is be-cause if it can be said of a “chairperson” that she can “preside at meetings” or that “a remark” can be “ad-dressed to her”, then logically speaking these things can be said of the superordinates of “chairperson”, like “person”, as well."
Therefore information at the synset level is rele-vant also at the supersense level.
"Furthermore, while individually each gloss doesn’t say too much about the narrow concept it is attached to (at least from a machine learning perspective) at the supersense level this information accumulates."
In fact it forms a small corpus of supersense-annotated data that can be used to train a classifier for supersense tagging of words or for other semantic classification tasks.
Formulating the problem in this fashion makes it possible to define also a very natural evaluation pro-cedure.
Systems can be trained on nouns listed in a given release of WordNet and tested on the nouns introduced in a later version.
The set of lexicogra-pher labels remains constant and can be used across different versions.
In this way systems can be tested on a more real-istic lexical acquisition task - the same task that lex-icographers carried out to extend the database.
"The task is then well defined and motivated, and easily standardizable."
"The categories typically used in named-entity recog-nition tasks are a subset of the noun supersense la-bels: “person”, “location”, and “group”."
Small la-bel sets like these can be sufficient in named-entity recognition.
"The distribution of common nouns, however, is more uniform."
We estimated this distribution by counting the occurrences of 744 unambiguous com-mon nouns newly introduced in WordNet 1.71.
Fig-ure 1 plots the cumulative frequency distribution of supersense tokens; the labels are ordered by decreas-ing relative frequency as in Table 1.
"The most frequent supersenses are “person”, “communication”, “artifact” etc."
"The three most fre-quent supersenses account for a little more of 50% of all tokens, and 9 supersenses account for 90% of all tokens."
A larger number of labels is needed for supersense tagging than for named-entity recogni-tion.
The figure also shows the distribution of labels for all unambiguous tokens in WordNet 1.6; the two distributions are quite similar.
The “new” nouns in WordNet 1.71 and the “old” ones in WordNet 1.6 constitute the test and training data that we used in our word classification exper- iments.
"Here we describe the experimental setup: training and test data, and features used."
We extracted from the Bllip corpus all occur-rences of nouns that have an entry in WordNet 1.6.
Bllip[REF_CITE]is a 40-million-word syntac-tically parsed corpus.
We used the parses to ex-tract the syntactic features described below.
"We then removed all ambiguous nouns, i.e., nouns that are tagged with more than one supersense label (72% of the tokens, 28.9% of the types)."
In this way we avoided dealing with the problem of ambiguity 3 .
We extracted a feature vector for each noun in-stance using the feature set described below.
Each vector is a training instance.
In addition we com-piled another training set from the example sen-tences and from the definitions in the noun database of WordNet 1.6.
"Overall this procedure produced 787,186 training instances[REF_CITE]841 train-ing instances from WordNet’s definitions, and 6,147 training instances from the example sentences."
"We used a mix of standard features used in word sense disambiguation, named-entity classification and lexical acquisition."
"The following sentence il-lustrates them: “The art-students, nine teen-agers, read the book”, art-students is the tagged noun: 1. part of speech of the neighboring words: !#%&quot; &apos;$ &amp;!( , *) ,$ +!+ , . &quot; $01/ &amp; , ... 2. single words in the surrounding context: 2/ $ , / $!;9 : , /2=$ &lt;?&gt;!!@ @ , /2$&apos;A? , ... 2 [Footnote_3]. bigrams and trigrams: / #&quot; . &quot; &apos;A$ ?  , / #&quot; G&quot; $&apos;A? , / . &quot;.#HI,$ ; !?@ , ... 4. N syntactically &quot; &apos;A$ ? +! governed elements under a given phrase: 5. syntactically N H $ !576 - governing elements under a given phrase: 6. coordinates/appositives: !/"
"3 A simple option to deal with ambiguous words would be to distribute an ambiguous noun’s counts to all its senses. How-ever, in preliminary experiments we found that a better accuracy is achieved using only non-ambiguous nouns. We will investi-gate this issue in future research."
"OP$?@ 7. spelling/morphological features: prefixes, suffixes, com-plex morphology: R!05$ , !R $0573 ..."
"RQ-2$=@ , $0AQ@ ... P/ 0573$ !"
"A , P/ $=@1A?U! ..."
Open class words were morphologically simpli-fied with the “morph” function included in Word-Net.
We parsed the WordNet definitions and exam-ple sentences with the same syntactic parser used for Bllip[REF_CITE].
It is not always possible to identify the noun that represents the synset in the WordNet glosses.
"For example, in the gloss for the synset relegation the example sentence is “He has been relegated to a post in Siberia”, where a verb is used instead of the noun."
When it was possible to identify the target noun the complete feature set was used; otherwise only the surrounding-word features (2) and the spelling fea-tures (7) of all synonyms were used.
With the def-initions it is much harder to individuate the target; consider the definition “a member of the genus Ca-nis” for dog.
For all definitions we used only the reduced feature set.
One training instance per synset was extracted from the example sentences and one training instance from the definitions.
"Overall, in the experiments we performed we used around 1.5 million features."
In a similar way to how we produced the training data we compiled a test set from the Bllip corpus.
We found all instances of nouns that are not in Word-Net 1.6 but are listed in WordNet 1.71 with only one supersense.
The majority of the novel nouns in WordNet 1.71 are unambiguous (more than 90%).
We refer to this test set as Test VFW X7V .
"We also randomly removed 755 noun types (20,394 tokens) from the training data and used them as an alternative test set."
We refer to this other test set as Test VFWY .
We then ran experiments using the averaged multiclass perceptron.
"We used a multiclass averaged perceptron classifier, which is an “ultraconservative” on-line learning al-gorithm[REF_CITE], that is a mul-ticlass extension of the standard perceptron learning to the multiclass case."
"It takes as input a training set Z \^ [ ] `_FaGcbedgf V , where each instance ihkj lnm rep-resents an instance of a noun and \q ."
Here q is the set of supersenses defined by WordNet.
Since for training and testing we used only unambiguous words Z there is always exactly one label per instance.
"Thus summarizes word tokens that belong to the dictionary, where each instance is represented as a vector of features extracted from the context in which the noun occurred;  is the total number of features; and a is the true label of  ."
"In general, a multiclass classifier for the dictio-nary is a function y |j l d q that maps fea-ture vectors to one of the possible supersenses of WordNet."
"In the multiclass } perceptron, one intro-duces a weight vector hpj l m for every a&apos;h0q and defines y implicitly by the so-called winner-take-all rule y ] z s b [u;S}Q~~   } _  (1)"
"Here s hj ln# m refers to the matrix of weights, with every column } corresponding to one of the weight vectors ."
The learning algorithm works as follows: Train-ing patterns are presented one at a time in the ] standard [ on-line learning setting.
Whenever y `z s b\{ aG an update step is performed; oth-erwise the weight vectors remain unchanged.
"To perform the update, one first computes the error set  containing those class labels that have received a higher score than the correct class: [  a&apos;h0q  } _ ¡ ~}; _ c (2)"
"An ultraconservative update scheme in its most }0 gen-eral form is then defined as follows: Update =} £¢!} ¢;;}  with [ v learning ¥} }; ¢! rates } [ fulfilling  v the ¢7} ¦t[ con-straints , ¤ f , and for a h{  ¨§ aG ."
Hence changes are limited to } for a©h  § aG .
"The sum constraint ensures that the update is balanced, which is crucial to guar-anteeing the convergence of the learning procedure (cf.[REF_CITE])."
"We have focused on the simplest case of uniform update weights, ¢ !} [   V   for a©h  ."
The algorithm is summa-rized in Algorithm 1.
"Notice that the multiclass perceptron algorithm learns all weight vectors in a coupled manner, in contrast to methods that perform multiclass classifi-cation by combining binary classifiers, for example, training a classifier for each class in a one-against-the-rest manner."
"The averaged version of the perceptr[REF_CITE], like the voted perceptr[REF_CITE], reduces the effect of over-training."
"In addition to the matrix of weight vectors s the model keeps track for each feature ª of each value it assumed during training, ª o , and the number of consecutive training instance presentations during which ] ª  .thisWhenweighttrainingwasisnotdonechangedthese,weightsor “lifearespanav-”, eraged and the final averaged weight ªr«1¬;­ of feature ª is computed as ] ª «1¬;­ [ ¤ o  ] ª o b (3) ¤ o ª oQb"
"For example, if there is a feature weight that is not updated until example 500, at which point it is incremented to value 1, and is not touched again until after example 1000, then the average weight of that feature in the averaged perceptron at ex-ample 750 will be: ®°¯;±r²F¯F¯;³ V ±r´F²F¯;µ , or 1/3."
"At ex-ample 1000 it will be ®° 1 ²F¯F /2 ¯; , ³*´F etc ²F¯;µ ."
We used the av-eraged model for evaluation and parameter setting; see below.
Figure 2 plots the results on test data of both models.
The average model produces a better-performing and smoother output.
"We used an implementation with full, i.e., not sparse, representation of the matrix for the percep-tron."
"Training and test are fast, at the expense of a slightly greater memory load."
"Given the great num-ber of features, we couldn’t use the full training set from the Bllip corpus."
"Instead we randomly sam-pled from roughly half of the available training data, yielding around 400,000 instances, the size of the training is close to 500,000 instances with also the WordNet data."
When training to test on Test VF¶
"Y , we removed from the WordNet training set the synsets relative to the nouns in Test VFW Y ."
"The only adjustable parameter to set is the number of passes on the training data, or epochs."
"While test-ing on Test VFW X7V we set this parameter using Test VFW Y , and vice versa for Test VFW Y ."
The estimated values for the stopping iterations were very close at roughly ten passes.
"As Figure 2 shows, the great amount of data requires many passes over the data, around 1,000, before reaching convergence (on Test VFW X7V )."
The classifier outputs the estimated supersense label of each ] instance of each unknown noun type.
The label · b of a noun type is obtained by voting [Footnote_4] : · ] b ¸1K[ } ~ ¼ y ] z s b [ a¾½ (4) »  d ¼ where x¿½ is the indicator function and h means that is a token of type .
4 During preliminary experiments we tried also creating one single aggregate pattern for each test noun type but this method produced worse results.
"The score on is 1 if · ] b [ q ] b , where q ] b is the correct label for , and 0 otherwise."
Table 2 summarizes the results of the experiments on Test VFW X7V (upper half) and on Test VFW Y (bottom half).
"A baseline was computed that always selected the most frequent label in the training data, “person”, which is also the most frequent in both Test VFW Y and Test VFW X7V ."
The baseline performances are in the low twenties.
The first and second columns report per-formance on tokens and types respectively.
The classifiers’ results are averages over 50 trials in which a fraction of the Bllip data was randomly selected.
One classifier was trained on 55% of the Bllip data (AP-B-55).
"An identical one was trained on the same data and, additionally, on the WordNet data (AP-B-55+WN)."
We also trained a classifier on 65% of the Bliip data (AP-B-65).
Adding the Word-Net data to this training set was not possible because of memory limitations.
The model also trained on WordNet outperforms on both test sets those trained only on the Bllip data.
A paired t-test proved the difference between models with and without Word-Net data to be statistically significant.
The “least” significant difference is between AP-B-65 Át[ tGt#Â and AP-B-55+WN (token) on Test VFWY :
À x .
In all other cases the À -level is much smaller.
These results seem to show that the positive im-pact of the WordNet data is not simply due to the fact that there is more training data [Footnote_5] .
5[REF_CITE]% of the Bllip data is approximately the size of the WordNet data and therefore AP-B-65 and AP-B-55+WN are trained on roughly the same amount of data.
Adding the WordNet data seems more effective than adding an equivalent amount of standard training data.
"Fig-ure 3 plots the results of the last set of (single trial) experiments we performed, in which we varied the amount of Bllip data to be added to the WordNet one."
The model with WordNet data often performs better than the model trained only on Bllip data even when the latter training set is much larger.
"Two important reasons why the WordNet data is particularly good are, in our opinion, the following."
The data is less noisy because it is extracted from sentences and definitions that are always “pertinent” to the class label.
"The data also contains instances of disambiguated polysemous nouns, which instead were excluded from the Bllip training."
This means that disambiguating the training data is important; unfortunately this is not a trivial task.
Using the WordNet data provides a simple way of getting at least some information from ambiguous nouns.
The type scores on both evaluations produced simi-lar results.
"This finding supports the hypothesis that the two evaluations are similar in difficulty, and that the two versions of WordNet are not inconsistent in the way they assign supersenses to nouns."
"The evaluations show, however, very different patterns at the token level."
This might be due to the fact that the label distribution of the training data is more similar to Test VFW Y than to Test VFW X7V .
"In particular, there are many new nouns in Test VFW X7V that belong to “abstract” classes [Footnote_6] , which seem harder to learn."
"6 Such as “communication” (e.g., reaffirmation) or “cogni-tion” (e.g., mind set)."
"Ab-stract classes are also more confusable; i.e., mem- bers of these classes are frequently mis-classified with the same wrong label."
"A few very frequently mis-classified pairs are communication/act, commu-nication/person and communication/artifact."
As a result of the fact that abstract nouns are more frequent in Test VFW X7V than in Test VFW Y the accuracy on tokens is much worse in the new evaluation than in the more standard one.
This has an impact also on the type scores.
Figure 4 plots the results on types for Test VFW Y and Test VFW X7V grouped in bins of test noun types ranked by decreasing frequency.
It shows that the first bin is harder in Test VFW X7V than in Test VFW Y .
"Overall, then, it seems that there are similarities but also important differences between the evalua-tions."
Therefore the new evaluation might define a more realistic task than cross-validation.
"We presented a new framework for word sense classification, based on the WordNet lexicographer classes, that extends named-entity classification."
Within this framework it is possible to use the in-formation contained in WordNet to improve classi-fication and define a more realistic evaluation than standard cross-validation.
"Directions for future re-search include the following topics: disambiguation of the training data, e.g. during training as in co-training; learning unknown ambiguous nouns, e.g., studying the distribution of the labels the classifier guessed for the individual tokens of the new word."
We present a machine learning frame-work for resolving other-anaphora.
"Be-sides morpho-syntactic, recency, and se-mantic features based on existing lexi-cal knowledge resources, our algorithm obtains additional semantic knowledge from the Web."
We search the Web via lexico-syntactic patterns that are specific to other-anaphors.
Incorporating this in-novative feature leads to an 11.4 percent-age point improvement in the classifier’s F -measure (25% improvement relative to results without this feature).
"Other-anaphors are referential NPs with the mod-ifiers “other” or “another” and non-structural an-tecedents: [Footnote_1] (1) An exhibition of American design and architec-ture opened in September in Moscow and will travel to eight other Soviet cities. ([Footnote_2]) [. . . ] the alumni director of a Big Ten university “I’d love to see sports cut back and so would a lot of my counterparts at other schools, [. . . ]” (3) You either believe Seymour can do it again or you don’t."
1 All examples are from the Wall Street Journal; the correct antecedents are in italics and the anaphors are in bold font.
"2 Antecedents are also available structurally in constructions “other than”, e.g., “few clients other than the state”. For a com-putational treatment of “other” with structural antecedents see[REF_CITE]."
"Beside the designer’s age, other risk factors for Mr. Cray’s company include the Cray-3’s [. . . ] chip technology."
"In (1), “eight other Soviet cities” refers to a set of So-viet cities excluding Moscow, and can be rephrased as “eight Soviet cities other than Moscow”."
"In ([Footnote_2]), “other schools” refers to a set of schools excluding the mentioned Big Ten university."
"2 Antecedents are also available structurally in constructions “other than”, e.g., “few clients other than the state”. For a com-putational treatment of “other” with structural antecedents see[REF_CITE]."
"In (3), “other risk factors for Mr. Cray’s company” refers to a set of risk factors excluding the designer’s age."
"In contrast, in list-contexts such as (4), the an-tecedent is available both anaphorically and struc-turally, as the left conjunct of the anaphor. 2 (4) Research shows AZT can relieve dementia and other symptoms in children [. . . ]"
We focus on cases such as (1–3).
Section 2 describes a corpus of other-anaphors.
"We present a machine learning approach to other-anaphora, using a Naive Bayes (NB) classifier (Sec-tion 3) with two different feature sets."
"In Section 4 we present the first feature set (F1) that includes standard morpho-syntactic, recency, and string com-parison features."
"However, there is evidence that, e.g., syntactic features play a smaller role in resolv-ing anaphors with full lexical heads than in pronom-inal anaphora[REF_CITE]."
"In-stead, a large and diverse amount of lexical or world knowledge is necessary to understand exam-ples such as (1–3), e.g., that Moscow is a (Soviet) city, that universities are informally called schools in American English and that age can be viewed as a risk factor."
"Therefore we add lexical knowledge, which is extracted from WordNet[REF_CITE]and from a Named Entity (NE) Recognition algo-rithm, to F1."
The algorithm’s performance with this feature set is encouraging.
"However, the semantic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2)."
"Many expres-sions, word senses and lexical relations are miss-ing from WordNet."
"Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor."
There have been efforts to extract missing lexical relations from corpora in order to build new knowl-edge sources and enrich existing ones[REF_CITE]. [Footnote_3]
"3 In parallel, efforts have been made to enrich WordNet by adding information in glosses[REF_CITE]."
"However, the size of the used corpora still leads to data sparseness[REF_CITE]and the extraction procedure can therefore require extensive smoothing."
"Moreover, some relations should probably not be encoded in fixed context-independent ontologies at all."
"Should, e.g., under-specified and point-of-view dependent hyponymy relations[REF_CITE]be included?"
"Should age, for example, be classified as a hyponym of risk fac-tor independent of context?"
"Building on our previous work[REF_CITE], we instead claim that the Web can be used as a huge additional source of domain- and context-independent, rich and up-to-date knowledge, with-out having to build a fixed lexical knowledge base (Section 5)."
We describe the benefit of integrating Web frequency counts obtained for lexico-syntactic patterns specific to other-anaphora as an additional feature into our NB algorithm.
This feature raises the algorithm’s F -measure from 45.5% to 56.9%.
"This data sample excludes several types of expressions containing “other”: (a) list-contexts (Ex. 4) and other-than contexts (foot-note 2), in which the antecedents are available struc-turally and thus a relatively unsophisticated proce-dure would suffice to find them; (b) idiomatic and discourse connective “other”, e.g., “on the other hand”, which are not anaphoric; and (c) reciprocal “each other” and “one another”, elliptic phrases e.g. “one X . . . the other(s)” and one-anaphora, e.g., “the other/another one”, which behave like pronouns and thus would require a different search method."
"Also excluded from the data set are samples of other-anaphors with non-NP antecedents (e.g., adjectival and nominal pre- and postmodifiers and clauses)."
Each anaphor was extracted in a 5-sentence con-text.
The correct antecedents were manually an-notated to create a training/test corpus.
"For each anaphor, we automatically extracted a set of po-tential NP antecedents as follows."
"First, we ex-tracted all base NPs, i.e., NPs that contain no further NPs within them."
"NPs containing a possessive NP modifier, e.g., “Spain’s economy”, were split into a possessor phrase, “Spain”, and a possessed entity, “economy”."
We then filtered out null elements and lemmatised all antecedents and anaphors.
"We use a Naive Bayes classifier, specifically the im-plementation in the Weka ML library. [Footnote_4]"
"4[URL_CITE]We also experimented with a decision tree classifier, with Neural Networks and Support Vector Machines with Sequential Minimal Optimization (SMO), all available from Weka. These classifiers achieved worse results than NB on our data set."
The training data was generated following the procedure employed[REF_CITE]for coreference resolution.
Every pair of an anaphor and its closest preceding antecedent created a pos-itive training instance.
"To generate negative train-ing instances, we paired anaphors with each of the NPs that intervene between the anaphor and its an-tecedent."
"This procedure produced a set of 3,084 antecedent-anaphor pairs, of which 500 (16%) were positive training instances."
The classifier was trained and tested using 10-fold cross-validation.
"We follow the general practice of ML algorithms for coreference resolution and com-pute precision (P), recall (R), and F-measure ( F ) on all possible anaphor-antecedent pairs."
"As a first approximation of the difficulty of our task, we developed a simple rule-based baseline al-gorithm which takes into account the fact that the lemmatised head of an other-anaphor is sometimes the same as that of its antecedent, as in (5). compatible, incompatible, unknown (5) These three countries aren’t completely off the hook, though."
They will remain on a lower-priority list that includes other countries [. . . ]
"For each anaphor, the baseline string-compares its last (lemmatised) word with the last (lemmatised) word of each of its possible antecedents."
"If the words match, the corresponding antecedent is cho-sen as the correct one."
"If several antecedents pro-duce a match, the baseline chooses the most re-cent one among them."
"If string-comparison returns no antecedent, the baseline chooses the antecedent closest to the anaphor among all antecedents."
The baseline assigns “yes” to exactly one antecedent per anaphor.
"Its P, R and F -measure are 27.8%."
"First, we trained and tested the NB classifier with a set of 9 features motivated by our own work on other-anaphora[REF_CITE]and previous ML research on coreference resoluti[REF_CITE]."
"A set of 9 features, F1, was automatically acquired from the corpus and from additional external re-sources (see summary in Table 1)."
NP FORM is based on the POS tags in the Wall Street Journal corpus and heuristics.
RESTR SUBSTR matches lemmatised strings and checks whether the antecedent string contains the anaphor string.
This allows to resolve examples such as “one woman ringer ... another woman”.
The values for GRAM FUNC were approxi-mated from the parse trees and Penn Treebank anno-tation.
The feature SYN PAR captures syntactic par-allelism between anaphor and antecedent.
The fea-ture SDIST measures the distance between anaphor and antecedent in terms of sentences. [Footnote_5] Semantic features.
5 We also experimented with a feature MDIST that measures intervening NP units. This feature worsened the overall perfor-mance of the classifier.
"GENDER AGR captures agree-ment in gender between anaphor and antecedent, gender having been determined using gazetteers, kinship and occupational terms, titles, and Word-Net."
"Four values are possible: “same”, if both NPs have same gender; “compatible”, if antecedent and anaphor have compatible gender, e.g., “lawyer ... other women”; “incompatible”, e.g., “Mr. Johnson ... other women”; and “unknown”, if one of the NPs is undifferentiated, i.e., the gender value is “un-known”."
"SEMCLASS : Proper names were classified using ANNIE, part of the GATE2 software package[URL_CITE]"
"Common nouns were looked up in WordNet, considering only the most frequent sense of each noun (the first sense in Word-Net)."
"In each case, the output was mapped onto one of the values in Table 1."
"The SEMCLASS AGR fea- ture compares the semantic class of the antecedent with that of the anaphor NP and returns “yes” if they belong to the same class; “no”, if they belong to different classes; and “unknown” if the seman-tic class of either the anaphor or antecedent has not been determined."
The RELATION between other-anaphors and their antecedents can partially be de-termined by string comparison (“same-predicate”) [Footnote_6] or WordNet (“hypernymy” and “meronymy”).
6 Same-predicate is not really a relation. We use it when the head noun of the anaphor and antecedent are the same.
"As other relations, e.g. “redescription” (Ex. (3), cannot be readily determined on the basis of the information in WordNet, the following values were used: “com-patible”, for NPs with compatible semantic classes, e.g., “woman ... other leaders”; and “incompati-ble”, e.g., “woman ... other economic indicators”."
Compatibility can be defined along a variety of pa-rameters.
The notion we used roughly corresponds to the root level of the WordNet hierarchy.
"Two nouns are compatible if they have the same SEM - CLASS value, e.g., “person”. “Unknown” was used if the type of relation could not be determined."
"Our algorithm performs significantly better than the baseline. [Footnote_7] While these results are encouraging, there were several classification errors."
7 We used a t-test with confidence level 0.05 for all signifi-cance tests.
Word sense ambiguity is one of the reasons for misclassifications.
Antecedents were looked up in WordNet for their most frequent sense for a context-independent assignment of the values of semantic class and relations.
"However, in many cases either the anaphor or antecedent or both are used in a sense that is ranked as less frequent in Wordnet."
"This might even be a quite frequent sense for a specific corpus, e.g., the word “issue” in the sense of “shares, stocks” in the WSJ."
Therefore there is a strong inter- action between word sense disambiguation and ref-erence resolution (see also[REF_CITE]).
Named Entity resolution is another weak link.
Several correct NE antecedents were classified as “antecedent = no” (false negatives) because the NER module assigned the wrong class to them.
The largest class of errors is however due to insuf-ficient semantic knowledge.
"Problem examples can roughly be classified into five partially overlapping groups: (a) examples that suffer from gaps in Word-Net, e.g., (2); (b) examples that require domain-, situation-specific, or general world knowledge, e.g., (3); (c) examples involving bridging phenomena (sometimes triggered by a metonymic or metaphoric antecedent or anaphor), e.g., (6); (d) redescriptions and paraphrases, often involving semantically vague anaphors and/or antecedents, e.g., (7) and (3); and (e) examples with ellipsis, e.g., (8). (6) The Justice Department’s view is shared by other lawyers [. . . ] (7) While Mr. Dallara and Japanese officials say the question of investors’ access to the U.S. and Japanese markets may get a disproportion-ate share of the public’s attention, a number of other important economic issues will be on the table at next week’s talks. (8) He sees flashy sports as the only way the last-place network can cut through the clutter of ca-ble and VCRs, grab millions of new viewers and tell them about other shows premiering a few weeks later."
"In (6), the antecedent is an organization-for-people metonymy."
"In (7), the question of investors’ access to the U.S. and Japanese markets is characterized as an important economic issue."
"Also, the head “is-sues” is lexically uninformative to sufficiently con-strain the search space for the antecedent."
"In (8), the antecedent is not the flashy sports, but rather flashy sport shows, and thus an important piece of infor-mation is omitted."
"Alternatively, the antecedent is a content-for-container metonymy."
"Overall, our approach misclassifies antecedents whose relation to the other-anaphor is based on sim-ilarity, property-sharing, causality, or is constrained to a specific domain."
These relation types are not — and perhaps should not be — encoded in WordNet.
With its approximately 3033M pages [Footnote_8] the Web is the largest corpus available to the NLP community.
"Building on our approach[REF_CITE], we suggest using the Web as a knowledge source for anaphora resolution."
"In this paper, we show how to integrate Web counts for lexico-syntactic patterns specific to other-anaphora into our ML approach."
"In the examples we consider, the relation between anaphor and antecedent is implicitly expressed, i.e., anaphor and antecedent do not stand in a structural relationship."
"However, they are linked by a strong semantic relation that is likely to be structurally ex-plicitly expressed in other texts."
We exploit this in-sight by adopting the following procedure: 1.
"In other-anaphora, a hyponymy/similarity rela-tion between the lexical heads of anaphor and antecedent is exploited or stipulated by the con-text, [Footnote_9] e.g. that “schools“ is an alternative term for universities in Ex. (2) or that age is viewed as a risk factor in Ex. (3). 2."
"9 In the Web feature context, we will often use “anaphor/antecedent” instead of the more cumbersome “lexical heads of the anaphor/antecedent”."
We select patterns that structurally explicitly express the same lexical relations.
"E.g., the list-context NP 1 and other NP 2 (as Ex. (4)) usually expresses hyponymy/similarity rela-tions between the hyponym NP 1 and its hyper-nym NP 2[REF_CITE]. 3."
"If the implicit lexical relationship between anaphor and antecedent is strong, it is likely that anaphor and antecedent also frequently cooccur in the selected explicit patterns."
We instantiate the explicit pattern for all anaphor-antecedent pairs.
"In (2) the pattern NP 1 and other NP 2 is instantiated with e.g., counterparts and other schools , sports and other schools and universities and other schools . 10 These instantiations can be searched in any corpus to determine their fre-quencies."
The rationale is that the most fre-quent of these instantiated patterns is a good clue for the correct antecedent. 4.
"As the patterns can be quite elaborate, most corpora will be too small to determine the cor-responding frequencies reliably."
"The instantia-tion universities and other schools , e.g., does not occur at all in the British National Cor-pus (BNC), a 100M words corpus of British English. [URL_CITE]"
"Therefore we use the largest corpus available, the Web."
We submit all instantiated patterns as queries making use of the Google API technology.
"Here, universities and other schools yields over 700 hits, whereas the other two instantiations yield under [Footnote_10] hits each."
10 These simplified instantiations serve as an example and are neither exhaustive nor the final instantiations we use; see Sec-tion 5.3.
High frequencies do not only occur for synonyms; the corresponding instantiation for the correct antecedent in Ex. (3) age and other risk factors yields over 400 hits on the Web and again none in the BNC.
"In addition to the antecedent preparation described in Section 2, further processing is necessary."
"First, pronouns can be antecedents of other-anaphors but they were not used as Web query input as they are lexically empty."
"Second, all modification was elim-inated and only the rightmost noun of compounds was kept, to avoid data sparseness."
"Third, using pat-terns containing NEs such as “Will Quinlan” in (9) also leads to data sparseness (see also the use of NE recognition for feature SEMCLASS ). (9) [. . . ]"
"Will Quinlan had not inherited a damaged retinoblastoma supressor gene and, therefore, faced no more risk than other children [. . . ]"
We resolved NEs in two steps.
"In addition to GATE’s classification into ENAMEX and NU - MEX categories, we used heuristics to automati-cally obtain more fine-grained distinctions for the categories LOCATION , ORGANIZATION , DATE and MONEY , whenever possible."
No further distinc-tions were made for the category PERSON .
"We classified LOCATIONS into COUNTRY , (US) STATE , CITY , RIVER , LAKE and OCEAN , using mainly gazetteers. 12 If an entity classified by GATE as ORGANIZATION contained an indication of the or-ganization type, we used this as a subclassifica-tion; therefore “Bank of America” is classified as BANK ."
"For DATE and MONEY entities we used simple heuristics to classify them further into DAY , MONTH , YEAR as well as DOLLAR ."
From now on we call A the list of possible an-tecedents and ana the anaphor.
"For (2), this list is A 2 = f counterpart, sport, university g (the pronoun “I” has been discarded) and ana 2 =school."
"For (9), they are A 9 = f risk, gene, person [=Will Quinlan] g and ana 9 =child."
We use the list-context pattern: [Footnote_13] (O1)(N 1 f sg g OR N 1 f pl g ) and other N 2 f pl g
"13 In all patterns in this paper, “OR” is the boolean operator, “N 1 ” and “N 2 ” are variables, all other words are constants."
"For common noun antecedents, we instantiate the pattern by substituting N 1 with each possible an-tecedent from set A , and N 2 with ana, as normally N 1 is a hyponym of N 2 in (O1), and the antecedent is a hyponym of the anaphor."
An instantiated pat-tern for Ex. (2) is (university OR universities) and other schools (
I 1 c in Table 3). [Footnote_14]
14 Common noun instantiations are marked by a superscript “c” and proper name instantiations by a superscript “p”.
"For NE antecedents we instantiate (O1) by substi-tuting N 1 with the NE category of the antecedent, and N 2 with ana ."
An instantiated pattern for Example (9) is (person OR persons) and other children ( I 1 p in Table 3).
"In this instantiation, N 1 (“person”) is not a hyponym of N 2 (“child”), instead N 2 is a hyponym of N 1 ."
"This is a consequence of the substitution of the antecedent (“Will Quinlan”) with its NE category (“person”); such an instanti-ation is not frequent, since it violates standard re-lations within (O1)."
"Therefore, we also instantiate (O1) by substituting N 1 with ana , and N 2 with the NE type of the antecedent ( I 2 p in Table 3)."
"Finally, for NE antecedents, we use an additional pattern: (O2)N 1 and other"
N 2 f pl g which we instantiate by substituting N 1 with the original NE antecedent and N 2 with ana ( I 3 p in Ta-ble 3).
Patterns and instantiations are summarised in Ta-ble 3.
We submit these instantiations as queries to the Google search engine.
"For each antecedent ant in A we obtain the raw frequencies of all instantiations it occurs in ( I 1 c for common nouns, or I 1 p , I 2 p , I 3 p for proper names) from the Web, yielding freq ( I 1 c ) , or freq ( I p 1) , freq ( I 2 p ) and freq ( I 3 p ) ."
We compute the maximum M ant over these frequencies for proper names.
For com-mon nouns M ant corresponds to freq ( I 1 c ) .
The in-stantiation yielding M ant is then called Imax ant .
Our scoring method takes into account the indi-vidual frequencies of ant and ana by adapting mu-tual information.
"We call the first part of Imax ant (e.g. “university OR universities”, or “child OR chil-dren”) X ant , and the second part (e.g. “schools” or “persons”) Y ant ."
"We compute the probability of Imax ant , X ant and Y ant , using Google to determine freq ( X ant ) and freq ( Y ant ) ."
M ant P r ( Imax ant ) = number of GOOGLE pages freq ( X ant )
P r ( X ant ) = number of GOOGLE pages freq ( Y ant )
P r ( Y ant ) = number of GOOGLE pages
We then compute the final score MI ant .
MI ant = log P rP ( Xr ( ant Imax )
P r ant ( Y ant ) )
"Results For each anaphor, the antecedent in A with the highest MI ant gets feature value “webfirst”. [Footnote_15] All other antecedents (including pronouns) get the fea-ture value “webrest”."
15 If several antecedents have the highest MIant they all get value “webfirst”.
"We chose this method instead of e.g., giving score intervals for two reasons."
"First, since score intervals are unique for each anaphor, it is not straightforward to incorporate them into a ML framework in a consistent manner."
"Second, this method introduces an element of competition be-tween several antecedents (see also[REF_CITE]), which the individual scores do not reflect."
"We trained and tested the NB classifier with the feature set F1, plus the Web feature."
The last row in Table 4 shows the results.
"We obtained a 9.1 per-centage point improvement in precision (an 18% im-provement relative to the F1 feature set) and a 12.8 percentage point improvement in recall (32% im-provement relative to F1), which amounts to an 11.4 percentage point improvement in F -measure (25% improvement relative to F1 feature set)."
"In particu-lar, all the examples in this paper were resolved."
Our algorithm still misclassified several an-tecedents.
"Sometimes even the Web is not large enough to contain the instantiated pattern, espe-cially when this is situation or speaker specific."
An-other problem is the high number of NE antecedents (39.6%) in our corpus.
"While our NER module is quite good, any errors in NE classification lead to incorrect instantiations and thus to incorrect classi-fications."
"In addition, the Web feature does not yet take into account pronouns (7.43% of all correct and potential antecedents in our corpus)."
In our own previous work[REF_CITE]we presented a preliminary symbolic ap-proach that uses Web counts and a recency-based tie-breaker for resolution of other-anaphora and bridging descriptions. (For another Web-based sym-bolic approach to bridging see[REF_CITE].)
The approach described in this paper is the first ma-chine learning approach to other-anaphora.
It is not directly comparable to the symbolic approaches above for two reasons.
"First, the approaches dif-fer in the data and the evaluation metrics they used."
"Second, our algorithm does not yet constitute a full resolution procedure."
"As the classifier oper-ates on the whole set of antecedent-anaphor pairs, more than one potential antecedent for each anaphor can be classified as “antecedent = yes”."
This can be amended by e.g. incremental processing.
"Also, the classifier does not know that each other-NP is anaphoric and therefore has an antecedent. (This contrasts with e.g. definite NPs.)"
"Thus, it can clas-sify all antecedents as “antecedent = no”."
"This can be remedied by using a back-off procedure, or a compe-tition learning approach[REF_CITE]."
"Fi-nally, the full resolution procedure will have to take into account other factors, e.g., syntactic constraints on antecedent realization."
Our approach is the first ML approach to any kind of anaphora that integrates the Web.
Using the Web as a knowledge source has considerable advantages.
"First, the size of the Web almost eliminates the prob-lem of data sparseness for our task."
"For this rea-son, using the Web has proved successful in sev-eral other fields of NLP, e.g., machine translati[REF_CITE]and bigram frequency estima-ti[REF_CITE]."
"In particular,[REF_CITE]have shown that using the Web handles data sparseness better than smoothing."
"Second, we do not process the returned Web pages in any way (tag-ging, parsing, e.g.), unlike e.g.[REF_CITE]."
"Third, the linguistically motivated patterns we use reduce long-distance dependencies between anaphor and antecedent to local dependen-cies."
"By looking up these patterns on the Web we obtain semantic information that is not and perhaps should not be encoded in an ontology (redescrip-tions, vague relations, etc.)."
"Finally, these local de-pendencies also reduce the need for prior word sense disambiguation, as the anaphor and the antecedent constrain each other’s sense within the context of the pattern."
"We presented a machine learning approach to other-anaphora, which uses a NB classifier and two sets of features."
"The first set consists of standard morpho-syntactic, recency, and semantic features based on WordNet."
The second set also incorpo-rates semantic knowledge obtained from the Web via lexico-semantic patterns specific to other-anaphora.
"Adding this knowledge resulted in a dramatic im-provement of 11.4% points in the classifier’s F -measure, yielding a final F -measure of 56.9%."
"To our knowledge, we are the first to integrate a Web feature into a ML framework for anaphora reso-lution."
"Adding this feature is inexpensive, solves the data sparseness problem, and allows to handle ex-amples with non-standard relations between anaphor and antecedent."
The approach is easily applicable to other anaphoric phenomena by developing appropri-ate lexico-syntactic patterns[REF_CITE].
Anaphora resolution is one of the most important research topics in Natural Lan-guage Processing.
"In English, overt pro-nouns such as she and definite noun phrases such as the company are anaphors that refer to preceding entities (an-tecedents)."
"In Japanese, anaphors are of-ten omitted, and these omissions are called zero pronouns."
There are two major ap-proaches to zero pronoun resolution: the heuristic approach and the machine learn-ing approach.
"Since we have to take var-ious factors into consideration, it is diffi-cult to find a good combination of heuris-tic rules."
"Therefore, the machine learn-ing approach is attractive, but it requires a large amount of training data."
"In this paper, we propose a method that com-bines ranking rules and machine learning."
"The ranking rules are simple and effective, while machine learning can take more fac-tors into account."
"From the results of our experiments, this combination gives better performance than either of the two previ-ous approaches."
Anaphora resolution is an important research topic in Natural Language Processing.
"For instance, machine translation systems should identify an-tecedents of anaphors (such as he or she) in the source language to achieve better translation quality in the target language."
"We are now studying open-domain question an-swering systems [URL_CITE] , and we expect QA systems to benefit from anaphora resolution."
Typical QA sys-tems try to answer a user’s question by finding rel-evant phrases from large corpora.
"When a correct answer phrase is far from the keywords given in the question, the systems will not succeed in find-ing the answer."
"If the system can correctly resolve anaphors, it will find keywords or answers repre-sented by anaphors, and the chances of finding the answer will increase."
"From this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles."
"In Japanese, anaphors are often omitted and these omissions are called zero pronouns."
"Since they do not give any hints (e.g., number or gender) about an-tecedents, automatic zero pronoun resolution is dif-ficult."
"In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’"
Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspa-per articles.
"They have discussed simple sentenses[REF_CITE], dialogues[REF_CITE], stereotypical lead sentences of newspaper ar-ticles[REF_CITE], intrasentential resoluti[REF_CITE]or organization names in newspaper ar-ticles[REF_CITE]."
There are two approaches to the problem: the heuristic approach and the machine learning ap- proach.
The Centering Theory[REF_CITE]is important in the heuristic approach.
"However, these improvements are not sufficient for resolving zeros accurately."
"We have to take even more factors into account, but it is difficult to maintain such heuris-tic rules."
"Therefore, recent studies employ machine learning approaches."
"However, it is also difficult to prepare a sufficient number of annotated corpora."
"In this paper, we propose a method that com-bines these two approaches."
"Heuristic ranking rules give a general preference, while a machine learn-ing method excludes inappropriate antecedent can-didates."
"From the results of our experiments, the proposed method shows better performance than ei-ther of the two approaches alone."
"Before giving a description of our methodology, we briefly introduce the grammar of the Japanese language here.  "
A Japanese sentence is a sequence of bunsetsus: .
"A bunsetsu is a se-quence of content words (e.g., nouns, adjectives, and verbs) followed by zero or more functional words  (e  .g.  , particles  .andA bunsetsuauxiliarymodifiesverbs):one of the following bunsetsus."
A particle (joshi) marks the grammatical case of the noun phrase immediately before it.
"For example, ga is nominative (subject), wo is accusative (object), ni is dative (object2), and wa marks a topic."
Tomu ga / Bobu ni / hon wo / okutta.
Tom=subj Bob=object2 book=object sent (Tom sent a book to Bob.)
"Bunsetsu dependency is represented by a list of bunsetsu    pairs  (  modifier  &quot;! , modified # )."
"For instance, indicates that there are four bunsetsus in this sentence and that the first bunsetsu modifies the fourth bunsetsu and so on."
"The last bunsetsu ! modifies no bunsetsu, which is in-dicated by ."
"It takes a long time to construct high-quality an-notated data, and we want to compare our results with conventional methods."
"Therefore, we obtained Seki’s data[REF_CITE], which are based on the Kyoto University Corpus [URL_CITE] 2.0."
These data are divided into two groups: gen-eral and editorial.
"According to his experiments, editorial is harder than general."
Perhaps this is caused by the differ-ence in rhetorical styles and the lengths of articles.
"The average number of sentences in an editorial ar-ticle is 28.7, while that in a general article is 13.9."
"However, we found problems in his data."
"For instance, the data contained ambiguous antecedents like dou-shi (the same person) or dou-sha (the same company) as correct antecedents."
We replaced these ‘correct answers’ with their explicit names.
We also removed zeros in quoted sentences because they are quite different from other sentences.
"In addition, we decided to use the output of ChaSen 2.2.9 [URL_CITE] and CaboCha 0.34 [URL_CITE] instead of the morphological information and the dependency in-formation provided by the Kyoto Corpus since clas-sification of the joshi (particles) in the Corpus was not satisfactory for our purpose."
"Since CaboCha was trained by Kyoto Corpus 3.0, CaboCha’s depen-dency output is very similar to that of the Corpus."
"In this paper, we combine heuristic ranking rules and machine learning."
"First, we describe how we ex-tract possible antecedents (candidates)."
"Second, we describe the rule-based ranking system and the ma-chine learning system."
"Finally, we describe how to combine these two methods."
We consider only anaphors for noun phrases fol-lowing Seki and other studies.
We assume that zeros are already detected.
"We also assume zeros are lo-cated at the starting point of a bunsetsu that contains a yougen (a verb, an adjective, or an auxiliary verb)."
"From now on, we use ‘verb’ instead of ‘yougen’ for readability."
A zero’s bunsetsu is a bunsetsu that con-tains the zero.
We further assume that each zero’s grammatical case is already determined by a zero detector and represented by corresponding particles.
"If a zero is the subject of a verb, its case is repre-sented by the particle ga."
"If it is an object, it is rep-resented by wo."
"If it is an object2, it is represented by ni."
We consider only these three cases.
A zero’s particle means such a particle.
"Since complex sentences are hard to analyze, each sentence is automatically split at conjunctive post-positions (setsuzoku joshi)[REF_CITE]."
"In order to distinguish the original complex sentence and the simpler sen-tences after the split, we call the former just a ‘sen-tence’ and the latter ‘post-split sentences’."
"When a conjunctive postposition appears in a relative clause, we do not split the sentence at that position."
"In the examples below, we split the first sentence at ‘and’ but do not split the second sentence at ‘and’."
She bought the book and sold it to him.
She bought the book that he wrote and sold.
A zero’s sentence is the (original) sentence that contains the zero.
"From now on, $ stands for a zero and % stands for a candidate of $ ’s antecedent. $ ’s particle is denoted ZP, and CP stands for % ’s next word that is % ’s particle or a punctuation symbol."
Candidates (possible antecedents) are enumerated on the fly by using the following method. 1.
"We &amp; extract a content word sequence as a candidate % if it is fol-lowed by a case marker (kaku-joshi, e.g., ga, wo), a topic marker (wa or mo), or a period. 2."
"If % ’s  is a verb, an adjective, an auxi-lary verb, an adverb, or a relative pronoun (ChaSen’s meishi-hijiritsu, e.g., koto (what he did) and toki (when she married)), % is ex-cluded. (If is a closing quotation mark, ( is checked instead.) 3."
"If % ’s is a pronoun or an adverbial noun (a noun that can also be used as an adverb, i.e., ChaSen’s meishi-fukushi-kanou), % is excluded. 4."
"If % is dou-shi (the person), it is replaced by the latest person name."
"If % is dou-sha (the company), it is replaced by the latest organi-zation name."
"If % is dou+suffix, it is replaced by the latest candidate that has the same suffix."
"For this task, we use a named entity recognizer[REF_CITE]."
The first step extracts a content word sequence from a bunsetsu.
"The second step excludes verb phrases, adjective phrases, and clauses."
"As a re-sult, we obtain only noun phrases."
The third step ex-cludes adverbial expressions like kotoshi (this year).
The forth step resolves anaphors like definite noun phrases in English.
"We should also resolve pro-nouns, but we did not because useful pronouns are rare in newspaper articles."
"In addition, we register a resolved zero as a new candidate."
"If $ ’s antecedent is determined to be *% ) , a new candidate % )+ is created for future zeros. % +) is a copy of %&amp;) except that % ) + ’s particle is ZP and %&quot;) ’s location is $ ’s location."
"In the training phase of the machine learning approach, we consider a correct answer as % ) ."
"Then, we can remove far candidates from the list."
"In this way, our zero resolver creates a ‘general purpose’ candidate list."
"However, some of the can-didates are inappropriate for certain zeros."
A verb usually does not have the same entity in two or more cases[REF_CITE].
"Therefore, our resolver excludes candidates that are filled in other cases of the verb."
"When a verb has two or more zeros, we resolve ga first, and its best candidate is excluded from the candidates of wo or ni."
Various heuristics have been reported in past litera-ture.
"Here, we use the following heuristics. 1."
"Forward center ranking[REF_CITE]: (topic , empathy , subject , object2 , object , others). 2. Property-sharing[REF_CITE]:"
"If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence."
"If a zero is an object, its antecedent is perhaps an object. 3."
"If a zero is the sub-ject of ‘eat,’ its antecedent is probably a per-son or an animal, and so on."
"We use Nihongo Goi Taikei[REF_CITE], which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate."
"Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Ap-pendix A for details.) 4."
"Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese ze-ros do not refer to noun phrases in relative clauses[REF_CITE]. (See Appendix B for details.)"
"Since sentences in newspaper articles are often complex and relative clauses are sometimes nested, we refine this rule in the following way. - A candidate’s relative clause is the inmost rel-ative clause that contains the candidate. - A relative clause finishes at the noun modified by the clause. - If $ appears before the finishing noun of % ’s rel-ative clause, the clause is still unfinished at $ ."
"Otherwise, the clause is already finished. - A quoted clause (with or without quotation marks “ ”) indicated by a quotation marker ‘to’ (‘that’ in ‘He said that she is ...’) is also re-garded as a relative clause. - We demote % after % ’s relative clause finishes."
It is not clear how to combine the above heuris-tics consistently.
"Here, we sort the candidates in a lexicographical order based on the above fea-tures of candidates."
"For instance, we can use a lexicographically increasing order defined by"
"Vi Re Ag Di Sa , where - Vi (for violation) is 1 if the candidate violates the semantic constraint."
"Otherwise, Vi is 0. - Re (for relative) is 1 if the candidate is in a rel-ative clause that has already finished before $ ."
"Otherwise, Re is 0. - Ag (for agreement) is 0 if CP=ZP holds. (Since most of wa and mo are subjects, they are re-garded as ga here.)"
"Otherwise, Ag is 1. - Di (for distance) is a non-negative integer that represents the number of post-split sentences between % and $ ."
"If a candidate’s Di is larger than maxDi, it is removed from the candidate list. - Sa (for salience) is 0 if CP is wa."
Sa is 1 if CP is ga.
Sa is 2 if CP is ni.
Sa is 3 if CP is wo.
"Otherwise, Sa is 4."
"We did not implement em-pathy because it makes the program more com-plex, and empathy verbs are rare in newspaper articles. . . . /  . . . ."
"For instance, holds."
The first ranked (lexicographically smallest) candi-date is regarded as the best candidate.
We employ lexicographical ordering because it seems the sim-plest way to rank candidates.
We put Vi in the first place because Vi was often regarded as a con-straint in the past literature.
We put Ag before Sa because Kameyama’s method was better than Walker’s  in Okumura   and[REF_CITE].
"There-fore, Vi Ag is expected to be a goodSa ordering."
The above ordering is an instance of this.
"Although we can consider various other features for zero pronoun resolution, it is difficult to com-bine these features consistently."
"Therefore, we use machine learning."
Support Vector Machines (SVMs) have shown good performance in various tasks in Natural Language Processing[REF_CITE].
"Here, we add features for complex sentences and analyze use-ful features by examining the weights of features."
We use the following features of % as well as CP.
CSem % ’s semantic categories. (See Appendix A.) CPPOS CP’s part-of-speech (POS) tags (rough and detailed).
CPOS The POS tags of the last word of % .
"Siblings When CP is wa or mo, it is not clear whether % is a subject."
"However, a verb rarely has the same entity in two or more cases."
"Therefore, if % modifies a verb that has a subject, % is not a subject."
"In the next example, hon is an object of katta."
"Ano / hon wa / Tomu ga / katta. that book=topic Tom=subj bought (As for that book, Tom bought it.)"
"In order to learn such things, we use sibling case-markers that modify the same verb as % ’s features."
We also use the following features of $ as well as ZP.
Conjunct The latest conjunctive postposition in the sentence and its classificati[REF_CITE].
ZSem Semantic categories of the verb that $ mod-ifies.
We use them only when the verb is sahen meishi + ‘suru.’
"Sahen meishi is a kind of noun that can be an object of the verb ‘suru’ (do) (e.g., ‘shop-ping’ in ‘do the shopping’)."
"We also use the following relations between $ and % as well as Ag, Vi, and Di."
Relative Whether % is in a relative clause.
Unfinished Whether the relative clause is unfin-ished at $ .
Intra (for intrasentential coreference)
Whether % explicitly appears in $ ’s sentence.
Sometimes it is difficult to distinguish cataphora from anaphora.
"Even if an antecedent appears in a preceding sentence, it is sometimes easier to find a candidate after $ , as illustrated by the case of ‘his’ in the next English example."
Bob and John separately drove to Charlie’s house. . . .
"Since his car broke down, John made a phone call. ."
"Even if Di holds, Intra does not necessarily hold because we introduce resolved zeros as new candidaites."
Parallel Whether % appears in a clause parallel to a clause in which a zero appears.
This will be useful for the resolution of a zero as with ‘it’ in the next English sentence.
He turned on the TV set and she turned it off.
"Whether % ’s bunsetsu appears imme-diately before $ ’s. In the following sentence, a can-didate ryoushin is located immediately before the zero."
Kare no / ryoushin wa / he+’s parents=topic ( $ ga) ikiteiru to / shinjiteiru. ( 3 =subj) alive+that believe (His parents believe that ( $ ) is still alive.)
"Here, we represent all of the above features by a boolean value: 0 or 1."
Semantic categories can be represented by a 0/1 vector whose 4 -th component corresponds to the 4 -th semantic category.
"Similarly, POS tags can be represented by a 0/1 vector whose 4 -th component corresponds to the 4 -th POS tag."
"On the other hand, Di has a non-negative integer value."
We also encode the distance by a 0/1 vector whose 4 -th component corresponds to the fact that the dis-tance is 4 .
The distance has an upper bound maxDi.
"In this way, we can represent a candidate by a boolean feature vector."
A candidate % 5 ’s feature vec-tor is denoted 675 .
"If a boolean feature appears only once in the given data, we remove the feature from the feature vectors.  The 5 6 5  training # , wheredata 8 5 is 8 compriseif % 5 is a correctthe setantecedentof pairs of a zero."
"Otherwise, 5 is ! ."
"By using the train- ; ing 8 data, SVM &amp;? 5  finds B@ , awheredecision 6 isfunctionthe feature : 6 vector 5 &lt; &gt;= 6 of a candidate % and ? 5 s are support vectors selected  C from the training data. &lt; 5 is a constant . . = is called a kernel function."
"If : 6 , holds, 6 is classified as a correct antecedent."
"Here, we use the following method to combine the ordering and SVM. 1. Sort candidates by using the lexicographical or-der. 2. Classify each candidate by using SVM in this order. 3."
"If : 6 5 is positive, stop there and sort the eval-uated candidates by : 6D5 in decreasing order. 4."
"If no candidate satisfies : 6E5 , . , return the best candidate in terms of : 6D5 ."
We conducted leave-one(-article)-out experiments.
"For each article, 29 other articles were used for training."
Table 1 compares the scores of the above methods. ‘First’ picks up the first candidate given by a given lexicographical ordering.
The acronym ‘vrads’ stands for the lexicographical ordering of Vi Re Ag Di Sa . ‘Best’ picks up the best can-didate in terms of : 6 without checking whether it is positive.
"Consequently, it is independent of the ordering (unless two or more candidates have the best value). ‘Svm1’ uses the ordinary SVM[REF_CITE]while ‘svm2’ uses a modified SVM for unbalanced data[REF_CITE], which gives a large penalty to misclassification of a minority (= positive) example. 5"
"In general, svm2 accepts more cadidates than svm1."
"According to this table, svm1 is too severe to exclude only bad candidates."
"We also tried the maximum entropy model [Footnote_6] (mem) and C4.[Footnote_5], but they were also too severe."
5 An ordinary SVM minimizes T while the modified SVM minimizes T2[&gt;\ ]_a`^ U T P Rcb T2[&gt;\ 2] ^ b ` U T where R Z N Rdb = number of negative exam-ples/number of positive examples.
"When we use SVM, we have to choose a good linear kernel ( = 6 kernel for better performance &amp;? 6 C ? .) forHereSVM, webecauseused theit was best according to our preliminary experiments."
We set maxDi at 3 because it gave the best results.
"The table also shows Seki’s scores for reference, but it is not fair to compare our scores with Seki’s scores directly because our data is slightly different from Seki’s."
"The number of zeros in general in our data is 347, while[REF_CITE]detected ze-ros[REF_CITE]and 404[REF_CITE]."
"The number of zeros in our editorial is 514, while[REF_CITE]resolved 498 detected zeros."
"In order to overcome the data sparseness,"
Seki used unannotated articles to get co-occurrence statistics.
"Without the data, their scores degraded about 5 points."
We have not conducted experiments that use unannotated corpora; this task is our future work.
"As we expected, instances of Vi Ag Sa show good performance."
"Without SVMs, ‘vrads’ is the best for general in the table."
It is interest-ing that such a simple ordering gives better perfor-mance than SVMs.
"However, the combination of ‘vrads’ and ‘svm2’ (= vrads+svm2) gives even bet-ter results."
"In general, ‘ e +svm2’ is better than ‘first’ and ‘ e +svm1.’"
"With SVM, ‘davrs+svm2’ gave the best result for editorial."
"Editorial articles some-times use anthropomorphism (e.g., The report says ...) that violates semantic constraints."
"Therefore, ‘vrads’ does not work well for such cases."
Table 2 shows the weights of the above features determined by svm2 for a fold of the leave-one-out experiment of ‘vrads+svm2 &quot;h * .’ &quot; The egf i h weightsas kj can @ be lm n given h by h rewriting : egf fo egfo .
"This table shows that Kameyama’s property-sharing (Ag), semantic violation (Vi), can-didate’s particle (CP), and distance (Di) are very important features."
"Our new features Parallel, Un-finished, and Intra also obtained relatively large weights."
Semantic categories ‘suggestions’ and ‘re-port’ reflect the fact that some articles use anthro-pomorphism.
These weights will be useful to de-sign better heuristic rules.
The fact that Unfinished’s weight almost cancels Relative’s weight justifies the
Yoshino ; (2001  ) @ used 6 C wx an  . ordinaryHe tried toSVMfind withuse- = 6 ful features by feature elimination.
"Since features are not completely independent, removing a heav-ily weighted feature does not necessarily degrade the system’s performance."
"Hence, feature elimination is more reliable for reducing the number of features."
"However, feature elimination takes a long time."
"On the other hand, feature weights can give rough guid-ance."
"According to the table, our new features (Par-allel, Unfinished, and Intra) obtained relatively large weights."
This implies their importance.
"When we eliminated these three features, vrads+svm2’s score for editorial dropped by 4 points."
"Therefore, combi-nations of these three features are useful."
"Recently,[REF_CITE]proposed an SVM-based tournament model that compares two candi-dates and selects the better one."
We would like to compare or combine their method with our method.
"For further improvement, we have to make the mor-phological analyzer and the dependency analyzer more reliable because they make many mistakes when they process complex sentences."
SVM has often been criticized as being too slow.
"However, the above data were small enough for the state-of-the-art SVM programs."
"The number of ex-amples in each set of training data was about 5,000– 6,100, and each training phase took only 5–18 sec-onds on a 2.4-GHz Pentium 4 machine."
"In order to make Japanese zero pronoun resolu-tion more reliable, we have to maintain complicated heuristic rules or prepare a large amount of training data."
"In order to alleviate this problem, we com-bined simple lexicographical orderings and SVMs."
"It turned out that a simple lexicographical ordering performed better than SVM, but their combination gave even better performance."
"By examining feature weights, we found that features for complex sen-tences are important in zero pronoun resolution."
We confirmed this by feature elimination.
We have to be careful about parallel structures for this analysis.
"According to CaboCha, Kare ga in the next example modifies a verb katte, which modifies another verb karita."
"However, katte is contained in a clause that modifies the noun hon."
Kare ga / katte / kanojo ga / karita / he=subj bought she=subj borrowed hon wa /omoshiroi. book=topic interesting (The book that he bought and she borrowed is interesting.)
The particle no (= “’s” in English) directly modi-fies a noun.
"For instance, Taro in Taro no hon (Taro’s book) is a book that Taro wrote or a book that Taro has."
"From this point of view, we also mark A in A no B (A’s B) as a candidate in a relative clause."
The paper presents a maximum entropy Chinese character-based parser trained on the Chinese Treebank (“CTB” hence-forth).
"Word-based parse trees in CTB are first converted into character-based trees, where word-level part-of-speech (POS) tags become constituent labels and character-level tags are de-rived from word-level POS tags."
A maximum entropy parser is then trained on the character-based corpus.
"The parser does word-segmentation, POS-tagging and parsing in a unified frame-  work."
An average label F-measure  and word-segmentation F-measure are achieved by the parser.
"Our re-sults show that word-level POS tags can improve significantly word-segmentation, but higher-level syntactic strutures are of little use to word segmentation in the max-imum entropy parser."
A word-dictionary helps to improve both word-segmentation and parsing accuracy.
"After Linguistic Data Consortium (LDC) re-leased the Chinese Treebank (CTB) developed at UPenn[REF_CITE], various statistical Chinese parsers[REF_CITE]have been built."
Techniques used in parsing En-glish have been shown working fairly well when ap-plied to parsing Chinese text.
"As there is no word boundary in written Chinese text, CTB is manually segmented into words and then labeled."
Parsers de-scribed[REF_CITE]and[REF_CITE]operate at word-level with the assumption that input sentences are pre-segmented.
The paper studies the problem of parsing Chi-nese unsegmented sentences.
"The first motivation is that a character-based parser can be used directly in natural language applications that operate at char-acter level, whereas a word-based parser requires a separate word-segmenter."
"The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annota-tions, provides us with an opportunity to create a highly-accurate word-segmenter."
It is widely known that Chinese word-segmentation is a hard problem.
There are multiple studies[REF_CITE]show-ing that the agreement between  two (tountrainedlower )  na-.tive speakers is about upper The agreement between multiple human subjects is even lower[REF_CITE].
"The rea-son is that human subjects may differ in segment-ing things like personal names (whether family and given names should be one or two words), num-ber and measure units and compound words, al-though these ambiguities do not change a human being’s understanding of a sentence."
Low agree-ment between humans affects directly evaluation of machines’ performance[REF_CITE]as it is hard to define a gold standard.
It does not nec-essarily imply that machines cannot do better than humans.
"Indeed, if we train a model with consis-tently segmented data, a machine may do a bet-ter job in “remembering” word segmentations."
"As will be shown shortly, it is straightforward to en-code word-segmentation information in a character- based parse tree."
"Parsing Chinese character streams therefore does effectively word-segmentation, part-of-speech (POS) tagging and constituent labeling at the same time."
"Since syntactical information influences directly word-segmentation in the pro-posed character-based parser, CTB allows us to test whether or not syntactic information is useful for word-segmentation."
A third advantage of parsing Chinese character streams is that Chinese words are more or less an open concept and the out-of-vocabulary (OOV) word rate is high.
"As morphol-ogy of the Chinese language is limited, extra care is needed to model unknown words when building a word-based model."
"Chinese characters, on the other hand, are almost closed."
"To demonstrate the OOV problem, we collect  a wordsen-and character vocabulary from the first tences of CTB, and compute their coverages on the corresponding  word and character tokenization of the last  of the corpus."
The word-based OOV onlyrate is  . while the character-based OOV rate is
The first step of training a character-based parser is to convert word-based parse trees into character-based trees.
We derive character-level tags from word-level POS tags and encode word-boundary in-formation with a positional tag.
Word-level POSs become a constituent label in character-based trees.
A maximum entropy parser[REF_CITE]parser is then built and tested.
Many language-independent feature templates in the English parser can be reused.
"Lexical features, which are language-dependent, are used to further improve the baseline models trained with language-independent features only."
Word-segmentation results will be presented and it will be shown that POSs are very helpful while higher-level syntactic structures are of little use to word-segmentation – at least in the way they are used in the parser.
CTB is manually segmented and is tokenized at word level.
"To build a Chinese character parser, we first need to convert word-based parse trees into character trees."
A few simple rules are employed in this conversion to encode word boundary informa-tion: 1.
Word-level POS tags become labels in charac-ter trees. 2.
Character-level tags are inherited from word-level POS tags after appending a positional tag; 3.
"For single-character words, the positional tag is “s”; for multiple-character words, the first char-acter is appended with a positional tag “b”, last character with a positional tag “e”, and all mid-dle characters with a positional tag “m”."
An example will clarify any ambiguity of the rules.
"For example, a word-parse tree “(IP (NP (NP  /NR ) (NP  /NN /NN ) ) (VP ! /VV ) &quot; /PU )” would become “(IP (NP (NP (NR /nrb /nrm /nre ) ) (NP (NN /nnb /nne ) (NN /nnb /nne ) ) ) (VP (VV ! /vvb /vve ) ) (PU &quot; /pus ) ).” (1)"
Note that the word-level POS “NR” becomes a la-bel of the constituent spanning the three characters “ ## ”.
The character-level tags of the constituent “ $$ ” are the lower-cased word-level POS tag plus a positional letter.
"Thus, the first character “ ” is assigned the tag “nrb” where “nr” is from the word-level POS tag and “b” denotes the begin-ning character; the second (middle) character “ ” gets the positional letter “m”, signifying that it is in the middle, and the last character “ ” gets the posi-tional letter “e”, denoting the end of the word."
Other words in the sentence are mapped similarly.
"After the mapping, the number of terminal tokens of the character tree is larger than that of the word tree."
"It is clear that character-level tags encode word boundary information, and chunk-level [Footnote_1] labels are word-level POS tags."
1 A chunk is here defined as a constituent whose children are all preterminals.
"Therefore, parsing a Chi-nese character sentence is effectively doing word-segmentation, POS-tagging and constructing syntac-tic structure at the same time."
"The maximum entropy parser[REF_CITE]is used in this study, for it offers the flexibility of inte-grating multiple sources of knowledge into a model."
"The maximum entropy model decomposes &apos;% )&amp; +( * ,.- , the probability of a parse tree ( given a sentence , , into the product of probabilities of individual parse actions, i.e., / 02461 5 %798&amp; 3 * ,;&lt;: 8&gt;5= 3@?A5CB - ."
"The parse ac-tions 8 5021 are an ordered sequence, where +D E is the number of actions associated with the parse ( ."
The mapping from a parse tree to its unique sequence of actions is 1-to-1.
"Each parse action is either tag-ging a word, chunking tagged words, extend-ing an existing constituent to another constituent, or checking whether an open constituent should be closed."
"Each component model takes the expo-nential form: 3@?A5CB 7 * ,;&lt;: 8 =5 3@A5CB? .-"
"F W  N S&amp; ;,@3 ?"
"A5CB&lt;: 8T5= &lt; -VU: 8 3 % &amp;98 3 : S&amp; ,;:&lt;8T5= - (2) where W S&amp; ;, :&lt;8 5= 3)A5CB? - is ) a 3 ?A5C normalization B term to ensure @3 ? that A5CB 3 %7&amp;98 3 * X, :&lt;8&gt;5= - is a probability, R N &amp;S,;&lt;: 8 5= and P N is the :&lt;8 weight - is aoffeature  . function (often binary)"
"Given a set of features and a corpus of training data, there exist efficient training algorithms[REF_CITE]to find the optimal parameters Y P  ."
The art of building a maximum entropy parser then reduces to choos-ing “good” features.
We break features used in this study into two categories.
The first set of features are derived from predefined templates.
"When these templates are applied to training data, features are generated automatically."
"Since these templates can be used in any language, features generated this way are referred to language-independent features."
The second category of features incorporate lexical in-formation into the model and are primarily designed to improve word-segmentation.
This set of features are language-dependent since a Chinese word dic-tionary is required.
The maximum entropy parser[REF_CITE]parses a sentence in three phases: (1) it first tags the input sentence.
Multiple tag sequences are kept in the search heap for processing in later stages; (2) Tagged tokens are grouped into chunks.
"It is pos-sible that a tagged token is not in any chunk; (3) A chunked sentence, consisting of a forest of many subtrees, is then used to extend a subtree to a new constituent or join an existing constituent."
Each ex-tending action is followed by a checking ac-tion which decides whether or not to close the ex-tended constituent.
"In general, when a parse action 8 3 is carried out, the context information, i.e., the 3@?A in- 5CB put sentence , and preceding parse actions 8 =5 , is represented by a forest of subtrees."
Feature func-tions operate on the forest context and the next parse action.
They are all of the form:
"R N2[ S&amp; ,X&lt;: 8&gt;=5 )3 A5CB? \- &lt;: 8 3 -^_]"
"Fa` N &amp;S,X&lt;: &gt;8 5= 3)A5CB? -cbd&amp;98 3 FO8 N -\: (3) where ` N S&amp; ;, &lt;: 8T5= @3 A5CB? - is a binary function on the con-text."
Some notations are needed to present features.
"We use egf to denote an input terminal token, h&lt;f its tag (preterminal), i f a chunk, and G f a constituent label, where the index j is relative to the current subtree: the subtree immediately left to the current is indexed as k , the second left to the current sub-tree is indexed as kml , the subtree immediately to the right is indexed as , so on and so forth. n  p repre-sents the root label of the  -child of the  subtree."
"If qXu , the child is counted from right."
"With these notations, we are ready to introduce language-independent features, which are broken down as follows: Tag Features"
"In the tag model, the context consists of a win-dow of five tokens – the token being tagged and two tokens to its left and right – and two tags on the left of the current word."
"The feature templates are tabulated in Table 1 (to save space, templates are grouped)."
"At training time, feature templates are in-stantiated by the training data."
"For example, when the template “ e A5? vhcw: ” is applied to the first charac-ter of the sample sentence, “(IP (NP (NP (NR /nrb /nrm /nre ) ) (NP (NN /nnb /nne ) (NN /nnb /nne ) ) ) (VP (VV ! /vvb /vve ) ) (PU &quot; /pus ) )”, a feature R @&amp; e A5?"
F *BOUNDARY* : {|- is generated.
"Note that e ?A5 is the token on the left and in this case, the boundary of the sentence."
The template “ :vh^w ” is instantiated similarly as R @&amp; }e wmF ~:vh w |{ - .
The model described so far does not depend on any Chinese word dictionary.
All features derived from templates in Section 3.1 are extracted from training data.
A problem is that words not seen in training data may not have “good” features associated with them.
"Fortunately, the maximum entropy framework makes it relatively easy to incorporate other sources of knowledge into the model."
"We present a set of language-dependent features in this section, primar-ily for Chinese word segmentation."
The language-dependent features are computed from a word list and training data.
"Formerly, let  be a list of Chinese words, where characters are sepa-rated by spaces."
"At the time of tagging characters (recall word-segmentation information is encoded in character-level tags), we test characters within a window of five (that is, two characters to the left and two to the right) and see if a character either starts, occurs in any position of, or ends any word on the list  ."
This feature templates are summarized in Ta- ble 5. { &amp;@efT:&lt;- tests if the character emf starts any word on the list  .
"Similarly, &amp;@emf&lt;:  - tests if the character ef occurs in any position of any word on the list  , and G &amp;@e f :&lt; - tests if the character e f is the last position of any word on the list  ."
A word list can be collected to encode different semantic or syntactic information.
"For example, a list of location names or personal names may help the model to identify unseen city or personal names; Or a closed list of functional words can be collected to represent a particular set of words sharing a POS."
This type of features would improve the model ro-bustness since unseen words will share features fired for seen words.
We will show shortly that even a relatively small word-list improves significantly the word-segmentation accuracy.
All experiments reported here are conducted on the latest LDC release of £ thewordsChinese.
"WordTreebankparse, whichtreesconsists of about l are converted to character trees using the procedure described in Section 2."
All traces and functional tags are stripped in training and testing.
Two re-sults are reported for the character-based parsers: the F-measure of word segmentation and F-measure of constituent labels.
"Formally, let #¤ ¥ @&amp; ^-\\: ¤§¦T@&amp; c- be the number of words of the   reference sentence and its parser output, respectively, and ¨©@&amp; ^- be the number of common words in the  sentence of test set, then the word segmentation F-measure is ;«C¬C­ F L 3 [ ¤ l ¥L &amp;@^-®¯¤3 ¨©&amp;@^- ¦ &amp;@^- ] ª (4)"
"The F-measure of constituent labels is computed similarly: ª p±°² F L 3 [ .l ¥L @&amp; ^ 3-2®D³$&amp;@^¦ - &gt;@&amp; ^^- ] : (5) where  ¥ @&amp; ^- and  ¦ &amp;@^ - are the number of con-stituents in the   reference parse tree and parser output, respectively, and D´&amp;@c- is the number of common constituents."
"Chunk-level labels converted from POS tags (e.g., “NR”, “NN” and “VV” etc in (1)) are included in computing label F-measures for character-based parsers."
The first question we have is whether CTB is large enough in the sense that the performance saturates.
The first set of experiments are intended to answer  this question.
"In these experiments, the first  the test set."
We start withCTB is used as the training  set  andof thethe resttraining setas and increase the training set each time by  .
Only language-independent features are used in these ex-periments.
Figure 1 shows the word segmentation F-measure and label F-measure versus the amount of training data.
"As can be seen, F-measures of both word segmentation and constituent label increase mono-tonically as the amount of training data increases."
"F-measure isIf all training data  is andused,labelthe wordF-measuresegmentation  ."
These results show that language-independent fea-tures work fairly well – a major advantage of data-driven statistical approach.
The learning curve also shows that the current training size has not reached a saturating point.
This indicates that there is room to improve our model by getting more training data.
"In this section, we present the main parsing results."
"As it has not been long since the second release of CTB and there is no commonly-agreed training and test set, we divide the entire corpus into 10 equal par-titions and hold each partition as a test set while the rest are used for training."
"For each training-test con-figuration, a baseline model is trained with only lan-guage independent features."
Baseline word segmen-tation and label F-measures are plotted with dotted-line in Figure 2.
We then add extra lexical features described in Section 3.1 to the model.
Lexical ques-tions are derived from a 58K-entry word list.
"The word list is broken into 4 sub-lists based on word length, ranging from 2 to 5 characters."
Lexical fea-tures are computed by answering one of the three questions in Table 5.
"Intuitively, these questions would help the model to identify word boundaries, which in turn ought to improve the parser."
This is confirmed by results shown in Figure 2.
The solid two lines represent results with enhanced lexical questions.
"As can be seen, lexical questions improve significantly both word segmentation and parsing across all experiments."
This is not surprising as lex-ical features derived from the word list are comple-mentary to language-independent features computed from training sentences. measures vs. the experiment numbers.
Lines with triangles: segmentation; Lines with circles: label; Dotted-lines: language-independent features only; Solid lines: plus lexical features.
"Another observation is that results vary greatly across experiment configurations: for the model trained with lexical features, the  secondand exper-word-iment has a label F-measure  , while  the sixth ex-segmentation F-measure  periment has a label F-measure  ."
The largeandvariancesword-segmentation F-measure justify multiple experiment runs.
"To reduce the vari-ances, we report numbers averaged over the 10 ex-periments in Table 6."
"Numbers on the row start-ing with “WS” are word-segmentation results, while numbers on the last row are F-measures of con-stituent labels."
The second column are average F-measures for the baseline model trained with only language-independent features.
The third column contains F-measures for the model trained with extra lexical features.
The last column are releative error measure isreduction.
The  bestandaveragelabel F-measureword-segmentationis  .
Table 6: WS: word-segmentation.
Baseline: language-independent features.
LexFeat: plus lex-ical features.
Numbers are averaged over the 10 ex-periments in Figure 2.
"Since CTB provides us with full parse trees, we want to know how syntactic information affects word-segmentation."
"To this end, we devise two sets of experiments: 1."
We strip all POS tags and labels in the Chinese Treebank and retain only word boundary infor-mation.
"To use the same maximum entropy parser, we represent word boundary by dummy constituent label “W”."
"For example, the sample sentence (1) in Section 2 is represented as: (W /wb /wm /we ) (W /wb /we ) (W /wb /we ) (W ! /wb /we ) (W &quot; /ws ). 2."
We remove all labels but retain word-level POS information.
The sample sentence above is rep-resented as: (NR /nrb /nrm /nre ) (NN /nnb /nne ) (NN /nnb /nne ) (VV ! /vvb /vve ) (PU &quot; /pus ).
"Since our parser operates at character level, and more training data is used, the best results are not directly compa-rable."
"The middle point of the learning curve in Fig-ure 1, which is trained with roughly 100K words, is at the same ballpark[REF_CITE]."
"The con-tribution of this work is that the proposed character-based parser does word-segmentation, POS tagging and parsing in a unified framework."
It is the first at-tempt to our knowledge that syntactic information is used in word-segmentation.
Chinese word segmentation is a well-known prob-lem that has been studied extensively[REF_CITE]and it is known that human agreement is relatively low.
"Without knowing and control-ling testing conditions, it is nearly impossible to compare results in a meaningful way."
"There-fore, we will compare our approach with some related work only without commenting on seg-mentation accuracy."
Our method is supervised in that the training data is manually labeled.
All these work assume that a lexi-con or some manually segmented data or both are available.
There are numerous work exploring semi-supervised or unsupervised algorithms to segment Chinese text.
The auto-matically learned lexicon is pruned using a mutual information criterion.
"We present a maximum entropy Chinese character-based parser which does word-segmentation, POS tagging and parsing in a unified framework."
"The flexibility of maximum entropy model allows us to integrate into the model knowledge from other sources, together with features derived automat-ically from training corpus."
"We have shown that a relatively small word-list can reduce word-segmentation error by as much  as l , and a word-segmentation F-measure  are obtained by the character-basedand label F-measureparser."
"Our results also show that POS information is very useful for Chinese word-segmentation, but higher-level syntactic information benefits little to word-segmentation."
"When building a Chinese named entity recognition system, one must deal with certain language-specific issues such as whether the model should be based on characters or words."
"While there is no unique answer to this question, we discuss in detail advantages and disadvantages of each model, identify problems in segmen-tation and suggest possible solutions, pre-senting our observations, analysis, and experimental results."
The second topic of this paper is classifier combination.
We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs.
The results demonstrate that classifier combination is an effective tech-nique of improving system performance: experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error.
Named entity (NE) recognition has drawn much at-tention in recent years.
"It was a designated task in a number of conferences, including the Mes-sage Understanding Conferences[REF_CITE], the Information Retrieval and Ex-traction Conference[REF_CITE], the Conferences on Natural Language Learning (Tjong[REF_CITE]; Tjong[REF_CITE]), and the recent Automatic Content Extraction Con-ference[REF_CITE]."
A variety of algorithms have been proposed for NE recognition.
"Many of these algorithms are, in principle, language-independent."
"However, when applying these algorithms to languages such as Chinese and Japanese, we must deal with cer-tain language-specific issues: for example, should we build a character-based model or a word-based model? how do word segmentation errors affect NE recognition? how should word segmentation and NE recognition interact with each other?"
"Besides word segmentation related issues, Chinese does not have capitalization, which is a very useful feature in iden-tifying NEs in languages such as English, Spanish, or Dutch."
How does the lack of features such as cap-italization affect the performance?
"In the first part of this paper, we discuss these language-specific issues in Chinese NE recogni-tion."
"In particular, we use a hidden Markov model (HMM) system as an example, and discuss various issues related to applying the HMM classifier to Chi-nese."
The HMM classifier is similar to the one de-scribed[REF_CITE].
"In the second part of this paper, we investigate the combination of a set of diverse NE recognition classifiers."
"Four statistical classifiers are combined in the experiments, including the above-mentioned hidden Markov model classifier, a transformation-based learning classifier[REF_CITE], a maximum entropy classifier[REF_CITE], and a robust risk minimization classi-fier[REF_CITE]."
"The remainder of this paper is organized as fol-lows: Section 2 describes the experiment data, Sec-tion 3 discusses specific issues related to Chinese NE recognition, Section 4 presents the four classi-fiers and approaches to combining these classifiers."
We used three annotated Chinese corpora in our ex-periments.
"The IBM-FBIS Corpus The Foreign Broadcast Information Service (FBIS) offers an extensive collection of translations and transcriptions of open source information monitored worldwide on diverse topics such as military af-fairs, politics, economics, and science and technol-ogy."
"The IBM-FBIS corpus consists of approxi-mately 3,000 Chinese articles obtained from FBIS (about 3.2 million Chinese characters in total)."
"This corpus was tagged by a native Chinese speaker with 32 NE categories, such as person, location, organi-zation, country, people, date, time, percentage, car-dinal, ordinal, product, substance, and salutation."
"There are approximately 300,000 NEs in the entire corpus, 16% of which are labeled as person, 16% as organization, and 11% as location."
"Chinese does not have delimiters between words, so a key design issue in Chinese NE recognition is whether to build a character-based model or a word-based model."
"In this section, we use a hid-den Markov model NE recognition system as an ex-ample to discuss language-specific issues in Chinese NE recognition."
"NE recognition can be formulated as a classification task, where the goal is to label each token with a tag indicating whether it belongs to a specific NE or is not part of any NE."
The HMM classifier used in our experiments follows the algorithm described[REF_CITE].
It performs sequence clas-sification by assigning each token either one of the NE types or the label “O” to represent “outside any NE”.
"The states in the HMM are organized into re-gions, one region for each type of NE plus one for “O”."
"Within each of the regions, a statistical lan-guage model is used to compute the likelihood of words occurring within that region."
"The transition probabilities are smoothed by deleted interpolation, and the decoding is performed using the Viterbi al-gorithm."
"To build a model for identifying Chinese NEs, we need to determine the basic unit of the model: char-acter or word."
"On one hand, the word-based model is attractive since it allows the system to inspect a larger window of text, which may lead to more in-formative decisions."
"On the other hand, a word seg-menter is not error-prone and these errors may prop-agate and result in errors in NE recognition."
"Two systems, a character-based HMM model and a word-based HMM model, were built for compar-ison."
The word segmenter used in our experiments relies on dictionaries and surrounding words in lo-cal context to determine the word boundaries.
"Dur-ing training, the NE boundaries were provided to the word segmenter; the latter is restricted to enforce word boundaries at each entity boundary."
"Therefore, at training time, the word boundaries are consistent with the entity boundaries."
"At test time, however, the segmenter could create words which do not agree with the gold-standard entity boundaries. model, the word-based HMM model, and the class-based HMM model. (The precision, recall, and F-measure presented in this table and throughout this paper are based on correct identification of all the attributes of an NE, including boundary, content, and type.)"
The performance of the character-based model and the word-based model are shown in Table 1.
"The two corpora used in the evaluation, the IBM-FBIS corpus and the IEER corpus, differ greatly in data size and the number of NE types."
"The IBM-FBIS training data consists of 3.1 million characters and the corresponding test data has 270,000 characters."
"As we can see from the table, for both corpora, the character-based model outperforms the word-based model, with a lead of 3 to 5.5 in F-measure."
The per-formance gap between two models is larger for the IEER data than for the IBM-FBIS data.
We also built a class-based NE model.
"After word segmentation, class tags such as number, chinese-name, foreign-name, date, and percent are used to replace words belonging to these classes."
Whether a word belongs to a specific class is identified by a rule-based normalizer.
The performance of the class-based HMM model is also shown in Table 1.
"For the IBM-FBIS corpus, the class-based model outperforms the word-based model; for the IEER corpus, the class-based model is worse than the word-based model."
"In both cases, the performance difference between the word-based model and the class-based model is very small."
The character-based model outperforms the class-based model in both tests.
"A more careful analysis indicates that although the word-based model performs worse than the character-based model overall in our evaluation, it performs better for certain NE types."
"For instance, the word-based model has a better performance for the organization category than the character-based model in both tests."
"While the character-based model has an F-measure of 65.07 (IBM-FBIS) and 64.76 (IEER) for the organization category, the word-based model achieves F-measure scores of 69.14 (IBM-FBIS) and 72.38 (IEER) respectively."
"One reason may be that organization names tend to contain many characters, and since the word-based model allows the system to analyze a larger window of text, it is more likely to make a correct guess."
We can integrate the character-based model and the word-based model by combining the decisions from the two models.
"For instance, if we use the de-cisions of the word-based model for the organiza-tion category, but use the decisions of the character-based model for all the other categories, the over-all F-measure goes up to 76.91 for the IEER data, higher than using either the character-based or word-based model alone."
Another way to integrate the two models is to use a hybrid model – starting with a word-based model and backing off to character-based model if the word is unknown.
We believe that one main reason for the lower per-formance of the word-based model is that the word granularity defined by the word segmenter is not suitable for the HMM model to perform the NE recognition task.
What exactly constitutes a Chinese word has been a topic of major debate.
We are in-terested in what is the best word granularity for our particular task.
"To illustrate the word granularity problem for NE tagging, we take person names as an example."
"Our word segmenter marks a person’s name as one word, consistent with the convention used by the Chinese treebank and many other word segmentation sys-tems."
"While this may be useful in other applications, it is certainly not a good choice for our NE model."
"Chinese names typically contain two or three char-acters, with family name preceding first name."
"Only a limited set of characters are used as family names, while the first name can be any character(s)."
"There-fore, the family name is a very important and use-ful feature in identifying an NE in the person cate-gory."
"By combining the family name and the first name into one word, this important feature is lost to the word-based model."
"In our tests, the word-based model performs much worse for the person category than the character-based model."
"We believe that, for the purpose of NE recognition, it is better to separate the family name from the first name in word segmen-tation, although this is not the convention used in the Chinese treebank."
"Other examples include the segmentation of words indicating dates, countries, locations, percent-ages, measures, and ordinals."
"For instance, “July 4th” is expressed by four characters “7th month 4th day” in Chinese."
"The word segmenter marks the four characters as a single word; however, the sec-ond and the last character are actually good features for indicating date, since the dates are usually ex-pressed using the same structure (e.g., “[REF_CITE]th” is expressed by “3rd month 25th day” in Chinese)."
"For reasons similar to the above, we believe that it is better to separate characters representing “month” and “day”, rather than combining the four charac-ters into one word."
"A similar problem can be ob-served in English with tokens such as “61-year-old man” if one is interested in identifying a person’s age, in which case ’year’ and ’old’ are good features for predication."
The above analysis suggests that a better way to apply a word segmenter in an NE system is to first adapt the segmenter so that the segmentation granu-larity is more appropriate to the particular task and model.
"As a guideline, characters that are good fea-tures for identifying NEs should not be combined with other characters into word."
Additional ex-amples include characters expressing “percent” and characters representing monetary measures .
Word segmentation errors can lead to mistakes in NE recognition.
"Suppose an NE consists of four characters    , if the word segmen-tation merges with a character preceding it, then this NE cannot be correctly identified by the word-based model since the boundary will be incor-rect."
"Besides inducing NE boundary errors, incor-rect word segmentation also leads to wrong match-ings between training examples and testing exam-ples, which may result in mistakes in identifying en-tities."
We computed the upper bound for the word-based model for the IBM-FBIS test presented in Table 1.
"The upper bound of performance is computed by dividing the total number of NEs whose bound-aries are also recognized as boundaries by the word segmenter by the total number of NEs in the cor-pus, which is the precision, recall, and also the F-measure."
"For the IBM-FBIS test data in Table 1, the upper bound of the word-based model is 95.7 F-measure."
We also did the following experiment to measure the effect of word segmentation errors: we gave the boundaries of NEs in the test data to the word segmenter and forced it to mark entity boundaries as word boundaries.
This eliminates the word seg-mentation errors that inevitably result in NE bound-ary errors.
"For the IBM-FBIS data, the word-based HMM model achieves 76.60 F-measure when the entity boundaries in the test data are given, and the class-based model achieves 77.77 F-measure, higher than the 77.19 F-measure by the character-based model in Table 1."
"For the IEER data, the F-measure of the word-based model improves from 70.83 to 73.74 when the entity boundaries are given, and the class-based model improves from 70.20 to 72.47."
"This suggests that with the improvement in Chi-nese word segmentation, the word-based model may achieve comparable or better performance than the character-based model."
Capitalization in English gives good evidence of names.
"Our HMM classifier for English uses a set of word-features to indicate whether a word con-tains all capitalized letters, only digits, or capital-ized letters and period, as described[REF_CITE]."
"However, Chinese does not have capitaliza-tion."
"When we applied the HMM system to Chinese, we retained such features since Chinese text also in-clude digits and roman words (such as in product or company names)."
"In an attempt to investigate the usefulness of such features for Chinese, we removed them from the system and observed very little dif-ference in overall performance (0.4 difference in F-measure)."
"To test the robustness of the model, we trained the system on the 100,000 word IBM-CT data and tested on the same IBM-FBIS data."
"The character-based model achieves 61.36 F-measure and the word-based model achieves 58.40 F-measure, compared to 77.19 and 74.17, respectively, using the 20 times larger IBM-FBIS training set."
This represents an ap-proximately 20% relative reduction in performance when trained on a related yet different and consider-ably smaller training set.
We plan to investigate fur-ther the relation between corpus type and size and performance.
This section investigates the combination of a set of classifiers for NE recognition.
We first introduce the classifiers used in our experiments and then describe the combination methods.
"Besides the HMM classifier mentioned in the previ-ous section, the following three classifiers were used in the experiments."
"Transformation-based learning is an error-driven algorithm which has two major steps: it starts by assigning some classification to each example, and then automatically proposing, evaluating and select-ing the classification changes that maximally de-crease the number of errors."
"TBL has some attractive qualities that make it suitable for the language-related tasks: it can au-tomatically integrate heterogeneous types of knowl-edge, without the need for explicit modeling (similar to Snow[REF_CITE], Maximum Entropy, decision trees, etc); it is error–driven, thus directly minimizes the ultimate evaluation measure: the er-ror rate."
The TBL toolkit used in this experiment is described[REF_CITE].
The model used here is based on the maxi-mum entropy model used for shallow parsing[REF_CITE].
"A sentence with NE tags is converted into a shallow tree: tokens not in any NE are assigned an “O” tag, while tokens within an NE are represented as constituents whose la-bel is the same as the NE type."
"For exam-ple, the annotated sentence “I will fly to (LO-CATION New York) (DATEREF tomorrow)” is represented as a tree “(S I/O will/O fly/O to/O (LOCATION New/LOCATION York/LOCATION) (DATEREF tomorrow/DATEREF) )”."
"Once an NE is represented as a shallow tree, NE recognition can be realized by performing shallow parsing."
We use the tagging and chunking model described[REF_CITE]for shallow parsing.
"In the tagging model, the context consists of a window of five tokens (including the token being tagged and two tokens to its left and two tokens to its right) and two tags to the left of the current token."
"Five groups of feature templates are used: token unigram, token bigram, token trigram, tag unigram and tag bigram (all within the context window)."
"In the chunking model, the context is limited to a window of three subtrees: the previous, current and next subtree."
Un-igram and bigram chunk (or tag) labels are used as features.
"This system is a variant of the text chunking system described[REF_CITE], where the NE recognition problem is regarded as a sequential token-based tagging problem."
"We denote by  (  !&quot;!! # ! # $# &amp;% ) the sequence of tokenized text, which is the input to our system."
"In token-based tagging, the goal is to assign a class-label &apos; to ev-ery token ."
"In our system, this is achieved by estimating the conditional probability )( *&apos; ,&quot;+ -. for every pos-sible class-label value + , where . is a feature vector associated with token ."
"The feature vector /. can depend on previously predicted class-labels  !0  , but the dependency is typically assumed to be lo-cal."
"Given such a conditional probability model, in the decoding stage, we estimate the best possi-ble sequence of &apos; ’s using a dynamic programming approach."
"In our system, the conditional probability model has the following parametric form: (4*&apos; +.   !! # #!#1 &apos;  &quot;:&lt;=; *?@&gt; . @ of D into the interval O ! .where ;=* JK &quot;  @ isisa thelineartruncationweight vector and B @ is a constant."
Parameters @ and B @ can be estimated from the training data.
This classification method is based on approxi-mately minimizing the risk function.
The general-ized Winnow method used[REF_CITE]implements such a robust risk minimization method.
We compared the performance of the four classi-fiers by training and testing them on the same data sets.
"We divided the IBM-FBIS corpus into three subsets: 2.8 million characters for training, 330,000 characters for development testing, and 330,000 characters for testing."
Table 2 shows the results of each classifier for the development test set and the evaluation set.
"The RRM and fnTBL classifiers are the best performers for the test set, followed by Max-Ent."
The HMM classifier lags behind by around 6 points in F-measure from the best system.
The pre-sented results are for character-based models.
"For comparison, we also computed two baselines: one in which each character is labeled with its most frequent label (Baseline1 in Table 2), and one in which each entity that was seen in training data is labeled with its most frequent classification (Base-line2 in Table 2 - this baseline is computed using the software provided with the[REF_CITE]shared task (Tjong[REF_CITE]))."
"The four classifiers differ in multiple dimensions, making them good candidates for combination."
We explored various ways to combine the results from the four classifiers.
"For the first part of the experimental setup, we consider the following classification framework: given the (probabilistic) output RQ( TSU- of V classi-fiers W !#!#!1# WYX , the classifier combination problem can be viewed a probability interpolation problem – compute the class probability distribution condi-tioned on the joint classifier output:"
"Z ^[ `] _acb 5] egd fihkj l Z`m!n![^]`_aRb&amp;] f^o nqp 8e r r rd \ (1) where is the i th classifier’s output, is an ob-servable context (e.g., a word trigram) and s is a combination function."
A commonly used combining scheme is through linear interpolation of the classi-fiers’ class probability distributions:
X ( Tu-t X wv t x- ( (t* X  ( Tx- &quot;z * (2)
"The weights z { encode the importance given to classifier in combination, for the context , and ( u-  is an estimation of the probability that the correct classification is , given that the output of the classifier on context is ."
"These param-eters from Equation (2) can be estimated, if needed, on development data."
"Table 3 presents the combination results, for different ways of estimating the interpolation pa-rameters."
"A simple combination method is the equal voting method (van[REF_CITE]; Tjong[REF_CITE]), where the parameters are } computed } as z *E| X and ( Tu- | , being the Kronecker operator: } *. : . &lt;D~ C. &lt; D"
"In other words, each of the classifiers votes with equal weight for the class that is most likely under its model, and the class receiving the largest num-ber of votes wins (i.e., it is selected as the classifi-cation output)."
"However, this procedure may lead to ties, where some classifications receive an identical number of votes – one usually resorts to randomly selecting one of the tied candidates in this case – Ta-ble 3 presents the average results obtained by this method, together with the variance obtained over 30 trials."
"To make the decision deterministically, the weights associated with the classifiers can be cho-sen as z *~  (  QgQ Q ."
"In this method, presented in Table 3 as weighted voting, better per-forming classifiers will have a higher impact on the final classification."
"In the previously described methods, also known as voting, each classifier gave its entire vote to one classification – its own output."
"However, Equa-tion (2) allows for classifiers to give partial credit to alternative classifications, through the probabil-ity ( u- ."
"In the experiments described here, this value is computed directly on the development data."
"However, the space of possible choices for , and is large enough to make the estimation unreliable, so we use two approximations, named Model 1 and Model 2 in Table 3: ( u-  ( u-&quot; 2 and ( u- )( Tx- , respec-tively."
Both probability distributions are estimated as smoothed relative frequencies on the develop-ment data.
"Interestingly, both methods underper-form the equal voting method, a fact which can be explained by inspecting the results in Table 2: the fnTBL method has an accuracy (computed on devel-opment data) lower than the MaxEnt accuracy, but it outperforms the latter on the test data."
"Since the parameters ( x- are computed on the devel-opment data, they are probably favoring the Max-Ent method, resulting in lower performance."
"On the other hand, the equal voting method does not suffer from this problem, as its parameters are not depen-dent on the development data."
"In a last set of exper-iments, we extend the classification framework to a larger space, in which we compute the conditional class probability distribution conditioned on an ar-bitrary set of features: ( u- sY   ( u- sY ([Footnote_3])  X"
3 Measured as  h|  .
"This setup allows us to still use the classifications of individual systems as features, but also allows for other types of conditioning features – for instance, one can use the output of any classifier (e.g., POS tags, text chunk labels, etc) as features."
"In the described experiments, we use the RRM method to compute the function s in Equation ([Footnote_3]), allowing the system to select a good performing combination of features."
3 Measured as  h|  .
"At training time, the sys-tem was fed the output of each classifier on the de-velopment data, and, in subsequent experiments, the system was also fed a flag stream which briefly iden-tifies some of the tokens (numbers, romanized char- acters, etc) and the output of each system in a differ-ent NE encoding scheme."
"In all the voting experiments, the NEs were en-coded in an IOB1 scheme, since it seems to be the most amenable to combination."
"Briefly, the IOB general encoding scheme associates a label with each word, corresponding to whether it begins a spe-cific entity, continues the entity, or is outside any en-tity."
Tjong[REF_CITE]describes in detail the IOB schemes.
The final experiment also has access to the output of systems trained in the IOB[Footnote_2] encoding.
2 a` a is the word associated with the context .
"The addition of each feature type resulted in better performance, with the final result yielding a 10% relative decrease of F-measure error when compared with the best performing system 3 ."
"Table 3 also includes an upper-bound on the classi-fier combination performance - the performance of the switch oracle, which selects the correct classifi-cation if at least one classifier outputs it."
"Table 3 shows that, at least for the examined types of combination, using a robust feature-based classifier to compute the classification distribution yields better performance than combining the clas-sifications through either voting or weighted inter-polation."
"The RRM-based classifier is able to in-corporate heterogenous information from multiple sources, obtaining a 2.8 absolute F-measure im-provement versus the best performing classifier and 1.0 F-measure gain over the next best method."
"Specifically, it uses a character-based trigram model for the class person, a word-based model for the class location, and a more complicated model for the class organi-zation."
"This decision is consistent with our observa-tion that the character-based model performs better than the word-based model for classes such as per-son, but is worse for classes such as organization."
It presents the re-sults of 15 systems that participated in an evalua-tion project for Information Retrieval and Informa-tion Extracti[REF_CITE].
"A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers."
This is similar in spirit to our application of the RRM classifier for combining classifier outputs.
"Classifier combination has been shown to be effective in improving the perfor-mance of NLP applications, and have been investigated[REF_CITE]and van[REF_CITE]for part-of-speech tag-ging, Tjong[REF_CITE]for base noun phrase chunking, and[REF_CITE]for word sense disambiguation."
"Among the investigated techniques were voting, probability interpolation, and classifier stacking."
We also applied the classifier combination technique discussed in this paper to English and German[REF_CITE].
"In this paper, we discuss two topics related to Chi-nese NE recognition: dealing with language-specific issues such as word segmentation, and combining multiple classifiers to enhance the system perfor-mance."
"In the described experiments, the character-based model consistently outperforms the word-based model – one major reason for this fact is that the segmentation granularity might not be suited for this particular task."
"Combining four statistical clas-sifiers, including a hidden Markov model classifier, a transformation-based learning classifier, a maxi-mum entropy classifier, and a robust risk minimiza-tion classifier, significantly improves the system per-formance, yielding a 10% relative reduction in F-measure error over the best performing classifier."
We explore how virtual examples (artifi-cially created examples) improve perfor-mance of text classification with Support Vector Machines (SVMs).
We propose techniques to create virtual examples for text classification based on the assump-tion that the category of a document is un-changed even if a small number of words are added or deleted.
We evaluate the pro-posed methods[REF_CITE]test set collection.
"Experimental results show vir-tual examples improve the performance of text classification with SVMs, especially for small training sets."
Corpus-based supervised learning is now a stan-dard approach to achieve high-performance in nat-ural language processing.
"However, the weakness of supervised learning approach is to need an anno-tated corpus, the size of which is reasonably large."
"Even if we have a good supervised-learning method, we cannot get high-performance without an anno-tated corpus."
The problem is that corpus annota-tion is labor intensive and very expensive.
"In or-der to overcome this, several methods are proposed, including minimally-supervised learning methods (e.g.,[REF_CITE]), and active learning methods (e.g.,[REF_CITE])."
The spirit behind these methods is to utilize precious labeled examples max-imally.
Another method following the same spirit is one using virtual examples (artificially created exam-ples) generated from labeled examples.
This method has been rarely discussed in natural language pro-cessing.
"In terms of active learning,[REF_CITE]mentioned the use of virtual examples in text classification."
"They did not, however, take forward this approach because it did not seem to be possi-ble that a classifier created virtual examples of doc-uments in natural language and then requested a hu-man teacher to label them."
"In the field of pattern recognition, some kind of virtual examples has been studied."
"The first re-port of methods using virtual examples with Sup-port Vector Machines (SVMs) is that[REF_CITE], who demonstrated significant improve-ment of the accuracy in hand-written digit recogni-tion (Section 3)."
"They created virtual examples from labeled examples based on prior knowledge of the task: slightly translated (e.g., 1 pixel shifted to the right) images have the same label (class) of the orig-inal image."
"The purpose of this study is to explore the effec-tiveness of virtual examples in NLP, motivated by the results[REF_CITE]."
"To our knowl-edge, use of virtual examples in corpus-based NLP has never been studied so far."
"It is, however, im-portant to investigate this approach by which it is expected that we can alleviate the cost of corpus an-notation."
"In particular, we focus on virtual examples with Support Vector Machines, introduced[REF_CITE]."
The reason for this is that SVM is one of most successful machine learning methods in NLP.
"For example, NL tasks to which SVMs have been applied are text classificati[REF_CITE], chunking[REF_CITE], dependency analysis[REF_CITE]and so forth."
"In this study, we choose text classification as a first case of the study of virtual examples in NLP be-cause text classification in real world requires mini-mizing annotation cost, and it is not too complicated to perform some non-trivial experiments."
"Moreover, there are simple methods, which we propose, to gen-erate virtual examples from labeled examples (Sec-tion 4)."
"We show how virtual examples can improve the performance of a classifier with SVM in text classification, especially for small training sets."
In this section we give some theoretical definitions of SVMs.
Assume that we are given the training data ( x ; y ; : : : ; ( x ; y ; x 2 R ; y 2 f +1 ; 1 g : n i i ) l l ) i i
"The decision function g in SVM framework is de-fined as: g ( x ) = sgn( f ( x )) ([Footnote_1]) f ( x ) = X y i i K ( x ; x l i ) + b (2) i =1 where K is a kernel function, b 2 R is a threshold, and i are weights."
"1 We discuss here only virtual examples which are generated from labeled examples. We do not consider examples, the labels of which are not known."
"Besides, the weights i satisfy the following constraints:"
C and X l 8 i : 0 i y i = 0 ; i i =1 where C is a misclassification cost.
The vectors x i with non-zero i are called Support Vectors.
"For linear SVMs, the kernel function K is defined as:"
K ( x ; x i ) = x x : i
"In this case, Equation 2 can be written as: f ( x ) = w x + b (3) where w = P l y x i =1 i i i ."
To train an SVM is to find i and b by solving the following optimization problem:
X 1 X i j y i y j K ( x ; x l l maximize i j ) i 2 i =1 i;j =1 C and X l subject to 8 i : 0 i y i = 0 : i i =1
"The solution gives an optimal hyperplane, which is a decision boundary between the two classes."
Figure 1 illustrates an optimal hyperplane and its support vec-tors.
Virtual examples are generated from labeled exam-ples. 1
"Based on prior knowledge of a target task, the label of a generated example is set to the same value as that of the original example."
"For example, in hand-written digit recognition, virtual examples can be created on the assumption that the label of an example is unchanged even if the example is shifted by one pixel in the four princi-pal directions[REF_CITE]."
Virtual examples that are generated from support vectors are called virtual support vectors[REF_CITE].
Reasonable virtual support vectors are expected to give a better optimal hyperplane.
"As-suming that virtual support vectors represent natu-ral variations of examples of a target task, the de-cision boundary should be more accurate."
Figure 2 illustrates the idea of virtual support vectors.
"Note that after virtual support vectors are given, the hy-perplane is different from that in Figure 1."
We assume on text classification the following:
The category of a document is un-changed even if a small number of words are added or deleted.
This assumption is reasonable.
In typical cases of text classification most of the documents usually contain two or more keywords which may indicate the categories of the documents.
"Following Assumption 1, we propose two meth-ods to create virtual examples for text classification."
One method is to delete some portion of a document.
The label of a virtual example is given from the orig-inal document.
The other method is to add a small number of words to a document.
"The words to be added are taken from documents, the label of which is the same as that of the document."
"Although one can invent various methods to create virtual exam-ples based on Assumption 1, we propose here very simple ones."
"Before describing our methods, we describe text representation which we used in this study."
"We to-kenize a document to words, downcase them and then remove stopwords, where the stopword list of freeWAIS-sf [Footnote_2] is used."
Stemming is not performed.
We adopt binary feature vectors where word fre-quency is not used.
Now we describe the two proposed methods: GenerateByDeletion and GenerateByAddition.
As-sume that we are given a feature vector (a document) x and x 0 is a generated vector from x .
GenerateBy-Deletion algorithm is: 1. Copy x to x 0 . 2.
"For each binary feature f of x 0 , if rand () t then remove the feature f , where rand () is a function which generates a random number from 0 to 1 , and t is a parameter to decide how many features are deleted."
"For example, suppose that we have a set of docu-ments as in Table 1."
"Some possible virtual examples generated from Document 1 by GenerateByDeletion algorithm are ( f 2 ; f 3 ; f 4 ; f 5 ; +1) , ( f 1 ; f 3 ; f 4 ; +1) , or ( f 1 ; f 2 ; f 4 ; f 5 ; +1) ."
"On the other hand, GenerateByAddition algo-rithm is: 1."
"Collect from a training set documents, the label of which is the same as that of x . 2."
Concatenate all the feature vectors (documents) to create an array a of features.
Each element of a is a feature which represents a word. 3. Copy x to x 0 . 4.
"For each binary feature f of x 0 , if rand () t then select a feature randomly from a and put it to x 0 ."
"For example, when we want to generate a virtual example from Document 2 in Table 1 by Generate-ByAddition algorithm, first we create an array a = ( f 1 ; f 2 ; f 3 ; f 4 ; f 5 ; f 2 ; f 4 ; f 5 ; f 6 ; f 2 ; f 3 ; f 5 ; f 6 ; f 7) ."
"In this case, some possible virtual examples by GenerateByAddition are ( f 1 ; f 2 ; f 4 ; f 5 ; f 6 ; +1) , ( f 2 ; f 3 ; f 4 ; f 5 ; f 6 ; +1) , or ( f 2 ; f 4 ; f 5 ; f 6 ; f 7 ; +1) ."
An example such as ( f 2 ; f 4 ; f 5 ; f 6 ; f 10 ; +1) is never generated from Document 2 because there are no positive documents which have f 10 .
We used the[REF_CITE]dataset [Footnote_3] to evaluate the proposed methods.
3 Available from David D. Lewis’s page[URL_CITE]
The dataset has several splits of a training set and a test set.
"We used here “ModApte” split, which is most widely used in the literature on text classification."
"This split has 9,603 training ex-amples and 3,299 test examples."
"We use, however, only the most frequent 10 categories."
Table 2 shows the 10 categories and the number of training and test exam-ples in each of the categories.
We use F-measure (van[REF_CITE]) as a primal performance measure to evaluate the result.
F-measure is defined as: 2) pq F-measure = (1 +2 p + q ([Footnote_4]) where p is precision and q is recall and is a param-eter which decides the relative weight of precision and recall.
4 We first tried t = 0 : 01 ; 0 : 05 ; and 0 : 10 with GenerateBy-Deletion using the 9603 size training set. The value t = 0 : 05 yielded best micro-average F-measure for the test set. We used the same value also for GenerateByAddition.
The p and the q are defined as: p = number of positive and correct outputs number of positive outputs q = number of positive and correct outputs number of positive examples
"In Equation 4, usually = 1 is used, which means it gives equal weight to precision and recall."
"When we evaluate the performance of a classifier to a multiple category dataset, there are two ways to compute F-measure: macro-averaging and micro-averaging[REF_CITE]."
"The former way is to first compute F-measure for each category and then aver-age them, while the latter way is to first compute pre-cision and recall for all the categories and use them to calculate the F-measure."
"Through our experiments we used our original SVM tools, the algorithm of which is based on SMO (Se-quential Minimal Optimization)[REF_CITE]."
"We used linear SVMs and set a misclassification cost C to 0 : 016541 which is 1 = ( the average of x x ) where x is a feature vector in the 9,603 size training set."
"For simplicity, we fixed C through all the experi-ments."
We built a binary classifier for each of the 10 categories shown in Table 2.
"First, we carried out experiments using GenerateBy-Deletion and GenerateByAddition separately to cre-ate virtual examples, where a virtual example was created per Support Vector."
We did not generate virtual examples from non support vectors.
We set the parameter t to 0 : 05 4 for GenerateByDeletion and GenerateByAddition for all the experiments.
To build an SVM with virtual examples we use the following steps: 1. Train an SVM. 2.
Extract Support Vectors. 3. Generate virtual examples from the Support Vectors. 4. Train another SVM using both the original la-beled examples and the virtual examples.
We evaluated the performance of the two methods depending on the size of a training set.
We created subsamples by selecting randomly from the 9603 size training set.
"We prepared seven sizes: 9603, 4802, 2401, 1200, 600, 300, and 150. [Footnote_5] Micro-average F-measures of the two methods are shown in Table 3."
"5 Since we selected samples randomly, some smaller training sets of low frequent categories may have had few or even zero positive examples."
We see from Table 3 that both the meth-ods give better performance than that of the origi-nal SVM.
"The smaller the number of examples in the training set is, the larger the gain is."
These results suggest that in the smaller training sets there are not enough various examples to give a accurate decision boundary and therefore the effect of virtual examples is larger at the smaller training sets.
It is reasonable to conclude that GenerateByDeletion and GenerateByAddition generated good virtual ex-amples for the task and this led to the performance gain.
"After we found that the simple two methods to generate virtual support vectors were effective, we examined a combined method which is to use both GenerateByDeletion and GenerateByAddition."
Two virtual examples are generated per Support Vector.
The performance of the combined method is also shown in Table 3.
The performance gain of the com-bined method is larger than that with either Gener-ateByDeletion or GenerateByAddition.
"Furthermore, we carried out another experiment with a combined method to create two virtual exam-ples with GenerateByDeletion and GenerateByAd-dition respectively."
"That is, four virtual examples were generated from a Support Vector."
The perfor-mance of that setting is shown in Table 3.
The best result is achieved by the combined method to create four virtual examples per Support Vector.
"For the rest of this section, we limit our discussion to the comparison of the results of the original SVM and SVM with four virtual examples per SV (SVM with 4 VSVs)."
The learning curves of the original SVM and SVM with 4 VSVs are shown in Figures 3 and 4.
It is clear that SVM with 4 VSVs outper-forms the original SVM considerably in terms of both micro-average F-measure and macro-average F-measure.
SVM with 4 VSVs achieves a given level of performance with roughly half of the labeled examples which the original SVM requires.
One might suppose that the improvement of F-measure is realized simply because the recall gets highly improved while the error rate increases.
We plot changes of the error rate for 32990 tests (3299 tests for each of the 10 categories) in Figure 5.
SVM with 4 VSVs still outperforms the original SVM signifi-cantly. [Footnote_6]
"6 We have done the significance test which is called “p-test”[REF_CITE], requiring significance at the 0.05 level. Although at the 9603 size training set the improvement of the error rate is not statistically significant, in all the other cases the improvement is significant."
The performance changes for each of the 10 cat-egories are shown in Tables 4 and 5.
SVM with 4 VSVs is better than the original SVM for almost all the categories and all the sizes except for “inter-est” and “wheat” at the 9603 size training set.
"For low frequent categories such as “ship”, “wheat” and “corn”, the classifiers of the original SVM perform poorly."
"There are many cases where they never out-put ‘positive’, i.e. the recall is zero."
It suggests that the original SVM fails to find a good hyperplane due to the imbalanced training sets which have very few positive examples.
"In contrast, SVM with 4 VSVs yields better results for such harder cases."
We have explored how virtual examples improve the performance of text classification with SVMs.
"For text classification, we have proposed methods to cre-ate virtual examples on the assumption that the label of a document is unchanged even if a small num-ber of words are added or deleted."
"The experimen-tal results have shown that our proposed methods improve the performance of text classification with SVMs, especially for small training sets."
"Although the proposed methods are not readily applicable to NLP tasks other than text classification, it is notable that the use of virtual examples, which has been very little studied in NLP, is empirically evaluated."
"In the future, it would be interesting to employ virtual examples with methods to use both labeled and unlabeled examples (e.g.,[REF_CITE])."
The combined approach may yield better results with a small number of labeled examples.
"Another interest-ing direction would be to develop methods to create virtual examples for the other tasks (e.g., named en-tity recognition, POS tagging, and parsing) in NLP."
We believe we can use prior knowledge on these tasks to create effective virtual examples.
"In this paper, experiments on automatic extraction of keywords from abstracts us-ing a supervised machine learning algo-rithm are discussed."
"The main point of this paper is that by adding linguistic know-ledge to the representation (such as syn-tactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as mea-sured by keywords previously assigned by professional indexers."
"In more detail, ex-tracting NP -chunks gives a better preci-sion than n-grams, and by adding the P O S tag(s) assigned to the term as a feature, a dramatic improvement of the results is ob-tained, independent of the term selection approach applied."
"Automatic keyword assignment is a research topic that has received less attention than it deserves, con-sidering keywords’ potential usefulness."
"Keywords may, for example, serve as a dense summary for a document, lead to improved information retrieval, or be the entrance to a document collection."
"However, relatively few documents have keywords assigned, and therefore finding methods to automate the as-signment is desirable."
"A related research area is that of terminology ex-traction (see e.g.,[REF_CITE]), where all terms describing a domain are to be extracted."
"The aim of keyword assignment is to find a small set of terms that describes a specific document, in-dependently of the domain it belongs to."
"However, the latter may very well benefit from the results of the former, as appropriate keywords often are of a terminological character."
"In this work, the automatic keyword extraction is treated as a supervised machine learning task, an ap-proach first proposed[REF_CITE]."
"Two im-portant issues are how to define the potential terms, and what features of these terms are considered dis-criminative, i.e., how to represent the data, and con-sequently what is given as input to the learning al-gorithm."
"In this paper, experiments with three term selection approaches are presented: n-grams; noun phrase ( NP ) chunks; and terms matching any of a set of part-of-speech (P O S) tag sequences."
"Four dif-ferent features are used: term frequency, collection frequency, relative position of the first occurrence, and the P O S tag(s) assigned to the term."
Treating the automatic keyword extraction as a su-pervised machine learning task means that a clas-sifier is trained by using documents with known keywords.
The trained model is subsequently ap-plied to documents for which no keywords are as-signed: each defined term from these documents is classified either as a keyword or a non-keyword; or—if a probabilistic model is used—the probabil-ity of the defined term being a keyword is given.
"The terms are all stemmed uni-grams, bigrams, and trigrams from the documents, after stopword removal."
"The features used are, for example, the frequency of the most frequent phrase component; the relative number of characters of the phrase; the first relative occurrence of a phrase com-ponent; and whether the last word is an adjective, as judged by the unstemmed suffix."
Turney reports that the genetic algorithm outputs better keywords than the decision trees.
Part of the same training and test material is later used[REF_CITE]for evaluating their algorithm in relation to Turney’s al-gorithm.
"This algorithm, which is based on naive Bayes, uses a smaller and simpler set of features— term frequency, collection frequency (idf), and rel-ative position—although it performs equally well."
"Frank et al. also discuss the addition of a fourth fea-ture that significantly improves the algorithm, when trained and tested on domain-specific documents."
This feature is the number of times a term is assigned as a keyword to other documents in the collection.
"It should be noted that the performance of the state-of-the-art keyword extraction is much lower than for many other NLP -tasks, such as tagging and parsing, and there is plenty of room for improve-ments."
"To give an idea of this, the results obtained by the genetic algorithm trained[REF_CITE], and the naive Bayes approach[REF_CITE]are presented."
The number of terms assigned must be explicitly limited by the user for these algorithms.
Turney and Frank et al. report the precision for five and fifteen keywords per document.
Recall is not re-ported in their studies.
"In Table 1 their results when training and testing on journal articles are shown, and the highest values for the two algorithms are pre-sented."
There are two drawbacks in common with the approaches proposed[REF_CITE]and[REF_CITE].
"First, the number of tokens in a keyword is limited to three."
"In the data used to train the classifiers evaluated in this paper, 9.1% of the manually assigned keywords consist of four tokens or more, and the longest keywords have eight tokens."
"Secondly, the user must state how many keywords to extract from each document, as both algorithms, for each potential keyword, output the probability of the term being a keyword."
"This could be solved by manually setting a threshold value for the probabil-ity, but this decision should preferably be made by the extraction system."
Finding potential terms—when no machine learn-ing is involved in the process—by means of P O S patterns is a common approach.
"For exam-ple,[REF_CITE]discuss an al-gorithm where the number of words and the fre-quency of a noun phrase, as well as the fre-quency of the head noun is used to determine what terms are keywords."
"An extraction sys-tem called LinkIT (see e.g.,[REF_CITE]) compiles the phrases having a noun as the head, and then ranks these according to the heads’ fre-quency."
The final example given in this paper is[REF_CITE]who apply statistical filters on the extracted noun phrases.
In that study it is con-cluded that term frequency is the best filter candi-date of the scores investigated.
"When P O S patterns are used to extract potential terms, the problem lies in how to restrict the number of terms, and only keep the ones that are relevant."
"In the case of professional indexing, the terms are normally limited to a domain-specific thesaurus, but not to those present only in the document to which they are assigned."
"For example,[REF_CITE]presents work where as a first step, all lemmas after stop word removal in a document are ranked accord-ing to the log-likelihood ratio, thus a list of content descriptors is obtained."
"These terms are then used to assign thesaurus terms, that have been automati-cally assigned associating lemmas during a training phase."
"In this paper, however, the concern is not to limit the terms to a set of allowed terms."
"As opposed[REF_CITE]and[REF_CITE], who experiment with keyword extraction from full-length texts, this work concerns keyword extraction from abstracts."
"The reason for this is that many journal papers are not available as full-length texts, but as abstracts only, as is the case for example on the Internet."
The starting point for this work was to examine whether the data representation suggested by Frank et al. was adequate for constructing a keyword ex-traction model from and for abstracts.
"As the results were poor, two alternatives to extracting n-grams as the potential terms were explored."
The first ap-proach was to extract all noun phrases in the docu-ments as judged by an NP -chunker.
"The second se-lection approach was to define a set of P O S tag se-quences, and extract all words or sequences of words that matched any of these, relying on a PoS tag-ger."
"These two different approaches mean that the length of the potential terms is not limited to some-thing arbitrary, but reflects a linguistic property."
The solution to limiting the number of terms—as the majority of the extracted words or phrases are not keywords—was to apply a machine learning algo-rithm to decide which terms are keywords and which are not.
"The output from the machine learning algo-rithm is binary (a term is either a keyword or not), consequently the system itself limits the amount of extracted keywords per document."
"As for the fea-tures, a fourth feature was added to the ones used by Frank et al., namely the P O S tag(s) assigned to the term."
This feature turned out to dramatically im-prove the results.
"The collection used for the experiments described in this paper consists of 2 000 abstracts in En-glish, with their corresponding title and keywords from the Inspec database."
"The abstracts are from the years 1998 to 2002, from journal papers, and from the disciplines Computers and Control, and In-formation Technology."
"Each abstract has two sets of keywords—assigned by a professional indexer— associated to them: a set of controlled terms, i.e., terms restricted to the Inspec thesaurus; and a set of uncontrolled terms that can be any suitable terms."
Both the controlled terms and the uncontrolled terms may or may not be present in the abstracts.
"However, the indexers had access to the full-length documents when assigning the keywords."
"For the experiments described here, only the uncontrolled terms were considered, as these to a larger extent are present in the abstracts (76.2% as opposed to 18.1%)."
"The set of abstracts was arbitrarily divided into three sets: a training set (to construct the model) consisting of 1 000 documents, a validation set (to evaluate the models, and select the best perform-ing one) consisting of 500 documents, and a test set (to get unbiased results) with the remaining 500 abstracts."
The set of manually assigned keywords were then removed from the documents.
"For all ex-periments the same training, validation, and test sets were used."
"This section begins with a discussion on the differ-ent ways the data were represented: in Section 4.1 the term selection approaches are described, and in Section 4.2 the features are discussed."
"Thereafter, a brief description of the machine learning approach is given."
"Finally in Section 4.4, the training and the evaluation of the classifiers are discussed."
"In this section, the three different term selection ap-proaches, in other words, the three definitions of what constitutes a term in a document, are described. n-grams"
"In a first set of runs, the terms were de-fined in a manner similar[REF_CITE]and[REF_CITE]. (Their studies were introduced in Section 2.)"
"All unigrams, bigrams, and trigrams were extracted."
"Thereafter a stoplist was used ([REF_CITE]), where all terms beginning or ending with a stopword were removed."
Finally all remain-ing tokens were stemmed using Porter’s stemmer[REF_CITE].
"In this paper, this manner of selecting terms is referred to as the n-gram approach."
The implementation differs[REF_CITE]in the following aspects:
"Only non-alphanumeric characters that were not present in any keyword in the training set were removed (keeping e.g., C++)."
"Numbers were removed only if they stood sep-arately (keeping e.g., 4YourSoul.com)."
Proper nouns were kept.
The stemming and the stoplist applied were dif-ferent.
The stems were kept even if they appeared only once (which is true for 80.0% of the keywords present in the training set).
"Initially, the same features th[REF_CITE]used for their domain-independent experiments were used."
Relative position of the first occurrence (the proportion of the document preceding the first occurrence).
"The representation differed in that the term fre-quency and the collection frequency were not weighted together, but kept as two distinct features."
"In addition, the real values were not discretised, only rounded off to two decimals, thus more decision-making was handed over to the algorithm."
The col-lection frequency was calculated for the three data sets separately.
"In addition, experiments with a fourth feature were performed."
This is the P O S tag or tags as-signed to the term by the same partial parser used for finding the chunks and the tag patterns.
"When a term consists of several tokens, the tags are treated like a sequence."
"As an example, an extracted phrase like random JJ excitations NNS gets the atomic fea-ture value JJ NNS ."
"In case a term occurs more than once in the document, the tag or tag sequence as-signed is the most frequently occurring one for that term in the entire document."
"In case of a draw, the first occurring one is assigned."
"As usual in machine learning, the input to the learn-ing algorithm consists of examples, where an exam-ple refers to the feature value vector for each, in this case, potential keyword."
"An example that is a manual keyword is assigned the class positive, and those that are not are given the class negative."
"The machine learning approach used for the experiments is that of rule induction, i.e., the model that is con-structed from the given examples, consists of a set of rules [Footnote_2] ."
2 The system is Rule Discovery System from Compumine AB.[URL_CITE]
"The strategy used to construct the rules is re-cursive partitioning (or divide-and-conquer), which has as the goal to maximise the separation between the classes for each rule."
"The system used allows for different ensemble techniques to be applied, meaning that a number of classifiers are generated and then combined to pre-dict the class."
The one used for these experiments is bagging[REF_CITE].
"In bagging, examples from the training data are drawn randomly with re-placement until a set of the original size is obtained."
This new set is then used to train a classifier.
This procedure is repeated n times to generate n classi-fiers that then vote to classify an instance.
It should be noted that my intention is not to ar-gue for this machine learning approach in favour of any other.
"However, one advantage with rules is that they may be inspected, and thus might give an in-sight into how the learning component makes its de-cisions, although this is less applicable when apply-ing ensemble techniques."
"The feature values were calculated for each ex-tracted unit in the training and the validation sets, that is for the n-grams, NP -chunks, stemmed NP -chunks, patterns, and the stemmed patterns respec-tively."
"In other words, the within-document fre-quency, the collection frequency, and the proportion of the document preceding the first appearance for each potential term were calculated."
"Also, the P O S tag(s) for each term were extracted."
"In addition, as the machine learning approach is supervised, the class was added, i.e., whether the term is a manually assigned keyword or not."
"For the stemmed terms, a unit was considered a keyword if it was equal to a stemmed manual keyword."
"For the unstemmed terms, the term had to match exactly."
"The measure used to evaluate the results on the validation set was the F-score, defined as    ! #&quot; %$&apos;(&amp; &amp; )* +,&quot;#)$-&amp;(&amp; combining the precision and the recall obtained."
"In this study, the main concern is the precision and the recall for the examples that have been assigned the class positive, that is how many of the suggested keywords are correct (precision), and how many of the manually assigned keywords that are found (re-call)."
"As the proportion of correctly suggested key-words is considered equally important as the amount of terms assigned by a professional indexer that was detected, was assigned the value 1, thus giving precision and recall equal weights."
"When calculating the recall, the value for the to-tal number of manually assigned keywords present in the documents is used, independent of the num-ber actually present in the different representations."
"This figure varies slightly for the unstemmed and the stemmed data, and for the two the correspond-ing value is used."
"Several runs were made for each representation, with the goal to maximise the performance as eval-uated on the validation set: first the weights of the positive examples were adjusted, as the data set is unbalanced."
A better performance was obtained when the positive examples in the training data out-numbered the negative ones.
"Thereafter experiments with bagging were performed, and also, runs with and without the P OS tag feature were made."
The results are presented next.
"In this section, the results obtained by the best per-forming model for each approach—as judged on the validation set—when run on the previously unseen test set are presented."
"It should, however, be noted that the number of possible runs is very large, by varying for example the number of classifiers gen-erated by the ensemble technique."
It might well be that better results are possible for any of the repre-sentations.
"As stemming with few exceptions led to better re-sults on the validation set over all runs, only these values are presented in this section."
"In Table 2, the number of assigned terms and the number of cor-rect terms, in total and on average per document are shown."
"Also, precision, recall, and the F-score are presented."
"For each approach, both the results with and without the P O S tag feature are given."
The length of the abstracts in the test set varies from 338 to 23 tokens (the median is 121 tokens).
The number of uncontrolled terms per document is 31 to 2 (the median is 9 keywords).
"The total number of stemmed keywords present in the stemmed test set is 3 816, and the average number of terms is 7.63."
"Their distribution over the 500 documents is 27 to three documents with 0 terms, with the median being 7."
"As for bagging, it was noted that although the accuracy (i.e., the number of correctly classified positive and negative examples divided by the total number of examples) improved when increasing the number of classifiers, the F-score often decreased."
"For the pattern approach without the tag features the best model consists of a 5-bagged classifier, for the pattern approach with the tag feature a 20-bagged, and finally for the n-gram approach with the tag fea-ture a 10-bagged classifier."
For the other three runs a single classifier had the best performance.
"When extracting the terms from the test set accord-ing to the n-gram approach, the data consisted of 42 159 negative examples, and 3 330 positive exam-ples, thus in total 45 489 examples were classified by the trained model."
Using this manner of extracting the terms meant that 12.8% of the keywords origi-nally present in the test set were lost.
"To summarise the n-gram approach (see Table 2), without the tag feature it finds on average 4.37 keywords per document, out of originally on aver-age 7.63 manual keywords present in the abstracts."
"However, the price paid for these correct terms is high: almost 38 incorrect terms per document."
"When adding the fourth feature, the number of cor-rect terms decreases slightly, while the number of incorrect terms is decreased to a third."
"If looking at the actual distribution of assigned terms for these two runs, this varies between 134(!) and 5 without the tag feature, and from 48 to 1 with the tag feature."
The median is 40 and 14 respectively.
The F-scores (F ) for these two runs are 17.6 and 33.9 respectively. 33.9 is the highest F-score that was achieved for the six runs presented here.
"When extracting the terms according to the stemmed chunking approach, the test set consisted of 13 579 negative, and 1 920 positive examples; in total 15 499 examples."
"An F-score (F ) of 22.7 is obtained without the P O S tag feature, and 33.0 with this feature."
"The number of terms on average per document is 16.38 without the tag feature, and 9.58 with it."
"If looking at each document, the number of keywords assigned varies from 46 to 0 (for three documents) with the median 16, and 29 to 0 (for four documents) with the median value being 9 terms."
"Extracting the terms with the chunking approach meant that slightly more than half of the keywords actually present in the test set were lost, and com-pared to the n-gram approach the number of cor-rect terms assigned was almost halved."
"The number of incorrect keywords, however, decreased consider-ably."
"But, the difference is shown when the P O S tag feature is included: the number of correctly assigned terms is more or less the same for this approach with or without the tag feature, while the number of in-correct terms is halved."
"When extracting the terms according to the stemmed pattern approach, the test data consisted of 33 507 examples."
"Of these were 3 340 positive, and 30 167 negative."
"The F-scores (F ) for the two runs, displayed in Table 2, are 25.6 (without the tag feature) and 28.1 (with the tag feature)."
The number of terms assigned on average per document is 5.04 and 3.05 without and with the tag feature respectively.
"The actual number of terms assigned per document is 100 to 0 (for three documents) without the tag feature, and 46 to 0 (for four documents) with the tag feature."
The median is 30 and 12 respectively.
"In this paper I have shown how keyword extrac-tion from abstracts can be achieved by using simple statistical measures as well as syntactic information from the documents, as input to a machine learn-ing algorithm."
"If first considering the term selec- tion approaches, extracting NP -chunks gives a better precision, while extracting all words or sequences of words matching any of a set of P O S tag pat-terns gives a higher recall compared to extracting n-grams."
The highest F-score is obtained by one of the n-gram runs.
The largest amount of assigned terms present in the abstracts are assigned by the pattern approach without the tag feature.
The pat-tern approach is also the approach which keeps the largest number of assigned terms after that the data have been pre-processed.
"Using phrases means that the length of the potential terms is not restricted to something arbitrary, rather the terms are treat as the units they are."
"However, of the patterns that were se-lected for the experiments discussed here none was longer than four tokens."
"If looking at all assigned keywords in the training set, 3.0% are then ruled out as potential terms."
The longest chunks in the test set that were correctly assigned are five tokens long.
"As for when syntactic information is included as a fea-ture (in the form of the P O S tag(s) assigned to the term), it is evident from the results presented in this paper that this information is crucial for assigning an acceptable number of terms per document, indepen-dent of what term selection strategy is chosen."
One shortcoming of the work is that there is cur-rently no relation between the different P O S tag fea-ture values.
"For example, a singular noun has no closer relationship to a plural noun than to an adjec-tive."
"In the future, the patterns should somehow be categorised reflecting their semantics, perhaps in a hierarchical manner, or morphological information could be removed."
"In this paper I have not touched upon the more in-tricate aspects of evaluation, but simply treated the manually assigned keywords as the gold standard."
"This is the most severe way to evaluate a keyword extractor, as many terms might be just as good, al-though for one reason or another not chosen by the human indexer."
Future work will examine alterna-tive approaches to evaluation.
"One possibility for a more liberal evaluation could be to use human evaluators with real information needs, as done[REF_CITE]."
"Another possibility would be to let several persons index each document, thus getting a larger set of acceptable terms to choose from."
"This would hopefully lead to a better precision, while re-call probably would be affected negatively; the im-portance of recall would then need to be reconsid-ered."
"Future work should also go in the direction of generating (as opposed to extracting) keywords, by for example exploring potential knowledge provided by a thesaurus."
This paper describes a method for utterance classification that does not require manual transcription of training data.
The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conven-tional word-trigram recognition requiring man-ual transcription.
"In our method, unsupervised training is first used to train a phone n-gram model for a particular domain; the output of recognition with this model is then passed to a phone-string classifier."
The classification ac-curacy of the method is evaluated on three dif-ferent spoken language system domains.
A major bottleneck in building data-driven speech pro-cessing applications is the need to manually transcribe training utterances into words.
"The resulting corpus of transcribed word strings is then used to train application-specific language models for speech recognition, and in some cases also to train the natural language components of the application."
"Some of these speech processing ap-plications make use of utterance classification, for exam-ple when assigning a call destination to naturally spoken user utterances[REF_CITE], or as an initial step in converting speech to actions in spoken interfaces[REF_CITE]."
In this paper we present an approach to utterance clas-sification that avoids the manual effort of transcribing training utterances into word strings.
"Instead, only the desired utterance class needs to be associated with each sample utterance."
The method combines automatic train-ing of application-specific phonotactic models together with token sequence classifiers.
The accuracy of this phone-string utterance classification method turns out to be surprisingly close to what can be achieved by conven-tional methods involving word-trigram language mod-els that require manual transcription.
"To quantify this, we present empirical accuracy results from three differ-ent call-routing applications comparing our method with conventional utterance classification using word-trigram recognition."
Previous work at AT&amp;T on utterance classification without words used information theoretic metrics to dis-cover “acoustic morphemes” from untranscribed utter-ances paired with routing destinations[REF_CITE].
"However, that approach has so far proved impractical: the major obstacle to practical utility was the low run-time detection rate of acoustic morphemes discovered during training."
This led to a high false rejection rate (be-tween 40% and 50% for 1-best recognition output) when a word-based classification algorithm (the one described by Wright et. al (1997)) was applied to the detected se-quence of acoustic morphemes.
"More generally, previous work using phone string (or phone-lattice) recognition has concentrated on tasks in-volving retrieval of audio or video[REF_CITE]."
"In those tasks, performance of phone-based systems was not comparable to the accuracy obtainable from word-based systems, but rather the rationale was avoiding the difficulty of building wide coverage statistical language models for handling the wide range of subject matter that a typical retrieval system, such as a system for retrieving news clips, needs to cover."
"In the work presented here, the task is somewhat different: the system can automatically learn to identify and act on relatively short phone subse-quences that are specific to the speech in a limited domain of discourse, resulting in task accuracy that is comparable to word-based methods."
In section 2 we describe the utterance classification method.
Section 3 describes the experimental setup and the data sets used in the experiments.
Section 4 presents the main comparison of the performance of the method against a “conventional” approach using manual tran-scription and word-based models.
Section 5 gives some concluding remarks.
The runtime operation of our utterance classification method is simple.
It involves applying two models (which are trained as described in the next subsection): A statistical n-gram phonotactic model and a phone string classification model.
"At runtime, the phonotactic model is used by an automatic speech recognition system to con-vert a new input utterance into a phone string which is mapped to an output class by applying the classification model. (We will often refer to an output class as an “ac-tion”, for example transfer to a specific call-routing des-tination)."
The configuration at runtime is as shown in Figure 1.
More details about the specific recognizer and classifier components used in our experiments are given in the Section 3.
The classifier can optionally make use of more infor-mation about the context of an utterance to improve the accuracy of mapping to actions.
"As noted in Figure 1, in the experiments presented here, we use a single addi-tional feature as a proxy for the utterance context, specif-ically, the identity of the spoken prompt that elicited the utterance."
"It should be noted, however, that inclusion of such additional information is not central to the method: Whether, and how much, context information to include to improve classification accuracy will depend on the ap-plication."
"Other candidate aspects of context may include the dialog state, the day of week, the role of the speaker, and so on."
Training is divided into two phases.
"First, train a phone n-gram model using only the training utterance speech files and a domain-independent acoustic model."
"Second, train a classification model mapping phone strings and prompts (the classifier inputs) to actions (the classifier outputs)."
The recognition training phase is an iterative proce-dure in which a phone n-gram model is refined succes-sively: The phone strings resulting from the current pass over the speech files are used to construct the phone n-gram model for the next iteration.
"In other words, this is a “Viterbi re-estimation” or “1-best re-estimation” pro-cess."
"We currently only re-estimate the n-gram model, so the same general-purpose HMM acoustic model is used for ASR decoding in all iterations."
"Other more expen-sive n-gram re-estimation methods can be used instead, including ones in which successive n-gram models are re-estimated from n-best or lattice ASR output."
Candi-dates for the initial model used in this procedure are an unweighted phone loop or a general purpose phonotactic model for the language being recognized.
The steps of the training process are as follows. (The procedure is depicted in Figure 2.) 1. Set the phone string model G to an initial phone string model.
"Initialize the n-gram order N to 1. (Here ‘order’ means the size of the n-grams, so for example 2 means bi-grams.) 2."
"Set S to the set of phone strings resulting from rec-ognizing the training speech files with G (after pos-sibly adjusting the insertion penalty, as explained below). 3. Estimate an n-gram model G 0 of order N from the set of strings S. 4."
"If N &lt; N max , set N ← N + 1 and G ← G 0 and go to step 2, otherwise continue with step 5. 5."
"For each recognized string s ∈ S, construct a clas-sifier input pair (s,r) where r is the prompt that elicited the utterance recognized as s. 6. Train a classification model M to generalize the training function f : (s,r) → a, where a is the action associated with the utterance recognized as s. 7. Return the classifier model M and the final n-gram model G 0 as the results of the training procedure."
"Instead of increasing the order N of the phone n-gram model during re-estimation, an alternative would be to iterate N max times with a fixed n-gram order, possibly with successively increased weight being given to the lan-guage model vs. the acoustic model in ASR decoding."
One issue that arises in the context of unsupervised recognition without transcription is how to adjust recog-nition parameters that affect the length of recognized strings.
"In conventional training of recognizers from word transcriptions, a “word insertion penalty” is typ-ically tuned after comparing recognizer output against transcriptions."
"To address this issue, we estimate the ex-pected speaking rate (in phones per second) for the rele-vant type of speech (human-computer interaction in these experiments)."
The token insertion penalty of the recog-nizer is then adjusted so that the speaking rate for auto-matically detected speech in a small sample of training data approximates the expected speaking rate.
Three collections of utterances from different domains were used in the experiments.
Domain A is the one stud-ied in previously cited experiments[REF_CITE].
Ut-terances for domains B and C are from similar interactive spoken natural language systems.
The utterances being classified are the cus-tomer side of live English conversations between AT&amp;T residential customers and an automated customer care system.
This system is open to the public so the num-ber of speakers is large (several thousand).
The average length of an utterance was 11.29 words.
The split between training and test utterances was such that the ut-terances from a particular call were either all in the train-ing set or all in the test set.
"Some utterances had more than one action associated with them, the average number of actions as-sociated with an utterance being 1.09."
This is a database of utterances from an in-teractive spoken language application relating to product line information.
The average length of an utter-ance was 3.95 words.
"Some utterances had more than one action associ-ated with them, the average number of actions associated with an utterance being 1.23."
"Domain C. This is a database of utterances from an interactive spoken language application relating to con-sumer order transactions (reviewing order status, etc.) in a limited domain."
The average length of an utter-ance was 8.88 words.
"Some utterances had more than one action associ-ated with them, the average number of actions associated with an utterance being 1.07."
"The same acoustic model was used in all the experiments reported here, i.e. for experiments with both the phone-based and word-based utterance classifiers."
This model has 42 phones and uses discriminatively trained 3-state[REF_CITE]Gaussians per state.
It uses feature space transformations to reduce the feature space to 60 fea-tures prior to discriminative maximum mutual informa-tion training.
"This acoustic model was trained by Andrej Ljolje and is similar to the baseline acoustic model used for experiments with the Switchboard corpus, an earlier version of which is described[REF_CITE]. (Like the model used here, the baseline model in those experiments does not involve speaker and environment normalizations.)"
The n-gram phonotactic models used were represented as weighted finite state automata.
These automata (with the exception of the initial unweighted phone loop) were constructed using the stochastic language modeling tech-nique described[REF_CITE].
"This modeling technique, which includes a scheme for backing off to probability estimates for shorter n-grams, was originally designed for language modeling at the word level."
Different possible classification algorithms can be used in our utterance classification method.
For the experiments reported here we use the BoosTexter classifier[REF_CITE].
Among the alternatives are decision trees[REF_CITE]and support vector machines[REF_CITE].
BoosTexter was originally designed for text categorization.
"It uses the AdaBoost algorithm[REF_CITE], a wide margin ma-chine learning algorithm."
"At training time, AdaBoost selects features from a specified space of possible fea-tures and associates weights with them."
A distinguishing characteristic of the AdaBoost algorithm is that it places more emphasis on training examples that are difficult to classify.
"The algorithm does this by iterating through a number of rounds: at each round, it imposes a distribu-tion on the training data that gives more probability mass to examples that were difficult to classify in the previ-ous round."
"In our experiments, 500 rounds of boosting were used; each round allows the selection of a new fea-ture and the adjustment of weights associated with exist-ing features."
"In the experiments, the possible features are identifiers corresponding to prompts, and phone n-grams or word n-grams (for the phone and word-based methods respectively) up to length 4."
Three experimental conditions are considered.
The suf-fixes (M and H) in the condition names refer to whether the two training phases (i.e. training for recognition and classification respectively) use inputs produced by ma-chine (M) or human (H) processing.
"PhonesMM This experimental condition is the method described in this paper, so no human transcriptions are used."
Unsupervised training from the training speech files is used to build a phone recognition model.
The classifier is trained on the phone strings resulting from recognizing the training speech files with this model.
"At runtime, the classifier is ap-plied to the results of recognizing the test files with this model."
The initial recogition model for the un-supervised recognition training process was an un-weighted phone loop.
The final n-gram order used in the recognition training procedure (N max in sec-tion 2) was 5.
WordsHM Human transcriptions of the training speech files are used to build a word trigram model.
The classifier is trained on the word strings resulting from recognizing the training speech files with this word trigram model.
"At runtime, the classifier is ap-plied to the results of recognizing the test files with the word trigram model."
WordsHH Human transcriptions of the training speech files are used to build a word trigram model.
The classifier is trained on the human transcriptions of the speech training files.
"At runtime, the classifier is applied to the results of recognizing the test files with the word trigram model."
"For all three conditions, median recognition and classi-fication time for test data was less than real time (i.e. the duration of test speech files) on current micro-processors."
"As noted earlier, the acoustic model, the number of boost-ing rounds, and the use of prompts as an additional clas-sification feature, are the same for all experimental con-ditions."
"To give an impression of the kind of phone sequences resulting from the automatic training procedure and ap-plied by the classifier at runtime, see Table 1."
"The table lists some examples of such phone strings learned from domain A training speech files, together with English words, or parts of words (shown in bold type), they may correspond to. (Of course, the words play no part in the method and are only included for expository purposes.)"
The phone strings are shown in the DARPA phone alpha-bet.
In this section we compare the accuracy of our phone-string utterance classification method (PhonesMM) with methods (WordsHM and WordsHH) using manual tran-scription and word string models.
"The results are presented as utterance classification rates, specifically the percentage of utterances in the test set for which the predicted action is valid."
"Here a valid predic-tion means that the predicted action is the same as one of the actions associated with the test utterance by a human labeler. (As noted in section 3, the average number of actions associated with an utterance was 1.09, 1.23, and 1.07 for domains A, B, and C, respectively.)"
"In this met-ric we only take into account a single action predicted by the classifier, i.e. this is “rank 1” classification ac-curacy, rather than the laxer “rank 2” classification ac-curacy (where the classifier is allowed to make two pre-dictions) reported by Gorin et. al (1999) and Petrovska et. al (2000)."
"In practical applications of utterance classification, user inputs are rejected if the confidence of the classifier in making a prediction falls below a threshold appropri-ate to the application."
"After rejection, the system may, for example, route the call to a human or reprompt the user."
"We therefore show the accuracy of classifying ac-cepted utterances at different rejection rates, specifically 0% (all utterances accepted), 10%, 20%, 30%, 40%, and 50%."
"Utterance classification accuracy rates, at various rejec-tion rates, for domain A are shown in Table 2 for the three experimental conditions described in section 3.4."
The corresponding results for domains B and C are shown in Tables 3 and 4.
The utterances in domain A are on average longer and more complex than in domain B; this may partly explain the higher classification rates for domain B.
"The gener-ally lower classification accuracy rates for domain C may reflect the larger set of actions for this domain (92 ac-tions, compared with 56 and 54 actions for domains A and B)."
"Another difference between the domains was that the recording quality for domain B was not as high as for domains A and C. Despite these differences between the domains, there is a consistent pattern for the compar-ison of most interest to this paper, i.e. the relative per-formance of utterance classification methods requiring or not requiring transcription."
Perhaps the most surprising outcome of these ex-periments is that the phone-based method with short “phrasal” contexts (up to four phones) has classifica-tion accuracy that is so close to that provided by the longer phrasal contexts of trigram word recognition and word-string classification.
"Of course, the re-estimation of phone n-grams employed in the phone-based method means that two-word units are implicitly modeled since the phone 5-grams modeled in recognition, and 4-grams in classification, can straddle word boundaries."
"The experiments suggest that if transcriptions are available (i.e. the effort to produce them has already been expended), then they can be used to slightly improve performance over the phone-based method (PhonesMM) not requiring transcriptions."
"For domains A and C, this would give an absolute performance difference of about 2%, while for domain B the difference is around 1%."
Whether it is optimal to train the word-based classifier on the transcriptions (WordsHH) or the output of the recog-nizer (WordsHM) seems to depend on the particular data set.
"When the operational setting of utterance classifica-tion demands very high confidence, and a high degree of rejection is acceptable (e.g. if sufficient human backup operators are available), then the small advantage of the word-based methods is reduced further to less than 1%."
This can be seen from the high rejection rate rows of the accuracy tables.
In this paper we have presented an utterance classifica-tion method that does not require manual transcription of training data.
The method combines unsupervised re-estimation of phone n-ngram recognition models together with a phone-string classifier.
The utterance classifica-tion accuracy of the method is surprisingly close to a more traditional method involving manual transcription of training utterances into word strings and recognition with word trigrams.
The measured absolute difference in classification accuracy (with no rejection) between our method and the word-based method was only 1% for one test domain and 2% for two other test domains.
The per-formance difference is even smaller (less than 1%) if high rejection thresholds are acceptable.
This performance level was achieved despite the large reduction in effort required to develop new applications with the presented utterance classification method.
Named Entity (NE) extraction is an important subtask of document processing such as in-formation extraction and question answering.
"A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking."
"However, there are some cases where segmentation gran-ularity contradicts the results of morphologi-cal analysis and the building units of NEs, so that extraction of some NEs are inherently im-possible in this setting."
"To cope with the unit problem, we propose a character-based chunk-ing method."
"Firstly, the input sentence is an-alyzed redundantly by a statistical morpholog-ical analyzer to produce multiple (n-best) an-swers."
"Then, each character is annotated with its character types and its possible POS tags of the top n-best answers."
"Finally, a support vec-tor machine-based chunker picks up some por-tions of the input sentence as NEs."
This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result.
We apply our method to IREX NE extraction task.
The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method.
"Named Entity (NE) extraction aims at identifying proper nouns and numerical expressions in a text, such as per-sons, locations, organizations, dates, and so on."
This is an important subtask of document processing like infor-mation extraction and question answering.
"A common standard data set for Japanese NE extrac-tion is provided by IREX workshop (IREX Committee, editor, 1999)."
"Generally, Japanese NE extraction is done in the following steps: Firstly, a Japanese text is seg-mented into words and is annotated with POS tags by a morphological analyzer."
"Then, a chunker brings together the words into NE chunks based on contextual informa-tion."
"However, such a straightforward method cannot ex-tract NEs whose segmentation boundary contradicts that of morphological analysis  outputs."
"For” isexamplesegmented, a sen-as “tence “  analyzer. “  / /  /  / / /  ” by a morphological ” (“Koizumi Jun’ichiro” – family and first names) as a person name and “  ” (“Septem-ber”) as a date will be extracted by combining word units."
"On the other hand, “ ” (abbreviation of North Korea) cannot be extracted as a name of location because it is contained by the word unit “  ” (visiting North Korea)."
Figure 1 illustrates the example with English translation.
Some previous works try to cope with the word unit problem: Uchimo[REF_CITE]introduces transformation rules to modify the word units given by a morphological analyzer.
Isozaki[REF_CITE]controls the parameters of a statistical morpholog-ical analyzer so as to produce more fine-grained output.
These method are used as a preprocessing of chunking.
"By contrast, we propose more straightforward method in which we perform the chunking process based on char-acter units."
Each character receives annotations with character type and multiple POS information of the words found by a morphological analyzer.
We make use of re-dundant outputs of the morphological analysis as the base features for the chunker to introduce more information-rich features.
We use a support vector machine (SVM)-based chunker yamcha[REF_CITE]for the chunking process.
Our method achieves better score than all the systems reported previously for IREX NE ex-traction task.
Section 2 presents the IREX NE extraction task.
Sec-tion 3 describes our method in detail.
"In section 4, we show the results of experiments, and finally we give con-clusions in section 5."
"The task of NE extraction in the IREX workshop is to recognize eight NE types as shown in Table 1 (IREX Committee, editor, 1999)."
"In their definitions, “ARTI-FACT” contains book titles, laws, brand names and so on."
The task can be defined as a chunking problem to iden-
Koizumi Jun’ichiro Prime-Minister particle  
September particle visiting-North-Korea
Prime Minister Koisumi Jun’ichiro will visit North Korea in September.
Named Entities in the Sentence:  /“Koizumi
"Jun’ichiro”/PERSON,  /“September”/DATE, /“North Korea”/LOCATION tify word sequences which compose NEs."
The chunking problem is solved by annotation of chunk tags to tokens.
"Five chunk tag sets, IOB1, IOB2, IOE1, IOE2[REF_CITE]and SE[REF_CITE], are commonly used."
"In IOB1 and IOB2 models, three tags I, O and B are used, meaning inside, outside and beginning of a chunk."
"In IOB1, B is used only at the beginning of a chunk that immediately follows another chunk, while in IOB2, B is always used at the beginning of a chunk."
"IOE1 and IOE2 use E tag instead of B and are almost the same as IOB1 and IOB2 except that the end points of chunks are tagged with E. In SE model, S is tagged only to one-symbol chunks, and B, I and E denote exactly the begin-ning, intermediate and end points of a chunk."
"Generally, the words given by the single output of a morphological analyzer are used as the units for chunking."
"By contrast, we take characters as the units."
We annotate a tag on each character.
"Figure 2 shows examples of character-based NE anno-tations according to the five tag sets. “  ”(PERSON), “ ”(LOCATION) and “ ”(LOCATION) are NEs in the sentence and annotated as NEs."
"While the detailed expla-nation of the tags will be done later, note that an NE tag is a pair of an NE type and a chunk tag."
"In this section, we describe our method for Japanese NE extraction."
The method is based on the following three steps: 1.
A statistical morphological/POS analyzer is applied to the input sentence and produces POS tags of the n-best answers. 2.
"Each character in the sentences is annotated with the character type and multiple POS tag information ac-cording to the n-best answers. 3. Using annotated features, NEs are extracted by an SVM-based chunker."
"Now, we illustrate each of these three steps in more detail."
Our Japanese morphological/POS analysis is based on Markov model.
Morphological/POS analysis can be de- fined as the determination of POS tag sequence once a segmentation into a word sequence is given.
The goal is to find the POS and word sequences and that maximize the following probability:  
Bayes’ rule allows   to be decomposed as the product of tag and word probabilities.
"We introduce approximations that the word probabil-ity is conditioned only on the tag of the word, and the tag probability is determined only by the immediately pre-ceding tag."
The probabilities are estimated from the fre-quencies in tagged corpora using Maximum Likelihood Estimation.
"Using these parameters, the most probable tag and word sequences are determined by the Viterbi al-gorithm."
"In practice, we use log likelihood as cost."
Maximiz-ing probabilities means minimizing costs.
"In our method, redundant analysis output means the top n-best answers within a certain cost width."
The n-best answers are picked up for each character in the order of the accu-mulated cost from the beginning of the sentence.
"Note that, if the difference between the costs of the best answer and n-th best answer exceeds a predefined cost width, we abandon the n-th best answer."
The cost width is defined as the lowest probability in all events which occur in the training data.
"From the output of redundant analysis, each character re-ceives a number of features."
POS tag information is sub-categorized so as to encode relative positions of charac-ters within a word.
For encoding the position we employ SE tag model.
"Then, a character is tagged with a pair of POS tag and the position tag within a word as one fea-ture."
"For example, the character at the initial, intermedi-ate and final positions of a common noun (Noun-General) are represented as “Noun-General-B”, “Noun-General-I” and “Noun-General-E”, respectively."
The list of tags for positions in a word is illustrated in Table 2.
Note that O tag is not necessary since every character is a part of a certain word.
Character types are also used for features.
We define seven character types as listed in Table 3.
Figure 3 shows an example of the features used for chunking process.
"We used the chunker yamcha[REF_CITE], which is based on support vector machines[REF_CITE]."
"Below we present support vector machine-based chunking briefly. class problem:Suppose we have ! #&quot; a set  of  training %&amp;$ &apos;&quot; $ ,datawherefor % a &amp;( binary +) *-, is a &quot;. feature ( )0/ vector 4352 6 of the i-th sample in the training data and is the label of the sample."
The goal &quot; is to find a decision function which accurately predicts for the decision function 7 an unseen .
"An support  vector : machine ;&lt;&gt;=   classifierfor an inputgives vector where ? @ ( &quot; (?L ON ( 1[REF_CITE]@  means that is a positive member, 7 &gt;N (  means that is a negative member."
The vectors are called support vectors.
Support vectors and other con-stants are determined L #N by solving a quadratic program-ming problem. is a kernel function which maps vectors into a higher dimensional space L ? . #
We N @ use  the  poly- TVN  .nomial kernel of degree 2 given by
"To facilitate chunking tasks by SVMs, we have to ex-tend binary classifiers to n-class classifiers."
"There are two well-known methods used for the extension, “One-vs-Rest method” and “Pairwise method”."
"In “One-vs- Rest method”, we prepare = binary classifiers, one be-tween a class and the rest of the classes."
"In “Pairwise method”, we prepare ,YX W binary classifiers between all pairs of classes."
Chunking is done deterministically either from the be-ginning or the end of sentence.
Figure 3 illustrates a snap-shot of chunking procedure.
Two character contexts on both sides are referred to.
Information of two preceding NE tags is also used since the chunker has already deter-mined them and they are available.
"In the example, to infer the NE tag (“O”) at the position ; , the chunker uses the features appearing within the solid box."
The model copes with the problem of word segmentation by character-based chunking.
"Furthermore, we introduce n-best answers as features for chunking to capture the fol-lowing behavior of the morphological analysis."
The am-biguity of word segmentation occurs in compound words.
"When both longer and shorter unit words are included in the lexicon, the longer unit words are more likely to be output by the morphological analyzer."
"Then, the shorter units tend to be hidden behind the longer unit words."
"However, introducing the shorter unit words is more nec-essary to named entity extraction to generalize the model, because the shorter units are shared by many compound words."
Figure 4 shows the example in which the shorter units are effective for NE extraction.
"In this example “ ” (Japan) is extracted as a location by second best answer, namely “Noun-Proper-Place-Country”."
Unknown word problem is also solved by the n-best answers.
Contextual information in Markov Model is lost at the position unknown word occurs.
"Then, preceding or succeeding words of an unknown word tend to be mis-taken in POS tagging."
"However, correct POS tags occur-ring in n-best answer may help to extract named entity."
Figure 5 shows such an example.
"In this example, the begining of the person name is captured by the best an-swer at the position 1 and the end of the person name is captured by the second best answer at the position 5."
"We use CRL NE data (IREX Committee, editor, 1999) for evaluation of our method."
"CRL NE data includes 1,174 newspaper articles and 19,262 NEs."
"We perform five-fold cross-validation on several settings to investigate the length of contextual feature, the size of redundant mor-phological analysis, feature selection and the degree of polynomial Kernel functions."
For the chunk tag scheme we use IOB2 model since 2 it gave the best result in a pilot study.
F-Measure ( ) is used for evaluation.
"Firstly, we compare the extraction accuracies of the mod-els by changing the length of contextual features and the direction of chunking."
Table 4 shows the result in accu-racy for each of NEs as well as the total accuracy of all NEs.
"For example, “L2R2” denotes the model that uses the features of two preceding and two succeeding char-acters. “For” and “Back” mean the chunking direction: “For” specifies the chunking direction from left to right, and “Back” specifies that from right to left."
"Concerning NE types except for “TIME”, “Back” di-rection gives better accuracy for all NE types than “For” direction."
"It is because suffixes are crucial feature for NE extraction. “For” direction gives better accuracy for “TIME”, since “TIME  ” often contains prefixes such as “ ”(a.m.) and “ ”(p.m.). “L2R2” gives the best accurary for most of NE types."
"For “ORGANIZATION”, the model needs longer contex-tual length of features."
The reason will be that the key prefixes and suffixes are longer in this NE type such as “ ”(company limited) and “  ”(research in-stitute).
Table 5 shows the results when we change the depth (the value n of the n-best answers) of redundant morphologi-cal analysis.
Redundant outputs of morphological analysis slightly improve the accuracy of NE extraction except for nu-meral expressions.
The best answer seems enough to extract numeral experssions except for “MONEY”.
It is because numeral expressions do not cause much errors in morphological analysis.
"To extract “MONEY”, the model needs more redundant output of morphological analysis."
A typical occurs at “  &quot;! ” (Canadian dollars = MONEY) which is not including training data and is analyzed as “ # ” (Canada = LOCATION).
The similar error occurs at “ $ %&amp; ! ” (Hong Kong dollars) and so on.
"We use POS tags, characters, character types and NE tags as features for chunking."
"To evaluate how they are effective we test four settings, that 3 is, “using all features (ALL)”, “excluding 3 characters ( Char.)”, “excluding character types ( Char 3 ."
Type)” and “excluding subcat-egory of POS tags ( POS subcat.)”.
"Table 6 shows the results for these settings. “Excluding Characters” gives the worst accuracy, im-plying that characters are indispensable for NE extrac-tion. “Excluding POS subcat.” results in worse accuracy."
"Some subcategories of POS include semantic informa-tion for proper nouns such that name, organization and location, and they are useful for NE extraction."
"For numeral expressions, “excluding Char Type” gives better accuracy."
The reason is that numbers in Kanji are not defined in our character type definition.
We alter degrees of kernel functions and check how the combination of features affects the results.
"As shown in Table 7, degree 2 gives the best accuracy for most of NE types."
The result shows that the combination of two fea-tures is effective for extract NE extraction.
"However, the tendency is not so significant in numeral expressions."
"In the experimentation above, we follow the features used in the preceding work[REF_CITE]."
Isozaki[REF_CITE]introduces the thesaurus – NTT Goi Taikei[REF_CITE]– to augment the feature set.
Table 8 shows the result when the class names in the thesaurus is used as features.
Note that we intro-duced the leaf node tag for each morpheme.
The the-saurus information is effective for NEs except for “ARTI-FACT” and “TIME”.
"Since “ARTIFACT” includes many unseen expressions, even if we introduce the information of the thesaurus, we cannot improve this model."
"Concern-ing “TIME”, the words and characters in this NE type are limited."
The information of thesaurus may not be neces-sary for “TIME” expression extraction.
"In this paper, we did not encode the tree structure of the thesaurus."
Intro-ducing hierarchical relationships in the thesaurus is one of our future works.
"While we must have a fixed feature set among all NE types in Pairwise method, it is possible to select differ-ent feature sets and models when applying One-vs-Rest method."
The best combined model achieves F-measure 87.21 (Table 9).
The model uses one-vs-rest method with the best model for each type shown in Table 4-8.
Our method attains the best result in the previously reported systems.
Previous works report that POS information in preced-ing and succeeding two-word window is the most effec-tive for Japanese NE extraction.
Our current work dis-proves the widespread belief about the contextual feature.
"In our experiments, the preceding and succeeding two or three character window is the best effective."
Our method employs exactly same chunker with the work by Yamada et. al. (2002).
"To see the influence of boundary contradiction between morphological anal-ysis and NEs, they experimented with an ideal setting in which morphological analysis provides the perfect re-sults for the NE chunker."
Their result shows F-measure 85.1 in the same data set as ours.
Those results show that our method solves more than the word unit problem compared with their results.
The proposed NE extraction method achieves F-measure 87.21 on CRL NE data.
This is the best result in the pre-viously reported systems.
We made use of character level information with redundant outputs of a statistical mor-phological analyzer in an SVM-based chunker.
It copes with the word unit problem in NE extraction.
"Further-more, the method is robust for both errors of the mor-phological analyzer and occurences of unknown words, because character level prefixes and suffixes of NEs are clues for finding them."
Fragments of possible words are used as features by the redundant morphological analy-sis.
"Though we tested this method only with Japanese, the method is applicable to any other languages that have word unit problem in NE extraction."
We address the text-to-text generation prob-lem of sentence-level paraphrasing — a phe-nomenon distinct from and more difficult than word- or phrase-level paraphrasing.
Our ap-proach applies multiple-sequence alignment to sentences gathered from unannotated compara-ble corpora: it learns a set of paraphrasing pat-terns represented by word lattice pairs and au-tomatically determines how to apply these pat-terns to rewrite new sentences.
"The results of our evaluation experiments show that the sys-tem derives accurate paraphrases, outperform-ing baseline systems."
This is a late parrot!
It’s a stiff!
"Bereft of life, it rests in peace!"
If you hadn’t nailed him to the perch he would be pushing up the daisies!
Its metabolical processes are of interest only to historians!
It’s hopped the twig!
It’s shuffled off this mortal coil!
It’s rung down the curtain and joined the choir invisible!
This is an EX-PARROT! —
"Monty Python, “Pet Shop”"
A mechanism for automatically generating multiple paraphrases of a given sentence would be of signif-icant practical import for text-to-text generation sys-tems.
"Applications include summarizati[REF_CITE]and rewriting[REF_CITE]: both could employ such a mechanism to produce candidate sentence paraphrases that other system components would filter for length, sophistication level, and so forth. [Footnote_1] Not surprisingly, therefore, paraphrasing has been a focus of generation research for quite some time[REF_CITE]."
"1 Another interesting application, somewhat tangential to generation, would be to expand existing corpora by providing"
One might initially suppose that sentence-level para-phrasing is simply the result of word-for-word or phrase-by-phrase substitution applied in a domain- and context-independent fashion.
"However, in studies of para-phrases across several domains[REF_CITE], this was gen-erally not the case."
"For instance, consider the fol-lowing two sentences (similar to examples found[REF_CITE]):"
"After the latest Fed rate cut, stocks rose across the board."
Winners strongly outpaced losers after Greenspan cut in-terest rates again.
Observe that “Fed” (Federal Reserve) and “Greenspan” are interchangeable only in the domain of US financial matters.
"Also, note that one cannot draw one-to-one cor-respondences between single words or phrases."
"For in-stance, nothing in the second sentence is really equiva-lent to “across the board”; we can only say that the en-tire clauses “stocks rose across the board” and “winners strongly outpaced losers” are paraphrases."
"This evidence suggests two consequences: (1) we cannot rely solely on generic domain-independentlexical resources for the task of paraphrasing, and (2) sentence-level paraphrasing is an important problem extending beyond that of paraphrasing smaller lexical units."
Our work presents a novel knowledge-lean algorithm that uses multiple-sequence alignment (MSA) to learn to generate sentence-level paraphrases essentially from unannotated corpus data alone.
"In contrast to previ-ous work using MSA for generati[REF_CITE], we need neither parallel data nor explicit infor-mation about sentence semantics."
"Rather, we use two comparable corpora, in our case, collections of articles produced by two different newswire agencies about the same events."
"The use of related corpora is key: we can capture paraphrases that on the surface bear little resem-blance but that, by the nature of the data, must be descrip-tions of the same information."
"Note that we also acquire paraphrases from each of the individual corpora; but the lack of clues as to sentence equivalence in single corpora means that we must be more conservative, only selecting as paraphrases items that are structurally very similar."
Our approach has three main steps.
"First, working on each of the comparable corpora separately, we compute lattices — compact graph-based representations — to find commonalities within (automatically derived) groups of structurally similar sentences."
"Next, we identify pairs of lattices from the two different corpora that are para-phrases of each other; the identification process checks whether the lattices take similar arguments."
"Finally, given an input sentence to be paraphrased, we match it to a lat-tice and use a paraphrase from the matched lattice’s mate to generate an output sentence."
The key features of this approach are:
Focus on paraphrase generation.
"In contrast to earlier work, we not only extract paraphrasing rules, but also au-tomatically determine which of the potentially relevant rules to apply to an input sentence and produce a revised form using them."
Flexible paraphrase types.
"Previous approaches to paraphrase acquisition focused on certain rigid types of paraphrases, for instance, limiting the number of argu-ments."
"In contrast, our method is not limited to a set of a priori-specified paraphrase types."
Use of comparable corpora and minimal use of knowl-edge resources.
"In addition to the advantages mentioned above, comparable corpora can be easily obtained for many domains, whereas previous approaches to para-phrase acquisition (and the related problem of phrase-based machine translati[REF_CITE]) required parallel corpora."
"We point out that one such approach, recently proposed[REF_CITE], also represents paraphrases by lat-tices, similarly to our method, although their lattices are derived using parse information."
"Moreover, our algorithm does not employ knowledge resources such as parsers or lexical databases, which may not be available or appropriate for all domains — a key issue since paraphrasing is typically domain-dependent."
"Nonetheless, our algorithm achieves good performance."
Previous work on automated paraphrasing has con-sidered different levels of paraphrase granularity.
Learning synonyms via distributional similarity has been well-studied[REF_CITE].
"These latter are the most closely related to the sentence-level paraphrases we desire, and so we focus in this section on template-induction approaches."
They assume that paths in dependency trees that take similar arguments (leaves) are close in mean-ing.
"However, only two-argument templates are consid-ered."
"Like us (and unlike Lin and Pantel, who employ a single large corpus), they use articles written about the same event in different newspapers as data."
"Our approach shares two characteristics with the two methods just described: pattern comparison by analysis of the patterns’ respective arguments, and use of non-parallel corpora as a data source."
"However, extraction methods are not easily extended to generation methods."
One problem is that their templates often only match small fragments of a sentence.
"While this is appropriate for other applications, deciding whether to use a given template to generate a paraphrase requires information about the surrounding context provided by the entire sen-tence."
Overview We first sketch the algorithm’s broad out-lines.
The subsequent subsections provide more detailed descriptions of the individual steps.
The major goals of our algorithm are to learn:
Figure 1 illustrates the main stages of our approach.
"During training, pattern induction is first applied inde-pendently to the two datasets making up a pair of compa-rable corpora."
Individual patterns are learned by applying multiple-sequence alignment to clusters of sentences de-scribing approximately similar events; these patterns are represented compactly by lattices (see Figure 3).
We then check for lattices from the two different corpora that tend to take the same arguments; these lattice pairs are taken to be paraphrase patterns.
"Once training is done, we can generate paraphrases as follows: given the sentence “The surprise bombing in-jured twenty people, five of them seriously”, we match it to the lattice X (injured/wounded) Y people, Z of them seriously which can be rewritten as Y were (wounded/hurt) by X, among them Z were in serious condition, and so by substituting arguments we can gen-erate “Twenty were wounded by the surprise bombing, among them five were in serious condition” or “Twenty were hurt by the surprise bombing, among them five were in serious condition”."
"Our first step is to cluster sentences into groups from which to learn useful patterns; for the multiple-sequence techniques we will use, this means that the sentences within clusters should describe similar events and have similar structure, as in the sentences of Figure 2."
This is accomplished by applying hierarchical complete-link clustering to the sentences using  a similarity metric based on word n-gram overlap (  ).
"The only sub-tlety is that we do not want mismatches on sentence de-tails (e.g., the location of a raid) causing sentences de-scribing the same type of occurrence (e.g., a raid) from being separated, as this might yield clusters too frag-mented for effective learning to take place. (Moreover, variability in the arguments of the sentences in a cluster is needed for our learning algorithm to succeed; see be-low.)"
"We therefore first replace all appearances of dates, numbers, and proper names [Footnote_2] with generic tokens."
2 Our crude proper-name identification method was to flag every phrase (extracted by a noun-phrase chunker) appearing capitalized in a non-sentence-initial position sufficiently often.
Clus-ters with fewer than ten sentences are discarded.
"In order to learn patterns, we first compute a multiple-sequence alignment (MSA) of the sentences in a given cluster."
"Pairwise MSA takes two sentences and a scor-ing function giving the similarity between words; it de-termines the highest-scoring way to perform insertions, deletions, and changes to transform one of the sentences into the other."
"Pairwise MSA can be extended efficiently to multiple sequences via the iterative pairwise align-ment, a polynomial-time method commonly used in com-putational biology[REF_CITE]. [Footnote_3]"
"3 Scoring function: aligning two identical words scores 1; inserting a word scores -0.01, and aligning two dif-ferent words scores -0.5 (parameter values taken[REF_CITE])."
"The results can be represented in an intuitive form via a word lat-tice (see Figure 3), which compactly represents (n-gram) structural similarities between the cluster’s sentences."
To transform lattices into generation-suitable patterns requires some understanding of the possible varieties of lattice structures.
"The most important part of the transfor-mation is to determine which words are actually instances of arguments, and so should be replaced by slots (repre-senting variables)."
"The key intuition is that because the sentences in the cluster represent the same type of event, such as a bombing, but generally refer to different in-stances of said event (e.g. a bombing in Jerusalem versus in Gaza), areas of large variability in the lattice should correspond to arguments."
"To quantify this notion of variability, we first formal-ize its opposite: commonality."
We define backbone nodes as those shared by more than 50% of the cluster’s sen-tences.
"The choice of 50% is not arbitrary — it can be proved using the pigeonhole principle that our strict-majority criterion imposes a unique linear ordering of the backbone nodes that respects the word ordering within the sentences, thus guaranteeing at least a degree of well-formedness and avoiding the problem of how to order backbone nodes occurring on parallel “branches” of the lattice."
"Once we have identified the backbone nodes as points of strong commonality, the next step is to identify the re-gions of variability (or, in lattice terms, many parallel dis-joint paths) between them as (probably) corresponding to the arguments of the propositions that the sentences rep-resent."
"For example, in the top of Figure 3, the words “southern city, “settlement of NAME”,“coastal resort of NAME”, etc. all correspond to the location of an event and could be replaced by a single slot."
Figure 3 shows an example of a lattice and the derived slotted lattice; we give the details of the slot-induction process in the Ap-pendix.
"Now, if we were using a parallel corpus, we could em-ploy sentence-alignment information to determine which lattices correspond to paraphrases."
"Since we do not have this information, we essentially approximate the parallel-corpus situation by correlating information from descrip-tions of (what we hope are) the same event occurring in the two different corpora."
Our method works as follows.
"Once lattices for each corpus in our comparable-corpus pair are computed, we identify lattice paraphrase pairs, using the idea that para-phrases will tend to take the same values as arguments ([REF_CITE];"
"More specifically, we take a pair of lattices from different cor-pora, look back at the sentence clusters from which the two lattices were derived, and compare the slot values of those cross-corpus sentence pairs that appear in arti-cles written on the same day on the same topic; we pair the lattices if the degree of matching is over a threshold tuned on held-out data."
"For example, suppose we have two (linearized) lattices slot1 bombed slot2 and slot3 was bombed by slot4 drawn from different corpora."
"If in the first lattice’s sentence cluster we have the sen-tence “the plane bombed the town”, and in the second lat-tice’s sentence cluster we have a sentence written on the same day reading “the town was bombed by the plane”, then the corresponding lattices may well be paraphrases, where slot1 is identified with slot4 and slot2 with slot3."
"To compare the set of argument values of two lattices, we simply count their word overlap, giving double weight to proper names and numbers and discarding auxiliaries (we purposely ignore order because paraphrases can con-sist of word re-orderings)."
"Given a sentence to paraphrase, we first need to iden-tify which, if any, of our previously-computed sentence clusters the new sentence belongs most strongly to."
"We do this by finding the best alignment of the sentence to the existing lattices. [Footnote_4] If a matching lattice is found, we choose one of its comparable-corpus paraphrase lattices to rewrite the sentence, substituting in the argument val-ues of the original sentence."
"4 To facilitate this process, we add “insert” nodes between backbone nodes; these nodes can match any word sequence and thus account for new words in the input sentence. Then, we per-form multiple-sequence alignment where insertions score -0.1 and all other node alignments receive a score of unity."
This yields as many para-phrases as there are lattice paths.
All evaluations involved judgments by native speakers of English who were not familiar with the paraphrasing sys-tems under consideration.
We implemented our system on a pair of comparable corpora consisting of articles produced[REF_CITE]and[REF_CITE]by the Agence France-Presse (AFP) and Reuters news agencies.
"Given our interest in domain-dependent paraphrasing, we limited attention to 9MB of articles, collected using a TDT-style document clustering system, concerning individual acts of violence in Israel and army raids on the Palestinian territories."
"From this data (after removing 120 articles as a held- out parameter-training set ), we extracted 43 slotted lat-tices from the AFP corpus and 32 slotted lattices from the Reuters corpus, and found 25 cross-corpus matching pairs; since lattices contain multiple paths, these yielded 6,534 template pairs. 5"
"Before evaluating the quality of the rewritings produced by our templates and lattices, we first tested the qual-ity of a random sample of just the template pairs."
"In our instructions to the judges, we defined two text units (such as sentences or snippets) to be paraphrases if one of them can generally be substituted for the other without great loss of information (but not necessarily vice versa). 6 Given a pair of templates produced by a system, the judges marked them as paraphrases if for many instanti-ations of the templates’ variables, the resulting text units were paraphrases. (Several labelled examples were pro-vided to supply further guidance)."
"To put the evaluation results into context, we wanted to compare against another system, but we are not aware of any previous work creating templates precisely for the task of generating paraphrases."
"Instead, we made a good-faith effort to adapt the DIRT system[REF_CITE]to the problem, selecting the [Footnote_6],534 highest-scoring templates it produced when run on our datasets. (The system[REF_CITE]was unsuitable for evaluation purposes because their paraphrase extraction component is too tightly coupled to the underlying in-formation extraction system.)"
"6 We switched to this “one-sided” definition because in ini-tial tests judges found it excruciating to decide on equivalence. Also, in applications such as summarization some information loss is acceptable."
"It is important to note some important caveats in making this comparison, the most prominent being that DIRT was not designed with sentence-paraphrase generation in mind — its templates are much shorter than ours, which may have affected the evaluators’ judgments — and was originally imple-mented on much larger data sets. [Footnote_7] The point of this eval-uation is simply to determine whether another corpus-based paraphrase-focused approach could easily achieve the same performance level."
"7 To cope with the corpus-size issue, DIRT was trained on an 84MB corpus of Middle-East news articles, a strict superset of the 9MB we used. Other issues include the fact that DIRT’s output needed to be converted into English: it produces paths like “N:of:N tide N:nn:N”, which we transformed into “Y tide of X” so that its output format would be the same as ours."
"In brief, the DIRT system works as follows."
Depen-dency trees are constructed from parsing a large corpus.
"Leaf-to-leaf paths are extracted from these dependency trees, with the leaves serving as slots."
"Then, pairs of paths in which the slots tend to be filled by similar val-ues, where the similarity measure is based on the mutual information between the value and the slot, are deemed to be paraphrases."
We randomly extracted 500 pairs from the two algo-rithms’ output sets.
The “individual” sets allowed us to broaden our sample’s coverage of the corpus. [Footnote_8]
"8 Each judge took several hours at the task, making it infea-sible to expand the sample size further."
"The pairs were presented in random order, and the judges were not told which system produced a given pair."
"As Figure 4 shows, our system outperforms the DIRT system, with a consistent performance gap for all the judges of about 38%, although the absolute scores vary (for example, Judge 4 seems lenient)."
The judges’ as-sessment of correctness was fairly constant between the full 100-instance set and just the 50-instance common set alone.
"In terms of agreement, the Kappa value (measuring pairwise agreement discounting chance occurrences 9 ) on the common set was 0.54, which corresponds to moder-ate agreement[REF_CITE]."
"Multiway agree-ment is depicted in Figure 4 — there, we see that in 86 of 100 cases, at least three of the judges gave the same correctness assessment, and in 60 cases all four judges concurred."
"Finally, we evaluated the quality of the paraphrase sen-tences generated by our system, thus (indirectly) testing all the system components: pattern selection, paraphrase acquisition, and generation."
We are not aware of another system generating sentence-level paraphrases.
"Therefore, we used as a baseline a simple paraphrasing system that just replaces words with one of their randomly-chosen WordNet synonyms (using the most frequent sense of the word that WordNet listed synonyms for)."
The number of substitutions was set proportional to the number of words our method replaced in the same sentence.
The point of this comparison is to check whether simple synonym sub-stitution yields results comparable to those of our algo-
"For this experiment, we randomly selected 20 AFP ar-ticles about violence in the Middle East published later than the articles in our training corpus."
"We found that after proper name substitution, only seven sentences in the test set appeared in the training set, [Footnote_11] which im-plies that lattices boost the generalization power of our method significantly: from seven to 59 sentences."
"11 Since we are doing unsupervised paraphrase acquisition, train-test overlap is allowed."
"Inter-estingly, the coverage of the system varied significantly with article length."
"For the eight articles of ten or fewer sentences, we paraphrased 60.8% of the sentences per ar-ticle on average, but for longer articles only [Footnote_9].3% of the sentences per article on average were paraphrased."
"9 One issue is that the Kappa statistic doesn’t account for varying difficulty among instances. For this reason, we actu-ally asked judges to indicate for each instance whether making the validity decision was difficult. However, the judges gen-erally did not agree on difficulty. Post hoc analysis indicates that perception of difficulty depends on each judge’s individual “threshold of similarity”, not just the instance itself."
"Our analysis revealed that long articles tend to include large portions that are unique to the article, such as personal stories of the event participants, which explains why our algorithm had a lower paraphrasing rate for such articles."
"Of the paraphrases generated by our system, the two evalua-tors deemed 81.4% and 78%, respectively, to be valid, whereas for the baseline system, the correctness results were 69.5% and 66.1%, respectively."
Agreement accord-ing to the Kappa statistic was 0.6.
"Note that judging full sentences is inherently easier than judging templates, be-cause template comparison requires considering a variety of possible slot values, while sentences are self-contained units."
"Figure 5 shows two example sentences, one where our MSA-based paraphrase was deemed correct by both judges, and one where both judges deemed the MSA-generated paraphrase incorrect."
Examination of the re-sults indicates that the two systems make essentially or-thogonal types of errors.
The baseline system’s relatively poor performance supports our claim that whole-sentence paraphrasing is a hard task even when accurate word-level paraphrases are given.
"We presented an approach for generating sentence level paraphrases, a task not addressed previously."
Our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus.
A flexible pattern-matching pro-cedure allows us to paraphrase an unseen sentence by matching it to one of the induced patterns.
Our approach generates both lexical and structural paraphrases.
Another contribution is the induction of MSA lat-tices from non-parallel data.
"Lattices have proven ad-vantageous in a number of NLP contexts[REF_CITE], but were usually produced from (multi-)parallel data, which may not be readily available for many applications."
"We showed that word lattices can be induced from a type of corpus that can be easily ob-tained for many domains, broadening the applicability of this useful representation."
"In this appendix, we describe how we insert slots into lattices to form slotted lattices."
Recall that the backbone nodes in our lattices represent words appearing in many of the sentences from which the lattice was built.
"As mentioned above, the intuition is that areas of high variability between backbone nodes may correspond to arguments, or slots."
"But the key thing to note is that there are actually two different phenomena giving rise to multiple parallel paths: argument variabil-ity, described above, and synonym variability."
"For exam-ple, Figure 6(b) contains parallel paths corresponding to the synonyms “injured” and “wounded”."
Note that we want to remove argument variability so that we can gen-erate paraphrases of sentences with arbitrary arguments; but we want to preserve synonym variability in order to generate a variety of sentence rewritings.
"To distinguish these two situations, we analyze the split level of backbone nodes that begin regions with multi-ple paths."
"The basic intuition is that there is probably more variability associated with arguments than with syn-onymy: for example, as datasets increase, the number of locations mentioned rises faster than the number of syn-onyms appearing."
"We make use of a synonymy threshold (set by held-out parameter-tuning to 30), as follows."
"If no more than % of all the edges out of a back-bone node lead to the same next node, we have high enough variability to warrant inserting a slot node."
"Otherwise, we incorporate reliable synonyms [Footnote_12] into the backbone structure by preserving all nodes that are reached by at least % of the sentences passing through the two neighboring backbone nodes."
"12 While our original implementation, evaluated in Section 4, identified only single-word synonyms, phrase-level synonyms can similarly be acquired by considering chains of nodes con-necting backbone nodes."
"Furthermore, all backbone nodes labelled with our spe-cial generic tokens are also replaced with slot nodes, since they, too, probably represent arguments (we con-dense adjacent slots into one)."
Nodes with in-degree lower than the synonymy threshold are removed un-der the assumption that they probably represent idiosyn-crasies of individual sentences.
See Figure 6 for exam-ples.
Figure 3 shows an example of a lattice and the slotted lattice derived via the process just described. of variability.
The double-boxed nodes are backbone nodes; edges show consecutive words in some sentence.
The synonymy threshold (set to 30% in this example) de-termines the type of variability.
"Motivated by the success of ensemble methods in machine learning and other areas of natu-ral language processing, we developed a multi-strategy and multi-source approach to question answering which is based on combining the re-sults from different answering agents searching for answers in multiple corpora."
"The answer-ing agents adopt fundamentally different strate-gies, one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques."
"We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels."
"Experiments evaluating the effectiveness of our answer resolution algo-rithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric."
"Traditional question answering (QA) systems typically employ a pipeline approach, consisting roughly of ques-tion analysis, document/passage retrieval, and answer se-lection (see e.g.,[REF_CITE])."
"Although a typical QA system classifies questions based on expected answer types, it adopts the same strategy for locating po-tential answers from the same corpus regardless of the question classification."
"In our own earlier work, we de-veloped a specialized mechanism called Virtual Annota-tion for handling definition questions (e.g., “Who was Galileo?” and “What are antibiotics?”) that consults, in addition to the standard reference corpus, a structured knowledge source (WordNet) for answering such ques-tions[REF_CITE]."
We have shown that better performance is achieved by applying Virtual Annotation and our general purpose QA strategy in parallel.
"In this paper, we investigate the impact of adopting such a multi-strategy and multi-source approach to QA in a more gen-eral fashion."
"Our approach to question answering is additionally motivated by the success of ensemble methods in ma-chine learning, where multiple classifiers are employed and their results are combined to produce the final output of the ensemble (for an overview, see[REF_CITE])."
Such ensemble methods have recently been adopted in question answering[REF_CITE].
"In our question answering system, PI-QUANT, we utilize in parallel multiple answering agents that adopt different processing strategies and consult dif-ferent knowledge sources in identifying answers to given questions, and we employ resolution mechanisms to com-bine the results produced by the individual answering agents."
We call our approach multi-strategy since we com-bine the results from a number of independent agents im-plementing different answer finding strategies.
We also call it multi-source since the different agents can search for answers in multiple knowledge sources.
"In this pa-per, we focus on two answering agents that adopt fun-damentally different strategies: one agent uses predomi-nantly knowledge-based mechanisms, whereas the other agent is based on statistical methods."
"Our multi-level resolution algorithm enables combination of results from each answering agent at the question, passage, and/or an-swer levels."
"Our experiments show that in most cases our multi-level resolution algorithm outperforms its com-ponents, supporting a tightly-coupled design for multi-agent QA systems."
"Experimental results show signifi-cant performance improvement over our single-strategy, single-source baselines, with the best performing multi-level resolution algorithm achieving a 35.0% relative im-provement in the number of correct answers and a 32.8% improvement in average precision, on a previously un-seen test set."
"In order to enable a multi-source and multi-strategy ap-proach to question answering, we developed a modu-lar and extensible QA architecture as shown in Figure 1[REF_CITE]."
"With a consistent interface defined for each component, this architecture allows for easy plug-and-play of individ-ual components for experimental purposes."
"In our architecture, a question is first processed by the question analysis component."
"The analysis results are represented as a QFrame, which minimally includes a set of question features that help activate one or more an-swering agents."
Each answering agent takes the QFrame and generates its own set of requests to a variety of knowledge sources.
"This may include performing search against a text corpus and extracting answers from the re-sulting passages, or performing a query against a struc-tured knowledge source, such as WordNet[REF_CITE]or Cyc[REF_CITE]."
"The (intermediate) results from the individual answering agents are then passed on to the answer resolution component, which combines and re-solves the set of results, and either produces the system’s final answers or feeds the intermediate results back to the answering agents for further processing."
"We have developed multiple answering agents, some general purpose and others tailored for specific ques-tion types."
Figure 1 shows the answering agents cur-rently available in PIQUANT.
The knowledge-based and statistical answering agents are general-purpose agents that adopt different processing strategies and consult a number of different text resources.
"The definition-Q agent targets definition questions (e.g., “What is peni-cillin?” and “Who is Picasso?”) with a technique called"
Virtual Annotation using the external knowledge source WordNet[REF_CITE].
"The KSP-based answer-ing agent focuses on a subset of factoid questions with specific logical forms, such as capital(?COUNTRY) and state tree(?STATE)."
"The answering agent sends requests to the KSP (Knowledge Sources Portal), which returns, if possible, an answer from a structured knowledge source[REF_CITE]."
"In the rest of this paper, we briefly describe our two general-purpose answering agents."
"We then focus on a multi-level answer resolution algorithm, applicable at dif-ferent points in the QA process of these two answering agents."
"Finally, we discuss experiments conducted to dis-cover effective methods for combining results from mul-tiple answering agents."
"We focus on two end-to-end answering agents designed to answer short, fact-seeking questions from a collection of text documents, as motivated by the requirements of the TREC QA track[REF_CITE]."
"Both answer-ing agents adopt the classic pipeline architecture, con-sisting roughly of question analysis, passage retrieval, and answer selection components."
"Although the answer-ing agents adopt fundamentally different strategies in their individual components, they have performed quite comparably in past TREC QA tracks[REF_CITE]."
"Our first answering agent utilizes a primarily knowledge-driven approach, based on Predictive Annotati[REF_CITE]."
"A key characteristic of this approach is that potential answers, such as person names, locations, and dates, in the corpus are predictively annotated."
"In other words, the corpus is indexed not only with keywords, as is typical for most search engines, but also with the se-mantic classes of these pre-identified potential answers."
"During the question analysis phase, a rule-based mech-anism is employed to select one or more expected an-swer types, from a set of about 80 classes used in the predictive annotation process, along with a set of ques-tion keywords."
"A weighted search engine query is then constructed from the keywords, their morphological vari-ations, synonyms, and the answer type(s)."
"The search en-gine returns a hit list of typically 10 passages, each con-sisting of 1-3 sentences."
"The candidate answers in these passages are identified and ranked based on three criteria: 1) match in semantic type between candidate answer and expected answer, 2) match in weighted grammatical rela-tionships between question and answer passages, and 3) frequency of answer in candidate passages (redundancy)."
The answering agent returns the top n ranked candidate answers along with a confidence score for each answer.
The second answering agent takes a statistical approach to question answering[REF_CITE].
"It models the distribution p(c|q,a), which measures the “correctness” (c) of an answer (a) to a ques-tion (q), by introducing a hidden variable representing the answer type (e) as follows: p(c|q, a) = P e p(c, e|q, a) ="
"P e p(c|e, q, a)p(e|q, a) p(e|q, a) is the answer type model which predicts, from the question and a proposed answer, the answer type they both satisfy. p(c|e,q,a) is the answer selection model."
"Given a question, an answer, and the predicted answer type, it seeks to model the correctness of this configura-tion."
"These distributions are modeled using a maximum entropy formulati[REF_CITE], using training data which consists of human judgments of question an-swer pairs."
"For the answer type model, 13K questions were annotated with 31 categories."
"For the answer selec-tion model, 892 questions from the TREC 8 and TREC 9 QA tracks were used, along with 4K trivia questions."
"During runtime, the question is first analyzed by the answer type model, which selects one out of a set of 31 types for use by the answer selection model."
"Simultane-ously, the question is expanded using local context anal-ysis[REF_CITE]with an encyclopedia, and the top 1000 documents are retrieved by the search engine."
"From these documents, the top 100 passages are chosen that 1) maximize the question word match, 2) have the desired answer type, 3) minimize the dispersion of ques-tion words, and 4) have similar syntactic structures as the question."
"From these passages, candidate answers are ex-tracted and ranked using the answer selection model."
"The top n candidate answers are then returned, each with an associated confidence score."
"Given two answering agents with the same pipeline archi-tecture, there are multiple points in the process at which (intermediate) results can be combined, as illustrated in Figure 2."
"More specifically, it is possible for one answer-ing agent to provide input to the other after the question analysis, passage retrieval, and answer selection phases."
"In PIQUANT, the knowledge based agent may accept in-put from the statistical agent after each of these three phases. [Footnote_1]"
"1 Although it is possible for the statistical agent to receive input from the knowledge based agent as well, we have not pur-sued that option because of implementation issues."
The contributions from the statistical agent are taken into consideration by the knowledge based answer-ing agent in a phase-dependent fashion.
The rest of this section details our combination strategies for each phase.
"One of the key tasks of the question analysis component is to determine the expected answer type, such as PERSON for “Who discovered America?” and DATE for “When did World War II end?”"
"This information is taken into ac-count by most existing QA systems when ranking candi-date answers, and can also be used in the passage retrieval process to increase the precision of candidate passages."
We seek to improve the knowledge-based agent’s performance in passage retrieval and answer selection through better answer type identification by consulting the statistical agent’s expected answer type.
"This task, however, is complicated by the fact that QA systems em-ploy different sets of answer types, often with different granularities and/or with overlapping types."
"For instance, while one system may generate ROYALTY for the ques-tion “Who was the King[REF_CITE]?”, another system may produce PERSON as the most specific an-swer type in its repertoire."
This is quite a serious problem for us as the knowledge based agent uses over 80 answer types while the statistical agent adopts only 31 categories.
"In order to distinguish actual answer type discrepan-cies from those due to granularity differences, we first manually created a mapping between the two sets of an-swer types."
"This mapping specifies, for each answer type used by the statistical agent, a set of possible correspond-ing types used by the knowledge-based agent."
"For exam-ple, the GEOLOGICALOBJ class is mapped to a set of finer grained classes: RIVER , MOUNTAIN , LAKE , and OCEAN ."
"At processing time, the statistical agent’s answer type is mapped to the knowledge-based agent’s classes (SA- types), which are then merged with the answer type(s) se-lected by the knowledge-based agent itself (KBA-types) as follows: 1."
"If the intersection of KBA-types and SA-types is non-null, i.e., the two agents produced consistent an-swer types, then the merged set is KBA-types. 2."
"Otherwise, the two sets of answer types are truly in disagreement, and the merged set is the union of KBA-types and SA-types."
The merged answer types are then used by the knowledge-based agent in further processing.
"The passage retrieval component selects, from a large text corpus, a small number of short passages from which an-swers are identified."
"Oftentimes, multiple passages that answer a question are retrieved."
Some of these passages may be better suited than others for the answer selection algorithm employed downstream.
"For example, consider “When was Benjamin Disraeli prime minister?”, whose answer can be found in both passages below: 1. Benjamin Disraeli, who had become prime minister in 1868, was born into Judaism but was baptized a Christian at the age of 12. 2."
"France had a Jewish prime minister in 1936, Eng-land in 1868, and Spain, of all countries, in 1835, but none of them, Leon Blum, Benjamin Disraeli or Juan Alvarez Mendizabel, were devoutly observant, as Lieberman is."
"Although the correct answer, 1868, is present in both passages, it is substantially easier to identify the answer from the first passage, where it is directly stated, than from the second passage, where recognition of parallel constructs is needed to identify the correct answer."
"Because of strategic differences in question analysis and passage retrieval, our two answering agents often re-trieve different passages for the same question."
"Thus, we perform passage-level combination to make a wider va-riety of passages available to the answer selection com-ponent, as shown in Figure 2."
The potential advantages are threefold.
"First, passages from agent [Footnote_2] may contain answers absent in passages retrieved by agent 1."
"2 On the other hand, such redundancy may result in error compounding, as discussed in Section 5.3."
"Sec-ond, agent [Footnote_2] may have retrieved passages better suited for the downstream answer selection algorithm than those re-trieved by agent 1."
"2 On the other hand, such redundancy may result in error compounding, as discussed in Section 5.3."
"Third, passages from agent [Footnote_2] may con-tain additional occurrences of the correct answer, which boosts the system’s confidence in the answer through the redundancy measure. [Footnote_2]"
"2 On the other hand, such redundancy may result in error compounding, as discussed in Section 5.3."
"2 On the other hand, such redundancy may result in error compounding, as discussed in Section 5.3."
Our passage-level combination algorithm adds to the passages extracted by the knowledge-based agent the top-ranked passages from the statistical agent that contain candidate answers of the right type.
"More specifically, the statistical agent’s passages are semantically annotated and the top 10 passages containing at least one candidate of the expected answer type(s) are selected. [Footnote_3]"
3 We selected the top 10 passages so that the same number of passages are considered from both answering agents.
"The answer selection component identifies, from a set of passages, the top n answers for the given question, with their associated confidence scores."
An answer-level combination algorithm takes the top answer(s) from the individual answering agents and determines the overall best answer(s).
"Of our three combination algorithms, this most closely resembles traditional ensemble methods, as voting takes place among the end results of individual an- swering agents to determine the final output of the ensem-ble."
"We developed two answer-level combination algo-rithms, both utilizing a simple confidence-based voting mechanism, based on the premise that answers selected by both agents with high confidence are more likely to be correct than those identified by only one agent. [Footnote_4]"
4 In future work we will be investigating weighted voting schemes based on question features.
"In both algorithms, named entity normalization is first per-formed on all candidate answers considered."
"In the first algorithm, only the top answer from each agent is taken into account."
"If the two top answers are equivalent, the answer is selected with the combined confidence from both agents; otherwise, the more confident answer is se-lected. [Footnote_5]"
5 The confidence values from both answering agents are nor-malized to be between 0 and 1.
"In the second algorithm, the top [Footnote_5] answers from each agent are allowed to participate in the voting pro-cess."
5 The confidence values from both answering agents are nor-malized to be between 0 and 1.
Each instance of an answer votes with a weight equal to its confidence value and the weights of equiv-alent answers are again summed.
"The answer with the highest weight, or confidence value, is selected as the system’s final answer."
"Since in our evaluation, the second algorithm uniformly outperforms the first, it is adopted as our answer-level combination algorithm in the rest of the paper."
"To assess the effectiveness of our multi-level answer res-olution algorithm, we devised experiments to evaluate the impact of the question, passage, and answer-level combi-nation algorithms described in the previous section."
The baseline systems are the knowledge-based and sta-tistical agents performing individually against a single reference corpus.
"In addition, our earlier experiments showed that when employing a single answer finding strategy, consulting multiple text corpora yielded better performance than using a single corpus."
"We thus con-figured a version of our knowledge-based agent to make use of three available text corpora, [Footnote_6] the AQUAINT cor-pus (news articles from 1998-2000), the TREC corpus (news articles from 1988-1994), [Footnote_7] and a subset of the En-cyclopedia Britannica."
6 The statistical agent is currently unable to consult multiple corpora.
"7 Both the AQUAINT and TREC corpora are available from the Linguistics Data Consortium,[URL_CITE]"
This multi-source version of the knowledge-based agent will be used in all answer resolu-tion experiments in conjunction with the statistical agent.
"We configured multiple versions of PIQUANT to eval-uate our question, passage, and answer-level combination algorithms individually and cumulatively."
"For cumula-tive effects, we 1) combined the algorithms pair-wise, and 2) employed all three algorithms together."
The two test sets were selected from the[REF_CITE]and 11 QA track questions[REF_CITE].
"For both test sets, we eliminated those questions that did not have known answers in the reference corpus."
"Further-more, from the[REF_CITE]test set, we discarded all defini-tion questions, [Footnote_8] since the knowledge-based agent adopts a specialized strategy for handling definition questions which greatly reduces potential contributions from other answering agents."
8 Definition questions were intentionally excluded by the track coordinator in the[REF_CITE]test set.
This results in a[REF_CITE]test set of 313 questions and a[REF_CITE]test set of 453 questions.
We ran each of the baseline and combined systems on the two test sets.
"For each run, the system outputs its top answer and its confidence score for each question."
All answers for a run are then sorted in descending order of the confidence scores.
Two established TREC QA eval-uation metrics are adopted to assess the results for each run as follows:
Table 1 shows our experimental results.
The top sec-tion shows the comparable baseline results from the sta-tistical agent (SA-SS) and the single-source knowledge-based agent (KBA-SS).
"It also includes results for the multi-source knowledge-based agent (KBA-MS), which improve upon those for its single-source counterpart (KBA-SS)."
"The middle section of the table shows the answer resolution results, including applying the question, pas-sage, and answer-level combination algorithms individu-ally (Q, P, and A, respectively), applying them pair-wise (Q+P, P+A, and Q+A), and employing all three algo-rithms (Q+P+A)."
"Finally, the last row of the table shows the relative improvement by comparing the best perform-ing system configuration (highlighted in boldface) with the better performing single-source, single-strategy base-line system (SA-SS or KBA-SS, in italics)."
"Overall, PIQUANT’s multi-strategy and multi-source approach achieved a 35.0% relative improvement in the number of correct answers and a 32.8% improvement in average precision on the[REF_CITE]data set."
"Of the com-bined improvement, approximately half was achieved by the multi-source aspect of PIQUANT, while the other half was obtained by PIQUANT’s multi-strategy feature."
"Al-though the absolute average precision values are com-parable on both test sets and the absolute percentage of correct answers is lower on the[REF_CITE]data, the im-provement is greater[REF_CITE]in both cases."
"This is because the[REF_CITE]questions were taken into ac-count for manual rule refinement in the knowledge-based agent, resulting in higher baselines on the[REF_CITE]test set."
We believe that the larger improvement on the previ-ously unseen[REF_CITE]data is a more reliable estimate of PIQUANT’s performance on future test sets.
"We applied an earlier version of our combination algo-rithms, which performed between our current P and P+A algorithms, in our submission to the[REF_CITE]QA track."
"Using the average precision metric, that version of PI-QUANT was among the top 5 best performing systems out of 67 runs submitted by 34 groups."
A cursory examination of the results in Table 1 allows us to draw two general conclusions about PIQUANT’s performance.
"First, all three combination algorithms ap-plied individually improved upon the baseline using both evaluation metrics on both test sets."
"In addition, overall performance is generally better the later in the process the combination occurs, i.e., the answer-level combina-tion algorithm outperformed the passage-level combina-tion algorithm, which in turn outperformed the question-level combination algorithm."
"Second, the cumulative im-provement from multiple combination algorithms is in general greater than that from the components."
"For in-stance, the Q+A algorithm uniformly outperformed the Q and A algorithms alone."
"Note, however, that the Q+P+A algorithm achieved the highest performance only on the[REF_CITE]test set using the % correct metric."
We believe that this is because of compounding errors that occurred during the multiple combination process.
"In ensemble methods, the individual components must make different mistakes in order for the combined sys-tem to potentially perform better than the component sys-tems[REF_CITE]."
"We examined the differences in results between the two answering agents from their question analysis, passage retrieval, and answer selection components."
"We focused our analysis on the potential gain/loss from incorporating contributions from the sta-tistical agent, and how the potential was realized as actual performance gain/loss in our end-to-end system."
"At the question level, we examined those questions for which the two agents proposed incompatible answer types."
"On the[REF_CITE]test set, the statistical agent in-troduced correct answer types in 6 cases and incorrect answer types in 9 cases."
"As a result, in some cases the question-level combination algorithm improved system performance (comparing A and Q+A) and in others it degraded performance (comparing P and Q+P)."
"On the other hand, on the[REF_CITE]test set, the statistical agent introduced correct and incorrect answer types in 15 and 6 cases, respectively."
"As a result, in most cases perfor-mance improved when the question-level combination al-gorithm was invoked."
The difference in question analysis performance again reflects the fact th[REF_CITE]ques-tions were used in question analysis rule refinement in the knowledge-based agent.
"At the passage level, we examined, for each ques-tion, whether the candidate passages contained the cor-rect answer."
Table 2 shows the distribution of ques-tions for which correct answers were (+) and were not (-) present in the passages for both agents.
The bold-faced cells represent questions for which the statistical agent retrieved passages with correct answers while the knowledge-based agent did not.
"This is because the statistical agent’s proposes in its 10 passages, on average, 29 candi-date answers, most of which are incorrect, of the proper semantic type per question."
"As the downstream answer selection component takes redundancy into account in an-swer ranking, incorrect answers may reinforce one an-other and become top ranked answers."
This suggests that the relative contributions of our answer selection features may not be optimally tuned for our multi-agent approach to QA.
We plan to investigate this issue in future work.
"At the answer level, we analyzed each agent’s top 5 answers, used in the combination algorithm’s voting pro-cess."
"Table 3 shows the distribution of questions for which an answer was found in 1st place, in 2nd-5th place, and not found in top 5."
"Since we employ a linear vot-ing strategy based on confidence scores, we classify the cells in Table 3 as follows based on the perceived likeli-hood that the correct answers for questions in each cell wins in the voting process."
"The boldfaced and underlined cells contain highly likely candidates, since a correct an-swer was found in 1st place by both agents. [Footnote_9] The bold-faced cells consist of likely candidates, since a 1st place correct answer was supported by a 2nd-5th place answer."
"9 These cells are not marked as definite because in a small number of cases, the two answers are not equivalent. For exam-ple, for the TREC 9 question, “Who is the emperor of Japan?”, Hirohito, Akihito, and Taisho are all considered correct answers based on the reference corpus."
"The italicized and underlined cells contain possible can-didates, while the rest of the cells cannot produce correct 1st place answers using our current voting algorithm."
These results represent success rates of 79.9% and 79.4% for our answer-level combination algorithm on the two test sets.
There has been much work in employing ensemble meth-ods to increase system performance in machine learning.
"In NLP, such methods have been applied to tasks such as POS tagging[REF_CITE], word sense dis-ambiguati[REF_CITE], parsing[REF_CITE], and machine translati[REF_CITE]."
"In question answering, a number of researchers have investigated federated systems for identifying answers to questions."
"For example,[REF_CITE]and[REF_CITE]employ techniques for utilizing both unstruc- tured text and structured databases for question answer-ing."
"However, the approaches taken by both these sys-tems differ from ours in that they enforce an order be-tween the two strategies by attempting to locate answers in structured databases first for select question types and falling back to unstructured text when the former fails, while we explore both options in parallel and combine the results from multiple answering agents."
The multi-agent approach to question answering most similar to ours is that[REF_CITE].
"They applied ensemble methods to combine the 67 runs sub-mitted to the[REF_CITE]QA track, using an unweighted centroid method for selecting among the 67 proposed an-swers for each question."
"However, their combined sys-tem did not outperform the top scoring system(s)."
"Fur-thermore, their approach differs from ours in that they fo-cused on combining the end results of a large number of systems, while we investigated a tightly-coupled design for combining two answering agents."
"In this paper, we introduced a multi-strategy and multi-source approach to question answering that enables com-bination of answering agents adopting different strategies and consulting multiple knowledge sources."
"In partic-ular, we focused on two answering agents, one adopt-ing a knowledge-based approach and one using statistical methods."
"We discussed our answer resolution component which employs a multi-level combination algorithm that allows for resolution at the question, passage, and answer levels."
"Best performance using the % correct metric was achieved by the three-level algorithm that combines af-ter each stage, while highest average precision was ob-tained by a two-level algorithm merging at the question and answer levels, supporting a tightly-coupled design for multi-agent question answering."
Our experiments showed that our best performing algorithms achieved a 35.0% relative improvement in the number of correct an-swers and a 32.8% improvement in average precision on a previously unseen test set.
This paper describes a novel multi-stage recog-nition procedure for deducing the spelling and pronunciation of an open set of names.
The overall goal is the automatic acquisition of un-known words in a human computer conver-sational system.
"The names are spoken and spelled in a single utterance, achieving a con-cise and natural dialogue flow."
The first recog-nition pass extracts letter hypotheses from the spelled part of the waveform and maps them to phonemic hypotheses via a hierarchical sub-lexical model capable of generating grapheme-phoneme mappings.
"A second recognition pass determines the name by combining information from the spoken and spelled part of the wave-form, augmented with language model con-"
Spoken dialogue systems are emerging as an effective means for humans to access information spaces through natural spoken interaction with computers.
"These sys-tems are usually implemented in such a way that their knowledge space is static, or is only augmented through human intervention from the system developers."
A sig-nificant enhancement to the usability of such systems would be the ability to automatically acquire new knowl-edge through spoken interaction with its end users.
"Such knowledge would include both the spelling and pronunci-ation of a new word, as well as an understanding of its us-age in the language (e.g., a semantic category)."
"However, this is a difficult task to carry out effectively, challeng-ing both with regard to the automatic acquisition of the sound-to-letter mapping from typically telephone-quality speech, and the system level aspect of integrating the usually off-line activities of system upgrade while seam-lessly continuing the conversation with the user."
"The research reported here is concerned with the ac-quisition of the user’s name, which is entered via a “speak and spell” mode, spoken sequentially for the first and last names respectively."
"It is our belief that this would be the most natural way for the user to enter the information, and therefore research has been focused on designing a framework to support that model."
"Acquiring names is particularly difficult, not only because English is known to have highly irregular letter-to-sound rules, but also be-cause American names come from a diverse collection of language groups."
"With the speak and spell entry mode, there are additional issues of locating the boundary be-tween the spoken and spelled portions of the utterance, and of formulating a joint solution."
"The framework for acquiring new names is applied to an enrollment phase of an existing spoken dialogue sys-tem, the ORION task delegation system[REF_CITE]."
"O RION allows users to specify tasks to be com-pleted off-line, and to later be delivered to the user at a designated time, via either telephone or e-mail."
"To en- ter the enrollment phase, the user calls the ORION system and responds to the prompt by saying, “new user,” which causes the system to enter a system-initiated subdialogue soliciting a number of facts to be entered into a form that will represent the system’s future knowledge of this indi-vidual."
"The system solicits the user’s full name, a pass-word for security measures, their work, home, and cell phone numbers, and their e-mail address, finally asking for the current time in order to establish the user’s refer-ence time zone."
"After all of the information has been entered, the sys-tem confirms its proposed spellings for the names, and, if verified by the user, automatically launches a system up-date that enters this new information into both the speech recognition component and the natural language (NL) grammar."
"Thus, the next time the user calls the system, they will be able to log on by speaking their name and password."
"If the user rejects the proposed spelling, the system solicits further input from them, in the form of a telephone keypad entry of the spelling of the misrecog-nized name."
This information is then incorporated into the search to propose a final hypothesis.
"Central to our methodology is the application of ANGIE[REF_CITE], a hierarchical framework capturing subword structure information, employed here to predict phoneme-grapheme mappings."
"In ANGIE , corpus-based statistical methods are combined with ex-plicit linguistic information to generalize from the obser-vation space to unseen words."
"Our approach extends work reported earlier[REF_CITE], in which spelling and pronunciation of unknown names are extracted from spoken input with the additional constraint of telephone keypad input."
"This work distinguishes itself in that, instead of requiring tele-phone keypad entries, a user is asked to speak and spell their name within a single utterance."
"The novelty lies in the use of a multi-stage recognizer, where the first stage proposes a letter graph derived from the spelled portion of the waveform."
A second recognition pass searches the pronounced name part of the waveform; this final search is constrained by a phoneme space derived from the letter graph via ANGIE letter-to-sound mappings.
"In the following, previous related work is outlined in Section 2."
"Sections 3 details the technology of sound-to-letter acquisition, and the techniques used to implement a recognition engine to serve our unique needs."
Section 4 is primarily concerned with the engineering aspects for the real time implementation.
Section 5 describes some evaluation results.
This paper concludes with a summary and a look to the future in Section 6.
"In the past, many researchers have worked on letter-to-sound algorithms for text-to-speech conversi[REF_CITE]."
"More recently, research is begin-ning to emerge in bi-directional sound-letter gener-ation and phoneme-to-grapheme conversion."
"These topics are important for application to speech recog-nition, for the purpose of automatically transcribing out-of-vocabulary (OOV) words at the spoken input."
"In[REF_CITE], a hierarchical approach was used for bi-directional sound-letter generation."
"On the Brown Corpus, it achieves word accuracies of 65% for spelling-to-pronunciation and 51% for pronunciation-to-spelling."
"Rentzepopoulos[REF_CITE]describes a hidden Markov model approach for phoneme-to-grapheme conversion, in seven Euro-pean languages on a number of corpora."
The algo-rithm gave high accuracies when applied to correctly transcribed words but was not applied to real recogni-tion output.
"The work of Marchand and Damper[REF_CITE]addresses both phoneme-to-grapheme and grapheme-to-phoneme conversion using a fusion of data-driven and pronunciation-by-analogy methods, obtaining word accuracies of 57.7% and 69.1% for phoneme-to-grapheme and grapheme-to-phoneme ex-periments respectively."
These were performed on a cor-pus of words from a general dictionary.
"Some work has focused on proper names, since names are a particularly challenging open set."
"In[REF_CITE], the problem of generating pronunciations for proper names is addressed."
A 45.5% word error rate is reported on a set of around 4500 names us-ing a decision tree method.
"Font Llitjos[REF_CITE]reports improvements on letter-to-sound performance on names by adding language ori-gin features, reporting 61.72% word accuracy on 56000 names."
"Galescu[REF_CITE]addresses bi-directional sound-letter generation using a data-driven joint -gram method on proper nouns, yielding around 41% word accuracy for letter-to-sound and 68% word ac-curacy for sound-to-letter."
Few have attempted to convert a spoken waveform with an unknown word to a grapheme sequence.
"Using a Dutch corpus, Decadt et al.[REF_CITE]use a memory-based phoneme-to-grapheme converter to derive graphemic output from phonemic recognition hypothe-ses.[REF_CITE].3% accuracy on training data but only 7.9% accuracy on OOV recognition test data."
"In a German system, Schillo[REF_CITE]built a grapheme recognizer for isolated words, towards the goal of unconstrained recognition in German."
Accuracies at-tained are up to 72.89% for city names.
The approach adopted in this work utilizes a multi-pass strategy consisting of two recognition passes on the spo-ken waveform.
"The first-stage recognizer extracts the spelled letters from the spoken utterance, treating the pro-nounced portion of the word as a generic OOV word."
"This is followed by an intermediate stage, where the hy-potheses of the letter recognition are used to construct a pruned search space for a final sound-to-letter recog-nizer which directly outputs grapheme sequences."
The ANGIE framework serves two important roles simultane-ously: specifying the sound/letter mappings and provid-ing language model constraints.
"The language model is enhanced with a morph N-gram, where the morph units are derived via corpus-based techniques."
"In the follow-ing sections, we first describe the ANGIE framework, fol-lowed by a detailed description of the multi-pass proce-dure for computing the spelling and pronunciation of the word from a waveform."
A NGIE is a hierarchical framework that encodes sub-word structure using context-free rules and a probability model.
"When trained, it can predict the sublexical struc-ture of unseen words, based on observations from training data."
"The framework has previously been applied in bi-directional letter/sound generati[REF_CITE], OOV detection in speech recogniti[REF_CITE], and phonological modeling[REF_CITE]."
"A parsing algorithm in ANGIE produces regular parse trees that comprise four distinct layers, capturing linguis-tic patterns pertaining to morphology, syllabification, phonemics and graphemics."
An example parse for the word “Benjamin” is given in Figure 1.
Encoded at the pre-terminal-to-terminal layers are letter-sound map-pings.
The grammar is specified through context-free rules; context dependencies are captured through a superimposed probability model.
The adopted model is motivated by the need for a balance between sufficient context constraint and potential sparse data problems from a finite observation space.
"It is also desirable for the model to be locally computable, for practical reasons associated with the goal of attaching the learned probabilities to the arcs in a finite state network."
"Given these considerations, the probability formulation that has been developed for ANGIE can be written as follows:         !&quot; # where .- % /&amp;$ % is the &apos;&quot;( ) column - in the parse tree and *$ %,+ , and % / 0 is the label at the 5:( ) row of the &quot;&apos; ( ) column in the two-dimensional parse grid. 8 is the total number of layers in the parse tree. &apos; and 5 start at the bottom left corner of the parse tree."
"In other words, each letter is predicted based on the entire preceding col-umn, and the column probability is built bottom-up based on a trigram model, considering both the child and the left sibling in the grid."
The probabilities are trained by tabulating counts in a corpus of parsed sentences.
"After training, the ANGIE models can be converted into a finite state transducer (FST) representation, via an algo-rithm developed[REF_CITE]."
"The FST compactly represents sound-to-letter mappings, with weights on the arcs encoding mapping probabilities along with subword structure."
"In essence, it can be considered as a bigram model on units identified as vertical columns of the parse tree."
"Each unit is associated with a grapheme and a phoneme pronunciation, enriched with other contextual factors such as morpho-syllabic properties."
"The FST out-put probabilities, extracted from the ANGIE parse, repre-sent bigram probabilities of a column sequence."
"While efficient and suitable for recognition search, this column bigram FST preserves the ability to generalize to OOV data from observations made at training."
"That is, despite having been trained on a finite corpus, it is capable of cre-atively licensing OOV words with non-zero probabilities."
"In this work, the probability model was trained on a lexicon of proper nouns, containing both first and last names."
"During the initial lexical acquisition phase, over 75,000 entries were added to the lexicon via an automatic procedure."
"Because this yielded many errors, manual cor-rections have been made, and are ongoing."
"In a second phase, a further 25,000 names are automatically added to the lexicon, using a two-step procedure."
"First, the gram-mar is trained on the original 75,000 words, then using the trained grammar, ANGIE is used to parse the addi-tional 25,000 new names."
These parses are immediately added to the full lexicon.
"Despite generating many erro-neous parses, performance improved with the additional training data."
"After training on the total 100,000 words, the column bigram FST is highly compact, containing around 2100 states and 25,000 arcs."
"In total, there are 214 unique graphemes (some of which are doubletons such as “th”) and 116 unique phoneme units."
The multi-stage speak and spell approach is tailored to accommodate utterances with a spoken name followed by the spelling of the name.
"As depicted in Figure 2, there are three stages: the first is a letter recognizer with an unknown word model, outputting a reduced search space favored by the letter hypotheses; the second pass com- piles the language models and sound-to-letter mappings into the reduced search space; a final pass uses the scores and search space defined in the previous stage to perform recognition on the waveform, simultaneously generating spelling and phonemic sequences on the word."
"At the core of this approach is the manipulation of FSTs, which permits us to flexibly reconfigure the search space during recognition time."
The entire linguistic search space in the recognizer can be represented by a single FST ( ) which embeds all the language model probabilities at the arc transitions.
"Generally, is rep-resented by a cascade of FST compositions: +  $ (1) where $ contains diphone label mappings, applies phonological rules, maps the lexicon to phonemic pro-nunciations, and is the language model."
The above compositions can be performed prior to run-time or on the fly.
"The first stage is a simple letter recognizer augmented with an OOV word model[REF_CITE], which is designed to absorb the spoken name portion of the waveform."
"The recognition engine is segment-based, us-ing context-dependent diphone acoustic units[REF_CITE]."
"Trained on general telephone-based data (which do not contain spelled names), the acoustic models con-tain 71 phonetic units and 1365 diphone classes."
"Using Bazzi’s OOV word modeling scheme, unknown words are represented by variable-length subword units that have been automatically derived."
"The language model, a letter 4-gram, is trained on a 100,000 name corpus, aug-mented with an unknown word at the beginning of each sentence."
"This first stage outputs a lattice in the form of an FST, which contains, at the output labels, an unknown word label for the spoken name part of the utterance and letter hypotheses which are useful for the later stages."
"A series of FST operations are performed on the out-put of the first stage, culminating in an FST that de-fines a reduced search space and integrates several knowl-edge sources, for the second recognition pass."
"Since the waveform consists of the spoken word followed by the spelling, the output FST of this stage is the concatenation of two component FSTs that are responsible for recogniz-ing the two portions of the waveform: a first FST maps phone sequences directly to letters, and a second FST, which supports the spelling component, maps phones to the spelled letters."
"The first FST is the most knowledge-intensive because it integrates the first pass hypotheses with their corre-sponding scores, together with additional language mod- els and ANGIE sound-to-letter mappings."
A subword tri-gram language model is applied to subword units that are automatically derived via a procedure that maximizes mutual information.
"Similar to work[REF_CITE], where subword units are derived from phones, the procedure employed here begins with letters and itera-tively combines them to form larger units."
"The following describes the step-by-step procedure for generating such a final FST ( ) customized for each spe-cific utterance, beginning with an input lattice ( ) from the first stage. preserves the acoustic and language model scores of the first stage. 1. Apply subword language model: is composed with a subword trigram ( )."
"The trigram is applied early because stronger constraints will prune away The composition involves  , mapping letter  improbable sequences, reducing the search space. se-quences to their respective subword units and , (  ) with letters at both the inputs and outputs, the reverse mapping."
This step produces an FST  +  where  (2) 2. Apply ANGIE model:  is composed with the col-umn bigram FST ( ).
This requires an intermedi-grapheme symbols.
"The result is  , where ate FST ( ), mapping letter sequences to ANGIE   +  (3) codifies language information from ANGIE , a subword trigram, and restrictions imposed by the letter recognizer."
"Given a letter sequence, out-puts phonemic hypotheses. 3. Apply phonological rules: The input and output se-quences of are reversed to yield , and we ap-   ply + (4) This expands ANGIE phoneme units to allowable phonetic sequences for recognition, in accordance with a set of pronunciation rules, using an algorithm described[REF_CITE]."
The resultant FST ( ) is a pruned lattice that embeds all the nec-essary language information to generate letter hy-potheses from phonetic sequences. 4. Create second half FST:
The FST ( ) necessary for processing the spelling part of the waveform is con-structed.
"This begins by composing , the FST con-an FST (  ) representing baseforms for the letters, taining letter hypotheses from the first stage, with followed by the application of phonological rules, similar to Step 3. +   (5) letter hypotheses, is input to an intermediate stage, where (see steps 1 to 3 in Section 3.2.2) is concatenated with (step 4)."
The result defines the search space for the final stage. 5.
Concatenate two parts: The final FST ( ) is created by concatenating the FSTs corresponding with the first ( ) and second ( ) portions of the speak and spell waveform. + (6)
"As described above, is particularly rich in knowl-edge constraints, because all the scores of the first stage are preserved."
"These are acoustic and language model scores associated with those hypotheses, deter-mined from the spelled part of the waveform."
"Hence contains hypotheses that are favored by the language and acoustics scores in the letter recognition pass, to be ap-plied to the spoken part of the waveform in the next pass."
The scores are enriched with an additional subword tri-gram and the ANGIE model to select plausible sound-to-letter mappings.
"The sound-to-letter recognizer conducts an entirely new search, using the enriched language models in a re-duced search space, along with the original acoustic mea-surements from the first pass."
"Mapping phonetic sym-bols to letter symbols, the input FST ( ) is equivalent to   , incorporating phonological rules and lan-guage constraints."
"It is then composed on-the-fly with a pre-loaded diphone-to-phone FST ( $ ), thereby complet-ing the search space as defined in Equation 1."
"The final letter hypothesis for the name is extracted from the output corresponding to the spoken name por-tion of the utterance, taken from the highest scoring path."
"Essentially, this final pass integrates acoustic information from the spelled and spoken portions of the waveform, with language model information from the grapheme-phoneme mappings and the morph N-gram."
Phoneme extraction is performed using an additional pass through the search engine of the recognizer.
"In the ORION system, the phoneme sequence is only computed after the user has confirmed the correct spelling."
"The procedure is analogous to the sound-to-letter process de-scribed above, except that, instead of using output from the first-stage letter recognizer, a single letter sequence constrains the search."
"The sequence may either be the answer as confirmed by the user during dialogue, or the highest scoring letter sequence output from the sound-to-letter recognizer."
"A series of FST compositions is per-formed to create an FST that can compute a phonemic sequence in accordance with ANGIE model mappings, as-sociated with the given letter sequence and the acoustic waveform."
"Again, the FST contains two portions, for pro-cessing each half of the speak and spell waveform."
"The first applies ANGIE to map phonetic symbols to phonemic symbols, restricted to paths that correspond with the in-put letter sequence."
The second half supports the spelled letter sequence.
"Following FST creation, the final FST is uploaded to the search engine, which conducts a new search using the FST and the original acoustic measure-ments."
The phoneme sequence for the name is taken as the output from the highest scoring path corresponding with the spoken part of the waveform.
"Our integration experiments were conducted in the ORION system, which is based on the GALAXY Commu-nicator architecture[REF_CITE]."
"In GALAXY , a central hub controls a suite of specialized servers, where interaction is specified via a “hub program” written in a scripting language."
"In order to carry out all of the activities required to specify, confirm, and commit new words to the sys-tem’s working knowledge, several augmentations were required to the pre-existing ORION system."
"To facili-tate the automatic new word acquisition process, two new servers have been introduced: the FST constructor and the system update server."
The role of the FST construc-tor is to perform the series of FST compositions to build FST as described previously.
"Via rules in the hub pro-gram, the constructor processes the output of the letter recognizer to derive an FST that becomes input to the fi-nal sound-to-letter recognizer."
"The second new server introduced here is the system update server, which comes into play once the user has confirmed the spelling of both their first and last names."
"At this point, the NL server is informed of the new word additions."
It has the capability to update its trained gram-mar both for internal and external use.
It also creates a new lexicon and class -gram for the recognizer.
"In addition to the NL update, the recognizer also needs to incorporate the new words into its search space."
"At present, we are approaching this problem by recompiling and reloading the recognizer’s search FSTs."
"In the future, we plan to augment the recognizer to support incremen-tal update of the lexical and language models."
"The system update server is tasked with re-generating the FSTs asyn-chronously, which are then automatically reloaded by the recognizer."
"Both the recognizer and the NL system are now capable of processing the newly specified name, a capability that will not be needed until the next time the new user calls the system."
One interesting aspect of the implementation for the above processing is that the system is able to make use of parallel threads so that the user does not experience delays while their name is being processed through the multiple stages.
Figure 3 illustrates a block diagram of the dialogue flow.
The letter recognizer processes the user’s first name during the main recognition cycle of the turn.
"Subsequently, a parallel second thread is launched, in which the second stage recognizer searches the FST created by the FST constructor as described previously."
"In the mean time, the main hub program continues the dialogue with the user, asking for information such as contact phone numbers and email address."
The user’s last name is processed similarly.
"At the end of the dia-logue, the system confirms the two names with the user."
"If they are verified, a system update is launched, while the system continues the dialogue with the user, perhaps enrolling their first task."
"If the user rejects a proposed spelling, the system will prompt them for a keypad entry of the name[REF_CITE], which will pro-vide additional constraints."
Experiments have been conducted to evaluate the ability to acquire spellings and pronunciations for an open set of names.
"We have selected a test set that combines utter-ances from a preliminary ORION data collection during new user enrollment and previous utterances collected from the JUPITER system[REF_CITE], where at the beginning of each phone call, users are asked to speak and spell their names in a single utterance."
"However, the trained models are designed to support both first and last names."
"As yet, no attempts have been made to separately model first and last names."
Two test sets are used for evaluation.
"Test Set A con-tains words that are present in ANGIE ’s 100K training vo-cabulary with 416 items of which 387 are unique; Test Set B contains words that are previously unseen in any of the training data, with 219 items of which 157 are unique."
These test sets have been screened as best as possible to ensure that the spelled component corresponds to the spo-ken name in the utterance.
"For each test set, letter error rates (LER) and word error rates (WER) are computed for the output for the first let-ter recognizer, and the output for the entire multi-stage system (Multi-Stage System I)."
"In an additional experi-ment, the subword trigram is omitted in the intermediate stage (Multi-Stage System II)."
Results are summarized in Tables 1 and 2.
"When evaluating output from the first-stage letter rec-ognizer only, it is found that errors remain high (40.4% WER for Test Set A and 58.9% WER for Test Set B)."
"It should be noted that none of the training data for the acoustic models contain any letter spellings, contribut-ing to relatively poor performance compared to that in other domains using the same models."
Many of the er-rors are also caused by poor detection of the transition from the spoken name to the spelled portion of the wave-form.
Deletions occur when part of the spelled portion is mistakenly identified as part of the unknown word or in-sertions arise when the end of a spoken word is confused for a spelled letter.
"However, the multi-stage system pro-duces a marked improvement if we compare it with the single-stage letter recognizer as a baseline."
"WER im-proves by 36.4% (from 40.4% to 25.7%) for Test Set A, and 17.0% (from 58.9% to 48.9%) for Test Set B."
The improvement is more pronounced for Test Set A because the words have been observed in the ANGIE training data.
"The most commonly confusable letter pairs are: M/N, A/E, J/G, Y/I, L/O, D/T. These letters are confusable both acoustically in the spelled letters as well as in the pronun-ciation of the spoken word."
"When the subword trigram is removed from the lan-guage model in the later stages, further WER improve-ments result in Test Set B (46.1%), although performance in Test Set A deteriorates."
"We infer that unknown words benefit more with a less constrained language model, and when more weighting is given to the ANGIE model for generating possible spelling alternatives."
"To evaluate the phoneme extraction accuracy, the best letter hypothesis of the multi-stage system is used to com-pute the phonemes, as described in Section 3.2.4."
"In the actual ORION system, when a user confirms the correct spelling of their name, if the name exists in the training pronunciation lexicon, the phoneme extraction stage may be redundant."
"This assumes the pronunciation lexicon it-self is reliable, and contains all the correct alternate pro-nunciations of the word."
"For the purpose of evaluation, we examine the phoneme outputs of both in-vocabulary Test Set A, and OOV Test Set B, whose phonemic base-forms have been hand-transcribed."
"Within ANGIE , phonemes are marked for lexical stress and syllable onset positions."
"There are also many spe-cial compound phonemic units (e.g., /sp, sk, st/)."
"A much smaller phoneme set of 50 units is derived for evaluation, by applying rules to collapse the phoneme hypotheses."
The phoneme error rate (PER) for Test Set A and B are depicted in Table 3.
Error rates are provided for the sub-sets of words where the letter hypotheses are either cor-rect or incorrect.
"Many of the confusable phoneme pairs are vowels: ih/iy, ae/aa, eh/ey."
"Other commonly confused phoneme pairs are: m/n, en/n, er/r, l/ow, d/t, s/z, th/dh."
"In another experiment, we evaluated the accuracy of the phoneme extraction by using the correct letter se-quence as input, instead of the highest scoring letter se-quence."
The PER for Test Set A is 7.2% and the PER for Test Set B is 13.3%.
"While phoneme error rates are generally higher than letter error rates, it should be noted that the reference baseforms for the names contain only one or two alternate pronunciations for each name."
"How-ever, it is not uncommon for a name to have many irreg-ular pronunciation variants, which are not covered in the reference baseforms."
"Also the phonemic baseform de-termined by the recognizer is likely to be one preferred by the system for the particular speaker, assumed to be the owner, of the name."
"Therefore, we believe that the baseforms favored by the system may be more appropri-ate for subsequent recognition, especially if the name is to be spoken by the same speaker."
This may be the case in spite of the mismatch between the favored phonemic baseform and that in the pronunciation dictionary.
This paper has described a methodology and implementa-tion for automatically acquiring user names in the ORION task delegation system.
"It has been shown that a novel multi-stage recognition procedure can handle an open set of names, given waveforms with the spoken name fol-lowed by the spelled letters."
"The overall system is also capable of incorporating the new name immediately into its language and lexical models, following the dialogue."
Future work is needed on many parts of the system.
"As more data are collected, future experiments will be con-ducted with larger test sets."
We can improve the letter recognizer by explicitly modeling the transition between the unknown word and the spelling component.
"For in-stance, by adding prosodic features we may be able to improve the detection of the onset of the spelling part."
"Our final selection process is based only on the pro-posed spellings obtained from the pronounced word, after feeding information from the spelled part into the second stage."
"However, performance may improve if we apply a strict constraint during the search, explicitly allowing only paths where the spoken and spelled part of the wave-forms agree on the name spelling."
"Alternatively, a length constraint can be imposed on the letter sequence, once it has been observed that the second stage hypotheses for the spoken and the spelled components are inconsistent in length."
"As an unconstrained name recognizer, the system de-scribed here handles in the same way both observed data and previously unseen data."
We would like to experiment with adding a parallel component that explicitly models some in-vocabulary words.
"This may boost overall accu-racy by lexicalizing the most common names, such that only words that are identified as OOV need to be pro-cessed by the ANGIE sound-to-letter stage."
"In regards to implementation, the current hub-server configuration has allowed us to rapidly implement the system and conduct experiments."
"The multi-threaded ap-proach, implemented using the hub scripting language, has been effective in allowing a smooth dialogue to pro-ceed while the multi-stage processing takes place in the background."
"However, we anticipate that the multi-stage approach can be improved by folding all three stages into a single recognition server, eventually allowing real-time operation."
"In this case, multi-threading would only be needed for the final stage that incorporates the new words into the on-line system."
The long-term objective of this work is to learn the pro-nunciations and spellings of general OOV data in spoken dialogue systems on domains where OOV proper nouns are prevalent.
Future experiments will involve general classes of unknown words such as names of geographical locations or businesses.
¸ ­±§y«ª«ª¯¹¯¨º¥¦¶m¥¦­±¢» ª ¸ §y¨0·¼³(¨ ­±³§· ª³Q½ªL¡=³¦·sµ¢¥ª¯½L¥¯§c¡H­®¥¦¨±­±«ª°­®q«ª«¶m¥¦µ ¨®§y¡±³½§l¥À¬¨®Ác¡H¶m¥µ«¦µ« ¡H³·L¡H§y§ ¸ ª³Q½ªL¡.·¼³¨ ¢»ºÂ¯§yª ¸ §y¨ ¢¡cÃª³uªÄ³¨ ¬­±¨º¥¦ªL¡±µ« ª¯½L¥µ £¨®³¦Å6§l»­®°³Qª+³· ªB¥u­±½L¨®¥µ¯ ¸
Æ.ÇÂ§o¡±³u^¾ ­®L¥u­ ­®¨±³Q½¯È´B³³¦­º¡6­®¨®¥££«ª¨±³§y¡®¡o­®L¥u­ ½L¡±§y¡ ³ªV­±­®½L¥¦*µ µ«½§l·¼¨®³¶Ê¥¦ªJ½ªL¥¦ªLª³¦­º¥u­±§ ¸ ¨HË £¡±½½B¡ÌÉÎ¥ª ­®¨±¸ «§l³Q¥Q¨±£L½³¨®µ«¥¦³­±¯Q«»y¯¥¦§yµ ª ¸ µ«½§y¨#§l¡o£¨®§ ³¸ ¸¢»­®µ««§³¸¥­®ª ´Z§c«ª ¸   ¸   ¸ = ¡®¥¦Ó ¨ºµZ¥¦£¨±³QÒ
D4¶m¥u?­®¢» ¥¦µ4=¯Z36KsD&lt;§yª ¸ §y¨m¢¡m¥Ô³ªO´½ªL³¦­g½ªL°¬Q¨±Ë Ñ µ¢¥¦ª¯Q½L¥¦¯Q£B§+§y¨H·¼­6¨®³¶Ö¥qµ¢¥¦ª¯Q³Q½L¥¦£¯Q½­ºÆ¥u.­±#Õ«³QªªL¯Qµ«¥¦*µ ¡±@£Z³ÏQ­±«ªV­ §-³¦³Q¬V¡H«­4¡H§y¾-­±Ï ½ ¸¸ ³«§§y¸¡ ·¼ª³­g¸ L¥]¬§¯Q¨®¥¸ ¥u­±¢»y¥¦µs¯Qª ¸ ¨y(Æ
Ç×§yª0¯¨º¥¦¶m¥u­®¢» ¥¦µ¯ ³¨®¶mÏ¥Q¾/§c¡±³u(¾ ´L¨±«°­±L««ªV­±¨®³ ¸ ½L­±«³@ª Æ
"IKMY&lt;*GK,@ .&lt; [QP+=F %C [ \ ]&gt; &lt; DHP+^,@ GKZ&lt; [_C%BQ`)^QP[ +JC@ ·¼¨®³¶;µ¢¥¦¨®¯§m½LªL¥¦ªªL³¦­®¥¦­±§ ¸ ³Q¨±£Z³¨º¥­®³Â£ZªL¡®¥u­®§m·¼³Q¨ ­±L§cµ«¥Q»º³¦¢¡/­6Á£B§c³Ãª³uµ«§ ¸ ¯§ ¦Ï´ÁmÉ£µ«³Q­®«ª¯c­±L§ ¯§yª ¸ ¥¯§yªV­s³¦.· ª³½ªB¥¦ª ¸ °§ ¸ ¢¥u­®§o³ªV­®ÉÆ ª ( V 9 ÇÈ½§-§Ì­±£L¨®³¥¦£Z³Q¡±¥¦=§ª+¥ ´Z§­6/¾ ¡H½L³³¶m¥¦£Z¨®³ª°"
Øy§ §yªV¸ #­¥¡4´B³·¼³Q³¦­ºµ«°¡6³u­®¾o¨®¥£¿ £«ª¯c­®§y»ºË ­±ÉèV­®­®*¡¥¨H·¼³Q­®°ªL¨X¯Ô¬§y¨±+Á°­±(¯¥^ª ¸ µ°¢#¡H­mµ«¥Q³¦·Î¡±¡*¡±¥§¨±§¸ ¸ ª³Q­®½¨®ªLÏÀ«ª¨±§ §y¸ µ« ¥¦ª ¸ ³¨®§ ª³Q½ªL­±L¥¦¥¦¨®§s¡±§yª °­±-­±L³Q¡±§/³QªV­±É¥¦¨®#§ ÉV­®¨®¥Q»­®§ ¸ ·¼¨®³§ ¶w¸ ¡
Ì­±¥§4ª ¸ ­±¨®£ª.Æ³Q½ªL¡-/§ °³­±¨®£¨±§y³Qµ°µ°§y³Q¬u¯¥¦¢»ªV­Ì£³¨®ªV³­±£ZÉV¨±­ ­±«¥§l¨±¡ §Î³¦­±·­±L§yª§ ½L¡±§½«ªg­®§cµ¢¥¦ª¯Q½L¥¦¯Q§¦Æª³Q¡± ¸ ­®³qÁV£Z³¦­®§y¡±«¯¨º ¸ §y¨c·¼³¨Î¥¦µ«X­±L§ ª³Q½ªL¢¡·¼³Q¨m­®­®³§¸ ­®¥¥Q¨±¡±¯Q¡±½­gµ¢¥§lª¯­±½L¥§Ô¯§ÆrÉ«¡H­±§yªL³ª·m³¥ ¥Äµ°¢¡H­¶m¥¦³¦Ë· Å6³¨À¨±§l¡6­®¨±¢»­®«³@ª éBñ/½L¨®Øy¥¦ª¥¦ª ¸mò ¥¨±³u¾o¡±ÃÁqÜzó¦ôQôQóu¡±³u¾ ³u¾wª³½ªB¡o´Z§ Á£B³­±§l¡H«Øy§ ¸ ¨®µ«¢¥¦´Lµ°Ád½L¡±«ª¥d·¼³¨±Ë «¯Qªgµ¢¥¦ª¯Q½L¥¦¯Q§o#Õ ª¯µ«¢¡± ¸ ¢»­®°³QªL¥¦¨®Á¦Æ ªBßo³½Lª ªQ«ªË ¨®¸ § ¸ £­®O¥ ¦ª ¥¦ªB¸ ¥¦¨®§y¡±µ«Á¡H½¢¡µ°³·À­®­®§q¥¦µ«É¯³Q¥¦¨±£µ«§l+¡ «ª0³¶m¥¦Ë ª¢¥¦ªÏ¢¡/£¨®§yªV­±§ ¸ «ªm­®§Ì·¼³µ« ³u°ªL¯-¡±½´L¡±§y­±«³ªB¡s³­±L§ £L¥£B§yÆ-§y¡±½µ°­®¡o·¼³¨=¡±.+ ¶m¥¦¨®§ ¸ «ªè§l»­®°³Qª d2 Æ =ª§ ³ i7jZjZkNlhmNn ·X­±§Î³Q¡H­À«£B³Q¨H­º¥¦ªV¢¡±¡±½§y¡o°ªq¥´Z³³¦­®¡H­±¨º¥¦£Ëç ! fhge £«ª¯c£¨®³§l¡±¡X¢¡#­±§ Q½B¥¦ªV­±°­6Á+¥ª ¸ V½L¥ ¦µ«°­6Ác³¦·@­®=§ ¡±§ ¸
Æ ø8ªg¥+ °ªL°¶m¥µ°âZ³Q  ¨H­·¼¨º¥¦§y¾/³Q¨±ÃZÏ³Qª§o 9 ¾4³½µ ¸ µ« Ã§4­±³Î½L¡±§ ¥¡4·¼§y¾w§ ¸ ¡±·¼¨®§ ³¸ ¶ö¥¡4¸£Z§y³Qµ«Ám¡®¡H«´¥]¬]µ«=§¥«¥µ ¥ª ´¸ µ«L¥¨±]§l¬¡H=§ ³Q½­±¨º§y¶²Æ ´
Z§-§y¥¡±«µ ÁdË ­±¨º¥ªL½¥¦¥¦­±ßÔ½Õ#¨º¥¦ÐL¨ºªµB¯¯µ«¥¦¢¡±£Lª ¸£¨±ªL¨y³V¥³ÏV½»º¡HªL­®¥c¡o½L¡±¨H­±­®L°ªL§y¥¦¯Ì­À­±LL¥§s]°­®¬Q­®¨®+§ ¥­±§ÀªL¡HªL¥uµ«­±ª¨º­®¯Q¥¦ªL½½L¨º¥¦¥¦¡±µ¢¥uµu¯Q£¨®³¦Å6§l»­®­±«³Qª ªL¸ +³¦·°³Qª-³¦·Üzü.¡H§y«¬V¯ËË ¨®§Ä¶mîl¥QÝ¡±ÆrÇÂµ«°ªL§J§ ª³ªL³½L¡±ªL¸ ¡-«ªq§ ¸ ­±§y¥Â¨±¶m­±³¡-­®¥¦³µÀ/· ³¦ªL· ôÈ·¼°ªL°ªL¥ª ¸ o[Footnote_2] ¥¦­±½¨º¥¦µXª ¸ ¨ &apos;p y¥Q¡ 2 %r .&lt; CsC%*|+{ }~&lt;Z[Uz((_G&apos;FHPGKC%%C
2 RoNQ 76 &gt;/ F0&gt;Z&lt; ?L3 % = 8 36K#D 5
"BQacPW[ %q ¡±³u«ª.¥´µ«§ÎîÆ ¥¦´LÇ×µ°§§yµ°§l»­®¥Î¨±´³Qª«µ°«ª¥¦¯Qµ«½LÁO¥¦µ Üz¥¸ ¢»­®¡g¾4¥Q¡d­±§J»y¥¡±§·¼³¨g¢¡Hgÿo³¶m¥¦]ªL¬u¥ ¦«¥«µ°Ëª@Ï ü§y¨cªB»ºµ«¥@´BÏL§y¥¦µ¢¡=ª ¸¢¡ è¸ £L³Q¥ªª§m¢¡HB¥¦½ÝÏ­® ³­±§-¶m¥¦­±£¨®¢³¦¥µ°Å6«§l»ÁÆc­±«³QÇ×ªm³ ¦ªÄªL¥u¡±½L­®»º½È¨®¥¥µ ¯¸ ¢ªËË ­±¸#´Z§cª¯µ«§l½L§y¡®¡®¡H§ ¥¦¸ ¨®ÁqÜ¢­±L­±«¡o¾4­±¥Q¡¨º¥¸³ª¥¦§-µ«X­®·¼³Q¨À§mèµ«­®³u¨®¥¬§yªLª¡±µ«¥¦§¦Ï­±«³°­±ªB Î«ª½­±§l¡ Õ «³Qª¯QªL¥¦µ« ¨®¡±ÁJËj­±¢¡³Ëz­ºªL³¦­d¥¦¨®]¥ ­±¬uËjµ¢¥°¥µ¢¥ª¯´µ«§Î½L¥¯¦«^ª§sB¥¥¦ªÄ¨ ¸µ«Ëæ­±¨®³£Á ª¸ ¢»¢»d­±·¼«³Q³¨® ªL¶m¥¦¨®¥u+Á­yÏ*¥¥ªª 2Q ¡-³¦­±L§gÕ#ªË µ¢¥¦¢ª¡±¯Q½L¡±¥¦¯Q§y§ ¸¡s¡®ÝÆ÷¢»ºg¢¯QÐL¨º¨®¥¡6­¶m¥¦£¥u£L¨±­±¢»y³V¥¦¥µ»º¯§yª«¡m¸ §yµ°¨4«¢¡°4¨®­±§ ¸ µ¢¥u­®­®³Ô§ ¸ ­±L­®³§ ªL¥¦­±½¨º¥¦µ¯Q ¸ §yßoµ¢¡±³Ïu­®¨±§l¡H½Lµ­º*¡ ¶m¥]Ác¬u¥¦¨®+Á ¥»y³¨ ¸ «ª¯ ­±­®§c½L¡®¥¦¯Q§Ì³.· ­®+§ µ«§y§ ¸ ªL¥ ¦­±½¨º¥¦@µ  ¸ ª«ª ­±L§d]ª³¨®³£½L´Z¡d§cÜ¢·¼³¬§y¨±¨ÌÁmÉ·¼¨±§ Q½L£µ«§¦ªVÏB¾4°³¨ ª¸ §y¡=¾oµ«°ÃQ¡H§  «¨®§Ì ,   d¨®£B³³Q¨ ­±¶m¥L§q^ß ¡±§y³Qª ¸ ]"
Ï ³¨®§X¯§yª¨º¥¦µ¥££¨®³Q¥Q»ºÌ¢¡ ­®³ÀÉV­®¨®¥Q»@­ ·¼¨®³¶  ¨®¥VÝºÆ
K9 V½³ªB¨®£½LÏXª¥È½¶dµ«¢¡6´B§y¨+³¦·-³·Àª³Q³ªV½ªL­®¡m­®+¡µ«­±§ ­®¸ Ô³ª¢»­®ºÂ§­±´LL¥¡±ÁÄ¢¡d³¦·Ë ·¼¨®³¯§y§ ª ¸ §yÏ4=¨ ¥¦«ªª ·¼¸ ³Q¨±¡H¶m¥¦­±ÉÔ£L¥¦´­HÁ¨®ªL¸ ¢»­±Ï4«³Q¥¦ªLª ¥¦¸ ¨®Ám­±³Èµ«³µ«³¥ÃV´BËj§y½Lµ4@£ ­±Æ-§y¶ L«¡Ì¥¦°­±£
Ë ­®££¥¦¨®«­±ª³Q«¯¥Q¬»ºL¥Q·¼³¯Q¡ ½L¥¦¨d¥­®¨º´Z¥¦§ªV­®­H¯«§y¬¨d¡4§yª»º^­®LL¥¦¥u³Q­=¨±­±£L½L§d­±³q¡H§y¡±¡H§½L¸ ¡
À­±L½L§¥¦¡±¸§ Æqç¸ ­±=¥§¨±§ªÂ´Z³­®³¦£L§m­®¡H¨±³­±§l¨º­±¡H¥¦§yª£§y¨ËË ­®L´Z¯¥¦+¥§ ­±§ª ¸¸ ³¨®­m§ ³¸£Lª°³¨±§lÍm+Ë ¡H½³½§l´­®¡Î°Ëæµ°­±ÐL«ªª§¯Q¸ ½L¥¦É­®L¥¦µ¢¡H¸­±§yªL¥¦«ª§³ªL½³¦¥ªL·c¨±¥Áw¥Èªª¯§y³­®ª ¥u¸¢­®»º§0¨m¸ ¥´ª««µ¯ª«³ªV­ËË Ü ½BôÌ¥¦µ³¨*¸ ¢·¼»­±¾4«³QªL¨ y¥¦¨®Á³·ÝºÆÌ¡±½Lç=»º½d¨=¾/¯³Q³Q¨ ¥¸ *¡ °ªd¥¡o³¨­±¸ §y¡±*¨ ­±µ«³-§l»­=«ª«¡H¶m°¥Øyµ°§X.­±¡HL§­ ½¶m¥¦½L£B  §y¨±¬¢¡H«³Qªm¨±§  V½«¨±§ ¸
Æ h ª¢¥¦ª*Æ §rßw¡±µ«Á¡H¡6­®­#³¦¶ /· î VÏ¥ô ¡
O-«ªª³°­±½L¢¥¦ªLµ«¡sÁ ¾4¸ ¨®§oÉV­®¨®¥Q»¸ ­®§ ¸ ·¼¨®³¶¹­±L§ Úq=ê äÕ =4ËæÕs¥  ¡H  ¶m¥ª¢¥¦ªÈÉ³ª Ü¼#Õ ¨zÅH¥]¬§l¥¦µjÆ°Ï îyïQï ÝºÆc  êoª·¼³Q¨H­®½ªL¥u­®µ«Á¦Ï@¥¦µ« ª½­±ª³Q½ªL¡Ì¥¦¨®§+µ«¢¡6­®§ ¸ ¥Q¡ ¶m¥ . ¡® - ½Lµ°««­±L«¡/µ«É³Qª@Ï£¨®³´L¥´µ«+Á ´Z§y»y¥¦½L¡±³¨®£³Ë µ«³¯Q«»y¥¦µ« Á¦ÏVª½­®¨º¡À¥ª ¸ ¶m¥¡® ª§-¥¦¨®§ ¥¦µ« Ã§Ì­®§+¡±°ªË ¯½Lµ«¥¨  ·¼³Q¨ÀÉ¥£µ«§¦Ï +  * lÝo¢¡o¶m¥¡®µ°«ª§-«µ § ¯  ½L  µ«¥cÜª * ³½L  ªL¡o@Ý½X¨o¢¡ «ªª§ ­±L½­±§Î§ ¨y¡®sÆ ¥¦èÁ §cªV­®¥³Q»ªQ­±­®¢»É­¥ =«µ °Á¥QlÏ Lª ¶m§ ½¥®­¡®§¨½#µ«±¡ °ªª §Ë ¡±°ªL¯¸ ½­±µ¢¥¦¨Ìª³³Q½ªL³V¡H§Ì*Ï´B¡±³­6¾/­±§g¯­±L§-ª ¸ ­6/¾+¥¯¨®¥¡®§y¡4µ¢¥¦´Z­-»y¥¦ªªL³¦­ ´Z§ ­±½L¡±§=ç ªóÏ«î Vóm³·s­±L§qî Ï ô mÃªªÈª³Q½ªL ¡ ¥££B§l¥¦¨c«ª ¥L§s¥d¨±µ«£½B¬u@¥ªQ¡ ¡®¥¦£µ«§ÀªL³½ªL«­±§cµ¢¥ª¯½L¥¯§Æ
K2 ¥¦ª ¸ ¾4§*·¼³  ¡@³Q . ½   ¥¦­H­±§yªV­±«³ª-³ªÌ­®§y¡±§Xª  
Vè ¸ ­®¥¨Hª­®³°ªL½LªL¡·¼¨®¾³¶/§y¨±§w­±L³§q´­ºÕ#¥¦ª«ª¯Q§µ°¢¸ ¡±O´ªBÁö¥u­±£½L¨®¨®³¦¥Å6µ§yËæ¯­±§y«ª³ªþ¸ §y¨Î½B¡H¾4«ª³¨¯¹¸ ¥ Ï ¡®¥¦ªLª§ ¸ ¥ª ¸ çcñ4§ ¸ ´ «µ °ªL¯½L¥µq³¶m¥ª¢¥¦ªËæ#Õ ª¯Qµ°¢¡± ¸ ¢»­®«³ªL¥¨±ÁÆ4ç=ª§c¶m¥uÅ6³Qµ¢¥¡®³¦·X¡H§y§ ¸ ¥ª ¸ ¸ ¥u­®¾4¨®§ ¥ ¦½­±³¶m¥¦­±¢¥µ«°Áo¥ª ¸ ¡ ±Á¡6­®¥¦­±¥µ°§yµ°«ªL¥¦­±§ ¸ ªL¥ ¦§yµ«Á ­±¡±L¥¦¨®§Ì­±§L§ ¶Î½Lµ­®°Ëj¯Qª ¸=§y­±¨º¥¦ªL¡±µ¢¥u­®¡±§c°³QªqÜ¼§Æ§y¨±§ ¯LÆ¶m#¥µ«§-¥ª ¸ ·¼¶mª§oª³Q½ª¥µ«§Ì£B¥¦«¨ %TIK&lt;Z[++P %F *D GK&gt;A&lt;&lt;Z [ +)F M:.&gt;ACEDHIKFH&gt;=@ %&lt; &lt;. [
DHO.&gt;AC=GKC%%C BQacTQDH&gt; _&lt; &gt;AC=\KGK@A&gt;J[.G&apos;DHPW[ &gt;yFH&gt; @A@y  + IKFbC%*=.&lt; .@APDHI
"Æ ³ ¡  p´ µ Ê ¨+¨ Ì7¨Ê ¨ µ Ê ¨µ ¥ ³W³®(¿ (®­ ¿¸ ¡ %¥ ¶ ¨ Ê¨ µ ¨ µ  r *Ï@&lt;.QP[ +FYC%[~.&lt; &lt;.CY&gt;ARÔ7&lt; &lt;@A&gt;AC%µ yDHO&gt; ~DHO.yF&gt; &amp;M .&lt; +X &gt;JP+CJ&gt; &lt;ÖDHO.Z&lt; [&lt;*.&gt;A.&lt; PW[ GK.&lt; [ÝDHO.+P &gt;AFdTQFHIWV%P++X DH&gt; &lt;&lt; DHIÓß@JGK&lt; .+CÛà(áuâuáIKaoGK&lt;.&gt;&lt;ã  â  &lt; ,&lt;Q&gt;AC%Oã).Ø AI&apos;@ \ &lt;*æ Á .&lt; C?QPIKF&lt; acIKFHPD%*F .&lt; %C @ &gt;&lt; A@ @.&gt;%FHP+X+&lt;Z[+.&lt;  +&lt;Z[%hX+IKF%+Z&lt; [L&gt;A&lt;.X+IKF%+~D %D *F .&lt; %C @ &gt; &lt;&lt;._I %D *F .&lt; %C @ DH&gt; &lt;,âéaoGK@ Z&lt; [ÛM:P+aoGK@APOZGW\...&lt; &gt;J .B"
"PD%F*&lt;.CE @&gt; ,&lt;  ¼» &lt;.[QP+%F  %+.&lt; [LPW @ND%F*GK&lt;QC%@&gt;&lt; º» +&lt;Z[ %hX+IKF%+&lt;Z[.GK@ %F*GK&lt;QC%@]G&apos;DH&gt;.&lt;  DHOQP1@%DHP+F7O.G&apos;\ &gt;A&lt;.SG &lt;Z&lt; [+~F à( SQuw {&lt;Z[E} {:&gt;J&lt;.GK&lt;.AC&gt; %Oå*.ïäD%FHBQDHO QIKDcGW&lt; KGK\ A&gt;JGKQ@ @JP?à|GK@A@_AI&apos;@ \.&lt; Po@A&gt;AQF*G&apos;F%[Q&gt;AX+DHAIK&gt; &lt;&gt; GW\ &gt;A]@ .APDHI4B@ ._C [QI4QIKD&lt; _&gt;A&lt;.@ABZ[&lt;.QP[ +Fb&gt;A&lt;QM&amp;IKFHaoG&apos;DHJIK&gt; &lt;ÖGK.&lt; [ DHO.@AP+^ &gt; &lt; QI[ QIKD&lt; =&lt; D*&gt;J_&lt; DHO.%P&lt; &lt;. +Y($ .¥Î­±¨º¥¦ªL¡±µ¢¥u­±«³¦·.´Z³¦­®   ($ ¥¦ª ¸ õ ö ]ÏB¥Q/¾ §yµ°@¥Q¡   µ«½ ¸ °ªL¯ÌÀ£Z³¥³ª /¨¸ ³ @ÝºÆXßoµ«³¦­®¨*¡H§y§ ¸ .¡ ¾4­® ¥«ª§ ¸ Ü°ªË  ª§l¡®*Ý ³¨ ¸ §y¨X­±³+¥]¬Q³ ¸ ½L¡H§À³¦·@µ¢¥¦ª¯Q½L¥¦¯QË ¡±£B§l°ÐB»ÎÃVªL³uµ°§ ¸ ¯§ÆÎü³Q¨-É¥¦£µ«§ÏZ³Qª³¦·s­®³¶m¥¦Ë ª ¢¥¦¡H§y§ ¸    ^Ü  Ý=¢¡Ì¥ ¦µ¢¡H³m¬§y¨±·¼¨®§ K9 VªV­Ì·¼³¨®¶1³¦· ­±L§=¨®´ ­±É­®¡ ¥  ¨±¤Ü³QªL¡± * ¸   õ ¨®]§ ÝÏ¸ ¥´ªÁ¸ ­®­® §g¡±Á¡6­®³Q¨±§o¶Ê¶m·¼¨®¥ª³¶GÁÎ¬Q­±¨®´L¥¦´ZµZ¯Q°ªËË ª«ª¯LÆ#=ç ª§Ì£Z³Q¡®¡H«´µ«§À¥££¨®³Q¥»ºm­±³¨®¨®§y»­/­±¢¡4£¨®³´µ«§y¶ ¢¡Z­±³o¬Q°·¼L¥¦­­±L#§ ¡H§y§ ¸ ¡ @­±¨º¥»­®§ ¸ · ¼¨®³¶×­®#§ ´«µ «ª¯½B¥¦µ ¸ ¢»­®«³ªL¥¨±ÁmB¥]¬+§ ³Qªµ«ªL³«ªL¥¦§y¥¦ªL°ªL´Á»º§y»ºÃ«ª¯ ­±L¶ «ª ¸ «¬ ¸ ½L¥¦µ« «ª×¥Äµ« ´¨º¥¦¨®Á ¸ ¢»­®°³QªL¥¦¨®Á¦ÆrÇÂ§ ¸ ¸ ª³#­ ­®¥Ã­®¢¡#¥¦££L¨±³V¥»º@Ï¦­®¨±Á«ª¯-­®³ Ã§y­±L§À¥¦³Q½ªV­#³¦· ¡±½£B§y¨±¬¢¡±«ª«¶m¥ ¦µjÆ ßo e ´B¡H³Qµ°½ ÷RøUl ­±§+ ksjZmNùQjúüûUksjZý ¨º¥u­®=­±B¥¦ªÈ¨±§yµ«¥¦ lhmNn ­±«¬§mÜª³¨®¶m¥¦µ« § ¸ /Ý ·¼¨®§ V½ªË ! (f e «§l¡.¥¨±§4½L¡±§ ¸ ­®³Ì»y¥¦£­®½¨®§/´Z³¦­®+­®§ ¸ ¢¡6­®¨±«´½­®«³ªL¥ K9 µ¦«ª·¼³¨±Ë ³ªÎ¡Àµ¢¥¥µ°«§ ¸"
"Û Q½L§y¡H­±«³ªL¥´µ«§ÞºÆXø8ª­±¢¡ ¶m¥ª¨yÏ³ªL§¶m¥ ¥u­®¡±«³¥ªÎ£¥µ«#§ª ¸¡H«­±]§ §ÀÝº*Æ ¨±êo§yµ°ªL¢¥´¨±«µ°­®¥¦­6Á«ªV=³¦­6·ZÁÀ­±¢¡L.¥¦­sÉ£«ªµ«·¼¢»³°¨®­±¶m¥u¨®­®°£³Qªm¨®§yÜ¼¡±´L¥Q¡H§­®§ ¸¸  ­±§g¬ ¸ ªL´½­Î¥µ«­±§mµ¢¥Q»ºÃ³¦· ¬£¸ ­±½L§yªL¨±§l¡-§ªü³¦³Q­+*¨ ³Qªµ«¥¦Á£µ«§]Ï °· =¥ ªL³½ª 0¥¦££Z«ª =·¼§yË «ª¸ «ªÉ­®³°ªV­±ªÉ­®¥]¬u¥¦¶m«µ¢¥¦¥¡® ´Lµ°§À½Lµ°¯«ªª§4¸ªQ­®«ª·¼³Q¨±¶m¥¦­±«³ª@ÏQ­±§+¯ªË þ ¥¦ª ¸ ÿ ³¦­® 2 /¨ ³ªË ­±§y¨ ¸ «¡H­±¨®«´½­®°³Q³¦· r¢¡s³ ¸ §yµ°§ ¸ ¥ü¨®§ V½§yªLÁ@Ü ÏÏ«îlÝ ­±¨º¥u­±L¨4­±µ«L§y¬¥ªµ4å#³¦·Ì¨±³Q´L³¥¦ªÐ ¸ ªB°ªÔ­±§³´B¡H§y¨±¬Q§ ¸Ä¸ ¢¡H­±¨®°´L½Ë ;;2 =* *?&lt; .7Gw.&quot;&lt; ?&lt; P++) *;*(574;*ox/@=* * ;89+% : * +* A% ?&lt; K &apos;;A4) ;*D2H+) Q. @JNz2;*=%A2;8B&apos;;89q7{* &apos;;.o,`. ) ) + *v2;89|=-* .1M`&apos;;)+* =&lt; .+PA2}57AN% &apos;;)A*X2;=* *DNO@~CNAN&quot;8B&apos;;89.7%A57@+xA@=* %A: +7. +% *W0KON, &quot;8J2;&lt; 89%+: =&lt; .7%I&apos;;=* rI&apos;32OtC8B&apos;;) )"
A8B:Q4) ;* @ ;89qQ*y=&lt; .1G&quot;. =&lt; &lt;?PA4;*=%A=&lt; *tC8B&apos;;) t743N. +2C.7P+;89N+*X&apos;;+) *
"A% .7P+%@&gt;AA5Q2H, *DNy.7%y&apos;;)A* * ;* ?."
"A% ?&lt; **=98J51@ A, B@ * &lt;?Q. I&apos;% ;*?r0&apos;3289%&apos;;*=7MO. : &lt;=51@ &lt; *=4351:"
"Qv* . ;)+8J2C5 @J51A, B@ C*"
"A% 7P. +y% @ 7: * %AN&quot;* ON* &quot;*?*=%A* &gt; )+LN* &quot;8J2H&apos;;,AP&quot;&apos;;897. %/Q4. (51@9k&apos;@ ;)A* t.743N+28B%&apos;;A) *W%A7P. +% @ *&quot;. &quot;N 8[REF_CITE]* &lt; ?&lt; .&quot;89%+(: ;&apos; .e&apos;;)A* % +Ee,/*.1M&quot;4;* @98957,+9@ *n?&lt; .Q%I&apos;;?* r0&apos;32RtC8B&apos;;XtC) ) +8J&lt;3))+=* KX?&lt; 7Gw. .&quot;&lt; &lt;?+P 0P -572;* .7%5Q2;;2 .&quot;?&lt; .Q% &lt; )&apos;;+) *"
"D2HN* +*=* ?&lt; Q. %I&apos;;?* r0&apos;32?/&gt; 5QN+N&quot;8B&apos;;89Q. %A51@ +% .Q&quot;% G2;*=*DN%+7P. +%/2CtC8B&apos;;y) 7: * AN% &quot;=*  ;8B:Q%+*=%I&apos;32o51`, .* )+;4 D2H* A) .7JN@ O57N* +N&quot;DN&apos;* ;.&apos;;)+*L2;=* DN*2 @% )+* /, 0. .++6 89+% :v57@97: . ;+) E8B&apos;;*=*+%I&apos;;89@QA% . ."
"X2HPA* &lt;3) %+QP. +%A2-. =&lt; .7I&apos;% ;=* * .7P+%ANRY &apos; *;m-)+L* .+% *D&lt;?*D2/;*7n&gt; ,0Ks?&lt; .7PA;%+%=&lt; 2o:L&lt;=*[REF_CITE]% &lt;3) ,/*?&lt; Q. *VB@ @9.D* &lt;p&apos;;=&lt;DN* 7. %I&apos;/;, =* 7&apos;. rI&apos;XMS;)."
Q4% ;* &lt;3V) + % 7P. +%VQ. %+@ 7. %A&lt;=*C4;* :&quot;9@ *D2;)+. 0Ko&apos;% ;*D2F&apos;;A51&apos;) ?
Q. I&apos;% ;*?r0&apos;X51% )+*.Q4. ;EOz?&lt; 1G. .&quot;&lt;=&lt;=+P 4v89y&apos;% ;
"A) *e&lt;=.+PA2=&gt;&lt; 51/% ;Q7. * %A2?&quot;&gt; ,&lt;?.QP+%I&apos;;89+% :(D57* &lt;3)L&lt;=.7I&apos;% ;=* r0&apos;-5Q2{[REF_CITE]% D2* ++6+6/*Q5Q. 3&lt; ) )"
"A5Q2-&apos;%+) e*;)A=&lt;(* .57N+&quot;Q&apos;351% ;Q: )5LW* .7MF:789q7,`=* *89%+% : %+9@.7P*D2+;%2C2;=* A2% )A;8BG* &apos;t=* 897: I&apos;) ;89%+:51%0Ks?&lt; Q. %*?r0&apos;Ws* ;6/D* &lt;?8J51@9@9KtC)A=* % 2H* *+2X2;PA&lt;3) ;89qQ*C&apos;;.(;&apos; +) *[REF_CITE];;&apos; 8J=&lt; +P @ * ."
"J57@ %+:7[REF_CITE]: 7* &gt;7,0KeA% . .*=4;G 572nV89 ;2 *=* *DeC%&lt; .Q.[REF_CITE]% &quot;YB%+8J573Y}%(})A5&quot;r /6 qQ* EeP*;+* B&apos;@Q&apos;328B% ;896+@92HK0*C% %7[REF_CITE]. &lt;p&apos;%;+8[REF_CITE]&lt;O2% +A;% +)&lt;?.&apos;;897. /% * 2 &apos; )/;*D2HP+@7. ,+%+DN* 0K, ?&lt; 7. /% 2H8JN=&lt; &quot;. * ;*89%+: 7: *De574/*;2* ;@989:7)I&apos;;9K&lt;@; Q Yu &quot; 57&lt; &lt;?P+4357=&lt; K¡51&apos;L¢7¢AY &quot;£  &amp;=«H¨=+=&amp;{&amp;=?=&amp;&amp;ª ²µ´IÃ7¾0¦3Ä=+=°H·7½SÁ9¯&amp;²¸·Q¦;=&amp;{¯DÂo¦-¯/=Æ"
"Á `*?=* 4&apos;;A57) %Ç&apos;;+) I2H. È* .Q&quot;, +% *;89%+:&apos;;7Q. =* %&lt;, 89 %+Q:"
YÌË &quot; ) 57%A51&apos;=&lt; &lt;=;P+P+43574351?&lt; -@K51&apos;V¢0Í0Yu:7* %AN&quot;* ;Î=* DN* +?2&lt;?&gt;-51.*@B&apos;;A) :7.7P*+Q:otC) +;)&apos; )+* N*% &quot;8BÏ *=;4;GG *=%//?&lt; *(* 8J2CA% .&lt; MS;74.8J2H&apos;};578J&lt;=@B57@Q.B@ @)+=*;8B4F:
"Q@J51%+8Bx+%/[REF_CITE]:&lt;=57I&apos;% :Q=Y-Ð* `, e** &lt;3&lt; )+Q2.;{;* * .7M+.OPA2&apos;;)+*;* ?&quot;r 6`* ?&lt; ;&apos; * 7. +, P/%+*;. %+.[REF_CITE];*W8BL&apos;% ;+) * +Y* &apos;;89Q. Ð%RÑn89* 9@ ? * %IqQ&gt;14* ;89;7: 89):Q51&apos;&gt;;*%ANz+, 89@951&apos;.&quot;;N&quot;* *=J2o@ @7.?&lt; Q. % &lt;?Q.?* %r0&apos;k*?,[REF_CITE]0&apos;;;P/*51(@8B7. +%(% MStC. A) 7. 9@ * t.743N+251ANt% .743N2;P&quot;ÒVr&quot;*=YÔÓ+Q4sC. .7[REF_CITE]%+8J51R% &gt;*N&quot;8J2HG &lt; . *=*;)A51&apos;O7. %+9K@ ;&apos; )A*B@ *=MJ&apos;OtC+) .7@9*=Gwt.?&lt; .Q%I&apos;;*?? *=98J51@ ,+9@ V89* AN% &quot;8J=&lt; 51&apos;;74XMS."
Q4V. 7: =* %/&quot;N =* &gt;k&apos;;)+4* ;89Q: Q&apos;e) &lt;?7. % ?* r0&apos;e57%AN ;&apos; )A*(?&lt; .Q% ?* r0&apos;;P/51@n2;P&quot;ÒVr89%&quot;MS.Q4;[REF_CITE]&apos;;897. % ?&lt; 7.
I&apos;35189% %+89+% z5O: @B7&apos;C. 1M. % ) +8J&lt;=57@9@&lt;?.7%/&quot;;&apos; * ;89Q: )
"Q&apos;k?&lt; Q. %*?;PA&lt;3)(;Ù +.[REF_CITE];*1YÈÓA.*?+r +@91* &gt;;. q7* * ;K *+* Q&apos;L5QN% * ?&lt; &apos;;89q7*D2 N 8J2H&apos;;89%+7[REF_CITE]: ;) `, ?* *=*=%&lt;?[REF_CITE]@ %+51* %ANMS*=%+89+% 8B* %¡&apos;;A) *&quot;; =Ú Û¸Ü 3&gt; &amp; Þ +3&gt; Û + ÝÙ uà × ?á ÝX."
"Û ã?ä 3&gt;*?&apos;3&lt;&quot;.%+.7&apos; 89%AN&quot;=* xA%+8B&apos;;*Q4. ;[REF_CITE]/% N ;&apos; +) *=&lt; 51% 0. &lt; ?&lt; +P 4z,0K&lt;3)A57A% &lt;?tC8B&apos;* ;) 7%ABKz@ 7. +% * / * .+% .7PA%L2H * *+2  *D2H6`* &lt;=89579@ @BKotC)A= * %L&lt;=7."
"A2% ;8JN0G. *=+%+8J:52[REF_CITE]=&lt; 57@B9K@ (MS7. 9@ @99@ @.?&lt; . +)+*[REF_CITE]3Y+% 7P. +%/~ @J2;+.&quot;&gt;kCe. +% 7[REF_CITE].. 3&lt; )A57%+8J51%+%qQ:7v&apos;* *;=+)A2,* {MS;&apos;.Q4)A51&apos;;E 57&lt; ?&lt; . &quot;89+% v&apos;: ;.X7: * %AN&quot;*%. *=%A2;* =&gt;Q2;W&apos;. ;)+*;* +*;=* I&apos;% Q2H&apos;. ;9KV@ +% .[REF_CITE];*1Y 0D* &lt;p&apos;v2X2H~ ;89.Qæ£Qp% )A.&gt;%8B%)+* =&lt; +.7, I&apos;%@9(*;=* £ 51,`%I. .@J2H;43516L8B. +% 6A8BA%)+[REF_CITE]B:*,A* JN@B@ +*2q7.7E* ;K ) 897: ) 6+* &lt;=892;897."
"R% X&gt; +, &quot;P &apos;&apos;;)+* &lt;=. *7: *=* J51&apos;@ ;89q7* @BK@9.+ tC8B&apos;m-&lt; )+;*)O;=*A8B) MS. :Q)O61* &gt;+7. /%* =&lt;&lt;?892s*;89.7:zMS% .74C5(;2;?*8J&apos;-&lt;.@:7%A=* 7P./% +&quot;N %A2*==4892N&gt;+&quot;*Q4?. ;=*+6 A) .7@9.%+7Q8uG:*=57@/&quot;."
"N&quot;* +@ .z6+*+89&lt;?{&apos; 7: * AN% &quot;* .;)A* +% QP. +%A2{;&apos; A51&apos;) =&lt; .7P+JN@ +% . /, * &quot;+, 89:7[REF_CITE]&apos;;*DN(.7ET?&lt; .7%?* ;* Y Z [Bç &amp;$ %7: * %+èæ*=4351^Ag@w&gt;&quot;t(* `ê^ ^`lX8B*/jìë7h%+=** ;Fh_ * +%y6+f * &quot;8J&lt;+% (: )+* +76. +G *=4;&apos;;89*D2}.+% QP. +A2% {&apos;;)A51&apos;576+6/[REF_CITE]89*"
"L5% (?&lt; .+PA2=}Y ïs510Ke% .1MR&apos;;)A* % .QP+[REF_CITE]% (% +) C* @ &quot;o. +% 1&apos;576. +/6[REF_CITE]* (% ) +* &lt;?Q. %A2;89N+=* * =&lt; 74. ;G+ +6 PA2}51&apos;{519@ @&quot;51%[REF_CITE]Ko% 7MA&apos;. ;A) *C%+.7P+/% 2F&apos;;A51&apos;) }576+6`* ;*+% 7&apos;. 89%*=9@ * 57%I&apos;X&lt;=.7I&apos;% ;*=rI&apos;32?YLm-)A*=?* MS.1* &gt;F51516% +6A4;.[REF_CITE]&lt; )/, 572;* . % . +)+.Q@B.Q7K:  89%=* ..=* %/&quot;N 89+% Q23: )/;V. ,`*7 ?Q. A2% ;89N+*=*. +) *;* %A7P. +A2% =[REF_CITE]A, @B*-v2;)+.;+) * :7* AN% &quot;*&lt; &quot;8J2H&apos;;AP, &quot;;&apos; 897. %y.*=(4 *+* %Q&apos;e?G9@ ?* =* 4W2HP&quot;ÒVr&quot;*xAqQ% *N J51%+[REF_CITE]: :Q* =&gt;[REF_CITE]v.7&quot;, &apos;35189+% *.7ET&apos;;+) z * %+7PA. % @ @J51,AB@ *@ MS.)+*;*oJ51@ %+[REF_CITE]: :Q*=Y Z [Bçnìñ[ òoh&quot;&quot; b0_FlAaDé =_ aDbQg i{^`ê h+ `^ _ ="
"N&quot;89%+: *V?&lt; . *=@ *;) :7* %AN&quot;*=4X89ù% ;5y2HK&quot;2H&apos;;* ;8J&lt;* %/mnPA3Q&lt; *=4-?* &apos;v57wY@    71&quot;&gt; 6+&amp;ËQQF7. ,A2;=* DNe&apos;* ;)%+QP. +% 51/% (N +6 * +89&lt;?,+9@ *{[REF_CITE]A% %+=* 4Hú{89%zÓ+=* %/3&lt; )RYkm-)A*=KXMS7PA."
ANz&apos;% ;)A51&apos; áIKaoGKQ&lt; &gt;]&lt;  HF .&lt; X*O &lt;.AC&gt; %O @AI&apos;\&lt;.P Q[ &gt;AC%O M a &lt; M a M   a M a &apos;&lt; X &lt; hG  Ï     Ï Ó Ò    Ï  ÑÏKÏ ÏKÏ   Ò          [h Ï  Ï  Ï    Ï Ï  P Ó  Ï   Ï  Ï  Ï     ÑK Ò   &gt; Ï Ó Ï Ó Ñ ÑK
Ï Ó     
Ï Ñ Ï Ó  &lt; Ñ    Ï Ó Ï Ò Ï ÑKÑ  Ï   I     Ò Ï     Ï    F Ï    Ð    
Ï  Ï ÒK  Ï hC Ñ Ñ&apos;Ó Ó  Ð Ï Ñ
Ó  Ò ÓÏ   D Ñ Ï  Ó Ï     Ó  Ñ ­±L³QªV­±¨®³°´L¨®£½½L­±«¡ºÝº³§ V½L¥µX­±³­®§mª½¶d´B§y¨c³¦³¨±¨® §y¡-«ª K9 ßoµ¢¡±³³£L¨±³Q«¡±§ ¡±³µ«½­±«³Qª¾/³Q½µ ¸ ´Z§ ­±³¾4«¯V­m­®§Ä³ªVÉ­±½L¥µÌµ¢¥¡®¡ ¸ ¢¡H­±¨®°´L½­±«³ª ª³´Á ­±L§Ìª½¶d´Z³¦³»y¨®¨®ªL§y/¡ °­±L§-³¨®£½L´½­o­±L§ ª½¶d´B§y¨À³¦·/¨±§yµ°§y¬u¥¦ªV=­ ³QªV­±É­º¡À°­±¢»º°­=L¥Q¡=§yª ¡¢»ªº
"Ü¢·¼³QJ=¨ ­±É¥§l¡+£«ª^µ«§¦Ï@¶m¥m¥Q¾4³½¨ µ«¸ ª­±§mB¥u³Q­cªQ³­®­®¡d¨®¡ ¢¡d¯­®°§l·¼¨®³¶ ÿ «¬§yªÔ³¨®§ 4«¯ Ý2 ­±B¥¦ª×¥Ä¾4³¨ ¸ ³½L¨±¨®«ª¯ # Ô­± /  ¢»º¾ ­6ç=¢» §ÌªL§¶m­®¥Q§q­®¨±½µ««§q°ªL¢§À´L½­®«µ­yÉ­®­±¡ºÝºÆ£¨®³´L¥´«µ° ­6ÁÂ³¦+·  ¸ §y¨-·¼³¨ ¥¯«¬QªJ¾4³¨ ¸ ö³£Z³Q¡±§ ¸ ³¦·/µ«­H­±§y¨®¡ ¢¡ ³Q£½­®§ ¸ ¥¦µ«³Qª­± þ §m£B¥u­±O³¨®¨®§y¡±£B³Qª ¸ «ª  ­±³ ö !Bu! ! ´Á  ½L¡±«ª¯ ¥c¨®§y½L¨®¡±«¬¡H³³­±«ª¯=·¼³¨®¶Î½Lµ«¥Ì¢»ºªL¥¦ þ ­±½¨º¥¦µ« Á ­®¥Ã§l¡=¥ ¸ ¬u¥¦ªV­®¥¯§ ³#· ­±§ ¸ ¢¡H­±¨®°´L½­±«³ªB¥¦µ@£¨®§y¡±ªV­®¥¦­±«³ª ³¦· ½LªL§y¨H­º¥¦«ªV­6Á¦Æ÷§Âª ¸ °­±«³ªB¥¦µoµ¢¥Q£L¨±³Q´L¥¦´«µ °­±«§y¡ ¯ª É   !uB! ! gÜ¼jÆ §Æ(­®§q£L¥¦­± «ªO­®§­®¨±«§¢¡  ¥¦¨®§=§l¡6­®°¶m¥¦­±§ ¸ ¥¿    ! &quot;  # &quot; $ &apos;% )&amp; ( * !B!uB! ! u! &quot; +/ 3 1 )7 8:8:8 1 ; &lt; !   ` @?,  +.# / &quot; $ %&apos;&amp;))( 798*:8:8 14*; &lt; =$ 0!Bu! ! % &amp; .&gt;&apos;   6Ü î]Ý"
DHP+FHTIK]@ G&apos;DHPWÛM[ :P+ac&gt;J&lt;Q&gt;A&lt;.  
"P &apos;aoGKC%J@ A&gt; QPd&lt; [ &gt;ACED%FH&gt;JQBQDH&gt; QC&lt; _&gt;A&lt;DHOQP?@APWG&apos;M .&lt; [  IKF_.P_Õ1IKF*[%:z|{ ED  GF Z{ , Kz D GF { GH z )íWê"
I GF z Z{ Z&lt; Ö[ }Eí  Ez: 6F ë Z{&lt; DHP+^% %PW[Q[ &gt;ACED%FH&gt;A.&gt; &lt;._GW\ &gt;Jy@ GKQ@.&gt;AAPbM@ &amp;} GF {?GK&lt;. [} KF  LA { Z&lt;.GK@s[Q&gt;ACED%FHA&gt; .BQDH&gt;AIK.&lt;  %PW[IK.&lt; yIK@ &lt;acIKFHTQO.IK@&gt; ãD@ %FHAPäC&gt; %acI IKDHOQ&gt;J&lt;QSQsó1O.P~[ &gt;JCED%FHy&gt; .&gt; .&lt; C=A&gt; &lt;+*D &lt;.@^%O.IWÕ &gt;J&lt;QyDH&gt; &gt;]@ &gt;AIK?&lt;  &lt; ^% .GKC%PW[ .GKQ&gt;J@AyDH&gt; &gt;&amp;
DHOQPAPWGW@ \ ³¦­®  ¨4  ¡H½Íd¸ ¢ÉH¡ Ëæ±­ ®¨ °ª´L¯½ ­±­±«³ ªd¬u³ §yµj¬§y¨#ÇÂ¯y§=§ ª »º¸ y§ ³V¨®¡H/¡ ¢¡­®s³+¡H­±¡H³­±¨®§ ¸ ¥u³Q­s¨4µ«¥¦4­ ¯¦¥­±VªÁ
Ë î#µ«¬Q¢¡B¥¡@­®§/¥ ¸ ¬u¥¦ªV­®¥¯*§ ­®L¥u³ªµ«ÁÌ°ª·¼³¨®¶m¥u­±«³Qª ·¼¨®³¶¹/¾ ³Q¨ ¸ ¡X­±L¥¦­sL¥]¬§o+¥ ¡H½³¦·¥u/­ µ°§l¥¡H­#³ª§À­H­®¨#«ª ³Q³ªÌ°­±-­±L/§ ½L¨±ªV¾4³¨ ¸
Ü¢·¼³¢»ºc­±L/§ µ¢¥Q¡±¡HË ³Qª ¸ °­±«³QªL¥¦µ ¸ ¢¡6­®¨±«´½­±«³ªÔ¢³Q£½­®§ ¸ Ýd¢¡d½L¡±§ ¸ Æwá4Ë ¥½L¡±§o³¦·@­®L¥u­yÏV¨º¥¦¨®ª³½LªL¡#­®L¥u­4 ¸ «µ«­±§y¨®¡sª³¦­/¡±ª «ªÎ¥ªVÁ-³·L­±L§4ªL³½ªL¡.L«»º ­±§y°¨*¯Qª ¸ §y»y¥¦ªd´B§ ¸ ¢¡HË ¥¦¶d´«¯½B¥u­±§ ¸ ­®¨®³½¯QJªQÉ­=´Z³³¦­º¡6­®¨®¥££«ª¯Ï ¥¦¨®§ ª³­ ¨®§ ¸ ´
VÁ+­±­®¨±««ªV­±¨®£Z³µ¢¥u­®°³Qª ¥¦µ«¯³Q¨±°­®¶¤¥¦ª ¸ » ³¦­®­±³ ¸ L¥¡4­® ´B§c§y£µ«³uÁ§ ¸
Ü¡±§ óÆ A ÝÆ ·¼³¨®^ß ¶d½£Lµ¢¥m¥¨®¥î-¢¡/­®§y¨±£¸µ«³u·¼Á³§ ¨®¸¶¤¿ ³¦·­®§/ªB¥u­±½L¨®¥µ­®¨±«ª¯
V # = 0 $ % &amp; (  !u!B!   
N  M P+ /?   #PQOR= 0 $ %+&amp; (/ 3 1 )7 u! B! ! 8 8:81 ; $ % &amp; .&gt;: &lt; &quot;   ÜzóQÝ
S + / 3 1 )7 8:8:8 1 ; &lt;   §y¨±§ NTVU3 ] [ E\ ]^ a`_ 3 ` 365)7.bXb b ` d;Y c 02eKf ºÏ Kgi/ S h=&lt; U Æ¹ü   ¸
XW ³Q¨ § É¦¥ ¶ £«µ ¦§ M Ï  ´ + Á0 / º» PQO  R ³³Q¡HË  3 []E\ ]^ a`_ 3 ` Ï 365  ) 4¾ 7 =§ b b b §y ` ; ª c 20 ¼· e. ³ f ¨º /. » ]g §= =h &lt; ­±L ¥¦­ ÏL¥¦4­ §l¥ º» g ªL³ ¸ §=«µ § ¬Q§
Zµ ³ ª ¥ ««ªª ¯ ±­  § £ » Q³ ¶ £ ½ ­º¥u¸ ­±«ª¯dª³«³Qª ³X· ¸ ±­  § » ½L¨±¨®§
Vª À­ ª ³ ¸ Î§ ¢¡o´Z³ ®¨ ±¨ u³ ¾4§ ¸  TQFHPW[ AX&gt; +DHA&gt; .&lt; SoDHO.@A&gt; aq y@ ?
JGKC@ %Coà ðd ò7 åuGK.&lt; DHOQP [ )@JGKC%C &gt;+Z&lt; Q[ A&gt; &lt;~à ðd s
R åsJ&gt; bDHO&lt; .P@.&lt; .GKSKP
ÇÂÉ  £Z j  /­ * ­®  L¥u
ÖjZm /­ kx ­± û §y &apos;zu ¨±=§  { dm | °µ« @ L¡H } UmNû ­±«µ  ~ m É k ¢¡H ! /­ ¡ ±³   û     ª³Q½ªL¡s­±L¥¦­ !fe  ¥¦¨®§g¢»º¨º ¥¦¯¨º¥ ¦¥ª¸ ¶m½¥¦­±ª½L¡H¥½Bµs¥¦¯µc§yÜ¼ª «¸ªÔ§y­®¨g¨®¥¶mªª+¡ ³¦³­m·À´Z§J³¨®££L¨±§ ³Q¸ µ«¢»³¯­±§ÁÝ¸=·¼³Q´Á ¨ ½L¡±«ª¯´B³­±È­±³ªVÉ­±½L¥µ#¥¦ª ¸ ³Q¨±£L³µ«³¯Q«»y¥ ¦µµ°½L§yÆ 8ªc­®/§ ³Q¶m¥¦ª¢¥ªÀ­®§y¡±]­®¨®§s¾4§X·¼³½L¨.¡±½L»ºcª³½LªLø ­±L§À¡H«ª¯Qµ«§/­±­±§y¨®¡  -Ï  Ï 
V¥¦ª ¸ ­±¾4³¨ ¸   VÏ°­± 2Q Ï ç ª¡H³Qµ«½­±«³ª¢=¡ ­±£¨®§ ¸ =­ ·¼³Q¨=­®§y¡±§dª³½ªB¡À­®³Q¡H­=î Ï ÏL¥ª ¸ îî-³½L¨±ªL¡4«ªg­®§ ¨±£L½L¨±§l¡H£Z­±«¬§yµ«Á¦Æ  ã ³Q³ª¯§yª ¸ §y¨Ì«ª­®§dµ«¥ª¯½B¥¦¯§Æ-L«¡=¢¡ÀªL³¦­-ª§y¡HË ­±L¨®°µ«Á³Q£­±«¶m¥¦µj´Z§y»y¥¦½L¡±­®§g¾4³¨ ¸ ¡ ³¨®¨±§l¡H£Z³ª ¸ °ªL¯g­®³ ¡®¥¦§q³Q¡H­g³Q^ª ¯Qª ¸ ¨m«¯L¥]¬Q¬QÁ^¨®½µ¢¥¦¨ ¶m¥³]Ág¨®£´Z§³Qµ«­®³¯³gÁ¥Æà¡®¡±°¯QßGªg´B·¼³Q¨o­®­®¨§+¡H³Q½µ«½ªB­±«³ª³Q¢»º¾40³¨ ¸¾4¡À§J§y£¨® µ«³u¶mÁ¦¡ Ï ³¦·-³¨®£³Qµ«³¯ÁÈ­®§q¯§yª ¸ §y­® ¯¨® §y¥u¨m¡±ÉO¬u¥¦¨®°Ë ¥¦´L°µ««ª§=¢¥ªBÝºÏ¥¡4 )
LÆ / ÇÂ³Q ÷Rø  Xw £   ½ @w* ­± x   û©m ³u¬ úüj §y ) ¨®  ¥ sûUk   ¥¦ª ¸ ½¨®¥QÁq³­±¡HÁ¡HË e G ! f ­±§y¶©¥Ü µ«ª³Q³ªL¡H½LªL¡o¸ §y¥¦¨±¨®§c«ª³QªL­6¾4¡H³À¸ §l§y¡6¨±­®§«¶m¸ ¥u­±§ V«½L³Q¥¦ª=µ««­±£B³ ¸³Q¨H­º¿­6¥¦ªVÁ£B¥Ëjª ´B¥¸ ¡±§ ¸­ ¾4¥Î/¾ «¯§y°¯QV­±§V/­¸ ´ÁÂ­±³ª§]Ý4§¥¦ªª½¸ ¶d­±´B³ÃQ§y¨Î³·=³½¨®¨±§y§l¡+³·o­±­º¥¦¨±Ë  Ëæ´L¥Q¡H§ ¸ Ü¼§l¥»º£¨±§ ¸ ¢»­®°¢¡ ª³Q½ª×«ª ­±L§È¨®£½L¡ºÝºÆõüL³¨g³V¡6µ«¥ª¯½B¥¦¯§l¾4§ L¥ ¸ ¥u­-µ«§y¥Q¡6=­ ¶m¥¦«ªËæå4ç-è¥ªª³¦­º¥u­®§ ¸ ³¨®£½LÏ@­±L¥¦­ ¾4+§³¨ ¸ ¡ ½µ ¸ §y^³ª ªL¡± ¸ £½­®³Qª°µ«ÁmªL¯­®§­±+ª­±³Q³Q«ªLªO¥¦µ@³u«ªL¬§y¡H¨®­®¥ªL¥¯§l¥¦ª³¦·X¸ ­±L¥§Ë ¾4¥ªd¨®¥Q¥£Á¦Æ£*¨®Ç×³]É°¶m§yª¥¦¡H­±½L»º§*­®«ª­±·¼³Q³¨®Ã¶mªd¥u­®³u«³¬Q¥¦¢¡¯/§ ³¦¥¦­oª ¥]¸ ¬u¥»y¥ ¦«µ¢¥¦´L½¨ºµ°§¥Ï³ªL´Á§ ª³Q½ªªL¡±^¸ µ«¢¡6­Î¨®°ªL+¯¥¦µ«@«ªL¡H­®¥¦ªB§y¡4#³·.­®§c¸ /µ«§y¡®¡Ì­±§y¾ ³Q¨ ¸ ·¼³¨®¶m«¨+¡/·¼¨±³Q¶ö­±L§  \ &gt;J&gt; ¿  *D &gt;ACEDHa  &gt; &lt; :IKF Ï ? F*GKÐKK.&lt; [QIKacAX@ *.ÏO
IKC%Ûá&lt; =Ó  &lt;.JGK&gt; &lt; &lt;.IKB.&lt; *GK&gt;A.&lt; PWoB[ .C%&gt;A&lt;QS  uI @&lt;.aRP+FNIKM!OQ&gt;ADHCNM:IKFDHOQP *[yDHC&gt; %yM7GK@ &lt;.[bDHO.PÕ1IKF*[_QPW[ [ &amp;A&gt; .&lt; &gt;A&lt;.~P &lt;ZbaoGKCE[ @J&gt;A&lt; &lt; DHP+^ (&gt;AZ&lt; [ 9A ZQ&lt; &gt;%DH&gt; @  ¢¡¡H­±«¶m¥¦­±«³¶m]¥ £¨®³u¬ ¸ §À«ªL¥Q½L¨®¥¦­±§À¨®§y¡±½µ°­®#ü³Q¨ É¥£µ«§¦°·c­±¨®½§Jå4ç-è0¾/§y¨±°¯Qª³¨®§ ¸ §y(ª ³Q£½­±Ë «ª=¯ ­±³ªd¥½¨º¥«ªd³¶m¥ª¢¥¦@ª ]Ï ¥Q½¨®¥QÁc¾4³½µ ¸ ¥½L ÿ ¡±  /³¬Q¨®
Áq·¼¨®§ K9  ¾4 ) ³  ¨ ¸
Ï ¬Q¨º¥¦¯Q§¦Ï ¶m§y¥¥¦ª««ªªµ«¯Á ´
"Zï Æ m°ªB¡6­®§y¥ ¸ ïðÆó g¥uîyôQô . Ý.¥¦ª ¸ õ ( ¸ lÝºÏ¦ª§y¬§y¨ ½L¡±§ ¸     #§l¥¦ª«ª«ªL¥¦µ¯    ,  ¼Ï *ÏQ¥ª .,    !e f  §m¶m @ ¥   « x £  Nj ¨®³ ) ´Lµ° _ §y¶ û©m @   ksj ³ ) ½LªQ * ­® Klhûãm x ¨®§   ¸ ¢¡-­±L§m«ª°­±¢¥¦µX³u¨±Ë ¥¦¯Q+§ ¯Q°¬Qª´Á­±L§dÉV­-´Z³V³­®¡H­±¨º¥¦££L°ªL¯m£L¥Q¡H§ÆcL§ »¬Q¥¦¯Q§»y¥¦ª×´Z§È°ªB¨®§y¥¡±§ ¸ ´Á µ«³u¾/§y¨±«ª¯È­±L§J­®¨±§l¡HË ³µ ¸ /¡ ·¼³Q¨ÀªQ­®¨±§yµ« ¥´«µ ­6ÁÏ¦´½­o­®L¥¡4­®§cª§y¯Q¥u­®«¬§ â §y­o³¦·*µ«³u¾/§y¨±«ª¯ ¥Q½L¨®¥QÁ¦ÆX§c£¨®³´Lµ°§y¶þ³½L£¨®°Ë ¶m=§ ¥¦¨®¨º«¥¦µ Áªg´B­±§l§-¥½L·¼³Q¡Hµ°=§«³u³¦@· «ª­®¯-§Ì¡±É«£Z§o¨®³«·­±LªV­§Ì/½L¡±«¨®£Bª¯³Q¨®Ó ¥-½L³³Q¡±¯§ µ«¸ ÆXËæ´L¥¡±½L§ ¸¡"
ÇÂ¾4 ´@Æ ¡±§y¥¦¨º»º§l¡s³·­±§cµ¢¥¨±¯Q§y¡H/­ ³Q¨±£½B¡/£L½´µ«¢» µ«Ád¥]¬u¥°µ¢¥´µ«§¦¿@­±L§ 3 §yµ°«§y¨s­À¥Üjó¦ôQôQóu/Ý ¡H³u¾ ­®L¥u­4­±L§=ÇÂ¥ªg´Z§ ­®¥Q»½L¡±§ ­®¸ «»y¥¡Ì¥¦µ «ª·¼¨±§y³¨®¶mµ°¢¥´¥u­®µ«§Ì«³¯Q½Lª§y¡±«ª¨®¥¯d¡±¡±³§y¥¨®½L»º§Î³·sª¯Qµ««ªÉÆ ¥¦ª ¸ ¡±ÁVªË ­6¾4³JÇ×µ«¢¥¦½L´µ«¡±§d«ª=¯µ°·¢ªB¥u­±½LªV¨®¥µVÉ¯Qª
Ï ¸ ¨X¡H¥¦§y§ª ¸¸ Ï¦]­®
Ï/§o¡HÁ¢»º¡H­±§y¶¤ÐLªÔ¥¦¨®§m­±L¸ §¡ «ª ¸ ª°­±§Â¥¦¨±­±¢µ«§l¡m«ªw³Q¶m  ¥¦ª¢¥ª@ÆöÇ×§yªw½L¡±«ª¯ ¡
"HÁ¡HË ­®Ï¨®¥Q»­®§ ¸Ï ¡H§y§ ¸ /·¼³¡±¨ÌÉm¶m¥·¢­À½³µ«ªVª­ ®§ÏL­®³Ï¥¥¦¨±ª §Ì¸ ·¼³½Lª ¸ g¿ LÏ  ·¼³Q¨ ·¼ ,   «ª«ª§ =   ÇÈ§Î  ¨®¥ , ª ¸  ³Q¡H§yµ°§l»­®§ ¸ ¦ômª³½ªB¡o , ·¼¨®³¶Ê  ³Ë ¶m¥¦ªL«¥ªJ«£Z³Q¡±°ªL¯g³ªLµ°Áq­±§m¨®§y¡H­±¨®¢»  ­®°³Qªq­®L¥u­c­®¥¦¨®§ «³¦¥ ª¯d¨®§y¡±­±½L¥µ°­®ª/¡ ·¼¨±³Q¶ ¸ ³½LªV³¦­±µ¢¥¦ª¯Q½L¥¦¯Q§y¡s­±L¥¦­ µ  o»ºB¥¦¨º¥»­®¡sÜ¼­±³Ì¨±§ ¸ ½L§s­±»ºL¥ªL/§ ³¦·L¯Q­±Ë ­±«ªµ¢¡±L¥]¬§-­®³Q¡±§+µ«É¢»y¥¦µ@°­±§y¶m¡®¥ª ¸ ·¼³¨®§ ¸ V½§y¨±«§y¡o³ ¦· ­6Á£BÛ6¶m¥Q½µ«°ªL³ªVÉ­Lª³Q½ªÞ#¥¦ª ¸ Û6«ª«ª 9 ³ªV­±­ ª ³Q½ªÞd½B¡H«ª­±L¨±§yµ°¢¥´µ«§+³QªQ­®É­®¡Ì·¼³Q½ª ¸ ´Á­±L§m¡HÁ¡HË ­±§y¶=Æ á4Á£Z³¦­±L§y¡±°Øy«ª¯d­±§d¯§yª ¸ ¨=¥¡À­±§Î³ª+§ ³¨®Ë ¡±£B³Qª ¸ «ª­±­±L§ V½°­±È³¨®§ÎL¾/³Q´­®¥°ªL§ ¸ îyôQô ù¥Q¨®Ág ? ¥¦­À 9 ï ù¬Q¯Ü¢­®+§ ³u¬¨º¥¦¯Q§À£¨®³´Ë ¶ö ) ¢¡o»y¥¦½L¡±§ ¸ ´ Ám­®§ * /  ¨º¥¦¨®­6³·X¡±§Ì¾4³¨ ¸ ¥¾4µ«¥Q¡µ ­±L§/·¥Q»­*­±B¥u­X³Q¡H­.ø8ªV­±§y¨±ªL­±É­º¡*°ªd³¶m¥ª¢¥¦ª ¸ ³Ìª³­ ½ ¡±§ ¸ ¢¥¨±°­®«»y¡s«ª³¨ ¸ §y­±³m´Z§c·¼³QªQ­=°ª ¸ £Z ¸ ­ºÝ§L ·¼¨®§ K9 V½ªB«§y¡/·¼³¨c¡H½L»ºª³Q½¥¦¨®§c¡HL³uªg«¥¦´Lµ°§ o Æ ­±L§¯¨®³½ª ¸ Ëz­®¨±½­®^ ¨®§y¡±³½§l¡Î]¥ ¬u¥°µ¢¥´µ«­±³Ä½L¥ª ¸ ´Z ¥½L¡±§s­®Ác¨®¨®§y¡±ªV­.­±§ ¸ °âZ§y¨±§yªVµ¢¥¦ªL¯½L¥¯*§ ·¥¦«µ «§y¡ Üÿo³¶m¥ ¦ªBÏXèµ¢¥]¬«»¦¥¦ª ¸"
Ó §y¨±¶m¥ª¢»yÝºÚ³Q¡H­ µ¢¥¦ª¯Q½L¥¦¯Q§y¡ L]¥ ¬Q#§ »ºL¥¦¨º¥¨®¢¡6­®¢»¯§yª ¸ §y¡HÁ¡H­±§y¶mÏu¬u¥¨±«³½B¡@¥ ¸ Å6½L¡H­HË ªV­º¡o³¦*· ­®+§ ¯Qª§y¨®¥µ.¡±Á¡H­±§y¶ ¸ §y¨®°´Z§ ¸ «ª­®¢¡o£L¥£B§y¨ ¡±½L«»º¯^V­#¥ ´Z¸ §=Å6½Lª¡6­®§l¡±ªV¡®­®¥¦¨®¿mÁ¦Æs¥¦=çµ«s½¨®¨4§y¡±£½½µ°¨®­®£B¡+³V/¾ ¡H§§y¨±=L§g¥³¡/´ª­º³ ¦¥¦­4«ª´B§ §y¸ ´ÁÈ­±³ ­±L¸ ³§ ®¥¦¡HÁ¡H­±¶­®Î ¸ §yªV­±¥µV­±³ ¸ ³Qµ«³¯ÁÌ³¦@· ³Q£½­±Ë «¡¡±ª°Øy§¦ÏV­®ªV½L§/£B¶Î¥¦´Z¨º/¨ ³¦·³¨º¡XÜ½¨±³¨®ªL¡± ¸ §y¡s¨® ³¦«ª@· ­®=§³¨®Ãª£½L³u.¡ ¡
H«Øy§¦ªLÏu³½ª³ªm½L¾4ª ³¨µ« ¡H¸­ ·¼³¨®¶m¡4«ªg­®§ ³Q¨±£½B¡®ÝÏL¥§y¡H­®¥´µ«¢¡H§ ¸ ·¼³¨oÿo³¶m¥¦ªL«¥ª@Æ ¶m¨®ü ¥u­®ªB¢»»ºÔ¢¡+¥Jª ¸ÿo§y³¶m¥¦¶m¥§mµ¢½¥¦µ«ªLª¯½L¥¥¦¯ª ¸ ­±B·¼§y¥u­Î°BªL¥°ªL+¡ §¦­6Æw¾/¯§J¨º¥¦¨±Ë
"Ufhg £  .j msù  ¡ ±½µ °­®¡Ä£¨®§y¡±§ ¸ ¾4¨®§0³´­º¥¦«ª§ ¸ ½L¡±«ª¯þîyôQôÏ ôôQôr¡±Ë ­±§yªL·¼¨®³¶ ­®§JüL¨±§yªL»º¡± ¸ §³¦·Ì­±L§qëÀ¥¦ªB¡±¥¨ ¸ ³ªË ­®É­®¨®¥Q»­±§ ¸ ¨®µ«¢¥¦´Lµ°§´Á ­®§Ä¡HÁ¡H­±,¶ Lªà¡6­º¥¦¨±­±«ª¯ ¯¥§y«ª ª¸ «ª§y¨H¯dË8¥¦óªÏ VÏ -¾4³¨ ¸ ÇÂ§Ì³Qªµ«¥ ¸ ¥dµ« ¡H­/³/· îÏ Vðð A- ªL³¦ ÿ - ­®  ¥¦­±§ ¸ ü¨®ªB»ºOª³Q½ªLÆ(§·¢­g³QªV / É­º¡ °­±«ª§q¡±¿Á¡6ÉºÏ­±¨º¥Ï§ ¸ ¡HÏ§y§ ¸ ¡ =¥¦¨®Ï §-­±§ ·¼³µ«Ï³u°ªL¯¿X¼Ï¶m¥¡HË $( ºÏ $&amp; ¼Ï"
"Q Ï  õ Ï  ¤ uÏ ,  $ _ * Ï ¥ +   ºÏ   ."
"Q ¦ &quot; Ï §   ºÏ .   ¨  LÏ +  .é ] $   °ªL , °ªL§¦¿ ($(]$   $  p ,($ ]$ $($]     õ    ]ÏQ¥ª *¥ ($ ¸ ]$ ? Æ &quot; ©      ?  (  Ï ¶mä «¥¦ÃQ§qµ« Ácü¨®¨®§y¡±ªL£B»º§l@»Ï-­s­®è£L§À¥¦ªLªB«¥u¡±­± ½L¨®L¥¥µ¡g¯§y­6¾4ª ¸³Ô¯§y·¼³ª ¨4¸ ¥¦ª«¶m¥u­®L«»º´B0§y°ªLªL³¯Q¨±ËÏ £ Uf(e i   «ª¢w ´½­ §o¥ªL¨±§d½­®¡H§y¶m¨s¥¦ªV¥¡#­±¢»µ«¥µ«°ªB¥¥u¨±­±´§ °¸­®¨®¥¨±¢¡HÁg­±³Q·¼³¨±¢»¨-«ªLµ«°¥¦ÁªÏy½L«¶m¡±½L¥¦¥­±§-µ°«Á ª³«ªd½LªL·¥uËÆ ¬³Q¨=³·s¶m¥¡®½Lµ°«ª§=Æ Ú³V¡6­-ª³½LªL¡Ì°ªÈè£L¥¦ªL«¡±^ E-"
Ü 2 Æ A  ²«ª ³½L¥¦¨@¨®Ã­±§l§y¡6­¿ ¸ ¥ud­º¥Q·¼³Ý §y·¼ª §y¸ ««ª«ªL§=¥u¥.­ ª »y¸¥¦ªcÎ´Z/§·¼³QªL¶m¡±¥¡®¸ ¨®½§µ«¸ ª§+¯Üz.§yª¥¸´§yµ«§¨ ­±óÝ¶m¥«³QÆëÀÏ@³u¥¦¾/ª ¸ ¾/³Q¨ ¸ ¡o°­±q³­±§y¨=§yª ¸    ¸ u §y¬ *&quot; §yÏ­®¨®§m¥¦É &quot;* ­®°³QªL¡Ì­±³­®§y¡±§gª¬§yªË  ­±§¸+§³¨®¨±£Z«ª0§l¡H£Z³³¥¦¨®ª ¶m´¸µ«§¥¦ªL§-³¦§y­®*· ­±§d¯Q¡Hª Á¸ ¡H­±¨y§y¶Æ «ªJè£L¥ª¢¡H¢¡o£L¨± Ù&quot;* *&quot; ¾4îlóÏ³¨ó  Q ¸
ÏZ¥¦ª ¸ µ«¢¡H­o³¦î ÿ Ï Q 2 ôQóÎª³½LªL¡oÉV­®¨®¥Q»­®§ ¸ 2 ·¼¨±³Q¶ ÿ ïÏ°î)  ¥ª ÿ ³ªÇÈËjµ« §Ôª§Ìè£L¨®§Ô¥¦ª¡±¢¡±½¨±Ëæ£L#Õ ¨±ª¢¡±§¯µ«¸ ¢¡± ­±L¥¦¸ ¢»­±«¡HÁ³QªL¡H­±¥¦¨®§y¶Á¦Æ £Z¨±·¼³¨®¶m¥¦ªL³Qª è£L¥ª§¢¡HÂµ«·¢­Ä¥¡cªV­®¾4³É¨º¡±­±ÉL¥­±¨ºªÂ¥·¼­±³¨§ +¸ üL´¨±Áà§yªL­±»ºÔ§ ¥¦ª¡HÁ¡H¸ ­±ÿo§y³¶mËæ¡±¥¦ªL«¥¸ §@ª ¸
Æ ¥¦µ«¯Q³¨®­®¶ «ª0]Ï ­®§ÈÐL¨®]Ï ¡
H­±§yLÏ£(¾4]Ï§ V½ °­±]Ï ¨±§yµ°¢¥´µ«§ÂuÏ Ü (  uÏ
"K¬  ] A$ &quot; Ï ¢¬ ]$ uÏ (   : LÏ õ ,  ]Ï = uÏ $  u  Ï ,, uÏ ¥   ]Ï  Ï @  ­ ]Ï «­  Ï $ 6¬  ºÏ =&apos;| Ï    Ï    uÏ  ÝÌ  ¥ª ¸ ¾4³ëo³u¾4¬Qè£L¥ª¢¡HÂL¥Q¡-­±L§m¨®¥¦­±§y¨ £L¥¨H­®«½µ¢¥¦¨-»ºL¥¦¨º¥Ë u$  ½µ ¸ L  ]¥ ¬Q§+´Z   ( §y   $  ¸ ,( ­±³Á . ,¬  «  µ ¸  (.   =   ¨º¥ÁÆ ­ ­±§y¨±¢¡H­±¢»È­±B¥u­Â¶m¥µ« ª§ ¸ ¨®«ª¨º¡q¥¦¨®§Ô½L¡±§ ¸ °­± ·¼«ª«ª§Îª³½LªL¡-¡6­º¥¦¨±­±«ª¯g°­®¡H­±¨®§y¡®¡H§ ¸ ³¦­®ó¦ô§y¨±¨®³¨º¡-´ÁJ­6Á£Z§m¾4¶m¥ ¸ §d·¼³¨d &quot; /¡H³½B¨ ¦Ë®Æî  »ºÈªL³½ªL  ¡ ¥¦ª ¸ ­®Á+·¼½¨H­®£L¨±³Q£L¥¦¯V¥u­±§ ¸ «ª+­®³¨®£³Qµ«³¯ÁÌ¡6@£ Æ ¢¡Ô¡±½¯¯Q§y¡H­®¡Ô¥¯Q¥°ªõ­±B¥u^­ ­®§(»y¥¦ªù´B§wµ¢¥¦ª¯Q½L¥¦¯Q °ÐB»Ì ¸ °§l¡*µ«°­® µ¢³¦Úq¥¦ª¥¦·4ªê=­±¯QLä½L§g¥¦¯QÕ]¥ ¬u§y=¥¦4«µ¢Ü¥¦´ËæÕs³¥«¨®µ ¡H=­­6Ág£Ü³¦¨±§l¢¡±¯¨®§yª µ«Á¦¸ #Ï è³¸ ½¥u­±­®O¥¥è«¬§µ¢¥ª ]¸ ¬Q³·¼³Q¨c­±L§gèµ¢¥ª³¢¨®»y£ZÝ=³´Z¨º¥m§y»y·¼¥¦¨®]½L¬V³¶¡±§¢» £ G£ Uf i7ý ûZø©jZmNj  ­±§y¨#¥ ¦¨®§4³µ«¢¥¦£L¡±§ ¸ §¦ÝÏ¥¥¦ª*¡ ¸«ªd­®ÿo³¶mø Qè¥¦ ªLËjÕs«¥.ä ª@ßÀÏu¶mã0¥è¡®µ«³u½¬µ«§yªª§§ / ¨®£½L¡cÜÕ#¨zÅH]¥ ¬§y»¦ÏLó¦ôôVóÝÆñ ³¸ ª§y½"
K® ¸ §yü¨=³½Lµ¢¥Q¨À¡±¶m¡±¥¦¿«¯¨®¦³ª ½¸ £Bg³¦#·¡H­±§y¶m¡À¥¦¨®§ µ¢¥¦¨®¯§y·¼«ª«+ª§¦Ï ¡H­±¶m¥¨±§-¶m R ¥Q &quot;% ½µ«°ªL§À &quot;* §y·¼³Qµ«°³u¾4§ ¸ ´V¥dÐLªL¥³QªL N ¡H³Ë &quot;* ªL¥ªQ­Ì³¨=ª½­®¨Ì³Q¨±¨®§y¡±£B³Qª ¸ ­±ª³½ªB¡À§yª ¸ «ª¯m«ª &quot; d³¨ ³QªL¡±³ªL¥ªQ¡H­±¶m¥¨±§Ì³Q¡H­±ª§y½­±§yÆ *  c  &quot;* ¥´L¡±ªL³· ¸
ÐBª°­±§¥¦ª ¸ °ª ¸ ÐLªL­®¥¨H­®§y¡c«ª èµ«³u¬§yª£B³V¡H§l¡ ¥¦ªÔ¥ ¸¸ °­±«³QªL¥¦µX»ºL¥µ°«§yª¯§Î·¼³¨c­®§m´Z³³¦­HË ¡H­±¨º¥¦££«ª¯È¥¦µ«¯³Q¨±°­®¶.Ï ­®§µ¢¥¦ª¯Q½L¥¦¯Q§m¢¡±¡±«ª¯J¡±³³¦· ­±L§d³V¡6­Ì¨±§yµ« ¥´µ«§-¯¨º¥¦¶m¥u­®¥¦µ@¯Qª ¸ µ«½§y+§y¨± ·¼³¨®§¦Ï ¾4§ /¾ §y¨±§+ª³¦=­ ¡±½¨®£¨®«¡±§ ¸ §c­®L¥u­=­±§d³u¬§y¨®¥¯§ ³´­®¥¦«ª§ ¸ §yªÐL¨ ®¨±½Lªª«­±§¡±Á¡H­±§y¶ ½L¡±«ªªB¥u­HË ½¨º¥¦µX¯Qª ¸ §y¨-ª³½ªB¡-¾4¥Q¡-¬¨®Áµ«³u¾-¥¡c¡±³uªÈ°ªÈ.¥´µ«§ ðuË8¥ÆÔèV­º¥¦¨±­±«ª¯q°­±O¡HÁ¡H­±­®¨®¥Q»­±§ ¸ ¡H§y§ ¸ ­±«ª°Ë ­±¢¥µ¬§y¨®¥¯§=¯«¬§yªg´Ág³QªV­±É­®½L¥¦µ«½§y¡4¢­H¨À¥ª ¸ ­±L£B§y¨H·¼³Q¨±¶m¥ªL§4¢³£L¥¨®¥´µ«§#­®³=­®³Q¡±·¼³¨#ÿo³¶m¥¦ªB§ @Æ ªLè¾4¥¦]¥ªL¬ª¢¥³¦¦­º¥uªB­±Ýs§ ¯¸ ¨®³ª½L³Q½+ª µ«¢¡µ¢¥¦ªL·¼¯¨®³½L¥¶à¯§y­±L§sÇÂÐLªL§ Ëæ¯ÉV­®¨º¥¦¨®«¥Qª»­®§ ¸ § Ëj¸ å4¥Îç-è-¯Q¥ª ª¸ ª³¨±ËË Uf § ¸ ¢ i ¡± « J bjZksl ~ ¢¡-£L  ¥¨H­c  ³¦·4­±L§mão³¨±­±Lª Ó §y¨±¶m¥ª¢»gÜzè¥¦ª ¸ °Ë £  ­®¥¦­±§ ¸
Ýºî  ³¦·X­®§ÎªL³½ªL=¡ ¾4¨®§ ¶m¥¦¨®Ã§ ¸ ´
"We examine clarification dialogue, a mechanism for refining user questions with follow-up questions, in the context of open domain Question Answering systems."
We develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering.
The algorithm is evaluated and shown to successfully recognize the occurrence of clarification dialogue in the majority of cases and to simplify the task of answer retrieval.
Question Answering Systems aim to determine an answer to a question by searching for a response in a collection of documents (see[REF_CITE]for an overview of current systems).
"In order to achieve this (see for example[REF_CITE]), systems narrow down the search by using information retrieval techniques to select a subset of documents, or paragraphs within documents, containing keywords from the question and a concept which corresponds to the correct question type (e.g. a question starting with the word “Who?” would require an answer containing a person)."
"The exact answer sentence is then sought by either attempting to unify the answer semantically with the question, through some kind of logical transformation (e.g.[REF_CITE]) or by some form of pattern matching (e.g.[REF_CITE])."
"Often, though, a single question is not enough to meet user’s goals and an elaboration or clarification dialogue is required, i.e. a dialogue with the user which would enable the answering system to refine its understanding of the questioner&apos;s needs (for reasons of space we shall not investigate here the difference between elaboration dialogues, clarification dialogues and coherent topical subdialogues and we shall hence refer to this type of dialogue simply as “clarification dialogue”, noting that this may not be entirely satisfactory from a theoretical linguistic point of view)."
"While a number of researchers have looked at clarification dialogue from a theoretical point of view (e.g.[REF_CITE]; van Beek at al. 1993), or from the point of view of task oriented dialogue within a narrow domain (e.g.[REF_CITE]), we are not aware of any work on clarification dialogue for open domain question answering systems such as the ones presented at the TREC workshops, apart from the experiments carried out for the (subsequently abandoned) “context” task in the TREC-10"
Here we seek to partially address this problem by looking at some particular aspect of clarification dialogues in the context of open domain question answering.
"In particular, we examine the problem of recognizing that a clarification dialogue is occurring, i.e. how to recognize that the current question under consideration is part of a previous series (i.e. clarifying previous questions) or the start of a new series; we then show how the recognition that a clarification dialogue is occurring can simplify the problem of answer retrieval."
The[REF_CITE]QA track included a &quot;context&quot; task which aimed at testing systems&apos; ability to track context through a series of questions[REF_CITE].
"In other words, systems were required to respond correctly to a kind of clarification dialogue in which a full understanding of questions depended on an understanding of previous questions."
"In order to test the ability to answer such questions correctly, a total of 42 questions were prepared by NIST staff, divided into 10 series of related question sentences which therefore constituted a type of clarification dialogue; the sentences varied in length between 3 and 8 questions, with an average of 4 questions per dialogue."
These clarification dialogues were however presented to the question answering systems already classified and hence systems did not need to recognize that clarification was actually taking place.
Consequently systems that simply looked for an answer in the subset of documents retrieved for the first question in a series performed well without any understanding of the fact that the questions constituted a coherent series.
"In a more realistic approach, systems would not be informed in advance of the start and end of a series of clarification questions and would not be able to use this information to limit the subset of documents in which an answer is to be sought."
"We manually analysed the TREC context question collection in order to determine what features could be used to determine the start and end of a question series, with the following conclusions: • Pronouns and possessive adjectives: questions such as “When was it born?”, which followed “What was the first transgenic mammal?”, were referring to some previously mentioned object through a pronoun (“it”)."
"The use of personal pronouns (“he”, “it”, …) and possessive adjectives (“his”, “her”,…) which did not have any referent in the question under consideration was therefore considered an indication of a clarification question.. • Absence of verbs: questions such as “On what body of water?” clearly referred to some previous question or answer. • Repetition of proper nouns: the question series starting with “What type of vessel was the modern Varyag?” had a follow-up question “How long was the Varyag?”, where the repetition of the proper noun indicates that the same subject matter is under investigation. • Importance of semantic relations: the first question series started with the question “Which museum in Florence was damaged by a major bomb explosion?”; follow-up questions included “How many people were killed?” and “How much explosive was used?”, where there is a clear semantic relation between the “explosion” of the initial question and the “killing” and “explosive” of the following questions."
"Questions belonging to a series were “about” the same subject, and this aboutness could be seen in the use of semantically related words."
It was therefore speculated that an algorithm which made use of these features would successfully recognize the occurrence of clarification dialogue.
"Given that the only available data was the collection of “context” questions used in TREC-10, it was felt necessary to collect further data in order to test our algorithm rigorously."
This was necessary both because of the small number of questions in the TREC data and the fact that there was no guarantee that an algorithm built for this dataset would perform well on “real” user questions.
"A collection of 253 questions was therefore put together by asking potential users to seek information on a particular topic by asking a prototype question answering system a series of questions, with “cue” questions derived from the TREC question collection given as starting points for the dialogues."
"These questions made up 24 clarification dialogues, varying in length from 3 questions to 23, with an average length of 12 questions (the data is available from the main author upon request)."
The differences between the TREC “context” collection and the new collection are summarized in the following table:
Groups Qs Av. len Max[REF_CITE]41 4 8 4[REF_CITE]12 23 3
The questions were recorded and manually tagged to recognize the occurrence of clarification dialogue.
"The questions thus collected were then fed into a system implementing the algorithm, with no indication as to where a clarification dialogue occurred."
The system then attempted to recognize the occurrence of a clarification dialogue.
Finally the results given by the system were compared to the manually recognized clarification dialogue tags.
In particular the algorithm was evaluated for its capacity to: • recognize a new series of questions (i.e. to tell that the current question is not a clarification of any previous question) (indicated by New in the results table) • recognize that the current question is clarifying a previous question (indicated by Clarification in the table)
Our approach to clarification dialogue recognition looks at certain features of the question currently under consideration (e.g. pronouns and proper nouns) and compares the meaning of the current question with the meanings of previous questions to determine whether they are “about” the same matter.
Given a question q 0 and n previously asked questions q -1 .. q -n we have a function Clarification_Question which is true if a question is considered a clarification of a previously asked question.
"In the light of empirical work such[REF_CITE], which indicates that questioners do not usually refer back to questions which are very distant, we only considered the set of the previously mentioned 10 questions."
A question is deemed to be a clarification of a previous question if: 1.
"There are direct references to nouns mentioned in the previous n questions through the use of pronouns (he, she, it, …) or possessive adjectives (his, her, its…) which have no references in the current question. 2."
The question does not contain any verbs 3.
"There are explicit references to proper and common nouns mentioned in the previous n questions, i.e. repetitions which refer to an identical object; or there is a strong sentence similarity between the current question and the previously asked questions."
In other words:
"Clarification_Question (q n ,q -1 ..q -n ) is true if 1. q 0 has pronoun and possessive adjective references to q -1 ..q -n 2. q 0 does not contain any verbs 3. q 0 has repetition of common or proper nouns in q -1 ..q -n or q 0 has a strong semantic similarity to some q ∈ q -1 .. q -n"
A major part of our clarification dialogue recognition algorithm is the sentence similarity metric which looks at the similarity in meaning between the current question and previous questions.
"WordNet[REF_CITE], a lexical database which organizes words into synsets, sets of synonymous words, and specifies a number of relationships such as hypernym, synonym, meronym which can exist between the synsets in the lexicon, has been shown to be fruitful in the calculation of semantic similarity."
"One approach has been to determine similarity by calculating the length of the path or relations connecting the words which constitute sentences (see for example[REF_CITE]and[REF_CITE]); different approaches have been proposed (for an evaluation see[REF_CITE]), either using all WordNet relations[REF_CITE]or only is-a relations[REF_CITE]."
"Our sentence similarity measure followed on these ideas, adding to the use of WordNet relations, part-of-speech information, compound noun and word frequency information."
"In particular, sentence similarity was considered as a function which took as arguments a sentence s 1 and a second sentence s 2 and returned a value representing the semantic relevance of s 1 in respect of s 2 in the context of knowledge B, i.e. semantic-relevance( s 1 , s 2 , B ) = n ∈ semantic-relevance(s 1 ,s,B) &lt; semantic-relevance(s 2 ,s, B) represents the fact that sentence s 1 is less relevant than s 2 in respect to the sentence s and the context B."
"In our experiments, B was taken to be the set of semantic relations given by WordNet."
"Clearly, the use of a different knowledge base would give different results, depending on its completeness and correctness."
"In order to calculate the semantic similarity between a sentence s 1 and another sentence s 2 , s 1 and s 2 were considered as sets P and Q of word stems."
The similarity between each word in the question and each word in the answer was then calculated and the sum of the closest matches gave the overall similarity.
"In other words, given two sets Q and P, where Q={qw 1 ,qw 2 ,…,qw n } and P={pw 1 ,pw 2 ,…,pw m }, the similarity between Q and P is given by 1&lt;p&lt;n"
"Argmax m similarity( qw p , pw m )"
"The function similarity( w 1 , w 2 ) maps the stems of the two words w 1 and w 2 to a similarity measure m representing how semantically related the two words are; similarity( w i , w j )&lt; similarity( w i , w k ) represents the fact that the word w j is less semantically related than w k in respect to the word w i ."
"In particular similarity=0 if two words are not at all semantically related and similarity=1 if the words are the same. similarity( w 1 , w 2 ) = h ∈ where 0 ≤ h ≤ 1."
"In particular, similarity( w 1 , w 2 ) = 0 if w 1 ∈ST ∨ w 2 ∈ST, where ST is a set containing a number of stop-words (e.g. “the”, “a”, “to”) which are too common to be able to be usefully employed to estimate semantic similarity."
"In all other cases, h is calculated as follows: the words w 1 and w 2 are compared using all the available WordNet relationships (is-a, satellite, similar, pertains, meronym, entails, etc.), with the additional relationship, “same-as”, which indicated that two words were identical."
"Each relationship is given a weighting indicating how related two words are, with a “same as” relationship indicating the closest relationship, followed by synonym relationships, hypernym, hyponym, then satellite, meronym, pertains, entails."
"So, for example, given the question “Who went to the mountains yesterday?” and the second question “Did Fred walk to the big mountain and then to mount Pleasant?”, Q would be the set {who, go, to, the, mountain, yesterday} and P would be the set {Did, Fred, walk, to, the, big, mountain, and, then, to, mount, Pleasant}."
"In order to calculate similarity the algorithm would consider each word in turn. “Who” would be ignored as it is a common word and hence part of the list of stop-words. “Go” would be related to “walk” in a is-a relationship and receive a score h 1 . “To” and “the” would be found in the list of stop-words and ignored. “Mountain” would be considered most similar to “mountain” (same-as relationship) and receive a score h 2 : “mount” would be in a synonym relationship with “mountain” and give a lower score, so it is ignored. “Yesterday” would receive a score of 0 as there are no semantically related words in Q."
The similarity measure of Q in respect to P would therefore be given by h 1 + h 2 .
"In order to improve performance of the similarity measure, additional information was considered in addition to simple word matching (see[REF_CITE]for a complete discussion): • Compound noun information."
"The motivation behind is similar to the reason for using chunking information, i.e. the fact that the word “United” in “United States” should not be considered similar to “United” as in “Manchester United”."
"As opposed to when using chunking information, however, when using noun compound information, the compound is considered a single word, as opposed to a group of words: chunking and compound noun information may therefore be combined as in “[the [United States] official team]”. • Proper noun information."
"The intuition behind this is that titles (of books, films, etc.) should not be confused with the “normal” use of the same words: “blue lagoon” as in the sentence “the film Blue Lagoon was rather strange” should not be considered as similar to the same words in the sentence “they swan in the blue lagoon” as they are to the sentence “I enjoyed Blue Lagoon when I was younger”. • Word frequency information."
"This is a step beyond the use of stop-words, following the intuition that the more a word is common the less it is useful in determining similarity between sentence."
"So, given the sentences “metatheoretical reasoning is common in philosophy” and “metatheoretical arguments are common in philosophy”, the word “metatheoretical” should be considered more important in determining relevance than the words “common”, “philosophy” and “is” as it is much more rare and therefore less probably found in irrelevant sentences."
"Word frequency data was taken from the Given that the questions examined were generic queries which did not necessarily refer to a specific set of documents, the word frequency for individual words was taken to be the word frequency given in the British National Corpus (see[REF_CITE])."
An implementation of the algorithm was evaluated on the TREC context questions used to develop the algorithm and then on the collection of 500 new clarification dialogue questions.
"The results on the TREC data, which was used to develop the algorithm, were as follows (see below for discussion and an explanation of each method):"
TREC Meth.0 Meth.1 Meth.2 Meth.3aMeth.3b
Clarif. 47 53 59 78 72
Where “New” indicates the ability to recognize whether the current question is the first in a new series of clarification questions and “Clarif.” (for “Clarification”) indicates the ability to recognize whether the current question is a clarification question.
The results for the same experiments conducted on the collected data were as follows:
Collected Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b
Clarif. 64 62 66 91 89
"This method did not use any linguistic information and simply took a question to be a clarification question if it had any words in common with the previous n questions, else took the question to be the beginning of a new series. 64% of questions in the new collection could be recognized with this simple algorithm, which did not misclassify any &quot;new&quot; questions."
"This method employed point 1 of the algorithm described in section 5: 62% of questions in the new collection could be recognized as clarification questions simply by looking for &quot;reference&quot; keywords such as he, she, this, so, etc. which clearly referred to previous questions."
Interestingly this did not misclassify any &quot;new&quot; questions.
"This method employed points 1 and 2 of the algorithm described in section 5: 5% of questions in the new collection could be recognized simply by looking for the absence of verbs, which, combined with keyword lookup (Method 1), improved performance to 66%."
Again this did not misclassify any &quot;new&quot; questions.
"This method employed the full algorithm described in section 5 (point 3 is the similarity measure algorithm described in section 6): clarification recognition rose to 91% of the new collection by looking at the similarity between nouns in the current question and nouns in the previous questions, in addition to reference words and the absence of verbs."
"Misclassification was a serious problem, however with correctly classified &quot;new&quot; questions falling to 67%."
"This was the same as method 3a, but specified a similarity threshold when employing the similarity measure described in section 6: this required the nouns in the current question to be similar to nouns in the previous question beyond a specified similarity threshold."
"This brought clarification question recognition down to 89% of the new collection, but misclassification of &quot;new&quot; questions was reduced significantly, with &quot;new&quot; questions being correctly classified 83% of the time."
Problems noted were: • False positives: questions following a similar but unrelated question series.
"E.g. &quot;Are they all Muslim countries?&quot; (talking about religion, but in the context of a general conversation about Saudi Arabia) followed by &quot;What is the chief religion in Peru?&quot; (also about religion, but in a totally unrelated context). • Questions referring to answers, not previous questions (e.g. clarifying the meaning of a word contained in the answer, or building upon a concept defined in the answer: e.g. &quot;What did Antonio Carlos Tobim play?&quot; following &quot;Which famous musicians did he play with?&quot; in the context of a series of questions about Fank Sinatra: Antonio Carlos Tobim was referred to in the answer to the previous question, and nowhere else in the exchange."
"These made up 3% of the missed clarifications. • Absence of relationships in WordNet, e.g. between &quot;NASDAQ&quot; and &quot;index&quot; (as in share index)."
"Absence of verb-noun relationships in WordNet, e.g. between to die and death, between &quot;battle&quot; and &quot;win&quot; (i.e. after a battle one side generally wins and another side loses), &quot;airport&quot; and &quot;visit&quot; (i.e. people who are visiting another country use an airport to get there)"
"As can be seen from the tables above, the same experiments conducted on the TREC context questions yielded worse results; it was difficult to say, however, whether this was due to the small size of the TREC data or the nature of the data itself, which perhaps did not fully reflect “real” dialogues."
"As regards the recognition of question in a series (the recognition that a clarification I taking place), the number of sentences recognized by keyword alone was smaller in the TREC data (53% compared to 62%), while the number of questions not containing verbs was roughly similar (about 6%)."
"The improvement given by computing noun similarity between successive questions gave worse results on the TREC data: using method 3a resulted in an improvement to the overall correctness of 19 percentage points, or a 32% increase (compared to an improvement of 25 percentage points, or a 38% increase on the collected data); using method 3b resulted in an improvement of 13 percentage points, or a 22% increase (compared to an improvement of 23 percentage points or a 35% increase on the collected data), perhaps indicating that in &quot;real&quot; conversation speakers tend to use simpler semantic relationships than what was observed in the TREC data."
Recognizing that a clarification dialogue is occurring only makes sense if this information can then be used to improve answer retrieval performance.
"We therefore hypothesized that noting that a questioner is trying to clarify previously asked questions is important in order to determine the context in which an answer is to be sought: in other words, the answers to certain questions are constrained by the context in which they have been uttered."
"The question “What does attenuate mean?”, for example, may require a generic answer outlining all the possible meanings of “attenuate” if asked in isolation, or a particular meaning if asked after the word has been seen in an answer (i.e. in a definite context which constrains its meaning)."
"In other cases, questions do not make sense at all out of a context."
"For example, no answer could be given to the question “where?” asked on its own, while following a question such as “Does Sean have a house anywhere apart from Scotland?” it becomes an easily intelligible query."
The usual way in which Question Answering systems constrain possible answers is by restricting the number of documents in which an answer is sought by filtering the total number of available documents through the use of an information retrieval engine.
The information retrieval engine selects a subset of the available documents based on a number of keywords derived from the question at hand.
"In the simplest case, it is necessary to note that some words in the current question refer to words in previous questions or answers and hence use these other words when formulating the IR query."
"For example, the question “Is he married?” cannot be used as is in order to select documents, as the only word passed to the IR engine would be “married” (possibly the root version “marry”) which would return too many documents to be of any use."
Noting that the “he” refers to a previously mentioned person (e.g. “Sean Connery”) would enable the answerer to seek an answer in a smaller number of documents.
"Moreover, given that the current question is asked in the context of a previous question, the documents retrieved for the previous related question could provide a context in which to initially seek an answer."
"In order to verify the usefulness of constraining the set of documents from in which to seek an answer, a subset made of 15 clarification dialogues (about 100 questions) from the given question data was analyzed by taking the initial question for a series, submitting it to the Google Internet Search Engine and then manually checking to see how many of the questions in the series could be answered simply by using the first 20 documents retrieved for the first question in a series."
"The results are summarized in the following diagram (Fig. 1): • 69% of clarification questions could be answered by looking within the documents used for the previous question in the series, thus indicating the usefulness of noting the occurrence of clarification dialogue. •[REF_CITE]% could not be answered by making reference to the previously retrieved documents, and to find an answer a different approach had to be taken."
"In particular: • 6% could be answered after retrieving documents simply by using the words in the question as search terms (e.g. “What caused the boxer uprising?”); • 14% required some form of coreference resolution and could be answered only by combining the words in the question with the words to which the relative pronouns in the question referred (e.g. “What film is he working on at the moment”, with the reference to “he” resolved, which gets passed to the search engine as “What film is Sean Connery working on at the moment?”); • 7% required more than 20 documents to be retrieved by the search engine or other, more complex techniques."
"An example is a question such as “Where exactly?” which requires both an understanding of the context in which the question is asked (“Where?” makes no sense on its own) and the previously given answer (which was probably a place, but not restrictive enough for the questioner). • 4% constituted mini-clarification dialogues within a larger clarification dialogue (a slight deviation from the main topic which was being investigated by the questioner) and could be answered by looking at the documents retrieved for the first question in the mini-series."
Recognizing that a clarification dialogue is occurring therefore can simplify the task of retrieving an answer by specifying that an answer must be in the set of documents used the previous questions.
"This is consistent with the results found in the TREC context task[REF_CITE], which indicated that systems were capable of finding most answers to questions in a context dialogue simply by looking at the documents retrieved for the initial question in a series."
"As in the case of clarification dialogue recognition, therefore, simple techniques can resolve the majority of cases; nevertheless, a full solution to the problem requires more complex methods."
The last case indicates that it is not enough simply to look at the documents provided by the first question in a series in order to seek an answer: it is necessary to use the documents found for a previously asked question which is related to the current question (i.e. the questioner could &quot;jump&quot; between topics).
"For example, given the following series of questions starting with Q 1 :"
Q 1 : When was the Hellenistic Age? […]
Q 5 : How did Alexander the great become ruler?
Q 6 : Did he conquer anywhere else?
"Q 7 : What was the Greek religion in the Hellenistic Age? where Q 6 should be related to Q 5 but Q 7 should be related to Q 1 , and not Q 6 ."
"In this case, given that the subject matter of Q 1 is more immediately related to the subject matter of Q 7 than Q 6 (although the subject matter of Q 6 is still broadly related, it is more of a specialized subtopic), the documents retrieved for Q 1 will probably be more relevant to Q 7 than the documents retrieved for Q 6 (which would probably be the same documents retrieved for Q 5 )"
It has been shown that recognizing that a clarification dialogue is occurring can simplify the task of retrieving an answer by constraining the subset of documents in which an answer is to be found.
An algorithm was presented to recognize the occurrence of clarification dialogue and is shown to have a good performance.
"The major limitation of our algorithm is the fact that it only considers series of questions, not series of answers."
"As noted above, it is often necessary to look at an answer to a question to determine whether the current question is a clarification question or not."
"Our sentence similarity algorithm was limited by the number of semantic relationships in WordNet: for example, a big improvement would come from the use of noun-verb relationships."
Future work will be directed on extending WordNet in this direction and in providing other useful semantic relationships.
"Work also needs to be done on using information given by answers, not just questions in recognizing clarification dialogue and on coping with the cases in which clarification dialogue recognition is not enough to retrieve an answer and where other, more complex, techniques need to be used."
It would also be beneficial to examine the use of a similarity function in which similarity decayed in function of the distance in time between the current question and the past questions.
"· ÃÓ ÀÔ¹Ïº$Õ ÀÅ¼ÖÀ¼­ÕÆÀ¼ÕÆÓÆÁÙØ¿,º$¼b¹Ï½ÛÚÜÓ6ÎBÀ¼ÆÚÜÓ Î»¿2Ñ :¸v¹»º$¼b¹F½¾º$~~Ç{·: ¹ÏÀÅÒº$Ó ¼&lt;Î»!½¾º!ÎÏÀÅº$ÝvÃËP=Þ­¸6½¾À¼­,º Ó ßnº$Ø­½»$º Õ~ßVÄ&lt;½¾$º Ý Ñ ÎÏº$ÕÆØnÁC¹»ÀÓ [¹ÏÞº Ò  Ò ÃºÙÐÆÀÅ¹gÄ²Ó6Ú Ò Î»º!½»½»ÀÝ6º ¹ÏÞº;,$ºÙ¹²â6Ó6Ø­Î»¼­¸6Ããåäæ¼M¹»Þ­À½Ó6¼;¹ÏºÙÐV¹&apos;ÁÙÓ6Î Ò Ò ¸"
Ò  ç º
"Ò Î»º!½¾º$¼b¹&lt;¸6¼èÀ¼VÝ6º!½g¹ÏÀÅ×b¸v¹ÏÀÅÓ )¼ ÀÅ¼b¹ÏÓ[¹ÏÞºéØ­½¾º :· ÈÉê  ºF¿, Î&apos;ÁÙÓ6¼VÝ  ½Ï¸v¹ÏÀÅÓ  Ò !ÁÌÞëÎÏº$ ¼ÀÅ¹»ÀÓ6¼Xã~&lt;à  Ò ÀÓ6Øn½¾ÃÄ Ò ÎÏÓ Ò Ó ½»º$ÕÖ,¿ ºÙ¹ÏÞÓÆÕ=½ Ó6ßÀ¼À¼× :· ÈÆ=É Ñß­¸ ½¾º!Õ&lt;Ø¼À×6¿ ,¿ ÓÆÕÆ ç ÀÅ¹Ï¼Ñ × ÎÏ¸6¿í¿,ÓÆÕÆº$:Ã ÎÏº$ÕÆØ­ÁÙ¹»ÀÓ6¼n½"
À¼ Ò  Ò ÃºÙÐÆÀÅ¹gÄ  Ò !
Ò  Þ­¸ ~½ ßnº$
"Ò Ó Î¾¹Ïº$ÕîÓ ¼ ç ÎÏÀÅ¹¾¹»º$~º  Ò Î»º!½¾º$¼b¹0¸ Ú¸6¿, Ò Ó ,ÓÆÕVÑ º$Ã½À¼ ç"
"ÞÀÂÁÌÞ )½¾À¿,{:¸ ç Ó6ÓÆÎÌÕÆÕVº$ÑöÃnÞÀÀÂ,½g¹Ï¼ Ó6¹ÏÞÎÏÄÒÚ¸6¿,ÀÅ÷TÄbÞÀº$º2ÃÕ­¿2?½ ÐVÀ×6ÎÏº$Ø¸v¿øÎ»!¹»º ÎÏÕÆÓ Ø­Ò ÁCÄÑ ¹ÏÀÅÓÁ$¿,¼&lt;=ÀÅ¼À¿ Ò  Ò  ¼ÀÅÍÑ ÓvÝ  ¸F¹ÏÎ»À×Ò Î»ÓvÝÎÏ¸6¿,$º ¿,¼ ÓÆÕÆº$ÃÛÓ!¼¹ÏÞºoÈÁÙÓ6× ç ÀÅ¹ÏÁÌÞVßkÓ   Ò ~ ç ÀÅ¹»Þ)¸OÁÙÓ6¿ Ò  Ó6Ú/¹»ÞÀÂ½ $$Õé¿,ÓÆÕÆº$Ã ç ÀÅ¹»Þx¸ Ò  Ó Ø­½¾ÃÄ Ò Î»Ó Ò Ób½¾º!Õù¹»Ó Ò ÀÁÙÑöÕº Ò $ $º ¼ ¹ÏÎ»Ó Ò ,Ä ,¿"
Previous work on minimizing weighted finite-state automata (including transducers) is limited to particular types of weights.
"We present efficient new minimization algorithms that apply much more generally, while being simpler and about as fast."
We also point out theoretical limits on minimization algo-rithms.
We characterize the kind of “well-behaved” weight semirings where our methods work.
"Outside these semirings, minimization is not well-defined (in the sense of producing a unique minimal automaton), and even finding the minimum number of states is in general NP-complete and inapproximable."
"It is well known how to efficiently minimize a determin-istic finite-state automaton (DFA), in the sense of con-structing another DFA that recognizes the same language as the original but with as few states as possible[REF_CITE]."
This DFA also has as few arcs as possible.
"Minimization is useful for saving memory, as when building very large automata or deploying NLP systems on small hand-held devices."
"When automata are built up through complex regular expressions, the savings from minimization can be considerable, especially when ap-plied at intermediate stages of the construction, since (for example) smaller automata can be intersected faster."
Recently the computational linguistics community has turned its attention to weighted automata that compute interesting functions of their input strings.
"A traditional automaton only returns an boolean from the set K = {true,false}, which indicates whether it has accepted the input."
"But a probabilistic automaton returns a prob-ability in K = [0,1], or equivalently, a negated log-probability in K = [0, ∞]."
A transducer returns an output string from K = ∆ ∗ (for some alphabet ∆).
Celebrated algorithms by Mohri (1997; 2000) have recently made it possible to minimize deterministic au-tomata whose weights (outputs) are log-probabilities or strings.
These cases are of central interest in language and speech processing.
"However, automata with other kinds of weights can also be defined."
"The general formulation of weighted automata[REF_CITE]permits any weight set K, if appropriate operations ⊕ and ⊗ are pro-vided for combining weights from the different arcs of the automaton."
"The triple (K, ⊕, ⊗) is called a weight semiring and will be explained below."
K-valued func-tions that can be computed by finite-state automata are called rational functions.
How does minimization generalize to arbitrary weight semirings?
The question is of practical as well as theoret-ical interest.
"Some NLP automata use the real semiring (R, +, ×), or its log equivalent, to compute unnormalized probabilities or other scores outside the range [0, 1][REF_CITE]."
Expectation semir-ings[REF_CITE]are used to handle bookkeeping when training the parameters of a probabilistic transducer.
"A byproduct of this paper is a minimization algorithm that works fully with those semirings, a new result permitting more efficient automaton processing in those situations."
"Surprisingly, we will see that minimization is not even well-defined for all weight semirings!"
"We will then (nearly) characterize the semirings where it is well-defined, and give a recipe for constructing minimization algorithms similar to Mohri’s in such semirings."
"Finally, we follow this recipe to obtain a specific, sim-ple and practical algorithm that works for all division semirings."
All the cases above either fall within this framework or can be forced into it by adding multiplica-tive inverses to the semiring.
"The new algorithm provides arguably simpler minimization for the cases that Mohri has already treated, and also handles additional cases."
We introduce weighted automata by example.
The trans-ducer below describes a partial function from strings to strings.
It maps aab 7→ xyz and bab 7→ wwyz.
"Since the transducer is deterministic, each input (such as aab) is accepted along at most one path; the correspond-ing output (such as xyz) is found by concatenating the output strings found along the path. ε denotes the empty string. a:y a:x b:zz1 2 b:z 0 a:wwy b:ε 5 3 b:εb:wwzzz 4 δ and σ standardly denote the automaton’s transition and output functions: δ(3, a) = 2 is the state reached by the a arc from state 3, and σ(3, a) = wwy is that arc’s output."
"In an automaton whose outputs (weights) were num-bers rather than strings like wwy, concatenating them would not be sensible; instead we would want to add or multiply the weights along the path."
In general ⊗ denotes the chosen operation for combining weights along a path.
The ⊗ operation need not be commutative—indeed concatenation is not—but it must be associative.
"K must contain (necessarily unique) weights, denoted 1 and 0, such that 1 ⊗ k = k ⊗ 1 = k and 0 ⊗ k = k ⊗ 0 = 0 for all k ∈ K."
"An unaccepted input (e.g., aba) is assigned the output 0."
"When ⊗ is string concatenation, 1 = ε, and 0 is a special object ∅ defined to satisfy the axioms."
"If an input such as aa were accepted along multiple paths, we would have to use another operation ⊕ to com-bine those paths’ weights into a single output for aa."
But that cannot happen with the deterministic automata treated by this paper.
"So we omit discussion of the prop-erties that ⊕ should have, and do not trouble to spell out its definition for the semirings (K, ⊕, ⊗) discussed in this paper. 1"
"We are only concerned with the monoid (K, ⊗)."
The following automaton is equivalent to the previous one since it computes the same function: a:yz a:x b:zzz1 2 b:ε 0 a:yz b:ww 3 5b:εb:zzz 4
"However, it distributes weights differently along the arcs, and states 1 and 3 can now obviously be merged (as can 2 and 4, yielding the minimal equivalent automaton)."
"For-mally we know that states 1 and 3 are equivalent because F 1 = F 3 , where F q denotes the suffix function of state q—the function defined by the automaton if the start state is taken to be q rather than 0. (Thus, F 3 (ab) = yz.)"
"Equivalent states can safely be merged, by deleting one and rerouting its incoming arcs to the other."
We will follow Mohri’s minimization strategy: [Footnote_1]. Turn the first automaton above into the second.
"1 Though appropriate definitions do exist for our examples. For example, take the ⊕ of two strings to be the shorter of the two, breaking ties by a lexicographic ordering."
This operation is called pushing (or quasi-determinization).
"Here, for instance, it “pushed ww back” through state 3. [Footnote_2]."
"2 That is, any other solution is isomorphic to the one found here if output weights are ignored."
"Merge equivalent states of the second automaton, by applying ordinary unweighted DFA minimization ([REF_CITE]section 4.13) as if each weighted arc label such as a:yz were simply a letter in a large alphabet. 3. Trim the result, removing useless states and arcs that are not on any accepting path (defined as a path whose weight is non-0 because it has no missing arcs and its last state is final)."
"We will only have to modify step [Footnote_1], generalizing push-ing to other semirings."
"1 Though appropriate definitions do exist for our examples. For example, take the ⊕ of two strings to be the shorter of the two, breaking ties by a lexicographic ordering."
Pushing makes heavy use of left quotients: we adopt the notation k\m for an element of K such that k ⊗ (k\m) = m.
This differs from the nota-tion k −1 ⊗ m (in which k −1 denotes an actual element of K) because k\m need not exist nor be unique.
"For exam-ple, ww\wwzzz = zzz (a fact used above) but wwy\wwzzz does not exist since wwzzz does not begin with wwy."
"If F is a function, α is a string, and k is a weight, we use some natural notation for functions related to F: k ⊗ F : (k ⊗ F)(γ) def = k ⊗ (F(γ)) k\F : a function (if one exists) with k ⊗ (k\F) ="
F α −1 F : (α −1 F)(γ) def = F(αγ) (standard notation)
"In effect, k\F and α −1 F drop output and input prefixes."
The intuition behind pushing is to canonicalize states’ suffix functions.
This increases the chance that two states will have the same suffix function.
"In the example of the previous section, we were able to replace F 3 with ww\F 3 (pushing the ww backwards onto state [Footnote_3]’s incoming arc), making it equal to F 1 so {1, [Footnote_3]} could merge."
"3 In general we treat F q as a partial function, so that range(F q ) excludes 0 (the weight of unaccepted strings). Left factors are unaffected, as anything can divide 0."
"3 In general we treat F q as a partial function, so that range(F q ) excludes 0 (the weight of unaccepted strings). Left factors are unaffected, as anything can divide 0."
"Since canonicalization was also performed at states 2 and 4, F 1 and F 3 ended up with identical representa-tions: arc weights were distributed identically along cor-responding paths from 1 and 3."
"Hence unweighted mini-mization could discover that F 1 = F 3 and merge {1, 3}."
Mohri’s pushing strategy—we will see others—is al-ways to extract some sort of “maximum left factor” from each suffix function F q and push it backwards.
"That is, he expresses F q = k ⊗ G for as “large” a k ∈ K as possible—a maximal common prefix—then pushes fac-tor k back out of the suffix function so that it is counted earlier on paths through q (i.e., before reaching q). q’s suffix function now has canonical form G (i.e., k\F q )."
How does Mohri’s strategy reduce to practice?
"For transducers, where (K,⊗) = (∆ ∗ ,concat), the maxi-mum left factor of F q is the longest common prefix of the strings in range(F q ). 3"
"Thus we had range(F 3 ) = {wwyz,wwzzz} above with longest common prefix ww."
"For the tropical semiring (R ≥0 ∪ {∞}, min, +), where k\m = m − k is defined only if k ≤ m, the maximum left factor k is the minimum of range(F q )."
But “maximum left factor” is not an obvious notion for all semirings.
"If we extended the tropical semir- ing with negative numbers, or substituted the semiring (R ≥0 , +, ×), keeping the usual definition of “maximum,” then any function would have arbitrarily large left factors."
A more √fundamentally problematic example√ is the semiring Z[ −5].
"It is defined as ({m+n −5 : m, n ∈ Z},+,×) where Z denotes the integers."
It is a stan-dard example of a commutative algebra in which fac-torization√ is not unique√ .
"For example, 6 = 2 ⊗ 3 = (1 + −5) ⊗ (1 − −5) and these [Footnote_4] factors cannot be factored further."
"4 A further wrinkle lies in deciding what and how to push; in general semirings, it can be necessary to shift weights forward as well as backward along paths. Modify the example above by pushing a factor of 2 backwards through state 2. Making F 2 = F 3 in this modified√ example now requires pushing 2 forward and then 1 + −5 backward through state 2."
This makes it impossible to canonicalize F 2 below: a:3 1 b:(1+sqrt(-5)) a:1 a:[Footnote_6] b:1 4 0 2 b:(2+2*sqrt(-5)) c:1 a:(1-sqrt(-5)) 3 b:2
"6 This argument only shows that pushing backward cannot give them the same suffix function. But pushing forward cannot help either, despite footnote 4, since 1 n on the arc to i has no right factors other than itself (the identity) to push forward."
What is the best left factor to extract√ from F 2 ?
We could left-divide F 2 by either 2 or 1 + −5.
"The former action allows us to merge {1, 2} and the latter to merge {2, 3}; but we cannot have it both ways."
So this automaton has no unique minimization!
The minimum of 4 states is achieved by two distinct answers (contrast footnote 2).
"It follows that known minimization techniques will not work in general semirings, as they assume state merge-ability to be transitive. [Footnote_4]"
"4 A further wrinkle lies in deciding what and how to push; in general semirings, it can be necessary to shift weights forward as well as backward along paths. Modify the example above by pushing a factor of 2 backwards through state 2. Making F 2 = F 3 in this modified√ example now requires pushing 2 forward and then 1 + −5 backward through state 2."
"In general the result of mini-mization is not even well-defined (i.e., unique)."
"Of course, given a deterministic automaton M, one may still seek an equivalent M̄ with as few states as pos-sible."
"But we will now see that even finding the minimum number of states is NP-complete, and inapproximable."
The NP-hardness proof [which may be skipped on a first reading] is by reduction from Minimum Clique Par-tition.
"Given a graph with vertex set V = {1, 2, . . . n} and edge set E, we wish to partition V into as few cliques as possible. (S ⊆ V is a clique of the graph iff ij ∈ E for all pairs i,j ∈ S.)"
"Determining the minimum num-ber of cliques is NP-complete and inapproximable: that is, unless P=NP, we cannot even find it within a factor of 2 or 3 or any other constant factor in polynomial time. [Footnote_5]"
5 This problem is just the dual of Graph Coloring. For de-tailed approximability results see[REF_CITE].
"Given such a graph, we reduce the clique problem to our problem."
"Consider the “bitwise boolean” semiring ({0, 1} n , OR , AND )."
"Each weight k is a string of n bits, denoted k 1 , . . . k n ."
"For each i ∈ V , define f i , k i , m i ∈ K as follows: f ji = 0 iff ij ∈ E; k ij = 1 iff i = j; m ij = 0 iff either ij ∈ E or i = j. Now consider the following automaton M over the alphabet Σ = {a, b, c 1 , . . . c n }."
"The states are {0, 1, . . . n, n+1}; 0 is the initial state and n + 1 is the only final state."
"For each i ∈ V , there is an n i i arc 0−−→ c i :1 i and arcs i−−→ a:k (n + 1) and i−−→ b:m (n + 1)."
A minimum-state automaton equivalent to M must have a topology obtained by merging some states of V .
Other topologies that could accept the same language (c 1 |c 2 | · · · |c n )(a|b) are clearly not minimal (they can be improved by merging final states or by trimming).
"We claim that for S ⊆ {1, 2, . . . n}, it is possible to merge all states in S into a single state (in the automaton) if and only if S is a clique (in the graph): • If S is a clique, then define k,m ∈ K by k i = 1 iff i ∈ S, and m i = 1 iff i 6∈ S. Observe that for every i ∈ S, we have k i = f i ⊗ k, m i = f i ⊗ m. So by pushing back a factor of f i at each i ∈ S, one can make all i ∈ S share a suffix function and then merge them. • If S is not a clique, then choose i,j ∈ S so that ij 6∈ E. Considering only bit i, there exists no bit pair (k i ,m i ) ∈ {0,1} 2 of which (k ii ,m ii ) = (1,0) and (k ij ,m ji ) = (0,1) are both left-multiples."
"So there can exist no weight pair (k,m) of which (k i ,m i ) and (k j , m j ) are both left-multiples."
It is therefore not pos-sible to equalize the suffix functions F i and F j by left-dividing each of them. 6 i and j cannot be merged.
"Thus, the partitions of V into cliques are identical to the partitions of V into sets of mergeable states, which are in 1-1 correspondence with the topologies of automata equivalent to M and derived from it by merging."
There is an N-clique partition of V iff there is an (N +2)-state au-tomaton.
"It follows that finding the minimum number of states is as hard, and as hard to approximate within a con-stant factor, as finding the minimum number of cliques."
The previous section demonstrated the existence of pathological weight semirings.
"We now partially charac-terize the “well-behaved” semirings (K, ⊕, ⊗) in which all automata do have unique minimizations."
"Except when otherwise stated, lowercase variables are weights ∈ K and uppercase ones are K-valued rational functions. [This section may be skipped, except the last paragraph.]"
"A crucial necessary condition is that (K,⊗) allow what we will call greedy factorization, meaning that given f ⊗F = g⊗G =6 0, it is always possible to express"
F = f 0 ⊗ H and G = g 0 ⊗ H.
"This condition holds for many practically useful semirings, commutative or other-wise."
"It says, roughly, that the order in which left factors are removed from a suffix function does not matter."
We can reach the same canonical H regardless of whether we left-divide first by f or g.
"Given a counterexample to this condition, one can con-struct an automaton with no unique√ minimization."
"Sim-ply follow the plan of the Z[ −5] example, putting F 1 = F, F 2 = f ⊗ F = g ⊗ G, F 3 = G. [Footnote_7] For ex-ample, in semiring (K, ⊗) = ({x n : n =6 1}, concat), put F 2 = x 2 ⊗ {(a, x 3 ), (b, x 4 )} = x 3 ⊗ {(a, x 2 ), (b, x 3 )}."
"7 Then factoring F 2 allows state 2 to merge with either 1 or 3; but all three states cannot merge, since any suffix function that could be shared by 1 and 3 could serve as H."
Some useful semirings do fail the condition.
"One is the “bitwise boolean” semiring that checks a string’s membership in two languages at once: (K,⊕,⊗) = ({00, 01, 10, 11}, OR , AND ). (Let F 2 = 01 ⊗ {(a, 11), (b, 00)} = 01 ⊗ {(a, 01), (b, 10)}.)"
R 2 under pointwise × (which computes a string’s probability under two models) fails similarly.
"So does (sets, ∩, ∪) (which collects features found along the accepting path)."
We call H a residue of F iff F = f 0 ⊗ H for some f 0 .
"Write F &apos; G iff F, G have a common residue."
"In these terms, (K, ⊗) allows greedy factorization iff F &apos; G when F, G are residues of the same nonzero function."
"More perspicuously, one can show that this holds iff &apos; is an equivalence relation on nonzero, K-valued functions."
"So in semirings where minimization is uniquely de-fined, &apos; is necessarily an equivalence relation."
"Given an automaton M for function F, we may regard &apos; as an equivalence relation on the states of a trimmed version of M: [Footnote_8] q &apos; r iff F q &apos; F r ."
8 Trimming ensures that suffix functions are nonzero.
"Let [r] = {r 1 ,...,r m } be the (finite) equivalence class of r: we can inductively find at least one function F [r] that is a common residue of F r 1 , . .. , F r m ."
"The idea behind minimization is to construct a machine M̄ whose states correspond to these equivalence classes, and where each [r] has suffix func-tion F [r] ."
The Appendix shows that M̄ is then minimal. 0
"If M has an arc q−→ a:k r, M̄ needs an arc [q]−→ a:k [r], where k 0 is such that a −1 F [q] = k 0 ⊗ F [r] ."
The main difficulty in completing the construction of M̄ is to ensure each weight k 0 exists.
"That is, F [r] must be carefully chosen to be a residue not only of F r 1 , . . . , F r m (which ultimately does not matter, as long as F [0] is a residue of F 0 , where 0 is the start state) but also of a −1 F [q] ."
"If M is cyclic, this imposes cyclic dependen-cies on the choices of the various F [q] and F [r] functions."
"We have found no simple necessary and sufficient con-dition on (K, ⊗) that guarantees a globally consistent set of choices to exist."
"However, we have given a useful nec- essary condition (greedy factorization), and we now give a useful sufficient condition."
"Say that H is a minimum residue of G 6= 0 if it is a residue of every residue of G. (If G has several minimum residues, they are all residues of one another.)"
"If (K, ⊗) is such that every G has a min-imum residue—a strictly stronger condition than greedy factorization—then it can be shown that G has the same minimum residues as any H &apos; G. In such a (K,⊗), M̄ can be constructed by choosing the suffix functions F [r] independently."
"Just let F [r] = F {r 1 ,...,r m } be a mini-mum residue of F r 1 ."
"Now consider again M’s arc q−→ a:k r: since a −1 F [q] &apos; a −1 F q &apos; F r &apos; F r 1 , we see F [r] is a (minimum) residue of a −1 F [q] , so that a weight k 0 can be 0 chosen for [q]−→ a:k [r]."
"A final step ensures that M̄ defines the function F. To describe it, we must augment the formalism to allow an initial weight ι(0) ∈ K, and a final weight φ(r) ∈ K for each final state r."
The weight of an accepting path from the start state 0 to a final state r is now defined to be ι(0) ⊗ (weights of arcs along the path) ⊗ φ(r).
"In M̄, we set ι([0]) to some k such that F 0 = k ⊗ F [0] , and set φ([r]) ="
F [r] (ε).
The mathematical construction is done.
"We now give an effective algorithm for minimization in the semiring (K, ⊗)."
"The algorithmic recipe has one in-gredient: along with (K, ⊗), the user must give us a left-factor functional λ that can choose a left factor λ(F) of any function F. Formally, if Σ is the input alphabet, then we require λ : (Σ ∗ → K) → K to have the following properties for any rational F : Σ ∗ → K and any k ∈ K: • Shifting: λ(k ⊗ F) = k ⊗ λ(F). • Quotient: λ(F)\λ(a −1 F) exists in K for any a ∈ Σ. • Final-quotient: λ(F)\F(ε) exists in K. [Footnote_9]"
"9 To show the final-quotient property given the other two, it suffices to show that λ(G) ∈ K has a right inverse in K, where G is the function mapping ε to 1 and everything else to 0."
The algorithm generalizes Mohri’s strategy as outlined in section 2.
We just use λ to pick the left factors during pushing.
The λ’s used by Mohri for two semirings were mentioned in section 3.
We will define another λ in sec-tion 6.
"Naturally, it can be shown that no λ can exist√ in a semiring that lacks greedy factorization, such as Z[ −5]."
The 3 properties above are needed for the strategy to work.
"The strategy also requires (K, ⊗) to be left can-cellative, i.e., k ⊗ m = k ⊗ m 0 implies m = m 0 (if k 6= 0)."
"In other words, left quotients by k are unique when they exist (except for 0\0)."
This relieves us from having to make arbitrary choices of weight during push-ing.
Incompatible choices might prevent arc labels from matching as desired during the merging step of section 2.
Given an input DFA.
"At each state q, simultaneously, we will push back λ(F q )."
This pushing construction is trivial once the λ(F q ) values are computed.
"An arc q−→ a:k r should have its weight changed from k to λ(F q ) \λ(a −1 F q ) = λ(F q )\λ(k ⊗ F r ), which is well-defined (by the quotient property and left cancellativity) [Footnote_10] and can be computed as λ(F q )\(k ⊗λ(F r )) (by the shift-ing property)."
"10 Except in the case 0\0, which is not uniquely defined. This arises only if F q = 0, i.e., q is a dead state that will be trimmed later, so any value will do for 0\0: arcs from q are irrelevant."
"Thus a subpath q−→ a:k r−→ b:` s, with weight 0 b:` 0 k ⊗ `, will become q−→ a:k r−→s, with weight k 0 ⊗ ` 0 = (λ(F q )\(k ⊗ λ(F r ))) ⊗ (λ(F r )\(` ⊗ λ(F s )))."
"In this way the factor λ(F r ) is removed from the start of all paths from r, and is pushed backwards through r onto the end of all paths to r."
"It is possible for this factor (or part of it) to travel back through multiple arcs and around cycles, since k 0 is found by removing a λ(F q ) factor from all of k ⊗ λ(F r ) and not merely from k."
"As it replaces the arc weights, pushing also replaces the initial weight ι(0) with ι(0) ⊗ λ(F 0 ), and replaces each final weight φ(r) with λ(F r )\φ(r) (which is well-defined, by the final-quotient property)."
"Altogether, push-ing leaves path weights unchanged (by easy induction). [Footnote_11]"
"11 One may prefer a formalism without initial or final weights. If the original automaton is free of final weights (other than 1), so is the pushed automaton—provided that λ(F) = 1 whenever F(ε) = 1, as is true for all λ’s in this paper. Initial weights can be eliminated at the cost of duplicating state 0 (details omitted)."
"After pushing, we finish with merging and trimming as in section 2."
"While merging via unweighted DFA mini-mization treats arc weights as part of the input symbols, what should it do with any initial and final weights?"
The start state’s initial weight should be preserved.
"The merg-ing algorithm can and should be initialized with a multi-way partition of states by final weight, instead of just a 2-way partition into final vs. non-final. [Footnote_12]"
"12 Alternatively, Mohri (2000, §4.5) explains how to tem-porarily eliminate final weights before the merging step."
The Appendix shows that this strategy indeed finds the unique minimal automaton.
It is worth clarifying how this section’s effective al-gorithm implements the mathematical construction from the end of section 4.
"At each state q, pushing replaces the suffix function F q with λ(F q )\F q ."
"The quotient proper-ties of λ are designed to guarantee that this quotient is defined, [Footnote_13] and the shifting property is designed to ensure that it is a minimum residue of F q . 14"
"13 That is, λ(F q )\F q (γ) exists for each γ ∈ Σ ∗ . One may show by induction on |γ| that the left quotients λ(F)\F(γ) ex-ist for all F. When |γ| = 0 this is the final-quotient property. For |γ| &gt; 0 we can write γ as aγ 0 , and then λ(F)\F(γ) = λ(F)\F(aγ 0 ) = λ(F)\(a −1 F)(γ 0 ) = (λ(F)\λ(a −1 F)) ⊗ (λ(a −1 F)\(a −1 F)(γ 0 )), where the first factor exists by the quotient property and the second factor exists by inductive hy-pothesis."
"In short, if the con-ditions of this section are satisfied, so are the conditions of section 4, and the construction is the same."
"The converse is true as well, at least for right cancella-tive semirings."
"If such a semiring satisfies the conditions of section 4 (every function has a minimum residue), then the requirements of this section can be met to obtain an effective algorithm: there exists a λ satisfying our three properties, 15 and the semiring is left cancellative. [Footnote_16]"
"16 Let hx, yi denote the function mapping a to x, b to y, and everything else to 0. Given km = km 0 , we have k ⊗ hm, 1i = k⊗hm 0 , 1i. Since the minimum residue property implies greedy factorization, we can write hm,1i = f ⊗ ha,bi, hm 0 ,1i = g ⊗ ha,bi. Then f ⊗ b = g ⊗ b, so by right cancellativity f = g, whence m = f ⊗ a = g ⊗ a = m 0 ."
"For the most important idea of this paper, we turn to a common special case."
"Suppose the semiring (K, ⊕, ⊗) defines k\m for all m, k 6= 0 ∈ K.[REF_CITE]sup-pose every k 6= 0 ∈ K has a unique two-sided inverse k −1 ∈ K. Useful cases of such division semirings in-clude the real semiring (R, +, ×), the tropical semiring extended with negative numbers (R∪{∞}, min, +), and expectation semirings[REF_CITE]."
Minimization has not previously been available in these.
We propose a new left-factor functional that is fast to compute and works in arbitrary division semirings.
"We avoid the temptation to define λ(F) as L range(F): this definition has the right properties, but in some semirings including (R ≥0 , +, ×) the infinite summation is quite ex-pensive to compute and may even diverge."
Instead (un-like Mohri) we will permit our λ(F) to depend on more than just range(F).
"Order the space of input strings Σ ∗ by length, breaking ties lexicographically."
"For example, ε &lt; bb &lt; aab &lt; aba &lt; abb."
"Then we can rewrite the identity F q = λ(F q ) ⊗ (λ(F q ) \F q ), using the shifting property, as x ⊗ X = x ⊗ λ(X)⊗(λ(F q )\F q )."
"As we have separately required the semir-ing to be left cancellative, this implies that X = λ(X) ⊗ (λ(F q )\F q )."
"So (λ(F q )\F q ) is a residue of any residue X of F q , as claimed. 15 Define λ(0) = 0."
"From each equivalence class of nonzero functions under &apos;, pick a single minimum residue (axiom of choice)."
"Given F, let [F] denote the minimum residue from its class."
Observe that F = f ⊗[F] for some f; right cancellativity implies f is unique.
So define λ(F) = f. Shifting property: λ(k ⊗ F) = λ(k ⊗ f ⊗ [F]) = k ⊗ f = k ⊗ λ(f ⊗ [F]) = k ⊗ λ(F).
Quotient property: λ(a −1 F) ⊗ [a −1 F] = a −1 F = a −1 (λ(F) ⊗ [F]) = λ(F) ⊗ a −1 [F] = λ(F) ⊗ λ(a −1 [F]) ⊗ [a −1 [F]] = λ(F) ⊗ λ(a −1 [F]) ⊗ [a −1 F] (the last step since a −1 [F] &apos; a −1 F).
"Applying right cancellativity, λ(a −1 F) = λ(F)⊗λ(a −1 [F]), showing that λ(F)\λ(a −1 F) exists."
Final-quotient property: Quotient exists since F(ε) = λ(F)⊗[F](ε). def F(min support(F)) ∈ K if F 6= 0 λ(F) = 0 if F = 0 where support(F) denotes the set of input strings to which F assigns a non-0 weight.
This λ clearly has the shifting property needed by section 5.
The quotient and final-quotient properties come for free because we are in a division semiring and because λ(F) = 0 iff F = 0.
"Under this definition, what is λ(F q ) for a suffix func-tion F q ?"
Consider all paths of nonzero weight [Footnote_18] from state q to a final state.
"18 In a division semiring, these are paths free of 0-weight arcs."
"If none exist, λ(F q ) = 0."
"Oth-erwise, min support(F q ) is the input string on the short-est such path, breaking ties lexicographically. [Footnote_19] λ(F q ) is simply the weight of that shortest path."
"19 The min exists since &lt; is a well-ordering. In a purely lex-icographic ordering, a ∗ b ⊆ Σ ∗ would have no min."
"To push, we must compute λ(F q ) for each state q."
"This is easy because λ(F q ) is the weight of a single, minimum-length and hence acyclic path from q. (Previous meth-ods combined the weights of all paths from q, even if infinitely many.)"
"It also helps that the left factors at dif-ferent states are related: if the minimum path from q be-gins with a weight-k arc to r, then it continues along the minimum path from r, so λ(F q ) = k ⊗ λ(F r )."
Below is a trivial linear-time algorithm for computing λ(F q ) at every q. Each state and arc is considered once in a breadth-first search back from the final states. len(q) and first(q) store the string length and first letter of a run-ning minimum of support(F q ) ∈ Σ ∗ .
"The runtime is O(|states|+t·|arcs|) if ⊗ has runtime t. If ⊗ is slow, this can be reduced to O(t · |states| + |arcs|) by removing line 16 and waiting until the end, when the minimum path from each non-final state q is fully known, to compute the weight λ(F q ) of that path."
Simply finish up by calling F IND -λ on each state q:
"F IND -λ(state q): 1. if λ(F q ) = 0 and len(q) &lt; ∞ then 2. λ(F q ) := σ(q, first(q)) ⊗ F IND -λ(δ(q, first(q))) 3. return λ(F q )"
"After thus computing λ(F q ), we simply proceed with pushing, merging, and trimming as in section 5. [Footnote_20]"
"20 It is also permissible to trim the input automaton at the start, or right after computing λ (note that λ(F q ) = 0 iff we should trim q). This simplifies pushing and merging. No trimming is then needed at the end, except to remove the one dead state that the merging step may have added to complete the automaton."
Push-ing runs in time O(t·|arcs|) and trimming in O(|states|+ |arcs|).
"Merging is worse, with time O(|arcs| log |states|)."
The trouble with Z[ −5] was that it “lacked” needed quotients.
"The example on p. 3 can easily be minimized (down to 3 states) if we regard it instead as defined over (C, +, ×)—letting us use any weights in C. Simply use section 6’s algorithm."
This new change-of-semiring trick can be used for other non-division semirings as well.
"One can extend the original weight semiring (K, ⊕, ⊗) to a division semiring by adding ⊗-inverses. [Footnote_21]"
"21 This is often possible but not always; the semiring must be cancellative, and there are other conditions. Even disregarding ⊕ because we are minimizing a deterministic automaton, it is not simple to characterize when the monoid (K, ⊗) can be em-bedded into a group ([REF_CITE]chapter 12)."
"In this way, the tropical semiring (R ≥0 ∪ {∞}, min, +) can be augmented with the negative reals to ob-tain (R ∪ {∞},min,+)."
"And the transducer semiring (∆ ∗ ∪ {∅}, min, concat) [Footnote_22] can be augmented by extend-ing the alphabet ∆ = {x,y,...} with inverse letters {x −1 , y −1 , . . .}."
22 Where min can be defined as in section 6 and footnote 1.
The minimized DFA we obtain may have “weird” arc weights drawn from the extended semiring.
"But the arc weights combine along paths to produce the original au-tomaton’s outputs, which fall in the original semiring."
"Let us apply this trick to the example of section 2, yielding the following pushed automaton in which F 1 = F 3 as desired. (x −1 , y −1 , . . . are written as X, Y, . . . , and λ(F q ) is displayed at each q.) a:ε 1 a:ε yz b:ZYzzz 2 z b:ε :xyz a:ε0 b:ZYXwwyz 5xyz 3 b:ε ε wwyz b:ZYzzz ε4"
"For example, the z −1 y −1 zzz output on the 3→4 arc was computed as λ(F 3 ) −1 ⊗ wwzzz ⊗ λ(F 4 ) = (wwyz) −1 ⊗ wwzzz ⊗ ε = z −1 y −1 w −1 w −1 wwzzz."
"This trick yields new algorithms for the tropical semir-ing and sequential transducers, which is interesting and perhaps worthwhile."
How do they compare with previ-ous work?
"Over the tropical semiring, our linear-time pushing al-gorithm is simpler than[REF_CITE], and faster by a log factor, because it does not require a priority queue. (Though this does not help the overall complexity of min-imization, which is dominated by the merging step.)"
"We also have no need to implement faster algorithms for spe-cial cases, as Mohri proposes, because our basic algo-rithm is already linear."
"Finally, our algorithm generalizes better, as it can handle negative weight cycles in the input."
These are useful in (e.g.) conditional random fields.
"On the other hand, Mohri’s algorithm guarantees a po-tentially useful property that we do not: that the weight of the prefix path reading α ∈ Σ ∗ is the minimum weight of all paths with prefix α."
"Commonly this approximates − log(p(most probable string with prefix α)), perhaps a useful value to look up for pruning."
"As for transducers, how does our minimization algo-rithm (above) compare with previous ones?"
"Following earlier work by Choffrut and others,[REF_CITE]de-fines λ(F q ) as the longest common prefix of range(F q )."
"He constrains these values with a set of simultaneous equations, and solves them by repeated changes of vari-able using a complex relaxation algorithm."
His imple-mentation uses various techniques (including a trie and a graph decomposition) to make pushing run in time O(|states| + |arcs| · max q |λ(F q )|). [Footnote_23][REF_CITE]gives a different computation of the same result.
23 We define |ε| = 1 to simplify the O(· · ·) expressions.
"To implement our simpler algorithm, we represent strings in ∆ ∗ as pointers into a global trie that extends upon lookup."
The strings are actually stored reversed in the trie so that it is fast to add and remove short pre-fixes.
"Over the extended alphabet, we use the pointer pair (k, m) to represent the string k −1 m where k, m ∈ ∆ ∗ have no common prefix."
Such pointer pairs can be equality-tested in O(1) time during merging.
"For k, m ∈ ∆ ∗ , k ⊗ m is computed in time O(|k|), and k\m in time O(| LCP (k,m)|) or more loosely O(|k|) (where LCP = longest common prefix)."
"The total time to compute our λ(F q ) values is therefore O(|states| + t · |arcs|), where t is the maximum length of any arc’s weight."
For each arc we then compute a new weight as a left-quotient by a λ value.
So our total run-time for pushing is O(|states| + |arcs| · max q |λ(F q )|).
"This may appear identical to Mohri’s runtime, but in fact our |λ(F q )| ≥"
"Mohri’s, though the two definitions share a worst case of t · |states|. [Footnote_24]"
"24 The |λ(F q )| term contributed by a given arc from q is a bound on the length of the LCP of the outputs of certain paths from q. Mohri uses all paths from q and we use just two, so our LCP is sometimes longer. However, both LCP s probably tend to be short in practice, especially if one bypasses LCP (k, k) with special handling for k\k = ε."
"Inverse letters must be eliminated from the minimized transducer if one wishes to pass it to any specialized al-gorithms (composition, inversion) that assume weights in ∆ ∗ ."
Fortunately this is not hard.
"If state q of the result was formed by merging states q 1 ,...q j , define ρ(q) ="
"LCS {λ(F q i ) : i = 1, . . . j} ∈ ∆ ∗ (where LCS = longest common suffix)."
Now push the minimized trans-ducer using ρ(q) −1 in place of λ(F q ) for all q.
"This cor-rects for “overpushing”: any letters ρ(q) that were unnec-essarily pushed back before minimization are pushed for-ward again, cancelling the inverse letters."
"In our running example, state 0 will push (xyz) −1 back and the merged state {1,3} will push (yz) −1 back."
"This is equivalent to pushing ρ(0) = xyz forward through state 0 and the yz part of it forward through {1,3}, canceling the z −1 y −1 at the start of one of the next arcs."
We must show that the resulting labels really are free of inverse letters.
Their values are as if the original push-ing had pushed back not λ(F q i ) ∈ ∆ ∗ but only its shorter def prefix λ̂(q i ) = λ(F q i )/ρ(q i ) ∈ ∆ ∗ (note the right quo-tient).
"In other words, an arc from q i to r i 0 with weight k ∈ ∆ ∗ was reweighted as λ̂(q i )\(k ⊗ λ̂(r i 0 ))."
Any in-verse letters in such new weights clearly fall at the left.
So suppose the new weight on the arc from q to r begins with an inverse letter z −1 .
"Then λ̂(q i ) must have ended with z for each i = 1, . . . j."
"But then ρ(q i ) was not the longest common suffix: zρ(q i ) is a longer one, a contra-diction (Q.E.D.)."
"Negative weights can be similarly eliminated after minimization over the tropical semiring, if desired, by substituting min for LCS ."
The optional elimination of inverse letters or nega-tive weights does not affect the asymptotic runtime.
A caveat here is that the resulting automaton no longer has a canonical form.
"Consider a straight-line automaton: pushing yields a canonical form as always, but inverse-letter elimination completely undoes pushing (λ̂(q i ) = ε)."
This is not an issue in Mohri’s approach.
"We have characterized the semirings over which weighted deterministic automata can be minimized (sec-tion 4), and shown how to perform such minimization in both general and specific cases (sections 5, 6, 7)."
"Our technique for division semirings and their subsemirings pushes back, at each state q, the output of a single, easily found, shortest accepting path from q."
This is simpler and more general than previous approaches that aggregate all accepting paths from q.
"Our new algorithm (section 6) is most important for previously unminimizable, practically needed division semirings: real (e.g., for probabilities), expectation (for learning[REF_CITE]), and additive with negative weights (for conditional random fields[REF_CITE])."
"It can also be used in non-division semirings, as for transducers."
"It is unpatented, easy to implement, comparable or faster in asymptotic runtime, and perhaps faster in practice (especially for the tropical semiring, where it seems preferable in most respects)."
Our approach applies also to R-weighted sequential transducers as[REF_CITE].
"Such automata can be regarded as weighted by the product semiring (R × ∆ ∗ , (+, min), (×, concat))."
"Equivalently, one can push the numeric and string components independently."
Our new pushing algorithm enables not only minimiza-tion but also equivalence-testing in more weight semir-ings.
Equivalence is efficiently tested by pushing the (de-terministic) automata to canonicalize their arc labels and then testing unweighted equivalence[REF_CITE].
We present improvements to a greedy decod-ing algorithm for statistical machine translation cubic (that reduce  its time complexity from at leastwhen applied naı̈vely) to prac-tically linear time [Footnote_1] without sacrificing trans-lation quality.
"1 Technically, the complexity is still . However, the quadratic component has such a small coefficient that it does not have any noticable effect on the translation speed for all reasonable inputs."
"We achieve this by integrat-ing hypothesis evaluation into hypothesis cre-ation, tiling improvements over the translation hypothesis at the end of each search iteration, and by imposing restrictions on the amount of word reordering during decoding."
Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s ([REF_CITE]1993;[REF_CITE]1996).
"Based on the conventions established[REF_CITE], these models are commonly referred to as the (IBM) Models 1-5."
One of the big challenges in building actual MT sys-tems within this framework is that of decoding: finding the translation candidate that maximizes the translation probability for the given input .
"Due to the complexity of the task, practical MT sys-tems usually do not employ optimal decoders (that is, decoders that are guaranteed to find an optimal solution within the constraints of the framework), but rely on ap-proximative algorithms instead."
Empirical evidence sug-gests that such algorithms can perform resonably well.
"For example,[REF_CITE], attribute only 5% of the translation errors of their Candide system, which uses a restricted stack search, to search errors."
"Using the same evaluation metric (but different evaluation data),[REF_CITE]report search error rates of 7.9% and 9.3%, respectively, for their decoders."
"Och et al. report word error rates of 68.68% for optimal search (based on a vari-ant of the A* algorithm), and 69.65% for the most re-stricted version of a decoder that combines dynamic pro-gramming with a beam search[REF_CITE]."
Their overall performance metric is the sentence error rate (SER).
"For decoding with IBM Model 3, they report SERs of about 57% (6-word sentences) and 76% (8-word sentences) for optimal decoding, 58% and 75% for stack decoding, and 60% and 75% for greedy decoding, which is the focus of this paper."
All these numbers suggest that approximative algo-rithms are a feasible choice for practical applications.
The purpose of this paper is to describe speed improve-ments to the greedy decoder mentioned above.
"While ac-ceptably fast for the kind of evaluation used[REF_CITE], namely sentences of up to 20 words, its speed becomes an issue for more realistic applications."
"Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation[REF_CITE](878 segments; ca. 25k tokens) requires, without any of the improvements described in this paper, over 440 CPU hours, using the simpler, “faster” algorithm  (de-scribed below)."
We will show that this time can be re-duced to ca. 40 minutes without sacrificing translation quality.
"In the following, we first describe the underlying IBM model(s) of machine translation (Section 2) and our hill-climbing algorithm (Section 3)."
"In Section 4, we discuss improvements to the algorithm and its implementation, and the effect of restrictions on word reordering."
Exploiting Bayes’ theorem    (1) they recast the problem of finding the best translation for a given input as    (2) is constant for any given input and can therefore be ignored. is typically calculated using an n-gram language model.
"For the sake of simplicity, we assume here and everywhere else in the paper that the ultimate task is to translate from a foreign language into English."
"The model pictures the conversion from English to a foreign language roughly as follows (cf. Fig. 1; note that because of the noisy channel approach, the modeling is “backwards”)."
"For each English word  , a fertility (with  ) is chosen. is called the fertility of ."
Each word is replaced by foreign words.
"After that, the linear order of the foreign words is rearranged."
"Finally, a certain number of so-called spurious words (words that have no counterpart in the origi-nal English) are inserted into the foreign text."
The probability of the value of depends on the length of the original English string.
"As a result, each foreign word is linked, by virtue of the derivation history, to either nothing (the imaginary NULL word), or exactly one word of the English source sen- / 1020202 The ! ! triple ( , #  4/ !2010205! 6 ( , withand &amp;  2020108! +! 9;-* :=., ,&lt; ! &quot;!$#%!&apos;)&amp; ( tence. ?@ ! ) (7 &gt; ! ! 2020108!A@ : is called &amp;   a sentence @ alignment."
"For all pairs such that , we say that is aligned with 6 , and 46 with , respectively."
"Since each of the changes occurs with a certain prob-ability, we can calculate the translation model probabil-ity of as the product of the individual probabilities of each of the changes."
The product of the translation model probability and the language model probability of is called the alignment probability of .
Detailed formulas for the calculation of alignment probabilities according to the various models can be found[REF_CITE].
It should be noted here that the calculation  of FHG  the F alignment probability of an entire alignment ( ) has linear complexity.
"Well will show below  that  by re-evaluating only fractions of an alignment ( ), we can reduce the evaluation cost to a constant time factor."
The task of the decoder is to revert the process just de-scribed.
In this subsection we recapitulate the greedy hill-climbing algorithm presented[REF_CITE].
"In contrast to all other decoders mentioned in Sec. 1, this algorithm does not process the input one word at a time to incrementally build up a full translation hypothe-sis."
"Instead, it starts out with a complete gloss of the input sentence, aligning each input word with the word that maximizes the inverse (with respect to the noisy chan-nel approach) translation probability N . (Note that for the calculation of the alignment probability, N is used.)"
"The decoder then systematically tries out various types of changes to the alignment: changing the translation of a word, inserting extra words, reordering words, etc."
These change operations are described in more detail below.
"In each search iteration, the algorithm makes a complete pass over the alignment, evaluating all possible changes."
"The simpler, “faster” version  of the algorithm consid-ers only one operation at a time."
"A more thorough variant applies up to two word translation changes, or inserts one zero fertility word in addition to a word translation change before the effect of these changes is evaluated."
"At the end of the iteration, the decoder permanently ap-plies that change, or, in the case of , change combina-tion, that leads to the biggest improvement in alignment probability, and then starts the next iteration."
This cycle is repeated until no more improvements can be found.
The changes to the alignment that the decoder consid-ers are as follows.
"CHANGE the translation of a word: For a given for-eign word , change the English word that is aligned with ."
"If has a fertility of 1, replace it with the new word  ; if it has a fertility of more than one, insert the new word  in the position that optimizes the alignment probability."
The list of candidates for  is derived from the inverse translation table ( N ).
"Typically, the top ten words on that list are considered, that is, for an input of length , 1&gt; possible change operations are evaluated during each CHANGE iteration. plexity ofIn theory  , a single CHANGE iteration in  has a com-: for each word , there is a certain prob-ability that changing the word translation of requires a pass over the complete English hypothesis in order to find the best insertion point."
"This is the case when is currently either spurious (that is, aligned with the NULL word), or aligned with a word with a fertility of more than one."
"The probability of this happening, however, is fairly small, so that we can assume for all practical pur-poses  that a CHANGE iteration in  has a complexity of allows up to two CHANGE operations   ."
"Since at a time, the respective   complexities for are in theory and in practice."
"We will argue below that by exploiting the notion of change dependencies, the  complexity for CHANGE can be reduced to practically for decoding as well, albeit with a fairly large coefficient."
"INSERT a so-called zero fertility word (i.e., an English word that is not aligned to any foreign word) into the En-hypothesis have to be considered,glish string."
"Since all possible positions in the English  , assuming a linear correlation between input length and hypothesis length."
ERASE a zero fertility word. .
JOIN two English J words.
"This is an asymmetrical op-eration $G  G : one word,  , stays where it is, the other one, , is removed from the English hypothesis G$  G  are. thenAll aligned withforeign words  J originally  . aligned with  Even  though a JOIN iteration has a complexity of, [Footnote_2] empirical data indicates that its actual time con-sumption is very small (cf. Fig. 6)."
2 There are 6587 possible join operations for an English string consisting of non-zero-fertility words.
This is because the chances of success of a join operation can be deter-eration.
"Suppose for the sake of simplicity thatmined very cheaply without actually performing the G  G op-  bility N is aligned with  J  onlyis zeroone word(which.isIftruethe mosttranslationof theproba-time), the resulting alignment probability will be zero."
F 02020 SWAP any two non-overlapping regions in the English string.
The number of possible swap operations in a string of length is   /  / !   &quot;#%$  &amp;  &apos;$   I J J / (  
The total decoding complexity of the search algorithm is the number of search iterations (I) times the number of search steps per search iteration (S) times 0 the evaluation cost per search step (E):
"We now show that the original implementation  43 of the for decoding, andalgorithm has a  complexity of (practically)for decoding, if swap opera-plexity istions are restricted  ."
"With unrestricted swapping, the com-."
"Since our argument is based on some assumptions that cannot be proved formally, we cannot provide  a formal complexity proof.."
"In the original implementation of the algo-rithm, the entire alignment is  evaluated FHG  after each search step (global evaluation, or )."
"Therefore, the eval-uation cost rises linearly with the length of the hypothe-sized alignment: The evaluation requires two passes over the English hypothesis (n-grams for the language model; fertility probabilities) and two passes over the input string (translation and distortion probabilities)."
We assume a length.
"Thus,high correlation  F between $G   input length and the hypothesis. graph shows the average runtimes (  ) of 10 different sample sentences of the respective length with swap op-erations restricted to a maximum swap segment size of 5 and a maximum swap distance of 2.  ."
The original algorithm pursues a highly in-efficient search strategy.
"At the end of each iteration, only the single best improvement is executed; all others, even when independent, are discarded."
"In other words, the al-gorithm needs one search iteration per improvement."
We assume that there is a linear correlation between input length and the number of improvements — an assump-
"Therefore,tion that is supported  by the empirical data in Fig. [Footnote_4]..    (  , restricted swapping)  ( , restricted swapping) (no restrictions on swapping)."
4 Thanks to Daniel Marcu for alerting us to this term in this context.
"The number of search steps per iteration is the sum of the number of search steps for CHANGE, SWAP, JOIN, INSERT, and ERASE."
The  highest order term in this sumis unrestricted SWAP with .
"With  restricted swapping, S has a theoretical complex- (due to JOIN) in  decoding, but the con-ity of tribution of the JOIN operation to overall time consump-tion is so small that it can be ignored for all practical pur-poses  ."
"Therefore, the average complexity  of in practice  - /. - , L and .   the J  total D complexity    of   in practice .  is 3 50 is"
"In decoding, which combines up to two CHANGE operations or one CHANGE operation and  one INSERT operation - /. - L , .  has J  a practical D  complexity .  . of   E ,  so that."
We discuss below how can be reduced to practically linear time for decoding as well.
"Every change to the alignment affects only a few of the individual probabilities that make up the overall align-ment score: the n-gram contexts of those places in the English hypothesis where a change occurs, plus a few translation model probabilities."
We call the — not neces-sarily contiguous — area of an alignment that is affected by a change the change’s local context.
"With respect to an efficient implementation of the greedy search, we can exploit the notion of local con-texts in two ways."
"First, we can limit probability recal-culations to the local context (that is, those probabilities that actually are affected by the respective change), and secondly, we can develop the notion of change dependen-cies: Two changes are independent if their local contexts do not overlap."
"As we will explain below, we can use this notion to devise a scheme of improvement caching and tiling (ICT) that greatly reduces the total number of alignments considered during the search."
"Our argument is that local probability calculations and ICT each  reduce the complexity   of the   algorithm / by prac-with  43 tically , that is, from to ."
"Thus  , the complexity for  decreases from to ."
"If we limit the search space for the second oper-ation (CHANGE or INSERT) in decoding to its lo-cal context, decoding, too, has practically linear com-plexity, even though with a much higher coefficient (cf Fig. 6)."
"The complexity of calculating the alignment probabil-  ity globally (that is, over the entire alignment) is ."
"However, since there  is  a FHG IK constant  upper bound [Footnote_3] on the size of local contexts, needs to be performed only once for the initial gloss, therafter, recalculation FH of G  only F  those probabilities affected by each change ( plexity from) suffices  ."
"3[REF_CITE]with a trigram language model: a swap of two large segments over a large distance affects four points in the English hypothesis, resulting in 7 trigrams, plus four individual distortion probabilities."
This reduces  / the overall decoding com-to with .
"Even though profoundly trivial, this improvement sig-nificantly reduces translation times, especially when im-provements are not tiled (cf. below and Fig. 2)."
"Based on the notions of local contexts and change depen-dencies, we devised the following scheme of improve-ment caching and tiling (ICT): During the search, we keep track of the best possible change affecting each local context. (In practice, we maintain a map that maps from the local context of each change that has been considered to the best change possible that affects exactly @ this con-text.)"
"At the end of the search iteration , we apply a very restricted stack search to find a good tiling of non-overlapping changes, all of which are applied."
The goal of this stack search is to find a tiling that maximizes the overal gain in alignment probability.
Possible improve-ments that overlap with higher-scoring @ $ ones are ignored.
"In the following search iteration , we restrict the search to changes that overlap with changes just applied."
"We can safely assume that there are no improvements to be found that are independent @ of the changes applied at the end of iteration : If there were such improvements, they would have been found in and applied after iteration @ ."
Figure 3 illustrates the procedure.
"We assume that improvements are, on average, evenly distributed over the input text."
"Therefore, we can expect the number of places where improvements can be applied to grow with the input length at the same rate as the num-ber of improvements."
"Without ICT, the number of iter-ations grows linearly with the input length, as shown in Fig. 4."
"With ICT, we can parallelize the improvement process and thus reduce the number of iterations for each search to a constant upper bound, which will be deter-mined by the average ‘improvement density’ of the do-main."
"One exception to this rule should be noted: since the expected number of spurious words (words with no counterpart in English) in the input is a function of the input length, and since all changes in word translation that involve the NULL word are mutually dependent, we should expect to find a very weak effect of this on the number of search iterations."
"Indeed, the scatter diagram in Fig.4 suggests a slight increase in the number of itera-tions as the input length increases. [Footnote_5]"
"5 Another possible explanation for this increase, especially at the left end, is that “improvement clusters” occur rarely enough not to occur at all in shorter sentences."
"At the same time, however, the number of changes con-sidered during each search iteration eventually decreases, because subsequent search iterations are limited to areas where a change was previously performed."
Empirical ev-idence as plotted on the right in Fig. 4 suggests that this effect “neutralizes” the increase in iterations in depen-dence of the input length: the total number of changes considered indeed appears to grow linearly with the in-put length.
"It should be noted that ICT, while it does change the course of the search, primarily avoids re-dundant search steps — it does not necessarily search a smaller search space, but searches it only once."
"The to-tal number of improvements found is roughly the same (15,299[REF_CITE]879 without for the entire test cor-pus with a maximum swap distance of 2 and a maximum swap segment size of 5)."
"With4.3 Restrictions  on Word Reordering, unlimited swapping swapping is by far the biggest consumer of processing time during decoding."
"When translating the Chinese test corpus from the 2002 TIDES MT evaluation [Footnote_6] without any limitations on swap-ping, swapping operations account for over 98% of the total search steps but for less than 5% of the improve-ments; the total translation time (with ICT) is about 34 CPU hours."
6 100 short news texts; 878 text segments; ca. 25K to-kens/words.
"For comparison, translating with a maximum swap segment size of 5 and a maximum swap distance of 2 takes ca. 40 minutes under otherwise unchanged cir-cumstances."
"It should be mentioned that in practice, it is generally not a good idea to run the decoder with without restric-tions on swapping."
"In order to cope with hardware and time limitations, the sentences in the training data are typ-ically limited in length."
"For example, the models used for the experiments reported here were trained on data with a sentence length limit of 40."
Sentence pairs where one of the sentences exceeded this limit were ignored in train-ing.
"Therefore, any swap that involves a distortion greater than that limit will result in the minimal (smoothed) dis-tortion probability and most likely not lead to an improve-ment."
The question is: How much swapping is enough?
Is there any benefit to it at all?
This is an interesting ques-tion since virtually all efficient MT decoders (e.g.[REF_CITE]) impose limits on word reordering.
"In order to determine the effect of swap restrictions on decoder performance, we translated the Chinese test cor-pus 101 times with restrictions on the maximum swap distance (MSD) and the maximum swap segment size (MSSS) ranging from 0 to 10 and evaluated the transla-tions with the BLEU [Footnote_7] metric[REF_CITE]."
"7 In a nutshell, the BLEU score measures the n-gram overlap between system-produced test translations and a set of human reference translations."
The results are plotted in Fig. 5.
"On the one hand, the plot seems to paint a pretty clear picture on the low end: score improvements are compar-atively large initially but level off quickly."
"Furthermore, the slight slope suggests slow but continuous improve-ments as swap restrictions are eased."
"For the Arabic test data from the same evaluation, we obtained a sim-ilar shape (although with a roughly level plateau)."
"On the other hand, the ‘bumpiness’ of the surface raises the question as to which of these differences are statistically significant."
We are aware of several ways to determine the statisti-cal significance of BLEU score differences.
"One is boot-strap resampling[REF_CITE][Footnote_8] to deter-mine confidence intervals, another one splitting the test corpus into a certain number of subcorpora (e.g. 30) and then using the t-test to compare the average scores over these subcorpora (cf."
8 Thanks to Franz Josef Och for pointing this option out to us.
Bootstrap resampling for the various system outputs leads to very similar confidence intervals of about 0.006 to 0.007 for a one-sided test at a confidence level of .95.
"With the t-score method, differences in score of 0.008 or higher seem to be significant at the same level of confidence."
"According to these metrics, none of the differences in the plot are significant, although the shape of the plot sug-gests that moderate swapping probably is a good idea."
"In addition to limitations of the accuracy of the BLEU method itself, variance in the decoders performance can blur the picture."
A third method to determine a confi-dence corridor is therefore to perform several random-ized searches and compare their performance.
"Follow-ing a suggestion by Franz Josef Och (personal commu-nications), we ran the decoder multiple times from ran-domized starting glosses for each sentence and then used the highest scoring one as the “official” system output."
This gives us a lower bound on the price in performance that we pay for search errors.
The results for up to ten searches from randomized starting points in addition to the baseline gloss are given in Tab. 1.
"Starting points were randomized by randomly picking one of the top 10 translation candidates (instead of the top candidate) for each input word, and performing a (small) random num-ber of SWAP and INSERT operations before the actual search started."
"In order to insure consistency across re-peated runs, we used a pseudo random function."
"In our experiments, we did not mix  and decoding."
The practical reason for this is that decoding takes more than ten times as long as  decoding.
"As the table illus-trates, running multiple searches in  from randomized starting points is more efficient that running once."
Choosing the best sentences from all decoder runs results in a BLEU score of 0.157.
"Interestingly, the decoding time from the default starting point is much lower (G1: ca. 40 min. vs. ca. 1 hour; G2: ca. 9.5 hours vs. ca. 11.3 hours), and the score, on average, is higher than when searching from a random starting point (G1: 0.143 vs. 0.127 (average); G2: 0.145 vs. 0.139 (average))."
This indicates that the default seeding strategy is a good one.
From the results of our experiments we conclude the following.
"First, Tab. 1 suggests that there is a good correla-tion between IBM Model 4 scores and the BLEU met-ric."
Higher alignment probabilities lead to higher BLEU scores.
"Even though hardly any of the score differ-ences are statistically significant (see confidence intervals above), there seems to be a trend."
"Secondly, from the swapping experiment we conclude that except for very local word reorderings, neither the IBM models nor the BLEU metric are able to recognize long distance dependencies (such as, for example, ac-counting for fundamental word order differences when translating from a SOV language into a SVO language)."
"This is hardly surprising, since both the language model for decoding and the BLEU metric rely exclusively on n-grams."
This explains why swapping helps so little.
"For a different approach that is based on dependency tree trans-formations, see[REF_CITE]."
"Thirdly, the results of our experiments with random-ized searches show that greedy decoding does not per-form as well on longer sentences as one might conclude from the findings[REF_CITE]."
"At the same time, the speed improvements presented in this paper make multiple searches feasible, allowing for an overall faster and better decoder."
"In this paper, we have analyzed the complexity of the greedy decoding algorithm originally presented[REF_CITE]and presented improvements that dras-tically reduce the decoder’s complexity and speed to practically linear time."
Experimental data suggests a good correlation between sentence length
"Figure 7 and 6: Timedecodingconsumption(with 10 translationsof the variousper changeinput wordtypescon-in sidered, a list of 498 candidates for INSERT, a maximum swap distance of 2 and a maximum swap segment size of 5)."
"The pro-files shown are cumulative, so that the top curve reflects the total decoding time."
"To put the times for decoding in perspective, the dashed line in the lower plot reflects the total decoding time in 7 decoding."
Operations not included in the figures consume so little time that their plots cannot be discerned in the graphs.
"The times shown are averages of 100 sentences each for length 10, 20,  , 80."
IBM Model 4 scores and the BLEU metric.
"The speed improvements discussed in this paper make multiple ran-domized searches per sentence feasible, leading to a faster and better decoder for machine translation with IBM Model 4."
We are very grateful to Franz Josef Och for various very helpful comments on the work reported in this paper.
This work was supported by DARPA-ITO grant[REF_CITE]- 00-1-9814.
"The discovery of semantic relations from text becomes increasingly important for applica-tions such as Question Answering, Informa-tion Extraction, Text Summarization, Text Un-derstanding, and others."
The semantic rela-tions are detected by checking selectional con-straints.
This paper presents a method and its results for learning semantic constraints to de-tect part-whole relations.
Twenty constraints were found.
"Their validity was tested on a 10,000 sentence corpus, and the targeted part-whole relations were detected with an accuracy of 83%."
"An important semantic relation for several NLP applica-tions is the part-whole relation, or meronymy."
Consider the text:
The car’s mail messenger is busy
"There are six part-whole relations in this text: 1) the mail car is part of the train, 2) the side door is part of the car, 3) the keyhole is part of the door, 4) the cab is part of the locomotive, 5) the door is part of the car, and 6) the car is part of the express train (the last two in the compound noun express car door)."
"Understanding part-whole relations allows Question Answering systems to address questions such as “What are the components of X?, What is X made of? and others."
"Question Answering, Information Extraction and Text Summarization systems often need to identify relations between entities as well as synthesize information gath-ered from multiple documents."
More and more knowl-edge intensive techniques are used to augment statistical methods when building advanced NLP applications.
This paper provides a method for deriving semantic constraints necessary to discover part-whole relations.
"There are different ways in which we refer to something as being a part of something else, and this led many re-searchers to claim that meronymy is a complex relation that “should be treated as a collection of relations, not as a single relation”[REF_CITE]."
"Based on linguistic and cognitive considerations about the way parts contribute to the structure of the wholes, Winston, Chaffin and Hermann[REF_CITE]determined in 1987 six types of part-whole rela-tions: Component-Integral object (wheel - car), Member-Collection (soldier - army), Portion-Mass (meter - kilo-meter), Stuff-Object (alcohol - wine), Feature-Activity (paying - shopping), and Place-Area (oasis - desert)."
"The part-whole relations in WordNet are classified into three basic types: Member-of (e.g., UK IS - MEMBER - OF NATO), Stuff-of (e.g., carbon IS - STUFF - OF coal), and all other part-whole relations grouped under the general name of Part-of (e.g., leg IS - PART - OF table)."
"In this paper we lump together all the part-whole relation types, but if necessary, one can train the system separately on each of the six meronymy types to increase the learning accuracy."
"Although part-whole relations were studied by philoso-phers, logicians, psychologists and linguists, not much work has been done to automatically identify the meronymy relation in text."
Hearst[REF_CITE]de-veloped a method for the automatic acquisition of hyper-nymy relations by identifying a set of frequently used and unambiguous lexico-syntactic patterns.
"Then, she tried applying the same method to meronymy, but with-out much success, as the patterns detected also expressed other semantic relations."
"Using Hearst’s method, they fo-cused on a small set of lexico-syntactic patterns that fre-quently refer to meronymy and a list of 6 seeds represent-ing whole objects."
Their system’s output was an ordered list of possible parts according to some statistical metrics.
The accuracy obtained for the first 50 parts was 55%.
"Since there are many ways in which something can be part of something else, there is a variety of lexico-syntactic structures that can express the meronymy se-mantic relation."
Expressions that reflect semantic rela-tions are either explicit or implicit.
The explicit ones are further broken down into unambiguous and ambiguous.
There are unambiguous lexical expressions that always convey a part-whole relation.
The substance consists of two ingre-dients.
The cloud was made of dust.
Iceland is a member of NATO.
In these cases the simple detection of the patterns leads to the discovery of part-whole relations.
"On the other hand, there are many ambiguous expres-sions that are explicit but convey part-whole relations only in some contexts."
These expressions can be detected only with complex semantic constraints.
The horn is part of the car. (whereas ‘‘He is part of the game’’ is not meronymic).
B. Implicit part-whole constructions
"In addition to the explicit patterns, there are other patterns that express part-whole relations implicitly."
"Examples are: girl’s mouth, eyes of the baby, door knob, oxygen-rich water, high heel shoes."
"In order to identify lexical forms that express part-whole relations, the following algorithm was used:  "
"Pick pairs of WordNet concepts , among which there is a part-whole relation ."
Extract lexico-syntactic patterns that link the two selected concepts of each pair by searching a collection of texts.
"For each pair of part-whole concepts determined above, search a collection of documents and retain only the sentences containing that pair."
We chose two dis-tinct text collections: SemCor 1.7 and LA Times from TREC-9.
"From each collection 10,000 sentences were selected randomly."
We manually inspected these sen-tences and picked only those in which the pairs referred to meronymy.
The result of this step is a list of lexico-syntactic ex-pressions that reflect meronymy.
"From syntactic point of view, these patterns can be classified in two major cate-gories:"
"Phrase-level patterns, where the part and whole concepts are included in the same phrase."
"For exam-ple, in the pattern “  ” the noun phrase that contains the part (X) and the prepositional phrase that contains the whole (Y) form a noun phrase (NP)."
"Throughout this paper, X represents the part, and Y represents the whole."
"Sentence-level patterns, where the part-whole rela-tion is intrasentential."
A frequent example is the pat-tern “  verb  ”.
The most frequent phrase-level patterns were: “  of  ” occurring 173 of 493 times or 35%; “  ’s  ” occurring 71 of 493 times or 14%;
The most frequent sentence-level pattern was “  Verb  ” occurring 18 of 42 times (43%).
These observations are consistent with the results[REF_CITE].
"Based on these statistics, we decided to focus in this paper only on the three patterns above."
"The problem, however, is that these are some of the most ambiguous part-whole relation patterns."
"For ex-ample, in addition to meronymic relations, the genitives can express POSSESSION (Mary’s toy), KINSHIP (Mary’s brother), and many other relations."
"The same is true for “  Verb  ” patterns (“Kate has green eyes” is meronymic, while “Kate has a cat” is POSSESSION )."
"As it can be seen, the genitives and the have-verb pat-terns are ambiguous."
Thus we need some semantic con-straints to differentiate the part-whole relations from the other possible meanings these patterns may have.
"The learning procedure proposed here is supervised, for the learning algorithm is provided with a set of in-puts along with the corresponding set of correct outputs."
"Based on a set of positive and negative meronymic train-ing examples provided and annotated by the user, the al-gorithm creates a decision tree and a set of rules that clas-sify new data."
The rules produce constraints on the noun constituents of the lexical patterns.
For the discovery of the semantic constraints we used C4.5 decision tree learning[REF_CITE].
"The learned function is represented by a decision tree, or a set of if-then rules."
The decision tree learning searches a complete hypothesis space from simple to complex hypotheses un-til it finds a hypothesis consistent with the data.
Its bias is a preference for the shorter tree that places high in-formation gain attributes closer to the root.
"The error in the training examples can be overcome by using different training and a test corpora, or by cross-validation tech-niques."
"C4.5 receives in general two input files, the NAMES file defining the names of the attributes, attribute values and classes, and the DATA file containing the examples."
"The output of C4.5 consists of two types of files, the TREE file containing the decision tree and some statis-tics, and the RULES file containing the rules extracted from the decision tree and some statistics for training and test data."
This last file also contains a default rule that is usually used to classify unseen instances when no other rule applies.
"Since our constraint learning procedure is based on the semantic information provided by WordNet, we need to preprocess the noun phrases (NPs) extracted and identify the part and the whole."
For each NP we keep only the largest word sequence (from left to right) that is defined in WordNet as a concept.
"For example, from the noun phrase “brown carving knife” the procedure retains only “carving knife”, as it is the WordNet concept with the largest number of words in the noun phrase."
"For each such concept, we manually annotate it with its corresponding sense in WordNet, for example carving knife 1 means sense number 1."
"In order to learn the constraints, we used the SemCor 1.7 and TREC 9 text collections."
From the first two sets of the[REF_CITE]000 sentences were selected.
A corpus “A” was thus created from the selected sentences of each text collection.
Each sentence in this corpus was then parsed using the syntac-tic parser developed by Charniak[REF_CITE].
"Focusing only on the sentences containing relations in-dicated by the three patterns considered, we manually annotated all the noun phrases in the 53,944 relation-ships matched by these patterns with their correspond-ing senses in WordNet (with the exception of those from SemCor). 6,973 of these relationships were part-whole relations, while 46,971 were not meronymic relations."
"We used for training a corpus of 34,609 positive exam-ples (6,973 pairs of NPs in a part-whole relation extracted from the corpus “A” and 27,636 extracted from WordNet as selected pairs) and 46,971 negative examples (the non-part-whole relations extracted from corpus “A”)."
Input: positive and negative meronymic examples of pairs of concepts.
Output: semantic constraints on concepts.
"Generalize the training examples Initially, the training corpus consists of examples that have the following format: part#sense; whole#sense; target , where target can be either “Yes” or “No”, as the rela-tion between the part and whole is meronymy or not."
"For example, oasis 1; desert 1; Yes in-dicates that between oasis and desert there is a meronymic relation."
"From this initial set of examples an intermediate cor-pus was created by expanding each example using the following format: part#sense, class part#sense; whole#sense, class whole#sense; target , where class part and class whole correspond to the WordNet semantic classes of the part, respec-tively whole concepts."
"For instance, the initial example aria 1; opera 1; Yes be-comes aria 1, entity 1; opera 1, abstraction 6; Yes , as the part concept aria 1 belongs to the entity 1 hierarchy in WordNet and the whole concept opera 1 is part of the abstraction 6 hierarchy."
"From this intermediate corpus a generalized set of training examples was built, retaining only the semantic classes and the target value."
"At this point, the generalized training corpus contains three types of examples: 1."
Positive examples X hierarchy#sense; Y hierarchy#sense;
Negative examples X hierarchy#sense; Y hierarchy#sense; No 3.
X hierarchy#sense; Y hierarchy#sense;
The third situation occurs when the training cor-pus contains both positive and negative examples for the same hierarchy types.
"For example, both rela-tions apartment 1; woman 1; No and hand 1; woman 1; Yes are mapped into the more general type entity 1; entity 1; Yes/No ."
"However, the first example is negative (a POSSESSION relation), while the second one is a positive example."
Table 1 summarizes the constraints learned by the pro-gram.
"The meaning of a constraint with the part Class-X, the whole Class-Y and the value 1 is “if Part is a Class-X and Whole is a Class-Y then it is a part-whole relation” and for the value 0 is “if Part is a Class-X and Whole is a Class-Y then it is not a part-whole relation”."
"For exam-ple, “if Part is an entity  and the Whole is a whole 2 then it is not a part-whole relation”. (whole 2 is the WordNet concept meaning “an assemblage of parts that is regarded as a single entity”)."
"When forming larger, more complex rules, if the part and the whole contain more then one value, one of these values is negated (preceded by !)."
For example for the part object 1 and the whole organism 1 the constraint is “if the Part is object 1 and not substance 1 and not natural object 1 and the Whole is organism 1 and not plant 2 and not animal 1 then
NO part-whole rela-tion”.
"To validate the constraints for extracting part-whole re-lations, a new test corpus “B” was created from other 10,000 sentences of TREC-9 LA Times news articles."
This corpus was parsed and disambiguated using a Word Sense Disambiguation system that has an accuracy of 81 when disambiguating nouns in open-doma[REF_CITE].
The results provided by the part-whole relation discovery procedure were validated by a human annotator.
"Let us define the precision and recall performance met-rics in this context:   .&amp;!!&quot;+* ,$#/$&amp;% ,()$%&amp;!*+( ,$ *  &amp;! #$&amp;%!*+&amp;,)( $* ,"
On the test corpus there were 119 meronymy relations expressed by the three patterns considered.
"The system retrieved 140 relations, of which 117 were meronymy re-lations and 23 were non-meronymy relations, yielding a precision of 83% and a recall of 98%."
Table 2 shows the results obtained for each of the three patterns and for all of them.
"However, there were other 43 manner relations found in the corpus, expressed by other than the three lexico-syntactic patterns considered in this paper, yielding a global meronymy relation coverage (recall) of 72 . [117/119+43]"
The errors are explained mostly by the fact that the genitives and the verb have are very ambiguous.
These lexico-syntactic patterns encode numerous rela-tions which are very difficult to disambiguate based only on the nouns they connect.
"The errors were also caused by the incorrect parsing of a few s-genitives, the use of the rules with smaller accuracy (e.g. 50%), the wrong word sense disambiguation of some concepts, and the lack of named entity recognition in WordNet (e.g., proper names of persons, places, etc.)."
The part-whole semantic relation occurs with high fre-quency in open text.
Its discovery is paramount for many applications.
In this section we mention only Question Answering.
"For many questions such as “What parts does General Electric manufacture?”, “What are the compo-nents of X?”, “What is Y made of?”, etc., the discovery of part-whole relations is necessary to assemble the right answer."
The concepts and part-whole relations acquired from a collection of documents can be useful in answering difficult questions that normally cannot be handled based solely on keywords matching and proximity.
"As the level of difficulty increases, Question Answering systems need richer semantic resources, including ontologies and larger knowledge bases."
Consider the question:
What does the AH-64A Apache helicopter consist of?
"For questions like this, the system must extract all the components the war helicopter has."
"Unless an ontology of such army attack helicopter parts exists in the knowl- edge base, which in an open domain situation is highly unlikely, the system must first acquire from the docu-ment collection all the direct and indirect pieces the he-licopter is made of."
"These parts can be scattered all over the text collection, so the Question Answering system has to gather together these partial answers into a single and concise hierarchy of parts."
This technique is called An-swer Fusion.
"Using a state-of-the-art Question Answering system[REF_CITE]adapted for Answer Fusi[REF_CITE]and including a meronymy module, the question presented above was answered by searching the Internet at the website for Defence Industries - Army[URL_CITE]"
The system started with the question focus AH-64A Apache helicopter and extracted and disambiguated all the meronymy relations using the part-whole module.
The following taxonomic ontology was created for this question:
AH-64A Apache Helicopter Helfire air-to-surface missile millimetre wave seeker 70mm Folding F[REF_CITE]m Cannon camera armaments
"For example, the relation “AH-64A Apache helicopter has part Hellfire air-to-surface missile” was determined from the sentence “ AH-64A Apache helicopter has a Longbow-millimeter wave fire control radar and a Hell-fire air-to-surface missile”."
"For validation only the heads of the noun phrases were considered as they occur in WordNet (i.e., helicopter and air-to-surface missile, re-spectively)."
The method presented in this paper for the detection and validation of part-whole relation is semi-automatic and has a better accuracy than the previous attempts[REF_CITE].
It discovers semi-automatically the part-whole lexico-syntactic patterns and learns (automat-ically) the semantic constraints needed for the disam-biguation of these generally applicable patterns.
We combined the results of the decision tree learning with an IS-A hierarchy (the WordNet IS-A relation) spe-cialization for a more accurate learning.
The method presented in this paper can be generalized to discover other semantic relations.
The only part-whole elements used in this algorithm were the patterns and the training examples.
"Thus the learning procedure and the validation procedure are generally applicable and we in-tend to use the method for the detection of other semantic relations such as manner, influence, and others."
The in-convenience of the method is that for a very precise learn-ing the number of examples (both positive and negative) should be very large.
We also intend to automate the detection of lexico- syntactic patterns and to discover constraints for all the part-whole patterns.
"In this paper we present O NTO S CORE , a sys-tem for scoring sets of concepts on the basis of an ontology."
We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence.
We conducted an annotation exper-iment and showed that human annotators can reliably differentiate between semantically co-herent and incoherent speech recognition hy-potheses.
"An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%)."
"Since controlled and restricted interactions between the user and the system increase recognition and understand-ing accuracy, such systems are reliable enough to be de-ployed in various real world applications, e.g. public transportation or cinema information systems."
"The more conversational a dialogue system becomes, the less pre-dictable are the users’ utterances."
Recognition and pro-cessing become increasingly difficult and unreliable.
"Today’s dialogue systems employ domain- and discourse-specific knowledge bases, so-called ontologies, to represent the individual discourse entities as concepts, and their relations to each other."
In this paper we present an algorithm for measuring the semantic coherence of sets of concepts against such an ontology.
"In the fol-lowing, we will show how the semantic coherence mea-surement can be applied to estimate how well a given speech recognition hypothesis (SRH) fits with respect to the existing knowledge representation, thereby providing a mechanism that increases the robustness and reliability of dialogue systems."
In Section 2 we discuss the problem of scoring and classifying SRHs in terms of their semantic coherence followed by a description of our annotation experiment.
Section 3 contains a description of the kind of knowledge representations employed by O NTO S CORE .
"We present the algorithm in Section 4, and an evaluation of the cor-responding system for scoring SRHs is given in Section 5."
A conclusion and additional applications are given in Section 6.
"While a simple one-best hypothesis interface between au-tomatic speech recognition (ASR) and natural language understanding (NLU) suffices for restricted dialogue sys-tems, more complex systems either operate on n-best lists as ASR) output or convert ASR word graphs[REF_CITE]into n-best lists, given the distribution of acoustic and language model scores[REF_CITE]."
"For example, in our data a user expressed the wish to see a specific city map again, as: [Footnote_1] (1) Ich würde die Karte gerne wiedersehen"
1 All examples are displayed with the German original on top and a glossed translation below.
I would the map like to see again
"Looking at two SRHs from the ensuing n-best list we found that Example (1a) constituted a suitable represen-tation of the utterance, whereas Example (1b) constituted a less adequate representation thereof. (1a) Ich würde die Karte eine wieder sehen I would the map one again see (1b) Ich würde die Karte eine Wiedersehen I would the map one Good Bye"
"Facing multiple representations of a single utterance consequently poses the question, which of the different hypotheses corresponds most likely to the user’s utter-ance."
Several ways of solving this problem have been proposed and implemented in various systems.
"Fre-quently the scores provided by the ASR system itself are used, e.g. acoustic and language model probabilities."
"More recently also scores provided by the NLU system have been employed, e.g. parsing scores or discourse scores[REF_CITE]."
"However, these methods assign higher scores to SRHs which are semantically incoher-ent and lower scores to semantically coherent ones and disagree with other."
"For instance, the acoustic and language model scores of Example (1b) are actually better than for Example (1a), which results from the fact that the frequencies and cor-responding probabilities for important expressions, such as Good Bye, are rather high, thereby ensuring their reli-able recognition."
Another phenomenon found in our data consists of hypotheses such as: (2) Zeige mir alle Vergnügen Show me all pleasures (3) Zeige mir alle Filmen Show me all Films
"In these cases language model scores are higher for Ex-ample (2) than Example (3), as the incorrect inflection on alle Filmen was less frequent in the training material than that of the correct inflection on alle Vergnügen."
Our data also shows - as one would intuitively ex-pect - that the understanding-based scores generally re-flect how well a given SRH is covered by the grammar employed.
In many less well-formed cases these scores do not correspond to the correctness of the SRH.
"Gener-ally we find instances where all existing scoring methods disagree with each other, diverge from the actual word er-ror rate and ignore the semantic coherence. [Footnote_2] Neither of the aforementioned approaches systematically employs the system’s knowledge of the domains at hand."
"2 As the numbers evident from large vocabulary speech recognition performance[REF_CITE], the occurrence of less well formed and incoherent SRHs increases the more con-versational a system becomes."
This in-creases the number of times where a suboptimal recogni-tion hypothesis is passed through the system.
"This means that, while there was a better representation of the actual utterance in the n-best list, the NLU system is processing an inferior one, thereby causing overall dialogue metrics, in the sense[REF_CITE], to decrease."
We pro-pose an alternative way to rank SRHs on the basis of their semantic coherence with respect to a given ontology rep-resenting the domains of the system.
"In a previous study[REF_CITE], we tested if human annotators could reliably classify SRHs in terms of their semantic coherence."
The task of the annotators was to determine whether a given hypothesis representsa n internally coherent utterance or not.
"In order to test the reliability of such annotations, we collected a corpus of SRHs."
The data collection was con-ducted by means of a hidden operator test.
"Each user-turn in the dialogue corresponded to a single intention, e.g. a route request or a sight information request."
"The audio files were then sent to the speech recognizer and the input to the seman-tic coherence scoring module, i.e. n-best lists of SRHs were recorded in log-files."
The final corpus consisted of 2.284 SRHs.
All hypotheses were then randomly mixed to avoid contextual influences and given to separate an-notators.
"The resulting Kappa statistics[REF_CITE]over the annotated data yields  , which seems to indicate that human annotators can reliably distinguish between coherent samples (as in Example (1a)) and inco-herent ones (as in Example (1b))."
"The aim of the work presented here, then, was to pro-vide a knowledge-based score, that can be employed by any NLU system to select the best hypothesis from a given n-best list."
"O NTO S CORE , the resulting system will be described below, followed by its evaluation against the human gold standard."
"In this section, we provide a description of the pre-existing knowledge source employed by O NTO S CORE , as far as it is necessary to understand the empirical data generated by the system."
It is important to note that the ontology employed in this evaluation existed already and was crafted as a general knowledge representation for various processing modules within the system. [Footnote_3]
"3 Alternative knowledge representations, such as W ORD - N ET , could have been employed in theory as well, however most of the modern domains of the system, e.g. electronic me-dia or program guides, are not covered by W ORD N ET ."
"Ontologies have traditionally been used to represent general and domain specific knowledge and are employed for various natural language understanding tasks, e.g. se-mantic interpretati[REF_CITE]."
"We propose an addi-tional way of employing ontologies, i.e. to use the knowl-edge modeled therein as the basis for evaluating the se-mantic coherence of sets of concepts."
"The system described herein can be employed indepen-dently of the specific ontology language used, as the un-derlying algorithm operates only on the nodes and named edges of the directed graph represented by the ontology."
"The specific knowledge base, e.g. written in DAML+OIL or OWL, [Footnote_4] is converted into a graph, consisting of: the class hierarchy, with each class corresponding to a concept representing either an entity or a process; the slots, i.e. the named edges of the graph corre-sponding to the class properties, constraints and re-strictions."
4 DAML+OIL and OWL are frequently used knowledge modeling languages originating in W3C and Semantic Web
The ontology employed herein has about 730 concepts and 200 relations.
"It includes a generic top-level ontol-ogy whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in dis-tinct parts as resulting from the ontological analysis."
The top-level was developed following the procedure outlined[REF_CITE].
"In the view of the ontology employed herein, Role is the most general class in the ontology and represents a role that any entity or process can perform."
It is di-vided into Event and Abstract Event.
"Event is used to describe a kind of role any entity or process may have in a ”real” situation or process, e.g. a building or an information search."
"It is contrasted with Abstract Event, which is abstracted from a set of situations and processes."
"It reflects no reality and is used for the gen-eral categorization and description, e.g. Number, Set, Spatial Relation."
There are two kinds of events: Physical Object and Process.
The class Physical Object describes any kind of objects we come in contact with - living as well as non-living - having a location in space and time in contrast to abstract objects.
"These objects refer to different domains, such as Sight and Route in the tourism domain, Av Medium and Actor in the TV and cinema domain, etc., and can be associated with certain relations in the pro-cesses via slot constraint definitions."
"The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the F RAME N ET data[REF_CITE]."
"Currently, there are four groups of processes (see Figure 1):"
"General Process, a set of the most general pro-cesses such as duplication, imitation or repetition processes;"
"Mental Process, a set of processes such as cog-nitive, emotional or perceptual processes;"
"Physical Process, a set of processes such as motion, transaction or controlling processes;"
"Social Process, a set of processes such as communication or instruction processes."
Let us consider the definition of the Information Search Process in the ontology.
"It is modeled as a subclass of the Cognitive Process, which is a sub-class of the Mental Process and inherits the follow-ing slot constraints: begin time, a time expression indicating the starting time point; end time, a time expression indicating the time point when the process is complete; state, one of the abstract process states, e.g. start, continue, interrupt, etc.; cognizer, filled with a class Person including its subclasses."
"Information Search Process features one ad-ditional slot constraint, piece-of-information."
"The possi-ble slot-fillers are a range of domain objects, e.g. Sight, Performance, or whole sets of those, e.g. Tv Program, but also processes, e.g. Controlling Tv Device Process."
"This way, an utterance such as: (4) I hätte gerne Informationen zum Schloss I would like information about castle can also be mapped onto Information Search Process, which has an agent of type User and a piece of information of type Sight."
Sight has a name of type Castle.
"Analogously, the utterance: (5) Wie kann ich den Fernseher steuern How can I the TV control can be mapped onto Information Search Process, which has an agent of type User and has a piece of information of type Controlling Tv Device Process."
"O NTO S CORE performs a number of processing steps, each of them will be described separately in the respec-tive subsections."
A necessary preprocessing step is to convert each SRH into a concept representation (CR).
For that purpose we augmented the system’s lexicon with specific concept mappings.
"That is, for each entry in the lexicon either zero, one or many corresponding concepts where added."
"A simple vector of the concepts, corresponding to the words in the SRH for which concepts in the lexicon exist, constitutes the resulting CR."
"All other words with empty concept mappings, e.g. articles, are ignored in the con-version."
"O NTO S CORE converts the domain model, i.e. an ontol-ogy, into a directed graph with concepts as nodes and re-lations as edges."
"One additional problem that needed to be solved lies in the fact that the directed subclass-of rela-tions enable path algorithms to ascend the class hierarchy upwards, but do not let them descend, therefore missing a significant set of possible paths."
"In order to remedy that situation the graph was enriched during its conversion by corresponding parent-of relations, which eliminated the directionality problems as well as avoids cycles and 0-paths."
"In order to find the shortest path between two con-cepts, O NTO S CORE employs the single source shortest path algorithm of Dijkstra (Cormen et al  ., 1990)."
"Given a concept representation CR , ..., , the algorithm runs once for each concept."
The Dijkstra algo-rithm calculates minimal paths from a source node to all other nodes.
"Then, the minimal paths connecting a given concept with every other concept in CR (excluding itself) are selected, resulting in an matrix of the respective paths."
"To score the minimal paths connecting all concepts with each other in a given CR, we first adopted a method pro- posed[REF_CITE]to score the se-mantic coherence of alternative sentence interpretations against graphs based on the Longman Dictionary of Con-temporary English (LDOCE)."
"To construct the graph the dictionary lemmata were represented as nodes in an isa hierarchy and their semantic relations were represented as edges, which were extracted automatically from the LDOCE."
"As  defined[REF_CITE],is the set of direct relations (both isa cepts); andand semantic relations   ) that  can  connect two nodes (con-is the set of corre-sponding weights, where the weight of each isa relation is set to and that  of each other  relation    to  ."
"For eachtwo concepts , the set denotes the scores of all possible paths  that &amp;(&apos; link the two concepts."
"The score for path #! &quot;$! % can be given as: *) +  -,./ where / represents the number of times the relation exists in path ! ."
"The ensuing distance between two con-cepts and is, then, defined as the minimum score derived between and , i.e.:  &apos; &amp; &quot; ) ! &amp; &quot;"
"The algorithm selects from the set of all paths between two concepts the one with the smallest weight, i.e. the cheapest."
The distances between all concept pairs in CR are summed up to a total score.
The set of concepts with the lowest aggregate score represents the combina-tion with the highest semantic relatedness.
"Also, their algorithm only allows for a relative judgment stating which of a set of interpretations given a single sentence is more seman-tically related."
"Since our objective is to compute semantic coherence scores of arbitrary CRs on an absolute scale, certain ex-tensions are necessary."
"In this application, the CRs to be scored can differ in terms of their content, the num-ber of concepts contained therein and their mappings to the original SRH."
"Moreover, in order to achieve absolute values, the final score should be related to the number of concepts in an individual set and the number of words in the original SRH."
"Therefore, the results must be normal-ized in order to allow for evaluation, comparability and clearer interpretation of the semantic coherence scores."
We modified the algorithm described above to make it applicable and evaluatable with respect to the task at hand as well as other possible tasks.
The basic idea is to calculate a score based on the path distances in .
"Since short distances indicate coherence and many con-cept pairs in a given may have no connecting path, we define the distance between two concepts and that are only  connected via isa relations in the knowledgebase as ."
This maximum value can also serve as a maximum for long distances and can thus help to prune the search tree for long paths.
This constant has to be set according to the structure of the knowledge base.
"For example, employing the ontology described above, the maximum distance between two concepts  does not ex-ceed ten and we chose in that case ."
"We can now define the semantic coherence score for as the average path length between all concept pairs in :   ,  &quot ;  &apos; &apos; &quot;"
"Since the ontology is a directed graph, we have pairs of concepts with possible  directed connections, i.e., a path from concept to concept  may be completely different to that from to or even be missing."
"As a symmetric alternative, we may want to consider a path from to and a path from to to be semantically equivalent and thus model every relation in a bidirectional ! &apos; way."
We can then compute a symmetric score &quot; as:   &quot ; $#  &amp;  &quot; &quot;  &apos; &quot; &quot; &apos; &apos; &apos; &quot;
O NTO S CORE implements both options.
"In the ontol-ogy currently employed by the system  some reverse re-lations can be found, e.g. given =Broadcast  and =Channel, there exists a path from to via the relation has-channel and a different path from to via the relation has-broadcast."
"However, such reverse relations are only sporadically represented in the ontol-ogy."
"Consequently &apos; , it is difficult to account for their in-fluence ! &quot; &apos; on &quot; in general."
"That is why we chose the &apos; function for the evaluation, i.e. only the best path &quot;  between a given pair of concepts, regardless of the direction, is taken into account."
"Given the algorithm proposed above, a significant num-ber of misclassifications for SRHs would result from the cases when an SRH contains a high proportion of func-tion words (having no conceptual mappings in the result-ing CR) and only a few content words."
Let’s consider the following example: (6) Wo den Informationen zu das gleiche Where the information to the same
The corresponding CR is constituted out of a single concept Information Search Process.
"O N TO S CORE would classify the CR as coherent with the highest possible score, as this is the only concept in the set."
"This, however, would often lead to misclassifications."
"We, therefore, included a post-processing technique that takes the relation between the number of ontology con-cepts % in a given CR and the total number of words %&apos;&amp; in the original SRH into * %+&amp; account."
This relation is defined by the ratio ( )%.
"O NTO S CORE automatically classifies an SRH as being incoherent irrespective of its semantic coherence score, if ( is less then the threshold set."
The threshold may be set freely.
The corresponding findings are presented in the evaluation section.
"Looking at an example of O NTO S CORE at work, we will examine the utterance given in Example (1)."
"The resulting two SRHs - -, and -, - are given in Example (1a) and (1b) respectively."
"The human annota-tors considered . , to be coherent and labeled ., as incoherent."
"According to the concept entries in the lexicon, the SRHs are transformed into two alternative concept representations."
"As no ambiguous words are corresponds to -, : corresponds to -, found in this example, and : Person; Map; Watch Perceptual Process ; : Person; Map; Parting Process ."
They are converted into a graph.
"According to the algo-rithm shown in Section 4.3, all paths between the con-yields the following non-cepts of each graph are calculated  and weighted."
Thispaths: & apos; &quot;   %     via the relation has-watcher;  &apos; &quot;   %     via the relation has-watchable object. &apos; &quot;      via the relation has-agent;
The ensuing results are:
According !! &quot; &apos;&apos; According &apos; to to &quot; &quot;! &apos; %$ &quot; &quot; &quot;#
"In both cases the results are sufficient for a relative judg-ment, i.e. -, constitutes a less semantically coherent structure as -, ."
"To allow for a binary classification into semantically coherent vs. incoherent samples, a cut-off threshold must be set."
The results of the correspond-ing experiments will be presented in Section 5.2.
"Due to lexical ambiguity, the process of transforming an n-best list of SRH to concept representations often re-sults in a set of CRs that is greater than 1, i.e. a given SRH * could be transformed into a set of CRs , ..., ."
"Word sense disambiguation could, therefore, also independently be performed using the semantic coher-ence scoring described herein as an additional application of our approach."
"However, that has not been investigated thoroughly yet."
"For example, lexicon entries for the words: yield a set of interpretations for an SRH such as: (7) Ich bin auf dem Philosophenweg I am on the Philosopher’s Walk and corresponding final scores:"
"The examination of the resulting scores allows us to conclude that constitutes the most semantically co-herent representation of the initial SRH, and PT display a slightly lesser degree of semantic coherence, whereas VU , PW and VX are much less coherent and may, thus, be considered inadequate."
"The O NTO S CORE software runs as a module in S MART K OM[REF_CITE], a multi-modal and multi-domain spoken dialogue system."
The system fea-tures the combination of speech and gesture as its input and output modalities.
"The domains of the system in-clude cinema and TV program information, home elec-tronic device control, mobile services for tourists, e.g. tour planning and sights information."
O NTO S CORE operates on n-best lists of SRHs pro-duced by the language interpretation module out of the ASR word graphs.
It computes a numerical ranking of alternative SRH and thus provides an important aid to the understanding component of the system in determin-ing the best SRH.
"The O NTO S CORE software employs two knowledge sources, an ontology (about 730 concepts and 200 relations) and a word/concept lexicon (ca. 3.600 words), covering the respective domains of the system."
The evaluation of O NTO S CORE was carried out on a dataset of 2.284 SRHs.
We reformulated the problem of measuring the semantic coherence in terms of classify-ing the SRHs into two classes: coherent and incoherent.
"To our knowledge, there exists no similar software per-forming semantic coherence scoring to be used for com- parison in this evaluation."
"Therefore, we decided to use the results from human annotation (s. Section 2.2) as the baseline."
A gold standard for the evaluation of O NTO S CORE was derived by the annotators agreeing on the correct so-lution in cases of disagreement.
"This way, we obtained 1.246 (54.55%)"
"SRH classified as coherent by humans, which is also assumed to be the baseline for this evalua-tion. mation of the scores (which range from 1 toAdditionally, we performed an inverse linear  transfor-), so that the output produced by O NTO S CORE is a score on the scale from 0 to 1, where higher scores indicate greater coherence."
"In order to obtain a binary classification of SRHs into coherent versus incoherent with respect to the knowledge base, we set a cutoff thresh old."
The depen-dency graph of the threshold value and the results of the program in % is shown in Figure 1.
The best results are achieved with the threshold 0.29.
"With this threshold, O NTO S CORE correctly classifies 1.487 SRH, i.e. 65.11% in the evaluation dataset (the word/concept relation is not taken into account at this point)."
"Figure 3 shows the dependency graph between ( , rep-resenting the threshold for the word/concept relation and the results of O NTO S CORE , given the best cutoff thresh-old for the classification (i.e. 0.29) derived in the previous experiments."
The best results are achieved with the ( .
"In other words, the proportion of concepts vs. words must be no less than 1 to 3."
"Under these settings, O NTO S CORE correctly classifies 1.672 SRH, i.e. 73.2% in the evalua-tion dataset."
"This way, the technique brings an additional improvement of 8.09% as compared to initial results."
The O NTO S CORE system described herein automatically performs ontology-based scoring of sets of concepts con- stituting an adequate representation of speech recogni-tion hypotheses.
"To date, the algorithm has been im-plemented in a software which is employed by a multi-domain and multi-modal dialogue system and applied to the task of scoring n-best lists of SRH, thus produc-ing a score expressing how well a given SRH fits within the domain model."
"For this task, it provides an alterna-tive knowledge-based score next to the ones provided by the ASR and the NLU system."
"In the evaluation of our system we employed an ontology that was not designed for this task, but already existed as the system’s internal knowledge representation."
"As future work we will examine how the computa-tion of a discourse dependent semantic coherence score, i.e. how well a given SRH fits within domain model with respect to the previous discourse, can improve the overall score."
"Additionally, we intend to calculate the semantic coherence score with respect to individual do-mains of the system, thus enabling domain recognition and domain change detection in complex multi-modal and multi-domain spoken dialogue systems."
"Currently, we are also beginning to investigate whether the proposed method can be applied to scoring sets of potential candi-dates for resolving the semantic interpretation of ambigu-ous, polysemous and metonymic language use."
We describe our approach to the construction and evaluation of a large-scale database called “CatVar” which contains categorial variations of English lexemes.
"Due to the prevalence of cross-language categorial variation in multilin-gual applications, our categorial-variation re-source may serve as an integral part of a di-verse range of natural language applications."
"Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities."
We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard.
This evaluation reveals that the categorial database achieves a high degree of precision and recall.
"Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%."
Natural Language Processing (NLP) applications may only be as good as the resources upon which they rely.
Resources specifying the relations among lexical items such as WordNet[REF_CITE]and HowNet[REF_CITE](among others) have inspired the work of many re-searchers in NLP[REF_CITE].
In this paper we introduce a new resource called Cat-Var which specifies the lexical relation Categorial Vari-ation on a large scale for English.
This resource has al-ready been used effectively in a wide range of monolin-gual and multilingual NLP applications.
"Upon its first public release, CatVar will be freely available to the re-search community."
We expect that the contribution of this resource will become more widely recognized through its future incorporation into additional NLP applications.
A categorial variation of a word with a certain part-of-speech is a derivationally-related word with possi-bly a different part-of-speech.
"For example, hunger , hunger and hungry  are categorial variations of each other, as are cross and across , and stab and stab ."
"Although this relation seems basic on the surface, this relation is critical to work in Information Retrieval (IR), Natural Language Generation (NLG) and Machine Trans-lation (MT)—yet there is no large scale resource avail-able for English that focuses on categorial variations. [Footnote_1]"
"1 It is the intention of the WordNet 1.7 developers to in-clude such information in their next version, but only for nouns and verbs (Christiane Fellbaum, pc.), not other pairings such as noun-adjective, verb-preposition relationships. Discussions are currently underway for sharing the CatVar database with Word-Net developers for more rapid development, extension, and mu-tual validation of both resources."
"In the rest of this paper, we discuss other available re-sources and how they differ from the CatVar database."
We then discuss how and what resources were used to build CatVar.
"Afterwards, we present three applications that use CatVar in different ways: Generation-Heavy MT, headline generation, and cross-language divergence un-raveling for bilingual alignment."
"Finally, we present a multi-component evaluation of the database."
Our evalu-ation reveals that the categorial database achieves a high degree of precision and recall and that it improves on the linkability of Porter stemmer by over 30%.
Lexical relations describe relative relationships among different lexemes.
"Lexical relations are either hierarchi-cal taxonomic relations (such as hypernymy, hyponymy and entailments) or non-hierarchical congruence rela- tions (such as identity, overlap, synonymy and antonymy)[REF_CITE]."
WordNet is the most well-developed and widely used lexical database of English[REF_CITE].
"In Word-Net, both types of lexical relations are specified among words with the same part of speech (verbs, nouns, ad-jectives and adverbs)."
WordNet has been used by many researchers for different purposes ranging from the con-struction or extension of knowledge bases such as SEN-SUS[REF_CITE]or the Lexical Conceptual Structure Verb Database (LVD)[REF_CITE]to the faking of meaning ambiguity as part of system evaluati[REF_CITE].
"In the context of these projects, one criticism of WordNet is its lack of cross-categorial links, such as verb-noun or noun-adjective re-lations."
Mel’čuk approaches lexical relations by defining a lex-ical combinatorial zone that specifies semantically related lexemes through Lexical Functions (LF).
"These functions define a correspondence between a key lexical item and a set of related lexical items (Mel’čuk, 1988)."
There are two types of functions: paradigmatic and syntagmatic[REF_CITE].
Paradigmatic LFs associate a lex-ical item with related lexical items.
The relation can be semantic or syntactic.
"Semantic LFs include Syn-onym(calling) = vocation, Antonym(small) = big, and Generic(fruit) = apple."
Syntactic LFs include Derived-Noun(expand)= expansion and Adjective(female) = fem-inine.
Syntagmatic LFs specify collocations with a lexeme given a specified relationship.
"For example, there is a LF that returns a light verb associated with the LF’s key: Light-Verb(attention) = pay."
Other LFs specify certain semantic associations such as Intensify-Qualifier(escape) = narrow and Degradation(milk) = sour.
Lexical Func-tions have been used in MT and Generation (e.g.[REF_CITE]).
"Although research on Lexical Functions provides an intriguing theoretical discussion, there are no large scale resources available for categorial variations induced by lexical functions."
This lack of resources shouldn’t sug-gest that the problem is too trivial to be worthy of in-vestigation or that a solution would not be a significant contribution.
"On the contrary, categorial variations are necessary for handling many NLP problems."
"For exam-ple, in the context of MT,[REF_CITE]claims that 98% of all translation divergences (variations in how source and target languages structure meaning) involve some form of categorial variation."
"Moreover, most IR systems require some way to reduce variant words to common roots to improve the ability to match queries[REF_CITE]."
"Given the lack of large-scale resources containing cat- egorial variations, researchers frequently develop and use alternative algorithmic approximations of such a re-source."
These approximations can be divided into Reduc-tionist (Analytical) or Expansionist (Generative) approxi-mations.
The former focuses on the conversion of several surface forms into a common root.
Stemmers such as the Porter stemmer[REF_CITE]are a typical example.
"The latter, or expansionist approaches, overgenerate possibili-ties and rely on a statistical language model to rank/select among them."
The morphological generator in Nitrogen is an example of such an approximati[REF_CITE].
There are two types of problems with approximations of this type: (1) They are uni-directional and thus lim-ited in usability—A stemmer cannot be used for genera-tion and a morphological overgenerator cannot be used for stemming; (2) The crude approximating nature of such systems cause many problems in quality and ef-ficiency from over-stemming/under-stemming or over-generation/under-generation.
"Consider, for example, the Porter stemmer, which stems commune , communication and communism to  ."
"And yet, it does not produce this same stem for communist or communicable  (stemmed to  and  respectively). [Footnote_2]"
"2 For a deeper discussion and classification of Porter stem-mer’s errors, see[REF_CITE]."
"Another ex-ample is the expansionist Nitrogen morphological gener-  ator  , where the morphological feature   applied to &quot;! returns eleven variations includ-ing # $&quot;!%&amp; , # &quot;! (  and # &quot;%! ) ."
Only two are correct ( &quot;%! &quot;! *&amp; ).
"Suchand overgeneration multiplied out at different points in a sen-tence expands the search space exponentially, and given various cut-offs in the search algorithm, might even ap-pear in some of the top ranked choices."
"Given these issues, our goal is to build a database of categorial variations that can be used with both expan-sionist and reductionist approaches without the cost of over/under-stemming/generation."
"The research reported herein is relevant to MT, IR, and lexicon construction."
"The CatVar database was developed using a combina-tion of resources and algorithms including the Lexi-cal Conceptual Structure (LCS) Verb and Preposition Databases[REF_CITE], the Brown Corpus section of the Penn Treebank[REF_CITE], an English mor-phological analysis lexicon developed for PC-Kimmo (Englex)[REF_CITE], NOMLEX[REF_CITE], Longman Dictionary of Contemporary English (LDOCE) 3[REF_CITE], WordNet 1.6[REF_CITE], and the Porter stemmer."
"The contribution of each of these sources is clearly labeled in the CatVar database, thus enabling the use of different cross-sections of the re-source for different applications. [Footnote_4]"
"4 For example, in a headline generation system (HeadGen), higher Bleu scores were obtained when using the portions of the CatVar database that are most relevant to nominalized events (e.g., NOMLEX)."
"Some of these resources were used to extract seed links between different words (Englex lexicon, NOMLEX and LDOCE)."
Others were used to provide a large-scale cov-erage of lexemes.
"In the case of the Brown Corpus, which doesn’t provide lexemes for its words, the Englex mor-phological analyzer was used together with the part of speech specified in the Penn Tree Bank to extract the lex-eme form."
"The Porter stemmer was later used as part of a clustering step to expand the seed links to create clusters of words that are categorial variants of each other, e.g., hunger , hungry  , hunger , hungriness ."
"The current version of the CatVar (version 2.0) in-cludes 62,232 clusters covering 96,368 unique lexemes."
"The lexemes belong to one of four parts-of-speech ([REF_CITE]%,[REF_CITE]%,[REF_CITE]% and Adverb 4%)."
Al-most half of the clusters currently include one word only.
Three-quarters of these single-word clusters are nouns and one-fifth are adjectives.
The other half of the words is distributed in a Zipf fashion over clusters from size 2 to 27.
Figure 1 shows the word-cluster distribution.
A smaller supplementary database devoted to verb-preposition variations was constructed solely from the LCS verb and preposition lexicon using shared LCS primitives to cluster.
The database was inspired by pairs such as cross and across which are used in Generation-Heavy MT.
"But since verb-preposition clus-ters are not typically morphologically related, they are kept separate from the rest of the CatVar database and they were not included in the evaluation presented in this paper. [Footnote_5]"
5 This supplementary database includes 242 clusters for more than 230 verbs and 29 prepositions. Other examples of verb-preposition clusters include: avoid and away from ; enter and into ; and border and beside (or next to ).
The CatVar is web-browseable[URL_CITE]Figure 2 shows the CatVar web-based interface with the hunger cluster as an example.
The interface allows searching clusters using regular expressions as well as cluster length restrictions.
The database is also available for researchers in perl/C and lisp searchable formats.
Our project is focused on resource building and evalua-tion.
"However, the CatVar database is relevant to a num-ber of natural language applications, including generation for MT, headline generation, and cross-language diver-gence unraveling for bilingual alignment."
"Each of these are discussed below, in turn."
The Generation-Heavy Hybrid Machine Translation (GHMT) model was introduced[REF_CITE]to han-dle translation divergences between language pairs with asymmetrical (poor-source/rich-target) resources.
The approach does not rely on a transfer lexicon or a com-mon interlingual representation to map between divergent structural configurations from source to target language.
"Instead, different alternative structural configurations are over-generated and these are statistically ranked using a language model."
The CatVar database is used as one of the constraints on the structural expansion step.
"For example, to allow the conflation of verbs such as make or cause and an argument such as development , the first condition for conflatability is finding a verb categorial variant of the argument development ."
In this case the verb categorial variant is develop . [Footnote_6]
6 The other conditions on conflatability and some detailed examples are discussed[REF_CITE]and[REF_CITE].
The HeadGen headline generator was introduced[REF_CITE]to create headlines automatically from newspaper text.
The goal is to generate an informa-tive headline (one that specifies the event and its partic-ipants) not just an indicative headline (which specifies the topic only).
The system is implemented as a Hidden Markov Model enhanced with a postprocessor that filters out headlines that do not contain a verbal or nominalized event.
This is achieved by verifying that there is at least one word in the generated headline that appears in CatVar as a V (a verbal event) or as a N whose verbal counterpart is in the same cluster (a nominalized event).
A recent study indicates that there is a significant im-provement in Bleu scores (using human-generated head-lines as our references) when running headline generation with the CatVar filter: [Footnote_7]
"7 For details about the Bleu evaluation metric, see[REF_CITE]."
HeadGen with CatVar filter: 0.1740
HeadGen with no CatVar filter: 0.1687
"This quantitative distinction correlates with human-perceived differences, e.g., between the two headlines Washingtonians fight over drugs and In the nation’s capi-tal (generated for the same story—with and without Cat-Var, respectively)."
DUSTer—Divergence Unraveling for Statistical Translation—was introduced[REF_CITE].
"In this system, common divergence types are systemat-ically identified and English sentences are transformed using a mapping referred to as -to- ."
"The objectiveto bear a closer resemblance to that of  another language is to enable more accurate alignment and projection of dependency trees in another language without requiring any training on dependency-tree data in that language. components of the DUSTer system: (1) In the -to-The CatVar database has been incorporated into two  mapping, e.g., the transformation from kick to LightVB kick (corresponding to the English/Spanish divergence pair kick/dar patada); and (2) During an automatic mark-ular -to-up phase prior  to this transformation, where the partic-mapping is selected from a set of possi-bilities based on the 2 input sentences."
"For example, the rule V[CatVar=N] -&gt; LightVB N is selected for the transformation above by first checking that the verb V is associated with a word of category N in Cat-Var."
Transforming divergent English sentences using this mechanism has been shown to facilitate word-level align-ment by reducing the number of unaligned and multiply-aligned words.
This section includes two evaluations concerned with dif-ferent aspects of the CatVar database.
The first evalua-tion calculates the recall and precision of CatVar’s clus-tering and the second determines the contribution of Cat-Var over Porter stemmer.
"To determine the recall and precision of CatVar given the lack of a gold standard, we asked 8 native speakers to evaluate 400 randomly-selected clusters."
Each annotator was given a set of 100 clusters (with two annotators per set).
Figure 3 shows a segment of the evaluation interface which was web-browseable.
The annotators were given detailed instructions and many examples to help them with the task.
They were asked to classify each word in every cluster as belonging to one of the following categories:
Perfect: This word definitely belongs in this cluster.
Perfect (except for part of speech problem).
Perfect (except for spelling problem).
Not Sure: It is not clear whether a word that is derivationally correct belongs in a set or not.
This word doesn’t belong in this cluster.
May not be a Real Word: This word is not known and couldn’t be found it in a dictionary.
The interface also provided an input text box to add missing words to a cluster.
"In calculating the inter-annotator agreement, we did not consider mismatches in word additions as disagree-ment since some annotators could not think up as many possible variations as others."
"After all, this was not an evaluation of their ability to think up variations, but rather of the coverage of the CatVar database."
The inter-annotator agreement was calculated as the percent-age of words where both annotators agreed out of all words.
"Even though there were six fine-grained classi-fications, the average inter-annotator agreement was high (80.75%)."
"Many of the disagreements, however, resulted from the fine-grainedness of the options available to the annotators."
"In a second calculation of inter-annotator agreement, we simplified the annotators’ choices by placing them into three groups corresponding to Perfect (Perfect and Perfect-but), Not-sure (Not-sure and May-not-be-a-real-word) and Wrong (Does-not-belong)."
This annotation-grouping approach is comparable to the clustering tech-niques used[REF_CITE]to “super-tag” fine grained annotations.
"After grouping the annotations, av-erage inter-annotator agreement rose up to 98.35%."
"The cluster modifications produced by each pair of an-notators assigned to the same cluster were then combined automatically in an approximation to post-annotation inter-annotator discussion, which traditionally results in agreement: (1) If both annotators agreed on a category, then it stands; (2) One annotator overrides another in cases where one is more sure than the other (i.e., Per-fect overrides Perfect-but-with-error/Not-sureand Wrong overrides Not-sure); (3)"
"In cases where one annotator considers a word Perfect while the other annotator con-sidered it Wrong, we compromise at Not-sure."
The union of all added words was included in the combined cluster.
None had spelling errors and only one word had a part-of-speech issue. 23 words (less than 3%) were marked as Not-sures.
And only 6 words (less than 1%) were marked as Wrong.
"Some of these words were clus-tered separately in the database. [Footnote_8] The rest of the missing words (81 words or 10% of all words) were not present in the database, but 50 of them (or 62%) were linkable to existing words in the CatVar using simple stemming (e.g., the Porter stemmer, whose relevance is described next)."
8[REF_CITE]words that were “not really missing” were clus-tered in 89 other clusters not included in the evaluation sample.
The precision was calculated as the ratio of perfect words to all original (i.e. not added) words: 91.82%.
The recall was calculated as the ratio of perfect words divided by all perfect plus all added words: 72.46%.
"However, if we exclude the not-really missing words, the adjusted recall value becomes 87.16%."
The harmonic mean or F-score [Footnote_9] of the precision and recall is 81.00% (or 89.43% for adjusted recall).
9 F-score = .
"To measure the contribution of CatVar with respect to the “linking together” of related words, it is important to de-fine the concept of linkability as the percentage of word-to-word links in the database resulting from a specific source."
"For example, Natural linkability refers to pairs of words whose form doesn’t change across categories such as zip and zip or afghan and afghan  ."
Porter linkability refers to words linkable by reduction to a com-mon Porter stem.
CatVar linkability is the linkability of two words appearing in the same CatVar cluster.
Figure 4 shows an example of all three types of links in the hunger cluster.
"Here, hunger and hunger are linked in three ways, Naturally (N), by the Porter stem-mer (P), and in CatVar (C)."
Porter links hungry  and hungriness via the common stem hungri but Porter doesn’t link either of these to hunger or hunger (stem hunger).
"The total number of links in this cluster is six, two of which are Porter-determinable and only one of which is naturally-determinable."
The calculation of linkability applies only to the por-tion of the database containing multi-word clusters (about half of the database) since single-word clusters have zero links.
10 A reviewer points out that the Porter stemmer could be
"It is important to point out that, for CatVar to be used in IR, it must be accompanied by an inflectional ana-lyzer that reduces words to their lexeme form (remov-ing plural endings from nouns or gerund ending from verbs). [Footnote_11] The contribution of CatVar is in its linking of words related derivationally not inflectionally."
"11 This is, in fact, the approach used in the HeadGen and DUSTer applications described above."
Work[REF_CITE]demonstrates an improved performance with derivational stemming over the Porter stemmer most of the time.
We have presented our approach to constructing and eval-uating a new large-scale database containing categorial variations of English words.
"In addition, we have de-scribed different applications for which it has proven use-ful."
Our evaluation indicates that CatVar has coverage and accuracy of over 80% (F-score) and also that the database improves the linkability of Porter stemmer by about 30%.
"These findings are significant contributions to several different communities, including Information Retrieval and Machine Translation."
Future work includes improving the word-cluster ra-tio and absorbing more of the single-word clusters into existing clusters or other single-word clusters.
We are also considering enrichment of the clusters with types of derivational relations such as “nominal-event” or “doer” to complement part-of-speech labels.
"Other lexical semantic features such telicity, sentience and change-of-state can also be induced from morphological cues[REF_CITE]."
"Additionally, we are interested in measuring the ap-plied contribution of using the CatVar in natural-language applications such as Information Retrieval."
"And finally, we intend to incorporate CatVar into new applications such as parallel corpus word alignment."
We present a neural network method for induc-ing representations of parse histories and us-ing these history representations to estimate the probabilities needed by a statistical left-corner parser.
"The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge."
"Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions."
"Unlike most problems addressed with machine learning, parsing natural language sentences requires choosing be-tween an unbounded (or even infinite) number of possi-ble phrase structure trees."
The standard approach to this problem is to decompose this choice into an unbounded sequence of choices between a finite number of possible parser actions.
This sequence is the parse for the phrase structure tree.
"We can then define a probabilistic model of phrase structure trees by defining a probabilistic model of each parser action in its parse context, and apply machine learning techniques to learn this model of parser actions."
Many statistical parsers[REF_CITE]are based on a history-based model of parser actions.
"In these models, the probabil-ity of each parser action is conditioned on the history of previous actions in the parse."
"But here again we are faced with an unusual situation for machine learning problems, conditioning on an unbounded amount of information."
A major challenge in designing a history-based statisti-cal parser is choosing a finite representation of the un-bounded parse history from which the probability of the next parser action can be accurately estimated.
Previ-ous approaches have used a hand-crafted finite set of fea-tures to represent the parse history[REF_CITE].
"In the work presented here, we automatically induce a finite set of real valued features to represent the parse history."
"We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs)[REF_CITE]."
This machine learn-ing method is specifically designed for processing un-bounded structures.
"It allows us to avoid making a priori independence assumptions, unlike with hand-crafted his-tory features."
But it also allows us to make use of our a priori knowledge by imposing structurally specified and linguistically appropriate biases on the search for a good history representation.
The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance.
"When trained on just part-of-speech tags, the resulting parser achieves the best current performance of a non-lexicalized parser on the Penn Treebank."
"When a relatively small vocab-ulary of words is used, performance is only marginally below the best current parser accuracy."
"If either the bi-ases are reduced or the induced history representations are replaced with hand-crafted features, performance de-grades."
"The parsing system we propose consists of two compo-nents, one which estimates the parameters of a proba-bility model for phrase structure trees, and one which searches for the most probable phrase structure tree given these parameters."
The probability model we use is gen-erative and history-based.
"At each step, the model’s stochastic process generates a characteristic of the tree or a word of the sentence."
"This sequence of decisions  is the derivation of the tree, which we will denote ."
"Because there is a one-to-one mapping from phrase struc-ture trees to our derivations, we can use the chain rule for conditional probabilities to derive the probability of a tree as the multiplication of the probabilities of each derivation decision   conditional   on that decision’s prior derivation history .        !    tree"
The  &quot; neural ! network # $ is used to estimate the parameters of this probability  &quot;  model   . %
"To define the parameters we need to choose the ordering of the decisions in a derivation, such as a top-down or shift-reduce ordering."
The ordering which we use here is that of a form of left-corner parser[REF_CITE].
A left-corner parser de-cides to introduced a node into the parse tree after the sub-tree rooted at the node’s first child has been fully parsed.
Then the subtrees for the node’s remaining children are parsed in their left-to-right order.
"We use a version of left-corner parsing which first applies right-binarization to the grammar, as is done[REF_CITE]except that we binarize down to nullary rules rather than to binary rules."
"This allows the choice of the chil-dren for a node to be done incrementally, rather than all the children having to be chosen when the node is first in-troduced."
We also extended the parsing strategy slightly to handle Chomsky adjunction structures (i.e. structures of the form [X [X &amp;&apos;&amp;&apos;&amp; ] [Y &amp;&apos;%&amp; &amp; ]]) as a special case.
The Chomsky adjunction is removed and replaced with ) a ( spe- * cial “modifier” link in the tree (becoming [X &amp;&apos;%&amp; &amp; [ Y &apos;&amp; &amp;%&amp; ]]).
We also compiled some frequent chains of non-branching nodes (such as [S [VP &apos;&amp; &amp;&apos;&amp; ]]) into a single node with a new label (becoming [S-VP %&amp; &apos;&amp; &amp; ]).
All these gram-mar transforms are undone before any evaluation of the output trees is performed.
An example of the ordering of the decisions in a deriva-tion is shown by the numbering on the left in figure 1.
"To precisely specify this ordering, it is sufficient to charac-terize the state of the parser as a stack of nodes which are in the process of being parsed, as illustrated on the right in figure 1."
The parsing strategy starts with a stack that con-tains a node labeled ROOT (step 0) and must end in the same configuration (step 9).
Each parser action changes the stack and makes an associated specification of a char-acteristic of the parse tree.
"The possible parser actions are the following, where + is a tag-word pair, , are project(S), shift(VBZ/runs), project(VP), shift(RB/often), attach, attach, attach. attach map stack .   , to .  and specify that - is the parent of , in the tree (steps 7, 8, and 9) modify map stack .  , to .  and specify that - is the modifier parent - of , in the tree (i.e. , is Chom-sky adjoined to ) (not illustrated)"
"Any valid sequence of these parser actions is a derivation / ! for a phrase structure tree.  The &quot;  neural ! # $ network estimates the parameters in two stages, first computing   % a representation of the derivation history 0 and then computing a probability distribution over the possible decisions given that history.    1 &quot;* 2 0 /  #"
"For the second stage, computing 1 &quot;* 2 , we use stan-dard neural network methods for probability estimati[REF_CITE]."
"A log-linear model (also known as a maximum entropy model, and as the normalized expo-nential output function) is used to estimate the probabil-ity distribution over the four types of decisions, shift-ing, projecting, attaching, and modifying."
"A sepa-rate log-linear model is used to estimate the probabil-ity distribution  over node labels given that projecting project , project . , which is  multiplied is chosen by the probability estimate for projecting project . to  get the probability . . estimates for that set of decisions project , project , .   project , project .   project ."
"Similarly, the probability estimate for shifting  the word which is actually observed in the sentence shift + . is computed with log-linear models."
"This means that val-ues for all possible words need to be computed, to do the normalization."
"The high cost of this  computation is re-duced by splitting the computation of shift + shift . into multiple stages  , first #!6  estimating a distribution over all possible tags shift . , and then estimating shift correct tag shift + shift a distribution  over #6! the 7 possible ! tag-word . . pairs given the   shift +#6!7 .   #!6 7 #!6 7 shift + shift .   shift #!6  shift .   shift ."
This means that only estimates for the tag-word pairs with the correct tag need to be computed.
"We also reduced the computational cost of terminal prediction by replacing the very large number of lower frequency tag-word pairs with tag-“unknown-word” pairs, which are also used for tag-word pairs which were not in the training set."
"We do not do any morphological analysis of unknown words, although we would expect some improvement in perfor-mance if we did."
"A variety of frequency thresholds were tried, as reported in section 6."
The most novel aspect of our parsing model is the way in / which  # the representation of the derivation history 0 is computed.
"Choosing this representation is a challenge for any  history-based # statistical parser, be-cause the history is of unbounded size."
"Log-linear models, as with most probability estimation meth-ods, require that there be a finite set of features on which the probability is conditioned."
The standard way to han-dle this problem is to hand-craft a finite set of features which provides a sufficient summary of the history[REF_CITE].
The probabilities are then assumed to be independent of all the information about the history which is not captured by the chosen features.
"The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand."
In this work we use a method for automatically in-ducing a finite representation of the derivation history.
The method is a form of multi-layered neural network called Simple Synchrony Networks[REF_CITE].
"The output layer of this network is the log-linear model which computes the func-tion 1 , discussed above."
"In addition the SSN has a hidden layer, which computes a finite vector of real valued fea-tures from /  a  sequence  of inputs specifying the derivation history."
This  hidden # layer vector is the history representation 0 .
"It is analogous to the hid-den state of a Hidden Markov Model (HMM), in that it represents the state of the underlying generative process and in that it is not explicitly specified in the output of the generative process."
The mapping 0 from the derivation history to the his-tory representation is computed with the recursive ap-plication of a function .
"As will be discussed in the next / section &amp;&apos;&amp;%&amp;  ,    maps  plus  previous  pre-definedhistoryfeaturesrepresenta-of the tions 0 derivation history to a real-valued vector 0 / &amp;&apos;&apos;&amp; &amp;  # ."
"Because the function is nonlinear, the induction of this history representation allows the train-ing process  to  explore a much more general set of esti-mators 1 0 than  would be possible with a log-linear model alone (i.e. 1 ). [Footnote_1]"
"1 As is standard, is the sigmoid activation function applied to a weighted sum of its inputs. Multi-layered neural networks of this form can approximate arbitrary mappings from inputs to outputs[REF_CITE], whereas a log-linear model alone can only estimate probabilities where the category-conditioned probability distributions of the pre-defined inputs are in a restricted form of the exponential family[REF_CITE]."
This generality makes this es-timation method less dependent on the choice of input representation .
"In addition, because the inputs to in-clude previous history representations, the mapping 0 is defined recursively."
"This recursion allows the input to 0 to be unbounded, because an unbounded derivation his-tory can be successively compressed into a fixed-length vector of history features."
Training a Simple Synchrony Network (SSN) is sim-ilar to training a log-linear model.
"First an appropriate error function is defined for the network’s outputs, and then some form of gradient descent learning is applied to search for a minimum of this error function. [Footnote_2]"
"2 We use the cross-entropy error function, which ensures that the minimum of the error function converges to the desired probabilities as the amount of training data increases[REF_CITE]. This implies that the minimum for any given dataset is an estimate of the true probabilities. We use the on-line version of Backpropagation to perform the gradient descent."
This learn-ing simultaneously tries to optimize the parameters of the output computation [Footnote_1] and the parameters of the mapping 0 from the derivation history to the history representation.
"1 As is standard, is the sigmoid activation function applied to a weighted sum of its inputs. Multi-layered neural networks of this form can approximate arbitrary mappings from inputs to outputs[REF_CITE], whereas a log-linear model alone can only estimate probabilities where the category-conditioned probability distributions of the pre-defined inputs are in a restricted form of the exponential family[REF_CITE]."
"With multi-layered networks such as SSNs, this training is not guaranteed to converge to a global optimum, but in practice a set of parameters whose error is close to the optimum can be found."
The reason no global optimum can be found is that it is intractable to find the optimal mapping 0 from the derivation history to the history rep-resentation.
"Given this difficulty, it is important to impose appropriate biases on the search for a good history repre-sentation."
"When researchers choose a hand-crafted set of features to represent the derivation history, they are imposing a domain-dependent bias on the learning process through the independence assumptions which are implied by this choice."
"In this work we do not make any independence assumptions, but instead impose soft biases to empha-size some features of the derivation history over oth-ers / ."
This ! is  achieved  through the choice of what features  are input explicitly to the computation of 0 / and what other history representations 0 /  #  are also input.  
"If the explicit features include the previous decision and the other history representations  # include the previous history representa-tion 0 , then (by induction !  ) any information about the derivation history   % could conceiv- ably be included in 0 ."
Thus such a model is making no a priori independence assumptions.
"However, some of this information is more likely to be included than other of this information, which is the source of the model’s soft biases."
The bias towards including certain information in the history representation arises from the recency bias in training recursively defined neural networks.
"The only trained parameters of the mapping 0 are the parameters of the function , which selects a subset of the information from a set of previous history representations and records it in a new history representation."
The training process automatically chooses the parameters of based on what information needs to be recorded.
"The recorded informa-tion may be needed to compute the output for the current step, or it may need to be passed on to future history rep-resentations to compute a future output."
"However, the more history representations intervene between the place where the information is input and the place where the in-formation is needed, the less likely the training is to learn to record this information."
"We can exploit this recency bias in inducing history representations by ensuring that information which is known to be important at a given step in the derivation is input directly to that step’s his-tory representation, and that as information becomes less relevant it has increasing numbers of history representa-tions to pass through before reaching the step’s history representation."
The principle we apply when designing the inputs to each history representation is that we want recency in this flow of information to match a linguisti-cally appropriate notion of structural locality.
"To achieve this structurally-determined inductive bias, we use Simple Synchrony Networks, which are specifi-cally designed for processing structures."
"A SSN divides the processing of a structure into a set of sub-processes, with one sub-process for each node of the structure."
"For phrase structure tree derivations, we divide a derivation into a set of sub-derivations by assigning a derivation step to the sub-derivation for the node top which is on the top of the stack prior to that step."
The SSN network then performs the same computation at each position in each sub-derivation.
"The unbounded nature of phrase structure trees does not pose a problem for this approach, because increasing the number of nodes only increases the num-ber of times the SSN network needs to perform a compu-tation, and not the number of parameters in the computa-tion which need to be trained."
"For each position in the sub-derivation for a node top , the SSN computes two real-valued vectors, namely /  #     0 and 1 0 . 0 / ! # is computed by applying ! the   function  to a set of pre-defined features of the derivation history plus a small set of previous history representations.     0 rep #     #  top where rep   is the most recent previous history rep-resentation for a node .   0     top  &quot;! rep top is a small set of nodes which are particularly relevant to decisions involving top ."
"This set always in-cludes top itself, but /  the ! remaining  nodes in top and the features in need to be chosen by the system designer."
"These choices determine how informa-tion flows from one history representation to another, and thus determines the inductive bias of the  system !  ."
We have designed top and so that the inductive bias reflects structural locality.
"Thus, top includes nodes which are structurally local to top ."
"These nodes are the left-corner ancestor of top (which is below top on the stack), top ’s left-corner child (its leftmost child # , if any), and top ’s most recent child (which is top , if any)."
"For right-branching structures, the left-corner ancestor is the parent, conditioning on which has been found to be beneficial[REF_CITE], as has conditioning on the left-corner child[REF_CITE]."
"Because these inputs include the history representations of both the left-corner ancestor and the most recent child, a derivation step always has access to the $&amp; history # % representation from the previous derivation step , and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history representation."
"Thus this model is making no a priori hard independence assumptions, just a priori soft biases."
"As mentioned above, top also includes top itself, which means that the inputs to always include the his-tory representation for the most recent derivation step as-signed to top ."
This input imposes an appropriate bias because the induced history features which are relevant to previous derivation decisions involving top are likely to be relevant to the decision at step as well.
"As a sim-ple example, in figure 1, the prediction of the left corner terminal of the VP node (step 4) and the decision that the S node is the root of the whole sentence (step 9) are both dependent on the fact that the node on the top of the stack in each case has the label S (chosen in step 3)."
The ! pre-defined  $ features of the derivation history which are input to for node top at step are chosen to reflect the information / which is directly rel-evant to choosing the next decision .
"In the parser  pre-sented here, these inputs are the last decision in the derivation, the label or tag of the sub-derivation’s node top , the tag-word pair for the most recently predicted terminal, and the tag-word pair for top ’s left-corner ter-minal (the leftmost  terminal it dominates)."
Inputting the last decision is sufficient to provide the SSN with a complete specification of the derivation history.
The re-maining features were chosen so that the inductive bias would emphasize these pieces of information.
"Once we have trained the SSN to estimate the param-eters of our probability model, we use these estimates to search the space of possible derivations to try to find the most probable derivation."
"Because we do not make a priori independence assumptions, searching the space of all possible derivations has exponential complexity, so it is important to be able to prune the search space if this computation is to be tractable."
"The left-corner ordering for derivations allows very severe pruning without signif-icant loss in accuracy, which is crucial to the success of our parser due to the relatively high computational cost of computing probability estimates with a neural network rather than with the simpler methods typically employed in NLP."
Our pruning strategy is designed specifically for left-corner parsing.
"We prune the search space in two different ways, the first applying fixed beam pruning at certain derivation steps and the second restricting the branching factor at all derivation steps."
The most important pruning occurs after each word has been shifted onto the stack.
When a partial derivation reaches this position it is stopped to see if it is one of the best 100 partial derivations which end in shifting that word.
Only a fixed beam of the best 100 derivations are allowed to continue to the next word.
Ex-periments with a variety of post-prediction beam widths confirms that very small validation performance gains are achieved with widths larger than 100.
To search the space of derivations in between two words we do a best-first search.
"This search is not restricted by a beam width, but a limit is placed on the search’s branching factor."
"At each point in a partial derivation which is being pursued by the search, only the 10 best alternative decisions are considered for continuing that derivation."
"This was done because we found that the best-first search tended to pur-sue a large number of alternative labels for a nonterminal before pursuing subsequent derivation steps, even though only the most probable labels ended up being used in the best derivations."
We found that a branching factor of 10 was large enough that it had virtually no effect on the val-idation accuracy.
We used the Penn Treebank[REF_CITE]to per-form empirical experiments on this parsing model.
"To test the effects of varying vocabulary sizes on perfor-mance and tractability, we trained three different mod-els."
"The simplest model (“SSN-Tags”) includes no words in the vocabulary, relying completely on the informa-tion provided by the part-of-speech tags of the words."
The second model (“[REF_CITE]”) uses all tag-word pairs which occur at least 200 times in the training set.
The remaining words were all treated as instances of the unknown-word.
This resulted in a vocabulary size of 512 tag-word pairs.
"The third model (“[REF_CITE]”) thresholds the vocabulary at 20 instances in the training set, resulting in 4242 tag-word pairs. [Footnote_3]"
"3 We used a publicly available tagger[REF_CITE]to provide the tags used in these experiments, rather than the hand-corrected tags which come with the corpus."
We determined appropriate training parameters and network size based on intermediate validation results and our previous experience with networks similar to the models[REF_CITE].
We trained two or three networks for each of the three vocabulary sizes and chose the best ones based on their validation perfor-mance.
"Training times vary but are long, being around [Footnote_4] days for a SSN-Tags model, 6 days for a[REF_CITE]model, and 10 days for a[REF_CITE]model (on a 502 MHz Sun Blade computer)."
"4 All the best networks had 80 hidden units. Weight decay regularization was applied at the beginning of training but re-duced to 0 by the end of training. Training was stopped when maximum performance was reached on the validation set, using a post-word beam width of 5."
We then tested the best mod-els for each vocabulary size on the testing set. 4 Standard measures of performance are shown in table 1. [Footnote_5]
"5 All our results are computed with the evalb program fol-lowing the standard criteria[REF_CITE], and using the standard training (sections 2–22, 39,832 sentences), validation (section 24, 1346 sentence), and testing (section 23, 2416 sen-tences) sets[REF_CITE]."
"The top panel of table 1 lists the results for the non-lexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser[REF_CITE], an earlier statistical left-corner parser[REF_CITE], and a PCFG[REF_CITE]."
The SSN-Tags model achieves performance which is much better than the only other broad coverage neural network parser[REF_CITE].
"The SSN-Tags model also does better than any other published results on parsing with just part-of-speech tags, as exemplified by the re-sults[REF_CITE]and[REF_CITE]."
The bottom panel of table 1 lists the results for the two lexicalized models ([REF_CITE]and[REF_CITE]) and five recent statistical parsers[REF_CITE].
"On the complete testing set, the performance of our lexicalized models is very close to the three best cur-rent parsers, which all achieve equivalent performance."
The performance of the best current parser[REF_CITE]represents only a 4% reduction in precision error and only a 7% reduction in recall error over the[REF_CITE]model.
"The SSN parser achieves this result us-ing much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words."
Another diffference between the three best parsers and ours is that we parse incrementally using a beam search.
"This allows use to trade off parsing accuracy for pars-ing speed, which is a much more important issue than training time."
"Running times to achieve the above lev-els of performance on the testing set averaged around 30 seconds per sentence for SSN-Tags, 1 minute per sen-tence[REF_CITE]and 2 minutes per sentence[REF_CITE](on a 502 MHz Sun Blade computer, aver-age 22.5 words per sentence)."
"But by reducing the num-ber of alternatives considered in the search for the most probable parse, we can greatly increase parsing speed without much loss in accuracy."
"With the[REF_CITE]model, accuracy slightly better than[REF_CITE]can be achieved at 2.7 seconds per sentence, and accuracy slightly better than[REF_CITE]can be achieved at 0.5 seconds per sentence[REF_CITE](on val-idation sentences at most 100 words long, average 23.3 words per sentence)."
"To investigate the role which induced history representa-tions are playing in this parser, we trained a number of additional SSNs and tested them on the validation set. [Footnote_6] The middle panel of table 2 shows the performance when some of the induced history representations are replaced with the label of their associated node."
6 The validation set is used to avoid repeated testing on the standard testing set. Sentences of length greater than 100 were excluded.
"The first four lines show the performance when this replacement is per-formed individually for each of the history representa-tions input to the computation of a new history represen-tation, namely that for the node’s left-corner ancestor, its most recent child, its left-corner child, and the previous parser action at the node itself, respectively."
The final line shows the performance when all these replacements are done.
In the first two models this replacement has the ef-fect of imposing a hard independence assumption in place of the soft biases towards ignoring structurally more dis-tant information.
This is because there is no other series of history representations through which the removed in-formation could pass.
"In the second two models this re-placement simply removes the bias towards paying atten-tion to more structurally local information, without im-posing any independence assumptions."
"In each modified model there is a reduction in perfor-mance, as compared to the case where all these history representations are used[REF_CITE]."
The biggest decrease in performance occurs when the left-corner an-cestor is represented with just its label (ancestor label).
"This implies that more distant top-down constraints and constraints from the left context are playing a big role in the success of the SSN parser, and suggests that parsers which do not include information about this context in their history features will not do well."
Another big de-crease in performance occurs when the most recent child is represented with just its label (child label).
"This im-plies that more distant bottom-up constraints are also playing a big role, probably including some information about lexical heads."
There is also a decrease in perfor-mance when the left-corner child is represented with just its label (lc-child label).
"This implies that the first child does tend to carry information which is relevant through-out the sub-derivation for the node, and suggests that this child deserves a special status in a history representa-tion."
"Interestingly, a smaller, although still substantial, degradation occurs when the previous history represen-tation for the same node is replaced with its node label."
We suspect that this is because the same information can be passed via its children’s history representations.
"Fi-nally, not using any of these sources of induced history features (all labels) results in dramatically worse perfor-mance, with a 58% increase in F-measure error over us-ing all three."
One bias which is conspicuously absent from our parser design is a bias towards paying particular attention to lexical heads.
"The concept of lexical head is central to theories of syntax, and has often been used in designing hand-crafted history features ([REF_CITE];"
Thus it is reasonable to suppose that the incorpo-ration of this bias would improve performance.
"On the other hand, the SSN may have no trouble in discovering the concept of lexical head itself, in which case incorpo-rating this bias would have little effect."
"To investigate this issue, we trained several SSN parsers with an explicit representation of phrasal head."
Results are shown in the lower panel of table 2.
"The first model (head identification) includes a fifth type of parser action, head attach, which is used to identify the head child of each node in the tree."
"Although incorrectly identifying the head child does not effect the performance for these evaluation measures, forcing the parser to learn this identification results in some loss in performance, as compared to the[REF_CITE]model."
"This is to be expected, since we have made the task harder with-out changing the inductive bias to exploit the notion of head."
"The second model (head word) uses the identifica-tion of the head child to determine the lexical head of the phrase. [Footnote_7] After the head child is attached to a node, the node’s lexical head is identified  # and  that word is added to the set of features input directly to the node’s subsequent history representations."
"7 If a node’s head child is a word, then that word is the node’s lexical head. If a node’s head child is a nonterminal, then the lexical head of the head child is the node’s lexical head."
This adds an inductive bias towards treating the lexical head as impor-tant for post-head parsing decisions.
"The results show that this inductive bias does improve performance, but not enough to compensate for the degradation caused by hav-ing to learn to identify head children."
"The lack of a large improvement suggests that the[REF_CITE]model al-ready learns the significance of lexical heads, but perhaps a different method for incorporating the bias towards con- ditioning on lexical heads could improve performance a little."
The third model (head word and child) extends the head word model by adding the head child to the set of structurally local nodes top .
"This addition does not result in an improvement, suggesting that the induced history representations can identify the significance of the head child without the need for additional bias."
"The degradation appears to be caused by increased problems with overtraining, due to the large number of additional weights."
Most previous work on statistical parsing has used a history-based probability model with a hand-crafted set of features to represent the derivation history[REF_CITE].
"In addition to the method proposed in this paper, another alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded feature sets."
"However, this causes efficiency problems."
Future work could use the induced history representations from our work to define efficiently computable tree ker-nels.
The only other broad coverage neural network parser[REF_CITE]also uses a neural network architec-ture which is specifically designed for processing struc-tures.
We believe that their poor performance is due to a network design which does not take into consideration the recency bias discussed in section 4.
"Ratnaparkhi’s parser (1999) can also be considered a form of neural network, but with only a single layer, since it uses a log-linear model to estimate its probabilities."
"Previous work on applying SSNs to natural language parsing[REF_CITE]has not been general enough to be applied to the Penn Treebank, so it is not possible to compare results directly to this work."
This paper has presented a method for estimating the pa-rameters of a history-based statistical parser which does not require any a priori independence assumptions.
A neural network is trained simultaneously to estimate the probabilities of parser actions and to induce a finite repre- sentation of the unbounded parse history.
"The probabili-ties of parser actions are conditioned on this induced his-tory representation, rather than being conditioned on a set of hand-crafted history features chosen a priori."
A beam search is used to search for the most probable parse given the neural network’s probability estimates.
"When trained and tested on the standard Penn Treebank datasets, the parser’s performance (89.1% F-measure) is only 0.6% below the best current parsers for this task, despite using a smaller vocabulary and less prior linguistic knowledge."
"The neural network architecture we use, Simple Syn-chrony Networks, not only allows us to avoid imposing hard independence assumptions, it also allows us to im-pose linguistically appropriate soft biases on the learn-ing process."
"SSNs are specifically designed for process-ing structures, which allows us to design the SSN so that the induced representations of the parse history are biased towards recording structurally local information about the parse."
"When we modify these biases so that some structurally local information tends to be ignored, performance degrades."
"When we introduce independence assumptions by cutting off access to information from more distant parts of the structure, performance degrades dramatically."
"On the other hand, we find that biasing the learning to pay more attention to lexical heads does not improve performance."
"Manually constructing an inventory of word senses has suffered from problems including high cost, arbitrary assignment of meaning to words, and mismatch to domains."
"To over-come these problems, we propose a method to assign word meaning from a bilingual comparable corpus and a bilingual dictionary."
It clusters second-language translation equivalents of a first-language target word on the basis of their translingually aligned dis-tribution patterns.
"Thus it produces a hierar-chy of corpus-relevant meanings of the target word, each of which is defined with a set of translation equivalents."
The effectiveness of the method has been demonstrated through an experiment using a comparable corpus con-sisting of Wall Street Journal and Nihon Kei-zai Shimbun corpora together with the EDR bilingual dictionary.
Word Sense Disambiguation (WSD) is an important subtask that is necessary for accomplishing most natu-ral language processing tasks including machine translation and information retrieval.
A great deal of research on WSD has been done over the past decade[REF_CITE].
"In contrast, word sense acqui-sition has been a human activity; inventories of word senses have been constructed by lexicographers based on their intuition."
"Manually constructing an inventory of word senses has suffered from problems such as high cost, arbitrary division of word senses, and mis-match to application domains."
We address the problem of word sense acquisition along the lines of the WSD where word senses are defined with sets of translation equivalents in another language.
Bilingual corpora or second-language cor-pora enable unsupervised WSD[REF_CITE].
"However, the correspondence between senses of a word and its translations is not one-to-one, and therefore we need to prepare an in-ventory of word senses, each of which is defined with a set of synonymous translation equivalents."
"Although conventional bilingual dictionaries usually group translations according to their senses, the grouping differs by dictionary."
"In addition, senses specific to a domain are often missing while many senses irrelevant to the domain or rare senses are included."
"To over-come these problems, we propose a method for pro-ducing a hierarchy of clusters of translation equiva-lents from a bilingual corpus and a bilingual diction-ary."
"To the best of our knowledge, there are two pre-ceding research papers on word sense acquisiti[REF_CITE]."
Both proposed distributional word clustering algorithms that are characterized by their capabilities to produce overlapping clusters.
"According to their algorithms, a polysemous word is assigned to multiple clusters, each of which represents one of its senses."
"These and our approach differ in how to define the word sense, i.e., a set of synonyms in the same language versus a set of translation equivalents in another language."
"However, it does not pro-duce definitions of senses such as sets of synonyms and sets of translation equivalents."
Most work on automatic extraction of synonyms from text corpora rests on the idea that synonyms have similar distribution patterns[REF_CITE].
"This idea is also useful for our task, i.e., extracting sets of synonymous trans-lation equivalents, and we adopt the approach to dis-tributional word clustering."
We need to mention that the singularity of our task makes the problem easier.
"First, we do not have to cluster all words of a language, but we only have to cluster a small number of translation equivalents for each target word, whose senses are to be extracted, separately."
"As a result, the problem of computational efficiency becomes less serious."
"Second, even if a translation equivalent itself is polysemous, it is not necessary to consider senses that are irrelevant to the target word."
"A translation equivalent usually represents one and only one sense of the target word, at least in case the language-pair is those with different origins like English and Japanese."
"Therefore, a non-overlapping clustering algorithm, which is far simpler than overlapping clustering algorithms, is suf-ficient."
"In conventional distributional word clustering, a word is characterized by a vector or weighted set consisting of words in the same language as that of the word it-self."
"In contrast, we propose a translingual distribu-tional word clustering method, whereby a word is characterized by a vector or weighted set consisting of words in another language."
It is based on the sense-vs.-clue correlation matrix calculation method we originally developed for unsupervised WSD[REF_CITE].
That method presupposes that each sense of a target word x is defined with a syno-nym set consisting of the target word itself and one or more translation equivalents which represent the sense.
"It calculates correlations between the senses of x and the words statistically related to x, which act as clues for determining the sense of x, on the basis of translingual alignment of pairs of related words."
Rows of the resultant correlation matrix are regarded as translingual distribution patterns characterizing trans-lation equivalents.
Sense-vs.-clue correlation matrix calculation method *) 1) Alignment of pairs of related words
"Let X(x) be the set of clues for determining the sen-se of a first-language target word x. That is,"
"X(x)={x’|(x, x’)∈R X }, where R X denotes the collection of pairs of related words extracted from a corpus of the first language."
"Henceforth, the j-th clue for determining the sense of x will be denoted as x’(j)."
"Furthermore, let Y(x, x’(j)) be the set consisting of all second-language counterparts of a first-language pair of related words x and x’(j)."
"Y(x, x’(j)) = {(y, y’) | (y, y’)∈R Y , (x, y)∈D, (x’(j), y’)∈D}, where R Y denotes the collection of pairs of related words extracted from a corpus of the second language, and D denotes a bilingual dictionary, i.e., a collection of pairs consisting of a first-language word and a second-language word that are translations of one an-other."
"Then, for each alignment, i.e., pair of (x, x’(j)) and (y, y’) (∈Y(x, x’(j))), a weighted set of common re-lated words Z((x, x’(j)), (y, y’ )) is constructed as fol-lows:"
"Z((x, x’(j)), (y, y’ )) = {x” / w(x”) | (x, x”)∈R X , (x’(j), x”)∈R X }."
"The weight of x”, denoted as w(x”), is determined as follows: - w(x”) = 1+α·MI(y, y’) when ∃y” (x”, y”)∈D, (y, y”)∈R Y , and (y’, y”)∈R Y . - w(x”) = 1 otherwise."
"This is where MI(y, y’) is the mutual information of y and y’."
The coefficient α was set to 5 in the experiment described in Section 4.
"The correlation between the i-th sense S(x, i) and the j-th clue x’(j) is defined as:"
"C ( S(x,i), x&apos; ( j) ) ="
"MI ( x, x&apos; ( j) ) ⋅ max A (( x, x&apos; ( j) ) , ( y, y&apos; ) , S(x,i) ) (y,y&apos;)∈Y(x,x&apos;( j)), y∈S(x,i)   . max A (( x, x&apos; ( j) ) , ( y, y&apos; ) , S(x,k) ) max k  (y∈y,Sy&apos;()x∈,Yk)(x,x&apos;( j)), "
"This is where MI(x, x’(j)) is the mutual information of x and x’(j), and A((x, x’(j)), (y, y’), S(x,i)), the plausi-bility of alignment of (x, x’(j)) with (y, y’) suggesting S(x, i), is defined as the weighted sum of the correla-tions between the sense and the common related words, i.e.,"
"A ( (x, x&apos; ( j)),(y, y&apos; ),S(x,i) ) = ∑ w(x&quot; ) ⋅ C ( S(x,i), x&quot; ) . x&quot;∈Z((x,x&apos;( j)),(y,y&apos;))"
"The correlations between senses and clues are cal- culated iteratively with the following initial values: C 0 (S(x, i), x’(j))=MI(x, x’(j))."
The number of iterations was set to 6 in the experiment.
Figure 1 shows how the correlation values converge.
"Advantages of using translingually aligned distribution patterns Translingual distributional word clustering has advan-tages over conventional monolingual distributional word clustering, when they are used to cluster transla-tion equivalents of a target word."
"First, it avoids clus-ters being degraded by polysemous translation equiva-lents."
Let “race” be the target word.
"One of its translation equivalents, “レース&lt; REESU &gt;”, is a poly-semous word representing “lace” as well as “race”."
"According to monolingual distributional word cluster-ing, “レース&lt; REESU &gt;” is characterized by a mixture of the distribution pattern for “レース&lt; REESU &gt;” repre-senting “race” and that for “レース&lt; REESU &gt;” repre-senting “lace”, which often results in degraded clusters."
"In contrast, according to translingual distributional word clustering, “レース&lt; REESU &gt;” is characterized by the distribution pattern for the sense of “race” that means “competition”."
"Second, translingual distributional word clustering can exclude from the clusters translation equivalents irrelevant to the corpus."
"For example, a bilingual dic-tionary renders “特徴&lt; TOKUCHOU &gt;” (“feature”) as a translation of “race”, but that sense of “race” is used infrequently."
"If it is the case in a given domain, “特徴 &lt; TOKUCHOU &gt;” has low correlation with most words related to “race”, and can therefore be excluded from any clusters."
We should also mention the data-sparseness prob-lem that hampers distributional word clustering.
"Gen-erally speaking, the problem becomes more difficult in translingual distributional word clustering, since the sparseness of data in two languages is multiplied."
"However, the sense-vs.-clue correlation matrix calcu-lation method overcomes this difficulty; it calculates the correlations between senses and clues iteratively to smooth out the sparse data."
Translingual distributional word clustering can also be implemented on the basis of word-for-word align-ment of a parallel corpus.
"However, availability of large parallel corpora is extremely limited."
"In contrast, the sense-vs.-clue correlation calculation method ac-cepts comparable corpora which are available in many domains."
"Naive translingual distributional word clustering based on the sense-vs.-clue correlation matrix calculation method is outlined in the following steps: 1) Define the sense of a target word by using each translation equivalent. 2) Calculate the sense-vs.-clue correlation matrix for the set of senses resulting from step 1). 3) Calculate similarities between senses on the basis of distribution patterns shown by the sense-vs.-clue correlation matrix. 4) Cluster senses by using a hierarchical agglomera-tive clustering method, e.g., the group-average method."
"However, this naive method is not effective be-cause some senses usually have duplicated definitions in step 1) despite the fact that the sense-vs.-clue corre-lation matrix calculation algorithm presupposes a set of senses without duplicated definitions."
"The algo-rithm is based on the “one sense per collocation” hy-pothesis, and it results in each clue having a high cor-relation with one and only one sense."
"A clue can never have high correlations with two or more senses, even when they are actually the same sense."
"Consequently, synonymous translation equivalents do not necessarily have high similarity."
"Figure 2(a) shows parts of distribution patterns for {promotion, 宣伝&lt; SENDEN &gt;}, {promotion, プロモー ション&lt; PUROMOUSHON &gt;}, and {promotion, 売り込み &lt; URIKOMI &gt;} all of which define the “sales activity” sense of “promotion”."
"We see that most clues for se-lecting that sense have higher correlation with {pro-motion, 宣伝&lt; SENDEN &gt;} than with {promotion, プロ モーション&lt; PUROMOUSHON &gt;} and {promotion, 売り 込み&lt; URIKOMI &gt;}."
This is because “宣伝&lt; SENDEN &gt;” is the most dominant translation equivalent of “promo-tion” in the corpus.
"To resolve the above problem, we calculated the sense-vs.-clue correlation matrix not only for the full set of senses but also for the set of senses excluding one of these senses."
"Excluding a definition of the sense, which includes the most dominant translation equiva-lent, allows most clues for selecting the sense to have the highest correlations with another definition of the same sense, which includes the second most dominant translation equivalent."
"Figure 2(b) shows parts of dis-tribution patterns for {promotion, プロモーション &lt; PUROMOUSHON &gt;} and {promotion, 売 り 込 み &lt; URIKOMI &gt;} shown by the sense-vs.-clue correlation matrix for the set of senses excluding {promotion, 宣 伝&lt; SENDEN &gt;}."
"We see that most clues for selecting the “sales activity” sense have higher correlations with {promotion, プロモーション&lt; PUROMOUSHON &gt;} than with {promotion, 売り込み&lt; URIKOMI &gt;}."
This is be-cause “プロモーション&lt; PUROMOUSHON &gt;” is the sec-ond most dominant translation equivalent in the corpus.
"We also see that the distribution pattern for {promo-tion, プロモーション&lt; PUROMOUSHON &gt;} in Fig. 2(b) is more similar to that for {promotion, 宣伝&lt; SENDEN &gt;} in Fig. 2(a) than that for {promotion, プロモーション &lt; PUROMOUSHON &gt;} in Fig. 2(a)."
"We call the distribution pattern for sense S 2 , result-ing from the sense-vs.-clue correlation matrix for the set of senses excluding sense S 1 , the distribution pat-tern for S 2 subordinate to S 1 , while we call the distri-bution pattern for sense S 2 , resulting from the sense-vs.-clue correlation matrix for the full set of senses, simply the distribution pattern for S 2 ."
We de-fine the similarity of S 2 to S 1 as the similarity of the distribution pattern for S 2 subordinate to S 1 to the dis-tribution pattern for S 1 .
Calculating the sense-vs.-clue correlation matrix for a set of senses excluding one sense is of course insufficient since three or more translation equivalents may represent the same sense of the target word.
"We should calculate the sense-vs.-clue correlation matrices both for the full set of senses and for the set of senses excluding one of these senses again, after merging similar senses into one."
"Repeating these procedures enables corpus-relevant but less dominant translation equivalents to be drawn up, while corpus-irrelevant ones are never drawn up."
"Thus, a hierarchy of cor-pus-relevant senses or clusters of corpus-relevant translation equivalents is produced."
"As shown in Fig. 3, our method repeats the following three steps: 1) Calculate sense-vs.-clue correlation matrices both for the full set of senses and for a set of senses ex-cluding each of these senses. 2) Calculate similarities between senses on the basis of distribution patterns and subordinate distribution patterns. 3) Merge each pair of senses with high similarity into one."
"The initial set of senses is given as Σ(x)={{x, y 1 }, {x, y 2 }, …, {x, y N }} where x is a target word in the first language, and y 1 , y 2 , …, and y N are translation equiva-lents of x in the second-language."
Translation equivalents that occur less frequently in the sec-ond-language corpus can be excluded from the initial set to shorten the processing time.
The details of the steps are described in the following sections.
"First, a sense-vs.-clue correlation matrix is calculated for the full set of senses."
"The resulting correlation ma-trix is denoted as C. That is, C(i, j) is the correlation between the i-th sense S(x,i) of a target word x and its j-th clue x’(j)."
"Then a set of active senses, Σ A (x), is determined."
"A sense is regarded active if and only if the ratio of clues with which it has the highest correlation exceeds a predetermined threshold θ (In the experiment in Sec-tion 4, θ was set to 0.05)."
"Σ A (x) = { S(x,i)| R(S(x,i)) &gt; θ } , where R(S(x, i)) denotes the ratio of clues having the highest correlation with S(x, i), i.e.,"
"R(S(x,i)) = {x&apos; ( j)|C(i,j) = maxC(k,j)} {x&apos; ( j)} . k"
Thus Σ A (x) consists of senses of the target word x that are relevant to the corpus.
"Finally, a sense-vs.-clue correlation matrix is cal-culated for the set of senses excluding each of the ac-tive senses."
The correlation matrix calculated for the set of senses excluding the k-th sense is denoted as C -k .
"That is, C -k (i, j) (i≠k) is the correlation between the i-th sense and the j-th clue that is calculated excluding the k-th sense."
"C -k (k, j) (j=1, 2, ...) are set to zero."
This redundant k-th row is included to maintain the same correspondence between rows and senses as in C.
"Similarity of the i-th sense S(x, i) to the j-th sense S(x, j), Sim(S(x, i), S(x, j)), is defined as the similarity of the distribution pattern for S(x, i) subordinate to S(x, j) to the distribution pattern of S(x, j)."
Note that this similarity is asymmetric and reflects which sense is more dominant in the corpus.
"It is probable that Sim(S(x, i), S(x, j)) is large but Sim(S(x, j), S(x, i)) is not when S(x, j) is more dominant than S(x, i)."
"According to the sense-vs.-clue correlation matrix, each sense is characterized by a weighted set of clues."
"Therefore, we used the weighted Jaccard coefficient as the similarity measure."
"That is, ∑ min { C - j (i,k),C(j,k) } Sim(S(x,i),S(x, j)) = ∑ max { C (i,k),C(j,k) } k - j k when S(x, j)∈Σ A (x)."
It should be noted that a sense is characterized by dif-ferent weighted sets of clues depending on which sense the similarity is calculated.
Note also that inac-tive senses are neglected because they are not reliable.
The set of senses is updated by merging every pair of mutually most-similar senses into one.
"Σ(x) ← Σ(x) – {S(x, i), S(x, j)} + {S(x, i)∪S(x, j)} if Sim(S(x,i),S(x, j)) = max{max j&apos; {Sim(S(x,i),S(x, j&apos; )),Sim(S(x, j&apos;), S(x,i))}} ,"
"Sim(S(x,i),S(x, j)) = max{max i&apos; {Sim(S(x,i&apos; ),S(x, j)),Sim(S(x, j),S(x,i&apos; ))}} , and Sim(S(x,i),S(x, j)) &gt; σ ."
"The σ is a predetermined threshold for similarity, which is introduced to avoid noisy pairs of senses be-ing merged."
"In the experiment in Section 4, σ was set to 0.25."
"If at least one pair of senses are merged, the whole procedure, i.e., the calculation of sense-vs.-clue ma-trices through the merger of similar senses, is repeated for the updated set of senses."
"Otherwise, the clustering procedure terminates."
Agglomerative clustering methods usually suffer from the problem of when to terminate merging.
"In our method described above, the similarity of senses that are merged into one does not necessarily decrease monotonically, which makes the problem more diffi-cult."
"At present, we are forced to output a dendrogram that represents the history of mergers and leave the final decision to humans."
The dendrogram consists of translation equivalents that are included in active senses in the final cycle.
Other translation equivalents are rejected as they are irrelevant to the corpus.
Our method was evaluated through an experiment us-ing a Wall Street Journal corpus (189 Mbytes) and a Nihon Keizai Shimbun corpus (275 Mbytes).
"First, collected pairs of related words, which we restricted to nouns and unknown words, were obtained from each corpus by extracting pairs of words co-occurring in a window, calculating mutual informa-tion of each pair of words, and selecting pairs with mutual information larger than the threshold."
"The size of the window was 25 words excluding function words, and the threshold for mutual information was set to zero."
"Second, a bilingual dictionary was prepared by collecting pairs of nouns that were translations of one another from the Japan Electronic Dictionary Research Institute (EDR) English-to-Japanese and Japa-nese-to-English dictionaries."
"The resulting dictionary includes 633,000 pairs of 269,000 English nouns and 276,000 Japanese nouns."
Evaluating the performance of word sense acquisi-tion methods is not a trivial task.
"First, we do not have a gold-standard sense inventory."
"Even if we have one, we have difficulty mapping acquired senses onto those in it."
"Second, there is no way to establish the complete set of senses appearing in a large corpus."
"Therefore, we evaluated our method on a limited number of target words as follows."
We prepared a standard sense inventory by select-ing 60 English target words and defining an average of 3.4 senses per target word manually.
"The senses were rather coarse-grained; i.e., they nearly corresponded to groups of translation equivalents within the entries of everyday English-Japanese dictionaries."
"We then sam-pled 100 instances per target word from the Wall Street Journal corpus, and we sense-tagged them manually."
"Thus, we estimated the ratios of the senses in the training corpus for each target word."
"We defined two evaluative measures, recall of senses and accuracy of sense definitions."
"The recall of senses is the proportion of senses with ratios not less than a threshold that are successfully extracted, and it varies with change of the threshold."
"We judged that a sense was extracted, when it shared at least one trans-lation equivalent with some active sense in the final cycle."
"To evaluate the accuracy of sense definitions while avoiding mapping acquired senses onto those in the standard sense inventory, we regard a set of senses as a set of pairs of synonymous translation equivalents."
Let T S be a set consisting of pairs of translation equiva-lents belonging to the same sense in the standard sense inventory.
"Likewise, let T(k) be a set consisting of pairs of translation equivalents belonging to the same active sense in the k-th cycle."
"Further, let U be a set of pairs of translation equivalents that are included in active senses in the final cycle."
Recall and precision of pairs of synonymous translation equivalents in the k-th cycle are defined as:
R(k) = T S ∩T(k) .
T S ∩U P(k) =
T S ∩T(k) .
"Further, F-measure of pairs of synonymous translation equivalents in the k-th cycle is defined as:"
F(k) = 2 ⋅ R(k) ⋅ P(k) .
R(k) + P(k)
The F-measure indicates how well the set of active senses coincides with the set of sense definitions in the standard senses inventory.
"Although the current method cannot determine the optimum cycle, humans can identify the set of appropriate senses from a hier-archy of senses at a glance."
"Therefore, we define the accuracy of sense definitions as the maximum F-measure in all cycles."
"To simplify the evaluation procedure, we clustered translation equivalents that were used to define the senses of each target word in the standard sense in-ventory, rather than clustering translation equivalents rendered by the EDR bilingual dictionary."
"The recall of senses for totally 201 senses of the 60 target words was: 96% for senses with ratios not less than 25%, 87% for senses with ratios not less than 5%, and 78% for senses with ratios not less than 1%."
"The accuracy of sense definitions, averaged over the 60 target words, was 77%."
The computational efficiency of our method proved to be acceptable.
Some clustering results are shown in Fig. 4.
These demonstrate that our proposed method shows a great deal of promise.
"At the same time, evaluating the re-sults revealed its deficiencies."
The first of these lies in the crucial role of the bilingual dictionary.
It is obvious that a sense is never extracted if the translation equiva-lents representing it are not included in it.
An exhaustive bilingual dictionary is therefore required.
"From this point of view, the EDR bilingual dictionary is fairly good."
The second deficiency lies in the fact that it performs badly for low-frequency or non-topical senses.
"For example, the sense of “bar” as the “legal profession” was clearly extracted, but its sense as a “piece of solid material” was not extracted."
We also compared our method with two alterna-tives: monolingual distributional clustering mentioned in Section 2.2 and naive translingual clustering men-tioned in Section 2.3.
"Figures 5(a), (b), and (c) show respective examples of clustering obtained by our method, the monolingual method, and the naive translingual method."
"Comparing (a) with (b) reveals the superiority of the translingual approach to the monolingual approach, and comparing (a) with (c) reveals the effectiveness of the subordinate distribu-tion pattern introduced in Section 2.3."
Note that delet-ing the corpus-irrelevant translation equivalents from the dendrograms in both (b) and (c) would not result in appropriate ones.
Our method has several practical advantages.
One of these is that it produces a corpus-dependent inventory of word senses.
"That is, the resulting inventory covers most senses relevant to a domain, while it excludes senses irrelevant to the domain."
"Second, our method unifies word sense acquisition with word sense disambiguation."
The sense-vs.-clue correlation matrix is originally used for word sense disambiguation.
"Therefore, our method guarantees that acquired senses can be distinguished by machines, and further it demonstrates the possibility of automatically optimizing the granularity of word senses."
Some limitations of the present methods are dis-cussed in the following with possible future extensions.
"First, our method produces a hierarchy of clusters but cannot produce a set of disjoint clusters."
It is very im-portant to terminate merging senses autonomously during an appropriate cycle.
Comparing distribution patterns (not subordinate ones) may be useful to ter-minate merging; senses characterized by complemen-tary distribution patterns should not be merged.
"Second, the present method assumes that each translation equivalent represents one and only one sense of the target word, but this is not always the case."
A Japanese Katakana word resulting from translitera-tion of an English word sometimes represents multiple senses of the English word.
It is necessary to detect and split translation equivalents representing more than one sense of the target word.
"Third, not only are acquired senses rather coarse-grained but also generic senses are difficult to acquire."
One of the reasons for this may be that we rely on co-occurrence in the window.
The fact that most distributional word clustering methods use syn-tactic co-occurrence suggests that it is the most effec-tive tool for extracting pairs of related words.
"We presented a translingual distributional word clus-tering method enabling word senses, exactly a hierar-chy of clusters of translation equivalents, to be ac-quired from a comparable corpus and a bilingual dic-tionary."
Its effectiveness was demonstrated through an experiment using Wall Street Journal and Nihon Kei-zai Shimbun corpora and the EDR bilingual dictionary.
"The recall of senses was 87% for senses whose ratios in the corpus were not less than 5%, and the accuracy of sense definitions was 77%."
Acknowledgments: This research was supported by the New Energy and Industrial Technology Develop-ment Organization of Japan.
We present an extension of the classic A* search procedure to tabular PCFG parsing.
The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions.
We discuss vari-ous estimates and give efficient algorithms for com-puting them.
"On average-length Penn treebank sen-tences, our most detailed estimate reduces the to-tal number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of pre-computation, reduces the work to less than 5%."
"Un-like best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation."
"Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time."
PCFG parsing algorithms with worst-case cubic-time bounds are well-known.
"However, when dealing with wide-coverage grammars and long sentences, even cu-bic algorithms can be far too expensive in practice."
Two primary types of methods for accelerating parse selec-tion have been proposed.
Parsing time is lin-ear and can be made arbitrarily fast by reducing n.
"This is a greedy strategy, and the actual Viterbi (highest proba-bility) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at ev-ery parse stage."
"In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed."
"This approach also dramat-ically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cu-bic time bound)."
We discuss best-first parsing further in section 3.3.
Both of these speed-up techniques are based on greedy models of parser actions.
"The beam search greedily prunes partial parses at each beam stage, and a best-first FOM greedily orders parse item exploration."
"If we wish to maintain optimality in a search procedure, the obvious thing to try is A* methods (see for example[REF_CITE])."
"We apply A* search to a tabular item-based parser, ordering the parse items based on a com-bination of their known internal cost of construction and a conservative estimate of their cost of completion (see figure 1)."
"A* search has been proposed and used for speech applications ([REF_CITE]); however, it has been little used, certainly in the recent statistical parsing literature, apparently because of difficulty in conceptualizing and computing effective ad-missible estimates."
"The contribution of this paper is to demonstrate effective ways of doing this, by precomput-ing grammar statistics which can be used as effective A* estimates."
The A* formulation provides three benefits.
"First, it substantially reduces the work required to parse a sen-tence, without sacrificing either the optimality of the an-swer or the worst-case cubic time bounds on the parser."
"Second, the resulting parser is structurally simpler than a FOM-driven best-first parser."
"Finally, it allows us to eas-ily prove the correctness of our algorithm, over a broad range of control strategies and grammar encodings."
"In this paper, we describe two methods of construct-ing A* bounds for PCFGs."
"One involves context sum-marization, which uses estimates of the sort proposed[REF_CITE], but considering richer summaries."
"The other involves grammar summarization, which, to our knowledge, is entirely novel."
"We present the esti-mates that we use, along with algorithms to efficiently calculate them, and illustrate their effectiveness in a tab-ular PCFG parsing algorithm, applied to Penn Treebank sentences."
"An agenda-based PCFG parser operates on parse items called edges, such as NP :[0,2], which denote a grammar symbol over a span."
"The parser maintains two data struc-tures: a chart or table, which records edges for which (best) parses have already been found, and an agenda of newly-formed edges waiting to be processed."
The core loop involves removing an edge from the agenda and combining that edge with edges already in the chart to create new edges.
"For example, NP :[0,2] might be re-moved from the agenda, and, if there were a rule S → NP VP and VP :[2,8] was already entered into the chart, the edge S : [0,8] would be formed, and added to the agenda if it were not in the chart already."
"The way an A* parser differs from a classic chart parser is that, like a best-first parser, agenda edges are processed according to a priority."
"In best-first parsing, this priority is called a figure-of-merit (FOM), and is based on various approximations to P(e|s), the frac-tion of parses of a sentence s which include an edge e (though see[REF_CITE]for an alternative notion of FOM)."
Edges which seem promising are explored first; others can wait on the agenda indefinitely.
"Note that even if we did know P(e|s) exactly, we still would not know whether e occurs in any best parse of s. Nonethe-less, good FOMs empirically lead quickly to good parses."
"Best-first parsing aims to find a (hopefully good) parse quickly, but gives no guarantee that the first parse discov-ered is the Viterbi parse, nor does it allow one to recog-nize the Viterbi parse when it is found."
"In A* parsing, we wish to construct priorities which will speed up parsing, yet still guarantee optimality (that the first parse returned is indeed a best parse)."
"With a categorical CFG chart parser run to exhaustion, it does not matter in what order one removes edges from the agenda; all edges involved in full parses of the sentence will be constructed at some point."
"A cubic time bound follows straightforwardly by simply testing for edge exis-tence, ensuring that we never process an edge twice."
"With PCFG parsing, there is a subtlety involved."
"In addition to knowing whether edges can be constructed, we also want to know the scores of edges’ best parses."
"Therefore, we record estimates of best-parse scores, updating them as better parses are found."
"If, during parsing, we find a new, better way to construct some edge e that has previously been entered into the chart, we may also have found a bet-ter way to construct any edges which have already been built using e. Best-first parsers deal with this by allowing an upward propagation, which updates such edges’ scores[REF_CITE]."
"If run to exhaustion, all edges’ Viterbi scores will be correct, but the propagation destroys the cubic time bound of the parser, since in effect each edge can be processed many times."
"In order to ensure optimality, it is sufficient that, for any edge e, all edges f which are contained in a best parse of e get removed from the agenda before e itself does."
"If we have an edge priority which ensures this or-dering, we can avoid upward propagation entirely (and omit the data structures involved in it) and still be sure that each edge leaves the agenda scored correctly."
"If the grammar happens to be in CNF, one way to do this is to give shorter spans higher priority than longer ones; this priority essentially gives the CKY algorithm."
"Formally, assume we have a PCFG G and a sentence s = 0 w n (we place indices as fenceposts between words)."
"An inside parse of an edge e = X:[i, j] is a derivation in G from X to i w j ."
"Let β G (e, s) denote the log-probability of a best inside parse of e (its Viterbi inside score). [Footnote_1]"
"1 Our use of inside score and outside score evokes the same picture as talk about inside and outside probabilities, but note that in this paper inside and outside scores always refer to (a bound on) the maximum (Viterbi) probability parse inside or outside some edge, rather than to the sum for all such parses."
"We will drop the G, s, and even e when context permits."
"Our parser, like a best-first parser, maintains estimates b(e, s) of β(e, s) which begin at −∞, only increase over time, and always represent the score of the best parses of their edges e discovered so far."
"Optimality means that for any e, b(e, s) will equal β G (e, s) when e is removed from the agenda."
"If one uses b(e, s) to prioritize edges, we show[REF_CITE], that the parser is optimal over ar-bitrary PCFGs, and a wide range of control strategies."
"This is proved using an extension of Dijkstra’s algorithm to a certain kind of hypergraph associated with parsing, shown in figure 1(b): parse items are nodes in the hyper-graph, hyperarcs take sets of parse items to their result item, and hyperpaths map to parses."
"Reachability from start corresponds to parseability, and shortest paths to Viterbi parses."
"The hypergraph shown in figure 1(b) shows a parse of the goal S : [0,3] which includes NP :[0,2]. [Footnote_2]"
"2 The example here shows a bottom-up construction of a parse tree. However, the present algorithm and estimates work just as well for top-down chart parsing, given suitable active items as nodes; see[REF_CITE]."
"This parse can be split into an inside portion (solid lines) and an outside portion (dashed lines), as indicated in figure 1(a)."
"The outside portion is an outside parse: formally, an outside parse of an edge X:[i, j] in sentence s = 0 w n is a deriva-tion from G’s root symbol to w 0i"
Xw jn .
"We use α G (e, s) to denote the score of a best outside parse of e."
"Using b(e, s) as the edge priority corresponds to a gen-eralization of uniform cost search on graphs[REF_CITE]."
"In the analogous generalization of A* search, we add to b(e, s) an estimate a(e, s) of the com-petion cost α G (e, s) (the cost of the dashed outside parse) to focus exploration on regions of the graph which appear to have good total cost."
A* search is correct as long as the estimate a satis-fies two conditions.
"First, it must be admissible, meaning that it must not underestimate the actual log-probability required to complete the parse."
"Second, it must be mono-tonic, meaning that as one builds up a tree, the combined log-probability β + a never increases."
"The proof of this is very similar to the proof of the uniform-cost case[REF_CITE], and so we omit it for space reasons (it can be found[REF_CITE])."
"Concretely, we can use b + a as the edge priority, pro-vided a is an admissible, monotonic estimate of α."
"We will still have a correct algorithm, and even rough heuris-tics can dramatically cut down the number of edges pro-cessed (and therefore total work)."
"We next discuss several estimates, describe how to compute them efficiently, and show the edge savings when parsing Penn treebank WSJ sentences."
"When parsing with a PCFG G, each edge e = X:[i,j] spans some interval [i, j] of the sentence and is labeled by some grammar symbol (or state) X. Our presentation assumes that G is a binarized grammar, and so in gen-eral X may be either a complete state like NP that was in an original n-ary grammar, or an intermediate state, like an Earley dotted rule, that is the result of implicit or explicit grammar binarization."
"For the edge e, its yield in s = 0 w n is the sequence of terminals that it spans ( i w j )."
Its context is its state X along with the rest of the terminals of sentence ( 0 w i X j w n ).
Scores are log-probabilities; lower cost is higher log-probability.
"So, ‘&gt;’ or ‘better’ will mean higher log-probability."
"One way to construct an admissible estimate is to sum-marize the context in some way, and to find the score of the best parse of any context that fits that summary."
"Let c(e, s) be the context of e in s. Let σ be a summary func-tion of contexts."
"We can then use the context summary estimate: a σ (e, s) = α G (e 0 , s 0 ) ≥ α G (e, s)max (e 0 ,s 0 ):σ(c(e 0 ,s 0 ))= σ(c(e,s))"
"That is, we return the exact Viterbi outside score for some context, generally not the actual context, whose summary matches the actual one’s summary."
"If the number of sum-maries is reasonable, we can precompute and store the estimate for each summary once and for all, then retrieve them in constant time per edge at parse time."
"If we give no information in the summary, the estimate will be constantly 0."
"This is the trivial estimate NULL , and corresponds to simply using inside estimates b alone as priorities."
"On the other extreme, if each context had a unique summary, then a(e, s) would be α G (e, s) itself."
"This is the ideal estimate, which we call TRUE ."
"In prac-tice, of course, precomputing TRUE would not be feasi-ble. [Footnote_3]"
"3 Note that our ideal estimate is not P(e|s) like the ideal FOM, rather it is P(T g,e )/P(T e ) (where T g,e is a best parse of the goal g among those which contain e, and T e is a best parse of e over the yield of e). That is, we are not estimating parser choice probabilities, but parse tree probabilities."
"We used various intermediate summaries, some illus-trated in figure 2."
"S 1 specifies only the total number of words outside e, while S specifies separately the number to the left and right."
SX also specifies e’s label.
SXL and SXR add the tags adjacent to e on the left and right re-spectively.
"S 1 XLR includes both the left and right tags, but merges the number of words to the left and right. [Footnote_4]"
4 Merging the left and right outside span sizes in S 1 XLR was done solely to reduce memory usage.
"As the summaries become richer, the estimates become sharper."
"As an example, consider an NP in the context “ VBZ NP , PRP VBZ DT NN .” shown in figure 2. [Footnote_5]"
"5 Our examples, and our experiments, use delexicalized sen-tences from the Penn treebank."
"The summary SX reveals only that there is an NP with 1 word to the left and 6 the right, and gives an estimate of −11.3."
This score is backed by the concrete parse shown in fig-ure 2(a).
"This is a best parse of a context compatible with what little we specified, but very optimistic."
It assumes very common tags in very common patterns.
"SXL adds that the tag to the left is VBZ , and the hypothesis that the NP is part of a sentence-initial PP must be abandoned; the best score drops to −13.9, backed by the parse in fig-ure 2(b)."
"Specifying the right tag to be “,” drops the score further to −15.1, given by figure 2(c)."
"The actual best parse is figure 2(d), with a score of −18.1."
"These estimates are similar to quantities calculated[REF_CITE]; in that work, they are interested in the related problem of finding best completions for strings which contain gaps."
"For the SX estimate, for example, the string would be the edge’s label and two (fixed-length) gaps."
"They introduce quantities essentially the same as our SX estimate to fill gaps, and their one-word update algorithms are similarly related to those we use here."
"The primary difference here is in the application of these quantities, not their calculation."
"The context summary estimates described above use local information, combined with span sizes."
"This gives the effect that, for larger contexts, the best parses which back the estimates will have less and less to do with the actual contexts (and hence will become increasingly optimistic)."
"Context summary estimates do not pin down the exact context, but do use the original grammar G."
"For grammar projection estimates, we use the exact context, but project the grammar to some G 0 which is so much simpler that it is feasible to first exhaustively parse with G 0 and then use the result to guide the search in the full grammar G."
"Formally, we have a projection π which maps gram-mar states of G (that is, the dotted rules of an Earley-style parser) to some reduced set."
This projection of states in-duces a projection of rules.
"If a set R = {r} of rules in G collide as the rule r 0 in G 0 , we give r 0 the probability"
P(r 0 ) = max r∈R P(r).
Note that the resulting grammar G 0 will not generally be a proper PCFG; it may assign more than probability 1 to the set of trees it generates.
"In fact, it will usually assign infinite mass."
"However, all that matters for our purposes is that every tree in G projects under π to a tree in G 0 with the same or higher probabil-ity, which is true because every rule in G does."
"There-fore, we know that α G (e, s) ≤ α G 0 (e, s)."
"If G 0 is much more compact than G, for each new sentence s, we can first rapidly calculate a π = α G 0 for all edges, then parse with G."
The identity projection ι returns G and therefore a ι is TRUE .
"On the other extreme, a constant projection gives NULL (if any rewrite has probability 1)."
"In between, we tried three other grammar projection estimates (examples in figure 3)."
"First, consider mapping all terminal states to a single terminal token, but not altering the grammar in any other way."
"If we do this projection, then we get the SX estimate from the last section (collapsing the termi-nals together effectively hides which terminals are in the context, but not their number)."
"However, the resulting grammar is nearly as large as G, and therefore it is much more efficient to use the precomputed context summary formulation."
"Second, for the projection XBAR , we tried collapsing all the incomplete states of each complete state to a single state (so NP → · CC NP and NP → · PP would both become NP 0 )."
"This turned out to be ineffective, since most productions then had merged probability 1."
"For our current grammar, the best estimate of this type was one we called F , for filter, which collapsed all com-plete (passive) symbols together, but did not collapse any terminal symbols."
"So, for example, a state like NP → · CC NP CC NP would become X → · CC X CC X (see section 3.3 for a description of our grammar encodings)."
This esti-mate has an interesting behavior which is complementary to the context summary estimates.
"It does not indicate well when an edge would be moderately expensive to in-tegrate into a sentence, but it is able to completely elimi-nate certain edges which are impossible to integrate into a full parse (for example in this case maybe the two CC tags required to complete the NP are not present in the future context)."
A close approximation to the F estimate can also be computed online especially quickly during parsing.
"Since we are parsing with the Penn treebank covering gram-mar, almost any (phrasal) non-terminal can be built over almost any span."
"As discussed[REF_CITE], the only source of constraint on what edges can be built where is the tags in the rules."
"Therefore, an edge with a label like NP → · CC NP CC NP can essentially be built whenever (and only whenever) two CC tags are in the edge’s right context, one of them being immediately to the right."
"To the extent that this is true, F can be ap-proximated by simply scanning for the tag configuration required by a state’s local rule, and returning 0 if it is present and −∞ otherwise."
This is the method we used to implement F ; exactly parsing with the projected gram-mar was much slower and did not result in substantial improvement.
"It is worth explicitly discussing how the F estimate dif-fers from top-down grammar-driven filtering standardly used by top-down chart parsers; in the treebank grammar, there is virtually no top-down filtering to be exploited (again, see[REF_CITE])."
"In a left-to-right parse, top-down filtering is a prefix licensing condition; F is more of a sophisticated lookahead condition on suf-fixes."
The relationships between all of these estimates are shown in figure 4.
The estimates form a join lattice (fig-ure 4(a)): adding context information to a merged con-text estimate can only sharpen the individual outside es-timates.
"In this sense, for example S ≺ SX ."
The lattice top is TRUE and the bottom is NULL .
"In addition, the minimum (t) of a set of admissible estimates is still an admissible estimate."
"We can use this to combine our ba-sic estimates into composite estimates: SXMLR = t ( SXL , SXR ) will be valid, and a better estimate than either SXL or SXR individually."
"Similarly, B is t ( SXMLR , S 1 XLR )."
"There are other useful grammar projections, which are beyond the scope of this paper."
"First, much recent statisti-cal parsing work has gotten value from splitting grammar states, such as by annotating nodes with their parent and even grandparent categories[REF_CITE]."
"This anno-tation multiplies out the state space, giving a much larger grammar, and projecting back to the unannotated state set can be used as an outside estimate."
"Second, and perhaps more importantly, this technique can be applied to lexical parsing, where the state projections are onto the delex-icalized PCFG symbols and/or onto the word-word de-pendency structures."
This is particularly effective when the tree model takes a certain factored form; see[REF_CITE]for details.
"Following[REF_CITE], we parsed unseen sen-tences of length 18–26 from the Penn Treebank, using the grammar induced from the remainder of the treebank. [Footnote_6] We tried all estimates described above."
"6 We chose the data set used by Charniak and coauthors, so as to facilitate comparison with previous work. We do however acknowledge that many of our current local estimates are less effective on longer spans, and so would work less well on 40– 50 word sentences. This is an area of future research."
"Rules were encoded as both inside (I) and outside (O) tries, shown in figure 5."
"Such an encoding binarizes the grammar, and compacts it."
"I-tries are as[REF_CITE], where NP → DT JJ NN becomes NP → X DT JJ NN and X DT JJ → DT JJ , and correspond to dropping the portion of an Earley dotted rule after the dot. [Footnote_7] O-tries, as[REF_CITE], turn NP → DT JJ NN into NP → X NP → · NN NN and X NP → · NN → DT JJ , and correspond to dropping the portion which precedes the dot."
"7[REF_CITE], the binarization is in the reverse direction; we binarize into a left chain because it is the standard direction implicit in chart parsers’ dotted rules, and the direction makes little difference in edge counts."
Figure 6 shows the overall savings for several estimates of each type.
"The I-tries were superior for the coarser estimates, while O-tries were superior for the finer estimates."
"In addition, only O-tries permit the accelerated version of F , since they explicitly declare their right requirements."
"Additionally, with I-tries, only the top-level intermedi-ate rules have probability less than 1, while for O-tries, one can back-weight probability as[REF_CITE], also shown in figure 5, enabling sub-parts of rare rules to be penalized even before they are completed. 8"
"For all sub-sequent results, we discuss only the O-trie numbers."
"Figure 8 lists the overall savings for each context sum-mary estimate, with and without F joined in."
"We see that the NULL estimate (i.e., uniform cost search) is not very effective – alone it only blocks 11% of the edges."
"But it is still better than exhaustive parsing: with it, one stops parsing when the best parse is found, while in exhaustive parsing one continues until no edges remain."
"Even the simplest non-trivial estimate, S , blocks 40% of the edges, and the best estimate BF blocks over 97% of the edges, a speed-up of over 35 times, without sacrificing optimality or algorithmic complexity."
"For comparison to previous FOM work, figure 7 shows, for an edge count and an estimate, the propor-tion of sentences for which a first parse was found us-ing at most that many edges."
"To situate our results, the FOMs used[REF_CITE]require 10K edges to parse 96% of these sentences, while BF re-quires only 6K edges."
"On the other hand, the more com-plex, tuned FOM[REF_CITE]is able to parse all of these sentences using around 2K edges, while BF requires 7K edges."
"Our estimates do not reduce the to-tal edge count quite as much as the best FOMs can, but they are in the same range."
"This is as much as one could possibly expect, since, crucially, our first parses are al- ways optimal, while the FOM parses need not be (and indeed sometimes are not). [Footnote_9] Also, our parser never needs to propagate score changes upwards, and so may be ex-pected to do less work overall per edge, all else being equal."
"9 In fact, the bias from the FOM commonly raises the bracket accuracy slightly over the Viterbi parses, but that difference nev-ertheless demonstrates that the first parses are not always the Viterbi ones. In our experiments, non-optimal pruning some-times bought slight per-node accuracy gains at the cost of a slight drop in exact match."
"This savings is substantial, even if no propaga-tion is done, because no data structure needs to be cre-ated to track the edges which are supported by each given edge (for us, this represents a factor of approximately 2 in memory savings)."
"Moreover, the context summary estimates require only a single table lookup per edge, while the accelerated version of F requires only a rapid quadratic scan of the input per sentence (less than 1% of parse time per sentence), followed by a table lookup per edge."
The complex FOMs[REF_CITE]re-quire somewhat more online computation to assemble.
It is interesting that SXR is so much more effective than SXL ; this is primarily because of the way that the rules have been encoded.
"If we factor the rules in the other direction, we get the opposite effect."
"Also, when com-bined with F , the difference in their performance drops from 10.3% to 0.[Footnote_8]%; F is a right-filter and is partially redundant when added to SXR , but is orthogonal to SXL ."
"8 However, context summary estimates which include the state compensate for this automatically."
"A disadvantage of admissibility for the context summary estimates is that, necessarily, they are overly optimistic as to the contents of the outside context."
"The larger the outside context, the farther the gap between the true cost and the estimate."
Figure 9 shows average outside esti-mates for Viterbi edges as span size increases.
"For small outside spans, all estimates are fairly good approxima-tions of TRUE ."
"As the span increases, the approximations fall behind."
"Beyond the smallest outside spans, all of the curves are approximately linear, but the actual value’s slope is roughly twice that of the estimates."
"The gap between our empirical methods and the true cost grows fairly steadily, but the differences between the empirical methods themselves stay relatively constant."
"This reflects the nature of these estimates: they have differing local in-formation in their summaries, but all are equally ignorant about the more distant context elements."
"The various lo-cal environments can be more or less costly to integrate into a parse, but, within a few words, the local restric-tions have been incorporated one way or another, and the estimates are all free to be equally optimistic about the remainder of the context."
"The cost to “package up” the local restrictions creates their constant differences, and the shared ignorance about the wider context causes their same-slope linear drop-off."
"This suggests that it would be interesting to explore other, more global, notions of context."
"We do not claim that our context estimates are the best possible – one could hope to find features of the context, such as number of verbs to the right or number of unusual tags in the context, which would partition the contexts more effectively than adjacent tags, especially as the outside context grows in size."
The amount of work required to (pre)calculate context summary estimates depends on how easy it is to effi-ciently take the max over all parses compatible with each context summary.
The benefit provided by an estimate will depend on how well the restrictions in that summary nail down the important features of the full context.
"To precalculate our A* estimates efficiently, we used a memoization approach rather than a dynamic programming approach."
"This re-sulted in code comparable in efficiency, but which was simpler to reason about, and, more importantly, allowed us to exploit sparseness when present."
"For example with left-factored trie encodings, 76% of (state, right tag) com-binations are simply impossible."
Tables which mapped arguments to returned results were used to memoize each procedure.
"In our experiments, we forced these tables to be filled in a precomputation step, but depending on the situation it might be advantageous to allow them to fill as needed, with early parses proceeding slowly while the tables populate."
"With the optimal forward estimate TRUE , the actual distance to the closest goal, we would never expand edges other than those in best parses, but computing TRUE is as hard as parsing the sentence in the first place."
"On the other hand, no precomputation is needed for NULL ."
In between is a trade off of space/time requirements for pre-computation and the online savings during the parsing of new sentences.
"Figure 8 shows the average savings ver-sus the precomputation time. [Footnote_10] Where on this curve one chooses to be depends on many factors; 9 hours may be too much to spend computing B , but an hour for SXMLR gives nearly the same performance, and the one minute required for SX is comparable to the I/O time to read the Penn treebank in our system."
10 All times are for a Java implementation running on a 2[REF_CITE]MHz Intel machine.
"The grammar projection estimate F had to be recom-puted for each sentence parsed, but took less than 1% of the total parse time."
"Although this method alone was less effective than SX (only 58.3% edge savings), it was ex-tremely effective in combination with the context sum-mary methods."
"In practice, the combination of F and SX is easy to implement, fast to initialize, and very effective: one cuts out 95% of the work in parsing at the cost of one minute of precomputation and 5 Mb of storage for outside estimates for our grammar."
"While the A* estimates given here can be used to accel-erate PCFG parsing, most high-performance parsing has utilized models over lexicalized trees."
These A* methods can be adapted to the lexicalized case.
"In that model, the score of a lexicalized tree is the product of the scores of two projections of that tree, one onto unlexicalized phrase structure, and one onto phrasal-category-free word-to-word dependency structure."
"Since this model has a projection-based form, grammar projec-tion methods are easy to apply and especially effective, giving over three orders of magnitude in edge savings."
"The total cost per sentence includes the time required for two exhaustive PCFG parses, after which the A* search takes only seconds, even for very long sentences."
"Even when a lexicalized model is not in this factored form, it still admits factored grammar projection bounds; we are currently investigating this case."
"An A* parser is simpler to build than a best-first parser, does less work per edge, and provides both an optimality guarantee and a worst-case cubic time bound."
We have described two general ways of constructing admissible A* estimates for PCFG parsing and given several specific estimates.
"Using these estimates, our parser is capable of finding the Viterbi parse of an average-length Penn tree-bank sentence in a few seconds, processing less than 3% of the edges which would be constructed by an exhaustive parser."
"We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previ-ously proposed phrase-based translation mod-els."
"Within our framework, we carry out a large number of experiments to understand bet-ter and explain why phrase-based models out-perform word-based models."
"Our empirical re-sults, which hold for all examined language pairs, suggest that the highest levels of perfor-mance can be obtained through relatively sim-ple means: heuristic learning of phrase trans-lations from word-based alignments and lexi-cal weighting of phrase translations."
"Surpris-ingly, learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance."
Learning only syntac-tically motivated phrases degrades the perfor-mance of our systems.
Various researchers have improved the quality of statis-tical machine translation system with the use of phrase translation.
Och et al. [1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntax-based translation system; Marcu and Wong [2002] in-troduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems [Footnote_1] are augmented with phrase transla-tion capability.
"1 Presentations at DARPA IAO Machine Translation Work-shop,[REF_CITE]-23, 2002, Santa Monica, CA"
"Phrase translation clearly helps, as we will also show with the experiments in this paper."
But what is the best method to extract phrase translation pairs?
"In order to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table."
Our experiments show that high levels of performance can be achieved with fairly simple means.
"In fact, for most of the steps necessary to build a phrase-based system, tools and resources are freely available for re-searchers in the field."
More sophisticated approaches that make use of syntax do not lead to better performance.
"In fact, imposing syntactic restrictions on phrases, as used in recently proposed syntax-based translation models [[REF_CITE]], proves to be harmful."
"Our ex-periments also show, that small phrases of up to three words are sufficient for obtaining high levels of accuracy."
Performance differs widely depending on the methods used to build the phrase translation table.
We found ex-traction heuristics based on word alignments to be better than a more principled phrase-based alignment method.
"However, what constitutes the best heuristic differs from language pair to language pair and varies with the size of the training corpus."
"In order to compare different phrase extraction methods, we designed a uniform framework."
We present a phrase translation model and decoder that works with any phrase translation table.
The phrase translation model is based on the noisy chan-nel model.
We use Bayes rule to reformulate the transla-tion probability for translating a foreign sentence into English as argmax  argmax  
This allows for a language model  and a separate translation model  .
"During decoding, the foreign input  sentence is seg-mented into a sequence of phrases ."
We assume a uniform probability distribution over all possible segmen-   tations.
Each foreign phrase in is translated into an En-glish phrase .
The English phrases may be reordered.
Phrase translation is modeled by a probability distribution .
"Recall that due to the Bayes rule, the translation direction is inverted from a modeling standpoint."
"Reordering of the English output phrases is modeled  by  a relative distortion probability distribution , where denotes the start position of the foreign phrase  that was translated into the th English phrase, and denotes the  end position of the foreign phrase trans-lated into the  th English phrase."
"In all our experiments, the distortion probability distri-bution  is trained using a joint probability model (see Section 3.3)."
"Alternatively  , we could  also  &quot; use ! a simpler distortion model with an appropriate value for the parameter ."
"In order to calibrate the output length, we introduce a factor # for each generated English word in addition to the trigram language model LM ."
This is a simple means to optimize performance.
"Usually, this factor is larger than 1, biasing longer output."
"In summary, the best English output sentence best given a foreign input sentence according to our model is best argmax  argmax  LM   # length $ % where  is decomposed into & apos;  &amp;  &amp; * )("
"For all our experiments we use the same training data, trigram language model [[REF_CITE]], and a specialized decoder."
"The phrase-based decoder we developed for purpose of comparing different phrase-based translation models em-ploys a beam search algorithm, similar to the one by Je-linek [1998]."
The English output sentence is generated left to right in form of partial translations (or hypothe-ses).
We start with an initial empty hypothesis.
A new hy-pothesis is expanded from an existing hypothesis by the translation of a phrase as follows: A sequence of un-translated foreign words and a possible English phrase translation for them is selected.
The English phrase is at-tached to the existing English output sequence.
The for-eign words are marked as translated and the probability cost of the hypothesis is updated.
The cheapest (highest probability) final hypothesis with no untranslated foreign words is the output of the search.
The hypotheses are stored in stacks.
"The stack + , contains all hypotheses in which - foreign words have been translated."
We recombine search hypotheses as done by Och et al. [2001].
"While this reduces the number of hypotheses stored in each stack somewhat, stack size is exponential with respect to input sentence length."
This makes an exhaustive search impractical.
"Thus, we prune out weak hypotheses based on the cost they incurred so far and a future cost estimate."
"For each stack, we only keep a beam of the best . hypotheses."
"Since the future cost estimate is not perfect, this leads to search errors."
"Our future cost estimate takes into account the estimated phrase translation cost, but not the expected distortion cost."
"We compute this estimate as follows: For each possi-ble phrase translation anywhere in the sentence (we call it a translation option), we multiply its phrase translation probability with the language model probability for the generated English phrase."
"As language model probabil-ity we use the unigram probability for the first word, the bigram probability for the second, and the trigram proba-bility for all following words."
"Given the costs for the translation options, we can com-pute the estimated future cost for any sequence of con-secutive foreign words by dynamic programming."
"Note that this is only possible, since 3254 we ignore distortion costs."
"Since there are only . /.10 such sequences for a foreign input sentence of length . , we can pre-compute these cost estimates beforehand and store them in a table."
"During translation, future costs for uncovered foreign words can be quickly computed by consulting this table."
"If a hypothesis has broken sequences of untranslated for-eign words, we look up the cost for each sequence and take the product of their costs."
"The beam size, e.g. the maximum number of hypothe-ses in each stack, is fixed to a certain number."
The number of translation options is linear with the sentence length.
"Hence, the time complexity of the beam search is quadratic with sentence length, and linear with the beam size."
"Since the beam size limits the search space and there-fore search quality, we have to find the proper trade-off between speed (low beam size) and performance (high beam size)."
"For our experiments, a beam size of only 100 proved to be sufficient."
"With larger beams sizes, only few sentences are translated differently."
"With our decoder, translating 1755 sentence of length 5-15 words takes about 10 minutes on a 2 GHz Linux system."
"In other words, we achieved fast decoding, while ensuring high quality."
We carried out experiments to compare the performance of three different methods to build phrase translation probability tables.
We also investigate a number of varia-tions.
"We report most experimental results on a German-English translation task, since we had sufficient resources available for this language pair."
We confirm the major points in experiments on additional language pairs.
"As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [[REF_CITE]] toolkit for the IBM models [[REF_CITE]]."
The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999].
A number of researchers have proposed to focus on the translation of phrases that have a linguistic motiva-tion [[REF_CITE]].
"They only consider word sequences as phrases, if they are con-stituents, i.e. subtrees in a syntax tree (such as a noun phrase)."
"To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntac-tic parsers [[REF_CITE]; Schmidt and Schulte im[REF_CITE]]."
The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002].
This model learns directly a phrase-level alignment of the parallel corpus.
The Giza++ toolkit was developed to train word-based translation models from parallel corpora.
"As a by-product, it generates word alignments for this data."
"We improve this alignment with a number of heuristics, which are described in more detail in Section 4.5."
"We collect all aligned phrase pairs that are consistent with the word alignment: The words in a legal phrase pair are only aligned to each other, and not to words outside [[REF_CITE]]."
"Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative fre-quency:  count count"
No smoothing is performed.
"If we collect all phrase pairs that are consistent with word alignments, this includes many non-intuitive phrases."
"For instance, translations for phrases such as “house the” may be learned."
Intuitively we would be inclined to be-lieve that such phrases do not help: Restricting possible phrases to syntactically motivated phrases could filter out such non-intuitive pairs.
Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic trans-lation models [[REF_CITE]].
"In these models, reordering of words is restricted to reorder-ing of constituents in well-formed syntactic parse trees."
"When augmenting such models with phrase translations, typically only translation of phrases that span entire syn-tactic subtrees is possible."
It is important to know if this is a helpful or harmful restriction.
"Consistent with Imamura [2002], we define a syntac-tic phrase as a word sequence that is covered by a single subtree in a syntactic parse tree."
"We collect syntactic phrase pairs as follows: We word-align a parallel corpus, as described in Section 3.1."
We then parse both sides of the corpus with syntactic parsers [[REF_CITE]; Schmidt and Schulte im[REF_CITE]].
"For all phrase pairs that are consistent with the word alignment, we additionally check if both phrases are sub-trees in the parse trees."
Only these phrases are included in the model.
"Hence, the syntactically motivated phrase pairs learned are a subset of the phrase pairs learned without knowl-edge of syntax (Section 3.1)."
"As in Section 3.1, the phrase translation probability distribution is estimated by relative frequency."
"Marcu and Wong [2002] proposed a translation model that assumes that lexical correspondences can be estab-lished not only at the word level, but at the phrase level as well."
"To learn such correspondences, they introduced a phrase-based joint probability model that simultaneously generates both the Source and Target sentences in a paral-lel corpus."
"Expectation Maximization learning in Marcu and Wong’s framework yields both (i) a joint probabil-ity distribution , which reflects the probability that phrases and  are translation equivalents; (ii) and a joint distribution / , which reflects the probability that a phrase at position is translated into a phrase at position ."
"To use this model in the context of our framework, we simply marginalize to conditional probabilities the joint probabilities estimated by Marcu and Wong [2002]."
"Note that this approach is consistent with the approach taken by Marcu and Wong themselves, who use conditional models during decoding."
We used the freely available Europarl corpus [Footnote_2] to carry out experiments.
2 The Europarl corpus is available[URL_CITE]
"This corpus contains over 20 million words in each of the eleven official languages of the Eu-ropean Union, covering the proceedings of the[REF_CITE]-2001. 1755 sentences of length 5-15 were reserved for testing."
In all experiments in Section 4.1-4.6 we translate from German to English.
"We measure performance using the BLEU score [[REF_CITE]], which estimates the accuracy of translation output with respect to a reference translation."
"First, we compared the performance of the three methods for phrase extraction head-on, using the same decoder (Section 2) and the same trigram language model."
Fig-ure 1 displays the results.
"In direct comparison, learning all phrases consistent with the word alignment (AP) is superior to the joint model (Joint), although not by much."
The restriction to only syntactic phrases (Syn) is harmful.
"We also included in the figure the performance of an IBM Model 4 word-based translation system (M4), which uses a greedy de-coder [[REF_CITE]]."
Its performance is worse than both AP and Joint.
"These results are consistent over training corpus sizes from 10,000 sentence pairs to 320,000 sentence pairs."
All systems improve with more data.
Table 1 lists the number of distinct phrase translation pairs learned by each method and each corpus.
"The num-ber grows almost linearly with the training corpus size, due to the large number of singletons."
The syntactic re-striction eliminates over 80% of all phrase pairs.
Note that the millions of phrase pairs learned fit easily into the working memory of modern computers.
Even the largest models take up only a few hundred megabyte of RAM.
"The restriction on syntactic phrases is harmful, because too many phrases are eliminated."
"But still, we might sus-pect, that these lead to more reliable phrase pairs."
One way to check this is to use all phrase pairs and give more weight to syntactic phrase translations.
"This can be done either during the data collection – say, by counting syntactic phrase pairs twice – or during translation – each time the decoder uses a syntactic phrase pair, it credits a bonus factor to the hypothesis score."
We found that neither of these methods result in signif-icant improvement of translation performance.
Even pe-nalizing the use of syntactic phrase pairs does not harm performance significantly.
"These results suggest that re-quiring phrases to be syntactically motivated does not lead to better phrase pairs, but only to fewer phrase pairs, with the loss of a good amount of valuable knowledge."
"One illustration for this is the common German “es gibt”, which literally translates as “it gives”, but really means “there is”. “Es gibt” and “there is” are not syn-tactic constituents."
"Note that also constructions such as “with regard to” and “note that” have fairly complex syn-tactic representations, but often simple one word trans-lations."
Allowing to learn phrase translations over such sentence fragments is important for achieving high per-formance.
How long do phrases have to be to achieve high perfor-mance?
Figure 2 displays results from experiments with different maximum phrase lengths.
All phrases consis-tent with the word alignment (AP) are used.
"Surprisingly, limiting the length to a maximum of only three words per phrase already achieves top performance."
"Learning longer phrases does not yield much improvement, and occasionally leads to worse results."
"Reducing the limit to only two, however, is clearly detrimental."
Allowing for longer phrases increases the phrase trans-lation table size (see Table 2).
The increase is almost lin-ear with the maximum length limit.
"Still, none of these model sizes cause memory problems."
"One way to validate the quality of a phrase translation pair is to check, how well its words translate to each other."
"For this, we need a lexical translation probability distribu-tion ."
We estimated it by relative frequency from the same word alignments as the phrase model. count  count
A special English NULL token is added to each En-glish sentence and aligned to each unaligned foreign word.
"Given a phrase pair and a word alignment be-tween the foreign word positions  . and the English word positions  - , we compute the lexical weight by & apos; )(     $   %"
See Figure 3 for an example.
"If there are multiple alignments for a phrase pair &amp; , we use the one with the highest lexical weight: &amp; &amp; max"
We use the lexical weight during translation as a additional factor.
This means that the model  is extended to &apos ;      /A (
The parameter B defines the strength of the lexical weight .
Good values for this parameter are around 0.25.
Figure 4 shows the impact of lexical weighting on ma-chine translation performance.
"In our experiments, we achieved improvements of up to 0.01 on the BLEU score scale."
"Again, all phrases consistent with the word align-ment are used (Section 3.1)."
Note that phrase translation with a lexical weight is a special case of the alignment template model [[REF_CITE]] with one word class for each word.
"Our simplifica-tion has the advantage that the lexical weights can be fac-tored into the phrase translation table beforehand, speed-ing up decoding."
"In contrast to the beam search decoder for the alignment template model, our decoder is able to search all possible phrase segmentations of the input sen-tence, instead of choosing one segmentation before de-coding."
Recall from Section 3.1 that we learn phrase pairs from word alignments generated by Giza++.
The IBM Models that this toolkit implements only allow at most one En-glish word to be aligned with a foreign word.
We remedy this problem with a heuristic approach.
"First, we align a parallel corpus bidirectionally – for-eign to English and English to foreign."
This gives us two word alignments that we try to reconcile.
"If we intersect the two alignments, we get a high-precision alignment of high-confidence alignment points."
"If we take the union of the two alignments, we get a high-recall alignment with additional alignment points."
We explore the space between intersection and union with expansion heuristics that start with the intersection and add additional alignment points.
The decision which points to add may depend on a number of criteria:
In which alignment does the potential alignment point exist?
Foreign-English or English-foreign?
Does the potential point neighbor already estab-lished points?
"Does “neighboring” mean directly adjacent (block-distance), or also diagonally adjacent?"
Is the English or the foreign word that the poten-tial point connects unaligned so far?
Are both un-aligned?
What is the lexical probability for the potential point?
The base heuristic [[REF_CITE]] proceeds as fol-lows: We start with intersection of the two word align-ments.
We only add new alignment points that exist in the union of two word alignments.
We also always re-quire that a new alignment point connects at least one previously unaligned word.
"First, we expand to only directly adjacent alignment points."
"We check for potential points starting from the top right corner of the alignment matrix, checking for align-ment points for the first English word, then continue with alignment points for the second English word, and so on."
This is done iteratively until no alignment point can be added anymore.
"In a final step, we add non-adjacent alignment points, with otherwise the same requirements."
"Figure 5 shows the performance of this heuristic (base) compared against the two mono-directional alignments (e2f, f2e) and their union (union)."
The figure also con-tains two modifications of the base heuristic: In the first (diag) we also permit diagonal neighborhood in the itera-tive expansion stage.
"In a variation of this (diag-and), we require in the final step that both words are unaligned."
The ranking of these different methods varies for dif-ferent training corpus sizes.
"For instance, the alignment f2e starts out second to worst for the 10,000 sentence pair corpus, but ultimately is competitive with the best method at 320,000 sentence pairs."
"The base heuristic is initially the best, but then drops off."
"The discrepancy between the best and the worst method is quite large, about 0.02 BLEU."
"For almost all training corpus sizes, the heuristic diag-and performs best, albeit not always significantly."
The initial word alignment for collecting phrase pairs is generated by symmetrizing IBM Model 4 alignments.
"Model 4 is computationally expensive, and only approxi-mate solutions exist to estimate its parameters."
The IBM Models 1-3 are faster and easier to implement.
For IBM Model 1 and 2 word alignments can be computed effi-ciently without relying on approximations.
"For more in-formation on these models, please refer to Brown et al. [1993]."
"Again, we use the heuristics from the Section 4.5 to reconcile the mono-directional alignments obtained through training parameters using models of increasing complexity."
"How much is performance affected, if we base word alignments on these simpler methods?"
"As Figure 6 indi- cates, not much."
"While Model 1 clearly results in worse performance, the difference is less striking for Model 2 and 3."
Using different expansion heuristics during sym-metrizing the word alignments has a bigger effect.
"We can conclude from this, that high quality phrase alignments can be learned with fairly simple means."
The simpler and faster Model 2 provides similar performance to the complex Model 4.
We validated our findings for additional language pairs.
Table 3 displays some of the results.
"For all language pairs the phrase model (based on word alignments, Sec-tion 3.1) outperforms IBM Model 4."
Lexicalization (Lex) always helps as well.
We created a framework (translation model and decoder) that enables us to evaluate and compare various phrase translation methods.
Our results show that phrase transla-tion gives better performance than traditional word-based methods.
We obtain the best results even with small phrases of up to three words.
Lexical weighting of phrase translation helps.
Straight-forward syntactic models that map con-stituents into constituents fail to account for important phrase alignments.
"As a consequence, straight-forward syntax-based mappings do not lead to better translations than unmotivated phrase mappings."
This is a challenge for syntactic translation models.
It matters how phrases are extracted.
The results sug-gest that choosing the right alignment heuristic is more important than which model is used to create the initial word alignments.
"In this paper, we introduce a generative prob-abilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transforma-tion into the noisy output of an OCR system."
"The model is designed for use in error correc-tion, with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks."
"We present an implementation of the model based on finite-state models, demonstrate the model’s ability to significantly reduce character and word er-ror rate, and provide evaluation results involv-ing automatic extraction of translation lexicons from printed text."
"Although a great deal of text is now available in elec-tronic form, vast quantities of information still exist pri-marily (or only) in print."
"Critical applications of NLP technology, such as rapid, rough document translation in the field[REF_CITE]or information retrieval from scanned documents[REF_CITE], can depend heavily on the quality of optical character recog-nition (OCR) output."
"Unfortunately, the output of commercial OCR systems is far from perfect, especially when the language in ques-tion is resource-poor (Kanungo et al., in revision)."
And efforts to acquire new language resources from hardcopy using OCR[REF_CITE]face something of a chicken-and-egg problem.
"The problem is compounded by the fact that most OCR system are black boxes that do not allow user tuning or re-training — Baird (1999, re-ported[REF_CITE]) comments that the lack of ability to rapidly retarget OCR/NLP applications to new languages is “largely due to the monolithic structure of current OCR technology, where language-specific con-straints are deeply enmeshed with all the other code.”"
"In this paper, we describe a complete probabilistic, generative model for OCR, motivated specifically by (a) the need to deal with monolithic OCR systems, (b) the fo-cus on OCR as a component in NLP applications, and (c) the ultimate goal of using OCR to help acquire resources for new languages from printed text."
"After presenting the model itself, we discuss the model’s implementation, training, and its use for post-OCR error correction."
"We then present two evaluations: one for standalone OCR correction, and one in which OCR is used to acquire a translation lexicon from printed text."
We conclude with a discussion of related research and directions for future work.
"Generative “noisy channel” models relate an observable string to an underlying sequence, in this case recog-nized character strings and underlying word   sequences ."
"This relationship is modeled by  , decom-posed by Bayes’s Rule into steps modeled by  (the source model) and (comprising sub-steps gen-erating from )."
"Each step and sub-step is completely modular, so one can flexibly make use of existing sub-models or devise new ones as necessary. [Footnote_1]"
"1 Note that the process of “generating” from is a math-ematical abstraction, not necessarily related to the operation of any particular OCR system."
"We begin with preliminary definitions and notation, illustrated   in Figure 1."
"A true word sequencecorresponds to a true character sequence   acter sequence is given by, and the OCR system’s   output char-. subsequences is represented asA segmentation of the true character  sequence  into."
Seg-ment boundaries are only allowed between characters.
"Subsequences  are denoted using  segmentation  ,   positions, and   , where."
The  define character subsequences. (The number of segments need not equal the number of words ! and need not be a word in .)
"Correspondingly, a segmentation of the OCR’d character  $ sequence # into &quot; subsequences is given by."
"Subsequences are denoted by % % %*# ,)  + (  % # , where %&apos;&amp; &apos;% &amp; , % , &amp; and . ."
"The ),/+ % define character subsequences ."
"Alignment chunks are pairs of 21 corresponding  truth and OCR subsequences: , 0 . [Footnote_2]"
2 The model is easily modified to permit =?B@&gt; A .
The generative process begins with production of the true word sequence  0-6 0-6 with 87:9 probability ( &lt;; 7  .
"Modeling; fortheexampleunder-, lying sequence at the word level facilitates integration with NLP models, which is our ultimate goal."
"For exam-ple, the distribution  can be defined using -grams, parse structure, or any other tool in the language model-ing arsenal."
"The first step in transforming to is generation of a character sequence , modeled as."
"This step accommodates the character-based nature of OCR sys-tems, and provides a place to model the mapping of dif-ferent character sequences to the same word sequence (case/font variation) or vice versa (e.g. ambiguous word segmentation in Chinese)."
If the language in question provides explicit word boundaries (e.g. words are sep-arated by spaces when printed) then we output ‘#’ to rep-resent visible word boundaries.
One possible for our example is = “This#is#an#example.”
"Subsequences are generated from by choosing a set of boundary  positions, ."
"This sub-step, modeled by , is motivated by the fact that most OCR sys-tems first perform image segmentation, and then perform recognition on a word by word basis."
"For a language with clear word boundaries (or reli-able tokenization or segmentation algorithms), one could simply use spaces to segment the character sequence in a non-probabilistic way."
"However, OCR systems may make segmentation errors and resulting subsequences may or may not be words."
"Therefore, a probabilistic seg-mentation model that accommodates word merge/split er-rors is necessary."
"If a segment boundary coincides with a word boundary, the word boundary marker ‘#’ is considered a part of the segment on both , sides C  JI."
"E1E A F possible segmentation for $G our example $ is H , i.e. = “This#is#”, = “#an#”, = “#ex”, = “ample.”"
Notice the merge error in segment 1 and the split error involving segments 3 and 4.
"Our characterization of the final step, transformation into an observed character sequence, is motivated by the need to model OCR systems’ character-level recognition er-rors."
"We model each subsequence as being trans-formed into an OCR subsequence , so  #    % and we assume each is transformed independently, al-lowing   LK M#  ON"
Any character-level string error model can be used to define  ; for example[REF_CITE]or[REF_CITE].
This is also a logical place to make use of confidence values if provided by the OCR system.
"We assume that # is always deleted (modeling merge errors), and can never be inserted."
"Boundary mark-ers at segment boundaries are re-inserted when segments are put together to create , since they will be part of the OCR output (not as #, but most likely as spaces)."
"For our example G , a possible JH result for *I this step is: = QP “Tlmsis D1: D1:F ”, = “an”, = “cx”, = “amp1e.”; % ."
The final generated string would there-fore be = “Tlmsis#an#cx#am1e.”.
"Assuming independence of the individual steps, the complete model estimates joint probability   %  %     % can be computed by summing over all possible that can transform to :     %)"
"We have implemented the generative model using a weighted finite state model (FSM) framework, which provides a strong theoretical foundation, ease of integra-tion for different components, and reduced implementa-tion time thanks to available toolkits such as the AT&amp;T FSM Toolkit[REF_CITE]."
"Each step is repre-sented and trained as a separate FSM, and the resulting FSMs are then composed together to create a single FSM that encodes the whole model."
Details of parameter esti-mation and decoding follow.
"The specific model definition and estimation methods assume  that a training corpus is available, containingtriples."
Generation of True Word Sequence.
"We use an n-gram language model as the source model for the origi-nal word sequence: an open vocabulary, trigram language model with back-off generated using CMU-Cambridge Toolkit[REF_CITE]."
"The model is trained on the from the training data using the Witten-Bell discounting option for smoothing, and encoded as a simple FSM."
We made a closed vocabulary assump-tion to evaluate the effectiveness of our model when all correct words are in its lexicon.
"Therefore, although the language model is trained on only the training data, the words in the test set are included in the language model FSM, and treated as unseen vocabulary."
From Words to Characters.
"We generate three dif-ferent character sequence variants for each word: up-per case, lower case, and leading case (e.g. this bution over case variations is learned from theTHIS, this, This )."
"For each word, the  distri-pairs in the training corpus."
"For words that do not ap-pear in the corpus, or do not have enough number of oc-currences to allow a reliable estimation, we back off to word-independent case variant probabilities. [Footnote_3]"
"3 Currently, we assume a Latin alphabet. Mixed case text is not included since it increases the number of alternatives drasti-cally; at run time mixed-case words are normalized as a prepro-cessing step."
Our current implementation makes an independent decision for each character pair whether to insert a boundary between them.
"To reduce the search space associated with the model, we limit the number of boundary insertions to one per word, allowing at most two-way word-level splits."
"The probability of insert-ing a segment boundary between two characters, condi-tioned on the character pair, is estimated from the training corpus, with Witten-Bell discounting[REF_CITE]used to handle unseen character pairs."
Character Sequence Transformation.
This step is implemented as a probabilistic string edit process.
The confusion tables for edit operations are estimated using Viterbi style training on pairs in training data.
"Our current implementation allows for substitution, deletion, and insertion errors, and does not use context characters. [Footnote_4] Figure 2 shows a fragment of a weighted FSM model for : it shows how the observed haner could be generated by underlying banker or hacker. [Footnote_5] Final Cleanup."
"4 We are working on conditioning on neighbor characters, and using character merge/split errors. These extensions are trivial conceptually, however practical constraints such as the FSM sizes make the problem more challenging."
"5 The probabilities are constructed for illustration, but realis-tic: notice how n is much more likely to be confused for c than k is."
"At this stage, special symbols that were inserted into the character sequence are removed and the final output sequence is formed."
"For instance, segment boundary symbols are removed or replaced with spaces depending on the language."
"Decoding is the process of finding the “best” for an  %   observed , namely   %   "
"Decoding within the FSM framework is straightforward: we first compose all the components of the model in or-der, and then invert the resulting FSM."
"This produces a single transducer that takes a sequence of OCR characters as input, and returns all possible sequences of truth words as output, along with their weights."
One can then simply encode OCR character sequences as FSMs and compose them with the model transducer to perform decoding.
"Note that the same output sequence can be generated through multiple paths, and we need to sum over all paths to find the overall probability of that sequence."
This can be achieved by determinizing the output FSM generated by the decoding process.
"However, for practical reasons, we chose to first find the -best paths in the resulting FSM and then combine the ones that generate the same output."
"The resulting lattice or -best list is easily integrated with other probabilistic models over words, or the most probable sequence can be used as the output of the post- OCR correction process."
We report on two experiments.
"In the first, we evalu-ate the correction performance of our model on real OCR data."
"In the second, we evaluate the effect of correction in a representative NLP scenario, acquiring a translation lexicon from hardcopy text."
"Although most researchers are interested in improving the results of OCR on degraded documents, we are pri-marily interested in developing and improving OCR in new languages for use in NLP."
"A possible approach to retargeting OCR for a new language is to employ an ex-isting OCR system from a “nearby” language, and then to apply our error correction framework."
"For these exper-iments, therefore, we created our experimental data by scanning a hardcopy Bible using both an English and a French OCR system. (See Kanungo et al. (in revision) and[REF_CITE]for discussion of the Bible as a resource for multilingual OCR and NLP.)"
We have used the output of the English system run on French input to simulate the situation where available resources of one language are used to acquire resources in another lan-guage that is similar.
"It was necessary to pre-process the data in order to eliminate the differences between the on-line version that we used as the ground truth and the hardcopy, such as footnotes, glossary, cross-references, page numbers."
"We have not corrected hyphenations, case differences, etc."
"Our evaluation metrics for OCR performance are Word Error Rate (WER) and Character Error Rate (CER), which are defined as follows: !    0   3 0 [Footnote_6] 3  7    "
"6 Each line contains a verse, so they can actually span several lines on a page."
WER 4 !  0   3 0 [Footnote_6] 3  D[Footnote_7]  CER
"6 Each line contains a verse, so they can actually span several lines on a page."
7 Other sub-models are always trained on all 9 training sec-tions.
"Since we are interested in recovering the original word sequence rather than the character sequence, evaluations are performed on lowercased and tokenized data."
"Note, however, that our system works on the original case OCR data, and generates a sequence of word IDs, that are con-verted to a lowercase character sequence for evaluation."
"We have divided the data, which has 29317 lines, into 10 equal size disjoint sets, and used the first 9 as the train-ing data, and the first 500 lines of the last one as the test data. [Footnote_6] The WER and CER for the English OCR system on the French test data were 18.31% and 5.01% respec-tively."
"6 Each line contains a verse, so they can actually span several lines on a page."
The error rates were 5.98% and 2.11% for the out-put generated by the French OCR system on the same input.
"When single characters and non-alphabetical to-kens are ignored, the WER and CER drop to 17.21% and 4.28% for the English OCR system; 4.96% and 1.68% for the French OCR system."
We evaluated the performance of our model by studying the reduction in WER and CER after correction.
"The in-put to the system was original case, tokenized OCR out-put, and the output of the system was a sequence of word IDs that are converted to lowercase character sequences for evaluation."
All the results are summarized in Table 1.
The condi-tions side gives various parameters for each experiment.
The language model (LM) is either (word) unigram or tri-gram.
"Word to character conversion (WC) can allow the three case variations mentioned earlier, or simply pick the most probable variant for each word."
"Segmentation (SG) can be disabled, or 2-way splits and merges may be allowed."
"Finally, the character level error model (EM) may be trained on various subsets of training data. 7 Ta-ble 2 gives the adjusted results when ignoring all single characters and tokens that do not contain any alphabetical character."
"As can be seen from the tables, as we increase the train-ing size of the character error model from one section to five sections, the performance increases."
"However, there is a slight decrease in performance when the training size is increased to 9 sections."
"This suggests that our training procedures, while effective, may require refinement as additional training data becomes available."
"When we re-place the unigram language model with a trigram model, the results improve as expected."
"However, the most inter-esting case is the last experiment, where word merge/split errors are allowed."
Word merge/split errors cause an exponential increase in the search space.
"If there are words that needs to be corrected together, they can be grouped in different ways; ranging from distinct tokens  to a single token."
"For each of those groups ( , there are possible correct word sequences where is the number of tokens in that group, is the maximum number of words that can merge together, and is the vocabulary size."
"Although it is possible to avoid some computation using dynamic pro-gramming, doing so would require some deviation from the FSM framework."
We have instead used several restrictions to reduce the search space.
"First, we allowed only 2-way merge and split errors, restricting the search space to bigrams."
We further reduce the search space by searching through only the bigrams that are seen in the training data.
"We also in-troduced character error thresholds, letting us eliminate candidates based on their length."
"For instance, if we are trying to correct a sequence of 10 characters and have set a threshold of 0.2, we only need check candidates whose length is between [Footnote_8] and 12."
"8 Alternatively, the English side can be obtained via OCR and corrected."
The last restriction we im-posed is to force selection of the most likely case for each word rather than allowing all three case variations.
"Despite all these limitations, the ability to handle word merge/split errors improves performance significantly."
It is notable that our model allows global interactions between the distinct components.
"As an example, if the input is “ter- re”, the system returns “mer se” as the most probable correction."
"When “la ter- re” is given as the in-put, interaction between the language model, segmenta-tion model, and the character error model chooses the correct sequence “la terre”."
"In this example, the lan-guage model overcomes the preference of the segmen-tation model to insert word boundaries at whitespaces."
We used the problem of unsupervised creation of trans-lation lexicons from automatically generated word align-ment of parallel text as a representative NLP task to eval-uate the impact of OCR correction on usability of OCR text.
We assume that the English side of the parallel text is online and its foreign language translation is generated using an OCR system. 8 Our goal is to apply our OCR error correcting procedures prior to alignment so the re-sulting translation lexicon has the same quality as if it had been derived from error-free text.
We trained an IBM style translation model[REF_CITE]using GIZA++[REF_CITE]on the 500 test lines used in our experiments paired with correspond-ing English lines from an online Bible.
"Word level align-ments generated by GIZA++ were used to extract cross-language word co-occurrence frequencies, and candidate translation lexicon entries were scored according to the log likelihood ratio[REF_CITE](cf.[REF_CITE])."
"We generated three such lexicons by pairing the En-glish with the French ground truth, uncorrected OCR out-put, and its corrected version."
"All text was tokenized, lowercased, and single character tokens and tokens with no letters were removed."
This method of generating a translation lexicon works well; as Table 3 illustrates with the top twenty entries from the lexicon generated using ground truth French.
"Figure 3 gives the precision-recall curves for the trans-lation lexicons generated from OCR using the English OCR system on French hardcopy input with and without correction, using the top 1000 entries of the lexicon gen-erated from ground truth as the target set."
"Since we are interested in the effect of OCR, independent of the per-formance of the lexicon generation method, the lexicon auto-generated from the ground truth provides a reason-able target set. (More detailed evaluation of translation lexicon acquisition is a topic for future work.)"
"The graph clearly illustrates that the precision of the translation lexicon generated using original OCR data de-grades quickly as recall increases, whereas the corrected version maintains its precision above 90% up to a recall of 80%."
"There has been considerable research on automatically correcting words in text in general, and correction of OCR output in particular."
"Unfortunately, there is no commonly used evaluation base for OCR error correction, making comparison of experimental results difficult."
Some systems integrate the post-processor with the ac-tual character recognizer to allow interaction between the two.
"In an early study,[REF_CITE]reports a word error rate of about 2% and a reject rate of 1%, with-out a dictionary."
"Many systems treat OCR as a black box, generally em-ploying word and/or character level -grams along with character confusion probabilities."
They do not use a lexicon but do require the probabilities assigned to individual characters by the OCR system.
Their system is designed around a stratified algorithm.
"The first phase performs isolated word cor-rection using rewrite rules, allowing words that are not in the lexicon."
"The second phase attempts correcting word split errors, and the last phase uses word bigram proba-bilities to improve correction."
The three phases interact with each other to guide the search.
"In comparison to our work, the main difference is our focus on an end-to-end generative model versus their stratified algorithm centered around correction."
"Depending on the value of k, correction can be restricted to sample language, or variations may be allowed."
They report reducing error rate from 33% to below 2% on OCR output of hand-written Spanish names from forms.
"Although it is limited to sin-gle errors, the system demonstrates the possibility of cor-recting OCR errors in morphologically rich languages."
The system uses multiple information re-sources to propose correction candidates and lets the user review the candidates and make corrections.
"Although segmentation errors have been addressed to some degree in previous work, to the best of our knowl-edge our model is the first that explicitly incorporates segmentation."
"Similarly, many systems make use of a language model, a character confusion model, etc., but none have developed an end-to-end model that formally describes the OCR process from the generation of the true word sequence to the output of the OCR system in a man-ner that allows for statistical parameter estimation."
Our model is also the first to explicitly model the conversion of a sequence of words into a character sequence.
"We have presented a flexible, modular, probabilistic gen-erative OCR model designed specifically for ease of in-tegration with probabilistic models of the sort commonly found in recent NLP work, and for rapid retargeting of OCR and NLP technology to new languages."
"In a rigorous evaluation of post-OCR error correction on real data, illustrating a scenario where a black-box commercial English OCR system is retargeted to work with French data, we obtained a 70% reduction in word error rate over the English-on-French baseline, with a re-sulting word accuracy of 97%."
It is worth noting that our post-OCR correction of the English OCR on French text led to better performance than a commercial French OCR system run on the same text.
We also evaluated the impact of error correction in a resource-acquisition scenario involving translation lex-icon acquisition from OCR output.
The results show that our post-OCR correction framework significantly improves performance.
"We anticipate applying the tech-nique in order to retarget cross-language IR technology — the results[REF_CITE]demonstrate that even noisy extensions to dictionary-based translation lex-icons, acquired from parallel text, can have a positive impact on cross language information retrieval perfor-mance."
"We are currently working on improving the correc-tion performance of the system, and extending our error model implementation to include character context and allow for character merge/split errors."
"We also intend to relax the requirement of having a word list, so that the model handles valid word errors."
We are also exploring the possibility of tuning a statis-tical machine translation model to be used with our model to exploit parallel text.
"If a translation of the OCR’d text is available, a translation model can be used to provide us with a candidate-word list that contains most of the correct words, and very few irrelevant words."
"Finally, we plan to challenge our model with other lan-guages, starting with Arabic, Turkish, and Chinese."
"Ara-bic and Turkish have phonetic alphabets, but also pose the problem of rich morphology."
Chinese will require more work due to the size of its character set.
"We are optimistic that the power and flexibility of our modeling framework will allow us to develop the necessary tech-niques for these languages, as well as many others."
We present a derivation of the alignment tem-plate model for statistical machine translation and an implementation of the model using weighted finite state transducers.
The approach we describe allows us to implement each con-stituent distribution of the model as a weighted finite state transducer or acceptor.
We show that bitext word alignment and translation un-der the model can be performed with standard FSM operations involving these transducers.
"One of the benefits of using this framework is that it obviates the need to develop special-ized search procedures, even for the generation of lattices or N-Best lists of bitext word align-ments and translation hypotheses."
We evaluate the implementation of the model on the French-to-English Hansards task and report alignment and translation performance.
The Alignment Template Translation Model (ATTM)[REF_CITE]has emerged as a promising modeling framework for statistical machine translation.
The ATTM attempts to overcome the deficiencies of word-to-word translation models[REF_CITE]through the use of phrasal translations.
The overall model is based on a two-level alignment between the source and the target sentence: a phrase-level alignment between source and target phrases and a word-level alignment between words in these phrase pairs.
"The goal of this paper is to reformulate the ATTM so that the operations we intend to perform under a sta-tistical translation model, namely bitext word alignment and translation, can be implementation using standard weighted finite state transducer (WFST) operations."
Our main motivation for a WFST modeling framework lies in the resulting simplicity of alignment and translation processes compared to dynamic programming or  de-coders.
The WFST implementation allows us to use stan-dard optimized algorithms available from an off-the-shelf FSM toolkit[REF_CITE].
"This avoids the need to develop specialized search procedures, even for the gen- eration of lattices or N-best lists of bitext word alignment or translation hypotheses."
Weighted Finite State Transducers for Statistical Ma-chine Translation (SMT) have been proposed in the literature to implement word-to-word translation mod-els[REF_CITE]or to perform trans-lation in an application domain such as the call routing task[REF_CITE].
One of the objec-tives of these approaches has been to provide an imple-mentation for SMT that uses standard FSM algorithms to perform model computations and therefore make SMT techniques accessible to a wider community.
Our WFST implementation of the ATTM has been developed with similar objectives.
We start off by presenting a derivation of the ATTM that identifies the conditional independence assumptions that underly the model.
The derivation allows us to spec-ify each component distribution of the model and imple-ment it as a weighted finite state transducer.
We then show that bitext word alignment and translation can be performed with standard FSM operations involving these transducers.
Finally we report bitext word alignment and translation performance of the implementation on the Canadian French-to-English Hansards task.
We present here a derivation of the alignment template translation model (ATTM)[REF_CITE]and give an implementation of the model using weighted finite state transducers (WFSTs).
The finite state model-ing is performed using the AT&amp;T FSM Toolkit[REF_CITE].
"In this model, the translation of a source language sen-tence to a target language sentence is described by a joint probability distribution over all possible segmentations and alignments."
This distribution is presented in Figure 1 and Equations 1-7.
"The components of the overall trans-lation model are the source language model (Term 2), the source segmentation model (Term 3), the phrase per-mutation model (Term 4), the template sequence model (Term 5), the phrasal translation model (Term 6) and the target language model (Term 7)."
"Each of these condi-tional distributions is modeled independently and we now define each in turn and present its implementation as a weighted  ! finite  state  acceptor  or  transducer  . (1) &quot; $# %! (2) &amp;# %! !   # (3) (4) &quot; #    () *! (5) +# ,,( * (6) (7)"
We begin by distinguishing words and phrases.
"We as-sume that is a phrase in the target language /. sentence that has length - and consists of words. tains wordsSimilarly, a phrase   in the source language  sentence con-, where is the NULL token."
We assume that each word in each language can be as-signed to a unique class 5 so that unambiguously spec-ifies a class 6 7 sequence 9 and specifies the class se-quence :.
"Throughout the model, if a sentence ; is segmented into phrases , we say to indi-cate that the words in the phrase sequence agree with the original sentence."
Source Language Model
The model assigns probabil-ity to any sentence in the source language; this prob-ability is not actually needed by the translation process when is given.
"As the first component in the model, a finite state acceptor &lt; is constructed for ."
Source Segmentation Model We introduce the phrase count random variable which specifies the number of phrases in a particular segmentation of the source lan-sentence.
"For a sentence of length = , there are guage &gt; @??   ways to segment it into  phrases # ."
"Motivated by this, we choose &gt;  the ?"
A distribution as D@??
E ( D  %= K $#  (8) $#  so that L .
We construct  a joint .&quot; #  distribution over all phrase seg-mentations &quot; $#  as $# (9)
P \ )
U V  &quot; U ] `^ _ /[a8bdc`e/ 1 where &quot; #   constant )U V  X j U
"The f normalization LMhg Ri S lkh iR WmX  U C#  , is chosen so that L ."
"Here, is a “unigram” distribution over source language phrases; we assume that we have an inventory of phrases from which this quantity can be estimated."
"In this way, the likelihood of a particular segmentation is determined by the likelihood of the phrases that result."
We now describe the finite state implementation of the source segmentation model and show how to compute the most likely n N segmentation  h Ri k  #() % # Ion p K under the model: . 1.
"For each source language sentence to be trans-lated, we implement a weighted finite state trans-ducer x that segments the sentence into all possible phrase sequences permissible given the inven-tory of phrases."
The U) score V W X &quot; of U a segmentation under x is given by S .
"We then generate a lattice of segmentations of (implemented as an acceptor &lt; ) by composing it with the transducer x , i.e. y &lt; z{x . ( 2."
We E | then G} decompose I uJ D 231 41 31 =%K )y ~ into V y = disjoint subsets y y so that y contains all segmentations of the source language phrases.
"To construct y , sentence with exactly we create an unweighted acceptor that accepts any phrase sequence of length ; for efficiency, the phrase vocabulary is restricted to the phrases in y . y is then obtained by the finite state composition y yz D  = . f 3."
For The normalization factors are obtained by sum-ming the probabilities of all segmentations in y .
This sum can be computed efficiently using lattice forward probabilities[REF_CITE].
"For a fixed , the most likely segmentation in y is found as  n &quot; f J U) V"
X &quot;U )1 (10) 4.
Finally we select the n optimal # () % segmentation  # % as  n 0q ktv. kk +   (11)
A portion of the segmentation transducer x for the French sentence nous avons une inflation galopante is presented in Figure 2.
"When composed with &lt; , x gen-erates the following two phrase segmentations: nous avons une inflation galopante and nous avons une in-flation galopante."
The “ ” symbol is used to indicate phrases formed by concatenation of consecutive words.
The phrases specified by the source segmentation model remain in the order that they appear in the source sen-tence.
Phrase Permutation Model We now define a model for the reordering of phrase sequences as determined by the previous model.
The phrase alignment sequence specifies a reordering of phrases into target language phrase order; the words within the phrases remain in the source language order  .
"The R .phraseThe phrasesequencealignmentis se-re-ordered into i  quence  ,#  is , modeled ( % as a first  order ,#  Markov  process (12) )  U # U ?    U V ."
U  D  K with .
The alignment sequence distri-bution is constructed to assign decreasing likelihood to phrase re-orderings that diverge from the original word order.
"Suppose  and  i , we set the Markov chain probabilities as follows ?[REF_CITE]U # U ?"
"W J6 ?E &amp; D  K 1    (13) W 6 is  a tuning U # U ? factorso andthat In the above equations, we V normalize k  V i  the U  probabilities o# U ?  ."
The finite state implementation of this model involves acceptortwo acceptors  that.
Wecontainsfirst buildall permutationsa unweightedofpermutationthe phrase in the source language (Knight and  corre-Al-sequence[REF_CITE]) .
We note that a path through acceptorsponds to an  alignmentfor the sourcesequencephrase sequence.
Figure 3nousshowsavonsthe  une inflation galopante. of  lengthof D vt q0w
A source phrase sequence words re -states  .
"For quires a permutation acceptor score, i.e. phrase alignments containinglong c # U ? phrase   sequencesfor each arcweandcomputethen aprunescore  the arcs c by thisare in-can therefore be applied whilecluded only if this score is above  isa constructedthreshold."
The second acceptor in the implementation of the phrase permutation model assigns alignment probabil-ities[REF_CITE]to a given permutation of the source phrase sequence (Figure 4).
"In this example, follows:the phrases  in the  source phrase . sequence . are specified   as(nous), (avons) and (une inflation galopante)."
We now show the computa-tion of some of the W6 alignment \ 1  probabilities[REF_CITE]in   this example D ## .. ( W ) 6 ? ? \ $ 1 %# &quot; . !
W 6 ? ? \ 1 &amp; &amp; 1 ! andNormalizing   D these # . terms  gives \ $ 1 %#   # . ( + \ 1 ) +* .
Template Sequence Model Here we describe the main component 5 : 67 of the model.
"An alignment template 9 specifies the 5 allowable 7 alignments be-and : 6 . tween .-0 [ /  the class sequences 9 is a - , binary, 0/1 valued matrix which is constructed J as follows:[REF_CITE]\ can be aligned to : , then 31 ; ."
"This 6 process may 6 allow J 31 9 to align otherwise 31 with the NULL token : , i.e. 21 , so that words can be freely 5 inserted in 6 7 translation."
"Given a pair of class sequences 9 and : , we specify exactly one matrix 9 5 : 67 ."
We say that is consistent with the target language phrase and the source language phrase is the class sequence for .
"In Section 4.1, we will outline a procedure to build a library of alignment templates  9 from 5 : 6 bitext 7 word-level alignments."
Each template c used in our model has an index in this template library.
Therefore any operation that involves a mapping to (from) template sequences will be implemented as a mapping to (from) a sequence of these indices.
We have described the segmentation and permutation processes that transform a source language sentence into phrases in target language phrase order.
The next step is to generate a consistent sequence of alignment tem-plates.
We assume that the templates are conditionally independent of each other and depend only on the source language phrase which generated each of them )V  U #  %U #  () %
U V)  U #   (14) maps any permutation i We will implement this model using  the R transducerof the phrasethatse-quence into a template sequence with probability as[REF_CITE].
"For every phrase , this transducer al-lows only the  templates #  that U are #  consistentenforceswiththe consis-withprobability , i.e. tency between each source phrase and alignment tem-plate."
Phrasal Translation Model
We assume that a tar-get phrase is generated independently by each alignment template and source phrase &quot; #   
"U V) &quot; U # ,()  U)V &quot; U # U    (15) tion modelThis allows &quot; us #  to describe the phrase-internal transla-as follows."
We assume that each word in the target phrase is produced independently and that the consistency is enforced 5 between  the #  words in \ and G the class sequence 9 so that 1 if  9 1 .  V    o#  7V 6 1 1 1 5  V V 6  1 #   o# )J ) 1 )1 (16)  1 # 1  is 1 a  translationis obtaineddictionaryas (Och and
The term[REF_CITE]) and   o# 1 21  1 #  L (17)   #
"We have assumed that 1 , i.e. that given the template, word alignments do not depend on the source language phrase. phrase 7 and a consistent alignment f tem-"
"For a given  5 : 6 plate , a weighted acceptor can be cording[REF_CITE]and 17.constructed to assign probability to f translated phrases ac-is constructed from four component machines , , and , constructed as follows."
The first acceptor ( / J implements the alignment matrix c  &amp;J .
"It c has -states and between any pair of states and , each  arc corresponds to a word alignment vari-able  c . c"
Therefore / J the number of transitions between states and  is equal to the number c  J c of non-zero val-ues  of 1 1 o .
The #[REF_CITE].state to has probability Thesecondmachine c G I \ / is an - unweighted K in the phrasetransducer [ 67 that maps the index to the corresponding word 1 .
"The third transducer  is the  lexicon transducer ,G that  maps the source  word # to the target word with probability . get word sequencesThe fourth acceptor  is unweighted and allows all tar-which can be specified by the tor for an alignment template .Figure f 5: Component transducers to construct the accep-class sequence 9 5 has - c M / J J states c ."
The number . of transitions between states and is equal to the number of target language words with class specified by 9 1 . ing the transducer corresponding to an alignment tem-Figure 5 shows all f the four component FSTs for build-we obtain as follows.
We first compose the four trans-plate from f our library.
"Having built these four machines, ducers, project the resulting transducer / onto the output la-bels, and determinize it under the , semiring."
This is implemented using AT&amp;T FSM tools as follows fsmcompose O # I D C # fsmproject -o f # fsmrmepsilon fsmdeterminize .
"Given an alignment template and a consistent source tion operations assign the probabilityphrase , we note that the composition  and &quot; # determiniza-[REF_CITE]to each consistent target phrase ."
This summa-rizes the construction of a transducer for a single align-ment template.
We now implement a transducer that maps se-quences of alignment templates to target language word sequences.
We identify all templates consistent with the phrases in the source language phrase sequence .
The transducer is constructed via the FSM union operation of the transducers that implement these templates.
"For the source phrase sequence (nous avons une inflation galopante), we show the transducer in plates ,Figure 6.  "
"Our . example   library consists of three tem-and . maps the source word nous to the target word we via J the . word alignment matrixspecified as . maps the source word ment matrixavons to the target phrase have ; a  via . the word \  align-specified as . maps the source phrase une inflation galopante to the target phrase run away inflation . via \ the word  alignment K . matrix f specified as f . is built f ! out of the three f component acceptors , , and ."
The acceptor 1 corresponds to the map-ping from the template 1 and the source phrase 1 to all consistent target phrases 1 .
K enforces the requirement that words where We note thatin the translation agree   with those in the phrase sequence.is modeled as a standard backoff trigram language model[REF_CITE].
Such a language model can be easily compiled as a weighted finite state acceptor[REF_CITE].
We will now describe how the alignment template trans-lation model can be used to perform word-level alignment of bitexts and translation of source language sentences.
"Given  a source language sentence and a target sen- tence , the word-to-word alignment between the sen-tences I n p  can n p  be n p found +n p as n K $#  )  X Ri kiR k Ri kh Ri k &quot;   I n p n p +n p n K specify the alignment n"
The variables between source phrases and target phrases while gives the word-to-word alignment within the phrase sequences.
"Given a source language sentence , the translation can Ion p be  found p n p  as n p +n p n K  i kX Ri qurk s+iR tvk Rikh iR k      # where is the translation of . n"
We implement the alignment and translation proce-dures in two steps.
"We first segment the source sentence into phrases, as described earlier Ion p n K Nh Ri tvq0wk &quot; #  #  (18) a sentence pairAfter segmenting   the % source sentence, the alignment ofis obtained as I n p n p n p K Xq0Rik iR k Ri &quot; p  p  p &quot;# n p n (19)  )1"
The translation is the same way as
"I n p p o  n ,p n p K p  p  p #&quot;n p n  )1 (20)  i kX iR k iR k Ri   qur +s mentationWe have } n described n p how to compute the optimal seg-[REF_CITE]in Section 2."
The seg-mentation process decomposes n p the source sentence into a phrase sequence n U .
This process also tags each source phrase with its position in the phrase se-quence.
We will now describe the alignment and trans-lation processes using finite state operations.
"Given a collection of alignment templates, it is not guar-anteed that every sentence pair in a bitext can be seg-mented into phrases for which there exist the consistent alignment templates needed to create an alignment be-tween the sentences."
We find in practice that this prob-lem arises frequently enough that most sentence pairs are assigned a probability of zero under the template model.
"To overcome this limitation, we add several types of “dummy” templates to the library that serve to align phrases when consistent templates could not otherwise be found. lows any source phraseThe first type of dummy n U template we introduce al-to align with any single word target U phrase I8 +n U 1 ."
K This template G  is defined D  n K as a triple c G  1 D   1 .
All thewhereentries of the matrix and are speci-fied to be ones.
The second type of dummy template al-lows source phrases to be deleted n U during the alignment process.
For U a source n U )  phrase  D  we n specify this tem-plate as .
The third type of template allows for insertions of single word target phrases.  
For 1  a target  D phrase  .
The 1 weprobabilitiesspecify this template #  foras 1 these added templates are not estimated; they are fixed as a global constant which is set so as to discourage their use except when no other suitable templates are available.
A lattice of possible alignments between  and is then obtained by the finite state composition 1  p z z z z  (21) where is an acceptor for the n target sentence  .
"We then compute the ML alignment[REF_CITE]by obtain-ing n the path with the highest probability, in ."
The path determines three types of alignments n U : phrasal align-ment n U between the source phrase n U and the target phrase; deletions of source phrases ; and insertions of tar-get words 1 .
"To determine  the word-level alignment be-tween the sentences and ,we are primarily interested in the first of these n U types of alignments."
"Given that n U the has aligned to the target n U phrase , we source phrase look up the hidden n U template variable that yielded this alignment. contains the the word-to-word alignment between these phrases."
The lattice of possible translations of is obtained using the weighted finite state composition:  p z   1z z z (22)
The translation with the highest probability[REF_CITE]can now be computed by obtaining the path with the highest score in .
"In terms of AT&amp;T FSM tools, this can be done as fol-lows # fsmproject n ] # fsmbestpath fsmrmepsilon"
A translation lattice[REF_CITE]can be gen-erated by pruning based on likelihoods or number of states.
"Similarly, an alignment lattice can be generated by pruning ."
We now evaluate this implementation of the alignment template translation model.
"To create the template library, we follow the procedure reported[REF_CITE]."
"We first obtain word alignments of bitext using IBM-4 translation models trained in each forming the union of these alignments (IBM-4 : translation direction (IBM-4 F and IBM-4 E), and ~ then 9 )."
We extract the library of alignment templates from the bitext alignment using the phrase-extract algorithm re-ported[REF_CITE].
This 9 procedure 5 : 67 identifies several alignment templates that are consis-tent with a source phrase .
We do not use word classes in the experiments reported here; therefore templates are specified by phrases rather than by class sequences.
"For a given pair of source and target phrases, we retain only the matrix of alignments that occurs most frequently in the training corpus."
"This is consistent with the intended application of these templates for translation and align-ment under the maximum likelihood criterion; in the cur-rent formulation, only one alignment will survive in any application of the models and there is no reason to retain mate the probabilityany of the less frequently #  occuring alignments."
We esti-by the relative frequency of phrasal translations found in bitext alignments.
"To restrict the memory requirements of the model # , we extract only the templates which have at most words in the source plates which have a probabilityphrase."
"Furthermore, we restrict   ourselves #  \ 1 \ to J the tem-for some source phrase ."
We present results on the French-to-English Hansards translation task[REF_CITE].
"We measured the alignment performance using precision, recall, and Alignment Error Rate (AER) metrics[REF_CITE]. which consists ofOur training set is # a \ subset +\ u\ \ of the Canadian HansardsFrench-English sentence pairs (Och and * ) Ney  &amp; ,  2000)."
The J English ) \ side of the bitext words J  &amp; ( # ) # unique D ) tokens \  \ &amp;  * had a total of ) and unique /J
D the French side contained words ( tokens).
Our template library consisted of templates.
Our test set consists of 500 unseen French sentences from Hansards for which both reference translations and word alignments are available[REF_CITE].
"We present the results under the ATTM in Table 1, where we distinguish word alignments produced by the templates from the template library against those produced by the templates introduced for alignment in Section 3.1."
"For comparison, we also align the bitext using IBM-4 trans-lation models."
We first observe that the complete set of word align-ments generated by the ATTM (ATTM-C) is relatively poor.
"However, when we consider only those word align-ments generated by actual alignment templates (ATTM-A) (and discard the alignments generated by the dummy templates introduced as described in Section 3.1), we obtain very high alignment precision."
This implies that word alignments within the templates are very accurate.
"However, the poor performance under the recall measure suggests that the alignment template library has relatively poor coverage of the phrases in the alignment test set."
We next measured the translation performance of ATTM on the same test set.
"The translation performance was measured using the BLEU[REF_CITE]and the NIST MT-eval metrics[REF_CITE], and Word Er-ror Rate (WER)."
The target language model was a trigram language model with modified Kneser-Ney smoothing trained on the English side of the bitext using the SRILM tookit[REF_CITE].
The performance of the model is reported in Table 2.
"For comparison, we also report per-formance of the IBM-4 translation model trained on the same corpus."
The IBM Model-4 translations were ob-tained using the ReWrite decoder[REF_CITE].
The results in Table 2 show that the alignment template model outperforms the IBM Model 4 under all three metrics.
This verifies that WFST implementation of the ATTM can obtain a performance that compares favor-ably to other well known research tools.
"We generate N-best lists from each translation lattice, and show the variation of their oracle-best BLEU scores in Table 3."
We observe that the oracle-best BLEU score increases with the size of the N-Best List.
We can there-fore expect to rescore these lattices with more sophis-ticated models and achieve improvements in translation quality.
The main motivation for our investigation into this WFST modeling framework for statistical machine translation lies in the simplicity of the alignment and translation pro-cesses relative to other dynamic programming or  de-coders[REF_CITE].
"Once the components of the align-ment template translation model are implemented as WF-STs, alignment and translation can be performed using standard FSM operations that have already been imple-mented and optimized."
"It is not necessary to develop spe-cialized search procedures, even for the generation of lat-tices and N-best lists of alignment and translation alter-natives."
The derivation of the ATTM was presented with the in-tent of clearly identifying the conditional independence assumptions that underly the WFST implementation.
This approach leads to modular implementations of the component distributions of the translation model.
These components can be refined and improved by changing the corresponding transducers without requiring changes to the overall search procedure.
However some of the mod-eling assumptions are extremely strong.
We note in par-ticular that segmentation and translation are carried out independently in that phrase segmentation is followed by phrasal translation; performing these steps independently can easily lead to search errors.
It is a strength of the ATTM that it can be directly constructed from available bitext word alignments.
How-ever this construction should only be considered an ini-tialization of the ATTM model parameters.
Alignment and translation can be expected to improve as the model is refined and in future work we will investigate iterative parameter estimation procedures.
We have presented a novel approach to generate align-ments and alignment lattices under the ATTM.
"These lat-tices will likely be very helpful in developing ATTM pa-rameter estimation procedures, in that they can be used to provide conditional distributions over the latent model variables."
We have observed that that poor coverage of the test set by the template library may be why the over-all word alignments produced by the ATTM are relatively poor; we will therefore also explore new strategies for template selection.
The alignment template model is a powerful model-ing framework for statistical machine translation.
It is our goal to improve its performance through new training procedures while refining the basic WFST architecture.
"Following the recent adoption by the machine translation community of automatic evalua-tion using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries."
"The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct ap-plication of the BLEU evaluation procedure does not always give good results."
Automated text summarization has drawn a lot of inter-est in the natural language processing and information retrieval communities in the recent years.
"A series of workshops on automatic text summarization ([REF_CITE]2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States[REF_CITE]and Japan[REF_CITE]have advanced the technology and produced a couple of experimental online systems ([REF_CITE])."
"Despite these efforts, however, there are no common, convenient, and repeatable evaluation meth-ods that can be easily applied to support system devel-opment and just-in-time comparison among different summarization methods."
The Document Understanding Conference[REF_CITE]run by the National Institute of Standards and Technol-ogy (NIST) sets out to address this problem by provid-ing annual large scale common evaluations in text summarization.
"However, these evaluations involve human judges and hence are subject to variability[REF_CITE]."
"For example,[REF_CITE]pointed out that 18% of the data contained multiple judgments in the[REF_CITE]single document evaluation [Footnote_1] ."
1 Multiple judgments occur when more than one performance score is given to the same system (or human) and human sum-mary pairs by the same human judge.
"To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC."
"Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations[REF_CITE]."
Section 2 gives an overview of the evaluation procedure used in DUC.
Section 3 discusses the IBM BLEU[REF_CITE]and[REF_CITE]n-gram co-occurrence scoring procedures and the application of a similar idea in evaluating summaries.
Section 4 compares n-gram co-occurrence scoring procedures in terms of their correla-tion to human results and on the recall and precision of statistical significance prediction.
Section 5 concludes this paper and discusses future directions.
"2[REF_CITE]have similar tasks, but summaries of 10, 50, 100, and 200 words are requested in the multi-document task[REF_CITE]."
"The training set comprised 30 sets of approximately 10 docu-ments each, together with their 100-word human written summaries."
"The test set comprised 30 un-seen documents. • Fully automatic multi-document summarization: given a set of documents about a single subject, participants were required to create 4 generic sum-maries of the entire set, containing 50, 100, 200, and 400 words respectively."
The document sets were of four types: a single natural disaster event; a single event; multiple instances of a type of event; and information about an individual.
"The training set comprised 30 sets of approximately 10 docu-ments, each provided with their 50, 100, 200, and 400-word human written summaries."
The test set comprised 30 unseen sets.
A total of 11 systems participated in the single-document summarization task and 12 systems partici-pated in the multi-document task.
"For each document or document set, one human sum-mary was created as the ‘ideal’ model summary at each specified length."
Two other human summaries were also created at each length.
"In addition, baseline sum-maries were created automatically for each length as reference points."
"For the multi-document summariza-tion task, one baseline, lead baseline, took the first 50, 100, 200, and 400 words in the last document in the collection."
"A second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and so on until it had a sum- mary of 50, 100, 200, or 400 words."
Only one baseline (baseline1) was created for the single document summa-rization task.
"To evaluate system performance NIST assessors who created the ‘ideal’ written summaries did pairwise com-parisons of their summaries to the system-generated summaries, other assessors’ summaries, and baseline summaries."
They used the Summary Evaluation Envi-ronment (SEE) 2.0 developed[REF_CITE]to support the process.
"Using SEE, the assessors compared the system’s text (the peer text) to the ideal (the model text)."
"As shown in Figure 1, each text was decomposed into a list of units and displayed in separate windows."
SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries.
"To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (green/dark gray highlight in the model summary window), and specify that the marked system units ex-press all, most, some, or hardly any of the content of the current model unit."
"To measure quality, assessors rate grammaticality 3 , cohesion 4 , and coherence 5 at five dif-ferent levels: all, most, some, hardly any, or none [Footnote_6] ."
"6 These category labels are changed to numerical values of 100%, 80%, 60%, 40%, 20%, and 0%[REF_CITE]."
"For example, as shown in Figure 1, an assessor marked sys-tem units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right)."
Recall at different compression ratios has been used in summarization research to measure how well an auto-matic system retains important content of original documents[REF_CITE].
"However, the simple sen-tence recall measure cannot differentiate system per-formance appropriately, as is pointed out[REF_CITE]."
"Therefore, instead of pure sentence recall score, we use coverage score C. We define it as fol-lows [Footnote_7] : C = Total number(Numberof ofMUsMUsin themarkedmodel) •summaryE (1) E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/[Footnote_4] for hardly any, and 0 for none."
"7[REF_CITE]uses a length adjusted version of coverage metric C’, where C’ = α*C + (1-α)*B. B is the brevity and α is a pa-rameter reflecting relative importance[REF_CITE]."
4 Do sentences in the summary fit in with their surrounding sentences?
"If we ignore E (set it to 1), we obtain simple sentence recall score."
We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the fol-lowing sections.
To automatically evaluate machine translations the ma-chine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU[REF_CITE].
The NIST[REF_CITE]scoring metric is based on BLEU.
The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical met-ric.
"To achieve this goal, they used a weighted average of variable length n-gram matches between system translations and a set of human reference translations and showed that a weighted average metric, i.e. BLEU, correlating highly with human assessments."
"Similarly, following the BLEU idea, we assume that the closer an automatic summary to a professional human summary, the better it is."
The question is: “Can we ap-ply BLEU directly without any modifications to evalu-ate summaries as well?”.
We first ran IBM’s BLEU evaluation script unmodified over the[REF_CITE]model and peer summary set.
The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three ref-erence summaries; while Spearman ρ for the multi-document task is 0.67 using one reference and 0.70 us-ing three.
These numbers indicate that they positively correlate at α = 0.01 [Footnote_8] .
"8 The number of instances is 14 (11 systems, 2 humans, and 1 baseline) for the single document task and is 16 (12 systems, 2 humans, and 2 baselines) for the multi-document task."
"Therefore, BLEU seems a prom-ising automatic scoring metric for summary evaluation."
"According[REF_CITE], BLEU is essentially a precision metric."
It measures how well a machine translation overlaps with multiple human translations using n-gram co-occurrence statistics.
N-gram precision in BLEU is computed as follows: ∑ ∑ Count clip (n−gram) p n = C∈{Candidates ∑ }n−gram∈C ∑ Count(n−gram) (2) C∈{Candidates}n−gram∈C
"Where Count clip (n-gram) is the maximum number of n-grams co-occurring in a candidate translation and a ref-erence translation, and Count(n-gram) is the number of n-grams in the candidate translation."
"To prevent very short translations that try to maximize their precision scores, BLEU adds a brevity penalty, BP, to the for-mula:"
BP =  1 if c &gt; r  e (1−|r|/|c|) if c ≤ r  ([Footnote_3])
3 Does the summary observe English grammatical rules inde-pendent of its content?
Where |c| is the length of the candidate translation and |r| is the length of the reference translation.
The BLEU formula is then written as follows:
BP • exp ∑ w log p  (4) N  n=1 n n 
"N is set at 4 and w n , the weighting factor, is set at 1/N. For summaries by analogy, we can express equation (1) in terms of n-gram matches following equation (2): ∑ ∑ Count match (n−gram) C n = C∈{Model ∑ Units}n−gram∈C ([Footnote_5]) ∑ Count(n−gram)"
5 Is the content of the summary expressed and organized in an effective way?
Where Count match (n-gram) is the maximum number of n-grams co-occurring in a peer summary and a model unit and Count(n-gram) is the number of n-grams in the model unit.
"Notice that the average n-gram coverage score, C n , as shown in equation 5 is a recall metric ings versus human ranking for the multi-document task data[REF_CITE]."
"The same system is at each vertical line with ranking given by different Ngram(1,4) n scores."
The straight line (AvgC) is the human ranking and n marks sum-maries of different sizes.
"Ngram(1,4) all combines results from all sizes. instead of a precision one as p n ."
Since the denominator of equation 5 is the total sum of the number of n-grams occurring at the model summary side instead of the peer side and only one model summary is used for each evaluation; while there could be multiple references used in BLEU and Count clip (n-gram) could come from matching different reference translations.
"Furthermore, instead of a brevity penalty that punishes overly short translations, a brevity bonus, BB, should be awarded to shorter summaries that contain equivalent content."
"In fact, a length adjusted average coverage score was used as an alternative performance metric[REF_CITE]."
"However, we set the brevity bonus (or penalty) to 1 for all our experiments in this paper."
"In summary, the n-gram co-occurrence statistics we use in the following sections are based on the following formula:"
"Ngram(i, j) ="
BB • exp ∑ w logC  (6) j n  n  n=i
"Where j ≥ i, i and j range from 1 to 4, and w n is 1/(j-i+1)."
"Ngram(1, 4) is a weighted variable length n-gram match score similar to the IBM BLEU score; while Ngram(k, k), i.e. i = j = k, is simply the average k-gram coverage score C k ."
"With these formulas, we describe how to evaluate them in the next section."
"In order to evaluate the effectiveness of automatic evaluation metrics, we propose two criteria: 



 1."
"Automatic evaluations should correlate highly, positively, and consistently with human assess-ments. 2."
The statistical significance of automatic evaluations should be a good predictor of the statistical signifi-cance of human assessments with high reliability.
"The first criterion ensures whenever a human recognizes a good summary/translation/system, an automatic evaluation will do the same with high probability."
"This enables us to use an automatic evaluation procedure in place of human assessments to compare system per-formance, as in the NIST MT evaluations[REF_CITE]."
The second criterion is critical in interpreting the sig-nificance of automatic evaluation results.
"For example, if an automatic evaluation shows there is a significant difference between run A and run B at α = 0.05 using the z-test (t-test or bootstrap resampling), how does this translate to “real” significance, i.e. the statistical signifi-cance in a human assessment of run A and run B?"
"Ide-ally, we would like there to be a positive correlation between them."
"If this can be asserted with strong reli-ability (high recall and precision), then we can use the automatic evaluation to assist system development and to be reasonably sure that we have made progress."
"As stated in Section 3, direct application of BLEU on the[REF_CITE]data showed promising results."
"However, BLEU is a precision-based metric while the human evaluation protocol in DUC is essentially recall-based."
We therefore prefer the metric given by equation 6 and use it in all our experiments.
"We then compare the Ngram(1,4) ranking with the human ranking."
Figure 2 shows the result[REF_CITE]multi-document data.
"Stopwords are ignored during the computation of Ngram(1,4) scores and words are stemmed using a Por-ter stemmer[REF_CITE]."
"The x-axis is the human ranking and the y-axis gives the corresponding Ngram(1,4) rankings for summaries of difference sizes."
The straight line marked by AvgC is the ranking given by human assessment.
"For example, a system at (5,8) means that human ranks its performance at the 5 th rank while Ngram(1,4) 400 ranks it at the 8 th ."
"If an automatic ranking fully matches the human ranking, its plot will coincide with the heavy diagonal."
A line with less de-viation from the heavy diagonal line indicates better correlation with the human assessment.
"To quantify the correlation, we compute the Spearman rank order correlation coefficient (ρ) for each N-gram(1,4) n run at different summary sizes (n)."
We also test the effect of inclusion or exclusion of stopwords.
The results are summarized in Table 1.
"Although these results are statistically significant (α = 0.025) and are comparable to IBM BLEU’s correlation figures shown in Section 3, they are not consistent across summary sizes and tasks."
"For example, the corre-lations of the single document task are at the 60% level; while they range from 50% to 80% for the multi-document task."
The inclusion or exclusion of stopwords also shows mixed results.
"In order to meet the require-ment of the first criterion stated in Section 3, we need better results."
"The Ngram(1,4) n score is a weighted average of variable length n-gram matches."
"By taking a log sum of the n-gram matches, the Ngram(1,4) n favors match of longer n-grams."
"For example, if “United States of America” occurs in a reference summary, while one peer sum-mary, A, uses “United States” and another summary, B, uses the full phrase “United States of America”, sum-mary B gets more contribution to its overall score sim-ply due to the longer version of the name."
"However, intuitively one should prefer a short version of the name in summarization."
"Therefore, we need to change the weighting scheme to not penalize or even reward shorter equivalents."
We conduct experiments to understand the effect of individual n-gram co-occurrence scores in ap-proximating human assessments.
Tables 2 and 3 show the results of these runs without and with stopwords respectively.
"For each set[REF_CITE]data, single document 100-word summarization task, multi-document 50, 100, 200, and 400 -word summarization tasks, we compute 4 dif-ferent correlation statistics: Spearman rank order corre-lation coefficient (Spearman ρ), linear regression t-test (LR t , 11 degree of freedom for single document task and 13 degree of freedom for multi-document task), Pearson product moment coefficient of correlation (Pearson ρ), and coefficient of determination (CD) for each Ngram(i,j) evaluation metric."
"Among them Spearman ρ is a nonparametric test, a higher number indicates higher correlation; while the other three tests are para-metric tests."
"Higher LR t , Pearson ρ, and CD also sug-gests higher linear correlation."
"Analyzing all runs according to Tables 2 and 3, we make the following observations: (1) Simple unigram, Ngram(1,1), and bi-gram, Ngram(2,2), co-occurrence statistics consistently 



 outperform (0.99 ≥ Spearman ρ ≥ 0.75) the weighted average of n-gram of variable length Ngram(1, 4) (0.88 ≥ Spearman ρ ≥ 0.55) in single and multiple document tasks when stopwords are ignored."
"Importantly, unigram performs especially well with Spearman ρ ranging from 0.88 to 0.99 that is better than the best case in which weighted average of variable length n-gram matches is used and is consistent across different data sets. (2) The performance of weighted average n-gram scores is in the range between bi-gram and tri-gram co-occurrence scores."
This might suggest some summaries are over-penalized by the weighted av-erage metric due to the lack of longer n-gram matches.
"For example, given a model string “United States, Japan, and Taiwan”, a candidate string “United States, Taiwan, and Japan” has a unigram score of 1, bi-gram score of 0.5, and tri-gram and 4-gram scores of 0 when the stopword “and” is ignored."
The weighted average n-gram score for the candidate string is 0. (3) Excluding stopwords in computing n-gram co-occurrence statistics generally achieves better cor-relation than including stopwords. 4.2 Statistical Significance of N-gram Co-Occurrence Scores versus Human As-sessments
"We have shown that simple unigram, Ngram(1,1), or bi-gram, Ngram(2,2), co-occurrence statistics based on equation 6 outperform the weighted average of n-gram matches, Ngram(1,4), in the previous section."
"To exam-ine how well the statistical significance in the automatic Ngram(i,j) metrics translates to real significance when human assessments are involved, we set up the follow-ing test procedures: (1) Compute pairwise statistical significance test such as z-test or t-test for a system pair (X,Y) at certain α level, for example α = 0.05, using automatic met-rics and human assigned scores. (2) Count the number of cases a z-test indicates there is a significant difference between X and Y based on the automatic metric."
Call this number N As . (3) Count the number of cases a z-test indicates there is a significant difference between X and Y based on the human assessment.
Call this number N Hs . (4) Count the cases when an automatic metric predicts a significant difference and the human assessment also does.
Call this N hit .
"For example, if a z-test in-dicates system X is significantly different from Y with α = 0.05 based on the automatic metric scores and the corresponding z-test also suggests the same based on the human agreement, then we have a hit. (5) Compute the recall and precision using the follow-ing formulas: recall = N hit N Hs precision = N hit N As"
A good automatic metric should have high recall and precision.
This implies that if a statistical test indicates a significant difference between two runs using the auto-matic metric then very probably there is also a signifi-cant difference in the manual evaluation.
This would be very useful during the system development cycle to gauge if an improvement is really significant or not.
Figure 3 shows the recall and precision curves for the[REF_CITE]single document task at different α levels and Figure 4 is for the multi-document task with differ- gram co-occurrence statistics versus human assessment[REF_CITE]single document task.
The 5 points on each curve represent val-ues for the 5 α levels. co-occurrence statistics versus human assessment[REF_CITE]multi-document task.
"Dark (black) solid lines are for average of all summary sizes, light (red) solid lines are for 50-word summaries, dashed (green) lines are for 100-word summaries, dash-dot lines (blue) are for 200-word summaries, and dotted (magenta) lines are for 400-word summaries. ent summary sizes."
Both of them exclude stopwords.
"We use z-test in all the significance tests with α level at 0.10, 0.05, 0.25, 0.01, and 0.005."
"From Figures 3 and 4, we can see Ngram(1,1) and Ngram(2,2) reside on the upper right corner of the recall and precision graphs."
"Ngram(1,1) has the best overall behavior."
"These graphs confirm Ngram(1,1) (simple unigram) is a good automatic scoring metric with good statistical significance prediction power."
"In this paper, we gave a brief introduction of the manual summary evaluation protocol used in the Document Understanding Conference."
"We then discussed the IBM BLEU MT evaluation metric, its application to sum-mary evaluation, and the difference between precision-based BLEU translation evaluation and recall-based DUC summary evaluation."
The discrepancy led us to examine the effectiveness of individual n-gram co-occurrence statistics as a substitute for expensive and error-prone manual evaluation of summaries.
"To evalu-ate the performance of automatic scoring metrics, we proposed two test criteria."
One was to make sure system rankings produced by automatic scoring metrics were similar to human rankings.
This was quantified by Spearman’s rank order correlation coefficient and three other parametric correlation coefficients.
Another was to compare the statistical significance test results be-tween automatic scoring metrics and human assess-ments.
We used recall and precision of the agreement between the test statistics results to identify good auto-matic scoring metrics.
"According to our experiments, we found that unigram co-occurrence statistics is a good automatic scoring metric."
It consistently correlated highly with human assessments and had high recall and precision in signifi-cance test with manual evaluation results.
"In contrast, the weighted average of variable length n-gram matches derived from IBM BLEU did not always give good cor-relation and high recall and precision."
"We surmise that a reason for the difference between summarization and machine translation might be that extraction-based summaries do not really suffer from grammar problems, while translations do."
Longer n-grams tend to score for grammaticality rather than content.
It is encouraging to know that the simple unigram co-occurrence metric works in the[REF_CITE]setup.
The reason for this might be that most of the systems par-ticipating in DUC generate summaries by sentence ex-traction.
We plan to run similar experiments[REF_CITE]data to see if unigram does as well.
"If it does, we will make available our code available via a website to the summarization community."
"Although this study shows that unigram co-occurrence statistics exhibit some good properties in summary evaluation, it still does not correlate to human assess-ment 100% of the time."
There is more to be desired in the recall and precision of significance test agreement with manual evaluation.
We are starting to explore vari-ous metrics suggested[REF_CITE].
"For example, weight n-gram matches differently according to their information content measured by tf, tfidf, or"
"In fact, NIST MT automatic scoring metric[REF_CITE]already integrates such modifications."
One future direction includes using an automatic ques-tion answer test as demonstrated in the pilot study in SUMMAC[REF_CITE].
"In that study, an auto-matic scoring script developed by Chris Buckley showed high correlation with human evaluations, al-though the experiment was only tested on a small set of 3 topics."
"According[REF_CITE], NIST spent about 3,000 man hours each[REF_CITE]and 2002 for topic and docu-ment selection, summary creation, and manual evalua-tion."
"Therefore, it would be wise to use these valuable resources, i.e. manual summaries and evaluation results, not only in the formal evaluation every year but also in developing systems and designing automatic evaluation metrics."
We would like to propose an annual automatic evaluation track in DUC that encourages participants to invent new automated evaluation metrics.
Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation met-rics.
The best automatic metric will be posted at the DUC website and used as an alternative in-house and repeatable evaluation mechanism during the next year.
In this way the evaluation technologies can advance at the same pace as the summarization technologies im-prove.
",  v[q}$ u}%).)NiUr[} 8  ,¢£i¤¥,¦c ¤N§iU¨¤©¨°¢±r},¥¨ªi v¢¦@r  ¦Nr¯@)}. ¥ ,} r¨). ¯.  ,rª7«z¬­¤}rª®¤}   r¯$¤Nr¯viU8}µrª,} [ ¤ªi:}T} ©·¤Nru¢8u¯©·¤}¹¸ ,} ,V}    ¦v  U¥ºrªi$rUri}.« ¹ªi¨¦v¤N  ,±¤N }¤ ©qNrF±rU¨viU8}\¤±¸¯U¨},¥®rUr}U N¦cr¤±¸»¦v¤H¼u¤}% ©·¤}},©° ©·,,}½}F$º).$ u§8¤¥U.}¤ © }, 8¢±¤N,@F¾N,±T¦v}« ¿ N1 | ~iÀ &lt;³Á³ iÂ8À| 1* ¦vÃ , rª},,³¤},[,  }Ä¥¨¨$}U,r¤N}r,)@¼rT¦cr,rF:,rª,­}r)[ª,¥v¥"
Recent TREC results have demonstrated the need for deeper text understanding methods.
This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system.
The approach is to transform questions and answer passages into logic representations.
World knowledge axioms as well as linguistic ax-ioms are supplied to the prover which renders a deep understanding of the relationship be-tween question text and answer text.
"Moreover, the trace of the proofs provide answer justifica-tions."
The results show that the prover boosts the performance of the QA system on TREC questions by 30%.
"Motivation In spite of significant advances made recently in the Question Answering technology, there still remain many problems to be solved."
"Some of these are: bridging the gap between question and answer words, pinpointing ex-act answers, taking into consideration syntactic and se-mantic roles of words, better answer ranking, answer jus-tification, and others."
The recent TREC results[REF_CITE]have demonstrated that many performing systems reached a plateau; the systems ranked from 4th to 14th answered correctly between 38.4% to 24.8% of the total number of questions.
It is clear that new ideas based on a deeper language understanding are necessary to push further the QA technology.
"In this paper we introduce one such novel idea, the use of automated reasoning in QA, and show that it is fea-sible, effective, and scalable."
"We have implemented a Logic Prover, called COGEX (from the permutation of the first two syllables of the verb excogitate) which uni-formly codifies the question and answer text, as well as world knowledge resources, in order to use its inference engine to verify and extract any lexical relationships be-tween the question and its candidate answers."
"The QA system includes traditional modules such as question processing, document retrieval, answer extrac-tion, built in ontologies, as well as many tools such as syntactic parser, name entity recognizer, word sense dis-ambiguati[REF_CITE], logic rep-resentation of text[REF_CITE]and others."
The Logic Prover is integrated in this rich NLP environ-ment and augments the QA system operation.
"As shown in Figure 1, the inputs to COGEX consist of logic representations of questions, potential answer para-graphs, world knowledge and lexical information."
The term Answer Logic Form (ALF) refers to the candidate answers in logic form.
Candidate answers returned by the Answer Extraction module are classified as open text due to the unpredictable nature of their grammatical struc-ture.
The term Question Logic Form (QLF) refers to the questions posed to the Question Answering system rep-resented in logic form.
The prover also needs world knowledge axioms sup-plied by the WordNet glosses transformed into logic rep-resentations.
"Additionally there are many other axioms representing equivalence classes of linguistic patterns, called NLP axioms."
All these are described below.
"The Axiom Builder converts the Logic Forms for the question, the glosses, and its candidate answers into ax-ioms."
"Based on the parse tree patterns in the question and answers, other NLP axioms are built to supplement the existing general NLP axioms."
"Once the axioms are complete and loaded, justification of the answer begins."
"If a proof fails, the relaxation module is invoked."
"The purpose of this module is twofold: (1) to compensate for errors in the text parsing and Logic Form transfor-mation phase, such as prepositional attachments and sub-ject/object detection in verbs, (2) to detect correct an-swers when the NLP and XWN (Extended WordNet) ax-ioms fail to provide all the necessary inferences."
"During the relaxation, arguments to predicates in the question are incrementally uncoupled, the proof score is reduced, and the justification is re-attempted."
"The loop between the Justification and the Relaxation modules continues until the proof succeeds, or the proof score is below a prede-fined threshold."
"When all the candidate answers are pro-cessed, the candidate answers are ranked based on their proof scores, with the output from COGEX being the ranked answers and the answer justifications."
A text logic form (LF) is an intermediary step between syntactic parse and the deep semantic form.
"The LF cod-ification acknowledges syntax-based relationships such as: (1) syntactic subjects, (2) syntactic objects, (3) prepo- sitional attachments, (4) complex nominals, and (5) ad-jectival/adverbial adjuncts."
Our approach is to derive the LF directly from the output of the syntactic parser which already resolves structural and syntactic ambiguities.
Essentially there is a one to one mapping of the words of the text into the predicates in the logic form.
The pred-icate names consist of the base form of the word concate-nated with the part of speech of the word.
Each noun has an argument that is used to represent it in other predi-cates.
One of the most important features of the Logic Form representation is the fixed-slot allocation mecha-nism of the verb predicates[REF_CITE].
This allows for the Logic Prover to see the difference between the role of the subjects and objects in a sentence that is not answerable in a keyword based situation.
Logic Forms are derived from the grammar rules found in the parse tree of a sentence.
There are far too many grammar rules in the English language to efficiently and realistically implement them all.
We have observed that the top ten most frequently used grammar rules cover 90% of the cases for WordNet glosses.
This is referred to as the 10-90 rule[REF_CITE].
Below we provide a sample sentence and its corresponding LF representation.
Heavy selling of Standard &amp; Poor’s 500-stock index fu-tures in Chicago relentlessly beat stocks downward.
"LF: heavy JJ(x1) &amp; selling NN(x1) &amp; of IN(x1,x6) &amp; Standard NN(x2) &amp; &amp; CC(x13,x2,x3) &amp; Poor NN(x3) &amp;’s POS(x6,x13) &amp; 500-stock JJ(x6) &amp; index NN(x4) &amp; future NN(x5) &amp; nn NNC(x6,x4,x5) &amp; in IN(x1,x8) &amp; Chicago NN(x8) &amp; relentlessly RB(e12) &amp; beat VB(e12,x1,x9) &amp; stocks NN(x9) &amp; down-ward RB(e12)"
Logic representation of WordNet glosses A major problem in QA is that often an answer is expressed in words different from the question keywords.
World knowledge is necessary to conceptually link ques-tions and answers.
WordNet glosses contain a source of world knowledge.
"To be useful in automated reasoning, the glosses need to be transformed into logic forms."
"Taking the same approach as for open text, we have parsed and represented in logic forms more than 50,000 WordNet glosses."
"For example, the gloss definition of concept sport NN#1 is an active diversion requiring physical exertion and competition, which yields the logic representation: active JJ(x1) &amp; diversion NN(x1) &amp; re-quire VB(e1,x1,x2) &amp; or CC(x2,x3,x4) &amp; physi-cal JJ(x3) &amp; exertion NN(x3) &amp; competition NN(x4)"
"In additions to world knowledge axioms, a QA Logic Prover needs linguistic knowledge."
This is what distin-guishes an NLP prover from a traditional mathematical prover.
General axioms that reflect equivalence classes of linguistic patterns need to be created and instantiated when invoked.
We call these NLP axioms and present below some examples together with questions that call them.
"Axiom partitioning mechanism The search strategy used is the Set of Support Strategy, which partitions the axioms used during the course of a proof into those that have support and those that are considered auxiliary[REF_CITE]."
The axioms with sup-port are placed in the Set of Support (SOS) list and are intended to guide the proof.
The auxiliary axioms are placed in the Usable list and are used to help the SOS infer new clauses.
This strategy restricts the search such that a new clause is inferred if and only if one of its par-ent clauses come from the Set of Support.
"The axioms that are placed in the SOS are the candidate answers, the question negated (to invoke the proof by contradiction), and axioms related to linking named entities to answer types."
"Axioms placed in the Usable list are: (1) Extended WordNet axioms, (2) NLP axioms, and (3) axioms based on outside world knowledge, such as people and organi-zations."
Inference rules The inference rule sets are based on hyperresolution and paramodulation.
"Hyperresolution is an inference rule that does multiple binary resolution steps in one, where bi-nary resolution is an inference mechanism that looks for a positive literal in one clause and negative form of that same literal in another clause such that the two literals can be canceled, resulting in a newly inferred clause."
Paramodulation introduces the notion of equality substi-tution so that axioms representing equality in the proof do not need to be explicitly included in the axiom lists.
"Additionally, similar to hyperresolution, paramodulation combines multiple substitution steps into one."
All modern theorem provers use hyperresolution and paramodulation inference rules since they allow for a more compact and efficient proof by condensing multi-ple steps into one.
"COGEX will continue trying to find a proof until the Set of Support becomes empty, a refutation is found, or the proof score drops below a predefined threshold."
"Two techniques have been implemented in COGEX to deal with incomplete proofs: 1. Count the number of unifications/resolutions with terms in the question along the longest search path in the proof attempts, and 2. Relax the question logic form by incrementally un-coupling arguments in the predicates, and/or removing prepositions or modifiers that are not crucial to the mean-ing of the text."
"For example in question, “How far is Yaroslavl from Moscow?” a candidate answer is “.. Yaroslavl, a city 250 miles north of Moscow.”"
By dropping the from predicate in the question makes the proof succeed for the candidate answer.
The following example illustrates how all these pieces are put together to generate answer proofs.
"QLF: organization AT(x2) ) &amp; company NN(x2) &amp; cre-ate VB(e1,x2,x6) &amp; Internet NN(x3) &amp; browser NN(x4) &amp; Mosaic NN(x5) &amp; nn NNC(x6,x3,x4,x5)"
"Question Axiom: -(exists e1 x2 x3 x5 x6 ( organization at(x2) &amp; company nn(x2) &amp; create vb(e1,x2,x6) &amp; inter-net nn(x3) &amp; browser nn(x4) &amp; mosaic nn(x5) &amp; nn nnc(x6,x3,x4,x5)))."
COGEX was implemented and integrated into a state-of-the-art Question Answering system that participated[REF_CITE].
"All questions are attempted by the prover, but if the proof fails the QA system resorts to other an-swer extraction methods that were part of the system be-fore the prover."
"Thus, some questions are answered by the QA system without the prover, some only by the prover and some by both the non-prover system and the prover."
The complete system answered 415 questions out of 500[REF_CITE]questions.
A careful anal-ysis indicates that the QA system without logic prover an-swered 317 questions and the prover can answer only 98 additional questions for which the system without prover failed.
Table 1 summarizes these results.
The added value of automated reasoning to the QA sys-tem is 30.9% (98/317).
This represents a significant im-provement in the performance of the logic prover for QA over the one reported[REF_CITE].
The failures of the prover are due primarily to the lack of linguistic axioms.
"A logic prover brings several advantages to question an-swering, but at a high cost."
"Some advantages are: the ca-pability of pinpointing exact answers that otherwise will be missed, answer justification, and a quantifiable mea-sure of how close a system is to providing an answer."
"However, the implementation of a QA logic prover is ex-pensive as it requires logic representation of text, world knowledge axioms and a large number of linguistic ax-ioms, that all take time to develop."
We investigate single-view algorithms as an al-ternative to multi-view algorithms for weakly supervised learning for natural language pro-cessing tasks without a natural feature split.
"In particular, we apply co-training, self-training, and EM to one such task and find that both self-training and FS-EM, a new variation of EM that incorporates feature selection, outperform co-training and are comparatively less sensitive to parameter changes."
"Multi-view weakly supervised learning paradigms such as co-training[REF_CITE]and co-EM[REF_CITE]learn a classification task from a small set of labeled data and a large pool of unla-beled data using separate, but redundant, views of the data (i.e. using disjoint feature subsets to represent the data)."
"Multi-view learning has been successfully ap-plied to a number of tasks in natural language processing (NLP), including text classificati[REF_CITE], named entity classifica-ti[REF_CITE], base noun phrase brack-eting[REF_CITE], and statistical parsing[REF_CITE]."
The theoretical performance guarantees of multi-view weakly supervised algorithms come with two fairly strong assumptions on the views.
"First, each view must be sufficient to learn the given concept."
"Second, the views must be conditionally independent of each other given the class label."
"When both conditions are met, Blum and Mitchell prove that an initial weak learner can be boosted using unlabeled data."
"Unfortunately, finding a set of views that satisfies both of these conditions is by no means an easy problem."
"In addition, recent empirical results[REF_CITE]and[REF_CITE]have shown that multi-view algorithms are quite sensitive to the two underlying as-sumptions on the views."
"Effective view factorization in multi-view learning paradigms, therefore, remains an im-portant issue for their successful application."
"In practice, views are supplied by users or domain experts, who deter-mine a natural feature split that is expected to be redun-dant (i.e. each view is expected to be sufficient to learn the target concept) and conditionally independent given the class label. [Footnote_1]"
"1[REF_CITE]argues that the conditional independence as-sumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices."
"We investigate here the application of weakly super-vised learning algorithms to problems for which no obvi-ous natural feature split exists and hypothesize that, in these cases, single-view weakly supervised algorithms will perform better than their multi-view counterparts."
"Motivated, in part, by the results[REF_CITE], we use the task of noun phrase coreference resolution for illustration throughout the paper. [Footnote_2]"
"2[REF_CITE]explore a heuristic method for view factorization for the related problem of anaphora resolution, but find that co-training shows no performance improvements for any type of German anaphor except pronouns over a baseline classifier trained on a small set of labeled data."
"In our experi-ments, we compare the performance of the Blum and Mitchell co-training algorithm with that of two com-monly used single-view algorithms, namely, self-training and Expectation-Maximization (EM)."
"In comparison to co-training, self-training achieves substantially superior performance and is less sensitive to its input parameters."
"EM, on the other hand, fails to boost performance, and we attribute this phenomenon to the presence of redun-dant features in the underlying generative model."
"Con-sequently, we propose a wrapper-based feature selection method[REF_CITE]for EM that results in perfor-mance improvements comparable to that observed with self-training."
"Overall, our results suggest that single-view weakly supervised learning algorithms are a viable al-ternative to multi-view algorithms for data sets where a natural feature split into separate, redundant views is not available."
The remainder of the paper is organized as follows.
Section 2 presents an overview of the three weakly su-pervised learning algorithms mentioned previously.
"In section 3, we introduce noun phrase coreference resolu-tion and describe the machine learning framework for the problem."
"In section 4, we evaluate the weakly supervised learning algorithms on the task of coreference resolution."
Section 5 introduces a method for improving the perfor-mance of weakly supervised EM via feature selection.
We conclude with future work in section 6.
"In this section, we give a high-level description of our im-plementation of the three weakly supervised algorithms that we use in our comparison, namely, co-training, self-training, and EM."
Co-training[REF_CITE]is a multi-view weakly supervised algorithm that trains two classifiers that can help augment each other’s labeled data using two separate but redundant views of the data.
"Each classifier is trained using one view of the data and predicts the la-bels for all instances in the data pool, which consists of a randomly chosen subset of the unlabeled data."
Each then selects its most confident predictions from the pool and adds the corresponding instances with their predicted labels to the labeled data while maintaining the class dis-tribution in the labeled data.
The number of instances to be added to the labeled data by each classifier at each iteration is limited by a pre-specified growth size to ensure that only the instances that have a high probability of being assigned the correct label are incorporated.
The data pool is refilled with in-stances drawn from the unlabeled data and the process is repeated for several iterations.
"During testing, each clas-sifier makes an independent decision for a test instance and the decision associated with the higher confidence is taken to be the final prediction for the instance."
Self-training is a single-view weakly supervised algo-rithm that has appeared in various forms in the literature.
The version of the algorithm that we consider here is a variation of the one presented[REF_CITE].
"Initially, we use bagging[REF_CITE]to train a committee of classifiers using the labeled data."
"Specifi-cally, each classifier is trained on a bootstrap sample cre-ated by randomly sampling instances with replacement from the labeled data until the size of the bootstrap sam-ple is equal to that of the labeled data."
Then each member of the committee (or bag) predicts the labels of all unla-beled data.
The algorithm selects an unlabeled instance for adding to the labeled data if and only if all bags agree upon its label.
This ensures that only the unlabeled in-stances that have a high probability of being assigned the correct label will be incorporated into the labeled set.
The above steps are repeated until all unlabeled data is labeled or a fixed point is reached.
The use of EM as a single-view weakly supervised clas-sification algorithm is introduced[REF_CITE].
"Like the classic unsupervised EM algorithm[REF_CITE], weakly supervised EM assumes a paramet-ric model of data generation."
The labels of the unlabeled data are treated as missing data.
The goal is to find a model such that the posterior probability of its parame-ters is locally maximized given both the labeled data and the unlabeled data.
"Initially, the algorithm estimates the model parame-ters by training a probabilistic classifier on the labeled instances."
"Then, in the E-step, all unlabeled data is prob-abilistically labeled by the classifier."
"In the M-step, the parameters of the generative model are re-estimated us-ing both the initially labeled data and the probabilistically labeled data to obtain a maximum a posteriori (MAP) hy-pothesis."
The E-step and the M-step are repeated for sev-eral iterations.
The resulting model is then used to make predictions for the test instances.
Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.
"In this sec-tion, we give an overview of the coreference resolution system to which the weakly supervised algorithms de-scribed in the previous section are applied."
The framework underlying the system is a standard combination of classification and clustering employed by supervised learning approaches (e.g.[REF_CITE]).
"Specifically, coreference res-olution is recast as a classification task, in which a pair of NPs is classified as co-referring or not based on con-straints that are learned from an annotated corpus."
Train-ing instances are generated by pairing each NP with each of its preceding NPs in the document.
The classification associated with a training instance is one of COREFER - ENT or NOT COREFERENT depending on whether the NPs co-refer in the text.
A separate clustering mechanism then coordinates the possibly contradictory pairwise classifi-cations and constructs a partition on the set of NPs.
We perform the experiments in this paper using our coreference resolution system (see[REF_CITE]).
"For the sake of complete-ness, we include the descriptions of the 25 features employed by the system in Table 1."
"Linguistically, the features can be divided into five groups: lexical, grammatical, semantic, positional, and others."
"However, we use naive Bayes rather than decision tree induction as the underlying learning algorithm to train a coreference classifier, simply because (1) it provides a generative model assumed by EM and hence facilitates comparison between different approaches and (2) it is more robust to the skewed class distributions inherent in coreference data sets than decision tree learners."
"When the corefer-ence system is used within the weakly supervised setting, a weakly supervised algorithm bootstraps the corefer- ence classifier from the given labeled and unlabeled data rather than from a much larger set of labeled instances."
We conclude this section by noting that view factor-ization is a non-trivial task for coreference resolution.
"For many lexical tagging problems such as part-of-speech tagging, views can be drawn naturally from the left-hand and right-hand context."
"For other tasks such as named en-tity classification, views can be derived from features in-side and outside the phrase under considerati[REF_CITE]."
"Unfortunately, neither of these op-tions is possible for coreference resolution."
We will ex-plore several heuristic methods for view factorization in the next section.
"In this section, we empirically test our hypothesis that single-view weakly supervised algorithms can potentially outperform their multi-view counterparts for problems without a natural feature split."
"To ensure a fair comparison of the weakly supervised algorithms, the experiments are designed to determine the best parameter setting of each algorithm (in terms of its effectiveness to improve performance) for the data sets we investigate."
"Specifically, we keep the parame-ters common to all three weakly supervised algorithms (i.e. the labeled and unlabeled data) constant and vary the algorithm-specific parameters, as described below."
We use the[REF_CITE]and[REF_CITE]coreference data sets for evaluation.
"The training set is composed of 30 “dry run” texts, 1 of which is se-lected to be the annotated text and the remaining 29 texts are used as unannotated data."
For[REF_CITE]training instances are generated from 84 NPs in the annotated text.
For[REF_CITE]training instances are generated from 87 NPs.
"The unlabeled data is composed of 488173 in-stances and 478384 instances for the MUC-6 and MUC-7 data sets, respectively."
Testing is performed by applying the bootstrapped coreference classifier and the clustering algorithm described in section 3 on the 20–30 “formal evaluation” texts for each of the MUC-6 and MUC-7 data sets.
The co-training parameters are set as follows.
We tested three pairs of views.
Table 2 re-produces the 25 features of the coreference system and shows the views we employ.
"Specifically, the three view pairs are generated by the following methods."
Mueller et al.’s heuristic method.
"Starting from two empty views, the iterative algorithm selects for each view the feature whose addition maximizes the per-formance of the respective view on the labeled data at each iteration. 3 This method produces the view pair V1 and V2 in Table 2 for the MUC-6 data set."
A different view pair is produced for MUC-7.
Random splitting of features into views.
"Starting from two empty views, an iterative algorithm that randomly chooses a feature for each view at each step is used to split the feature set."
The resulting view pair V[Footnote_3] and V4 is used for both the MUC-6 and MUC-7 data sets.
3 Space limitation precludes a detailed description of this method.[REF_CITE]for details.
Splitting of features according to the feature type.
"Specifically, one view comprises the lexico-syntactic features and the other the remaining ones."
"This approach produces the view pair V5 and V6, which is used for both data sets."
"We tested pool sizes of 500, 1000, 5000."
"We tested values of 10, 50, 100, 200, 250."
Number of co-training iterations.
We monitored per-formance on the test data at every 10 iterations of co-training and ran the algorithm until performance stabi-lized.
"Given the labeled and unla-beled data, self-training requires only the specification of the number of bags."
We tested all odd number of bags between 1 and 25.
"Given the labeled and unlabeled data, EM has only one parameter — the number of iterations."
We ran EM to convergence and kept track of its test set performance at every iteration.
"Results are shown in Table 3, where performance is re-ported in terms of recall, precision, and F-measure using the model-theoretic MUC scoring program[REF_CITE]."
"The baseline coreference system, which is trained only on the labeled document using naive Bayes, achieves an F-measure of 55.5 and 43.8 on the MUC-6 and MUC-7 data sets, respectively."
The results shown in row 2 of Table 3 correspond to the best F-measure scores achieved by co-training for the two data sets based on co-training runs that comprise all of the parameter combinations described in the previous subsection.
The parameter settings with which the best results are obtained are also shown in the table.
"To get a better picture of the behavior of co-training, we present the learning curve for the co-training run that gives rise to the best F-measure for the MUC-6 data set in Figure 1."
"The horizontal (dotted) line shows the performance of the baseline system, which achieves an F-measure of 55.5, as described above."
"As co-training progresses, F-measure peaks at iteration 220 and then gradually drops below that of the baseline after iteration 570."
"Although co-training produces substantial improve-ments over the baseline at its best parameter settings, a closer examination of our results reveals that they cor-roborate previous findings: the algorithm is sensitive not only to the number of iterations, but to other input pa-rameters such as the pool size and the growth size as well[REF_CITE]."
The lack of a principled method for determining these param-eters in a weakly supervised setting where labeled data is scarce remains a serious disadvantage for co-training.
Self-training results are shown in row 3 of Table 3: self-training performs substantially better than both the baseline and co-training for both data sets.
"In contrast to co-training, however, self-training is relatively insensi- tive to its input parameter."
Figure 2 shows the fairly con-sistent performance of self-training with seven or more bags for the MUC-6 data set.
We observe similar trends for the MUC-7 data set.
These results are consistent with empirical studies of bagging across a variety of classifi-cation tasks where seven to 25 bags are deemed sufficient[REF_CITE].
"To gain a deeper insight into the behavior of self-training, we plot the learning curve for self-training using 7 bags in Figure 3, again for the MUC-6 data set."
"At itera-tion 0 (i.e. before any unlabeled data is incorporated), the F-measure score achieved by self-training is higher than that of the baseline system (58.5 vs. 55.5)."
The observed difference is due to voting within the self-training algo-rithm.
Voting has proved to be an effective technique for improving the accuracy of a classifier when training data is scarce by reducing the variance of a particular training corpus[REF_CITE].
"After the first iteration, there is a rapid increase in F-measure, which is accompanied by large gains in precision and smaller drops in recall."
"These results are consistent with our intuition regarding self-training: at each iteration the algorithm incorporates only instances whose label it is most confident about into the labeled data, thereby ensuring that precision will in-crease. 4"
"As we can see from Table 3, the recall level achieved by co-training is much lower than that of self-training."
This is an indication that each co-training view is insuf-ficient to learn the concept: the feature split limits any interaction of features in different views that might pro-duce better recall.
"Overall, these results provide evidence that self-training is a better alternative to co-training for weakly supervised learning for problems such as corefer-ence resolution where no natural feature split exists."
"On the other hand, EM only gives rise to modest per-formance gains over the baseline system, as we can see from row [Footnote_4] of Table 3."
"4 When tackling the task of confusion set disambiguation,[REF_CITE]observe only modest gains from self-training by bootstrapping from a seed corpus of one million words. We speculate that a labeled data set of this size can possibly enable them to train a reasonably good classifier with which self-training can only offer marginal benefits, but the re-lationship between the behavior of self-training and the size of the seed (labeled) corpus remains to be shown."
"The performance of EM depends in part on the correctness of the underlying generative model[REF_CITE], which in our case is naive Bayes."
"In this model, an instance with feature values  ,  ,  and class is created by first choosing the class with prior probability  and then generat-ing each available feature  with probability  &quot;!   independently, under the assumption that the feature val-ues are conditionally independent given the class."
"As a result, model correctness is adversely affected by redun-dant features, which clearly invalidate the conditional in-dependence assumption."
"In fact, naive Bayes is known to be bad at handling redundant features[REF_CITE]."
We hypothesize that the presence of redundant fea- tures causes the generative model and hence EM to per-form poorly.
"Although self-training depends on the same model, it only makes use of the binary decisions returned by the model and is therefore more robust to the naive Bayes assumptions, as reflected in its fairly impressive empirical performance. [Footnote_5]"
5 It is possible for naive Bayes classifiers to return optimal classifications even if the conditional independence assumption is violated.[REF_CITE]for an analysis.
"In contrast, the fact that EM re-lies on the probability estimates of the model makes it more sensitive to the correctness of the model."
"If our hypothesis regarding the presence of redundant features were correct, then feature selection could re-sult in an improved generative model, which could in turn improve the performance of weakly supervised EM."
This section discusses a wrapper-based feature selection method for EM.
We now describe the FS-EM algorithm for boosting the performance of weakly supervised algorithms via feature selection.
"Although named after EM, the algorithm as de-scribed is potentially applicable to all single-view weakly supervised algorithms."
"FS-EM takes as input a super-vised learner, a single-view weakly supervised learner, a labeled data set # , and an unlabeled data set $ ."
"In addi-tion, it assumes knowledge of the positive class prior (i.e. the true percentage of positive instances in the data) like co-training and requires a deviation threshold that we will explain shortly."
"FS-EM, which has a two-level bootstrapping structure, is reminiscent of the meta-bootstrapping algorithm intro-duced[REF_CITE]."
"The outer-level boot-strapping task is feature selection, whereas the inner-level task is to learn a bootstrapped classifier from labeled and unlabeled data as described in section 4."
"At a high level, FS-EM uses a forward feature selection algorithm to im-pose a total ordering on the features based on the order in which the features are selected."
"Specifically, FS-EM per-forms the three steps below for each feature %&apos;&amp; that has not been selected."
"First, it uses the weakly supervised learner to train a classifier ( from the labeled and unla-beled data ( # ) $ ) using only the feature %&apos;&amp; as well as the features selected thus far."
"Second, the algorithm uses ( to classify all of the instances in #*+) $ ."
"Finally, FS-EM trains a new model on just $ , which is now labeled by ( ."
"At the end of the three steps, exactly one model is trained for each feature that has not been selected."
"The forward selection algorithm then selects the feature with which the corresponding model achieves the best performance on # (w.r.t. the true labels of the instances in # ) for addi-tion to , /. (the set of features selected thus far). 6 The process is repeated until all features have been selected."
"Unfortunately, since # can be small, selecting a fea-ture for incorporation into , -1.20 by measuring the per-formance of the corresponding model on # may not ac-curately reflect the actual model performance."
"To han-dle this problem, FS-EM has a preference for adding fea-tures whose inclusion results in a classification in which the positive class prior (i.e. the probability that an in-stance is labeled as positive), 3 &amp; , does not deviate from the true positive class prior, 3 , by more than a pre-specified threshold value, 4 ."
A large deviation from the true prior is an indication that the resulting classification of the data does not correspond closely to the actual classification.
This algorithmic bias is particularly useful for weakly su-pervised learners (such as EM) that optimize an objective function other than classification accuracy and can poten-tially produce a classification that is substantially differ-ent from the actual one.
"Specifically, FS-EM attempts to ensure that the classification produced by the weakly supervised learner weakly agrees with the actual classi-fication, where the weak disagreement rate between two classifications is defined as the difference between their positive class priors."
Note that weak agreement is a nec-essary but not sufficient condition for two classifications to be identical. 7
"Nevertheless, if the addition of any of the features to , -1.20 does not produce a classification that weakly agrees with the true one, FS-EM picks the feature whose inclu-sion results in a positive class prior that has the least de-viation instead."
This step can be viewed as introducing “pseudo-random” noise into the feature selection process.
"The hope is that the deviation of the high-scoring, “high-deviation” features can be lowered by first incorporating those with “low deviation”, thus continuing to strive for weak agreement while potentially achieving better per-formance on # ."
"The final set of features, ,&amp;&quot;7 , is composed of the first [Footnote_8] features chosen by the feature selection algorithm, where 8 is the largest number of features that can achieve the best performance on # subject to the condition that the corresponding classification produced by the weakly supervised algorithm weakly disagrees with the true one by at most 4 ."
8 Seven is used because we follow the choice of previous work[REF_CITE]. Addi-tional experiments in which EM is run for 5 and 9 iterations give similar results.
"The output of FS-EM is a classifier that the weakly supervised learner learns from # and $ using only the features in , &amp;&quot;[Footnote_7] ."
"7 In other words, ;=&lt;&gt;;/ does not imply that the correspond-ing classifications are identical."
The pseudo-code describing FS-EM is shown in Figure 4.
"We instantiate FS-EM with naive Bayes as the supervised learner and EM as the weakly supervised learner, provid-ing it with the same amount of labeled and unlabeled data as in previous experiments and setting 4 to 0.01."
EM is run for 7 iterations whenever it is invoked. 8 Results us-ing FS-EM are shown in row 5 of Table 3.
"In comparison to EM, F-measure increases from 57.6 to 65.4 for MUC-[Footnote_6], and from 46.4 to 60.5 for MUC-7, allowing FS-EM to even surpass the performance of self-training."
6 The reason for using only 9 (instead of 9 and : ) in the validation step is primarily to preclude the possibility of get-ting a poor estimation of model performance as a result of the presence of potentially inaccurately labeled data from : .
These results are consistent with our hypothesis that the perfor-mance of EM can be boosted by improving the underly-ing generative model using feature selection.
"Finally, although FS-EM is only applicable to two-class problems, it can be generalized fairly easily to han-dle multi-class problems, where the true label distribution is assumed to be available and the weak agreement rate can be measured based on the similarity of two distribu-tions."
We have investigated single-view algorithms (self-training and EM) as an alternative to multi-view algo-rithms (co-training) for weakly supervised learning for problems that do not appear to have a natural feature split.
Experimental results on two coreference data sets indi-cate that self-training outperforms co-training under vari-ous parameter settings and is comparatively less sensitive to parameter changes.
"While weakly supervised EM is not able to outperform co-training, we introduce a varia-tion of EM, FS-EM, for boosting the performance of EM via feature selection."
"Like self-training, FS-EM easily outperforms co-training."
Co-training algorithms such as CoBoost[REF_CITE]and Greedy Agreement[REF_CITE]that explicitly trade classifier agreement on unlabeled data against error on labeled data may be more robust to the underlying assumptions of co-training and can conceiv-ably perform better than the Blum and Mitchell algorithm for problems without a natural feature split. [Footnote_9] Other less studied single-view weakly supervised algorithms in the NLP community such as co-training with different learn-ing algorithms[REF_CITE]and graph mincuts[REF_CITE]can be similarly ap-plied to these problems to further test our original hy-pothesis.
"9 Indeed,[REF_CITE]show that, when the condi-tional independence assumption of the views is satisfied, view classifiers whose agreement on unlabeled data is explicitly max-imized will have low generalization error."
We plan to explore these possibilities in future research.
We describe a syntax-based algorithm that au-tomatically builds Finite State Automata (word lattices) from semantically equivalent transla-tion sets.
These FSAs are good representa-tions of paraphrases.
"They can be used to ex-tract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets."
"Our FSAs can also predict the correctness of alternative semantic renderings, which may be used to evaluate the quality of translations."
"In the past, paraphrases have come under the scrutiny of many research communities."
Information retrieval re-searchers have used paraphrasing techniques for query re-formulation in order to increase the recall of information retrieval engines[REF_CITE].
Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems[REF_CITE].
"And researchers in multi-document text summarizati[REF_CITE], information extracti[REF_CITE], and question answering[REF_CITE]have focused on identi-fying and exploiting paraphrases in the context of recog-nizing redundancies, alternative formulations of the same meaning, and improving the performance of question an-swering systems."
"In previous work[REF_CITE], paraphrases are represented as sets or pairs of semantically equiva-lent words, phrases, and patterns."
"Although this is ade-quate in the context of some applications, it is clearly too weak from a generative perspective."
"Assume, for exam-ple, that we know that text pairs (stock market rose, stock market gained) and (stock market rose, stock prices rose) have the same meaning."
"If we memorized only these two pairs, it would be impossible to infer that, in fact, con-sistent with our intuition, any of the following sets of phrases are also semantically equivalent: {stock market rose, stock market gained, stock prices rose, stock prices gained } and {stock market, stock prices } in the con-text of rose or gained; {market rose }, {market gained }, {prices rose } and {prices gained } in the context of stock; and so on."
"In this paper, we propose solutions for two problems: the problem of paraphrase representation and the problem of paraphrase induction."
"We propose a new, finite-state-based representation of paraphrases that enables one to encode compactly large numbers of paraphrases."
We also propose algorithms that automatically derive such repre-sentations from inputs that are now routinely released in conjunction with large scale machine translation evalu-ations[REF_CITE]: multiple English translations of many foreign language texts.
"For instance, when given as input the 11 semantically equivalent English transla-tions in Figure 1, our algorithm automatically induces the FSA in Figure 2, which represents compactly 49 distinct renderings of the same semantic meaning."
"Our FSAs capture both lexical paraphrases, such as {fighting, bat-tle}, {died, were killed} and structural paraphrases such as {last week’s fighting, the battle of last week}."
The contexts in which these are correct paraphrases are also conveniently captured in the representation.
"In previous work,[REF_CITE]used word lattices for language generation, but their method involved hand-crafted rules."
"For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious prob- lem."
"In contrast, we want to ensure the correctness of all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic."
"For example, when given as input the same sentences in Figure 1, one instantiation of the MSA algorithm produces the FSA in Figure 3, which contains many “bad” paths such as the battle of last week’s fight-ing took at least 12 people lost their people died in the fighting last week’s fighting (See Section 4.2.2 for a more quantitative analysis.)."
"It’s still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering[REF_CITE]."
But we chose to approach this problem from another direction.
"As a result, we propose a new syntax-based algorithm to produce FSAs."
"In this paper, we first introduce the multiple transla-tion corpus that we use in our experiments (see Section 2)."
We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3).
An important part of the paper is dedicated to evaluating the quality of the finite-state rep-resentations that we derive (see Section 4).
"Since our rep-resentations encode thousands and sometimes millions of equivalent verbalizations of the same meaning, we use both manual and automatic evaluation techniques."
Some of the automatic evaluations we perform are novel as well.
"The data we use in this work is the LDC-available Multiple-Translation Chinese (MTC) Corpus [Footnote_1] developed for machine translation evaluation, which contains 105 news stories (993 sentences) from three sources of jour-nalistic Mandarin Chinese text."
1 Linguistic Data Consortium (LDC) Catalog[REF_CITE]ISBN 1-58563-217-1.
These stories were inde-pendently translated[REF_CITE]translation agen-cies.
"Each sentence group, which consists of 11 semanti-cally equivalent translations, is a rich source for learning lexical and structural paraphrases."
"In our experiments, we use 899 of the sentence groups — the sentence groups with sentences longer than 45 words were dropped."
"Our syntax-based alignment algorithm, whose pseu-docode is shown in Figure 4, works in three steps."
"In the first step (lines 1-5 in Figure 4), we parse every sentence in a sentence group and merge all resulting parse trees into a parse forest."
"In the second step (line 6), we extract an FSA from the parse forest and then we compact it fur-ther using a limited form of bottom-up alignment, which we call squeezing (line 7)."
"In what follows, we describe each step in turn."
"Given a sentence group, we pass each of the 11 sentences to Charniak’s (2000) parser to get 11 parse trees."
The first step in the algorithm is to merge these parse trees into one parse-forest-like struc-ture using a top-down process.
"Let’s consider a simple case in which the parse for-est contains one single tree, Tree 1 in Figure 5, and we are adding Tree 2 to it."
"Since the two trees correspond to sentences that have the same meaning and since both trees expand an S node into an NP and a V P, it is rea-sonable to assume that NP 1 is a paraphrase of NP 2 and V P 1 is a paraphrase of V P 2 ."
"We merge NP 1 with NP 2 and V P 1 with V P 2 and continue the merging process on each of the subtrees recursively, until we either reach the leaves of the trees or the two nodes that we examine are expanded using different syntactic rules."
"When we apply this process to the trees in Figure 5, the NP nodes are merged all the way down to the leaves, and we get “12” as a paraphrase of “twelve” and “people” as a paraphrase of “persons”; in contrast, the two V Ps are expanded in different ways, so no merging is done beyond this level, and we are left with the information that “were killed” is a paraphrase of “died”."
We repeat this top-down merging procedure with each of the 11 parse trees in a sentence group.
"So far, only constituents with same syntactic type are treated as para-phrases."
"However, later we shall see that we can match word spans whose syntactic types differ."
The matching process described above appears quite strict – the expansions must match exactly for two nodes to be merged.
But consider the fol-lowing parse trees: 1.(S (NP 1 people)(V P 1 were killed in this battle)) 2.(S (NP 2 this battle)(V P 2 killed people))
"If we applied the algorithm described above, we would mistakenly align NP 1 with NP 2 and V P 1 with V P 2 — the algorithm described so far makes no use of lexical information."
"To prevent such erroneous alignments, we also imple-ment a simple keyword checking procedure."
"We note that since the word “battle” appears in both V P 1 and NP 2 , this can serve as an evidence against the merging of (NP 1 , NP 2 ) and (V P 1 , V P 2 )."
A similar argument can be constructed for the word “people”.
"So in this exam-ple we actually have double evidence against merging; in general, one such clue suffices to stop the merging."
Our keyword checking procedure acts as a filter.
A list of keywords is maintained for each node in a syntactic tree.
"This list contains all the nouns, verbs, and adjectives that are spanned by a syntactic node."
"Before merging two nodes, we check to see whether the keyword lists asso-ciated with them share words with other nodes."
"That is, supposed we just merged nodes A and B, and they are ex-panded with the same syntactic rule into A 1 A 2 ...A n and B 1 B 2 ...B n respectively; before we merge each A i with B i , we check for each B i if its keyword list shares com-mon words with any A j (j 6= i)."
"If they do not, we con-tinue the top-down merging process; otherwise we stop."
"In our current implementation, a pair of synonyms can not stop an otherwise legitimate merging, but it’s possi-ble to extend our keyword checking process with the help of lexical resources such as WordNet in future work."
Mapping Parse Forests into Finite State Automata.
The process of mapping Parse Forests into Finite State Automata is simple.
We simply traverse the parse forest top-down and create alternative paths for every merged node.
"For example, the parse forest in Figure 5 is mapped into the FSA shown at the bottom of the same figure."
"In the FSA, there is a word associated with each edge."
Dif-ferent paths between any two nodes are assumed to be paraphrases of each other.
Each path that starts from the BEGIN node and ends at the END node corresponds to either an original input sentence or a paraphrase sen-tence.
"Since we adopted a very strict matching criterion in top-down merging, a small difference in the syntactic structure of two trees prevents some legitimate mergings from taking place."
This behavior is also exacer-bated by errors in syntactic parsing.
"Hence, for instance, three edges labeled detroit at the leftmost of the top FSA in Figure 6 were kept apart."
"To compensate for this ef-fect, our algorithm implements an additional step, which we call squeezing."
"If two different edges that go into (or out of) the same node in an FSA are labeled with the same word, the nodes on the other end of the edges are merged."
We apply this operation exhaustively over the FSAs pro-duced by the top-down merging procedure.
Figure 6 il-lustrates the effect of this operation: the FSA at the top of this figure is compressed into the more compact FSA shown at the bottom of it.
"Note that in addition to reduc-ing the redundant edges, this also gives us paraphrases not available in the FSA before squeezing (e.g. {reduced to rubble, blasted to ground})."
"Therefore, the squeezing operation, which implements a limited form of lexically driven alignment similar to that exploited by MSA algo-rithms, leads to FSAs that have a larger number of paths and paraphrases."
The evaluation for our finite state representations and al-gorithm requires careful examination.
"Obviously, what counts as a good result largely depends on the applica-tion one has in mind."
"If we are extracting paraphrases for question-reformulation, it doesn’t really matter if we out-put a few syntactically incorrect paraphrases, as long as we produce a large number of semantically correct ones."
"If we want to use the FSA for MT evaluation (for exam-ple, comparing a sentence to be evaluated with the pos-sible paths in FSA), we would want all paths to be rela-tively good (which we will focus on in this paper), while in some other applications, we may only care about the quality of the best path (not addressed in this paper)."
"Sec-tion 4.1 concentrates on evaluating the paraphrase pairs that can be extracted from the FSAs built by our system, while Section 4.2 is dedicated to evaluating the FSAs di-rectly."
"By construction, different paths between any two nodes in the FSA representations that we derive are para-phrases (in the context in which the nodes occur)."
"To evaluate our algorithm, we extract paraphrases from our FSAs and ask human judges to evaluate their correctness."
We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a co-training-based paraphrase extraction algorithm[REF_CITE].
"To the best of our knowledge, this is the most relevant work to compare against since it aims at extracting paraphrase pairs from parallel corpus."
"Un-like our syntax-based algorithm which treats a sentence as a tree structure and uses this hierarchical structural in-formation to guide the merging process, their algorithm treats a sentence as a sequence of phrases with surround-ing contexts (no hierarchical structure involved) and co-trains classifiers to detect paraphrases and contexts for paraphrases."
It would be interesting to compare the re-sults from two algorithms so different from each other.
"For the purpose of this experiment, we randomly se-lected 300 paraphrase pairs (S syn ) from the FSAs pro-duced by our system."
"Since the co-training-based al-gorithm[REF_CITE]takes paral-lel corpus as input, we created out of the[REF_CITE]× 993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into 112 equivalent trans-lation pairs.)."
"Regina Barzilay kindly provided us the list of paraphrases extracted by their algorithm from this par-allel corpus, from which we randomly selected another set of 300 paraphrases (S cotr )."
"Each judge was asked to assess the correctness of 150 para-phrase pairs (75 pairs from each system) based on the context, i.e., the sentence group, from which the para-phrase pair was extracted."
"Judges were given three choices: “Correct”, for perfect paraphrases, “Partially correct”, for paraphrases in which there is only a par-tial overlap between the meaning of two paraphrases (e.g. while {saving set, aid package} is a correct paraphrase pair in the given context, {set, aide package} is consid-ered partially correct), and “Incorrect”."
The results of the evaluation are presented in Table 1.
"Although the four evaluators were judging four differ-ent sets, each clearly rated a higher percentage of the out-puts produced by the syntax-based alignment algorithm as “Correct”."
We should note that there are parameters specific to the co-training algorithm that we did not tune to work for this particular corpus.
"In addition, the co-training algorithm recovered more paraphrase pairs: the syntax-based algorithm extracted 8666 pairs in total with 1051 of them extracted at least twice (i.e. more or less reliable), while the numbers for the co-training algorithm is 2934 out of a total of 16993 pairs."
This means we are not comparing the accuracy on the same recall level.
"Aside from evaluating the correctness of the para-phrases, we are also interested in the degree of overlap between the paraphrase pairs discovered by the two algo-rithms so different from each other."
"We find that out of the 1051 paraphrase pairs that were extracted from more than one sentence group by the syntax-based algorithm, 62.3% were also extracted by the co-training algorithm; and out of the 2934 paraphrase pairs from the results of co-training algorithm, 33.4% were also extracted by the syntax-based algorithm."
"This shows that in spite of the very different cues the two different algorithms rely on, they do discover a lot of common pairs."
"In order to (roughly) estimate the recall (of lexical syn-onyms) of our algorithm, we use the synonymy relation in WordNet to extract all the synonym pairs present in our corpus."
"This extraction process yields the list of all WordNet-consistent synonym pairs that are present in our data. (Note that some of the pairs identified as synonyms by WordNet, like “follow/be”, are not really synonyms in the contexts defined in our data set, which may lead to artificial deflation of our recall estimate.)"
"Once we have the list of WordNet-consistent paraphrases, we can check how many of them are recovered by our method."
Table 2 gives the percentage of pairs recovered for each range of average sentence length (ASL) in the group.
"Not surprisingly, we get higher recall with shorter sen-tences, since long sentences tend to differ in their syn-tactic structures fairly high up in the parse trees, which leads to fewer mergings at the lexical level."
"The recall on the task of extracting lexical synonyms, as defined by WordNet, is not high."
"But after all, this is not what our algorithm has been designed for."
It’s worth notic-ing that the syntax-based algorithm also picks up many paraphrases that are not identified as synonyms in Word-Net.
"However, the WordNet-based recall figures suggest that WordNet can be used as an additional source of information to be exploited by our algorithm."
"We noted before that apart from being a natural represen-tation of paraphrases, the FSAs that we build have their own merit and deserve to be evaluated directly."
"Since our FSAs contain large numbers of paths, we design auto-matic evaluation metrics to assess their qualities."
"If we take our claims seriously, each path in our FSAs that connects the start and end nodes should correspond to a well-formed sentence."
We are interested in both quan-tity (how many sentences our automata are able to pro-duce) and quality (how good these sentences are).
"To an-swer the first question, we simply count the number of paths produced by our FSAs."
"Table [Footnote_3] gives the statistics on the number of paths pro-duced by our FSAs, reported by the average length of sentences in the input sentence groups."
3 We thank Regina Barzilay for providing us this set of re-sults
"For example, the sentence groups that have between 10 and 20 words pro-duce, on average, automata that can yield 4468 alterna-tive, semantically equivalent formulations."
"Note that if we always get the same degree of merging per word across all sentence groups, the number of paths would tend to increase with the sentence length."
This is not the case here.
Apparently we are getting less merg-ing with longer sentences.
"But still, given 11 sentences, we are capable of generating hundreds, thousands, and in some cases even millions of sentences."
"Obviously, we should not get too happy with our abil-ity to boost the number of equivalent meanings if they are incorrect."
"To assess the quality of the FSAs generated by our algorithm, we use a language model-based metric."
We train a [Footnote_4]-gram model over one year of the Wall Street Journal using the CMU-Cambridge Statistical Lan-guage Modeling toolkit (v2).
"4 Note that FSAs produced right after keyword checking will not yield any non-zero repetition ratio. However, if there are mis-alignment not prevented by keyword checking in an FSA, it may contain paths with erroneous repetition of words after squeezing."
"For each sentence group SG, we use this language model to estimate the aver-age entropy of the 11 original sentences in that group (ent(SG))."
We also compute the average entropy of all the sentences in the corresponding FSA built by our syntax-based algorithm (ent(FSA)).
"As the statistics in Table 4 show, there is little difference between the av-erage entropy of the original sentences and the average entropy of the paraphrase sentences we produce."
"To bet-ter calibrate this result, we compare it with the average entropy of 6 corresponding machine translation outputs (ent(MTS)), which were also made available by LDC in conjunction with the same corpus."
"As one can see, the difference between the average entropy of the machine produced output and the average entropy of the origi-nal 11 sentences is much higher than the difference be-tween the average entropy of the FSA-produced outputs and the average entropy of the original 11 sentences."
"Ob-viously, this does not mean that our FSAs only produce well-formed sentences."
But it does mean that our FSAs produce sentences that look more like human produced sentences than machine produced ones according to a lan-guage model.
"Not surprisingly, the language model we used in Sec-tion 4.2.1 is far from being a perfect judge of sentence quality."
Recall the example of “bad” path we gave in Sec-tion 1: the battle of last week’s fighting took at least 12 people lost their people died in the fighting last week’s fighting.
Our [Footnote_4]-gram based language model will not find any fault with this sentence.
"4 Note that FSAs produced right after keyword checking will not yield any non-zero repetition ratio. However, if there are mis-alignment not prevented by keyword checking in an FSA, it may contain paths with erroneous repetition of words after squeezing."
"Notice, however, that some words (such as “fighting” and “people”) appear at least twice in this path, although they are not repeated in any of the source sentences."
These erroneous repetitions in-dicate mis-alignment.
"By measuring the frequency of words that are mistakenly repeated, we can now examine quantitatively whether a direct application of the MSA algorithm suffers from different constituent orderings as we expected."
"For each sentence group, we get a list of words that never appear more than once in any sentence in this group."
"Given a word from this list and the FSA built from this group, we count the total number of paths that contain this word (C) and the number of paths in which this word appears at least twice (C r , i.e. number of er-roneous repetitions)."
"We define the repetition ratio to be C r /C, which is the proportion of “bad” paths in this FSA according to this word."
"If we compute this ra-tio for all the words in the lists of the first 499 groups [Footnote_2] and the corresponding FSAs produced by an instantia-tion of the MSA algorithm 3 , the average repetition ra-tio is 0.0304992 (14.76% of the words have a non-zero repetition ratio, and the average ratio for these words is 0.206671)."
"2 MSA runs very slow for longer sentences, and we believe using the first 499 groups should be enough to make our point."
"In comparison, the average repetition ratio for our algorithm is 0.0035074 (2.16% of the words have a non-zero repetition ratio 4 , and the average ratio for these words is 0.162309)."
The presence of different constituent orderings does pose a more serious problem to the MSA algorithm.
"Recently,[REF_CITE]have proposed an au-tomatic MT system evaluation technique (the BLEU score)."
"Given an MT system output and a set of refer- ence translations, one can estimate the “goodness” of the MT output by measuring the n-gram overlap between the output and the reference set."
"The higher the overlap, i.e., the closer an output string is to a set of reference transla-tions, the better a translation it is."
We hypothesize that our FSAs provide a better repre-sentation against which the outputs of MT systems can be evaluated because they encode not just a few but thou-sands of equivalent semantic formulations of the desired meaning.
"Ideally, if the FSAs we build accept all and only the correct renderings of a given meaning, we can just give a test sentence to the reference FSA and see if it is accepted by it."
"Since this is not a realistic expecta-tion, we measure the edit distance between a string and an FSA instead: the smaller this distance is, the closer it is to the meaning represented by the FSA."
"To assess whether our FSAs are more appropriate rep-resentations for evaluating the output of MT systems, we perform the following experiment."
"For each sentence group, we hold out one sentence as test sentence, and try to evaluate how much of it can be predicted from the other 10 sentences."
We compare two different ways of estimat-ing the predictive power. (a) we compute the edit distance between the test sentence and the other 10 sentences in the set.
The minimum of this distance is ed(input). (b) we use dynamic programming to efficiently compute the minimum distance (ed(FSA)) between the test sentence and all the paths in the FSA built from the other 10 sen-tences.
"The smaller the edit distance is, the better we are predicting a test sentence."
"Mathematically, the differ-ence between these two measures ed(input) − ed(FSA) characterizes how much is gained in predictive power by building the FSA."
We carry out the experiment described above in a “leave-one-out” fashion (i.e. each sentence serves as a test sentence once).
Now let ed gain be the average of ed(input) − ed(FSA) over the 11 runs for a given group.
We compute this for all 899 groups and find the mean for ed gain to be 0.91 (std. dev = 0.78).
Table 5 gives the count for groups whose ed gain falls into the specified range.
We can see that the majority of ed gain falls under 2.
We are also interested in the relation between the pre-dictive power of the FSAs and the number of reference translations they are derived from.
"For a given group, we randomly order the sentences in it, set the last one as the test sentence, and try to predict it with the first 1, 2, 3, ... 10 sentences."
We investigate whether more sentences yield an increase in the predictive power.
"Let ed(FSA n ) be the edit distance from the test sen-tence to the FSA built on the first n sentences; similarly, let ed(input n ) be the minimum edit distance from the test sentence to an input set that consists of only the first n sentences."
Table 6 reports the effect of using differ-ent number of reference translations.
The first column shows that each translation is contributing to the predic-tive power of our FSA.
"Even when we add the tenth trans-lation to our FSA, we still improve its predictive power."
The second column shows that the more sentences we add to the FSA the larger the difference between its predic-tive power and that of a simple set.
The results in Table 6 suggest that our FSA may be used in order to refine the BLEU metric[REF_CITE].
"In this paper, we presented a new syntax-based algorithm that learns paraphrases from a newly available dataset."
The multiple translation corpus that we use in this paper is the first instance in a series of similar corpora that are built and made publicly available by LDC in the context of a series of DARPA-sponsored MT evaluations.
The algorithm we proposed constructs finite state represen-tations of paraphrases that are useful in many contexts: to induce large lists of lexical and structural paraphrases; to generate semantically equivalent renderings of a given meaning; and to estimate the quality of machine transla-tion systems.
More experiments need to be carried out in order to assess extrinsically whether the FSAs we pro-duce can be used to yield higher agreement scores be-tween human and automatic assessments of translation quality.
"In our future work, we wish to experiment with more flexible merging algorithms and to integrate better the top-down and bottom-up processes that are used to in- duce FSAs."
We also wish to extract more abstract para-phrase patterns from the current representation.
"Such pat-terns are more likely to get reused – which would help us get reliable statistics for them in the extraction phase, and also have a better chance of being applicable to unseen data."
"We present a simple method for language inde-pendent and task independent text categoriza-tion learning, based on character-level n-gram language models."
Our approach uses simple information theoretic principles and achieves effective performance across a variety of lan-guages and tasks without requiring feature se-lection or extensive pre-processing.
"To demon-strate the language and task independence of the proposed technique, we present experimen-tal results on several languages—Greek, En-glish, Chinese and Japanese—in several text categorization problems—language identifica-tion, authorship attribution, text genre classifi-cation, and topic detection."
Our experimental results show that the simple approach achieves state of the art performance in each case.
Text categorization concerns the problem of automati-cally assigning given text passages (paragraphs or doc-uments) into predefined categories.
"Due to the rapid ex-plosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways."
"Such techniques are currently being ap-plied in many areas, including language identification, authorship attributi[REF_CITE], text genre classificati[REF_CITE], topic identificati[REF_CITE], and subjective sen-timent classificati[REF_CITE]."
"Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, lin-ear least squares models, neural networks, and K-nearest neighbor classifiers[REF_CITE]."
"A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning over the feature space."
"Of these two steps, feature engineering is critical to achieving good performance in text categoriza-tion problems."
"Once good features are identified, almost any reasonable technique for learning a classifier seems to perform well[REF_CITE]."
"Unfortunately, the standard classification learning methodology has several drawbacks for text categoriza-tion."
"First, feature construction is usually language de-pendent."
Various techniques such as stop-word removal or stemming require language specific knowledge to de-sign adequately.
"Moreover, whether one can use a purely word-level approach is itself a language dependent issue."
"In many Asian languages such as Chinese or Japanese, identifying words from character sequences is hard, and any word-based approach must suffer added complexity in coping with segmentation errors."
"Second, feature se-lection is task dependent."
"For example, tasks like au-thorship attribution or genre classification require atten-tion to linguistic style markers[REF_CITE], whereas topic detection systems rely more heavily on bag of words features."
"Third, there are an enormous num-ber of possible features to consider in text categorization problems, and standard feature selection approaches do not always cope well in such circumstances."
"For exam-ple, given an enormous number of features, the cumu-lative effect of uncommon features can still have an im-portant effect on classification accuracy, even though in-frequent features contribute less information than com-mon features individually."
"Consequently, throwing away uncommon features is usually not an appropriate strat-egy in this doma[REF_CITE]."
"Another problem is that feature selection normally uses indirect tests, such as χ 2 or mutual information, which involve setting arbi- trary thresholds and conducting a heuristic greedy search to find good feature sets."
"Finally, by treating text cate-gorization as a classical classification problem, standard approaches can ignore the fact that texts are written in natural language, meaning that they have many implicit regularities that can be well modeled with specific tools from natural language processing."
"In this paper, we propose a straightforward text cate-gorization learning method based on learning category-specific, character-level, n-gram language models."
"Al-though this is a very simple approach, it has not yet been systematically investigated in the literature."
"We find that, surprisingly, we obtain competitive (and often superior) results to more sophisticated learning and feature con-struction techniques, while requiring almost no feature engineering or pre-processing."
"In fact, the overall ap-proach requires almost no language specific or task spe-cific pre-processing to achieve effective performance."
"The success of this simple method, we think, is due to the effectiveness of well known statistical language mod-eling techniques, which surprisingly have had little sig-nificant impact on the learning algorithms normally ap-plied to text categorization."
"Nevertheless, statistical lan-guage modeling is also concerned with modeling the se-mantic, syntactic, lexicographical and phonological regu-larities of natural language—and would seem to provide a natural foundation for text categorization problems."
"One interesting difference, however, is that instead of explic-itly pre-computing features and selecting a subset based on arbitrary decisions, the language modeling approach simply considers all character (or word) subsequences occurring in the text as candidate features, and implic-itly considers the contribution of every feature in the fi-nal model."
"Thus, the language modeling approach com-pletely avoids a potentially error-prone feature selection process."
"Also, by applying character-level language mod-els, one also avoids the word segmentation problems that arise in many Asian languages, and thereby achieves a language independent method for constructing accurate text categorizers."
"The dominant motivation for language modeling has tra-ditionally come from speech recognition, but language models have recently become widely used in many other application areas."
"The goal of language modeling is to predict the prob-ability of naturally occurring word sequences, s = w 1 w 2 ...w N ; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur)."
"Given a word se-quence w 1 w 2 ...w N to be used as a test corpus, the quality of a language model can be measured by the empirical perplexity and entropy scores on this corpus v u uY N 1"
Perplexity = N t (1) i=1
Pr(w i |w 1 ...w i−1 )
Entropy = log 2 Perplexity (2) where the goal is to minimize these measures.
The simplest and most successful approach to lan-guage modeling is still based on the n-gram model.
By the chain rule of probability one can write the probability of any word sequence as
Y N Pr(w 1 w 2 ...w N ) =
Pr(w i |w 1 ...w i−1 ) (3) i=1
An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(w i |w 1 ...w i−1 ) are the previous n − 1 words; i.e.
A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed frequency of each of the patterns #(w i−n+1 ...w i )
Pr(w i |w i−n+1 ... w i−1 ) = #(w i−n+1 ...w i−1 ) (4) where #(.) denotes the number of occurrences of a spec-ified gram in the training corpus.
"Although one could at-tempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of W n events, where W is the size of the word vocabulary."
This quickly overwhelms modern computational and data re-sources for even modest choices of n (beyond 3 to 6).
"Also, because of the heavy tailed nature of language (i.e. Zipf’s law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling."
One standard ap-proach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially miss-ing n-grams) is to use some sort of back-off estimator.
"Pr(w i |w i−n+1 ... w i−1 )   P̂r(w i |w i−n+1 ... w i−1 ), if #(w i−n+1 ...w i ) &gt; 0 =  β(w i−n+1 ...w i−1 ) × Pr(w i |w i−n+2 ... w i−1 ), otherwise (5) where"
P̂r(w i |w i−n+1 ... w i−1 ) = discount #(w i−n+1 ...w i ) #(w i−n+1 ...w i−1 ) (6) is the discounted probability and β(w i−n+1 ... w i−1 ) is a normalization constant β(w i−n+1 ... w i−1 ) = X 1 − P̂r(x|w i−n+1 ... w i−1 ) x∈(w i−n X +1 ... w i−1 x) (7) 1 − P̂r(x|w i−n+2 ... w i−1 ) x∈(w i−n+1 ... w i−1 x)
"The discounted probability (6) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing[REF_CITE]."
The details of the smoothing techniques are omitted here for simplicity.
"The language models described above use individual words as the basic unit, although one could instead con-sider models that use individual characters as the ba-sic unit."
The remaining details remain the same in this case.
"The only difference is that the character vocabu-lary is always much smaller than the word vocabulary, which means that one can normally use a much higher order, n, in a character-level n-gram model (although the text spanned by a character model is still usually less than that spanned by a word model)."
"The benefits of the character-level model in the context of text classification are several-fold: it avoids the need for explicit word seg-mentation in the case of Asian languages, it captures im-portant morphological properties of an author’s writing, it models the typos and misspellings that are common in informal texts, it can still discover useful inter-word and inter-phrase features, and it greatly reduces the sparse data problems associated with large vocabulary models."
"In this paper, we experiment with character-level models to achieve flexibility and language independence."
Our approach to applying language models to text cat-egorization is to use Bayesian decision theory.
"Assume we wish to classify a text D into a category c ∈ C = {c 1 , ..., c |C| }."
A natural choice is to pick the category c that has the largest posterior probability given the text.
"That is, c ∗ = arg max{Pr(c|D)} (8) c∈C"
"Using Bayes rule, this can be rewritten as c ∗ = arg max{Pr(D|c) Pr(c)} (9) c∈C = arg max{Pr(D|c)} (10) c∈C n Y N o = arg max"
Pr c (w i |w i−n+1 ... w i−1 ) (11) c∈C i=1 where deducing Eq. (10) from Eq. (9) assumes uniformly weighted categories (since we have no other prior knowl-edge).
"Here, Pr(D|c) is the likelihood of D under cate-gory c, which can be computed by Eq. (11)."
Likelihood is related to perplexity and entropy by Eq. (1) and Eq. (2).
"Therefore, our approach is to learn a separate language model for each category, by training on a data set from that category."
"Then, to categorize a new text D, we sup-ply D to each language model, evaluate the likelihood (or entropy) of D under the model, and pick the winning category according to Eq. (10)."
The inference of an n-gram based text classifier is very similar to a naive-Bayes classifier.
"In fact, n-gram classifiers are a straightforward generalization of naive- Bayes: A uni-gram classifier with Laplace smoothing corresponds exactly to the traditional naive-Bayes clas-sifier."
"However, n-gram language models, for larger n, possess many advantages over naive-Bayes classifiers, in-cluding modeling longer context and applying superior smoothing techniques in the presence of sparse data."
We now proceed to present our results on several text categorization problems on different languages.
"Specif-ically, we consider language identification, Greek author-ship attribution, Greek genre classification, English topic detection, Chinese topic detection and Japanese topic de-tection."
"For the sake of consistency with previous re-search[REF_CITE], we measure categorization performance by the overall accuracy, which is the number of correctly iden-tified texts divided by the total number of texts consid-ered."
"We also measure the performance with Macro F-measure, which is the average of the F-measures across all categories."
F-measure is a combination of precision and recall[REF_CITE].
The first text categorization problem we examined was language identification—a useful pre-processing step in information retrieval.
"Language identification is proba-bly the easiest text classification problem because of the significant morphological differences between languages, even when they are based on the same character set. 1"
"In our experiments, we considered one chapter of Bible that had been translated into 6 different languages: English, French, German, Italian, Latin and Spanish."
"In each case, we reserved twenty sentences from each language for testing and used the remainder for training."
"For this task, with only bi-gram character-level models and any smoothing technique, we achieved 100% accuracy."
The second text categorization problem we examined was author attribution.
"A famous example is the case of the Federalist Papers, of which twelve instances are claimed to have been written both by Alexander Hamilton and James Madis[REF_CITE]."
Authorship attribution is more challenging than language identifica-tion because the difference among the authors is much more subtle than that among different languages.
We con-sidered a data set used[REF_CITE]con-sisting of 20 texts written by 10 different modern Greek authors (totaling 200 documents).
"In each case, 10 texts from each author were used for training and the remain-ing 10 for testing."
The results using different orders of n-gram models and different smoothing techniques are shown in Table [Footnote_1].
1 Language identification from speech is much harder.
"With 3-grams and absolute smoothing, we observe 90% accuracy."
This result compares favorably to the 72% accuracy reported[REF_CITE]which is based on linear least square fit (LLSF).
"The third problem we examined was text genre classifi-cation, which is an important application in information retrieval[REF_CITE]."
We con-sidered a Greek data set used[REF_CITE]consisting of 20 texts of 10 different styles extracted from various sources (200 documents total).
"For each style, we used 10 texts as training data and the remaining 10 as test-ing."
The results of learning an n-gram based text classifier are shown in Table 2.
"The fourth problem we examined was topic detection in text, which is a heavily researched text categorization problem[REF_CITE]."
"Here we demon-strate the language independence of the language mod-eling approach by considering experiments on English, Chinese and Japanese data sets."
"This collection consists of 19,974 non-empty documents distributed evenly across 20 newsgroups."
"We use the newsgroups to form our categories, and randomly select 80% of the documents to be used for training and set aside the remaining 20% for testing."
"In this case, as before, we merely considered text to be a sequence of characters, and learned character-level n-gram models."
The resulting classification accuracies are reported in in Table 3.
"With 3-gram (or higher order) models, we consistently obtain accurate performance, peaking at 89% accuracy in the case of 6-gram models with Witten-Bell smoothing. (We note that word-level models were able to achieve 88% accuracy in this case.)"
"These results compare favorably to the state of the art re-sult of 87.5% accuracy reported[REF_CITE], which was based on a combination of an SVM with error correct output coding (ECOC)."
"Chinese topic detection is often thought to be more challenging than English, because words are not white-space delimited in Chinese text."
This fact seems to require word segmentation to be performed as a pre-processing step before further classificati[REF_CITE].
"However, we avoid the need for explicit segmen-tation by simply using a character level n-gram classifier."
For Chinese topic detection we considered a data set investigated[REF_CITE].
The corpus in this case is a subset of the TREC-5 data set created for research on Chinese text retrieval.
"To make the data set suitable for text categorization, documents were first clustered into 101 groups that shared the same headline (as indicated by an SGML tag) and the six most frequent groups were selected to make a Chinese text categorization data set."
"In each group, 500 documents were randomly selected for training and 100 documents were reserved for testing."
"We observe over 80% accuracy for this task, using bi-gram (2 Chinese characters) or higher order models."
This is the same level of performance reported[REF_CITE]for an SVM approach using word segmentation and feature selection.
Japanese poses the same word segmentation issues as Chinese.
"Word segmentation is also thought to be neces-sary for Japanese text categorizati[REF_CITE], but we avoid the need again by considering character level language models."
We consider the Japanese topic detection data inves-tigated[REF_CITE].
This data set was con- verted from the NTCIR-J1 data set originally created for Japanese text retrieval research.
The data has 24 cate-gories.
"The testing set contains 10,000 documents dis-tributed unevenly between categories (with a minimum of 56 and maximum of 2696 documents per category)."
This imbalanced distribution causes some difficulty since we assumed a uniform prior over categories.
"Although this is easily remedied, we did not fix the problem here."
"Never-theless, we obtain experimental results in Table 5 that still show an 84% accuracy rate on this problem (for 6-gram or higher order models)."
"This is the same level of per-formance as that reported[REF_CITE], which uses an SVM approach with word segmentation, morphology analysis and feature selection."
The perplexity of a test document under a language model depends on several factors.
"The two most influential fac-tors are the order, n, of the n-gram model and the smooth-ing technique used."
"Different choices will result in differ-ent perplexities, which could influence the final decision in using Eq. (10)."
We now experimentally assess the in-fluence of each of these factors below.
The order n is a key factor in n-gram language models.
If n is too small then the model will not capture enough context.
"However, if n is too large then this will create severe sparse data problems."
Both extremes result in a larger perplexity than the optimal context length.
Figures 1 and 2 illustrate the influence of order n on classifica-tion performance and on language model quality in the previous five experiments (all using absolute smoothing).
Note that in this case the entropy (bits per character) is the average entropy across all testing documents.
"From the curves, one can see that as the order increases, classi-fication accuracy increases and testing entropy decreases, presumably because the longer context better captures the regularities of the text."
"However, at some point accu- racy begins to decrease and entropy begins to increase as the sparse data problems begin to set in."
"Interest-ingly, the effect is more pronounced in some experiments (Greek genre classification) but less so in other experi-ments (topic detection under any language)."
"The sensi-tivity in the Greek genre case could still be attributed to the sparse data problem (the over-fitting problem in genre classification could be more serious than the other prob-lems, as seen from the entropy curves)."
Another key factor affecting the performance of a lan-guage model is the smoothing technique used.
Figures 3 and 4 show the effects of smoothing techniques on clas-sification accuracy and testing entropy (Chinese topic de-tection and Japanese topic detection are not shown in the figure to save space).
"Here we find that, in most cases, the smoothing tech-nique does not have a significant effect on text catego-rization accuracy, because of the small vocabulary size of character level n-gram models."
"However, there are two exceptions—Greek authorship attribution and Greek text genre classification—where Good-Turing smoothing is not as effective as other techniques, even though it gives better test entropy than some others."
"Since our goal is to make a final decision based on the ranking of perplexi-ties, not just their absolute values, a superior smoothing method in the sense of perplexity reduction (i.e. from the perspective of classical language modeling) does not necessarily lead to a better decision from the perspec-tive of categorization accuracy."
"In fact, in all our exper-iments we have found that it is Witten-Bell smoothing, not Good-Turing smoothing, that gives the best results in terms of classification accuracy."
Our observation is con-sistent with previous research which reports that Witten-Bell smoothing achieves benchmark performance in char-acter level text compressi[REF_CITE].
"For the most part, however, one can use any standard smooth-ing technique in these problems and obtain comparable performance, since the rankings they produce are almost always the same."
"In principle, any language model can be used to perform text categorization based on Eq. (10)."
"However, n-gram models are extremely simple and have been found to be effective in many applications."
"For example, character level n-gram language models can be easily applied to any language, and even non-language sequences such as DNA and music."
"Character level n-gram models are widely used in text compression—e.g., the PPM model[REF_CITE]—and have recently been found to be effective in text classification problems as well[REF_CITE]."
The PPM model is a weighted lin-ear interpolation n-gram models and has been set as a benchmark in text compression for decades.
"Building an adaptive PPM model is expensive however[REF_CITE], and our back-off models are relatively much sim-pler."
"Using compression techniques for text categoriza-tion has also been investigated[REF_CITE], where the authors seek a model that yields the minimum compression rate increase when a new test document is introduced."
"However, this method is found not to be gen-erally effective nor efficient[REF_CITE]."
"In our ap-proach, we evaluate the perplexity (or entropy) directly on test documents, and find the outcome to be both effec-tive and efficient."
Many previous researchers have realized the impor-tance of n-gram models in designing language indepen-dent text categorization systems[REF_CITE].
"However, they have used n-grams as features for a traditional feature selection pro-cess, and then deployed classifiers based on calculating feature-vector similarities."
"Feature selection in such a classical approach is critical, and many required proce-dures, such as stop word removal, are actually language dependent."
"In our approach, all n-grams are considered as features and their importance is implicitly weighted by their contribution to perplexity."
Thus we avoid an error prone preliminary feature selection step.
We have presented an extremely simple approach for lan-guage and task independent text categorization based on character level n-gram language modeling.
The approach is evaluated on four different languages and four differ-ent text categorization problems.
"Surprisingly, we ob-serve state of the art or better performance in each case."
"We have also experimentally analyzed the influence of two factors that can affect the accuracy of this approach, and found that for the most part the results are robust to perturbations of the basic method."
"The wide appli-cability and simplicity of this approach makes it imme-diately applicable to any sequential data (such as natu-ral language, music, DNA) and yields effective baseline performance."
We are currently investigating more chal-lenging problems like multiple category classification us-ing the[REF_CITE]data set[REF_CITE]and subjec-tive sentiment classificati[REF_CITE].
"To us, these results suggest that basic statistical language modeling ideas might be more relevant to other areas of natural lan-guage processing than commonly perceived."
Research supported by Bell University Labs and MI-
We present an application of ambiguity pack-ing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
"Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduc-tion operating on packed parse forests, and a maximum-entropy model for stochastic output selection."
"Furthermore, we propose the use of standard parser evaluation methods for auto-matically evaluating the summarization qual-ity of sentence condensation systems."
An ex-perimental evaluation of summarization qual-ity shows a close correlation between the au-tomatic parse-based evaluation and a manual evaluation of generated strings.
"Overall sum-marization quality of the proposed system is state-of-the-art, with guaranteed grammatical-ity of the system output due to the use of a constraint-based parser/generator."
"Recent work in statistical text summarization has put for-ward systems that do not merely extract and concate-nate sentences, but learn how to generate new sentences from hSummary, Texti tuples."
"Depending on the cho-sen task, such systems either generate single-sentence “headlines” for multi-sentence text[REF_CITE], or they provide a sentence condensation module designed for combination with sentence extraction sys-tems[REF_CITE]."
"The chal-lenge for such systems is to guarantee the grammatical-ity and summarization quality of the system output, i.e. the generated sentences need to be syntactically well-formed and need to retain the most salient information of the original document."
For example a sentence extraction system might choose a sentence like:
"The UNIX operating system, with implementations from Apples to Crays, appears to have the advan-tage. from a document, which could be condensed as:"
UNIX appears to have the advantage.
"In the approach[REF_CITE], selec-tion and ordering of summary terms is based on bag-of-words models and n-grams."
"Such models may well produce summaries that are indicative of the original’s content; however, n-gram models seem to be insufficient to guarantee grammatical well-formedness of the system output."
"To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches[REF_CITE]and[REF_CITE]."
"In these approaches, decisions about which material to in-clude/delete in the sentence summaries do not rely on rel-ative frequency information on words, but rather on prob-ability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries."
A related area where linguistic parsing systems have been applied successfully is sentence simplifica-tion.
"However, these approaches do not employ statistical learning techniques to disambiguate simplifi-cation decisions, but iteratively apply symbolic reduction rules, producing a single output for each sentence."
The goal of our approach is to apply the fine-grained tools for stochastic Lexical-Functional Grammar (LFG) parsing to the task of sentence condensation.
"The system presented in this paper is conceptualized as a tool that can be used as a standalone system for sentence condensation or simplification, or in combination with sentence extrac-tion for text-summarization beyond the sentence-level."
"In our system, to produce a condensed version of a sen-tence, the sentence is first parsed using a broad-coverage LFG grammar for English."
The parser produces a set of functional (f)-structures for an ambiguous sentence in a packed format.
It presents these to the transfer compo-nent in a single packed data structure that represents in one place the substructures shared by several different in-terpretations.
The transfer component operates on these packed representations and modifies the parser output to produce reduced f-structures.
The reduced f-structures are then filtered by the generator to determine syntac-tic well-formedness.
A stochastic disambiguator using a maximum entropy model is trained on parsed and manu-ally disambiguated f-structures for pairs of sentences and their condensations.
"Using the disambiguator, the string generated from the most probable reduced f-structure produced by the transfer system is chosen."
"In contrast to the approaches mentioned above, our system guaran-tees the grammaticality of generated strings through the use of a constraint-based generator for LFG which uses a slightly tighter version of the grammar than is used by the parser."
"As shown in an experimental evaluation, sum-marization quality of our system is high, due to the com-bination of linguistically fine-grained analysis tools and expressive stochastic disambiguation models."
A second goal of our approach is to apply the standard evaluation methods for parsing to an automatic evaluation of summarization quality for sentence condensation sys-tems.
"Instead of deploying costly and non-reusable hu-man evaluation, or using automatic evaluation methods based on word error rate or n-gram match, summariza-tion quality can be evaluated directly and automatically by matching the reduced f-structures that were produced by the system against manually selected f-structures that were produced by parsing a set of manually created con-densations."
Such an evaluation only requires human labor for the construction and manual structural disambigua-tion of a reusable gold standard test set.
"Matching against the test set can be done automatically and rapidly, and is repeatable for development purposes and system com-parison."
"As shown in an experimental evaluation, a close correspondence can be established for rankings produced by the f-structure based automatic evaluation and a man-ual evaluation of generated strings."
"In this section, each of the system components will be described in more detail."
"In this project, a broad-coverage LFG gram-mar and parser for English was employed (see Riezleretal.(2002))."
"The parser produces a set of context-free constituent (c-)structures and associated functional (f-)structures for each input sentence, repre-sented in packed form (see[REF_CITE])."
For sentence condensation we are only interested in the predicate-argument structures encoded in f-structures.
"For example, Fig. 1 shows an f-structure manually selected out of the 40 f-structures for the sentence:"
"A prototype is ready for testing, and Leary hopes to set requirements for a full system by the end of the year."
The transfer component for the sentence condensation system is based on a component previously used in a ma-chine translation system (see[REF_CITE]).
It consists of an ordered set of rules that rewrite one f-structure into another.
"Structures are broken down into flat lists of facts, and rules may add, delete, or change individ-ual facts."
Rules may be optional or obligatory.
"In the case of optional rules, transfer of a single input structure may lead to multiple alternate output structures."
"The transfer component is designed to operate on packed input from the parser and can also produce packed representations of the condensation alternatives, using methods adapted from parse packing. 1"
"An example rule that (optionally) removes an adjunct is shown below: +adjunct(X,Y), in-set(Z,Y) ?=&gt; delete-node(Z,r1), rule-trace(r[Footnote_1],del(Z,X))."
1 The packing feature of the transfer component could not be employed in these experiments since the current interface to the generator and stochastic disambiguation component still requires unpacked representations.
"This rule eliminates an adjunct, Z, by deleting the fact that Z is contained within the set of adjuncts, Y, associated with the expression"
"X. The + before the adjunct(X,Y) fact marks this fact as one that needs to be present for the rule to be applied, but which is left unaltered by the rule application."
"The in-set(Z,Y) fact is deleted."
"Two new facts are added. delete-node(Z,r1) indicates that the structure rooted at node Z is to be deleted, and rule-trace(r[Footnote_1],del(Z,X)) adds a trace of this rule to an accumulating history of rule applications."
1 The packing feature of the transfer component could not be employed in these experiments since the current interface to the generator and stochastic disambiguation component still requires unpacked representations.
This history records the relation of transferred f-structures to the original f-structure and is available for stochastic dis-ambiguation.
"Rules used in the sentence condensation transfer sys-tem include the optional deletion of all intersective ad-juncts (e.g., He slept in the bed. can become He slept., but He did not sleep. cannot become He did sleep. or He slept.), the optional deletion of parts of coordinate struc-tures (e.g., They laughed and giggled. can become They giggled.), and certain simplifications (e.g. It is clear that the earth is round. can become The earth is round. but It seems that he is asleep. cannot become He is asleep.)."
"For example, one possible post-transfer output of the sen-tence in Fig. 1 is shown in Fig. 2."
The transfer rules are independent of the grammar and are not constrained to preserve the grammaticality or well-formedness of the reduced f-structures.
"Some of the re-duced structures therefore may not correspond to any En-glish sentence, and these are eliminated from future con-sideration by using the generator as a filter."
The filter-ing is done by running each transferred structure through the generator to see whether it produces an output string.
"If it does not, the structure is rejected."
"For example, for the f-structure in Fig. 1, the transfer system proposed 32 possible reductions."
"After filtering these structures by generation, 16 reduced f-structures comprising possible condensations of the input sentence survive."
A prototype is ready.
A prototype is ready for testing.
Leary hopes to set requirements for a full system.
A prototype is ready and Leary hopes to set require-ments for a full system.
A prototype is ready for testing and Leary hopes to set requirements for a full system.
Leary hopes to set requirements for a full system by the end of the year.
A prototype is ready and Leary hopes to set require-ments for a full system by the end of the year.
A prototype is ready for testing and Leary hopes to set requirements for a full system by the end of the year.
"In order to guarantee non-empty output for the over-all condensation system, the generation component has to be fault-tolerant in cases where the transfer system op-erates on a fragmentary parse, or produces non-valid f-structures from valid input f-structures."
Robustness tech-niques currently applied to the generator include insertion and deletion of features in order to match invalid transfer-output to the grammar rules and lexicon.
"Furthermore, repair mechanisms such as repairing subject-verb agree-ment from the subject’s number value are employed."
"As a last resort, a fall-back mechanism to the original un-condensed f-structure is used."
These techniques guaran-tee that a non-empty set of reduced f-structures yielding grammatical strings in generation is passed on to the next system component.
"In case of fragmentary input to the transfer component, grammaticaliy of the output is guar-anteed for the separate fragments."
"In other words, strings generated from a reduced fragmentary f-structure will be as grammatical as the string that was fed into the parsing component."
"After filtering by the generator, the remaining f-structures were weighted by the stochastic disambigua-tion component."
"Similar to stochastic disambiguation for constraint-based parsing[REF_CITE], an exponential (a.k.a. log-linear or maximum-entropy) probability model on transferred structures is es-timated from a set of training data."
The data for estima-tion consists of pairs of original sentences y and gold-standard summarized f-structures s which were manu-ally selected from the transfer output for each sentence.
"For training data {(s j , y j )} mj=1 and a set of possible sum-marized structures S(y) for each sentence y, the objective was to maximize a discriminative criterion, namely the conditional likelihood L(λ) of a summarized f-structure given the sentence."
Optimization of the function shown below was performed using a conjugate gradient opti- mization routine: m e λ·f(s j ) L(λ) = log Y .P j=1 s∈S(y j ) e λ·f(s)
At the core of the exponential probability model is a vec-tor of property-functions f to be weighted by parameters λ.
"For the application of sentence condensation, 13,000 property-functions of roughly three categories were used: • Property-functions indicating attributes, attribute-combinations, or attribute-value pairs for f-structure attributes (≈ 1,000 properties) • Property-functions indicating co-occurences of verb stems and subcategorization frames (≈ 12,000 prop-erties) • Property-functions indicating transfer rules used to arrive at the reduced f- structures (≈ 60 properties)."
"A trained probability model is applied to unseen data by selecting the most probable transferred f-structure, yielding the string generated from the selected struc-ture as the target condensation."
The transfered f-structure chosen for our current example is shown in Fig. 3.
"This structure was produced by the following set of transfer rules, where var refers to the indices in the rep-resentation of the f-structure: rtrace(r13,keep(var(98),of)), rtrace(r161,keep(system,var(85))), rtrace(r1,del(var(91),set,by)), rtrace(r1,del(var(53),be,for)), rtrace(r20,equal(var(1),and)), rtrace(r20,equal(var(2),and) ), rtrace(r2,del(var(1),hope,and)), rtrace(r22,delb(var(0),and))."
"These rules delete the adjunct of the first conjunct (for testing), the adjunct of the second conjunct (by the end of the year), the rest of the second conjunct (Leary hopes to set requirements for a full system), and the conjunction itself (and)."
"Evaluation of quality of sentence condensation systems, and of text summarization and simplification systems in general, has mostly been conducted as intrinsic evalua-tion by human experts."
"Recently, Papineni et al.’s (2001) proposal for an automatic evaluation of translation sys-tems by measuring n-gram matches of the system out-put against reference examples has become popular for evaluation of summarization systems."
"In addition, an au-tomatic evaluation method based on context-free deletion decisions has been proposed[REF_CITE]."
"However, for summarization systems that employ a linguistic parser as an integral system component, it is possible to employ the standard evaluation techniques for parsing directly to an evaluation of summarization quality."
A parsing-based evaluation allows us to measure the semantic as-pects of summarization quality in terms of grammatical-functional information provided by deep parsers.
"Further-more, human expertise was necessary only for the cre-ation of condensed versions of sentences, and for the manual disambiguation of parses assigned to those sen-tences."
"Given such a gold standard, summarization qual-ity of a system can be evaluated automatically and re-peatedly by matching the structures of the system out-put against the gold standard structures."
"The standard metrics of precision, recall, and F-score from statisti-cal parsing can be used as evaluation metrics for mea-suring matching quality: Precision measures the number of matching structural items in the parses of the sys-tem output and the gold standard, out of all structural items in the system output’s parse; recall measures the number of matches, out of all items in the gold stan-dard’s parse."
F-score balances precision and recall as (2 × precision × recall)/(precision + recall).
"For the sentence condensation system presented above, the structural items to be matched consist of rela-tion(predicate, argument) triples."
"For example, the gold-standard f-structure of Fig. 2 corresponds to 23 depen-dency relations, the first 14 of which are shared with the reduced f-structure chosen by the stochastic disambigua-tion system: tense(be:0, pres), mood(be:0, indicative), subj(be:0, prototype:2), xcomp(be:0, ready:1), stmt_type(be:0, declarative), vtype(be:0, copular), subj(ready:1, prototype:2), adegree(ready:1, positive), atype(ready:1, predicative), det(prototype:2, a:7), num(prototype:2, sg), pers(prototype:2, 3), det_form(a:7, a), det_type(a:7, indef), adjunct(be:0, for:12), obj(for:12, test:14), adv_type(for:12, vpadv), psem(for:12, unspecified), ptype(for:12, semantic), num(test:14, sg), pers(test:14, 3), pform(test:14, for), vtype(test:14, main)."
"Matching these f-structures against each other corre-sponds to a precision of 1, recall of .61, and F-score of .76."
The fact that our method does not rely on a compar-ison of the characteristics of surface strings is a clear advantage.
"Such comparisons are bad at handling exam-ples which are similar in meaning but differ in word or-der or vary structurally, such as in passivization or nom-inalization."
Our method handles such examples straight-forwardly.
Fig. 4 shows two serialization variants of the condensed sentence of Fig. 2.
The f-structures for these examples are similar to the f-structure assigned to the gold standard condensation shown in Fig. 2 (except for the relations ADJUNT-TYPE:parenthetical ver-sus ADV-TYPE:vpadv versus ADV-TYPE:sadv).
"An evaluation of summarization quality that is based on matching f-structures will treat these examples equally, whereas an evaluation based on string matching will yield different quality scores for different serializations."
"In the next section, we present experimental results of an automatic evaluation of the sentence condensation system described above."
These results show a close cor-respondence between automatically produced evaluation results and human judgments on the quality of generated condensed strings.
"The sentences and condensations we used are taken from data for the experiments[REF_CITE], which were provided to us by Daniel Marcu."
These data consist of pairs of sentences and their condensed versions that have been extracted from computer-news articles and abstracts of the Ziff-Davis corpus.
"Out of these data, we parsed and manually disambiguated 500 sentence pairs."
These included a set of 32 sentence pairs that were used for testing purposes[REF_CITE].
"In or-der to control for the small corpus size of this test set, we randomly extracted an additional 32 sentence pairs from the 500 parsed and disambiguated examples as a second test set."
The rest of the 436 randomly selected sentence pairs were used to create training data.
"For the purpose of discriminative training, a gold-standard of transferred f-structures was created from the transfer output and the manually selected f-structures for the condensed strings."
This was done automatically by selecting for each exam-ple the transferred f-structure that best matched the f-structure annotated for the condensed string.
"In the automatic evaluation of f-structure match, three different system variants were compared."
"Firstly, ran-domly chosen transferred f-structures were matched against the manually selected f-structures for the man-ually created condensations."
This evaluation constitutes a lower bound on the F-score against the given gold standard.
"Secondly, matching results for transferred f-structures yielding the maximal F-score against the gold standard were recorded, giving an upper bound for the system."
"Thirdly, the performance of the stochastic model within the range of the lower bound and upper bound was measured by recording the F-score for the f-structure that received highest probability according to the learned dis-tribution on transferred structures."
"In order to make our results comparable to the re-sults[REF_CITE]and also to investigate the correspondence between the automatic evaluation and human judgments, a manual evaluation of the strings gen-erated by these system variants was conducted."
Two hu-man judges were presented with the uncondensed sur-face string and five condensed strings that were displayed in random order for each test example.
"The five con-densed strings presented to the human judges contained (1) strings generated from three randomly selected f-structures, (2) the strings generated from the f-structures which were selected by the stochastic model, and (3) the manually created gold-standard condensations extracted from the Ziff-Davis abstracts."
The judges were asked to judge summarization quality on a scale of increasing quality from 1 to 5 by assessing how well the generated strings retained the most salient information of the orig-inal uncondensed sentences.
Grammaticality of the sys-tem output is optimal and not reported separately.
Results for both evaluations are reported for two test corpora of 32 examples each.
Testset I contains the sentences and condensations used to evaluate the system described[REF_CITE].
"Testset II consists of another randomly extracted 32 sentence pairs from the same do-main, prepared in the same way."
Fig. 5 shows evaluation results for a sentence conden-sation run that uses manually selected f-structures for the original sentences as input to the transfer component.
These results demonstrate how the condenstation system performs under the optimal circumstances when the parse chosen as input is the best available.
"Fig. 6 applies the same evaluation data and metrics to a sentence conden-sation experiment that performs transfer from packed f-structures, i.e. transfer is performed on all parses for an ambiguous sentence instead of on a single manually se-lected parse."
"Alternatively, a single input parse could be selected by stochastic models such as the one described[REF_CITE]."
"A separate phase of parse disam-biguation, and perhaps the effects of any errors that this might introduce, can be avoided by transferring from all parses for an ambiguous sentence."
"This approach is com-putationally feasible, however, only if condensation can be carried all the way through without unpacking."
"Our technology is not yet able to do this (in particular, as men-tioned earlier, we have not yet implemented a method for stochastic disambiguation on packed f-structures)."
"How-ever, we conducted a preliminary assessment of this pos-sibility by unpacking and enumerating the transferred f-structures."
"For many sentences this resulted in more can-didates than we could operate on in the available time and space, and in those cases we arbitrarily set a cut-off on the number of transferred f-structures we considered."
"Since transferred f-structures are produced according to the number of rules applied to transfer them, in this setup the transfer system produces smaller f-structures first, and cuts off less condensed output."
"The result of this ex-periment, shown in Fig. 6, thus provides a conservative estimate on the quality of the condensations we might achieve with a full-packing implementation."
"In Figs. 5 and 6, the first row shows F-scores for a random selection, the system selection, and the best pos-sible selection from the transfer output against the gold standard."
"The second rows show summarization quality scores for generations from a random selection and the system selection, and for the human-written condensa-tion."
The third rows report compression ratios.
"As can be seen from these tables, the ranking of system variants produced by the automatic and manual evaluation con-firm a close correlation between the automatic evaluation and human judgments."
"A comparison of evaluation re-sults across colums, i.e. across selection variants, shows that a stochastic selection of transferred f-structures is indeed important."
"Even if all f-structures are transferred from the same linguistically rich source, and all gener-ated strings are grammatical, a reduction in error rate of around 50% relative to the upper bound can be achieved by stochastic selection."
"In contrast, a comparison be-tween transfer runs with and without perfect disambigua-tion of the original string shows a decrease of about 5% in F-score, and of only .1 points for summarization quality when transferring from packed parses instead of from the manually selected parse."
This shows that it is more im-portant to learn what a good transferred f-structure looks like than to have a perfect f-structure to transfer from.
"The compression rates associated with the systems that used stochastic selection is around 60%, which is accept-able, but not as aggressive as human-written condensa-tions."
"Note that in our current implementation, in some cases the transfer component was unable to operate on the packed representation."
In those cases a parse was cho-sen at random as a conservative estimate of transfer from all parses.
This fall-back mechanism explains the drop in F-score for the upper bound in comparing Figs. 5 and 6.
We presented an approach to sentence condensation that employs linguistically rich LFG grammars in a parsing/generation-based stochastic sentence condensa-tion system.
"Fine-grained dependency structures are out-put by the parser, then modified by a highly expressive transfer system, and filtered by a constraint-based gener-ator."
Stochastic selection of generation-filtered reduced structures uses a powerful Maximum-Entropy model.
"As shown in an experimental evaluation, summarization quality of the system output is state-of-the-art, and gram-maticality of condensed strings is guaranteed."
Robustness techniques for parsing and generation guarantee that the system produces non-empty output for unseen input.
"Overall, the summarization quality achieved by our system is similar to the results reported[REF_CITE]."
This might seem disappoint-ing considering the more complex machinery employed in our approach.
It has to be noted that these re-sults are partially due to the somewhat artificial na-ture of the data that were used in the experiments[REF_CITE]and therefore in our experi-ments: The human-written condensations in the data set extracted from the Ziff-Davis corpus show the same word order as the original sentences and do not exhibit any structural modification that are common in human-written summaries.
"For example, humans tend to make use of structural modifications such as nominalization and verb alternations such as active/passive or transi-tive/intransitive alternations in condensation."
"Such alter-nations can easily be expressed in our transfer-based approach, whereas they impose severe problems to ap-proaches that operate only on phrase structure trees."
"In the given test set, however, the condensation task re-stricted to the operation of deletion."
"A creation of addi-tional condensations for the original sentences other than the condensed versions extracted from the human-written abstracts would provide a more diverse test set, and fur-thermore make it possible to match each system output against any number of independent human-written con-densations of the same original sentence."
"This idea of computing matching scores to multiple reference exam-ples was proposed[REF_CITE], and later[REF_CITE]for evaluation of machine transla-tion systems."
"Similar to these proposals, an evaluation of condensation quality could consider multiple reference condensations and record the matching score against the most similar example."
Another desideratum for future work is to carry condensation all the way through without unpacking at any stage.
"Work on employing packing techniques not only for parsing and transfer, but also for genera-tion and stochastic selection is currently underway (see[REF_CITE])."
"This will eventually lead to a system whose components work on packed represen-tations of all or n-best solutions, but completely avoid costly unpacking of representations."
"This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation."
"The MAP framework is gen-eral enough to include some previous model adaptation approaches, such as corpus mixing[REF_CITE], for example."
Other approaches falling within this framework are more effec-tive.
"In contrast to the results[REF_CITE], we show F-measure parsing accuracy gains of as much as 2.5% for high accuracy lexicalized pars-ing through the use of out-of-domain treebanks, with the largest gains when the amount of in-domain data is small."
MAP adaptation can also be based on either supervised or unsupervised adap-tation data.
"Even when no in-domain treebank is available, unsupervised techniques provide a sub-stantial accuracy gain over unadapted grammars, as much as nearly 5% F-measure improvement."
A fundamental concern for nearly all data-driven ap-proaches to language processing is the sparsity of la-beled training data.
"The sparsity of syntactically anno-tated corpora is widely remarked upon, and some recent papers present approaches to improving performance in the absence of large amounts of annotated training data."
"Statistical model adaptation based on sparse in-domain data, however, is neither a new problem nor unique to pars-ing."
It has been studied extensively by researchers work-ing on acoustic modeling for automatic speech recognition (ASR) ([REF_CITE];
One of the meth-ods that has received much attention in the ASR literature is maximum a posteriori (MAP) estimati[REF_CITE].
"In MAP estimation, the parameters of the model are considered to be random variables themselves with a known distribution (the prior)."
"The prior distribution and the max-imum likelihood distribution based on the in-domain obser-vations then give a posterior distribution over the parame-ters, from which the mode is selected."
"If the amount of in-domain (adaptation) data is large, the mode of the posterior distribution is mostly defined by the adaptation sample; if the amount of adaptation data is small, the mode will nearly coincide with the mode of the prior distribution."
"The intu-ition behind MAP estimation is that once there are sufficient observations, the prior model need no longer be relied upon."
"Indeed, this approach can be used for any gen-erative probabilistic model, such as part-of-speech taggers."
"In their language modeling approach, in-domain counts are mixed with the out-of-domain model, so that, if the num-ber of observations within the domain is small, the out-of-domain model is relied upon, whereas if the number of observations in the domain is high, the model will move toward a Maximum Likelihood (ML) estimate on the in-domain data alone."
The case of a parsing model trained via relative frequency estimation is identical: in-domain counts can be combined with the out-of-domain model in just such a way.
"We will show below that weighted count merging is a special case of MAP adaptation; hence the approach[REF_CITE]cited above is also a special case of MAP adaptation, with a particular parameterization of the prior."
This parameterization is not necessarily the one that opti-mizes performance.
"In the next section, MAP estimation for PCFGs is pre-sented."
"This is followed by a brief presentation of the PCFG model that is being learned, and the parser that is used for the empirical trials."
"We will present empirical results for multiple MAP adaptation schema, both starting from the Penn Wall St. Journal treebank and adapting to the Brown corpus, and vice versa."
We will compare our su-pervised adaptation performance with the results presented[REF_CITE].
"In addition to supervised adaptation, i.e. with a manually annotated treebank, we will present results for unsupervised adaptation, i.e. with an automatically an-notated treebank."
"We investigate a number of unsupervised approaches, including multiple iterations, increased sample sizes, and self-adaptation."
"In the maximum a posteriori estimation framework de-scribed in detail[REF_CITE], the model pa-rameters θ are assumed to be a random vector in the space"
"Θ. Given an observation sample x, the MAP estimate is ob-tained as the mode of the posterior distribution of θ denoted as g(. | x) θMAP = argmax g(θ | x) = argmax f(x | θ)g(θ) (1) θ θ"
"In the case of n-gram model adaptation, as discussed[REF_CITE], the objective is to estimate probabilities for a discrete distribution across words, en-tirely analogous to the distribution across mixture compo-nents within a mixture density, which is a common use for MAP estimation in ASR."
"A practical candidate for the prior distribution of the weights ω 1 , ω 2 , · · · , ω K , is its conjugate prior, the Dirichlet density,"
"K g(ω 1 , ω 2 , · · · , ω K | ν 1 , ν 2 , · · · , ν K ) ∝ Y ω νi i −1 (2) i=[Footnote_1] where ν i &gt; 0 are the parameters of the Dirichlet distribu-tion."
"1 An additional condition for well-formedness is that the PCFG is consistent or tight, i.e. there is no probability mass lost to in-finitely large trees.[REF_CITE]proved that this con-dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus."
"With such a prior, if the expected counts for the i-th component is denoted as c i , the mode of the posterior distri-bution is obtained as (ν i − 1) + c i ω̂ i = 1 ≤ i ≤ K. (3) K K P k=1 (ν k − 1) +"
P k=1 c k
"We can use this formulation to estimate the posterior, but we must still choose the parameters of the Dirichlet."
"First, let us introduce some notation."
"A context-free grammar (CFG) G = (V, T, P, S † ), consists of a set of non-terminal symbols V , a set of terminal symbols T, a start symbol S † ∈ V , and a set of rule productions P of the form: A → γ, where A ∈ V and γ ∈ (V ∪ T) ∗ ."
"A probabilistic context-free grammar (PCFG) is a CFG with a probability assigned to each rule, such that the probabilities of all rules expanding a given non-terminal sum to one; specifically, each right-hand side has a probability given the left-hand side of the rule 1 ."
"Let A denote the left-hand side of a production, and γ i the i-th possible expansion of A. Let the probability estimate for the production A → γ i according to the out-of-domain model be denoted as P(γ i | A) and let the expected adapta-tion counts be denotede as c(A → γ i )."
Then the parameters of the prior distribution for left-hand side A are chosen as ν Ai = τ
A P(γ i | A) + 1 (4)1 ≤ i ≤ K. where τ
A is the left-hande side dependent prior weighting pa-rameter.
This choice of prior parameters defines the MAP estimate of the probability of expansion γ i from the left-hand side A as τ
A P(γ i | A) + c(A → γ i ) 1 ≤ i ≤ K. (5)
P̂(γ i | A) = e K τ
A + P k=1 c(A → γ k )
Note that the MAP estimates with this parameterization re-duce to the out-of-domain model parameters in the absence of adaptation data.
"Each left-hand side A has its own prior distribution, pa-rameterized with τ"
This presents an over-parameterization problem.
We follow[REF_CITE]in adopt-ing a parameter tying approach.
"As pointed out[REF_CITE], two methods of parameter ty-ing, in fact, correspond to two well known model mixing approaches, namely count merging and model interpolation."
"Let P and c denote the probabilities and counts from the out-of-domaine e model, and let P and c denote the probabili-ties and counts from the adaptation model (i.e. in-domain)."
If the left-hand side dependent prior weighting parameter is chosen as τ
"A = c(A)α, (6) β e the MAP adaptation reduces to count merging, scaling the out-of-domain counts with a factor α and the in-domain counts with a factor β: c(A) βα"
Pe(γ i | A) + c(A → γ i )
P̂(γ i | A) = e c(A) βα + c(A) αec(A →e γ i ) + βc(A → γ i ) = (7) αec(A) + βc(A)
"If the left-hand side dependent prior weighting parameter is chosen as c(A) 1−λλ , 0 &lt; λ &lt; 1 if c(A) &gt; 0 τ A = (8) 1 otherwise the MAP adaptation reduces to model interpolation using interpolation parameter λ: c(A) 1−λλ P(γ i | A) + c(A → γ i ) c(eA)"
P(γ i | A) = b 1−λλ + c(A) λ
P(γ i | A) +
P(γ i | A) = 1−λ e λ + 1 1−λ = λP(γ i | A) + (1 − λ)P(γ i | A) (9) e
"While we will not be presenting empirical results for other parameter tying approaches in this paper, we should point out that the MAP framework is general enough to allow for other schema, which could potentially improve perfor-mance over simple count merging and model interpolation approaches."
"For example, one may choose a more com-plicated left-hand side dependent prior weighting parameter such as c(A) 1−λλ , 0 &lt; λ &lt; 1 if c(A) c(A) &gt; θ τ A = c(A) βα otherwisee e (10) for some threshold θ."
"Such a schema may do a better job of managing how quickly the model moves away from the prior, particularly if there is a large difference in the respec-tive sizes of the in-domain and out-of domain corpora."
We leave the investigation of such approaches to future research.
"Before providing empirical results on the count merging and model interpolation approaches, we will introduce the parser and parsing models that were used."
"For the empirical trials, we used a top-down, left-to-right (incremental) statistical beam-search parser[REF_CITE]."
We refer readers to the cited papers for de-tails on this parsing algorithm.
"Briefly, the parser maintains a set of candidate analyses, each of which is extended to attempt to incorporate the next word into a fully connected partial parse."
"As soon as “enough” candidate parses have been extended to the next word, all parses that have not yet attached the word are discarded, and the parser moves on to the next word."
"This beam search is parameterized with a base beam parameter γ, which controls how many or how few parses constitute “enough”."
"Candidate parses are ranked by a figure-of-merit, which promotes better can-didates, so that they are worked on earlier."
"The figure-of-merit consists of the probability of the parse to that point times a look-ahead statistic, which is an estimate of how much probability mass it will take to connect the parse with the next word."
"It is a generative parser that does not require any pre-processing, such as POS tagging or chunking."
It has been demonstrated in the above papers to perform compet-itively on standard statistical parsing tasks with full cover-age.
Baseline results below will provide a comparison with other well known statistical parsers.
"The PCFG is a Markov grammar[REF_CITE], i.e. the production probabilities are estimated by decomposing the joint probability of the categories on the right-hand side into a product of conditionals via the chain rule, and making a Markov assumption."
"Thus, for example, a first order Markov grammar conditions the probability of the category of the i-th child of the left-hand side on the cat-egory of the left-hand side and the category of the (i-1)-th child of the left-hand side."
The benefits of Markov gram-mars for a top-down parser of the sort we are using is de-tailed[REF_CITE].
"Further, as in Roark (2001a; 2003), the production probabilities are conditioned on the label of the left-hand side of the production, as well as on features from the left-context."
"The model is smoothed using standard deleted interpolation, wherein a mixing parameter λ is esti-mated using EM on a held out corpus, such that probability of a production A → γ, conditioned on j features from the left context, X 1j = X 1 . . ."
"X j , is defined recursively as"
P(A → γ | X 1j ) =
"P(γ | A, X 1j ) (11) 1 ) + λP(γ | A, X 1j−1 ) j = (1 − λ)bP(γ | A, X where P is the maximum likelihood estimate of the condi-tional probabilityb ."
"These conditional probabilities decom-pose via the chain rule as mentioned above, and a Markov assumption limits the number of previous children already emitted from the left-hand side that are conditioned upon."
These previous children are treated exactly as other con-ditioning features from the left context.
Table 1 gives the conditioning features that were used for all empirical trials in this paper.
There are different conditioning features for parts-of-speech (POS) and non-POS non-terminals.
"Deleted interpolation leaves out one feature at a time, in the reverse order as they are presented in the table 1."
The grammar that is used for these trials is a PCFG that is induced using relative frequency estimation from a trans-formed treebank.
The trees are transformed with a selec-tive left-corner transformati[REF_CITE]that has been flattened as presented[REF_CITE].
"This transform is only applied to left-recursive productions, i.e. productions of the form A → Aγ."
The transformed trees look as in figure 1.
"The transform has the benefit for a top-down incremental parser of this sort of delaying many of the parsing decisions until later in the string, without un-duly disrupting the immediate dominance relationships that provide conditioning features for the probabilistic model."
The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evalua-tion [Footnote_2] .
2[REF_CITE]for a presentation of the transform/de-transform paradigm in parsing.
"For the trials reported in the next section, the base beam parameter is set at γ = 10."
"In order to avoid being pruned, a parse must be within a probability range of the best scoring parse that has incorporated the next word."
"Let k be the num-ber of parses that have incorporated the next word, and let p̃ be the best probability from among that set."
Then the prob-
The parsing models were trained and tested on treebanks from the Penn Treebank II.
"For the Wall St. Journal portion, we used the standard breakdown: sections 2-21 were kept training data; section 24 was held-out development data; and section 23 was for evaluation."
"For the Brown corpus por-tion, we obtained the training and evaluation sections used[REF_CITE]."
"In that paper, no held-out section was used for parameter tuning [Footnote_3] , so we further partitioned the training data into kept and held-out data."
3 ability of a parse must be above p̃k10 γ to avoid being pruned.
"The sizes of the corpora are given in table 2, as well as labels that are used to refer to the corpora in subsequent tables."
The first results are for parsing the Brown corpus.
"Table 3 presents our baseline performance, compared with the[REF_CITE]results."
Our system is labeled as ‘MAP’.
All parsing results are presented as labeled precision and recall.
"The goal is not to improve upon Gildea’s parsing performance, but rather to try to get more benefit from the out-of-domain data."
"While our performance is 0.5- 1.5 percent better than Gildea’s, the same trends hold – low eighties in accuracy when using the Wall St. Journal (out-of-domain) training; mid eighties when using the Brown corpus training."
Notice that using the Brown held out data with the Wall St. Journal training improved precision substantially.
Tuning the parameters on in-domain data can make a big difference in parser performance.
"Choosing the smoothing parameters as Gildea did, based on the distribution within the corpus itself, may be effective when parsing within the same distribution, but appears less so when using the tree-bank for parsing outside of the domain."
Table 4 gives the baseline performance on section 23 of the WSJ Treebank.
"Note, again, that the Gildea results are for sentences ≤ 40 words in length, while all others are for all sentences in the test set."
"Also, Gildea did not report per-formance of a Brown corpus trained parser on the WSJ."
"Our performance under that condition is not particularly good, but again using an in-domain held out set for parameter tun-ing provided a substantial increase in accuracy, somewhat more in terms of precision than recall."
"Our baseline results for a WSJ section 2-21 trained parser are slightly better than the Gildea parser, at more-or-less the same level of perfor-mance[REF_CITE]and[REF_CITE], but sev-eral points below the best reported results on this task."
Table 5 presents parsing results on the Brown;E test set for models using both in-domain and out-of-domain training data.
"The table gives the adaptation (in-domain) treebank that was used, and the τ A that was used to combine the adap-tation counts with the model built from the out-of-domain treebank."
"Recall that αec(A) times the out-of-domain model yields count merging, with α the ratio of out-of-domain to in-domain counts; and αc(A) times the out-of-domain model yields model interpolation, with α the ratio of out-of-domain to in-domain probabilities."
This resulted in a 0.25 improvement in the F-measure.
"In our case, combining the counts in this way yielded a half a point, perhaps because of the in-domain tuning of the smoothing parameters."
"However, when we optimize α em-pirically on the held-out corpus, we can get nearly a full point improvement."
Model interpolation in this case per- forms nearly identically to count merging.
"Adaptation to the Brown corpus, however, does not ad-equately represent what is likely to be the most common adaptation scenario, i.e. adaptation to a consistent domain with limited in-domain training data."
"The Brown corpus is not really a domain; it was built as a balanced corpus, and hence is the aggregation of multiple domains."
The reverse scenario – Brown corpus as out-of-domain parsing model and Wall St. Journal as novel domain – is perhaps a more natural one.
"In this direction,[REF_CITE]also reported very small improvements when adding in the out-of-domain treebank."
"This may be because of the same issue as with the Brown corpus, namely that the optimal ratio of in-domain to out-of-domain is not 1 and the smoothing parameters need to be tuned to the new domain; or it may be because the new domain has a million words of training data, and hence has less use for out-of-domain data."
"To tease these apart, we par-titioned the WSJ training data (sections 2-21) into smaller treebanks, and looked at the gain provided by adaptation as the in-domain observations grow."
These smaller treebanks provide a more realistic scenario: rapid adaptation to a novel domain will likely occur with far less manual annotation of trees within the new domain than can be had in the full Penn Treebank.
"Table 6 gives the baseline performance on WSJ;23, with models trained on fractions of the entire 2-21 test set."
"Sec-tions 2-21 contain approximately 40,000 sentences, and we partitioned them by percentage of total sentences."
"From ta-ble 6 we can see that parser performance degrades quite dra-matically when there is less than 20,000 sentences in the training set, but that even with just 2000 sentences, the sys-tem outperforms one trained on the Brown corpus."
Table 7 presents parsing accuracy when a model trained on the Brown corpus is adapted with part or all of the WSJ training corpus.
"From this point forward, we only present results for count merging, since model interpolation con-sistently performed 0.2-0.5 points below the count merging approach 4 ."
The τ A mixing parameter was empirically opti-mized on the held out set when the in-domain training was just 10% of the total; this optimization makes over a point difference in accuracy.
"Like Gildea, with large amounts of in-domain data, adaptation improved our performance by half a point or less."
"When the amount of in-domain data is small, however, the impact of adaptation is much greater."
Their automatically annotated corpus was the output of a speech recognizer which used the out-of-domain n-gram model.
"In our case, we use the pars-ing model trained on out-of-domain data, and output a set of candidate parse trees for the strings in the in-domain cor-pus, with their normalized scores."
"These normalized scores (posterior probabilities) are then used to give weights to the features extracted from each candidate parse, in just the way that they provide expected counts for an expectation maxi-mization algorithm."
"For the unsupervised trials that we report, we collected up to 20 candidate parses per string 5 ."
"We were interested in investigating the effects of adaptation, not in optimizing per-formance, hence we did not empirically optimize the mixing parameter τ"
"A for the new trials, so as to avoid obscuring the effects due to adaptation alone."
"Rather, we used the best performing parameter from the supervised trials, namely 0.20ec(A)."
"Since we are no longer limited to manually anno-tated data, the amount of in-domain WSJ data that we can include is essentially unlimited."
"Hence the trials reported go beyond the 40,000 sentences in the Penn WSJ Treebank, to include up to 5 times that number of sentences from other years of the WSJ."
Table 8 shows the results of unsupervised adaptation as we have described it.
Note that these improvements are had without seeing any manually annotated Wall St. Journal treebank data.
"Using the approximately 40,000 sentences in f2-21, we derived a 3.8 percent F-measure improvement over using just the out of domain data."
"Going beyond the size of the Penn Treebank, we continued to gain in accuracy, reaching a total F-measure improvement of 4.2 percent with 200 thousand sentences, approximately [Footnote_5] million words."
"5 Because of the left-to-right, heuristic beam-search, the parser does not produce a chart, rather a set of completed parses."
"A second iteration with this best model, i.e. re-parsing the 200 thousand sentences with the adapted model and re-training, yielded an additional 0.65 percent F-measure improvement, for a total F-measure improvement of [Footnote_4].85 percent over the baseline model."
"4 This is consistent with the results presented[REF_CITE], which found a small but con-sistent improvement in performance with count merging versus model interpolation for n-gram modeling."
"A final unsupervised adaptation scenario that we inves-tigated is self-adaptation, i.e. adaptation on the test set it-self."
"Because this adaptation is completely unsupervised, thus does not involve looking at the manual annotations at all, it can be equally well applied using the test set as the un-supervised adaptation set."
"Using the same adaptation proce-dure presented above on the test set itself, i.e. producing the top 20 candidates from WSJ;23 with normalized posterior probabilities and re-estimating, we produced a self-adapted parsing model."
"This yielded an F-measure accuracy of 76.8, which is a 1.1 percent improvement over the baseline."
What we have demonstrated in this paper is that maximum a posteriori (MAP) estimation can make out-of-domain train-ing data beneficial for statistical parsing.
In the most likely scenario – porting a parser to a novel domain for which there is little or no annotated data – the improvements can be quite large.
"Like active learning, model adaptation can reduce the amount of annotation required to converge to a best level of performance."
"In fact, MAP coupled with active learning may reduce the required amount of annotation further."
There are a couple of interesting future directions for this research.
"First, a question that is not addressed in this paper is how to best combine both supervised and unsupervised adaptation data."
"Since each in-domain resource is likely to have a different optimal mixing parameter, since the super-vised data is more reliable than the unsupervised data, this becomes a more difficult, multi-dimensional parameter op-timization problem."
"Hence, we would like to investigate au-tomatic methods for choosing mixing parameters, such as EM."
"Also, an interesting question has to do with choosing which treebank to use for out-of-domain data."
"For a new domain, is it better to choose as prior the balanced Brown corpus, or rather the more robust Wall St. Journal treebank?"
Perhaps one could use several out-of-domain treebanks as priors.
"Most generally, one can imagine using k treebanks, some in-domain, some out-of-domain, and trying to find the best mixture to suit the particular task."
"The conclusion[REF_CITE], that out-of-domain tree-banks are not particularly useful in novel domains, was pre-mature."
"Instead, we can conclude that, just as in other sta-tistical estimation problems, there are generalizations to be had from these out-of-domain trees, providing more robust estimates, especially in the face of sparse training data."
Conditional random fields for sequence label-ing offer advantages over both generative mod-els like HMMs and classifiers applied at each sequence position.
"Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the devel-opment of standard evaluation datasets and ex-tensive comparison among methods."
"We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported sin-gle model."
Improved training methods based on modern optimization algorithms were crit-ical in achieving these results.
We present ex-tensive comparisons between models and train-ing methods that confirm and strengthen pre-vious results on shallow parsing and training methods for maximum-entropy models.
Sequence analysis tasks in language and biology are of-ten described as mappings from input sequences to se-quences of labels encoding the analysis.
"In language pro-cessing, examples of such tasks include part-of-speech tagging, named-entity recognition, and the task we shall focus on here, shallow parsing."
"Shallow parsing iden-tifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or informa-tion extracti[REF_CITE]."
"The paradigmatic shallow-parsing problem is NP chunking, which finds the non-recursive cores of noun phrases called base NPs."
"The pioneering work[REF_CITE]in-troduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics."
"The task was extended to additional phrase types for the[REF_CITE]shared task (Tjong[REF_CITE]), which is now the standard evaluation task for shallow parsing."
Most previous work used two main machine-learning approaches to sequence labeling.
"The first approach re-lies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs)[REF_CITE]or multilevel Markov models[REF_CITE]."
"The second approach views the sequence labeling problem as a sequence of classification problems, one for each of the labels in the sequence."
The classification re-sult at each position may depend on the whole input and on the previous k classifications. [Footnote_1]
"1[REF_CITE]used transformation-based learning[REF_CITE], which for the present purposes can be tought of as a classification-based method."
The generative approach provides well-understood training and decoding algorithms for HMMs and more general graphical models.
"However, effective genera-tive models require stringent conditional independence assumptions."
"For instance, it is not practical to make the label at a given position depend on a window on the in-put sequence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable."
"Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models."
"The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy[REF_CITE]and a variety of other linear classifiers, including winnow[REF_CITE], AdaBoost[REF_CITE], and support-vector machines[REF_CITE]."
"Furthermore, they are trained to min-imize some function related to labeling error, leading to smaller error in practice if enough training data are avail-able."
"In contrast, generative models are trained to max-imize the joint probability of the training data, which is not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the case in practice."
"However, since sequential classifiers are trained to make the best local decision, unlike generative mod-els they cannot trade off decisions at different positions against each other."
"In other words, sequential classifiers are myopic about the impact of their current decision on later decisions[REF_CITE]."
This forced the best sequential classifier systems to re-sort to heuristic combinations of forward-moving and backward-moving sequential classifiers[REF_CITE].
Conditional random fields (CRFs) bring together the best of generative and classification models.
"Like classi-fication models, they can accommodate many statistically correlated features of the inputs, and they are trained dis-criminatively."
"But like generative models, they can trade off decisions at different sequence positions to obtain a globally optimal labeling."
"In the present work, we show that CRFs beat all re-ported single-model NP chunking results on the standard evaluation dataset, and are statistically indistinguishable from the previous best performer, a voting arrangement of 24 forward- and backward-looking support-vector clas-sifiers[REF_CITE]."
"To obtain these results, we had to abandon the original iterative scal-ing CRF training algorithm for convex optimization al-gorithms with better convergence properties."
We provide detailed comparisons between training methods.
"The generalized perceptron proposed[REF_CITE]is closely related to CRFs, but the best CRF train-ing methods seem to have a slight edge over the general-ized perceptron."
"We focus here on conditional random fields on sequences, although the notion can be used more generally[REF_CITE]."
Such CRFs define conditional probability distributions p(Y |X) of label se-quences given input sequences.
"We assume that the ran-dom variable sequences X and Y have the same length, and use x = x 1 · · · x n and y = y 1 · · · y n for the generic input sequence and label sequence, respectively."
"A CRF on (X, Y ) is specified by a vector f of local features and a corresponding weight vector λ."
"Each local feature is either a state feature s(y, x, i) or a transition feature t(y, y 0 , x, i), where y, y 0 are labels, x an input sequence, and i an input position."
"To make the notation more uniform, we also write s(y, y 0 , x, i) = s(y 0 , x, i) s(y, x, i) = s(y i , x, i) t(y i−1 , y i , x, i) i &gt; 1 t(y, x, i) = 0 i = 1 for any state feature s and transition feature t. Typically, features depend on the inputs around the given position, although they may also depend on global properties of the input, or be non-zero only at some positions, for instance features that pick out the first or last labels."
The CRF’s global feature vector for input sequence x and label sequence y is given by
"F (y, x) = X f(y, x, i) i where i ranges over input positions."
"The conditional probability distribution defined by the CRF is then p λ (Y |X) = exp λ · F (Y , X) (1) Z λ (X) where"
"Z λ (x) = X exp λ · F (y, x) y"
"Any positive conditional distribution p(Y |X) that obeys the Markov property p(Y i |{Y j } j6=i , X) = p(Y i |Y i−1 , Y i+1 , X) can be written in the form (1) for appropriate choice of feature functions and weight vector[REF_CITE]."
"The most probable label sequence for input sequence x is ŷ = arg max p λ (y|x) = arg max λ · F (y, x) y y because Z λ (x) does not depend on y. F (y, x) decom-poses into a sum of terms for consecutive pairs of labels, so the most likely y can be found with the Viterbi algo-rithm."
"We train a CRF by maximizing the log-likelihood of a given training set T = {(x k , y k )} Nk=1 , which we assume fixed for the rest of this section:"
"L λ = = PP k log p λ (y k |x k ) k [λ · F (y k , x k ) − log Z λ (x k )]"
"To perform this optimization, we seek the zero of the gra-dient ∇L λ = X F(y k , x k ) − E p λ (Y |x k ) F (Y , x k ) (2) k"
"In words, the maximum of the training data likelihood is reached when the empirical average of the global fea-ture vector equals its model expectation."
"The expectation E p λ (Y |x) F (Y , x) can be computed efficiently using a variant of the forward-backward algorithm."
"For a given x, define the transition matrix for position i as"
"M i [y, y 0 ] = exp λ · f(y, y 0 , x, i)"
"Let f be any local feature, f i [y,y 0 ] = f(y,y 0 ,x,i), F(y,x) = i f(y i−1 ,y i ,x,i), and let ∗ denote component-wisePmatrix product."
"E p λ (Y |x) F(Y , x) = X p λ (y|x)F(y, x) y α i−1 (f i ∗ M i )β &gt;i = X i Z λ (x) Z λ (x) = α n · 1 &gt; where α i and β i the forward and backward state-cost vectors defined by α i−1 M i 0 &lt; i ≤ n α i = 1 i = 0"
M i+1 β &gt;i+1 1 ≤ i &lt; n β &gt;i = 1 i = n
"Therefore, we can use a forward pass to compute the α i and a backward bass to compute the β i and accumulate the feature expectations."
"To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior[REF_CITE]:"
"L 0λ = X [λ · F (y k , x k ) − log Z λ (x k )] k − kλk 2 + const 2σ 2 with gradient ∇L 0λ = λ X F(y k , x k ) − E p λ (Y |x k ) F (Y , x k ) − σ 2 k"
"Those methods are very sim-ple and guaranteed to converge, but[REF_CITE]and[REF_CITE]showed for classification, their conver-gence is much slower than that of general-purpose convex optimization algorithms when many correlated features are involved."
"Concurrently with the present work,[REF_CITE]tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shal-low parsing problem."
Our work shows that precon-ditioned conjugate-gradient (CG)[REF_CITE]or limited-memory quasi-Newton (L-BFGS)[REF_CITE]perform comparably on very large prob-lems (around 3.8 million features).
"We compare those algorithms to generalized iterative scaling (GIS)[REF_CITE], non-preconditioned CG, and voted perceptron training[REF_CITE]."
All algorithms except voted perceptron maximize the penalized log-likelihood: λ ∗ = argmax λ
L 0λ .
"However, for ease of exposition, this discussion of training methods uses the unpenalized log-likelihood L λ ."
Conjugate-gradient (CG) methods have been shown to be very effective in linear and non-linear optimizati[REF_CITE].
"Instead of searching along the gra-dient, conjugate gradient searches along a carefully cho-sen linear combination of the gradient and the previous search direction."
CG methods can be accelerated by linearly trans-forming the variables with preconditioner[REF_CITE].
"The purpose of the pre-conditioner is to improve the condition number of the quadratic form that locally approximates the objective function, so the inverse of Hessian is reasonable precon-ditioner."
"However, this is not applicable to CRFs for two reasons."
"First, the size of the Hessian is dim(λ) 2 , lead-ing to unacceptable space and time requirements for the inversion."
"In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian."
However in our case the Hessian has the form
"H λ def = ∇ 2 L λ = − X {E [F (Y , x k ) × F (Y , x k )] k −EF (Y , x k ) × EF (Y , x k )} where the expectations are taken with respect to p λ (Y |x k )."
"Therefore, every Hessian element, includ-ing the diagonal ones, involve the expectation of a prod-uct of global feature values."
"Unfortunately, computing those expectations is quadratic on sequence length, as the forward-backward algorithm can only compute expecta-tions of quantities that are additive along label sequences."
We solve both problems by discarding the off-diagonal terms and approximating expectation of the square of a global feature by the expectation of the sum of squares of the corresponding local features at each position.
The ap- proximated diagonal term H f for feature f has the form
H f =
"Ef(Y , x k ) 2 2 − X X Z λ (x) f(Y , x k )"
"M i [y, y 0 ] i y,y 0"
"If this approximation is semidefinite, which is trivial to check, its inverse is an excellent preconditioner for early iterations of CG training."
"However, when the model is close to the maximum, the approximation becomes un-stable, which is not surprising since it is based on fea-ture independence assumptions that become invalid as the weights of interaction features move away from zero."
"Therefore, we disable the preconditioner after a certain number of iterations, determined from held-out data."
We call this strategy mixed CG training.
Newton methods for nonlinear optimization use second-order (curvature) information to find search directions.
"As discussed in the previous section, it is not practi-cal to obtain exact curvature information for CRF train-ing."
"Limited-memory BFGS (L-BFGS) is a second-order method that estimates the curvature numerically from previous gradients and updates, avoiding the need for an exact Hessian inverse computation."
"Compared with preconditioned CG, L-BFGS can also handle large-scale problems but does not require a specialized Hessian ap-proximations."
An earlier study indicates that L-BFGS performs well in maximum-entropy classifier training[REF_CITE].
There is no theoretical guidance on how much infor-mation from previous steps we should keep to obtain sufficiently accurate curvature estimates.
"In our exper-iments, storing 3 to 10 pairs of previous gradients and updates worked well, so the extra memory required over preconditioned CG was modest."
A more detailed descrip-tion of this method can be found elsewhere[REF_CITE].
"Unlike other methods discussed so far, voted perceptron training[REF_CITE]attempts to minimize the differ-ence between the global feature vector for a training in-stance and the same feature vector for the best-scoring labeling of that instance according to the current model."
"More precisely, for each training instance the method computes a weight update λ t+1 = λ t + F (y k , x k ) − F (ŷ k , x k ) (3) in which ŷ k is the Viterbi path ŷ k = arg max λ t · F (y , x k ) y"
"Like the familiar perceptron algorithm, this algorithm re-peatedly sweeps over the training instances, updating the weight vector as it considers each instance."
"Instead of taking just the final weight vector, the voted perceptron algorithm takes the average of the λ t ."
Figure 1 shows the base NPs in an example sentence.
"Fol-lowing[REF_CITE], the input to the NP chunker consists of the words in a sentence anno-tated automatically with part-of-speech (POS) tags."
"The chunker’s task is to label each word with a label indi-cating whether the word is outside a chunk (O), starts a chunk (B), or continues a chunk (I)."
"For example, the tokens in first line of Figure 1 would be labeled BIIBIIOBOBIIO."
"NP chunking results have been reported on two slightly different data sets: the original RM data set[REF_CITE], and the modified[REF_CITE]ver-sion of Tjong[REF_CITE]."
"Although the chunk tags in the[REF_CITE]are somewhat different, we found no significant accuracy differences between models trained on these two data sets."
"There-fore, all our results are reported on the[REF_CITE]data set."
"We also used a development test set, provided by Michael Collins, derived[REF_CITE]tagged with the[REF_CITE]POS tagger."
Our chunking CRFs have a second-order Markov depen-dency between chunk tags.
This is easily encoded by making the CRF labels pairs of consecutive chunk tags.
"That is, the label at position i is y i = c i−1 c i , where c i is the chunk tag of word i, one of O, B, or I. Since B must be used to start a chunk, the label OI is impossible."
"In addi-tion, successive labels are constrained: y i−1 = c i−2 c i−1 , y i = c i−1 c i , and c 0 ="
"O. These contraints on the model topology are enforced by giving appropriate features a weight of −∞, forcing all the forbidden labelings to have zero probability."
"Our choice of features was mainly governed by com-puting power, since we do not use feature selection and all features are used in training and testing."
"We use the following factored representation for features f(y i−1 , y i , x, i) = p(x, i)q(y i−1 , y i ) (4) where p(x, i) is a predicate on the input sequence x and current position i and q(y i−1 , y i ) is a predicate on pairs of labels."
"For instance, p(x, i) might be “word at posi-tion i is the” or “the POS tags at positions i − 1, i are"
"Because the label set is finite, such a factoring of f(y i−1 , y i , x, i) is always possible, and it allows each input predicate to be evaluated just once for many fea-tures that use it, making it possible to work with millions of features on large training sets."
Table 1 summarizes the feature set.
"For a given po-sition i, w i is the word, t i its POS tag, and y i its label."
"For any label y = c 0 c, c(y) = c is the corresponding chunk tag."
"For example, c(OB) ="
"B. The use of chunk tags as well as labels provides a form of backoff from the very small feature counts that may arise in a second-order model, while allowing significant associations be-tween tag pairs and input predicates to be modeled."
"To save time in some of our experiments, we used only the 820,000 features that are supported in the CoNLL train-ing set, that is, the features that are on at least once."
"For our highest F score, we used the complete feature set, around 3.8 million in the CoNLL training set, which con-tains all the features whose predicate is on at least once in the training set."
The complete feature set may in princi-ple perform better because it can place negative weights on transitions that should be discouraged if a given pred-icate is on.
"As discussed previously, we need a Gaussian weight prior to reduce overfitting."
We also need to choose the num-ber of training iterations since we found that the best F score is attained while the log-likelihood is still improv-ing.
"The reasons for this are not clear, but the Gaussian prior may not be enough to keep the optimization from making weight adjustments that slighly improve training log-likelihood but cause large F score fluctuations."
We used the development test set mentioned in Section 4.1 to set the prior and the number of iterations.
"The standard evaluation metrics for a chunker are preci-sion P (fraction of output chunks that exactly match the reference chunks), recall R (fraction of reference chunks returned by the chunker), and their harmonic mean, the F1 score F 1 = 2 ∗ P ∗ R/(P + R) (which we call just F score in what follows)."
"The relationships between F score and labeling error or log-likelihood are not direct, so we report both F score and the other metrics for the models we tested."
For comparisons with other reported results we use F score.
"Ideally, comparisons among chunkers would control for feature sets, data preparation, training and test proce-dures, and parameter tuning, and estimate the statistical significance of performance differences."
"Unfortunately, reported results sometimes leave out details needed for accurate comparisons."
"We report F scores for comparison with previous work, but we also give statistical signifi-cance estimates using McNemar’s test for those methods that we evaluated directly."
Testing the significance of F scores is tricky because the wrong chunks generated by two chunkers are not directly comparable.
"However, bootstrap variances in preliminary experiments were too high to allow any conclusions, so we used instead a McNemar paired test on labeling disagreements[REF_CITE]."
"All the experiments were performed with our Java imple-mentation of CRFs,designed to handle millions of fea-tures, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0."
Minor variants support voted perceptr[REF_CITE]and MEMMs[REF_CITE]with the same efficient feature encoding.
"GIS, CG, and L-BFGS were used to train CRFs and MEMMs."
"Table 2 gives representative NP chunking F scores for previous work and for our best model, with the com-plete set of 3.8 million features."
The last row of the table gives the score for an MEMM trained with the mixed CG method using an approximate preconditioner.
The pub-lished F score for voted perceptron is 93.53% with a dif-ferent feature set[REF_CITE].
The improved result given here is for the supported feature set; the complete feature set gives a slightly lower score of 94.07%.
"All the results in the rest of this section are for the smaller supported set of 820,000 features."
Figures 2a and 2b show how preconditioning helps training convergence.
"Since each CG iteration involves a line search that may require several forward-backward procedures (typically between 4 and 5 in our experiments), we plot the progress of penalized log-likelihood L 0λ with respect to the num-ber of forward-backwardevaluations."
"The objective func-tion increases rapidly, achieving close proximity to the maximum in a few iterations (typically 10)."
"In contrast, GIS training increases L 0λ rather slowly, never reaching the value achieved by CG."
The relative slowness of it-erative scaling is also documented in a recent evaluation of training methods for maximum-entropy classificati[REF_CITE].
"In theory, GIS would eventually con-verge to the L 0λ optimum, but in practice convergence may be so slow that L 0λ improvements may fall below numerical accuracy, falsely indicating convergence."
Mixed CG training converges slightly more slowly than preconditioned CG.
"On the other hand, CG without preconditioner converges much more slowly than both preconditioned CG and mixed CG training."
"However, it is still much faster than GIS."
We believe that the superior convergence rate of preconditioned CG is due to the use of approximate second-order information.
"This is con-firmed by the performance of L-BFGS, which also uses approximate second-order information. [Footnote_2]"
"2 Although L-BFGS has a slightly higher penalized log-likelihood, its log-likelihood on the data is actually lower than that of preconditioned CG and mixed CG training."
"Although there is no direct relationship between F scores and log-likelihood, in these experiments F score tends to follow log-likelihood."
"Indeed, Figure 3 shows that preconditioned CG training improves test F scores much more rapidly than GIS training."
Table 3 compares run times (in minutes) for reaching a target penalized log-likelihood for various training meth-ods with prior σ = 1.0.
"GIS is the only method that failed to reach the target, after 3,700 iterations."
"We cannot place the voted perceptron in this table, as it does not opti-mize log-likelihood and does not use a prior."
"However, it reaches a fairly good F-score above 93% in just two training sweeps, but after that it improves more slowly, to a somewhat lower score, than preconditioned CG train-ing."
The accuracy rate for individual labeling decisions is over-optimistic as an accuracy measure for shallow pars-ing.
"For instance, if the chunk BIIIIIII is labled as OIIIIIII, the labeling accuracy is 87.5%, but recall is 0."
"However, individual labeling errors provide a more convenient basis for statistical significance tests."
One such test is McNemar test on paired observations[REF_CITE].
"With McNemar’s test, we compare the correctness of the labeling decisions of two models."
The null hypothesis is that the disagreements (correct vs. incorrect) are due to chance.
Table 4 summarizes the results of tests between the models for which we had labeling decisions.
"These tests suggest that MEMMs are significantly less accurate, but that there are no significant differences in accuracy among the other models."
"We have shown that (log-)linear sequence labeling mod-els trained discriminatively with general-purpose opti-mization methods are a simple, competitive solution to learning shallow parsers."
"These models combine the best features of generative finite-state models and discrimina-tive (log-)linear classifiers, and do NP chunking as well as or better than “ad hoc” classifier combinations, which were the most accurate approach until now."
In a longer version of this work we will also describe shallow pars-ing results for other phrase types.
"There is no reason why the same techniques cannot be used equally successfully for the other types or for other related tasks, such as POS tagging or named-entity recognition."
"On the machine-learning side, it would be interest-ing to generalize the ideas of large-margin classification to sequence models, strengthening the results[REF_CITE]and leading to new optimal training algorithms with stronger guarantees against overfitting."
"On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoid-ing the label bias problem that may have hindered earlier classifier-based parsers[REF_CITE]."
"However, work in that direction has so far addressed only parse reranking[REF_CITE]."
Full discriminative parser training faces significant algo-rithmic challenges in the relationship between parsing al-ternatives and feature values[REF_CITE]and in computing feature expectations.
Automatic restoration of punctuation from un-punctuated text has application in improving the fluency and applicability of speech recog-nition systems.
"We explore the possibility that syntactic information can be used to improve the performance of an HMM-based system for restoring punctuation (specifically, commas) in text."
"Our best methods reduce sentence error rate substantially — by some 20%, with an ad-ditional 8% reduction possible given improve-ments in extraction of the requisite syntactic in-formation."
"The move from isolated word to connected speech recog-nition engendered a qualitative improvement in the nat-uralness of users’ interactions with speech transcription systems, sufficient even to make up in user satisfaction for some modest increase in error rate."
"Nonetheless, such systems still retain an important source of unnaturalness in dictation, the requirement to utter all punctuation ex-plicitly."
"In order to free the user from this burden, a tran-scription system would have to reconstruct the punctua-tion from the word sequence."
"For certain applications — for instance, transcription of naturally occurring speech not originally targeted to a speech recognizer (as broad-cast audio) — there is no alternative to performing recon-struction of punctuation."
Reconstruction of different punctuation marks is likely to respond to different techniques.
"Reconstruction of pe-riods, question marks, and exclamation marks, for in-stance, is in large part the problem of sentence bound-ary detection."
"In this paper, we address the problem of comma restoration."
"The published literature on intrasen-tence punctuation restoration is quite limited, the state of the art represented by Beeferman, Berger, and Lafferty’s"
"CYBERPUNC system, which we review in Section 2, and reimplement as a baseline for our own experiments. (See Section 5 for discussion of related work.)"
The CYBERPUNC system uses a simple HMM with tri-gram probabilities to model the comma restoration prob-lem.
"It is trained on fully punctuated text, and then tested for precision and recall in reconstructing commas in text that has had them removed."
Our replication of the trigram-based method yields a sentence accuracy of 47%.
"However, the role of the comma in text is closely related to syntactic constituency."
"In both cases, one expects to see commas at the beginning or end of con-stituents, rather than in the middle."
But this type of cor-relation is difficult to model with a flat model such as an HMM.
"For this reason, we explore here the use of syn-tactic constituency information for the purpose of comma restoration."
We show that even very rarefied amounts of syntactic information can dramatically improve comma restoration performance; our best method accurately re-stores 58% of sentences.
"Furthermore, even approximate syntactic information provides significant improvement."
"There is, of course, great variation in appropriate punc-tuation of a single word stream. [Footnote_1]"
"1 In an old unattributed joke, an English professor asks some students to punctuate the word sequence “Woman without her man is nothing”. The male students preferred “Woman, without her man, is nothing.” whereas the female proposed “Woman! Without her, man is nothing.” No, it’s not funny, but it does make the point."
"For this reason, inde-pendent human annotators consider only about 86% of the sentences in the test set to be correct with respect to comma placement[REF_CITE]."
"Thus, a move from 47% to 58% is a quite substantial improve-ment, essentially a reduction in sentence error rate of some 30%."
There has been a tremendous amount of research since the early 1990’s on the problem of parsing using statisti-cal models and evaluated by statistical measures such as crossing brackets rate.
"Statistical parsing, like language modeling, is presumably of interest not in and of itself but rather by virtue of its contribution to some end-user appli-cation."
"In the case of language modeling, speech recog-nition is the leading exemplar among a large set of end-user natural-language-processing applications that bene-fit from the technology."
"Further, the statistical figures of merit are appropriate just insofar as they vary more or less continuously and monotonically with the performance of the end-user application."
"Again, in the case of language modeling, speech recognition error rate is generally ac-knowledged to improve in direct relation to reduction in cross-entropy of the language model employed."
"For statistical parsing, it is much more difficult to say what applications actually benefit from this component technology in the sense that incremental improvements to the technology as measured by the statistical figures of merit provide incremental benefit to the application."
The leading argument for parsing a sentence is that this establishes the structure upon which semantic interpreta-tion can be performed.
"But it is hard to imagine in what sense an 85% correct parse is better than an 80% correct parse, as the semantic interpretation generated off of each is likely to be wrong."
"Barring a sensible notion of “par-tially correct interpretation” and an end-user application in which a partially correct interpretation is partially as good as a fully correct one, we would not expect statisti-cal parsers to be useful for end user applications based on sentence interpretation. [Footnote_2]"
"2 We might expect nonstatistical parsers also not to be use-ful, but for a different reason, their fragility. Rather than de-livering partially correct results, they partially deliver correct results. But that is a different issue."
"In fact, to the authors’ knowl-edge, the comma restoration results presented here are the first instance of an end-user application that bears on its face this crucial property, that incremental improvement on statistical parsing provides incremental improvement in the application."
"As a baseline, we replicated the three-state HMM method[REF_CITE]."
"In this section, we describe that method, which we use as the basis for our extensions."
The input to comma restoration is a sentence x = x 1 . . . x n of words and punctuation but no commas.
"We would like to generate a restored string y = y 1 . . . y n+c , which is the string x with c commas inserted."
The se-lected y should maximize conformance with a simple tri- gram model: y ∗ = argmax y Y p(y | y y ) n+c i i−2 i−1 i=1
We take the string x to be the observed output of an HMM with three states and transition probabilities de-pendent on output; the states encode the position of com-mas in a reconstructed string.
Figure 1 depicts the au-tomaton.
"The start state (1) corresponds to having seen a word with no prior or following comma, state (2) a word with a following comma, and state (3) a word with a prior but no following comma."
It is easy to see that a path through the automaton traverses a string y with probabil-
Q n+c ity i=1 p(y i | y i−2 y i−1 ).
The decoded string y ∗ can therefore be computed by Viterbi decoding. [Footnote_3]
"3 As it turns out, the same computation can be done using a two-state model. This automaton does not, however, lend itself as easily to extensions."
This method requires a trigram language model p().
"We train this language model on sections 02–22 of the Penn Treebank Wall Street Journal data (WSJ) [Footnote_4] , com-prising about 36,000 sentences."
"4 For consistency, we use the version of the Wall Street Jour-nal data that was used[REF_CITE]for their CY - BERPUNC experiments. This comprises sections 02–23 of the Wall Street Journal (the last of these being used as test data) with minor variations from the Treebank version, for instance, a small number of missing sentences and some variation in the tags. Runs of the experiments below using the Treebank ver-sions of the data yield essentially identical results."
The CMU Statistical Language Modeling Toolkit[REF_CITE]was used to generate the model.
Katz smoothing was used to incorporate lower-order models.
The model was then tested on the approximately 2300 sentences[REF_CITE].
Precision of the comma restoration was 71.1% and recall 55.2%.
"F-measure, calculated as 2PR/(P + R), where P is precision and R recall, is 62.2%."
"Sentence accuracy, the percentage of sentences cor-rectly restored, was 47.0%. (These results are presented as model 1 in Table 1.)"
This is the baseline against which we evaluate our alternative comma restoration models.
"Beeferman et al. present an alternative trigram model, which computes the following: y ∗ = argmax y Y n+c p(y i | y i−2 y i−1 ) i=1 (1 − p(, | y i−2 y i−1 )) δ(y i ) where 0 y i =, δ(y i ) = 1 otherwise"
"That is, an additional penalty is assessed for not placing a comma at a given position."
"By penalizing omission of a comma between two words, the model implicitly re-wards commas; we would therefore expect higher recall and correspondinglylower precision. 5"
"In fact, the method with the omission penalty (model 2 in Table 1), does have higher recall and lower precision, essentially identical F-measure, but lower sentence accuracy."
"Henceforth, the models described here do not use an omission penalty."
"Insofar as commas are used as separators or delimiters, we should see correlation of comma position with con-stituent structure of sentences."
A simple test reveals that this is so.
We define the start count sc i of a string posi-tion i as the number of constituents whose left boundary is at i. The end count ec i is defined analogously.
"For ex-ample, in Figure 2, sc 0 is 4, as the constituents labeled JJ, NPB, S, and S start there; ec 0 is 0."
We compute the end count for positions that have a comma by first drop-ping the comma from the tree.
"Thus, at position [Footnote_5], sc 5 is 2 (constituents DT, NPB) and ec 5 is 4 (constituents JJ, ADJP, VP, S)."
"5 Counterintuitively,[REF_CITE]come to the opposite expectation, and their reported results bear out their intuition. We have no explanation for this disparity with our results."
We expect to find that the distributions of sc and ec for positions in which a comma is inserted should differ from those in which no comma appears.
Figure 3 reveals that this intuition is correct.
The charts show the per-centage of string positions with each possible value of sc (resp. ec) for those positions with commas and those without.
"We draw the data again from sections 02–22 of the Wall Street Journal, using as the specification for the constituency of sentences the parses for these sentences from the Penn Treebank."
"The distributions are quite dif-ferent, hinting at an opportunity for improved comma restoration."
"The ec distribution is especially well differentiated, with a cross-over point at about 2 constituents."
"We can add this kind of information, a single bit specifying an ec as follows."
"We replace p(y i |cy y ) value of k or greater (call it ec i ), to the language model, i−2 i−1 with the proba-lower order models p(cy | y ecc bility p(y i | y i−2 y i−1 ec i )."
"We smooth the model using i−1 i ), p(y i | ecc i i ), p(y i ). [Footnote_6] These distributions can be estimated from the training data directly, and smoothed appropriately."
"6 Alternative backoff paths, for instance backing off first to p(y i | y i−2 y i−1 ), exhibit inferior performance."
Adding just this one bit of information provides signifi-cant improvement to comma restoration performance.
"As it turns out, a k value of 3 turns out to maximize perfor-mance. [Footnote_7] Compared to the baseline, F-measure increases to 63.2% and sentence accuracy to 52.3%."
"7 With k = 2 (model 4), precision drops precipitously to 60.4%, recall stays roughly the same at 66.4%."
"This exper-iment shows that constituency information, even in rar-efied form, can provide significant performance improve-ment in comma restoration. (Figure 1 lists performance figures as model 3.)"
"Of course, this result does not provide a practical al-gorithm for comma restoration, as it is based on a prob-abilistic model that requires data from a manually con-structed parse for the sentence to be restored."
"To make the method practical, we might replace the Treebank parse with a statistically generated parse."
"In the sequel, we use Collins’s statistical parser[REF_CITE]as our canon-ical automated approximation of the Treebank."
"We can train a similar model, but using ec values extracted from ends for string positions with and without commas."
Chart (a) shows the percentage of constituents with various val-ues of sc (number of constituents starting at the posi-tion) for string positions with commas (square points) and without (diamond points).
Chart (b) shows the corre-sponding pattern for values of ec (number of constituents ending).
"Collins parses of the training data, and use the model to restore commas on a test sentence again using ec values from the Collins parse of the test datum."
"This model, listed as model 5 in Table 1, has an F-measure of 64.5%, better than the pure trigram model (62.2%), but not as good as the oracular Treebank-trained model (68.4%)."
The other metrics show similar relative orderings.
"In this model, since the test sentence has no commas initially, we want to train the model on the parses of sen-tences that have had commas removed, so that the model is being applied to data as similar as possible to that on which it was trained."
"We would expect, and experiments verify (model 6), that training on the parses with com-mas retained yields inferior performance (in particular, F-measure of 64.1% and sentence accuracy of 48.6%)."
"Again consistent with expectations, if we could clairvoy-antly know the value of ecc i based on a Collins parse of the test sentence with the commas that we are trying to restore (model 7), performance is improved over model 5; F-measure rises to 66.8%."
The steady rise in performance from models 6 to 5 to 7 to 3 exactly tracks the improved nature of the syntac-tic information available to the system.
"As the quality of the syntactic information better approximates ground truth, our ability to restore commas gradually and mono-tonically improves."
The examples above show that even a tiny amount of syn-tactic information can have a substantive advantage for comma restoration.
"In order to use more information, we might imagine using values of ec directly, rather than thresholding."
"However, this quickly leads to data spar-sity problems."
"To remedy this, we assume independence between the bigram in the conditioning context and the syntactic information, that is, we take p(y i | y i−2 y i−1 ec i ) ≈ p(y i | y i−2 y i−1 )p(y i | y i−1 ec i ) p(y i )"
"This model [Footnote_8] (model 8) has an F-measure of 72.1% due to a substantial increase in recall, demonstrating that the increased articulation in the syntactic information avail-able provides a concomitant benefit."
"8 We back off the first term in the approximation as before, and the second to p(y i | y i−1 )."
"Although the sen-tence accuracy is slightly less than that with thresholded ec, we will show in a later section that this model com-bines well with other modifications to generate further"
Additional information from the parse can be useful in predicting comma location.
"In this section, we incorpo-rate part of speech information into the model, generating model 10."
"We estimate the joint probability of each word x i and its part of speech X i as follows: p(x i , X i | x i−2 , X i−2 , x i−1 , X i−1 , ec) ≈ p(x i | x i−2 x i−1 ec)p(X i | X i−2 X i−1 )"
"The first term is computed as in model 8, the second back-ing off to bigram and unigram models."
Adding a part of speech model in this way provides a further improvement in performance.
"F-measure improves to 74.8%, sentence accuracy to 57.[Footnote_9]%, a 28% improvement over the base-line."
"9 An alternative method of resolving the data sparsity issues is to back off the model p(y i | y i−2 y i−1 ec i ), for instance to p(y i | y i−2 y i−1 ) or to p(y i | y i−1 ec i ). Both of these perform less well than the approximation in model 8."
"These models (8 and 10), like model 3, assumed availability of the Treebank parse and part of speech tags."
"Using the Collins-parse-generated parses still shows improvement over the corresponding model 5: an F-measure of 70.1% and sentence accuracy of 54.[Footnote_9]%, twice the improvement over the baseline as exhibited by model 5."
"9 An alternative method of resolving the data sparsity issues is to back off the model p(y i | y i−2 y i−1 ec i ), for instance to p(y i | y i−2 y i−1 ) or to p(y i | y i−1 ec i ). Both of these perform less well than the approximation in model 8."
"We compare our comma restoration methods to those[REF_CITE], as their results use only textual information to predict punctuation."
Several researchers have shown prosodic information to be useful in predict-ing punctuati[REF_CITE](along with related phenomena such as dis-fluencies and overlapping speech[REF_CITE]).
"These studies, typically based on augmenting a Marko-vian language model with duration or other prosodic cues as conditioning features, show that prosody information is orthogonal to language model information; combined models outperform models based on each type of infor-mation separately."
"We would expect therefore, that our techniques would similarly benefit from the addition of prosodic information."
"In the introduction, we mentioned the problem of sen-tence boundary detection, which is related to the punc-tuation reconstruction problem especially with regard to predicting sentence boundary punctuation such as peri-ods, question marks, and exclamation marks. (This prob-lem is distinct from the problem of sentence boundary disambiguation, where punctuation is provided, but the categorization of the punctuation as to whether or not it marks a sentence boundary is at issue[REF_CITE].)"
They argue that a linguistic segmentation is useful for improving the per-formance and utility of language models and speech rec-ognizers.
"Like the present work, they segment clean text rather than automatically transcribed speech."
"Stevenson and Gaizausk[REF_CITE]and Goto and Renals[REF_CITE]address the sentence boundary detection problem directly, again us-ing lexical and, in the latter, prosodic cues."
The experiments reported here — like much of the previ-ous work in comma restorati[REF_CITE]and sentence boundary disambiguation and restorati[REF_CITE](though not all[REF_CITE]) — assume an ideal ref-erence transcription of the text.
The performance of the method on automatically transcribed speech with its con-comitant error remains to be determined.
A hopeful sign is the work of Kim and Woodland[REF_CITE]on punctuation reconstruction using prosodic in-formation.
The performance of their system drops from an F-measure of 78% on reference transcriptions to 44% on automatically transcribed speech at a word error rate of some 20%.
"Nonetheless, prosodic features were still useful in improving the reconstructed punctuation even in the automatically transcribed case."
The simple HMM model that we inherit from earlier work dramatically limits the features of the parse that we can easily appeal to in predicting comma locations.
"Many alternatives suggest themselves to expand the options, including maximum entropy models, which have been previously successfully applied to, inter alia, sentence boundary detecti[REF_CITE], and transformation-based learning, as used in part-of-speech tagging and statistical parsing applications[REF_CITE]."
"In addition, all of the methods above are essentially nonhierarchical, based as they are on HMMs."
"An alter-native approach would use the statistical parsing model itself as a model of comma placement, that is, to select the comma placement for a string such that the resulting reconstructed string has maximum likelihood under the statistical parsing model."
"This approach has the benefit that the ramifications of comma placement on all aspects of the syntactic structure are explored, but the disadvan-tage that the longer distance lexical relationships found in a trigram model are eliminated."
"Nonetheless, even under these severe constraints and using quite simple features distilled from the parse, we can reduce sentence error by 20%, with the potential of another 8% if statistical parsers were to approach Tree-bank quality."
"As such, comma restoration may stand as the first end-user application that benefits from statisti-cal parsing technology smoothly and incrementally."
"Fi-nally, our methods use features that are orthogonal to the prosodic features that other researchers have explored."
They therefore have the potential to combine well with prosodic methods to achieve further improvements.
We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.
The models use syntactic and lexical features.
A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-of-the-art decision-based discourse parser.
A set of empirical evaluations shows that our dis-course parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.
"By exploiting information encoded in human-produced syntactic trees[REF_CITE], research on prob-abilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy[REF_CITE]."
The absence of semantic and discourse annotated corpora prevented similar developments in se-mantic/discourse parsing.
"Fortunately, recent annotation projects have taken significant steps towards developing semantic[REF_CITE]and discourse[REF_CITE]annotated cor-pora."
Some of these annotation efforts have already had a computational impact.
"For example,[REF_CITE]developed statistical models for automatically inducing semantic roles."
"In this paper, we describe proba-bilistic models and algorithms that exploit the discourse-annotated corpus produced[REF_CITE]."
"A discourse structure is a tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans (called dis-course spans)."
An example of a discourse structure is the tree given in Figure 1.
"Each internal node in a dis-course tree is characterized by a rhetorical relation, such as ATTRIBUTION and ENABLEMENT ."
Within a rhetorical re-lation a discourse span is also labeled as either NUCLEUS or SATELLITE .
The distinction between nuclei and satel-lites comes from the empirical observation that a nucleus expresses what is more essential to the writer’s purpose than a satellite.
Discourse trees can be represented graph-ically in the style shown in Figure 1.
The arrows link the satellite to the nucleus of a rhetorical relation.
Arrows are labeled with the name of the rhetorical relation that holds between the linked units.
"Horizontal lines correspond to text spans, and vertical lines identify text spans which are nuclei."
"In this paper, we introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees."
We show how syntactic and lexical information can be exploited in the process of identifying elementary units of discourse and building sentence-level discourse trees.
"Our evalu-ation indicates that the discourse parsing model we pro-pose is sophisticated enough to achieve near-human lev-els of performance on the task of deriving sentence-level discourse trees, when working with human-produced syntactic trees and discourse segments."
"For the experiments described in this paper, we use a pub-licly available corpus[REF_CITE]that contains 385"
Wall Street Journal articles from the Penn Treebank.
The corpus comes conveniently partitioned into a Training set of 347 articles (6132 sentences) and a Test set of 38 ar-ticles (991 sentences).
Each document in the corpus is paired with a discourse structure (tree) that was manually built in the style of Rhetorical Structure Theory[REF_CITE]. (See[REF_CITE]for de-tails concerning the corpus and the annotation process.)
"Out of the 385 articles in the corpus, 53 have been inde-pendently annotated by two human annotators."
We used this doubly-annotated subset to compute human agree-ment on the task of discourse structure derivation.
In our experiments we used as discourse structures only the dis-course sub-trees spanning over individual sentences.
"Because the discourse structures had been built on top of sentences already associated with syntactic trees from the Penn Treebank, we were able to create a composite corpus which allowed us to perform an empirically driven syntax-discourse relationship study."
This composite cor-pus was created by associating each sentence in the dis-course corpus with its corresponding Penn Treebank syn-tactic parse tree    and its correspond-ing sentence-level discourse tree    .
"Al-though human annotators were free to build their dis-course structures without enforcing the existence of well-formed discourse sub-trees for each sentence, in about 95% of the cases in the[REF_CITE]corpus, there exists a discourse sub-tree ! &quot;# associated with each sentence ."
"The remaining 5% of the sentences cannot be used in our approach, as no well-formed dis-course tree can be associated with these sentences."
"Therefore, our Training section consists of a set of 5809 triples of the form $ !&amp;%   #%(&quot; #() which are used to train the parameters of the statistical models."
"Our Test section consists of a set of 946 triples of a similar form, which are used to evaluate the perfor-mance of our discourse parser."
The[REF_CITE]corpus uses 110 different rhetori-cal relations.
"We found it useful to also compact these re-lations into classes, as described[REF_CITE], and operate with the resulting 18 labels as well (seen as coarser granularity rhetorical relations)."
"Operating with different levels of granularity allows one to get deeper insight into the difficulties of assigning the appropriate rhetorical relation, if any, to two adjacent text spans."
We break down the problem of building sentence-level discourse trees into two sub-problems: discourse seg-mentation and discourse parsing.
"Discourse segmenta-tion is covered by this section, while discourse parsing is covered by Section 4."
Discourse segmentation is the process in which a given text is broken into non-overlapping segments called ele-mentary discourse units (edus).
"In the present work, ele-mentary discourse units are taken to be clauses or clause-like units that are unequivocally the NUCLEUS or SATEL - LITE of a rhetorical relation that holds between two adja-cent spans of text (see[REF_CITE]for details)."
Our approach to discourse segmentation breaks the prob-lem further into two sub-problems: sentence segmen-tation and sentence-level discourse segmentation.
"The problem of sentence segmentation has been studied ex-tensively, and tools such as those described[REF_CITE]and[REF_CITE]can handle it well."
"In this section, we present a discourse segmenta-tion algorithm that deals with segmenting sentences into elementary discourse units."
The discourse segmenter proposed here takes as input a sentence and outputs its elementary discourse unit bound-aries.
"Our statistical approach to sentence segmentation uses two components: a statistical model which assigns a probability to the insertion of a discourse boundary af-ter each word in a sentence, and a segmenter, which uses the probabilities computed by the model for inserting dis-course boundaries."
We first focus on the statistical model.
A good model of discourse segmentation needs to ac-count both for local interactions at the word level and for global interactions at more abstract levels.
"Consider, for example, the syntactic tree in Figure 2."
"According to our hypothesis, the discourse boundary inserted be-tween the words says and it is best explained not by the words alone, but by the lexicalized syntactic structure [VP(says) [VBZ(says) * SBAR(will)]], sig-naled by the boxed nodes in Figure 2."
"Hence, we hy-pothesize that the discourse boundary in our example is best explained by the global interaction between the verb (the act of saying) and its clausal complement (what is being said)."
"Given a sentence ,.+ -&apos;/   , we first find the syntactic parse tree of ."
We used in our exper-iments both syntactic parse trees obtained using Char-niak’s parser (2000) and syntactic parse trees from the PennTree bank.
"Our statistical model assigns a segment-ing probability 9:; (5 &lt; - 5 % for each word - 5 , where ; &gt;5 &gt;= ? boundary, no-boundary @ ."
"Because our model is concerned with discourse segmentation at sentence level, we define 9: boundary &lt; - 8 %(+CB , i.e., the sentence boundary is always a discourse boundary as well."
Our model uses both lexical and syntactic features for determining the probability of inserting discourse boundaries.
We apply canonical lexical head projection rules[REF_CITE]in order to lexicalize syntactic trees.
"For each word - , the upper-most node with lex-ical head - which has a right sibling node determines the features on the basis of which we decide whether to insert a discourse boundary."
"We denote such node DFE , and the features we use are node DGE , its parent DIH , and the siblings of DJE ."
"In the example in Figure 2, we de-termine whether to insert a discourse boundary after the word says using as features node DKHA+ P!!Q &quot;R !"
P and its children D E + &quot;!&quot;P and DJWX+ZY!T\[ ]_^  .
"We use our corpus to estimate the likelihood of inserting a discourse boundary between word - and the next word using formula (1), 9:#; &lt;-K(%  #[REF_CITE](DhEJ* W 24242&amp; (1) d #D H  E DhW&gt;&amp; where the numerator represents all the counts of the rule D H (D E[REF_CITE]for which a discourse boundary has been inserted after word - , and the denominator repre-sents all the counts of the rule."
"Because we want to account for boundaries that are motivated lexically as well, the counts used in formula (1) are defined over lexicalized rules."
"Without lexicalization, the syntactic context alone is too general and fails to dis-tinguish genuine cases of discourse boundaries from in-correct ones."
"As can be seen in Figure 3, the same syn-tactic context may indicate a discourse boundary when the lexical heads passed and without are present, but it may not indicate a boundary when the lexical heads priced and at are present."
The discourse segmentation model uses the corpus pre-sented in Section 2 in order to estimate probabilities for inserting discourse boundaries using equation (1).
We also use a simple interpolation method for smoothing lex-icalized rules to accommodate data sparseness.
"Once we have the segmenting probabilities given by the statistical model, a straightforward algorithm is used to implement the segmenter."
"Given a syntactic tree , the algorithm inserts a boundary after each word - for which : boundary &lt; -K% . 9"
"In the setting presented here, the input to the discourse parser is a Discourse Segmented Lexicalized Syntactic Tree (i.e., a lexicalized syntactic parse tree in which the discourse boundaries have been identified), henceforth called a DS-LST."
An example of a DS-LST in the tree in Figure 2.
"The output of the discourse parser is a dis-course parse tree, such as the one presented in Figure 1."
"As in other statistical approaches, we identify two components that perform the discourse parsing task."
"The first component is the parsing model, which assigns a probability to every potential candidate parse tree."
"For-mally, given a discourse tree  and a set of parameters r , the parsing r model estimates the conditional probabil-ity 9: &lt; ."
The most likely parse is then given by formula (2). :p  xwy3+.{| } 39~ :: &lt; r (2)
"The second component is called the discourse parser, and it is an algorithm for finding p: twv xwy ."
We first focus on the parsing model.
A discourse parse tree can be formally represented as a set of tuples.
"The discourse tree in Figure 1, for example, can be formally written as the set of tuples ?"
"ATTRIBUTION - SN [1,1,3] % ENABLEMENT - NS [2,2,3] @ ."
"A tu-ple is of the form F &amp;%({%w , and denotes a discourse rela-tion  that holds between the discourse span that contains edus through { , and the discourse span that contains edus { :B through  ."
"Each relation  also signals explic-itly the nuclearity assignment, which can be NUCLEUS - SATELLITE ( NS ) , SATELLITE - NUCLEUS ( SN ) , or NUCLEUS - NUCLEUS ( NN ) ."
This notation assumes that all relations  are binary relations.
The assumption is justified empiri-cally: 99% of the nodes of the discourse trees in our cor-pus are binary nodes.
Using only binary relations makes our discourse model easier to build and reason with.
In what follows we make use of two functions: func-tion  applied to a tuple F &amp;%7{ %! yields the discourse relation  ; function  applied to a tuple F &amp;7% { %! yields the structure  &amp;%7{%! .
"Given a set of adequate parameters r , our discourse model estimates the goodness of a dis-course parse tree :p using formula (3). :9  &lt; r + 9  &lt; r !( &lt; r (3) &gt; ( ~3"
"For each tuple = p: , the probability 9x estimates the goodness of the structure of ."
"We expect these proba-bilities to prefer the hierarchical structure (1, (2, 3)) over ((1,2), 3) for the discourse tree in Figure 1."
"For each tu-ple =  , the probability 9 W estimates the goodness of the discourse relation of ."
We expect these probabili-ties to prefer the rhetorical relation ATTRIBUTION - NS over CONTRAST - NN for the relation between spans 1 and  6%( in the discourse tree in Figure 1.
The overall probability of a discourse tree is obtained multiplying the structural probabilities &gt;9 x and the relational probabilities &gt;9 W for all the tuples in the discourse tree. r
Our discourse model uses as the information present in the input DS-LST.
"However, given such a tree &gt; as input, one cannot estimate probabilities such as :9  &lt; &gt; , without running into a severe sparseness problem."
"To overcome this, we map the input DS-LST into a more abstract representation that contains only the salient features of the DS-LST."
This mapping leads to the notion of a dominance set over a discourse segmented lexicalized syntactic tree.
"In what follows, we define this notion and show that it provides adequate parameteriza-tion for the discourse parsing problem."
The dominance set of a DS-LST contains feature repre-sentations of a discourse segmented lexicalized syntactic tree.
Each feature is a representation of the syntactic and lexical information that is found at the point where two edus are joined together in a DS-LST.
Our hypothesis is that such “attachment” points in the structure of a DS-LST (the boxed nodes in the tree in Figure 4) carry the most indicative information with respect to the potential discourse tree we want to build.
A set representation of the “attachment” points of a DS-LST is called the domi-nance set of a DS-LST.
For each edu  we identify a word - in  as the head word of edu  and denote it  .  is defined as the word with the highest occurrence as a lexical head in the lexi-calized tree among all the words in  .
The node in which  occurs highest is called the head node of edu  and is denoted DJ .
The edu which has as head node the root of the DS-LST is called the exception edu.
"In our example, the head word for edu 2 is Zm+ ^ ! , and its head node is D  Y+ T\[ ]^ ! ; the head word for edu 3 is 6+ , and its head node is D  MY+ &quot;\ ."
The exception edu is edu 1.
"For each edu  which is not the exception edu, there exists a node which is the parent of the head node of  , and the lexical head of this node is guaranteed to belong to a different edu than  , call it  ."
We call this node the attachment node of  and denote it DG .
"In our example, the attachment node of edu 2 is DG+ !&quot;P , and its lexical head says belongs to edu 1; the attachment node of edu 3 is DJ¡+ ! , and its lexical head use belongs to edu 2."
We write formally that two edus  and  are linked through a head node DG and an attachment node D  as :¥D%  #§%(D  .
The dominance set of a DS-LST is given by all the edu pairs linked through a head node and an attachment node in the DS-LST.
Each element in the dominance set represents a dominance relationship between the edus in-volved.
Figure 4 shows the dominance set p for our ex-ample DS-LST.
"We say that edu 2 is dominated by edu 1 (shortly written : ¦¨B ), and edu 3 is dominated by edu 2 ( J¦© )."
Our discourse parsing model uses the dominance r in setequa- p of a DS-LST as the conditioning parameter tion (3).
The discourse parsing model we propose uses the dominance set p to compute the probability of a dis-course parse tree p: according to formula (4). 9:: &lt;+  9 x  &lt;¬   x  % (  3~  _# &lt; ¬ _  # %( (4)
Different projections of p are used to accurately estimate the structure probabilities &gt;9 x and the relation probabili-ties 9­W associated with a tuple in a discourse tree.
"The projection functions ¬ _  and ¬ _  ensure that, for each tuple =  , only the information in p relevant to is to be conditioned upon."
"In the case of 93x (the prob-ability of the structure  &amp;%({ %w ), we filter out the lexical heads and keep only the syntactic labels; also, we filter out all the elements of p which do not have at least one edu inside the span of ."
"In our running example, for in-stance, for &apos;+ ENABLEMENT - NS  \ ¥6% (%  , ¬ _  x  %+ ? \%&amp;®h¯, % % % #6% @ ."
"The span of is  \% , and set p has two elements involving edus from it, namely the dominance relationships ¡¦³B and F¦m ."
"To decide the appropriate structure, ¬ _  keeps them both; this is because a different dominance relation-ship between edus 1 and 2, namely B¦e , would most likely influence the structure probability of ."
"In the case of 9W (the probability of the relation  ), we keep both the lexical heads and the syntactic la-bels, but filter out the edu identifiers (clearly, the rela-tion between two spans does not depend on the posi-tions of the spans involved); also, we filter out all the elements of p whose dominance relationship does not hold across the two sub-spans of ."
"In our running ex-ample, for ,+ ENABLEMENT - NS  6%¥\% , ¬ _ ! %+ ? _ :&amp;@ ."
"The two sub-spans of are  6% and  6% , and only the dominance relationship ¨¦µ holds across these spans; the other dominance relation-ship in p , ¶¦³B , does not influence the choice for the relation label of ."
The conditional probabilities involved in equation (4) are estimated from the training corpus using maximum likelihood estimation.
A simple interpolation method is used for smoothing to accommodate data sparseness.
The counts for the dependency sets are also smoothed using symbolic names for the edu identifiers and accounting only for the distance between them.
Our discourse parser implements a classical bottom-up algorithm.
The parser searches through the space of all legal discourse parse trees and uses a dynamic pro-gramming algorithm.
"If two constituents are derived for the same discourse span, then the constituent for which the model assigns a lower probability can be safely dis-carded."
Figure 5 shows a discourse structure created in a bottom-up manner for the DS-LST in Figure 2.
"Tu-ple ENABLEMENT - NS [2,2,3] has a score of 0.40, obtained as the product between the structure probability 9 x of 0.47 and the relation probability 9 W of 0.88."
"Tuple ATTRIBUTION - SN [1,1,3] has a score of 0.37 for the struc-ture, and a score of 0.009 for the relation."
The final score for the entire discourse structure is 0.001.
All probabil-ities used were estimated from our training corpus.
"Ac-cording to our discourse model, the discourse structure in Figure 5 is the most likely among all the legal discourse structures for our example sentence."
In this section we present the evaluations carried out for both the discourse segmentation task and the discourse parsing task.
"For this evaluation, we re-trained Char-niak’s parser (2000) such that the test sentences from the discourse corpus were not seen by the syntactic parser during training."
"We train our discourse segmenter on the Training sec-tion of the corpus described in Section 2, and test it on the Test section."
The training regime uses syntactic trees from the Penn Treebank.
The metric we use to evalu-ate the discourse segmenter records the accuracy of the discourse segmenter with respect to its ability to insert inside-sentence discourse boundaries.
"That is, if a sen-tence has 3 edus, which correspond to 2 inside-sentence discourse boundaries, we measure the ability of our al-gorithm to correctly identify these 2 boundaries."
"We re-port our evaluation results using recall, precision, and F-score figures."
"This metric is harsher than the metric pre-viously used[REF_CITE], who assesses the perfor-mance of a discourse segmentation algorithm by count-ing how often the algorithm makes boundary and no-boundary decisions for every word in a sentence."
We compare the performance of our probabilistic dis-course segmenter with the performance of the decision-based segmenter proposed[REF_CITE]and the per-formance of two baseline algorithms.
"The first base-line (  ) uses punctuation to determine when to in-sert a boundary; because commas are often used to in-dicate breaks inside long sentences,  inserts dis-course boundaries after each comma."
"The second base-line ( ®G pª ) uses syntactic information; because long sentences often have embedded sentences, ®G pª in-serts discourse boundaries after each text span whose corresponding syntactic subtree is labeled S, SBAR, or SINV."
"We also compute the agreement between human annotators on the discourse segmentation task ( pª ), using the doubly-annotated discourse corpus mentioned in Section 2."
"Table 1 shows the results obtained by the algorithm described in this paper ( &gt; _ ) using syntactic trees produced by Charniak’s parser (2000), in com-parison with the results obtained by the algorithm de-scribed[REF_CITE](  ), and baseline algo-rithms  and ®J!pª , on the same test set."
"Cru-cial to the performance of the discourse segmenter is the recall figure, because we want to find as many dis-course boundaries as possible."
The baseline algorithms are too simplistic to yield good results (recall figures of 28.2% and 25.4%).
The algorithm presented in this pa-per gives an error reduction in missed discourse bound-aries of 24.5% (recall accuracy improvement from 77.1% to 82.7%)[REF_CITE].
The overall error reduc-tion is of 15.1% (improvement in F-score from 80.1% to 83.1%).
"In order to asses the impact on the performance of the discourse segmenter due to incorrect syntactic parse trees, we also carry an evaluation using syntactic trees from the Penn Treebank."
The results are shown in row &gt;&gt; .
Perfect syntactic trees lead to a further er-  ror reduction of 9.5% (F-score improvement from 83.1% to 84.7%).
The performance ceiling for discourse seg-mentation is given by the human annotation agreement F-score of 98.3%.
"We train our discourse parsing model on the Training sec-tion of the corpus described in Section 2, and test it on the Test section."
The training regime uses syntactic trees from the Penn Treebank.
The performance is assessed us-ing labeled recall and labeled precision as defined by the standard Parseval metric[REF_CITE].
"As men-tioned in Section 2, we use both 18 labels and 110 la-bels for the discourse relations."
The recall and precision figures are combined into an F-score figure in the usual manner.
The discourse parsing model uses syntactic trees pro-duced by Charniak’s parser (2000) and discourse seg-ments produced by the algorithm described in Section 3.
"We compare the performance of our model (  ) with the performance of the decision-based discourse parsing model (  ) proposed[REF_CITE], and with the performance of a baseline algorithm ( ®hpq9 )."
"The baseline algorithm builds right-branching discourse trees labeled with the most frequent relation encountered in the training set (i.e., ELABORATION - NS )."
"We also com-pute the agreement between human annotators on the dis-course parsing task ( Xpq9 ), using the doubly-annotated discourse corpus mentioned in Section 2."
The results are shown in Table 2.
"The baseline algorithm has a perfor-mance of 23.4% and 20.7% F-score, when using 18 la-bels and 110 labels, respectively."
"Our algorithm has a performance of 49.0% and 45.6% F-score, when using 18 labels and 110 labels, respectively."
"These results rep-resent an error reduction of 18.8% (F-score improvement from 37.2% to 49.0%) over a state-of-the-art discourse parser[REF_CITE]when using 18 labels, and an error reduction of 15.7% (F-score improvement from 35.5% to 45.6%) when using 110 labels."
"The performance ceiling for sentence-level discourse structure derivation is given by the human annotation agreement F-score of 77.0% and 71.9%, when using 18 labels and 110 labels, respectively."
"The performance gap between the results of &gt;  and human agreement is still large, and it can be attributed to three possible causes: errors made by the syntactic parser, errors made by the discourse segmenter, and the weakness of our discourse model."
"In order to quantitatively asses the impact in perfor-mance of each possible cause of error, we perform further experiments."
We replace the syntactic parse trees pro-duced by Charniak’s parser at 90% accuracy ( · ) with the corresponding Penn Treebank syntactic parse trees produced by human annotators (  ).
"We also replace the discourse boundaries produced by our discourse seg-menter at 83% accuracy ( §· ) with the discourse bound-aries taken[REF_CITE], which are produced by the human annotators ( 3¸ )."
The results are shown in Table 3.
"The results in col-umn  show that using perfect syntactic trees leads to an error reduction of 14.5% (F-score improvement from 49.0% to 56.4%) when using 18 labels, and an error reduction of 12.9% (F-score improvement from 45.6% to 52.6%) when using 110 labels."
The results in col-umn ·  ¸ show that the impact of perfect discourse segmentation is double the impact of perfect syntactic trees.
"Human-level performance on discourse segmen-tation leads to an error reduction of 29.0% (F-score im-provement from 49.0% to 63.8%) when using 18 labels, and an error reduction of 25.6% (F-score improvement from 45.6% to 59.5%) when using 110 labels."
"Together, perfect syntactic trees and perfect discourse segmentation lead to an error reduction of 52.0% (F-score improvement from 49.0% to 75.5%) when using 18 labels, and an error reduction of 45.5% (F-score improvement from 45.6% to 70.3%) when using 110 labels."
The results in column  in Table 3 compare extremely favorable with the results in column Xpq9 in Table 2.
The discourse parsing model produces unlabeled discourse structure at a per-formance level similar to human annotators (F-score of 96.2%).
Our evaluation shows that our discourse model is sophisticated enough to match near-human levels of performance.
"In this paper, we have introduced a discourse parsing model that uses syntactic and lexical features to estimate the adequacy of sentence-level discourse structures."
Our model defines and exploits a set of syntactically moti-vated lexico-grammatical dominance relations that fall naturally from a syntactic representation of sentences.
The most interesting finding is that these dominance relations encode sufficient information to enable the derivation of discourse structures that are almost indis-tinguishable from those built by human annotators.
"Our experiments empirically show that, at the sentence level, there is an extremely strong correlation between syntax and discourse."
This is even more remarkable given that the discourse corpus[REF_CITE]was built with no syntactic theory in mind.
The annotators used[REF_CITE]were not instructed to build discourse trees that were consistent with the syntax of the sentences.
"Yet, they built discourse structures at sentence level that are not only consistent with the syntactic structures of sen-tences, but also derivable from them."
"Recent work on Tree Adjoining Grammar-based lexi-calized models of discourse[REF_CITE]has al-ready shown how to exploit within a single framework lexical, syntactic, and discourse cues."
Various linguis-tics studies have also shown how intertwined syntax and discourse are[REF_CITE].
"However, to our knowl-edge, this is the first paper that empirically shows that the connection between syntax and discourse can be compu-tationally exploited at high levels of accuracy on open domain, newspaper text."
Another interesting finding is that the performance of current state-of-the-art syntactic parsers[REF_CITE]is not a bottleneck for coming up with a good solution to the sentence-level discourse parsing problem.
Little improvement comes from using manually built syntactic parse trees instead of automatically derived trees.
"How-ever, experiments show that there is much to be gained if better discourse segmentation algorithms are found; 83% accuracy on this task is not sufficient for building highly accurate discourse trees."
We believe that semantic/discourse segmentation is a notoriously under-researched problem.
"For example,[REF_CITE]present a semantic parser that optimistically assumes that has access to perfect seman-tic segments."
Our results suggest that more effort needs to be put on semantic/discourse-based segmentation.
Im-provements in this area will have a significant impact on both semantic and discourse parsing.
This paper investigates bootstrapping for statis-tical parsers to reduce their reliance on manu-ally annotated training data.
"We consider both a mostly-unsupervised approach, co-training, in which two parsers are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data."
The selection of labeled training examples is an integral part of both frameworks.
We propose several selection methods based on the criteria of minimizing er-rors in the data and maximizing training util-ity.
We show that incorporating the utility cri-terion into the selection method results in better parsers for both frameworks.
Current state-of-the-art statistical parsers[REF_CITE]are trained on large annotated corpora such as the Penn Treebank[REF_CITE].
"How-ever, the production of such corpora is expensive and labor-intensive."
"Given this bottleneck, there is consider-able interest in (partially) automating the annotation pro-cess."
"To overcome this bottleneck, two approaches from ma-chine learning have been applied to training parsers."
"One is sample selecti[REF_CITE], a variant of active learning[REF_CITE], which tries to identify a small set of unlabeled sen- tences with high training utility for the human to label [Footnote_1] ."
"1 In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably."
Sentences with high training utility are those most likely to improve the parser.
"The other approach, and the fo-cus of this paper, is co-training[REF_CITE], a mostly-unsupervised algorithm that replaces the human by hav-ing two (or more) parsers label training examples for each other."
The goal is for both parsers to improve by boot-strapping off each other’s strengths.
"Because the parsers may label examples incorrectly, only a subset of their out-put, chosen by some selection mechanism, is used in or-der to minimize errors."
The choice of selection method significantly affects the quality of the resulting parsers.
We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training utility from sample selection.
"The selection mechanism is integral to both sample selection and co-training; however, because co-training and sam-ple selection have different goals, their selection methods focus on different criteria: co-training typically favors se-lecting accurately labeled examples, while sample selec-tion typically favors selecting examples with high train-ing utility, which often are not sentences that the parsers already label accurately."
"In this work, we investigate se-lection methods for co-training that explore the trade-off between maximizing training utility and minimizing er-rors."
"Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training[REF_CITE], in which the selected examples are man-ually checked and corrected before being added to the training data."
"For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain."
"For corrected co-training, we show that selecting examples with high training util-ity reduces the number of sentences the human annotator has to check."
"For both frameworks, we show that selec-tion methods that maximize training utility find labeled examples that result in better trained parsers than those that only minimize error."
The two classifiers are initially trained on a small amount of annotated seed data; then they label unannotated data for each other in an iterative training process.
"Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can boost an initial weak learner using unlabeled data."
"The theory underlying co-training has been extended[REF_CITE]to prove that, by maximizing their agreement over the unlabeled data, the two learn-ers make few generalization errors (under the same in-dependence assumption adopted by Blum and Mitchell)."
In this paper we investi-gate methods for selecting labeled examples produced by two statistical parsers.
We do not explicitly maximize agreement (along the lines of Abney’s algorithm (2002)) because it is too computationally intensive for training parsers.
The pseudocode for our co-training framework is given in Figure 1.
It consists of two different parsers and a cen-tral control that interfaces between the two parsers and the data.
"At each co-training iteration, a small set of sen-tences is drawn from a large pool of unlabeled sentences and stored in a cache."
Both parsers then attempt to label every sentence in the cache.
"Next, a subset of the newly labeled sentences is selected to be added to the train-ing data."
"The examples added to the training set of one parser (referred to as the student) are only those produced by the other parser (referred to as the teacher), although the methods we use generalize to the case in which the parsers share a single training set."
"During selection, one parser first acts as the teacher and the other as the student, and then the roles are reversed."
"In each iteration, selection is performed in two steps."
"First, each parser uses some scoring function, f, to assess the parses it generated for the sentences in the cache. [Footnote_2] Second, the central control uses some selection method, S, to choose a subset of these labeled sentences (based on the scores assigned by f) to add to the parsers’ training data."
"2 In our experiments, both parsers use the same scoring func-tion."
"The focus of this paper is on the selection phase, but to more fully investigate the effect of different selection methods we also consider two possible scoring functions."
The scoring function attempts to quantify the correctness of the parses produced by each parser.
"An ideal scor-ing function would give the true accuracy rates (e.g., F-score, the combined labeled precision and recall rates)."
"In practice, accuracy is approximated by some notion of confidence."
"For example, one easy-to-compute scor-ing function measures the conditional probability of the (most likely) parse."
"If a high probability is assigned, the parser is said to be confident in the label it produced."
"In our experimental studies, we considered the selec-tion methods’ interaction with two scoring functions: an oracle scoring function f F-score that returns the F-score of the parse as measured against a gold standard, and a practical scoring function f prob that returns the condi-tional probability of the parse. 3"
"Based on the scores assigned by the scoring function, the selection method chooses a subset of the parser la-beled sentences that best satisfy some selection criteria."
"One such criterion is the accuracy of the labeled exam-ples, which may be estimated by the teacher parser’s con-fidence in its labels."
"However, the examples that the teacher correctly labeled may not be those that the stu-dent needs."
We hypothesize that the training utility of the examples for the student parser is another important criterion.
Training utility measures the improvement a parser would make if that sentence were correctly labeled and added to the training set.
"Like accuracy, the utility of an unlabeled sentence is difficult to quantify; therefore, we approximate it with values that can be computed from features of the sentence."
"For example, sentences contain-ing many unknown words may have high training util-ity; so might sentences that a parser has trouble parsing."
"Under the co-training framework, we estimate the train-ing utility of a sentence for the student by comparing the score the student assigned to its parse (according to its scoring function) against the score the teacher assigned to its own parse."
"To investigate how the selection criteria of utility and accuracy affect the co-training process, we considered a number of selection methods that satisfy the requirements of accuracy and training utility to varying degrees."
The different selection methods are shown below.
"For each method, a sentence (as labeled by the teacher parser) is selected if: • above-n (S above-n ): the score of the teacher’s parse (using its scoring function) ≥ n. • difference (S diff-n ): the score of the teacher’s parse is greater than the score of the student’s parse by some threshold n. • intersection (S int-n ): the score of the teacher’s parse is in the set of the teacher’s n percent highest-scoring labeled sentences, and the score of the stu-dent’s parse for the same sentence is in the set of the student’s n percent lowest-scoring labeled sen-tences."
"Each selection method has a control parameter, n, that determines the number of labeled sentences to add at each co-training iteration."
It also serves as an indirect control of the number of errors added to the training set.
"For ex-ample, the S above-n method would allow more sentences to be selected if n was set to a low value (with respect to the scoring function); however, this is likely to reduce the accuracy rate of the training set."
The above-n method attempts to maximize the accu-racy of the data (assuming that parses with higher scores are more accurate).
"The difference method attempts to maximize training utility: as long as the teacher’s label-ing is more accurate than that of the student, it is cho-sen, even if its absolute accuracy rate is low."
The inter-section method attempts to maximize both: the selected sentences are accurately labeled by the teacher and incor-rectly labeled by the student.
Experiments were performed to compare the effect of the selection methods on co-training and corrected co-training.
"We consider a selection method, S 1 , superior to another, S 2 , if, when a large unlabeled pool of sen-tences has been exhausted, the examples selected by S 1 (as labeled by the machine, and possibly corrected by the human) improve the parser more than those selected by S 2 ."
"All experiments shared the same general setup, as described below."
"For two parsers to co-train, they should generate com-parable output but use independent statistical models."
"In our experiments, we used a lexicalized context free grammar parser developed[REF_CITE], and a lex-icalized Tree Adjoining Grammar parser developed[REF_CITE]."
Both parsers were initialized with some seed data.
"Since the goal is to minimize human annotated data, the size of the seed data should be small."
"In this pa-per we used a seed set size of 1, 000 sentences, taken from section 2 of the Wall Street Journal (WSJ) Penn Tree-bank."
"The total pool of unlabeled sentences was the re-mainder of sections 2-21 (stripped of their annotations), consisting of about 38,000 sentences."
The cache size is set at 500 sentences.
We have explored using different settings for the seed set size[REF_CITE].
The parsers were evaluated on unseen test sentences (section 23 of the WSJ corpus).
Section 0 was used as a development set for determining parameters.
"The eval-uation metric is the Parseval F-score over labeled con-stituents: F-score = 2×LR×LPLR+LP , where LP and LR are labeled precision and recall rate, respectively."
"Both parsers were evaluated, but for brevity, all results reported here are for the Collins parser, which received higher Par-seval scores."
"We first examine the effect of the three selection meth-ods on co-training without correction (i.e., the chosen machine-labeled training examples may contain errors)."
"Because the selection decisions are based on the scores that the parsers assign to their outputs, the reliability of the scoring function has a significant impact on the per-formance of the selection methods."
We evaluate the ef-fectiveness of the selection methods using two scoring functions.
"In Section 4.2.1, each parser assesses its out-put with an oracle scoring function that returns the Par-seval F-score of the output (as compared to the human annotated gold-standard)."
This is an idealized condition that gives us direct control over the error rate of the la-beled training data.
"By keeping the error rates constant, our goal is to determine which selection method is more successful in finding sentences with high training utility."
"In Section 4.2.2 we replace the oracle scoring function with f prob , which returns the conditional probability of the best parse as the score."
We compare how the selection methods’ performances degrade under the realistic con-dition of basing selection decisions on unreliable parser output assessment scores.
The goal of this experiment is to evaluate the selection methods using a reliable scoring function.
"We therefore use an oracle scoring function, f F-score , which guaran-tees a perfect assessment of the parser’s output."
"This, however, may be too powerful."
"In practice, we expect even a reliable scoring function to sometimes assign high scores to inaccurate parses."
"We account for this effect by adjusting the selection method’s control parameter to af-fect two factors: the accuracy rate of the newly labeled training data, and the number of labeled sentences added at each training iteration."
"A relaxed parameter setting adds more parses to the training data, but also reduces the accuracy of the training data."
Figure 2 compares the effect of the three selection methods on co-training for the relaxed (left graph) and the strict (right graph) parameter settings.
Each curve in the two graphs charts the improvement in the parser’s ac-curacy in parsing the test sentences (y-axis) as it is trained on more data chosen by its selection method (x-axis).
The curves have different endpoints because the selection methods chose a different number of sentences from the same 38K unlabeled pool.
"For reference, we also plotted the improvement of a fully-supervised parser (i.e., trained on human-annotated data, with no selection)."
"For the more relaxed setting, the parameters are chosen so that the newly labeled training data have an average accuracy rate of about 85%: • S above-70% requires the labels to have an F-score ≥ 70%."
It adds about 330 labeled sentences (out of the 500 sentence cache) with an average accuracy rate of 85% to the training data per iteration. • S diff-10% requires the score difference between the teacher’s labeling and the student’s labeling to be at least 10%.
It adds about 50 labeled sentences with an average accuracy rate of 80%. • S int-60% requires the teacher’s parse to be in the top 60% of its output and the student’s parse for the same sentence to be in its bottom 60%.
It adds about 150 labeled sentences with an average accuracy rate of 85%.
"Although none rivals the parser trained on human an-notated data, the selection method that improves the parser the most is S diff-10% ."
One interpretation is that the training utility of the examples chosen by S diff-10% outweighs the cost of errors introduced into the training data.
Another interpretation is that the other two selection methods let in too many sentences containing errors.
"In the right graph, we compare the same S diff-10% with the other two selection methods using stricter control, such that the average accuracy rate for these methods is now about 95%: • S above-90% now requires the parses to be at least 90% correct."
It adds about 150 labeled sentences per iteration. • S int-30% now requires the teacher’s parse to be in the top 30% of its output and the student’s parse for the same sentence in its bottom 30%.
It adds about 15 labeled sentences.
"The stricter control on S above-90% improved the parser’s performance, but not enough to overtake S diff-10% after all the sentences in the unlabeled pool had been considered, even though the training data of S diff-10% contained many more errors."
"S int-30% has a faster initial improvement [Footnote_4] , closely tracking the progress of the fully-supervised parser."
"4 A fast improvement rate is not a central concern here, but it will be more relevant for corrected co-training."
"However, the stringent re-quirement exhausted the unlabeled data pool before train-ing the parser to convergence."
"S int-30% might continue to help the parser to improve if it had access to more un-labeled data, which is easier to acquire than annotated data [Footnote_5] ."
5 This oracle experiment is bounded by the size of the anno-tated portion of the WSJ corpus.
"Comparing the three selection methods under both strict and relaxed control settings, the results suggest that training utility is an important criterion in selecting train-ing examples, even at the cost of reduced accuracy."
"To determine the effect of unreliable scores on the se-lection methods, we replace the oracle scoring function, f F-score , with f prob , which approximates the accuracy of a parse with its conditional probability."
"Although this is a poor estimate of accuracy (especially when computed from a partially trained parser), it is very easy to compute."
The unreliable scores also reduce the correlation between the selection control parameters and the level of errors in the training data.
"In this experiment, we set the parame-ters for all three selection methods so that approximately 30-50 sentences were added to the training data per iter-ation."
"The average accuracy rate of the training data for S above-70% was about 85%, and the rate for S diff-30% and S int-30% was about 75%."
"As expected, the parser performances of all three selec-tion methods using f prob (shown in Figure 3) are lower than using f F-score (see Figure 2)."
"However, S diff-30% and S int-30% helped the co-training parsers to improve with a 5% error reduction (1% absolute difference) over the parser trained only on the initial seed data."
"In con-trast, despite an initial improvement, using S above-70% did not help to improve the parser."
"In their experiments on NP identifiers,[REF_CITE]observed a sim-ilar effect."
They hypothesize that co-training does not scale well for natural language learning tasks that require a huge amount of training data because too many errors are accrued over time.
Our experimental results suggest that the use of training utility in the selection process can make co-training parsers more tolerant to these accumu-lated errors.
"To address the problem of the training data accumulating too many errors over time, Pierce and Cardie proposed a semi-supervised variant of co-training called corrected co-training, which allows a human annotator to review and correct the output of the parsers before adding it to the training data."
The main selection criterion in their co-training system is accuracy (approximated by confi-dence).
They argue that selecting examples with nearly correct labels would require few manual interventions from the annotator.
We hypothesize that it may be beneficial to consider the training utility criterion in this framework as well.
We perform experiments to determine whether select-ing fewer (and possibly less accurately labeled) exam- ples with higher training utility would require less effort from the annotator.
"In our experiments, we simulated the interactive sample selection process by revealing the gold standard."
"As before, we compare the three selection methods using both f F-score and f prob as scoring func-tions. [Footnote_6]"
"6 The selection control parameters are the same as the previ-ous set of experiments, using the strict setting (i.e., Figure 2(b)) for fF-score."
Figure 4 shows the effect of the three selection meth-ods (using the strict parameter setting) on corrected co-training.
"As a point of reference, we plot the improve-ment rate for a fully supervised parser (same as the one in Figure 2)."
"In addition to charting the parser’s perfor-mance in terms of the number of labeled training sen-tences (left graph), we also chart the parser’s performance in terms of the the number of constituents the machine mislabeled (right graph)."
"The pair of graphs indicates the amount of human effort required: the left graph shows the number of sentences the human has to check, and the right graph shows the number of constituents the human has to correct."
"Comparing S above-90% and S diff-10% , we see that S diff-10% trains a better parser than S above-90% when all the unlabeled sentences have been considered."
It also im-proves the parser using a smaller set of training exam-ples.
"Thus, for the same parsing performance, it requires the human to check fewer sentences than S above-90% and the reference case of no selection (Figure 4(a))."
"On the other hand, because the labeled sentences selected by S diff-10% contain more mistakes than those selected by S above-90% , S diff-10% requires slightly more corrections than S above-90% for the same level of parsing perfor-mance; though both require fewer corrections than the reference case of no selection (Figure 4(b))."
"Because the amount of effort spent by the annotator depends on the number of sentences checked as well as the amount of corrections made, whether S diff-10% or S above-90% is more effort reducing may be a matter of the annotator’s preference."
The selection method that improves the parser at the fastest rate is S int-30% .
"For the same parser performance level, it selects the fewest number of sentences for a hu-man to check and requires the human to make the least number of corrections."
"However, as we have seen in the earlier experiment, very few sentences in the unlabeled pool satisfy its stringent criteria, so it ran out of data be-fore the parser was trained to convergence."
At this point we cannot determine whether S int-30% might continue to improve the parser if we used a larger set of unlabeled data.
We also consider the effect of unreliable scores in the corrected co-training framework.
A comparison between the selection methods using f prob is reported in Figure 5.
The left graph charts parser performance in terms of the number of sentences the human must check; the right charts parser performance in terms of the number of con-stituents the human must correct.
"As expected, the unreli-able scoring function degrades the effectiveness of the se-lection methods; however, compared to its unsupervised counterpart (Figure 3), the degradation is not as severe."
"In fact, S diff-30% and S int-30% still require fewer train-ing data than the reference parser."
"Moreover, consistent with the other experiments, the selection methods that at-tempt to maximize training utility achieve better parsing performance than S above-70% ."
"Finally, in terms of reduc-ing human effort, the three selection methods require the human to correct comparable amount of parser errors for the same level of parsing performance, but for S diff-30% and S int-30% , fewer sentences need to be checked."
"Corrected co-training can be seen as a form of active learning, whose goal is to identify the smallest set of un-labeled data with high training utility for the human to label."
Active learning can be applied to a single learner[REF_CITE]and to multiple learners[REF_CITE].
"In the context of parsing, all previ-ous work[REF_CITE]has focussed on single learners."
Corrected co-training is the first application of active learning for mul-tiple parsers.
We are currently investigating comparisons to the single learner approaches.
"Our approach is similar to co-testing[REF_CITE], an active learning technique that uses two classi-fiers to find contentious examples (i.e., data for which the classifiers’ labels disagree) for a human to label."
"There is a subtle but significant difference, however, in that their goal is to reduce the total number of labeled training ex-amples whereas we also wish to reduce the number of corrections made by the human."
"Therefore, our selection methods must take into account the quality of the parse produced by the teacher in addition to how different its parse is from the one produced by the student."
The inter-section method precisely aims at selecting sentences that satisfy both requirements.
Exploring different selection methods is part of our on-going research effort.
We have considered three selection methods that have dif-ferent priorities in balancing the two (often competing) criteria of accuracy and training utility.
"We have em-pirically compared their effect on co-training, in which two parsers label data for each other, as well as corrected co-training, in which a human corrects the parser labeled data before adding it to the training set."
"Our results sug-gest that training utility is an important selection criterion to consider, even at the cost of potentially reducing the ac-curacy of the training data."
"In our empirical studies, the selection method that aims to maximize training utility, S diff-n , consistently finds better examples than the one that aims to maximize accuracy, S above-n ."
"Our results also suggest that the selection method that aims to maxi-mize both accuracy and utility, S int-n , shows promise in improving co-training parsers and in reducing human ef-fort for corrected co-training; however, a much larger un-labeled data set is needed to verify the benefit of S int-n ."
The results of this study indicate the need for scor-ing functions that are better estimates of the accuracy of the parser’s output than conditional probabilities.
"Our oracle experiments show that, by using effective selec-tion methods, the co-training process can improve parser peformance even when the newly labeled parses are not completely accurate."
This suggests that co-training may still be beneficial when using a practical scoring function that might only coarsely distinguish accurate parses from inaccurate parses.
"Further avenues to ex-plore include the development of selection methods to efficiently approximate maximizing the objective func-tion of parser agreement on unlabeled data, following the work[REF_CITE]and[REF_CITE]."
"Also, co-training might be made more effective if partial parses were used as training data."
"Finally, we are conducting ex-periments to compare corrected co-training with other ac-tive learning methods."
We hope these studies will reveal ways to combine the strengths of co-training and active learning to make better use of unlabeled data.
"Statistical measures of word similarity have ap-plication in many areas of natural language pro-cessing, such as language modeling and in-formation retrieval."
We report a comparative study of two methods for estimating word co-occurrence frequencies required by word sim-ilarity measures.
"Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures."
"We base the evaluation on one TOEFL question set and two practice questions sets, each consisting of a number of multiple choice questions seek-ing the best synonym for a given target word."
"For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this con-text."
Our best combination of similarity mea-sure and frequency estimation method answers 6-8% more questions than the best results pre-viously reported for the same question sets.
Many different statistical tests have been proposed to measure the strength of word similarity or word associ-ation in natural language texts[REF_CITE].
These tests attempt to measure dependence between words by using statistics taken from a large corpus.
"In this context, a key assump-tion is that similarity between words is a consequence of word co-occurrence, or that the closeness of the words in text is indicative of some kind of relationship between them, such as synonymy or antonymy."
"Although word sequences in natural language are un-likely to be independent, these statistical tests provide quantitative information that can be used to compare pairs of co-occurring words."
"Also, despite the fact that word co-occurrence is a simple idea, there are a vari-ety of ways to estimate word co-occurrence frequencies from text."
"Two words can appear close to each other in the same document, passage, paragraph, sentence or fixed-size window."
The boundaries for determining co-occurrence will affect the estimates and as a consequence the word similarity measures.
"Statistical word similarity measures play an impor-tant role in information retrieval and in many other natu-ral language applications, such as the automatic creation of thesauri[REF_CITE]and word sense disambiguati[REF_CITE]."
"Recently,[REF_CITE]provide an analysis on different measures of independence in the context of association rules."
Word similarity is also used in language modeling ap-plications.
"In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback[REF_CITE]."
"In our investigation of frequency estimates for word similarity measures, we compare the results of sev-eral different measures and frequency estimates to solve human-oriented language tests."
"Our investigation is based in part on the questions used by Landauer and Du-mais, and by Turney."
"An example of such tests is the determination  of  the  best synonym in a set of alternatives  context    for  a  specific  , astargetshownwordin figurein1a."
"Ideally, the context can provide support to choose best al-ternative for each question."
"We also investigate questions where no context is available, as shown in figure 2."
These questions provides an easy way to assess the performance of measures and the co-occurrence frequency estimation methods used to compute them.
"Although word similarity has been used in many dif-ferent applications, to the best of our knowledge, ours is the first comparative investigation of the impact of co-occurrence frequency estimation on the performance of word similarity measures."
"In this paper, we provide a comprehensive study of some of the most widely used similarity measures with frequency estimates taken from a terabyte-sized corpus of Web data, both in the presence of context and not."
"In addition, we investigate frequency estimates for co-occurrence that are based both on docu-ments and on a variety of different window sizes, and ex-amine the impact of the corpus size on the frequency es-timates."
"In questions where context is available, we also investigate the effect of adding more words from context."
The remainder of this paper is organized as follows: In section 2 we briefly introduce some of the most commonly used methods for measuring word similarity.
In section 3 we present methods to assess word co-occurrence frequencies.
"Section 4 presents our experi-mental evaluation, which is followed by a discussion of the results in section 5."
"The notion for co-occurrence of two words can depicted by a contingency table, as shown in table 1 ! ."
Each dimen- # $ sion represents &amp;% a random discrete variable &quot; with range (presence or absence of word &apos; in a given text window or document).
"Each cell in the table repre- sent 021436 the 5 joint frequency ( +) -* ,/) . ;: &apos; =&lt;?&gt; , where is the maximum number of co-occurrences."
"Un-der an independence assumption, the values of the cells in the contingency table are calculated using the prob-abilities in table 2."
The methods described below per-form different measures of how distant observed values are from expected values under an independence assump-tion.
"Occasionally, a context is available and can pro-vide support for the co-occurrence and alternative meth-ods can be 8; used : to  exploit &gt; this ;8 context : &gt; ."
"The procedures to estimate , as well , will be described in section 3."
We first present methods to measure the similarity be-tween two words and when no context is available.
This measure for word similarity was first used in this context[REF_CITE].
The measure is given by equation 1 and is called Pointwise Mutual Infor-mation.
"It is a straightforward transformation ;8 : of the inde- &gt; pendence 8 /&gt; 7H8; assumption : &gt; (on a specific point), ;: , into a ratio."
Positive values indicate that words occur together more than would be expected under an independence assumption.
Negative values indicate that one word tends to appear only when the other does not.
Values close to zero indicate independence. !YZU6[\^S ]_!
W YL]] MeMefQ YZUQfY \`-MeQfYLU gY[ ] `&amp;] ` (1)
This test is directly derived from observed and ex-pected values in the contingency tables.
Tp l$q o9rtsDls l$q o $qo ` ] i ]
Wkj Q j $m n @ mKn B (2) l
"The h statistic determines a specific way to calculate the difference between values expected under indepen-dence and 5 observed ones, as depicted in equation 2."
"The values ( , u correspond to the observed frequency esti-mates."
The likelihood ratio test provides an alternative to check two simple hypotheses based on parameters of a distribution.
H2: (i.e. not independent).
"These two conditionals T8: ;: are v used 6&gt; ; as : sample v % &gt;d\| in }w&gt; the like-lihood } function { , where in this particular case represents :_|\}w&gt; the parameter of the binomial 8;:  distribution &gt;;: ~ &gt; % / &gt; ."
"Under  hypothe-sis 8;: H1 ?v , &gt; K;:  % , and for H2, .  W"
QMeQMeQfYQfYL] ]$Y YZU `U f` fRU +``+  QQMeMeQfYQfYL]]KAYZUAY U ``  ] ` (3)
Equation 3 represents the likelihood ratio.
"Asymptoti-cally, $ is h distributed."
This measure corresponds to the expected value of two random variables using the same equation as PMI.
Av-erage mutual information was used as a word similarity measure[REF_CITE]and is given by equation 4.
QfR`-MeQf`Qf+[-w` NORQTS U  S ] `FW j j (4) $mKn @  n Bl
"Similarity between two words can also be in-ferred from a  context  ,(if givenand ). areGivenrelateda contextif their   co-occurrence with words in context are similar."
The PMI between each context word and form a vector.
The elements in the vector represents the similar-ity weights of  and .
"The cosine value between the two vectors corresponding to and represents the similarity between the two words in the specified context, as depicted in equation 5."
M NPORQfYLTY[ U T[-Y ] `  ]6`JW ¡ $ [gY U ` ] ¡ R R  Y[ ] ` ] (5)
Values closer to one indicate more similarity whereas values close to zero represent less similarity.
In this method the conditional probability of each word  in given (and ) is computed.
"The accumu-lated distance between the conditionals for all words in context represents the similarity between the two words, as shown in equation 6."
This method was proposed as an alternative word similarity measure in language modeling to overcome zero-frequency problems of bigrams[REF_CITE].  QfY U gY ] `JW  j ¢K MeQfY  r MeQfY Y9¤$`&amp; (6)
"In this measure, a smaller value indicates a greater sim-ilarity."
The conditional probabilities between each word in the context and the two words and are used to calculate the mutual information of the conditionals (equation 7).
This method was also used in Dagan et. al. (1999 ¥ ).
M QfY   MeMeQfYQfY  Y9¤$` NPO  QfY U Y ] `XWj e  (7)
This is an alternative to the Mutual Information for-mula (equation 8).
It helps to avoid zero frequency prob-lem by averaging the two distributions and also provides a symmetric measure (AMIC is not symmetric).
This method was also used in Dagan et. al. (1999).  Q§A¨¬ \ © ¦ ­W MeQfY  YZU `¯®°MeQfY¤  YL6`] M ¥D² QfY U gY ] `JWP¦  QMeQfY &amp;¨  MM `` O ± ®Z¦  QMeQfY Y9¤$`&amp;¨ (8)
"The context is represented by , which is any subset of the context ."
"In fact, Turney ar-gued that bigger sets are worse because they narrow the estimate and as consequence can be affected by noise."
"As a consequence, Turney used only one word ³ from the context, discarding the remaining words."
The chosen word was the one that has  biggest pointwise information ) is fixed 8; when : the &gt; method with .
"Moreover, ( is used to find the best for , so is also fixed and can be ignored ;8 : , which v &gt; transforms ;8 :   &gt; the equation into the conditional ."
It is interesting to note that the equation is not the traditional n-gram model since no ordering is imposed on the words and also due to the fact that the words in this formula can be separated from one another by other words.   QfYZU&amp;YL[ d]  `JW MeQfYLMe] Qf[Y  U [ `-YMe]
Qf[YZ U&amp;`[  ` (9)
Many other measures for word similarities exists.
We now discuss some alternatives to estimate word co-occurrence frequencies from an available corpus.
All probabilities mentioned in previous section can be es-timated from these frequencies.
We describe two dif-ferent approaches: a window-oriented approach and a document-oriented approach.
"Let ( ) * be the frequency of and the co-occurrence 0 fre-quency of be denoted by ( )´@&amp;, )/B ."
Letand be the size of the corpus in words.
"In the window-oriented approach, individual word frequencies are the corpus fre-quencies."
The maximum ;8 : &gt;H likelihood ( +) *\µ 0 . estimate (MLE) forin the corpus is
"The joint frequency ( A) @ , +) B is estimated by the number of windows where the two words co-occur."
"The window size may vary,[REF_CITE]used windows of size 2 and 5."
Let the number 0¸1Z of 3d5 windows of size ¶ in the corpus be ´·) .
"The ;8 : MLE D&gt; of the ( A) @ , co-occurrence +) B µ 0 ) ´· . probability is given by in this case )´· In most common 0 E0 case º¶ , »V¼ windows."
"The totalarefrequencyoverlappingof,win-and dows for co-occurrence should be adjusted to reflect the multiple counts of the same co-occurrence."
One method dows by  to account for J½  overlap  is to .divideThis methodthe totalalsocountreinforcesof win-closer co-occurrences by assigning them a larger weight.
"Smoothing techniques can be applied to address the zero-frequencyproblem, or alternatively, the window size can be increased, which also increases the chance of co-occurrence."
"To avoid inconsistency, windows do not to cross document boundaries."
"In information retrieval, one commonly uses document statistics rather than individual word statistics."
"In an document-oriented ½ approach, the frequency of a word is denoted by ( ) * and corresponds to the number of doc-uments in which the word appears, regardless of how fre-quently it occurs in each document."
The number of docu-ments is denoted by Â .
The MLE 8; for : an H&gt; individual ½ ( ) \* µ Â word in document oriented approach is .
"The co-occurrence ½ frequency of two words and , denoted by ( ) @ ,) B , is the number of documents where the words co-occur."
"If we require only that the words co-occur in the same document, no distinction is made between distantly occurring words and adjacent words."
"This distortion can be reduced by imposing a maximal distance for co-occurrence, (i.e. a fixed-sized window), but the frequency will still be the number of documents where the two words co-occur within this dis-istance ;8 : ."
"The  MLE &gt;!Ã for ½ ( A) the 6@ ,/) B co-occurrence  , since 0 14 in 63 5 this Â approachin the document-oriented approach."
An alternative to the Window and Document-oriented ap-proach is to use syntactical informati[REF_CITE].
"For this purpose, a Parser or Part-Of-Speech tag-ger must be applied to the text and only the interesting pairs of words in correct syntactical categories used."
"In this case, the fixed window can be superseded by the re-sult of the syntax analysis or tagging process and the fre-quency of the pairs can be used directly."
"Alternatively, the number of documents that contain the pair can also be used."
"However, the nature of the language tests in this work make it impractical to be applied."
"First, the alter-natives are not in a context, and as such can have more than one part-of-speech tag."
"Occasionally, it is possible to infer that the syntactic category of the alternatives from context of the target word , if there is such a context ."
"When the alternatives, or the target word , are mul-tiwords then the problem is harder, as depicted in the first example of figure 7."
"Also, both parsers and POS tagger make mistakes, thus introducing error."
"Finally, the size of the corpus used and its nature intensify the parser/POS taggers problems."
We evaluate the methods and frequency estimates using 3 test sets.
The first test set is a set of TOEFL questions first used[REF_CITE]and also[REF_CITE].
This test set contains 80 synonym questions and for v  each  question one and four alternative op-tions ( ) are given.
"The other two test sets, which we will refer to as TS1 and TS2, are practice questions for the TOEFL."
"These v ºv+ two ÅÄ test sets also contain four al-ternatives options, , and is given in context (within a sentence)."
For all test sets the answer to each question is known and unique.
"For comparison purposes, we also use TS1 and TS2 with no context."
"For the three test sets, TOEFL, TS1 and TS2 without context, we applied the word and document-oriented fre-quency estimates presented."
"We investigated a variety of window sizes, varying the window size from 2 to 256 by powers of 2."
"The labels used in figures 3, 5, 6, 8, 9, 10, 12 are com-posed from a keyword indicating the frequency estimate used (W-window oriented; and DR-document retrieval oriented) and a keyword indicating the word similarity measure."
For no-context measures the keywords are: PMI-Pointwise Mutual Information; CHI-Chi-Squared; MI-Average mutual information; and LL-Log-likelihood.
For the measures with context: CP-Cosine pointwise mu-tual information; L1-L1 norm; AMIC-Average Mutual
Information in the presence of context  ; IRAD-Jensen-Shannon Divergence  ; and PMIC- - Pointwise Mutual Information with words of context.
"For TS1 and TS2 with context, we also investigate Tur-ney’s hypothesis that the outcome of adding more words from is negative, using DR-PMIC."
The result of this experiment is shown in figures 10 and 12 for TS1 and TS2 respectively.  
"It is important to note that in some of the questions, or one or more of the ’s are multi-word strings."
"For these questions, we assume that the strings may be treated as collocations and use them “as is”, adjusting the size of the windows by the collocation size when appli-cable."
The corpus used for the experiments is a terabyte of Web data crawled from the general web in 2001.
"In order to balance the contents of the corpus, a breadth-first order search was used from a initial seed set of URLs represent-ing the home page of 2392 universities and other educa-tional organizations[REF_CITE]."
No duplicate pages are included in the collection and the crawler also did not allow a large number of pages from the same site to be downloaded simultaneously.
"Overall, the collection contains 53 billion words and 77 million documents."
A key characteristic of this corpus is that it consists of HTML files.
"These files have a focus on the presentation, and not necessarily on the style of writing."
Parsing or tagging these files can be a hard process and prone to in-troduction of error in rates bigger than traditional corpora used in NLP or Information Retrieval.
"We also investigate the impact of the collection size on the results, as depicted in figures 4, 11 and 13 for TOEFL, TS1 and TS2 test sets, respectively."
The results for the TOEFL questions are presented in figure 3.
The best performance found is 81.25% of the questions correctly answered.
That result used DR-PMI with a window size of 16-32 words.
"This is an im-provement over the results presented[REF_CITE]using Latent Semantic Analysis, where 64.5% of the questions were answered correctly, and[REF_CITE], using pointwise mutual information and doc-ument retrieval, where the best result was 73.75%."
"Although we use a similar method (DR-PMI), the dif-ference between the results presented here and Turney’s results may be due to differences in the corpora and dif-ferences in the queries."
Turney uses Altavista and we used our own crawl of web data.
We can not compare the collections since we do not know how Altavista col-lection is created.
"As for the queries, we have more con-trol over the queries since we can precisely specify the window size and we also do not know how queries are evaluated in Altavista."
"PMI performs best overall, regardless of estimates used (DR or W)."
"W-CHI performs up to 80% when using win-dow estimates, outperforming DR-CHI."
"MI and LL yield exactly the same results (and the same ranking of the al-ternatives), which suggests that the binomial distribution is a good approximation for word occurrence in text."
"The results for MI and PMI indicate that, for the &quot;Æ two discrete % random variables and (and range  ), no further gain is achieved by calculating the expectation in the divergence."
Recall that the divergence formula has an embedded expectation to be calculated be-tween the joint probability of these two random variables and their independence.
"The peak of information is ex- actly where È both words co-occur, i.e. when  and , and not any of the other three possible combinations."
"Similar trends are seen when using TS1 and no con-text, as depicted in figure 5."
"PMI is best overall, and DR-PMI and W-PMI outperform each other with different windows sizes."
W-CHI has good performance in small windows sizes.
"MI and LL yield identical (poor) results, being worst than chance for some window sizes."
"In the test set TS2 with no context, the trend seen be-tween TOEFL and TS1 is repeated, as shown in figure 8."
PMI is best overall but W-CHI performs better than PMI in three cases.
DR-CHI performs poorly for small win-dows sizes.
MI and LL also perform poorly in compari-son with PMI.
"The peak performance is 75%, using DR-PMI with a window size of 64."
The result are not what we expected when context is used in TS1 and TS2.
"In TS1, figure 6, only one of the measures, DR-PMIC-1, outperforms the results from non-context measures, having a peak of 80% correct an-swers."
The condition for the best result (one word from context and a window size of 8) is similar to the one used for the best score reported by Turney.
"L1, AMIC and IRAD perform poorly, worst than chance for some window sizes."
"One difference in the results is that for DR-PMIC-1 only the best word from context was used, while the other methods used all words but stopwords."
"We examine the context and discovered that using more words degrades the performance of DR-PMIC in all dif-ferent windows sizes but, even using all words except stopwords, the result from DR-PMIC is better than any other contextual measure - 76% correct answers in TS1 (with DR-PMIC and a window size of 8)."
"For TS2, no measure using context was able to perform better than the non-contextual measures."
DR-PMIC-1 performs better overall but has worse performance than DR-CP with a window size of 8.
"In this test set, the per-formance of DR-CP is better than W-CP."
"L1 performs better than AMIC but both have poor results, IRAD is never better than chance."
"The context in TS2 has more words than TS1 but the questions seem to be harder, as shown in figure 7."
"In some of the TS2 questions, the tar-get word or one of the alternatives uses functional words."
"We also investigate the influence of more words from context in TS2, as depicted in figure 12, where the trends seen with TS1 are repeated."
The results in TS1 and TS2 suggest that the available context is not very useful or that it is not being used prop-erly.
"Finally, we selected the method that yields the best performance for each test set to analyze the impact of the corpus size on performance, as shown in fig-ures 4, 11 and 13."
For TS1 we use W-PMI with a win-dow size of 2 (W-PMI2) when no context is used and DR-PMIC-1 with a window size of 8 (DR-PMIC8-1) when context is used.
"For those measures, very little im-provement is noticed after 500 GBytes for DR-PMIC8-1, roughly half of the collection size."
No apparent improve-ment is achieved after 300-400 GBytes for W-PMI2.
"For TS2 we use DR-PMI with a window size of 64[REF_CITE]when no context is used, and DR-PMIC-1 with a windows size of 64 ([REF_CITE]-1) when context is used."
It is clear that for TS2 no substantial improve-ment[REF_CITE]-1 is achieved by increasing the corpus size to values bigger than 300-400
The most interesting impact of corpus size was on TOEFL test set using DR-PMI with a window size of 16[REF_CITE].
"Using the full corpus is no better than using 5% of the corpus, and the best result, 82.5% correct answers, is achieved when using 85-95% of corpus size."
"Using a large corpus and human-oriented tests we de-scribe a comprehensive study of word similarity mea-sures and co-occurrence estimates, including variants on corpus size."
"Without any parameter training, we were able to correctly answer at least 75% questions in all test sets."
"From all combinations of estimates and measures, document retrieval with a maximum window of 16 words and pointwise mutual information performs best on aver-age in the three test sets used."
"However, both document or windows-oriented approach for frequency estimates pro-duce similar results in average."
"The impact of the corpus size is not very conclusive, it suggests that the increase in the corpus size normally reaches an asymptote, but the points where this occurs is distinct among different mea-sures and frequency estimates."
"Our results outperform the previously reported results on test sets when no context is used, being able to cor-rectly answer 81.25% of TOEFL synonym questions, compared with a previous best result of 73.5%."
A hu-man average score on the same type of questions is 64.5%[REF_CITE].
"We also perform better than previous work on another test set used as prac-tice questions for TOEFL, obtaining 80% correct answers compared to a best result of 74% from previous work."
"We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag con-texts via a dependency network representa-tion, (ii) broad use of lexical features, includ-ing jointly conditioning on multiple consecu-tive words, (iii) effective use of priors in con-ditional loglinear models, and (iv) fine-grained modeling of unknown word features."
"Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result."
Almost all approaches to sequence problems such as part-of-speech tagging take a unidirectional approach to con-ditioning inference along the sequence.
"Regardless of whether one is using HMMs, maximum entropy condi-tional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left, e.g.,[REF_CITE])."
"There are a few excep-tions, such as Brill’s transformation-based learning[REF_CITE], but most of the best known and most successful approaches of recent years have been unidirectional."
Most sequence models can be seen as chaining to-gether the scores or decisions from successive local mod-els to form a global model for an entire sequence.
Clearly the identity of a tag is correlated with both past and future tags’ identities.
"However, in the unidirectional (causal) case, only one direction of influence is explicitly consid-ered at each local point."
"For example, in a left-to-right first-order HMM, the current tag t 0 is predicted based on the previous tag t −1 (and the current word). [Footnote_1] The back-ward interaction between t 0 and the next tag t +1 shows up implicitly later, when t +1 is generated in turn."
"1 Rather than subscripting all variables with a position index, we use a hopefully clearer relative notation, where t 0 denotes the current position and t −n and t +n are left and right context tags, and similarly for words."
"While unidirectional models are therefore able to capture both directions of influence, there are good reasons for sus-pecting that it would be advantageous to make informa-tion from both directions explicitly available for condi-tioning at each local point in the model: (i) because of smoothing and interactions with other modeled features, terms like P(t 0 |t +1 , . . .) might give a sharp estimate of t 0 even when terms like P(t +1 |t 0 , . . .) do not, and (ii) jointly considering the left and right context together might be especially revealing."
"In this paper we exploit this idea, using dependency networks, with a series of local con-ditional loglinear (aka maximum entropy or multiclass logistic regression) models as one way of providing ef-ficient bidirectional inference."
"Secondly, while all taggers use lexical information, and, indeed, it is well-known that lexical probabilities are much more revealing than tag sequence probabilities[REF_CITE], most taggers make quite limited use of lexical probabilities (compared with, for example, the bilexical probabilities commonly used in current sta-tistical parsers)."
"While modern taggers may be more prin-cipled than the classic CLAWS tagger[REF_CITE], they are in some respects inferior in their use of lexical information: CLAWS, through its IDIOMTAG module, categorically captured many important, correct taggings of frequent idiomatic word sequences."
"In this work, we incorporate appropriate multiword feature templates so that such facts can be learned and used automatically by the model."
"Having expressive templates leads to a large number of features, but we show that by suitable use of a prior (i.e., regularization) in the conditional loglinear model – something not used by previous maximum entropy tag-gers – many such features can be added with an overall positive effect on the model."
"Indeed, as for the voted per-ceptron[REF_CITE], we can get performance gains by reducing the support threshold for features to be in-cluded in the model."
"Combining all these ideas, together with a few additional handcrafted unknown word fea-tures, gives us a part-of-speech tagger with a per-position tag accuracy of 97.24%, and a whole-sentence correct rate of 56.34% on Penn Treebank WSJ data."
"This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented[REF_CITE], using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model[REF_CITE]."
"When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM[REF_CITE]or a conditional Markov model (CMM)[REF_CITE])."
"In such models, the probability assigned to a tagged sequence of words x = ht, wi is the product of a sequence of local portions of the graphical model, one from each time slice."
"For example, in the left-to-right CMM shown in figure 1(a),"
"P(t, w) = Y P(t i |t i−1 , w i ) i"
"That is, the replicated structure is a local model P(t 0 |t −1 ,w 0 ). [Footnote_2] Of course, if there are too many con-ditioned quantities, these local models may have to be estimated in some sophisticated way; it is typical in tag-ging to populate these models with little maximum en-tropy models."
2 Throughout this paper we assume that enough boundary symbols always exist that we can ignore the differences which would otherwise exist at the initial and final few positions.
"For example, we might populate a model for P(t 0 |t −1 , w 0 ) with a maxent model of the form: exp(λ ht 0 ,t −1 i + λ ht 0 ,w 0 i )"
"P λ (t 0 |t −1 , w 0 ) ="
"P exp(λ ht 00 ,t −1 i + λ ht 00 ,w 0 i ) t 00"
"In this case, the w 0 and t −1 can have joint effects on t 0 , but there are not joint features involving all three variables (though there could have been such features)."
"We say that this model uses the feature templates ht 0 , t −1 i (previous tag features) and ht 0 , w 0 i (current word features)."
"Clearly, both the preceding tag t −1 and following tag t +1 carry useful information about a current tag t 0 ."
"Unidi-rectional models do not ignore this influence; in the case of a left-to-right CMM, the influence of t −1 on t 0 is ex-plicit in the P(t 0 |t −1 ,w 0 ) local model, while the influ-ence of t +1 on t 0 is implicit in the local model at the next position (via P(t +1 |t 0 ,w +1 ))."
The situation is reversed for the right-to-left CMM in figure 1(b).
"From a seat-of-the-pants machine learning perspective, when building a classifier to label the tag at a certain posi-tion, the obvious thing to do is to explicitly include in the local model all predictive features, no matter on which side of the target position they lie."
"There are two good formal reasons to expect that a model explicitly condi-tioning on both sides at each position, like figure 1(c) could be advantageous."
"First, because of smoothing effects and interaction with other conditioning features (like the words), left-to-right factors like P(t 0 |t −1 ,w 0 ) do not always suffice when t 0 is implicitly needed to de-termine t −1 ."
"For example, consider a case of observation bi[REF_CITE]for a first-order left-to-right CMM."
The word to has only one tag ( TO ) in the PTB tag set.
"The TO tag is often preceded by nouns, but rarely by modals ( MD )."
"In a sequence will to fight, that trend indicates that will should be a noun rather than a modal verb."
"However, that effect is completely lost in a CMM like (a): P(t will |will, hstarti) prefers the modal tagging, and P( TO |to, t will ) is roughly 1 regardless of t will ."
"While the model has an arrow between the two tag positions, that path of influence is severed. [Footnote_3] The same problem ex-ists in the other direction."
"3 Despite use of names like “label bias”[REF_CITE]or “observation bias”, these effects are really just unwanted explaining-away effects ([REF_CITE]19), where two nodes which are not actually in causal competition have been modeled as if they were."
"If we use the symmetric right- to-left model , fight will receive its more common noun tagging by symmetric reasoning."
"However, the bidirec-tional model (c) discussed in the next section makes both directions available for conditioning at all locations, us-ing replicated models of P(t 0 |t −1 , t +1 , w 0 ), and will be able to get this example correct. [Footnote_4]"
"4 The effect of indirect influence being weaker than direct in-fluence is more pronounced for conditionally structured models, but is potentially an issue even with a simple HMM. The prob-abilistic models for basic left-to-right and right-to-left HMMs with emissions on their states can be shown to be equivalent us-ing Bayes’ rule on the transitions, provided start and end sym-bols are modeled. However, this equivalence is violated in prac-tice by the addition of smoothing."
"While the structures in figure 1(a) and (b) are well-understood graphical models with well-known semantics, figure 1(c) is not a standard Bayes’ net, precisely because the graph has cycles."
"Rather, it is a more general de-pendency network[REF_CITE]."
"Each node represents a random variable along with a local condi-tional probability model of that variable, conditioned on the source variables of all incoming arcs."
"In this sense, the semantics are the same as for standard Bayes’ nets."
"However, because the graph is cyclic, the net does not correspond to a proper factorization of a large joint prob-ability estimate into local conditional factors."
Consider the two-node cases shown in figure 2.
"Formally, for the net in (a), we can write P(a, b) = P(a)P(b|a)."
"For (b) we write P(a,b) = P(b)P(a|b)."
"However, in (c), the nodes A and B carry the information P(a|b) and P(b|a) respectively."
"The chain rule doesn’t allow us to recon-struct P(a,b) by multiplying these two quantities."
"Un-der appropriate conditions , we could reconstruct P(a, b) from these quantities using Gibbs sampling, and, in gen-eral, that is the best we can do."
"However, while recon-structing the joint probabilities from these local condi-tional probabilities may be difficult, estimating the local probabilities themselves is no harder than it is for acyclic models: we take observations of the local environments and use any maximum likelihood estimation method we desire."
"In our experiments, we used local maxent models, but if the event space allowed, (smoothed) relative counts would do."
"Cyclic or not, we can view the product of local probabil-ities from a dependency network as a score: score(x) ="
Y P(x i |Pa(x i )) i where Pa(x i ) are the nodes with arcs to the node x i .
"In the case of an acyclic model, this score will be the joint prob-ability of the event x, P(x)."
"In the general case, it will not be."
"However, we can still ask for the event, in this case the tag sequence, with the highest score."
"For dependency net-works like those in figure 1, an adaptation of the Viterbi algorithm can be used to find the maximizing sequence in polynomial time."
"Figure 3 gives pseudocode for the concrete case of the network in figure 1(d); the general case is similar, and is in fact just a max-plus version of standard inference algorithms for Bayes’ nets ([REF_CITE]97)."
"In essence, there is no difference between inference on this network and a second-order left-to-right CMM or HMM."
"The only difference is that, when the Markov window is at a position i, rather than receiving the score for P(t i |t i−1 , t i−2 , w i ), one receives the score for P(t i−1 |t i , t i−2 , w i−1 )."
There are some foundational issues worth mention-ing.
"As discussed previously, the maximum scoring se-quence need not be the sequence with maximum likeli-hood according to the model."
There is therefore a worry with these models about a kind of “collusion” where the model locks onto conditionally consistent but jointly un-likely sequences.
Consider the two-node network in fig-ure 2(c).
"If we have the following distribution of ob-servations (in the form ab) h11, 11, 11, 12, 21, 33i, then clearly the most likely state of the network is 11."
"How-ever, the score of 11 is P(a = 1|b = 1)P(b = 1|a = 1) = 3/4 × 3/4 = 9/16, while the score of 33 is 1."
An ad-ditional related problem is that the training set loss (sum of negative logarithms of the sequence scores) does not bound the training set error (0/1 loss on sequences) from above.
"Consider the following training set, for the same network, with each entire data point considered as a label: h11, 22i."
"The relative-frequency model assigns loss 0 to both training examples, but cannot do better than 50% error in regenerating the training data labels."
These is-sues are further discussed[REF_CITE].
Preliminary work of ours suggests that practical use of dependency networks is not in general immune to these theoretical concerns: a dependency network can choose a sequence model that is bidirectionally very consistent but does not match the data very well.
"However, this problem does not appear to have prevented the networks from per-forming well on the tagging problem, probably because features linking tags and observations are generally much sharper discriminators than tag sequence features."
It is useful to contrast this framework with the con-ditional random fields[REF_CITE].
"The CRF approach uses similar local features, but rather than chaining together local models, they construct a sin-gle, globally normalized model."
The principal advan-tage of the dependency network approach is that advan-tageous bidirectional effects can be obtained without the extremely expensive global training required for CRFs.
"To summarize, we draw a dependency network in which each node has as neighbors all the other nodes that we would like to have influence it directly."
Each node’s neighborhood is then considered in isolation and a local model is trained to maximize the conditional like-lihood over the training data of that node.
"At test time, the sequence with the highest product of local conditional scores is calculated and returned."
"We can always find the exact maximizing sequence, but only in the case of an acyclic net is it guaranteed to be the maximum likelihood sequence."
The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III[REF_CITE].
"We extracted tagged sentences from the parse trees. [Footnote_5] We split the data into training, development, and test sets as[REF_CITE]."
"5 Note that these tags (and sentences) are not identical to those obtained from the tagged/pos directories of the same disk: hundreds of tags in the RB/RP/IN set were changed to be more consistent in the parsed/mrg version. Maybe we were the last to discover this, but we’ve never seen it in print."
"Table 1 lists character- istics of the three splits. [Footnote_6] Except where indicated for the model BEST , all results are on the development set."
"6 Tagger results are only comparable when tested not only on the same data and tag set, but with the same amount of training data.[REF_CITE]illustrates very clearly how tagging perfor-mance increases as training set size grows, largely because the percentage of unknown words decreases while system perfor-mance on them increases (they become increasingly restricted as to word class)."
"One innovation in our reporting of results is that we present whole-sentence accuracy numbers as well as the traditional per-tag accuracy measure (over all tokens, even unambiguous ones)."
This is the quantity that most sequence models attempt to maximize (and has been mo-tivated over doing per-state optimization as being more useful for subsequent linguistic processing: one wants to find a coherent sentence interpretation).
"Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more com-mon ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence."
"Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results."
"The per-state models in this paper are log-linear mod-els, building upon the models[REF_CITE]and[REF_CITE], though some models are in fact strictly simpler."
"The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words. [Footnote_7] We present the results of three classes of experi-ments: experiments with directionality, experiments with lexicalization, and experiments with smoothing."
"7 Except where otherwise stated, a count cutoff of 2 was used for common word features and 35 for rare word features (tem-plates need a support set strictly greater in size than the cutoff before they are included in the model)."
"In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words’ tags."
Table 2 lists the discussed networks.
"All networks have the same ver-tical feature templates: ht 0 , w 0 i features for known words and various ht 0 , σ(w 1n ) i word signature features for all words, known or not, including spelling and capitaliza-tion features (see section 3.3)."
"Just this vertical conditioning gives an accuracy of 93.69% (denoted as “Baseline” in table 2). [Footnote_8] Condition- ing on the previous tag as well (model L, ht 0 ,t −1 i fea-tures) gives 95.79%."
"8[REF_CITE]noted that such a simple model got 90.25%, but this was with no unknown word model beyond a prior distribution over tags.[REF_CITE]raise this baseline to 92.34%, and with our sophisticated unknown word model, it gets even higher. The large number of unambiguous tokens and ones with very skewed distributions make the base-"
"The reverse, model R, using the next tag instead, is slightly inferior at 95.14%."
"Model L+R, using both tags simultaneously (but with only the individual-direction features) gives a much better accu-racy of 96.57%."
"Since this model has roughly twice as many tag-tag features, the fact that it outperforms the uni-directional models is not by itself compelling evidence for using bidirectional networks."
"However, it also out-performs model L+L 2 which adds the ht 0 ,t −2 i second-previous word features instead of next word features, which gives only 96.05% (and R+R 2 gives 95.25%)."
"We conclude that, if one wishes to condition on two neigh-boring nodes (using two sets of 2-tag features), the sym-metric bidirectional model is superior."
"High-performance taggers typically also include joint three-tag counts in some way, either as tag trigrams[REF_CITE]or tag-triple features ([REF_CITE])."
"Models LL, RR, and CR use only the vertical features and a single set of tag-triple features: the left tags (t −2 , t −1 and t 0 ), right tags (t 0 , t +1 , t +2 ), or centered tags (t −1 , t 0 , t +1 ) respectively."
"Again, with roughly equivalent feature sets, the left context is better than the right, and the centered context is better than either unidirectional context."
"Lexicalization has been a key factor in the advance of statistical parsing models, but has been less exploited for tagging."
"Words surrounding the current word have been occasionally used in taggers, such[REF_CITE], Brill’s transformation based tagger[REF_CITE], and the HMM model[REF_CITE], but neverthe-less, the only lexicalization consistently included in tag-ging models is the dependence of the part of speech tag of a word on the word itself."
"In maximum entropy models, joint features which look at surrounding words and their tags, as well as joint fea-tures of the current word and surrounding words are in principle straightforward additions, but have not been in-corporated into previous models."
We have found these features to be very useful.
We explore here lexicaliza-tion both alone and in combination with preceding and following tag histories.
Table 3 shows the development set accuracy of several models with various lexical features.
All models use the same rare word features as the models in Table 2.
The first two rows show a baseline model using the current word only.
The count cutoff for this feature was 0 in the first model and 2 for the model in the second row.
"As there are no tag sequence features in these models, the ac-curacy drops significantly if a higher cutoff is used (from a per tag accuracy of about 93.7% to only 60.2%)."
The third row shows a model where a tag is de-cided solely by the three words centered at the tag po-sition (3W).
"As far as we are aware, models of this sort have not been explored previously, but its accu-racy is surprisingly high: despite having no sequence model at all, it is more accurate than a model which uses standard tag fourgram HMM features (ht 0 , w 0 i, ht 0 , t −1 i, ht 0 , t −1 , t −2 i, ht 0 , t −1 , t −2 , t −3 i, shown in Table 2, model L+LL+LLL)."
The fourth and fifth rows show models with bi-directional tagging features.
"The fourth model (3W+ TAGS ) uses the same tag sequence features as the last model in Table 2 (ht 0 , t −1 i, ht 0 , t −1 , t −2 i, ht 0 , t −1 , t +1 i, ht 0 , t +1 i, ht 0 , t +1 , t +2 i) and current, previ-ous, and next word."
"The last model has in ad-dition the feature templates ht 0 , w 0 , t −1 i, ht 0 , w 0 , t +1 i, ht 0 ,w −1 ,w 0 i, and ht 0 ,w 0 ,w +1 i, and includes the im-provements in unknown word modeling discussed in sec-tion 3.3. [Footnote_9] We call this model BEST ."
"9[REF_CITE]use ht −1 ,t 0 ,w 0 i templates in their “full-second order” HMM, achieving an accuracy of 96.86%. Here we can add the opposite tiling and other features."
BEST has a to-ken accuracy on the final test set of 97.24% and a sen-tence accuracy of 56.34% (see Table 4).
"A 95% confi-dence interval for the accuracy (using a binomial model) is (97.15%, 97.33%)."
"In order to understand the gains from using right con-text tags and more lexicalization, let us look at an exam-ple of an error that the enriched models learn not to make."
"An interesting example of a common tagging error of the simpler models which could be corrected by a determinis-tic fixup rule of the kind used in the IDIOMTAG module[REF_CITE]is the expression as X as (often, as far as)."
"This should be tagged as/RB X/{RB,JJ} as/IN in the Penn Treebank."
"A model using only current word and two left tags (model L+L 2 in Table 2), made 87 errors on this expression, tagging it as/IN X as/IN – since the tag sequence probabilities do not give strong reasons to dis-prefer the most common tagging of as (it is tagged[REF_CITE]% of the time)."
"However, the model 3W+ TAGS , which uses two right tags and the two surrounding words in addition, made only 8 errors of this kind, and model BEST made only 6 errors."
"Most of the models presented here use a set of un-known word features basically inherited[REF_CITE], which include using character n-gram pre-fixes and suffixes (for n up to 4), and detectors for a few other prominent features of words, such as capitaliza-tion, hyphens, and numbers."
"Doing error analysis on un-known words on a simple tagging model (with ht 0 , t −1 i, ht 0 , t −1 , t −2 i, and hw 0 , t 0 i features) suggested several ad-ditional specialized features that can usefully improve performance."
By far the most significant is a crude com-pany name detector which marks capitalized words fol-lowed within 3 words by a company name suffix like Co. or Inc.
"This suggests that further gains could be made by incorporating a good named entity recognizer as a prepro-cessor to the tagger (reversing the most common order of processing in pipelined systems!), and is a good example of something that can only be done when using a condi-tional model."
"Minor gains come from a few additional features: an allcaps feature, and a conjunction feature of words that are capitalized and have a digit and a dash in them (such words are normally common nouns, such as CFC-12 or F/A-18)."
We also found it advantageous to use prefixes and suffixes of length up to 10.
"Together with the larger templates, these features contribute to our unknown word accuracies being higher than those of pre-viously reported taggers."
"With so many features in the model, overtraining is a dis-tinct possibility when using pure maximum likelihood es-timation."
We avoid this by using a Gaussian prior (aka quadratic regularization or quadratic penalization) which resists high feature weights unless they produce great score gain.
The regularized objective F is:
"F(λ) = X log(P λ (t i |w, t)) + X n λ 2j i j=1 2σ 2"
"Since we use a conjugate-gradientprocedure to maximize the data likelihood, the addition of a penalty term is eas-ily incorporated."
"Both the total size of the penalty and the partial derivatives with repsect to each λ j are triv-ial to compute; these are added to the log-likelihood and log-likelihood derivatives, and the penalized optimization procedes without further modification."
We have not extensively experimented with the value of σ 2 – which can even be set differently for different pa-rameters or parameter classes.
"All the results in this paper use a constant σ 2 = 0.5, so that the denominator disap-pears in the above expression."
Experiments on a simple model with σ made an order of magnitude higher or lower both resulted in worse performance than with σ 2 = 0.5.
"Our experiments show that quadratic regularization is very effective in improving the generalization perfor-mance of tagging models, mostly by increasing the num-ber of features which could usefully be incorporated."
"The number of features used in our complex models – in the several hundreds of thousands, is extremely high in com-parison with the data set size and the number of features used in other machine learning domains."
We describe two sets of experiments aimed at comparing models with and without regularization.
"One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features."
"The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is ad-vocated[REF_CITE], and used in all the stochastic LFG work[REF_CITE]."
"How-ever, until recently, its role and importance have not been widely understood."
"For example,[REF_CITE]attribute the perceived limited success of logistic regres-sion for text categorization to a lack of use of regular-ization."
"At any rate, regularized conditional loglinear models have not previously been applied to the prob-lem of producing a high quality part-of-speech tagger:[REF_CITE],[REF_CITE], and[REF_CITE]all present unregularized models."
"Indeed, the result[REF_CITE]that including low support features helps a voted perceptron model but harms a max-imum entropy model is undone once the weights of the maximum entropy model are regularized."
Table 5 shows results on the development set from two pairs of experiments.
"The first pair of models use com-mon word templates ht 0 , w 0 i, ht 0 , t −1 , t −2 i and the same rare word templates as used in the models in table 2."
The second pair of models use the same features as model BEST with a higher frequency cutoff of 5 for common word features.
"For the first pair of models, the error reduction from smoothing is 5.3% overall and 20.1% on unknown words."
"For the second pair of models, the error reduction is even bigger: 16.2% overall after convergence and 5.8% if looking at the best accuracy achieved by the unsmoothed model (by stopping training after 75 iterations; see be-low)."
"The especially large reduction in unknown word er-ror reflects the fact that, because penalties are effectively stronger for rare features than frequent ones, the presence of penalties increases the degree to which more general cross-word signature features (which apply to unknown words) are used, relative to word-specific sparse features (which do not apply to unknown words)."
"Secondly, use of regularization allows us to incorporate features with low support into the model while improving performance."
Table 6 contrasts our results with those[REF_CITE].
"Since the models are not the same, the exact numbers are incompa-rable, but the difference in direction is important: in the regularized model, performance improves with the inclu-sion of low support features."
"Finally, in addition to being significantly more accu-rate, smoothed models train much faster than unsmoothed ones, and do not benefit from early stopping."
"For ex-ample, the first smoothed model in Table 5 required 80 conjugate gradient iterations to converge (somewhat ar-bitrarily defined as a maximum difference of 10 −4 in fea-ture weights between iterations), while its corresponding unsmoothed model required 335 iterations, thus training was roughly 4 times slower. [Footnote_10] The second pair of models required 134 and 370 iterations respectively."
"10 On a 2GHz PC, this is still an important difference: our largest models require about 25 minutes per iteration to train."
"As might be expected, unsmoothed models reach their highest gen-eralization capacity long before convergence and accu-racy on an unseen test set drops considerably with fur-ther iterations."
"This is not the case for smoothed mod-els, as their test set accuracy increases almost monoton-ically with training iterations. [Footnote_11] Figure 4 shows a graph of training iterations versus accuracy for the second pair of models on the development set."
"11 In practice one notices some wiggling in the curve, but the trend remains upward even beyond our chosen convergence point."
"We have shown how broad feature use, when combined with appropriate model regularization, produces a supe-rior level of tagger performance."
"While experience sug- gests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform[REF_CITE], the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them)."
"While part-of-speech tagging is now a fairly well-worn road, and our ability to win performance increases in this domain is starting to be limited by the rate of er-rors and inconsistencies in the Penn Treebank training data, this work also has broader implications."
"Across the many NLP problems which involve sequence mod-els over sparse multinomial distributions, it suggests that feature-rich models with extensive lexicalization, bidirec-tional inference, and effective regularization will be key elements in producing state-of-the-art results."
Evaluating competing technologies on a com-mon problem set is a powerful way to improve the state of the art and hasten technology trans-fer.
Yet poorly designed evaluations can waste research effort or even mislead researchers with faulty conclusions.
Thus it is important to ex-amine the quality of a new evaluation task to es-tablish its reliability.
This paper provides an ex-ample of one such assessment by analyzing the task within the[REF_CITE]question answer-ing track.
"The analysis demonstrates that com-parative results from the new task are stable, and empirically estimates the size of the dif-ference required between scores to confidently conclude that two runs are different."
"The goal of the question answering track is to foster re-search on systems that retrieve answers rather than docu-ments, with particular emphasis on systems that function in unrestricted domains."
"To date the track has consid-ered only a very restricted version of the general ques-tion answering problem, finding answers to closed-class questions in a large corpus of newspaper articles."
"Kupiec defined a closed-class question as “a question stated in natural language, which assumes some definite answer typified by a noun phrase rather than a procedural an-swer”[REF_CITE]."
The first difference was that systems were to return exact an-swers rather than the text snippets containing an answer that were accepted previously.
The second difference was that systems were required to return exactly one response per question and the questions were to be ranked by the system’s confidence in the answer it had found.
The change to exact answers was motivated by the be-lief that a system’s ability to recognize the precise extent of the answer is crucial to improving question answering technology.
The problems with using text snippets as re-sponses were illustrated in the[REF_CITE]track.
"Each of the answer strings shown in Figure 1 was judged correct for the question What river in the US is known as the Big Muddy?, yet earlier responses are clearly better than later ones."
Accepting only exact answers as correct forces sys-tems to demonstrate that they know precisely where the answer lies in the snippets.
"The second change, ranking questions by confidence in the answer, tested a system’s ability to recognize when it has found a correct answer."
Systems must be able to recognize when they do not know the answer to avoid returning incorrect responses.
In many applications re-turning a wrong answer is much worse than returning a “Don’t know” response.
Incorporating these two changes into the previous QA task resulted in the following task definition.
Participants were given a large corpus of newswire articles and a set of 500 closed-class questions.
Some of the questions did not have answers in the document collection.
A run con-sisted of exactly one response for each question.
"A re-sponse was either a [document-id, answer-string] pair or the string “NIL”, which was used to indicate that the sys-tem believed there was no correct answer in the collec-tion."
"Within a run, questions were ordered from most confident response to least confident response."
All runs were required to be produced completely automatically— no manual intervention of any kind was permitted.
The document collection used as the source of answers was the the AQUAINT Corpus of English News Text (LDC catalog number[REF_CITE]).
"The collection is comprised of documents from three different sources: the AP newswire from 1998–2000, the New York Times newswire from 1998–2000, and the (English portion of the) Xinhua News[REF_CITE]–2000."
"There are approximately [Footnote_1],033,000 documents and 3 gigabytes of text in the collection."
"1 Unfortunately, some errors remain in the test questions. Scores were nevertheless computed over all 500 questions as released by NIST."
The test set of questions were drawn from MSNSearch and AskJeeves logs.
NIST assessors searched the docu-ment collection for answers to candidate questions from the logs.
"NIST staff selected the final test set from among the candidates that had answers, keeping some questions for which the assessors found no answer."
"NIST corrected the spelling, punctuation, and grammar of the questions in the logs 1 , but left the content as it was."
NIST did not include any definition questions (Who is Duke Elling-ton?
"What are polymers?) in the test set, but otherwise made no attempt to control the relative number of differ-ent types of questions in the test set."
"A system response consisting of an [document-id, answer-string] pair was assigned exactly one judgment by a human assessor as follows: wrong: the answer string does not contain a correct an-swer or the answer is not responsive; not supported: the answer string contains a correct an-swer but the document returned does not support that answer; not exact: the answer string contains a correct answer and the document supports that answer, but the string contains more than just the answer (or is miss-ing bits of the answer); right: the answer string consists of exactly a correct an-swer and that answer is supported by the document returned."
Only responses judged right were counted as correct in the final scoring.
"A NIL response was counted as correct if there is no known answer in the document collection for that question (i.e., the assessors did not find an an-swer during the candidate selection phase and no system returned a right response for it)."
Forty-six questions have no known answer in the collection.
"The scoring metric used, called the confidence-weighted score, was chosen to emphasize the system’s ability to correctly rank its responses."
The metric is an analog of document retrieval’s uninterpolated average precision in that it rewards a system for a correct answer early in the ranking more than it rewards for a correct an-swer later in the ranking.
"More formally, if there are questions in the test set, the confidence-weighted score is defined to be  number correct in first ranks"
Table 1 gives evaluation results for a subset of the runs submitted to the[REF_CITE]QA track.
The table in-cludes one run each from the ten groups who submitted the top-scoring runs.
The run shown in the table is the run with the best confidence-weighted score (“Score”).
"Also given in the table are the percentage of questions answered correctly, and the precision and recall for rec-ognizing when there is no correct answer in the document collection (“NIL Accuracy”)."
Precision of recognizing no answer is the ratio of the number of times NIL was re-turned and correct to the number of times it was returned; recall is the ratio of the number of times NIL was returned and correct to the number of times it was correct (46).
QA systems have become increasingly complex over the four years of the TREC track such that there is now lit-tle in common across all systems.
Generally a system will classify an incoming question according to an ontology of question types (which varies from small sets of broad cat-egories to highly-detailed hierarchical schemes) and then perform type-specific processing.
"The web was used as a data source by most systems, though it was used in different ways."
For some systems the web was the primary source of an answer that the system then mapped to a document in the corpus to return as a response.
Other % NIL Accuracy Run Tag Score Correct Prec[REF_CITE]exactanswer    pris2002   
Still other systems used the web as one of sev-eral sources whose combined evidence selected the final response.
The results in Table 1 illustrate that the confidence-weighted score does indeed emphasize a system’s abil-ity to rank correctly answered questions before incor-rectly answered questions.
"For example, the exactan-swer run has a greater confidence-weightedscore than the pris2002 run despite answering 19 fewer questions cor-rectly (54.2 % answered correctly vs. 58.0 % answered correctly)."
The systems used a variety of approaches to creating their question rankings.
Almost all systems used question type as a factor since some question types are easier to answer than others.
Some systems use a score to rank candidate answers for a question.
"When that score is comparable across questions, it can also be used to rank questions."
A few groups used a training set of previous years’ questions and answers to learn a good feature set and corresponding weights to predict confidence.
"Many systems used NIL as an indicator that the system couldn’t find an answer (rather than the system was sure there was no answer), so ranked NIL responses last."
"With the ex-ception of the top-scoring[REF_CITE]run, though, the NIL accuracy scores are low, indicating that systems had trouble recognizing when there was no answer in the doc-ument collection."
The TREC QA track is a comparative evaluation.
"In a comparative evaluation, each of two methods is used to solve a common sample set of problems, and the meth-ods’ output is scored using some evaluation metric."
The method whose output produces a better evaluation score is assumed to be the more effective method.
An important feature of a comparative evaluation is that only relative scores are required.
"In other words, the only requirement of the evaluation methodology for a comparative evalua-tion is that it reliably rank better methods ahead of worse methods."
The remainder of this paper examines the question of whether the QA task defined above reliably ranks sys-tems.
The first aspect of the investigation examines whether human assessors can recognize exact answers.
"The evidence suggests that they can, though the differ-ences of opinion as to correctness observed in earlier QA tracks remain."
The second part of the investigation looks at the effect the differences of opinion have on rankings of systems given that there is only response per question and the evaluation metric emphasizes the systems’ ranking of questions by confidence.
The final aspect of the investi-gation addresses the sensitivity of the evaluation.
"While evaluation scores can be computed to an arbitrary number of decimal places, not all differences are meaningful."
The sensitivity analysis empirically determines the minimum difference in scores required to have a small probability of error in concluding that one system is better than the other.
"While the idea of an exact answer is intuitively obvi-ous, it is very difficult to formally define."
"As with correct-ness, exactness is essentially a personal opinion."
Thus whether or not an answer is exact is ultimately up to the assessor.
NIST did provide guidelines to the assessors regarding exactness.
The guidelines stated that exact an-swers need not be the most minimal response possible.
"For example, “Mississippi river” should be accepted as exact for the Big Muddy question despite the fact that “river” is redundant since all correct responses must be a river."
"The guidelines also suggested that ungrammatical responses are generally not exact; a location question can have “in Mississippi” as an exact answer, but not “Mis-sissippi in”."
The guidelines also emphasized that even “quality” responses—strings that contained both a cor-rect answer and justification for that answer—were to be considered inexact for the purposes of this evaluation.
"To test whether assessors consistently recognize ex-act answers, each question was independently judged by three different assessors."
"Note, however, that there were only 3725 pairs that had at least one judge assign a judg-ment that was something other than ‘wrong’."
"Thus, there was some disagreement among the judges for half of all responses that were not obviously wrong."
Table 2 shows the distribution of the assessors’ dis-agreements.
Each response pair is associated with a triple of judgments according to the three judgments assigned by the different assessors.
"In the table the judgments are denoted by W for wrong, R for right, U for unsupported, and X for inexact."
"The table shows the number of pairs that are associated with each triple, plus the percentage of the total number of disagreements that that triple rep-resents."
The largest number of disagreements involves right and inexact judgments: the RRX and RXX combinations account for a third of the total disagreements.
Fortunately inspection of these disagreements reveals that they do not in general represent a new category of disagreement.
"In-stead, many of the granularity differences observed in earlier QA judgment sets[REF_CITE]are now reflected in this distinction."
"For example, a correct response for Who is Tom Cruise married to? is Nicole Kidman."
"Some assessors accepted just “Kidman”, but others marked “Kidman” as inexact."
"Some assessors also accepted “actress Nicole Kidman”, which some rejected as inexact."
Similar issues arose with dates and place names.
"For dates and quantities, there was disagreement whether slightly off responses are wrong or inexact."
"For example, when the correct response is[REF_CITE]is[REF_CITE]wrong or inexact?"
This last distinction doesn’t matter very much in practice since in either case the response is not right.
The TREC-8 track demonstrated that QA evaluation re-sults based on text snippets and mean reciprocal rank scoring is stable despite differences in assessor opin-ions[REF_CITE].
"Given that the exact an-swer judgments reflect these same differences of opinion, are confidence-weighted scores computed over only one response per question also stable?"
We repeat the test for stability used in TREC-8 to answer this question.
"The three assessors who judged a question were arbi-trarily assigned as assessor 1, assessor 2, or assessor 3."
"The assessor 1 judgments for all questions were gathered into judgment set 1, the assessor 2 judgments into judg-ment set 2, and the assessor 3 judgments into judgment set 3."
"These three judgment sets were combined through adjudication into a final judgment set, which is the judg-ment set used to produce the official[REF_CITE]scores."
Each run was scored using each of the four judgment sets.
"For each judgment set, the runs were ranked in or-der from most effective to least effective using either the confidence-weightedscore or the raw number of correctly answered questions."
The distance between two rankings of runs was computed using a correlation measure based on Kendall’s[REF_CITE].
Kendall’s computes the distance between two rankings as the minimum number of pairwise adjacent swaps to turn one ranking into the other.
"The distance is normalized by the number of items being ranked such that two identical rankings produce a correlation of , the correlation between a ranking and its perfect inverse is , and the expected correlation of two rankings chosen at random is ."
Table 3 gives the correlations between all pairs of rankings for both evalu-ation metrics.
"The average correlation with the adjudicated ranking for the TREC-8 results was 0.956;[REF_CITE]where two assessors judged each question, the average correla-tion was 0.967."
The correlations for the exact answer case are somewhat smaller: the average correlation is 0.930 for the confidence-weighted score and 0.945 for the raw count of number correct.
"Correlations are slightly higher for the adjudicated judgment set, probably because the adjudicated set has a very small incidence of errors."
The higher correlation for the raw count measure likely re-flects the fact that the confidence-weighted score is much more sensitive to differences in judgments for questions at small (close to one) ranks.
Smaller correlations between system rankings indicate that comparative results are less stable.
It is not surprising that an evaluation based on one response per question is less stable than an evaluation based on five responses per question—there is inherently less information included in the evaluation.
At issue is whether the rankings are sta-ble enough to have confidence in the evaluation results.
It would be nice to have a critical value for such that cor-relations greater than the critical value guarantee a quality evaluation.
"Unfortunately, no such value can exist since values depend on the set of runs being compared."
"In prac-tice, we have considered correlations greater than 0.9 to be acceptable[REF_CITE], so both evaluating using the confidence-weighted score and evaluating using the raw count of number correct are sufficiently stable."
The vast majority of “swaps” (pairs of run such that one member of the pair evaluates as better under one eval-uation condition while the other evaluates as better under the alternate condition) that occur when using different human assessors involve systems whose scores are very similar.
There is a total of 177 swaps that occur when the three one-judge rankings are compared with the adjudi-cated ranking when using the confidence-weighted score.
"Only 4 of the 177 swaps involve pairs of runs whose dif-ference in scores, , is at least 0.05 as computed using the adjudicated judgment set, and there are no swaps when is at least 0.07."
"As will be shown in the next section, runs with scores that are this similar should be assumed to be equally effective, so some swapping is to be expected."
Human judgments are not the only source of variability when evaluating QA systems.
"As is true with document retrieval systems, QA system effectiveness depends on the questions that are asked, so the particular set of ques-tions included in a test set will affect evaluation results."
"Since the test set of questions is assumed to be a ran-dom sample of the universe of possible questions, there is always some chance that a comparison of two systems using any given test set will lead to the wrong conclusion."
"The probability of an error can be made arbitrarily small by using arbitrarily many questions, but there are practi-cal limits to the number of questions that can be included in an evaluation."
"Following our work for document retrieval evalua-ti[REF_CITE], we can use the runs submitted to the QA track to empirically determine the relationship between the number of questions in a test set, the observed difference in scores ( ), and the likelihood that a single comparison of two QA runs leads to the cor-rect conclusion."
"Once established, the relationship can be used to derive the minimum difference in scores required for a certain level of confidence in the results given there are 500 questions in the test set."
The core of the procedure is comparing the effective-ness of a pair runs on two disjoint question sets of equal size to see if the two sets disagree as to which of the runs is better.
We define the error rate as the percentage of comparisons that result in a swap.
"Since the QA track used 500 questions, we can directly compute the error rate for question set sizes up to 250 questions."
"By fitting curves to the values observed for question set sizes up to 250, we can extrapolate the error rates to question sets up to 500 questions."
"When calculating the error rate, the difference between two runs’ confidence-weighted scores is categorized into one of 21 bins based on the size of the difference."
The first bin contains runs with a difference of less than 0.01 (including no difference at all).
The next bin contains runs whose difference is at least 0.01 but less than 0.02.
"The limits for the remaining bins increase by increments of 0.01, with the last bin containing all runs with a differ-ence of at least 0.2."
The requirement that the question sets be disjoint en-sures that the comparisons are made on independent sam-ples of the space of questions.
"That is, we assume a uni-verse of all possible closed-class questions, and an (un-known) probability distribution of the scores for each of the two runs."
We also assume that the set of questions used in the[REF_CITE]QA track is a random sample of the universe of questions.
"A random selection from the TREC question set gives a random, paired selection from each of the runs’ confidence-weightedscore distributions."
"We take one random sample as a base case, and a differ-ent random sample (the disjoint sets) as the test case to see if the results agree."
Each question set size from 1 to 250 is treated as a sep-arate experiment.
"Within an experiment, we randomly se-lect two disjoint sets of questions of the required size."
"We compute the confidence-weighted score over both ques-tion sets for all runs, then count the number of times we see a swap for all pairs of runs using the bins to segre-gate the counts by size of the difference in scores."
"The entire procedure is repeated 10 times (i.e., we perform 10 trials), with the counts of the number of swaps kept as running totals over all trials 2 ."
The ratio of the number of swaps to the total number of cases that land in a bin is the error rate for that bin.
The error rates computed from this procedure are then used to fit curves of the form  where and  are parameters to be estimated   and is the size of the question set.
A different curve is fit for each different bin.
The input to the curve-fitting proce-dure used only question set sizes greater than 20 since smaller question set sizes are both uninteresting and very noisy.
"Curves could not be fit for the first bin (differences less than .01), for the same reason, or for bins where dif-ferences were greater than 0.16."
Curves could not be fit for large differences because too much of the curve is in the long flat tail.
The resulting extrapolated error rate curves are plot-ted in Figure 2.
"In the figure, the question set size is plotted on the x-axis and the error rate is plotted on the y-axis."
The error rate for 500 questions when a differ-ence of 0.05 in confidence-weighted scores is observed is approximately 8 %.
"That is, if we know nothing about two systems except their scores which differ by 0.05, and if we repeat the experiment on 100 different sets of 500 questions, then on average we can expect 8 out of those 100 sets to favor one system while the remaining 92 to favor the other."
"The horizontal line in the graph in Figure 2 is drawn at an error rate of 5 %, a level of confidence commonly used in experimental designs."
"For question set sizes of 500 questions, there needs to be an absolute difference of at least 0.07 in confidence-weighted scores before the error rate is less than 5 %."
"Using the 5 % error rate standard, the pris2002,[REF_CITE]D1, and IBMPQSQACYC runs from Table 1 should be considered equivalently effective, as should the uwmtB3,[REF_CITE]C, isi02, limsiQalir[Footnote_2], and ali2002b runs."
"2 While the two question sets used within any one trial are disjoint, and thus independent samples, the question sets across trials are drawn from the same initial set of 500 questions and thus overlap. Because the question sets among the different"
"Evaluating natural language processing technology is critical to advancing the state of the art, but also con-sumes significant resources."
It is therefore important to validate new evaluation tasks and to establish the bound-aries of what can legitimately be concluded from the eval-uation.
This paper presented an assessment of the task in the[REF_CITE]QA track.
"While the task in earlier QA tracks had already been validated, changes to the 2002 task were significant enough to warrant further examination."
"In particular, the 2002 task required systems to return exact answers, to re-turn one response per question, and to rank questions by confidence in the response; the evaluation metric empha-sized the ranking."
Each of these changes could increase the variability in the evaluation as compared to the earlier task.
"Examination of the track results did show some in-crease in variability, but also confirmed that system com-parisons are sufficiently stable for an effective evaluation."
"Human assessors do not always agree as to whether an an-swer is exact, but the differences reflect the well-known differences in opinion as to correctness rather than inher-ent difficulty in recognizing whether an answer is exact."
"The confidence-weighted score is sensitive to changes in judgments for questions that are ranked highly, and there-fore is a less stable measure than a raw count of num-ber correct."
"Nonetheless, all of the observed inversions in confidence-weighted scores when systems were evalu-ated using different judgment sets were between systems whose scores differed by less than 0.07, the smallest dif-ference for which the error rate of concluding two runs are different is less than 5 % for test sets of 500 ques-tions."
"A major part of the cost an evaluation is building the necessary evaluation infrastructure such as training mate-rials, scoring procedures, and judgment sets."
The net cost of an evaluation is greatly reduced if such infrastructure is reusable since the initial costs are amortized over many additional users.
Reusable infrastructure also accelerates the pace of technological advancement since it allows re-searchers to run their own experiments and receive rapid feedback as to the quality of alternative methods.
"Un-fortunately, neither the initial task within the TREC QA track nor the[REF_CITE]task produces a reusable QA test collection."
"That is, it is not currently possible to use the judgment set produced during TREC to accu-rately evaluate a QA run that uses the same document and question sets as the TREC runs but was not judged by the human assessors."
"Methods for approximating evalua-tion scores exist[REF_CITE], but they are not completely reliable."
A key area for future work is to devise a truly reusable QA evalua-tion infrastructure.
The standard metrics for evaluation of the output of NLP systems are precision and recall.
"Given an ar-guably correct list of the units that a system would identify if it performed perfectly, there should in principle be no discrepancy between the units identi-fied by a system and the units that are either useful in a particular application or are preferred by human beings for use in a particular task."
"But when the satis-factory output can take many different forms, as in summarization and generation, evaluation by preci-sion and recall is not sufficient."
"In these cases, the challenge for system designers and users is to effec- tively distinguish between systems that provide gen-erally satisfactory output and systems that do not."
NP chunks[REF_CITE]and technical terms[REF_CITE]fall into this difficult-to-assess category.
NPs are recursive structures.
"For the maximal NP large number of recent newspaper articles on biomedical science and clinical practice, a full-fledged parser would legitimately identify (at least) seven NPs in addition to the maximal one: large number; recent newspaper articles; large number of recent newspaper articles; biomedical science; clini-cal practice; biomedical science and clinical prac-tice; and recent newspaper articles on biomedical science and clinical practice."
"To evaluate the per-formance of a parser, NP chunks can usefully be evaluated by a gold standard; many systems (e.g.,[REF_CITE]and[REF_CITE]) use the Penn Treebank for this type of evalua-tion."
"But for most applications, output that lists a maximal NP and each of its component NPs is bulky and redundant."
Even a system that achieves 100% precision and recall in identifying all of the NPs in a document needs criteria for determining which units to use in different contexts or applications.
Technical terms are a subset of NP chunks.
"Jac-quemin (2001:3) defines terms as multi-word “vehi-cles of scientific and technical information”. [Footnote_1] The operational difficulty, of course, is to decide whether a specific term is a vehicle of scientific and technical information (e.g., birth date or light truck)."
1 Jacquemin does not use the modifier technical.
Evalua-tion of mechanisms that filter out some terms while retaining others is subject to this difficulty.
This is exactly the kind of case where context plays a sig-nificant role in deciding whether a term conforms to a definition and where experts disagree.
"In this paper, we turn to an information access task in order to assess terms identified by different techniques."
"There are two basic types of information access mechanisms, searching and browsing."
"In searching, the user generates the search terms; in browsing, the user recognizes potentially useful terms from a list of terms presented by the system."
"When an information seeker can readily think up a suitable term or linguistic expression to represent the informa-tion need, direct searching of text by user-generated terms is faster and more effective than browsing."
"However, when users do not know (or can’t remem-ber) the exact expression used in relevant documents, they necessarily struggle to find relevant information in full-text search systems."
Experimental studies have repeatedly shown that information seekers use many different terms to describe the same concept and few of these terms are used frequently[REF_CITE].
"When in-formation seekers are unable to figure out the term used to describe a concept in a relevant document, electronic indexes are required for successful infor-mation access."
NP chunks and technical terms have been pro-posed for use in this task[REF_CITE].
NP chunks and technical terms have also been used in phrase browsing and phrase hierarchies ([REF_CITE];
"In fact, the distinction between task-based evaluation of a system and preci-sion/recall evaluation of the quality of system output is similar to the extrinsic/intrinsic evaluation of summarizati[REF_CITE]."
"In order to focus on the subjects’ choice of index terms rather than on other aspects of the information access process, we asked subject to find answers to questions in a college level text book."
Subjects used the Experimental Searching and Browsing Interface (ESBI) to browse a list of terms that were identified by different techniques and then merged.
Subjects select an index term by clicking on it in order to hy-perlink to the text itself.
"By design, ESBI forces the subjects to access the text indirectly, by searching and browsing the list of index terms, rather than by direct searching of the text."
Three sets of terms were used in the experiment: one set (HS) was identified using the head-sorting method[REF_CITE]; the second set (TT) was identified by an implementation of the technical term algorithm[REF_CITE]; a third set (HUM) was created by a human indexer.
The methods for identifying these terms will be discussed in greater detail below.
"Somewhat to our surprise, subjects displayed a very strong preference for the index terms that were identified by the human indexer."
"Table 1 shows that when measured by percentage terms selected, sub-jects chose over 13% of the available human terms, but only 1.73% and 1.43% of the automatically se- lected terms; by this measure the subjects’ preference for the human terms was more than 7 times greater than the preference for either of the automatic tech-niques. (In Table 1 and in the rest of this paper, all index term counts are by type rather than by token, unless otherwise indicated.)"
This initial experiment strongly indicates that 1) peo-ple have a demonstrable preference for different types of index terms; 2) these human terms are a very good gold standard.
"If subjects use a greater propor-tion of the terms identified by a particular technique, the terms can be judged better than the terms identi-fied by another technique, even if the terms are dif-ferent."
Any automatic technique capable of identifying terms that are preferred over these human terms would be a very strong system indeed.
"Fur-thermore, the properties of the terms preferred by the experimental subjects can be used to guide design of systems for identifying and selecting NP chunks and technical terms."
"In the next section, we describe the design of the experiment and in Section 3, we report on what the experimental data shows about human preferences for different kinds of index terms."
Our experiment assesses the index terms vis a vis their usefulness in a strictly controlled information access task.
"Subjects responded to a set of questions whose answers were contained in a 350 page college-level text (Rice, Ronald E., McCreadie, Maureen and Chang, Shan-ju L. (2001) Accessing and Browsing Information and Communication."
"Cambridge, MA: MIT Press.)"
Subjects used the Experimental Search-ing and Browsing Interface (ESBI) which forces them to access text via the index terms; direct text searching was prohibited. 25 subjects participated in the experiment; they were undergraduate and gradu-ate students at Rutgers University.
"The experiments were conducted by graduate students at the Rutgers University School of Communication, Information and Library Studies (SCILS)."
Subjects used the Experimental Searching and Browsing Interface (ESBI) to find the answers to the questions.
"After an initial training session, ESBI pre-sents the user with a Search/Browse screen (not shown); the question appears at the top of the screen."
"The subject may enter a string to search for in the index, or click on the &quot;Browse&quot; button for access to the whole index."
"At this point, &quot;search&quot; and &quot;browse&quot; apply only to the list of index terms, not to the text."
The user may either browse the entire list of index terms or may enter a search term and specify criteria to select the subset of terms that will be returned.
Most people begin with the latter option because the complete list of index terms is too long to be easily browsed.
The user may select (click on) an index term to view a list of the contexts in which the term appears.
"If the context appears useful, the user may choose to view the term in its full context; if not, the user may either do additional browsing or start the process over again."
Figure 1 shows a screen shot of ESBI after the searcher has entered the string democracy in the search box.
This view shows the demo question and the workspace for entering answers.
The string was (previously) entered in the search box and all index terms that include the word democracy are displayed.
"Although it is not illustrated here, ESBI also permits substring searching and the option to specify case sensitivity."
"Regardless of the technique by which the term was identified, terms are organized by grammatical head of the phrase."
"Preliminary analysis of our results has shown that most subjects like this analysis, which resembles standard organization of back-of-the-book indexes."
"Readers may notice that the word participation appears at the left-most margin, where it represents the set of terms whose head is participation."
The in-dented occurrence represents the individual term.
Selecting the left-most occurrence brings up contexts for all phrases for which participation is a head.
Se-lecting on the indented occurrence brings up contexts for the noun participation only when it is not part of a larger phrase.
This is explained to subjects during the pre-experimental training and an experimenter is present to remind subjects of this distinction if a question arises during the experiment.
"Readers may also notice that in Figure 1, one of the terms, participation require, is ungrammatical."
This particular error was caused by a faulty part-of-speech tag.
"But since automatically identified index terms typically include some nonsensical terms, we have left these terms in – these terms are one of the problems that information seekers have to cope with in a realistic task-based evaluation."
"After conducting initial testing to find out what types of questions subjects founder hard or easy, we spent considerable effort to design a set of 26 questions of varying degrees of difficulty."
"To obtain an initial assessment of difficulty, one of the experimenters used ESBI to answer all of the questions and rate each question with regard to how difficult it was to answer using the ESBI system."
"For example, the question What are the characteristics of Marchionini&apos;s model of browsing? was rated very easy because searching on the string marchionini reveals an index term Marchionini&apos;s which is linked to the text sentence: Marchionini&apos;s model of browsing considers five interactions among the information-seeking factors of &quot;task, domain, setting, user charac-teristics and experience, and system content and in-terface&quot; (p.107)."
"The question What factors determine when users decide to stop browsing? was rated very difficult because searching on stop (or synonyms such as halt, cease, end, terminate, finish, etc.) reveals no helpful index terms, while searching on factors or browsing yields an avalanche of over 500 terms, none with any obvious relevance."
"After subjects finished answering each question, they were asked to rate the question in terms of its difficulty."
"A positive correlation between judgments of the experimenters and the experimental subjects (Sharp et al., under submission) confirmed that we had successfully devised questions with a range of difficulty."
"In general, questions that included terms actually used in the index were judged easier; ques-tions where the user had to devise the index terms were judged harder."
"To avoid effects of user learning, questions were presented to subjects in random order; in the one hour experiment, subjects answered an average of about 9 questions."
"Although the primary goal of this research is to point the way to improved techniques for automatic crea-tion of index terms, we used human created terms to create a baseline."
"For the human index terms, we used the pre-existing back-of-the-book index, which we believe to be of high quality. [Footnote_2]"
2 Jim Snow prepared the index under the supervision of SCILS Professor James D. Anderson.
The two techniques for automatic identification were the technical terms algorithm[REF_CITE]and the head sorting method ([REF_CITE].
"In the implemen-tation of the Justeson and Katz’ algorithm, technical terms are multi-word NPs repeated above some threshold in a corpus; in the head sorting method, technical terms are identified by grouping noun phrases with a common head (e.g., health-care work-ers and asbestos workers), and selecting as terms those NPs whose heads appear in two or more phrases."
"Definitionally, technical terms are a proper subset of terms identified by Head Sorting."
"Differ-ences in the implementations, especially the pre-processing module, result in there being some terms identified by Termer that were not identified by Head Sorting."
"Table 2 shows the number of terms identified by each method. (*Because some terms are identified by more than one technique, the percentage adds up to more than 100%.)"
The fewest terms (673) were iden-tified by the human method; in part this reflects the judgment of the indexer and in part it is a result of restrictions on index length in a printed text.
The largest number of terms (7980) was identified by the head sorting method.
"This is because it applies looser criteria for determining a term than does the Justeson and Katz algorithm which imposes a very strict standard--no single word can be considered a term, and an NP must be repeated in full to be con-sidered a term."
"However, the fact that subjects assigned a high rank to many of the terms identified by Head Sorting suggested that the technical term algorithm was failing to pick up many potentially useful index terms."
"In preparation for the experiment, all index terms were merged into a single list and duplicates were removed, resulting in a list of nearly 10,000 index terms."
"In the experiment, we logged the terms that sub-jects searched for (i.e., entered in a search box) and selected."
"In this paper, we report only on the terms that the subjects selected (i.e., clicked on)."
"This is because if a subject entered a single word, or a sub-part of a word in the search box, ESBI returned to them a list of index terms; the subject then selected a term to view the context in which it appears in the text."
This term might have been the same term origi-nally searched for or it might have been a super-string.
"The terms that subjects selected for searching are interesting in their own right, but are not analyzed here."
"At the outset of this experiment, we did not know whether it would be possible to discover differences in human preferences for terms in the information access task reported on in this paper."
We therefore started our research with the null hypothesis that all index terms are created equal.
"If users selected index terms in roughly the same proportion as the terms occur in the text, the null hypothesis would be proven."
The results strongly discredit the null hypothesis.
"Table 3 shows that when measured by percentage of terms selected, subjects selected on over 13.2% of the available human terms, but only 1.73% and 1.43% respectively of the automatically selected terms."
"Ta-ble 3 also shows that although the human index terms formed only 6% of the total number of index terms, 40% of the terms which were selected by subjects in order to view the context were identified by human indexing."
"To determine whether the numbers represent statisti-cally significant evidence that the null hypothesis is wrong, we represent the null hypothesis (H T) ) as (1) and the falsification of the null hypothesis (H A ) as (2)."
"H T : P 1 /µ 1 = P 2 /µ 2 (1) H A: P 1 /µ 1 ≠ P 2 /µ 2 (2) P i is the expected percentage of the selected terms that are type i in all the selected terms; µ i is the ex-pected percentage if there is no user preference, i.e. the proportion of this term type i in all the terms."
We rewrite the above as (3).
"H T : X = 0 H A : X ≠ 0 X = P 1 /µ 1 ─ P 2 /µ 2 (3) Assuming that X is normally distributed, we can use a one-sample t test on X to decide whether to accept the hypothesis (1)."
The two-tailed t test (df =222) produces a p-value of less than .01% for the compari-son of the expected and selected proportions of a) human terms and head sorted terms and b) human terms and technical terms.
"In contrast, the p-value for the comparison of head-sorted and technical terms was 33.7%, so we draw no conclusions about relative preferences for head sorted and technical terms."
"We also considered the possibility that our formu-lation of questions biased the terms that the subjects selected, perhaps because the words of the questions overlapped more with the terms selected by one of the methods. 3 We took the following steps: 1) For each search word, calculate the number of terms overlapping with it from each source. 2) Based on these numbers, determine the proportion of terms provided by each method. 3) Sum the proportions of all the search words."
"As measured by the terms the subjects saw during browsing, 22% were human terms, 62% were head sorted terms and 16% were technical terms."
"Using the same reasoning about the null hypothesis as above, the p-value for the comparison of the ratios of human and head sorted terms was less than 0.01%, as was the comparison of the ratios of the human and techni-cal terms."
This supports the validity of the results of the initial test.
"In contrast, the p-value for the com-parison of the two automatic techniques was 77.3%."
Why did the subjects demonstrate such a strong preference for the human terms?
Table 4 illustrates some important differences between the human terms and the automatically identified terms.
"The terms selected on are longer, as measured in number of words, and more complex, as measured by number of prepositions per index terms and by number of con-tent-bearing words."
"As shown in Table 5, the differ-ence of these complexity measures between human terms and automatically identified terms are statisti-cally significant."
"Since longer terms are more specific than shorter terms (for example, participation in a democracy is longer and more specific than democracy), the results suggest that subjects prefer the more specific terms."
"If this result is upheld in future research, it has practi-cal implications for the design of automatic term identification systems."
"In this paper, our primary focus is on the question of what makes index terms &apos;better&apos;, as measured by user preferences in a question-answering task."
"Also of interest, of course, is what makes index terms &apos;better&apos; in terms of how accurate the resulting users&apos; answers are."
The problem is that any facile judgment of free-text answer accuracy is bound to be arbitrary and potentially unreliable; we discuss this in detail in [26].
"Nevertheless, we address the issue in a prelimi-nary way in the current paper."
"We used an ad hoc set of canonical answers to score subjects&apos; answers on a scale of 1 to 3, where 1 stands for &apos;very accurate&apos;, 2 stands for &apos;partly accurate&apos; and 3 represents &apos;not at all accurate&apos;."
"Using general loglinear regression (Poisson model) under the hypothesis that these two variables are independent of each other, our analysis showed that there is a systematic relationship (significance probability is 0.0504) between source of selected terms and answer accuracy."
"Specifically, in cases where subjects used more index terms identified by the human indexer, the answers were more accurate."
"On the basis of our initial accuracy judgments, we can therefore draw the preliminary conclusion that terms that were better in that they were preferred by the experimental subjects were also better in that they were associated with better answers."
We plan to con-duct a more in-depth analysis of answer accuracy and will report on it in future work.
But the primary question addressed in this paper is how to reliably assess NP chunks and technical terms.
"These results constitute experimental evidence that the index terms identified by the human indexer constitute a gold standard, at least for the text used in the experiment."
"Any set of index terms, regardless of the technique by which they were created or the crite-ria by they were selected, can be compared vis a vis their usefulness in the information access task."
The contribution of this paper is the description of a task-based gold-standard method for evaluating the usefulness and therefore the quality of NP chunks and technical terms.
"In this section, we address a number of questions about this method. 1) What properties of terms can this technique be used to study? • One word or many."
There are two parts to the process of identifying NP terms: NP chunks that are candidate terms must be identified and candidate terms must be fil-tered in order to select a subset appropriate for use in the intended application.
A byproduct of this technique is that single-word terms are excluded.
"In part, this is be-cause it is much harder to determine in con-text which single words actually qualify as terms."
"But dictionaries of technical termi-nology have many one-word terms. • Simplex or complex NPs (e.g.,[REF_CITE]) identify simplex or base NPs – NPs which do not have any component NPs -- at least in part because this bypasses the need to solve the quite difficult attachment prob-lem, i.e., to determine which simpler NPs should be combined to output a more com-plex NP."
"But if people find complex NPs more useful than simpler ones, it is impor-tant to focus on improvement of techniques to reliably identify more complex terms. • Semantic and syntactic terms variants."
But independent of the question of how to recognize variants is the question of which variants are to be preferred for differ-ent kinds of uses. • Impact of errors.
Real-world NLP systems have a measurable error rate.
"By conducting experiments in which terms with errors are include in the set of test terms, the impact of these errors can be measured."
"The useful-ness of a set of terms presumably is at least in part a function of the impact of the errors, whether the errors are a by-product of the algorithm or the implementation of the algo-rithm. 2) Could the set of human index terms be used as a gold standard without conducting the human subject experiments?"
"This of course could be done, but then the terms are being evaluated by a fixed standard – by definition, no set of terms can do better than the gold standard."
This experimental method leaves open the possi-bility that there is a set of terms that is better than the gold standard.
"In this case, of course, the gold standard would no longer be a gold standard -- perhaps we would have to call it a platinum standard. 3) How reproducible is the experiment?"
The ex-periment can be re-run with any set of terms deemed to be representative of the content of the Rice text.
The preparation of the materials for additional texts is admittedly time-consuming.
But over time a sizable corpus of experimental materials in different domains could be built up.
These materials could be used for training as well as for testing. 4) How extensible is the gold standard?
The ex-perimental protocol will be validated only if equally useful index terms can be created for other texts.
We anticipate that they can. 5) How can this research help in the design of real world NLP systems?
This technique can help in assessing the relative usefulness of exist-ing techniques for identifying terms.
"It is possi-ble, for example, there already exist techniques for identifying terms that are superior to the two tested here."
"If we can find such systems, their al-gorithms should be preferred."
"If not, there re-mains a need for development of algorithms to identify single word terms and complex phrases. 6) Do the benefits of this evaluation technique outweigh the costs?"
"Given the fundamental dif-ficulty of evaluating NP chunks and technical terms, task-based evaluation is a promising sup-plement to evaluation by precision and recall."
"These relatively time-consuming human subject experiments surely will not be undertaken by most system developers; ideally, they should be performed by neutral parties who do not have a stake in the outcome. 7) Should automated indexes try to imitate hu-man indexers?"
Automated indexes should con-tain terms that are most easily processed by users.
"If the properties of such terms can be re-liably discovered, developers of systems that identify terms intended to be processed by peo-ple surely should pay attention."
In this paper we have reported on a rigorous experi-mental technique for black-box evaluation of the use-fulness of NP chunks and technical terms in an information access task.
Our experiment shows that it is possible to reliably identify human preferences for sets of terms.
The set of human terms created for use in a back-of-the-book index serves as a gold standard.
An ad-vantage of the task-based evaluation is that a set of terms could outperform the gold standard; any system that could do this would be a good system indeed.
The two automatic methods that we evaluated performed much less well than the terms created by the human indexer; we plan to evaluate additional techniques for term identification in the hope of iden-tifying automatic methods that identify index terms that people prefer over the human terms.
"We also plan to prepare test materials in different domains, and assess in greater depth the properties of the terms that our experimental subjects preferred; our goal is to develop practical guidelines for the identification and selection of technical terms that are optimal for human users."
We will also study the impact of se-mantic differences between terms on user preferences and investigate whether terms which are preferred for information access are equally suitable for other NLP tasks.
"We are grateful to the other members of the Rutgers NLP-I research group, Lu Liu, Mark Sharp, and Xiaojun Yuan, for their valuable contribution to this project."
"We also thank Paul Kantor, Judith L. Kla-vans, Evelyne Tzoukermann , Min Yen Kan, and three anonymous reviewers for their helpful sugges-tions."
Funding for this research has been provided by the Rutgers University Information Science and Technology Council.
This paper describes an unsupervised algo-rithm for placing unknown words into a taxon-omy and evaluates its accuracy on a large and varied sample of words.
"The algorithm works by first using a large corpus to find semantic neighbors of the unknown word, which we ac-complish by combining latent semantic analy-sis with part-of-speech information."
"We then place the unknown word in the part of the tax-onomy where these neighbors are most concen-trated, using a class-labelling algorithm devel-oped especially for this task."
"This method is used to reconstruct parts of the existing Word-Net database, obtaining results for common nouns, proper nouns and verbs."
We evaluate the contribution made by part-of-speech tag-ging and show that automatic filtering using the class-labelling algorithm gives a fourfold im-provement in accuracy.
"The importance of automatic methods for enriching lex-icons, taxonomies and knowledge bases from free text is well-recognized."
"For rapidly changing domains such as current affairs, static knowledge bases are inadequate for responding to new developments, and the cost of building and maintaining resources by hand is prohibitive."
This paper describes experiments which develop auto-matic methods for taking an original taxonomy as a skele-ton and fleshing it out with new terms which are discov-ered in free text.
The method is completely automatic and it is completely unsupervised apart from using the origi-nal taxonomic skeleton to suggest possible classifications for new terms.
We evaluate how accurately our meth-ods can reconstruct the WordNet taxonomy[REF_CITE].
The problem of enriching the lexical information in a taxonomy can be posed in two complementary ways.
"Firstly, given a particular taxonomic class (such as fruit) one could seek members of this class (such as apple, ba-nana)."
"This problem is addressed[REF_CITE],[REF_CITE]and more recently[REF_CITE]."
"Secondly, given a partic-ular word (such as apple), one could seek suitable tax-onomic classes for describing this object (such as fruit, foodstuff)."
The work in this paper addresses the second of these questions.
The goal of automatically placing new words into a taxonomy has been attempted in various ways for at least ten years[REF_CITE].
"The process for placing a word w in a taxonomy T using a corpus C often contains some version of the following stages: • For a word w, find words from the corpus C whose occurrences are similar to those of w. Consider these the ‘corpus-derived neighbors’ N(w) of w. • Assuming that at least some of these neighbors are already in the taxonomy T, map w to the place in the taxonomy where these neighbors are most con-centrated."
"A more recent example is the top-down algorithm[REF_CITE], which seeks the node in T which shares the most collocational properties with the word w, adding 42 concepts taken from The Lord of the Rings with an accuracy of 28%."
The algorithm as presented above leaves many degrees of freedom and open questions.
What methods should be used to obtain the corpus-derived neighbors N(w)?
This question is addressed in Section 2.
"Given a col-lection of neighbors, how should we define a “place in the taxonomy where these neighbors are most concen-trated?”"
"This question is addressed in Section 3, which defines a robust class-labelling algorithm for mapping a list of words into a taxonomy."
"In Section 4 we describe experiments, determining the accuracy with which these methods can be used to reconstruct the WordNet taxon-omy."
"To our knowledge, this is the first such evaluation for a large sample of words."
Section 5 discusses related work and other problems to which these techniques can be adapted. 2 Finding semantic neighbors:
Combining latent semantic analysis with part-of-speech information.
"There are many empirical techniques for recognizing when words are similar in meaning, rooted in the idea that “you shall know a word by the company it keeps”[REF_CITE]."
"It is certainly the case that words which repeat-edly occur with similar companions often have related meanings, and common features used for determining this similarity include shared collocations[REF_CITE], co-occurrence in lists of objects[REF_CITE]and latent semantic analysis[REF_CITE]."
"The method used to obtain semantic neighbors in our experiments was a version of latent semantic analysis, descended from that used by Hearst and Schütze (1993, §4)."
Other words were assigned co-ordinates determined by the number of times they oc-cured within the same context-window (15 words) as one of the 1000 column-label words in a large corpus.
"This gave a matrix where every word is represented by a row-vector determined by its co-occurence with frequently oc-curing, meaningful words."
"Since this matrix was very sparse, singular value decomposition (known in this con-text as latent semantic analysis[REF_CITE]) was used to reduce the number of dimensions from 1000 to 100."
This reduced vector space is called WordSpace ([REF_CITE]§4).
Similarity between words was then computed using the cosine sim-ilarity measure ([REF_CITE]p. 28).
"Such techniques for measuring similarity between words have been shown to capture semantic properties: for example, they have been used successfully for recog-nizing synonymy[REF_CITE]and for finding correct translations of individual terms[REF_CITE]."
"The corpus used for these experiments was the British National Corpus, which is tagged for parts-of-speech."
"This enabled us to build syntactic distinctions into WordSpace — instead of just giving a vector for the string test we were able to build separate vectors for the nouns, verbs and adjectives test."
An example of the contribu- tion of part-of-speech information to extracting seman-tic neighbors of the word fire is shown in Table 2.
"As can be seen, the noun fire (as in the substance/element) and the verb fire (mainly used to mean firing some sort of weapon) are related to quite different areas of mean-ing."
Building a single vector for the string fire confuses this distinction — the neighbors of fire treated just as a string include words related to both the meaning of fire as a noun (more frequent in the BNC) and as a verb.
Part of the goal of our experiments was to investi-gate the contribution that this part-of-speech information made for mapping words into taxonomies.
"As far as we are aware, these experiments are the first to investigate the combination of latent semantic indexing with part-of-speech information."
"Given a collection of words or multiword expressions which are semantically related, it is often important to know what these words have in common."
"All adults with normal language competence and world knowledge are adept at this task — we know that plant, animal and fun-gus are all living things, and that plant, factory and works are all kinds of buildings."
"This ability to classify objects, and to work out which of the possible classifications of a given object is appropriate in a particular context, is es-sential for understanding and reasoning about linguistic meaning."
We will refer to this process as class-labelling.
The approach demonstrated here uses a hand-built tax-onomy to assign class-labels to a collection of similar nouns.
"As with much work of this nature, the taxonomy used is WordNet (version 1.6), a freely-available broad-coverage lexical database for English[REF_CITE]."
"Our algorithm finds the hypernyms which subsume as many as possible of the original nouns, as closely as pos-sible [Footnote_1] ."
"1 Another method which could be used for class-labelling is given by the conceptual density algorithm[REF_CITE], which those authors applied to word-sense disambiguation. A different but related idea is presented[REF_CITE], who use a principle from information theory to model selectional preferences for verbs using differ-ent classes from a taxonomy. Their algorithm and goals are different from ours: we are looking for a single class-label for semantically related words, whereas for modelling selectional preferences several classes may be appropriate."
The concept v is said to be a hypernym of w if w is a kind of v. For this reason this sort of a taxonomy is sometimes referred to as an ‘ IS A hierarchy’.
"For ex-ample, the possible hypernyms given for the word oak in WordNet 1.6 are oak ⇒ wood ⇒ plant material ⇒ material, stuff ⇒ substance, matter ⇒ object, physical object ⇒ entity, something oak, oak tree ⇒ tree ⇒ woody plant, ligneous plant ⇒ vascular plant, tracheophyte ⇒ plant, flora, plant life ⇒ life form, organism, being, living thing ⇒ entity, something"
Let S be a set of nouns or verbs.
"If the word w ∈ S is recognized by WordNet, the WordNet taxonomy assigns to w an ordered set of hypernyms H(w)."
Consider the union
H = [ H(w). w∈S
This is the set of all hypernyms of any member of S.
Our intuition is that the most appropriate class-label for the set S is the hypernym h ∈ H which subsumes as many as possible of the members of S as closely as possible in the hierarchy.
"There is a trade-off here between sub-suming ‘as many as possible’ of the members of S, and subsuming them ‘as closely as possible’."
This line of rea-soning can be used to define a whole collection of ‘class-labelling algorithms’.
"For each w ∈ S and for each h ∈ H, define the affinity score function α(w, h) between w and h to be f(dist(w, h)) if h ∈ H(w) α(w, h) = (1) −g(w, h) if h ∈/ H(w), where dist(w, h) is a measure of the distance between w and h, f is some positive, monotonically decreasing func-tion, and g is some positive (possibly constant) function."
"The function f accords ‘positive points’ to h if h sub-sumes w, and the condition that f be monotonically de-creasing ensures that h gets more positive points the closer it is to w. The function g subtracts ‘penalty points’ if h does not subsume w."
"This function could depend in many ways on w and h — for example, there could be a smaller penalty if h is a very specific concept than if h is a very general concept."
"The distance measure dist(w,h) could take many forms, and there are already a number of distance mea-sures available to use with WordNet (Budanitsky and"
The easiest method for assigning a distance between words and their hypernyms is to count the num-ber of intervening levels in the taxonomy.
"This assumes that the distance in specificity between ontological levels is constant, which is of course not the case, a problem addressed[REF_CITE]."
"Given an appropriate affinity score, it is a simple matter to define the best class-label for a collection of objects."
"Definition1 Let S be a set of nouns, let H = S w∈S H(w) be the set of hypernymsof S and let α(w, h) be an affinity score function as defined in equation (1)."
"The best class-label h max (S) for S is the node h max ∈ H with the highest total affinity score summed over all the members of S, so h max is the node which gives the max-imum score max X α(w, h). h∈H w∈S"
"Since H is determined by S, h max is solely determined by the set S and the affinity score α."
"In the event that h max is not unique, it is customary to take the most specific class-label available."
"In theory, rather than a class-label for related strings, we would like one for related meanings — the concepts to which the strings refer."
"To implement this for a set of words, we alter our affinity score function α as follows."
Let C(w) be the set of concepts to which the word w could refer. (So each c ∈ C is a possible sense of w.)
"Then f(dist(c, h)) if h ∈ H(c) if h ∈/ H(c), (2) α(w,h) = max c∈C(w) −g(w, c)"
This implies that the ‘preferred-sense’ of w with respect to the possible subsumer h is the sense closest to h.
"In practice, our class-labelling algorithm implements this preference by computing the affinity score α(c, h) for all c ∈ C(w) and only using the best match."
"This selec-tive approach is much less noisy than simply averaging the probability mass of the word over each possible sense (the technique used[REF_CITE], for example)."
The precise choice of class-labelling algorithm depends on the functions f and g in the affinity score function α of equation (2).
"There is some tension here between being correct and being informative: ‘correct’ but unin-formative class-labels (such as entity, something) can be obtained easily by preferring nodes high up in the hier-archy, but since our goal in this work was to classify un-known words in an informative and accurate fashion, the functions f and g had to be chosen to give an appropriate balance."
"After a variety of heuristic tests, the function f was chosen to be 1 f = dist(w, h) 2 , where for the distance function dist(w, h) we chose the computationally simple method of counting the number of taxonomic levels between w and h (inclusively to avoid dividing by zero)."
For the penalty function g we chose the constant g = 0.25.
"The net effect of choosing the reciprocal-distance-squared and a small constant penalty function was that hypernyms close to the concept in question received mag-nified credit, but possible class-labels were not penalized too harshly for missing out a node."
This made the algo-rithm simple and robust to noise but with a strong prefer-ence for detailed information-bearing class-labels.
This configuration of the class-labelling algorithm was used in all the experiments described below.
"To test the success of our approach to placing unknown words into the WordNet taxonomy on a large and signif-icant sample, we designed the following experiment."
"If the algorithm is successful at placing unknown words in the correct new place in a taxonomy, we would expect it to place already known words in their current position."
"The experiment to test this worked as follows. • For a word w, find the neighbors N(w) of w in WordSpace."
"Remove w itself from this set. • Find the best class-label h max (N(w)) for this set (using Definition 1). • Test to see if, according to WordNet, h max is a hy-pernym of the original word w, and if so check how closely h max subsumes w in the taxonomy."
"Since our class-labelling algorithm gives a ranked list of possible hypernyms, credit was given for correct clas-sifications in the top 4 places."
"This algorithm was tested on singular common nouns (PoS-tag nn1), proper nouns (PoS-tag np0) and finite present-tense verbs (PoS-tag vvb)."
"For each of these classes, a random sample of words was selected with corpus frequencies ranging from 1000 to 250."
"For the noun categories, 600 words were sam-pled, and for the finite verbs, 420."
"For each word w, we found semantic neighbors with and without using part-of-speech information."
"The same experiments were carried out using 3, 6 and 12 neighbors: we will focus on the re-sults for 3 and 12 neighbors since those for 6 neighbors turned out to be reliably ‘somewhere in between’ these two."
"The experiments in this paper describe one combination of algorithms for lexical acquisition: both the finding of semantic neighbors and the process of class-labelling could take many alternative forms, and an exhaustive evaluation of such combinations is far beyond the scope of this paper."
"Various mathematical models and distance measures are available for modelling semantic proxim-ity, and more detailed linguistic preprocessing (such as chunking, parsing and morphology) could be used in a variety of ways."
"As an initial step, the way the granularity of part-of-speech classification affects our results for lex-ical acquistion will be investigated."
"The class-labelling algorithm could be adapted to use more sensitive mea-sures of distance[REF_CITE], and corre-lations between taxonomic distance and WordSpace sim-ilarity used as a filter."
The coverage and accuracy of the initial taxonomy we are hoping to enrich has a great influence on success rates for our methods as they stand.
"Since these are precisely the aspects of the taxonomy we are hoping to improve, this raises the question of whether we can use automati-cally obtained hypernyms as well as the hand-built ones to help classification."
"This could be tested by randomly removing many nodes from WordNet before we begin, and measuring the effect of using automatically derived classifications for some of these words (possibly those with high confidence scores) to help with the subsequent classification of others."
"The use of semantic neighbors and class-labelling for computing with meaning go far beyond the experimen-tal set up for lexical acquisition described in this pa-per — for example,[REF_CITE]used the idea of a most informative subsuming node (which can be re-garded as a kind of class-label) for disambiguation, as did[REF_CITE]with the conceptual density algorithm."
"Taking a whole domain as a ‘context’, this approach to disambiguation can be used for lexical tun-ing."
"For example, using the Ohsumed corpus of medical abstracts, the top few neighbors of operation are amputa-tion, disease, therapy and resection."
"Our algorithm gives medical care, medical aid and therapy as possible class-labels for this set, which successfully picks out the sense of operation which is most important for the medical do-main."
The level of detail which is appropriate for defining and grouping terms depends very much on the domain in question.
"For example, the immediate hypernyms offered by WordNet for the word trout include fish, foodstuff, salmonid, malacopterygian, teleost fish, food fish, saltwater fish"
Many of these classifications are inappropriately fine-grained for many circumstances.
"To find a degree of abstraction which is suitable for the way trout is used in the BNC, we found its semantic neighbors which in-clude herring swordfish turbot salmon tuna."
The highest-scoring class-labels for this set are
The preferred labels are the ones most humans would an-swer if asked what a trout is.
"This process can be used to select the concepts from an ontology which are ap-propriate to a particular domain in a completely unsuper-vised fashion, using only the documents from that do-main whose meanings we wish to describe."
A serious bottleneck in the development of trainable text summarization systems is the shortage of training data.
"Constructing such data is a very tedious task, especially because there are in general many different correct ways to summarize a text."
Fortunately we can utilize the Internet as a source of suitable training data.
"In this paper, we present a summarization system that uses the web as the source of training data."
"The procedure involves structuring the articles downloaded from various websites, building adequate corpora of (summary, text) and (extract, text) pairs, training on positive and negative data, and automatically learning to perform the task of extraction-based summarization at a level comparable to the best DUC systems."
The task of an extraction-based text summarizer is to select from a text the most important sentences that are in size a small percentage of the original text yet still as informative as the full text[REF_CITE].
"Typically, trainable summarization systems characterize each sentence according to a set of predefined features and then learn from training material which feature combinations are indicative of good extract sentences."
"In order to learn the characteristics of indicative summarizing sentences, a large enough collection of (summary, text) pairs must be provided to the system."
"Research in automated text summarization is constantly troubled by the difficulty of finding or constructing large collections of (extract, text) pairs."
"Usually, (abstract, text) pairs are available and can be easily obtained (though not in sufficient quantity to support fully automated learning for large domains)."
But abstract sentences are not identical to summary sentences and hence make direct comparison difficult.
"Therefore, some algorithms have been introduced to generate (extract, text) pairs expanded from (abstract, text) inputs[REF_CITE]."
The explosion of the World Wide Web has made accessible billions of documents and newspaper articles.
"If one could automatically find short forms of longer documents, one could build large training sets over time."
"However, one cannot today retrieve short and long texts on the same topic directly."
News published on the Internet is an exception.
"Although it is not ideally organized, the topic orientation and temporal nature of news makes it possible to impose an organization and thereby obtain a training corpus on the same topic."
"We hypothesize that weekly articles are sophisticated summaries of daily ones, and monthly articles are summaries of weekly ones, as shown in Figure 1."
"Under this hypothesis, how accurate an extract summarizer can one train?"
"In this paper we first describe the corpus reorganization, then in Section 3 the training data formulation and the system, the system evaluation in Section 4, and finally future work in Section 5."
The Yahoo Full Coverage Collection (YFCC) was downloaded[URL_CITE]
The full coverage texts were downloaded based on a snapshot of the links contained in Yahoo Full Coverage at that time.
"A spider crawled the top eight categories: U.S., World, Business, Technology, Science, Health, Entertainment, and Sports."
All news links in each category were saved in an index page that contained the headline and its full text URL.
A page fetcher then downloaded all the pages listed in the snapshot index file.
"Under the eight categories, there are 463 subcategories, 216590 news articles."
All the articles in the YFCC are preprocessed as following.
Each article is in the original raw html form with actual contents buried in layers of irrelevant tags and markings.
Identifying the text body is a challenging process[REF_CITE].
"The system identifies the main body of the article using a set of retrieval templates, and then further eliminates useless information embedded in the main body by considering each opening and closing tag set."
"For example, if the tag name indicates the contents between the opening and closing tags are images or just meta-info, the contents is discarded."
"The clean texts are then processed by a sentence breaker, Lovin’s stemmer, a part-of-speech tagger, and converted into standard XML form."
The news articles posted under Yahoo Full Coverage are from 125 different web publishers.
"Except for some well-known sites, the publishing frequencies for the rest of the sites are not known."
"But Yahoo tends to use those publishers over and over again, leaving for each publisher a trail of publishing habit."
"Our system records the publishing date for each article from each publisher chronologically, and then calculates the publishing frequency for each publisher."
"Over all the articles from a publisher, the system computes the minimum publishing gap (MPG) between two articles."
"If the MPG is less than 3 days or the MPG is unknown in the case of publishers seen only once in the YFCC, then this publisher is labeled as a daily publisher."
"If the MPG is greater than 3 days but less than 15, it is labeled as a weekly publisher."
Publishers with all other MPG values are labeled as monthly publishers.
"For each article in the collection, the system relabels it as a daily, weekly, or monthly publication."
"Each domain under each category in the collection is then restructured into a hierarchy by year, months within the year, weeks of each month, and finally days of each week."
The visualization of an example of the hierarchical structure of the domain Africa under category World is shown in Figure 2.
"Recognizing (summary, text) pairs automatically from the web repository is the key to overcoming the constant shortage of summarization training data."
"After taking a closer examination of the reorganized YFCC, one notices that for each day, there are a number of articles published that update the progress of a particular news topic."
Daily articles are published by identified daily publishers.
"Then at the end of each week, there are several weekly articles published by weekly publishers on the same topic."
"At the end of each month, again there are articles on the same topic posted by publishers labeled as monthly publishers."
"There is a common thematic connection between the daily articles and the weekly articles, and between the weekly articles and the monthly articles."
"The daily articles on a particular event are more detailed, and are written step-by-step as it was happening."
The weekly articles review the daily articles and recite important snippets from the daily news.
The monthly articles are written in a more condensed fashion quoting from the weeklies.
"Instead of asking human judges to identify informative sentences in documents, and since beautifully written “summaries” are already available, we need to align the sentences from the daily articles with each weekly article sentence, and align weekly article sentences with each monthly article sentence, in order to collect the (summary, text) pairs and eventually generate (extract, text) pairs."
The pairs are constructed at both sentence and document levels.
"In our system, three methods for sentence-level and document-level alignment are investigated: • extraction-based:"
"We duplicated this algorithm but replaced inputs with (summary, text), parents and their respective children in the hierarchical domain tree."
"Thus for example, the summary is a monthly article when the text is a weekly article or a weekly article when the text is a daily one."
We start with the cosine-similarity metric stated[REF_CITE]and keep deleting sentences that are not related to the summary document until any more deletion would result in a drop in similarity with the summary.
The resulting set of sentences is the extract concerning the topic discussed in the summary.
"It forms the pair (extract, text)."
"If there is more than one summary for a particular text (nonsummary article), the resulting extracts will vary if the summary articles are written on the same event, but are focused on different perspectives."
"Thus, a summary article may be aligned with several extracts and extracts generated from a single text may align with many summaries."
"The relationship amongst summaries, extracts, and texts forms a network topology."
"To generate sentence level alignment, we replaced the input with (summary sentence, text) pairs."
"Starting with a nonsummary text, the sentences that are irrelevant to the summary sentence are deleted repeatedly, resulting in the preservation of sentences similar in meaning to the summary sentence."
"For each sentence in the summary, it is aligned with a number of nonsummary sentences to form (summary sentence, nonsummary sentences) pairs."
This alignment is done for each sentence of the summary articles.
"Finally for each nonsummary we group together all the aligned sentences to form the pair (extract, text). • similarity-based: inspired by sentence alignment for multilingual parallel corpora in Machine Translati[REF_CITE], we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level."
"In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain."
"Measuring the cosine-similarity between two articles, we can decide whether they are close semantically."
This method has been widely used in Information Retrieval[REF_CITE].
"To extend this idea, we measure the cosine-similarity between two sentences, one from a summary (weekly or monthly article) and the other one from a nonsummary (daily or weekly article)."
"If the similarity score between the two crosses a predetermined threshold, the two sentences are aligned to form the pair (summary sentence, text sentence)."
The relationship between sentences is many-to-many.
"With any particular nonsummary article, sentences that are aligned with summary sentences form the extract and the pair (extract, text). • summary-based: concerned with the noise that may accompany similarity calculations from extraction-based and similarity-based alignments, we align an entire summary article with all its nonsummary articles published in the same time period, as determined from the previously described chronological reorganization."
"The alignment results are pairs of the format (summary, texts)."
One summary can only be aligned with a certain group of nonsummaries.
Each nonsummary can be aligned with many summaries.
No sentence level alignment is done with this method.
The main goal of a leaning-based extraction summarization system is to learn the ability to judge whether a particular sentence in a text appear in the extract or not.
"Therefore, two sets of training data are needed, one indicative enough for the system to select a sentence to be in the extract (labeled as positive data), the other indicative enough for the system to keep the sentence from being added to the extract (labeled as negative data)."
"For each of the alignment methods, we produce summary training data and nonsummary training data for each domain in the YFCC."
"From extraction-based and similarity-based alignment methods, for each nonsummary article, there are two sets of sentences, the set of sentences that compose the extract with the respect to some summary article or align with summary sentences, and the rest of the sentences that are not related to the summary or aligned."
The two sets of sentences over all articles in the domain form the positive and negative training data sets.
"Using summary-based alignment, all the summary articles are in the positive training set, and all the nonsummary material is in the negative set."
Full texts are used.
We treat each domain independently.
"Using a bigram model, we estimate the desirability of a sentence appearing in the extract P(S) from the summary training data as:"
P(w 1 | start)
P(w 2 | w 1 )…P(w n | w n-1 )
We estimate the desirability of a sentence not appearing in the extract P’(S) from the nonsummary training data as:
P’(w 1 | start) P’(w 2 | w 1 )…P’(w n | w n-1 )
"For each domain in the YFCC, a summary bigram table and a nonsummary bigram table are created."
"In our system, we started with a similar idea of a lattice for summary extraction."
"In Figure 3, E states emit sentences that are going to be in the extract, and N states emit all other sentences."
"Given an input sentence, if P(S) is greater than P’(S), it means that the sentence has a higher desirability of being an extraction sentence; otherwise, the sentence will not be included in the resulting extract."
"After reading in the last sentence from the input, the extract is created by traversing the path from start state to end state and only outputting the sentences emitted by the E states."
The extracts generated are in size shorter than the original texts.
"However, the number of sentences that E states emit cannot be predetermined."
This results in unpredictable extract length.
"Most frequently, longer extracts are produced."
The system needs more control over how long extracts will be in order for meaningful evaluation to be conducted.
"To follow up on the lattice idea, we used the following scoring mechanism:"
R = P(S) / P’(S)
R indicates the desirability ratio of the sentence being in the extract over it being left out.
"For each sentence from the input, it is assigned an R score."
Then all the sentences with their R scores are sorted in descending order.
"With respect to the length restriction, we choose only the top n R-scored sentences."
"On average for each domain, the summary-bigram table contains 20000 entries; the nonsummary-bigram table contains 173000 entries."
"When an unknown text or a set of unknown texts come in to be summarized, the system needs to select the most appropriate pair of bigram tables to create the extract."
The most desirable domain for an unknown text or texts contains articles focusing on the same issues as the unknown ones.
"Two methods are used: • topic signature[REF_CITE]: a topic signature is a family of related terms {topic, signature}, where topic is the target concept and signature is a vector of related terms."
The topic in the formula is assigned with the domain name.
"To construct the set of related words, we consider only nouns because we are only interested in the major issues discussed in the domain, not in how those issues evolved."
Each noun in the domain receives a tf.idf score. 30 top-scoring nouns are selected to be the signature representing the domain.
"For each test text, its signature is computed with the same tf.idf method against each domain."
The domain that has the highest number of overlaps in signature words is selected and its bigram tables are used to construct the extract of the test text.
The following table illustrates.
"Inputs are three sets of 10 documents each from the[REF_CITE]training corpus concerning the topics on Africa, earthquake, and Iraq, respectively."
The scores are the total overlaps between a domain and each individual test set.
The Three sets are all correctly classified. • hierarchical signature: each domain is given a name when it was downloaded.
The name gives a description of the domain at the highest level.
"Since the name is the most informative word, if we gather the words that most frequently co-occur within the sentence(s) that contain the name itself, a list of less informative but still important words can become part of the domain signature."
"Using this list of words, we find another list of words that most frequently co-occur with each of them individually."
"Therefore, a three-layer hierarchical domain signature can be created: level one, the domain name; level two, 10 words with the highest co-occurrences with the domain name; level three, 10 words that most frequently co-occur with level two signatures."
Again only nouns are considered.
"For example, for domain on Iraq, the level one signature is “Iraq”; level two signatures are “Saddam”, “sanction”, “weapon”, “Baghdad”, and etc.; third level signatures are “Gulf”, “UN”, “Arab”, “security”, etc."
The document signature for the test text is computed the same way as in the topic signature method.
"Overlap between the domain signature and the document signature is computed with a different scoring system, in which the weights are chosen by hand."
"If level one is matched, add 10 points; for each match at level two, add 2 points; for each match at level three, add 1 point."
The domain that receives the highest points will be selected.
A much deeper signature hierarchy can be created recursively.
"Through experiment, we see that a three-level signature suffices."
The following table shows the effects of this method:
"Since it worked well for our test domains, we employed the topic-signature method in selecting training domains."
"To determine which of the alignment methods of Section 3.1 is best, we need true summaries, not monthly or weekly articles from the web."
"We tested the equivalencies of the three methods on three sets of articles from the[REF_CITE]training corpus, which includes human-generated “gold standard” summaries."
"They are on the topics of Africa, earthquake, and Iraq."
The following table shows the results of this experiment.
"Each entry demonstrates the cosine similarity, using the tf.idf score, of the extracts generated by the system using training data created from the alignment method in the column, compare to the summaries generated by human."
"We see that all three methods produce roughly equal extracts, when compared with the gold standard summaries."
The summary-based alignment method is the least time consuming and the most straightforward method to use in practice.
"All articles in each directory are used to make the selection of its corresponding training domain, as described in Section 3.5."
"Even if no domain completely covers the event, the best one is selected by the system."
"To evaluate system performance on summary creation, we randomly selected one article from each directory from the[REF_CITE]testing corpus, for each article, there are three human produced summaries."
Our system summarizes each article three times with the length restriction respectively set to the lengths of the three human summaries.
We also evaluated the[REF_CITE]single-document summarization baseline system results (first 100 words from each document) to set a lower bound.
"To see the upper bound, each human generated summary is judged against the other two human summaries on the same article."
"The following table is the evaluation results using the SEE system version 1.0[REF_CITE], with visualization in Figure 4."
"Summary model units are graded as full, partial, or none in completeness in coverage with the peer model units."
And Figure 5 shows an example of the comparison between the human-created summary and the system-generated extract.
The performance is reported on four metrics.
Recall measures how well a summarizer retains original content.
Precision measures how well a system generates summaries.
SRECALL and SPRECISION are the strict recall and strict precision that take into consideration only units with full completeness in unit coverage.
LRECALL and LPRECISION are the lenient recall and lenient precision that count units with partial and full completeness in unit coverage.
"Extract summaries that are produced by our system has comparable performance in recall with SMU, meaning that the coverage of important information is good."
But our system shows weakness in precision due to the fact that each sentence in the system-generated extract is not compressed in any way.
Each sentence in the extract has high coverage over the human summary.
But sentences that have no value have also been included in the result.
"This causes long extracts on average, hence, the low average in precision measure."
"Since our sentence ranking mechanism is based on desirability, sentences at the end of the extract are less desirable and can be removed."
This needs further investigation.
Clearly there is the need to reduce the size of the generated summaries.
"In order to produce simple and concise extracts, sentence compression needs to be performed[REF_CITE]."
"Despite the problems, however, our system’s performance places it at equal level to the top-scoring systems[REF_CITE]."
"Now that the[REF_CITE]material is also available, we will compare our results to their top-scoring system as well."
"One important stage in developing a learning-based extraction summarization system is to find sufficient and relevant collections of (extract, text) pairs."
This task is also the most difficult one since resources of constructing the pairs are scarce.
"To solve this bottleneck, one wonders whether the web can be seen as a vast repository that is waiting to be tailored in order to fulfill our quest in finding summarization training data."
We have discovered a way to find short forms of longer documents and have built an extraction-based summarizer learning from reorganizing news articles from the World Wide Web and performing at a level comparable[REF_CITE]systems.
We are excited about the power of how reorganization of the web news articles has brought us and will explore this idea in other tasks of natural language processing.
Multi-document summarization naturally comes into picture for future development.
Our corpus organization itself is in the form of multiple articles being summarized into one (monthly or weekly).
How do we learn and use this structure to summarize a new set of articles?
Headline generation is another task that we can approach equipped with our large restructured web corpus.
"We believe that the answers to these questions are embedded in the characteristics of the corpus, namely the WWW, and are eager to discover them in the near future."
